<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.minimizingregret.com/" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at January 29, 2019 10:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-6555947.post-6062056358171439265">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/suresh.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://feedproxy.google.com/~r/TheGeomblog/~3/G-UqSGr3bSg/fat-session-2-systems-and-measurement.html">FAT* Session 2: Systems and Measurement.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Building systems that have fairness properties and monitoring systems that do A/B testing on us.<br /><br /><a href="https://algorithmicfairness.wordpress.com/2019/01/28/fat-papers-systems-and-measurement/">Session 2 of FAT*</a>: my opinionated summary.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=G-UqSGr3bSg:UU9jYzynrL0:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=G-UqSGr3bSg:UU9jYzynrL0:63t7Ie-LG7Y"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=63t7Ie-LG7Y" border="0" /></a>
</div><img src="http://feeds.feedburner.com/~r/TheGeomblog/~4/G-UqSGr3bSg" alt="" width="1" height="1" /></div>







<p class="date">
by Suresh Venkatasubramanian (noreply@blogger.com) <a href="http://feedproxy.google.com/~r/TheGeomblog/~3/G-UqSGr3bSg/fat-session-2-systems-and-measurement.html"><span class="datestr">at January 29, 2019 06:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09877">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09877">Dominating Sets and Connected Dominating Sets in Dynamic Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hjuler:Niklas.html">Niklas Hjuler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Italiano:Giuseppe_F=.html">Giuseppe F. Italiano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parotsidis:Nikos.html">Nikos Parotsidis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saulpic:David.html">David Saulpic</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09877">PDF</a><br /><b>Abstract: </b>In this paper we study the dynamic versions of two basic graph problems:
Minimum Dominating Set and its variant Minimum Connected Dominating Set. For
those two problems, we present algorithms that maintain a solution under edge
insertions and edge deletions in time $O(\Delta\cdot \text{polylog}~n)$ per
update, where $\Delta$ is the maximum vertex degree in the graph. In both
cases, we achieve an approximation ratio of $O(\log n)$, which is optimal up to
a constant factor (under the assumption that $P \ne NP$). Although those two
problems have been widely studied in the static and in the distributed
settings, to the best of our knowledge we are the first to present efficient
algorithms in the dynamic setting.
</p>
<p>As a further application of our approach, we also present an algorithm that
maintains a Minimal Dominating Set in $O(min(\Delta, \sqrt{m}))$ per update.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09877"><span class="datestr">at January 29, 2019 02:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09863">Efficient Multiparty Interactive Coding for Insertions, Deletions and Substitutions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gelles:Ran.html">Ran Gelles</a>, Yael T. Kalai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramnarayan:Govind.html">Govind Ramnarayan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09863">PDF</a><br /><b>Abstract: </b>In the field of interactive coding, two or more parties wish to carry out a
distributed computation over a communication network that may be noisy. The
ultimate goal is to develop efficient coding schemes that can tolerate a high
level of noise while increasing the communication by only a constant factor
(i.e., constant rate).
</p>
<p>In this work we consider synchronous communication networks over an arbitrary
topology, in the powerful adversarial insertion-deletion noise model. Namely,
the noisy channel may adversarially alter the content of any transmitted
symbol, as well as completely remove a transmitted symbol or inject a new
symbol into the channel. We provide efficient, constant rate schemes that
successfully conduct any computation with high probability as long as the
adversary corrupts at most $\varepsilon /m$ fraction of the total
communication, where $m$ is the number of links in the network and
$\varepsilon$ is a small constant. This scheme assumes the parties share a
random string to which the adversarial noise is oblivious. We can remove this
assumption at the price of being resilient to $\varepsilon / (m\log m)$
adversarial error.
</p>
<p>While previous work considered the insertion-deletion noise model in the
two-party setting, to the best of our knowledge, our scheme is the first
multiparty scheme that is resilient to insertions and deletions. Furthermore,
our scheme is the first computationally efficient scheme in the multiparty
setting that is resilient to adversarial noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09863"><span class="datestr">at January 29, 2019 02:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09858">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09858">Utility Preserving Secure Private Data Release</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhaliwal:Jasjeet.html">Jasjeet Dhaliwal</a>, Geoffrey So, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parker=Wood:Aleatha.html">Aleatha Parker-Wood</a>, Melanie Beck <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09858">PDF</a><br /><b>Abstract: </b>Differential privacy mechanisms that also make reconstruction of the data
impossible come at a cost - a decrease in utility. In this paper, we tackle
this problem by designing a private data release mechanism that makes
reconstruction of the original data impossible and also preserves utility for a
wide range of machine learning algorithms. We do so by combining the
Johnson-Lindenstrauss (JL) transform with noise generated from a Laplace
distribution. While the JL transform can itself provide privacy guarantees
\cite{blocki2012johnson} and make reconstruction impossible, we do not rely on
its differential privacy properties and only utilize its ability to make
reconstruction impossible. We present novel proofs to show that our mechanism
is differentially private under single element changes as well as single row
changes to any database. In order to show utility, we prove that our mechanism
maintains pairwise distances between points in expectation and also show that
its variance is proportional to the the dimensionality of the subspace we
project the data into. Finally, we experimentally show the utility of our
mechanism by deploying it on the task of clustering.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09858"><span class="datestr">at January 29, 2019 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09739">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09739">A Faster Solution to Smale's 17th Problem I: Real Binomial Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paouris:Grigoris.html">Grigoris Paouris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillipson:Kaitlyn.html">Kaitlyn Phillipson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rojas:J=_Maurice.html">J. Maurice Rojas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09739">PDF</a><br /><b>Abstract: </b>Suppose $F:=(f_1,\ldots,f_n)$ is a system of random $n$-variate polynomials
with $f_i$ having degree $\leq\!d_i$ and the coefficient of $x^{a_1}_1\cdots
x^{a_n}_n$ in $f_i$ being an independent complex Gaussian of mean $0$ and
variance $\frac{d_i!}{a_1!\cdots a_n!\left(d_i-\sum^n_{j=1}a_j \right)!}$.
Recent progress on Smale's 17th Problem by Lairez --- building upon seminal
work of Shub, Beltran, Pardo, B\"{u}rgisser, and Cucker --- has resulted in a
deterministic algorithm that finds a single (complex) approximate root of $F$
using just $N^{O(1)}$ arithmetic operations on average, where
$N\!:=\!\sum^n_{i=1}\frac{(n+d_i)!}{n!d_i!}$ ($=n(n+\max_i
d_i)^{O(\min\{n,\max_i d_i)\}}$) is the maximum possible total number of
monomial terms for such an $F$. However, can one go faster when the number of
terms is smaller, and we restrict to real coefficient and real roots? And can
one still maintain average-case polynomial-time with more general probability
measures?
</p>
<p>We show the answer is yes when $F$ is instead a binomial system --- a case
whose numerical solution is a key step in polyhedral homotopy algorithms for
solving arbitrary polynomial systems. We give a deterministic algorithm that
finds a real approximate root (or correctly decides there are none) using just
$O(n^2(\log(n)+\log\max_i d_i))$ arithmetic operations on average. Furthermore,
our approach allows Gaussians with arbitrary variance. We also discuss briefly
the obstructions to maintaining average-case time polynomial in $n\log \max_i
d_i$ when $F$ has more terms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09739"><span class="datestr">at January 29, 2019 02:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09527">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09527">Bipartite Envy-Free Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Segal=Halevi:Erel.html">Erel Segal-Halevi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09527">PDF</a><br /><b>Abstract: </b>Bipartite Envy-Free Matching (BEFM) is a relaxation of perfect matching. In a
bipartite graph with parts X and Y, a BEFM is a matching of some vertices in X
to some vertices in Y, such that each unmatched vertex in X is not adjacent to
any matched vertex in Y (so the unmatched vertices do not "envy" the matched
ones). The empty matching is always a BEFM. This paper presents sufficient and
necessary conditions for the existence of a non-empty BEFM. These conditions
are based on cardinality of neighbor-sets, similarly to Hall's condition for
the existence of a perfect matching. The conditions can be verified in
polynomial time, and in case they are satisfied, a non-empty BEFM can be found
by a polynomial-time algorithm. The paper presents some applications of BEFM as
a subroutine in fair division algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09527"><span class="datestr">at January 29, 2019 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09515">Black Box Submodular Maximization: Discrete and Continuous Settings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lin.html">Lin Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Mingrui.html">Mingrui Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassani:Hamed.html">Hamed Hassani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karbasi:Amin.html">Amin Karbasi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09515">PDF</a><br /><b>Abstract: </b>In this paper, we consider the problem of black box continuous submodular
maximization where we only have access to the function values and no
information about the derivatives is provided. For a monotone and continuous
DR-submodular function, and subject to a bounded convex body constraint, we
propose Black-box Continuous Greedy, a derivative-free algorithm that provably
achieves the tight $[(1-1/e)OPT-\epsilon]$ approximation guarantee with
$O(d/\epsilon^3)$ function evaluations. We then extend our result to the
stochastic setting where function values are subject to stochastic zero-mean
noise. It is through this stochastic generalization that we revisit the
discrete submodular maximization problem and use the multi-linear extension as
a bridge between discrete and continuous settings. Finally, we extensively
evaluate the performance of our algorithm on continuous and discrete submodular
objective functions using both synthetic and real data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09515"><span class="datestr">at January 29, 2019 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09505">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09505">Note on distance matrix hashing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>I. A. Junussov <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09505">PDF</a><br /><b>Abstract: </b>Hashing algorithm of dynamical set of distances is described. Proposed
hashing function is residual. Data structure which implementation accelerates
computations is presented
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09505"><span class="datestr">at January 29, 2019 02:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09434">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09434">Parameterized Complexity of Safe Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belmonte:R=eacute=my.html">Rémy Belmonte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katsikarelis:Ioannis.html">Ioannis Katsikarelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lampis:Michael.html">Michael Lampis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ono:Hirotaka.html">Hirotaka Ono</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09434">PDF</a><br /><b>Abstract: </b>In this paper we study the problem of finding a small safe set $S$ in a graph
$G$, i.e. a non-empty set of vertices such that no connected component of
$G[S]$ is adjacent to a larger component in $G - S$. We enhance our
understanding of the problem from the viewpoint of parameterized complexity by
showing that (1) the problem is W[2]-hard when parameterized by the pathwidth
$pw$ and cannot be solved in time $n^{o(pw)}$ unless the ETH is false, (2) it
admits no polynomial kernel parameterized by the vertex cover number $vc$
unless $\mathrm{PH} = \Sigma^{\mathrm{p}}_{3}$, but (3) it is fixed-parameter
tractable (FPT) when parameterized by the neighborhood diversity $nd$, and (4)
it can be solved in time $n^{f(cw)}$ for some double exponential function $f$
where $cw$ is the clique-width. We also present (5) a faster FPT algorithm when
parameterized by solution size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09434"><span class="datestr">at January 29, 2019 02:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09423">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09423">Subspace arrangements, graph rigidity and derandomization through submodular optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raz:Orit_E=.html">Orit E. Raz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09423">PDF</a><br /><b>Abstract: </b>This paper presents a deterministic, strongly polynomial time algorithm for
computing the matrix rank for a class of symbolic matrices (whose entries are
polynomials over a field). This class was introduced, in a different language,
by Lov\'asz [Lov] in his study of flats in matroids, and proved a duality
theorem putting this problem in $NP \cap coNP$. As such, our result is another
demonstration where ``good characterization'' in the sense of Edmonds leads to
an efficient algorithm. In a different paper Lov\'asz [Lov79] proved that all
such symbolic rank problems have efficient probabilistic algorithms, namely are
in $BPP$. As such, our algorithm may be interpreted as a derandomization
result, in the long sequence special cases of the PIT (Polynomial Identity
Testing) problem. Finally, Lov\'asz and Yemini [LoYe] showed how the same
problem generalizes the graph rigidity problem in two dimensions. As such, our
algorithm may be seen as a generalization of the well-known deterministic
algorithm for the latter problem.
</p>
<p>There are two somewhat unusual technical features in this paper. The first is
the translation of Lov\'asz' flats problem into a symbolic rank one. The second
is the use of submodular optimization for derandomization. We hope that the
tools developed for both will be useful for related problems, in particular for
better understanding of graph rigidity in higher dimensions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09423"><span class="datestr">at January 29, 2019 02:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09355">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09355">Nearly Optimal Sparse Polynomial Multiplication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09355">PDF</a><br /><b>Abstract: </b>In the sparse multiplication problem, one is asked to multiply two sparse
polynomials $f$ and $g$ in time that is proportional to the size of the input
plus the size of the output. The polynomials are given via lists of their
coefficients $F$ and $G$, respectively. Cole and Hariharan (STOC 02) have given
a nearly optimal algorithm when the coefficients are positive, and Arnold and
Roche (ISSAC 15) devised an algorithm running in time proportional to the
``structural sparsity'' of the product, i.e. the set $\mathrm{supp}(F) +
\mathrm{supp}(G)$. The latter algorithm is particularly efficient when there
not ''too many cancellations'' of coefficients in the product.
</p>
<p>In this work we give a clean, nearly optimal algorithm for the sparse
polynomial multiplication problem, resolving an open question posed by Roche
(ISSAC 18).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09355"><span class="datestr">at January 29, 2019 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09349">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09349">Large Minors in Expanders</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chuzhoy:Julia.html">Julia Chuzhoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nimavat:Rachit.html">Rachit Nimavat</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09349">PDF</a><br /><b>Abstract: </b>In this paper we study expander graphs and their minors. Specifically, we
attempt to answer the following question: what is the largest function
$f(n,\alpha,d)$, such that every $n$-vertex $\alpha$-expander with maximum
vertex degree at most $d$ contains {\bf every} graph $H$ with at most
$f(n,\alpha,d)$ edges and vertices as a minor? Our main result is that there is
some universal constant $c$, such that $f(n,\alpha,d)\geq \frac{n}{c\log
n}\cdot \left(\frac{\alpha}{d}\right )^c$. This bound achieves a tight
dependence on $n$: it is well known that there are bounded-degree $n$-vertex
expanders, that do not contain any grid with $\Omega(n/\log n)$ vertices and
edges as a minor. The best previous result showed that $f(n,\alpha,d) \geq
\Omega(n/\log^{\kappa}n)$, where $\kappa$ depends on both $\alpha$ and $d$.
Additionally, we provide a randomized algorithm, that, given an $n$-vertex
$\alpha$-expander with maximum vertex degree at most $d$, and another graph $H$
containing at most $\frac{n}{c\log n}\cdot \left(\frac{\alpha}{d}\right )^c$
vertices and edges, with high probability finds a model of $H$ in $G$, in time
poly$(n)\cdot (d/\alpha)^{O\left( \log(d/\alpha) \right)}$.
</p>
<p>We note that similar but stronger results were independently obtained by
Krivelevich and Nenadov: they show that $f(n,\alpha,d)=\Omega
\left(\frac{n\alpha^2}{d^2\log n} \right)$, and provide an efficient algorithm,
that, given an $n$-vertex $\alpha$-expander of maximum vertex degree at most
$d$, and a graph $H$ with $O\left( \frac{n\alpha^2}{d^2\log n} \right)$
vertices and edges, finds a model of $H$ in $G$.
</p>
<p>Finally, we observe that expanders are the `most minor-rich' family of graphs
in the following sense: for every $n$-vertex and $m$-edge graph $G$, there
exists a graph $H$ with $O \left( \frac{n+m}{\log n} \right)$ vertices and
edges, such that $H$ is not a minor of $G$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09349"><span class="datestr">at January 29, 2019 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09298">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09298">Bipartitioning of directed and mixed random graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lipowski:Adam.html">Adam Lipowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferreira:Antonio_Luis.html">Antonio Luis Ferreira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lipowska:Dorota.html">Dorota Lipowska</a>, Manuel A. Barroso <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09298">PDF</a><br /><b>Abstract: </b>We show that an intricate relation of cluster properties and optimal
bipartitions, which takes place in undirected random graphs, extends to
directed and mixed random graphs. In particular, the satisfability threshold
coincides with the relative size of the giant OUT component reaching~{1/2}.
Moreover, when counting undirected links as two directed ones, the partition
cost, and cluster properties, as well as location of the replica symmetry
breaking transition for these random graphs depend primarily on the total
number of directed links and not on their specific distribution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09298"><span class="datestr">at January 29, 2019 02:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09234">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09234">Plantinga-Vegter algorithm takes average polynomial time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cucker:Felipe.html">Felipe Cucker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erg=uuml=r:Alperen_A=.html">Alperen A. Ergür</a>, Josue Tonelli-Cueto <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09234">PDF</a><br /><b>Abstract: </b>We exhibit a condition-based analysis of the adaptive subdivision algorithm
due to Plantinga and Vegter. The first complexity analysis of the PV Algorithm
is due to Burr, Gao and Tsigaridas who proved a $O\big(2^{\tau d^{4}\log
d}\big)$ worst-case cost bound for degree $d$ plane curves with maximum
coefficient bit-size $\tau$. This exponential bound, it was observed, is in
stark contrast with the good performance of the algorithm in practice. More in
line with this performance, we show that, with respect to a broad family of
measures, the expected time complexity of the PV Algorithm is bounded by
$O(d^7)$ for real, degree $d$, plane curves. We also exhibit a smoothed
analysis of the PV Algorithm that yields similar complexity estimates. To
obtain these results we combine robust probabilistic techniques coming from
geometric functional analysis with condition numbers and the continuous
amortization paradigm introduced by Burr, Krahmer and Yap. We hope this will
motivate a fruitful exchange of ideas between the different approaches to
numerical computation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09234"><span class="datestr">at January 29, 2019 02:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09161">Competitive Online Optimization under Inventory Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Qiulin.html">Qiulin Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yi:Hanling.html">Hanling Yi</a>, John Pang, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Minghua.html">Minghua Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wierman:Adam.html">Adam Wierman</a>, Michael Honig, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Yuanzhang.html">Yuanzhang Xiao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09161">PDF</a><br /><b>Abstract: </b>This paper studies online optimization under inventory (budget) constraints.
While online optimization is a well-studied topic, versions with inventory
constraints have proven difficult. We consider a formulation of
inventory-constrained optimization that is a generalization of the classic
one-way trading problem and has a wide range of applications. We present a new
algorithmic framework, \textsf{CR-Pursuit}, and prove that it achieves the
minimal competitive ratio among all deterministic algorithms (up to a
problem-dependent constant factor) for inventory-constrained online
optimization. Our algorithm and its analysis not only simplify and unify the
state-of-the-art results for the standard one-way trading problem, but they
also establish novel bounds for generalizations including concave revenue
functions. For example, for one-way trading with price elasticity, the
\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within a
small additive constant (i.e., 1/3) to the lower bound of $\ln \theta+1$, where
$\theta$ is the ratio between the maximum and minimum base prices.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09161"><span class="datestr">at January 29, 2019 02:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09154">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09154">Star Routing: Between Vehicle Routing and Vertex Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Donne:Diego_Delle.html">Diego Delle Donne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tagliavini:Guido.html">Guido Tagliavini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09154">PDF</a><br /><b>Abstract: </b>We consider an optimization problem posed by an actual newspaper company,
which consists of computing a minimum length route for a delivery truck, such
that the driver only stops at street crossings, each time delivering copies to
all customers adjacent to the crossing. This can be modeled as an abstract
problem that takes an unweighted simple graph $G = (V, E)$ and a subset of
edges $X$ and asks for a shortest cycle, not necessarily simple, such that
every edge of $X$ has an endpoint in the cycle.
</p>
<p>We show that the decision version of the problem is strongly NP-complete,
even if $G$ is a grid graph. Regarding approximate solutions, we show that the
general case of the problem is APX-hard, and thus no PTAS is possible unless P
$=$ NP. Despite the hardness of approximation, we show that given any
$\alpha$-approximation algorithm for metric TSP, we can build a
$3\alpha$-approximation algorithm for our optimization problem, yielding a
concrete $9/2$-approximation algorithm.
</p>
<p>The grid case is of particular importance, because it models a city map or
some part of it. A usual scenario is having some neighborhood full of
customers, which translates as an instance of the abstract problem where almost
every edge of $G$ is in $X$. We model this property as $|E - X| = o(|E|)$, and
for these instances we give a $(3/2 + \varepsilon)$-approximation algorithm,
for any $\varepsilon &gt; 0$, provided that the grid is sufficiently big.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09154"><span class="datestr">at January 29, 2019 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=334">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/01/28/tcs-talk-wednesday-february-6-ran-canetti-bu-and-tau/">TCS+ talk: Wednesday, February 6 — Ran Canetti, BU and TAU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The new season of TCS+ is about to start! Our first talk for Spring will take place next Wednesday, February 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Ran Canetti</strong> from BU and TAU will speak about “<em>Fully Bideniable Interactive Encryption</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: While standard encryption guarantees secrecy of the encrypted plaintext only against an attacker that has no knowledge of the communicating parties’ keys and randomness of encryption, deniable encryption [Canetti et al., Crypto’96] provides the additional guarantee that the plaintext remains secret even in face of entities that attempt to coerce (or bribe) the communicating parties to expose their internal states, including the plaintexts, keys and randomness. To achieve this guarantee, deniable encryption equips the parties with faking algorithms which allow them to generate fake keys and randomness that make the ciphertext appear consistent with any plaintext of the parties’ choice. To date, however, only partial results were known: Either deniability against coercing only the sender, or against coercing only the receiver [Sahai-Waters, STOC ‘14] or schemes satisfying weaker notions of deniability [O’Neil et al., Crypto ‘11].</p>
<p>In this paper we present the first fully bideniable interactive encryption scheme, thus resolving the 20-years-old open problem. Our scheme also provides an additional and new guarantee: Even if the sender claims that one plaintext was used and the receiver claims a different one, the adversary has no way of figuring out who is lying – the sender, the receiver, or both. This property, which we call off-the-record deniability, is useful when the parties don’t have means to agree on what fake plaintext to claim, or when one party defects against the other. Our protocol has three messages, which is optimal [Bendlin et al., Asiacrypt’11], and needs a globally available reference string. We assume subexponential indistinguishability obfuscation (IO) and one-way functions.</p>
<p>Joint work with Sunoo Park and Oxana Poburinnaya.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/01/28/tcs-talk-wednesday-february-6-ran-canetti-bu-and-tau/"><span class="datestr">at January 28, 2019 04:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/01/28/first-airoyoung-phd-school/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/01/28/first-airoyoung-phd-school/">First AIROYoung PhD School</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
March 26-29, 2019 Rome, Italy https://workshop.airoyoung.org/2019#phd-school This is a three-days PhD School with theoretical classes and lab sessions on cutting-edge topics arising in Optimization and Simulation</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/01/28/first-airoyoung-phd-school/"><span class="datestr">at January 28, 2019 11:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-6555947.post-8169216749697822290">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/suresh.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://feedproxy.google.com/~r/TheGeomblog/~3/TROyy-Os39k/fat-blogging.html">FAT* blogging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I'll be blogging about each session of papers from the FAT* Conference. So as not to clutter your feed, the posts will be housed at the fairness blog that I co-write along with Sorelle Friedler and Carlos Scheidegger.<br /><br />The first post is on <a href="https://algorithmicfairness.wordpress.com/2019/01/27/fat-papers-framing-and-abstraction/">Session 1: Framing and Abstraction</a>.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=TROyy-Os39k:gYq646_XYaU:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=TROyy-Os39k:gYq646_XYaU:63t7Ie-LG7Y"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=63t7Ie-LG7Y" border="0" /></a>
</div><img src="http://feeds.feedburner.com/~r/TheGeomblog/~4/TROyy-Os39k" alt="" width="1" height="1" /></div>







<p class="date">
by Suresh Venkatasubramanian (noreply@blogger.com) <a href="http://feedproxy.google.com/~r/TheGeomblog/~3/TROyy-Os39k/fat-blogging.html"><span class="datestr">at January 28, 2019 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=600">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/">Selling your town to the marijuana industry</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;">I vowed to quit with marijuana, but I just can’t.  It’s addictive.</p>
<p style="text-align: justify;">We can go back to 2016, when voters were hit with legalese that can only be described as a trap.  Basically, under the mask of legalizing the consumption of marijuana, the ballot question was really about opening recreational pot shops around the corner.  No doubt many, many people voted for legalization without knowledge of this and with no desire to have pot shops in their town.  What exultation must have come from the lawyers working for the industry, when their masterstroke made it to the fine print:</p>
<h3 style="text-align: justify;">A town voting to legalize marijuana <del>may</del> MUST open pot shops.</h3>
<p style="text-align: justify;">At the same time, the administration of Newton changed.  Councilors who liked the place the way it is and wanted to protect it lost to others who wanted it more vibrant.  The new councilors and the new mayor sided with the marijuana industry.</p>
<p style="text-align: justify;">The way in which they eventually won is sinister.  The context was that everybody in Newton wants at least some restriction on the number of marijuana stores.  But don’t take my word for this claim: even the pro-pot councilors believe so, and in fact almost unanimously they put a question on the ballot about restricting the number of stores.  At the same time, many people in Newton wanted zero stores.  In another masterstroke of the saga, the councilors were able to put one group against the other.  They added another question about having zero stores, following a massive, grassroots petition which however should have put the question at a different time. Then they forced the people who wanted zero stores to vote against restricting the number of stores. This is genius.  Also, if it isn’t illegal I believe it should be.  And in perfect coup style, media outlets censored several pieces explaining the situation to the voters. The end result was what the administration had always wanted: no restriction on the number of stores. Ignore the alarms of the doctors, the police officers, and the people.  What do they know about what’s best for Newton? A joint is like a pint of beer!  Except beer does not give you permanent brain damage. Whatever, the bottom line is that the revenue will do good things for the city! Oh yes, the revenue.  Newton has 1 billion dollars in deficit.  You read well, 1 billion.  For decades we will have a fraction of the city budget wiped out to repay that. I guess they can say we are so desperately in debt that we should rake in every penny we can zone in town.  But I think a more accurate perspective is that even in their wildest dreams, cannabis sales won’t make a dent in that.  And maybe they should spend a couple of minutes thinking about the dozens of other ways we can bring money to the city without bringing the drugs.</p>
<p style="text-align: justify;">Executing their sophisticated plan cost in the neighborhood of $100k, mostly spent on a political strategy group which helped win the election.  To add insult to injury, key members of this marijuana combine, including the political strategists and those who funded them, don’t live in Newton but in towns where recreational pot stores are banned.  The marijuana combine is effectively carving out suburban Boston in areas where it’s good to live and areas where it’s good to sell pot.</p>
<p style="text-align: justify;">As is well known, nobody has any problem with legalizing marijuana consumption.  Moreover, there is absolutely no problem with buying this stuff over the internet, or stocking up at out-of-the-way stores.  Well, absolutely no problem except one.  The money wouldn’t go into the pockets of X, Y, and Z.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/"><span class="datestr">at January 28, 2019 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09017">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09017">Finding a Mediocre Player</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dumitrescu:Adrian.html">Adrian Dumitrescu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09017">PDF</a><br /><b>Abstract: </b>Consider a totally ordered set $S$ of $n$ elements; as an example, a set of
tennis players and their rankings. Further assume that their ranking is a total
order and thus satisfies transitivity and anti-symmetry.Following Frances Yao
(1974), an element (player) is said to be $(i,j)$-\emph{mediocre} if it is
neither among the top $i$ nor among the bottom $j$ elements of $S$. More than
$40$ years ago, Yao suggested a stunningly simple algorithm for finding an
$(i,j)$-mediocre element: Pick $i+j+1$ elements arbitrarily and select the
$(i+1)$-th largest among them. She also asked: "Is this the best algorithm?" No
one seems to have found such an algorithm ever since.
</p>
<p>We first provide a deterministic algorithm that beats the worst-case
comparison bound in Yao's algorithm for a large range of values of $i$ (and
corresponding suitable $j=j(i)$). We then repeat the exercise for randomized
algorithms; the average number of comparisons of our algorithm beats the
average comparison bound in Yao's algorithm for another large range of values
of $i$ (and corresponding suitable $j=j(i)$); the improvement is most notable
in the symmetric case $i=j$. Moreover, the tight bound obtained in the analysis
of Yao's algorithm allows us to give a definite answer for this class of
algorithms. In summary, we answer Yao's question as follows: (i) "Presently
not" for deterministic algorithms and (ii) "Definitely not" for randomized
algorithms. (In fairness, it should be said however that Yao posed the question
in the context of deterministic algorithms.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09017"><span class="datestr">at January 28, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09007">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09007">The conjugate gradient algorithm on well-conditioned Wishart matrices is almost deteriministic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deift:Percy.html">Percy Deift</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trogdon:Thomas.html">Thomas Trogdon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09007">PDF</a><br /><b>Abstract: </b>We prove that the number of iterations required to solve a random positive
definite linear system with the conjugate gradient algorithm is almost
deterministic for large matrices. We treat the case of Wishart matrices.
Precisely, we prove that for most choices of error tolerance, as the matrix
increases in size, the probability that the iteration count deviates from an
explicit deterministic value tends to zero. In addition, for a fixed iteration,
we show that the norm of the error vector and the norm of the residual converge
exponentially fast in probability, converge in mean, converge almost surely
and, after centering and rescaling, converge in distribution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09007"><span class="datestr">at January 28, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08836">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08836">A Laplacian Approach to $\ell_1$-Norm Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonifaci:Vincenzo.html">Vincenzo Bonifaci</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08836">PDF</a><br /><b>Abstract: </b>We propose a novel differentiable reformulation of the linearly-constrained
$\ell_1$ minimization problem, also known as the basis pursuit problem. The
reformulation is inspired by the Laplacian paradigm of network theory and leads
to a new family of gradient-based, matrix-free methods for the solution of
$\ell_1$ minimization problems. We analyze the iteration complexity of a
natural solution approach to the reformulation, based on a multiplicative
weights update scheme, as well as the iteration complexity of an accelerated
gradient scheme. The accelerated method, in particular, yields an improved
worst-case bound on the complexity of matrix-free methods of basis pursuit.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08836"><span class="datestr">at January 28, 2019 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08805">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08805">Metric Spaces with Expensive Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kerber:Michael.html">Michael Kerber</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nigmetov:Arnur.html">Arnur Nigmetov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08805">PDF</a><br /><b>Abstract: </b>In algorithms for finite metric spaces, it is common to assume that the
distance between two points can be computed in constant time, and complexity
bounds are expressed only in terms of the number of points of the metric space.
We introduce a different model where we assume that the computation of a single
distance is an expensive operation and consequently, the goal is to minimize
the number of such distance queries. This model is motivated by metric spaces
that appear in the context of topological data analysis.
</p>
<p>We consider two standard operations on metric spaces, namely the construction
of a $1+\varepsilon$-spanner and the computation of an approximate nearest
neighbor for a given query point. In both cases, we partially explore the
metric space through distance queries and infer lower and upper bounds for yet
unexplored distances through triangle inequality. For spanners, we evaluate
several exploration strategies through extensive experimental evaluation. For
approximate nearest neighbors, we prove that our strategy returns an
approximate nearest neighbor after a logarithmic number of distance queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08805"><span class="datestr">at January 28, 2019 11:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08711">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08711">Optimal Bribery in Voting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08711">PDF</a><br /><b>Abstract: </b>Studying complexity of various bribery problems has been one of the main
research focus in computational social choice. In all the models of bribery
studied so far, the briber has to pay every voter some amount of money
depending on what the briber wants the voter to report and the briber has some
budget at her disposal. Although these models successfully capture many real
world applications, in many other scenarios, the voters may be unwilling to
deviate too much from their true preferences. In this paper, we study the
computational complexity of the problem of finding a preference profile which
is as close to the true preference profile as possible and still achieves the
briber's goal subject to budget constraints. We call this problem Optimal
Bribery. We consider three important measures of distances, namely, swap
distance, footrule distance, and maximum displacement distance, and resolve the
complexity of the optimal bribery problem for many common voting rules. We show
that the problem is polynomial time solvable for the plurality and veto voting
rules for all the three measures of distance. On the other hand, we prove that
the problem is NP-complete for a class of scoring rules which includes the
Borda voting rule, maximin, Copeland$^\alpha$ for any $\alpha\in[0,1]$, and
Bucklin voting rules for all the three measures of distance even when the
distance allowed per voter is $1$ for the swap and maximum displacement
distances and $2$ for the footrule distance even without the budget constraints
(which corresponds to having an infinite budget). For the $k$-approval voting
rule for any constant $k&gt;1$ and the simplified Bucklin voting rule, we show
that the problem is NP-complete for the swap distance even when the distance
allowed is $2$ and for the footrule distance even when the distance allowed is
$4$ even without the budget constraints.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08711"><span class="datestr">at January 28, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08708">Almost Boltzmann Exploration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Harsh.html">Harsh Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kong:Seo_Taek.html">Seo Taek Kong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srikant:R=.html">R. Srikant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Weina.html">Weina Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08708">PDF</a><br /><b>Abstract: </b>Boltzmann exploration is widely used in reinforcement learning to provide a
trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et
al., 2017) it has been shown that pure Boltzmann exploration does not perform
well from a regret perspective, even in the simplest setting of stochastic
multi-armed bandit (MAB) problems. In this paper, we show that a simple
modification to Boltzmann exploration, motivated by a variation of the standard
doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB
problem with $K$ arms, where $\alpha&gt;0$ is a parameter of the algorithm. This
improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm
inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also
show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in
stochastic MAB problems with graph-structured feedback, without knowledge of
the graph structure, where $\beta(G)$ is the independence number of the
feedback graph. Additionally, we present extensive experimental results on real
datasets and applications for multi-armed bandits with both traditional bandit
feedback and graph-structured feedback. In all cases, our algorithm performs as
well or better than the state-of-the-art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08708"><span class="datestr">at January 28, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08686">On the Complexity of Approximating Wasserstein Barycenter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kroshnin:Alexey.html">Alexey Kroshnin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dvinskikh:Darina.html">Darina Dvinskikh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dvurechensky:Pavel.html">Pavel Dvurechensky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gasnikov:Alexander.html">Alexander Gasnikov</a>, Nazarii Tupitsa, Cesar Uribe <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08686">PDF</a><br /><b>Abstract: </b>We study the complexity of approximating Wassertein barycenter of $m$
discrete measures, or histograms of size $n$ by contrasting two alternative
approaches, both using entropic regularization. The first approach is based on
the Iterative Bregman Projections (IBP) algorithm for which our novel analysis
gives a complexity bound proportional to $\frac{mn^2}{\varepsilon^2}$ to
approximate the original non-regularized barycenter.
</p>
<p>Using an alternative accelerated-gradient-descent-based approach, we obtain a
complexity proportional to $\frac{mn^{2.5}}{\varepsilon} $. As a byproduct, we
show that the regularization parameter in both approaches has to be
proportional to $\varepsilon$, which causes instability of both algorithms when
the desired accuracy is high. To overcome this issue, we propose a novel
proximal-IBP algorithm, which can be seen as a proximal gradient method, which
uses IBP on each iteration to make a proximal step. We also consider the
question of scalability of these algorithms using approaches from distributed
optimization and show that the first algorithm can be implemented in a
centralized distributed setting (master/slave), while the second one is
amenable to a more general decentralized distributed setting with an arbitrary
network topology.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08686"><span class="datestr">at January 28, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08668">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08668">Guarantees for Spectral Clustering with Fairness Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleindessner:Matth=auml=us.html">Matthäus Kleindessner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samadi:Samira.html">Samira Samadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morgenstern:Jamie.html">Jamie Morgenstern</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08668">PDF</a><br /><b>Abstract: </b>Given the widespread popularity of spectral clustering (SC) for partitioning
graph data, we study a version of constrained SC in which we try to incorporate
the fairness notion proposed by Chierichetti et al. (2017). According to this
notion, a clustering is fair if every demographic group is approximately
proportionally represented in each cluster. To this end, we develop variants of
both normalized and unnormalized constrained SC and show that they help find
fairer clusterings on both synthetic and real data. We also provide a rigorous
theoretical analysis of our algorithms. While there have been efforts to
incorporate various constraints into the SC framework, theoretically analyzing
them is a challenging problem. We overcome this by proposing a natural variant
of the stochastic block model where h groups have strong inter-group
connectivity, but also exhibit a "natural" clustering structure which is fair.
We prove that our algorithms can recover this fair clustering with high
probability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08668"><span class="datestr">at January 28, 2019 11:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08639">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08639">Dolha - an Efficient and Exact Data Structure for Streaming Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fan.html">Fan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Lei.html">Lei Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeng:Li.html">Li Zeng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gou:Xiangyang.html">Xiangyang Gou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08639">PDF</a><br /><b>Abstract: </b>A streaming graph is a graph formed by a sequence of incoming edges with time
stamps. Unlike static graphs, the streaming graph is highly dynamic and time
related. In the real world, the high volume and velocity streaming graphs such
as internet traffic data, social network communication data and financial
transfer data are bringing challenges to the classic graph data structures. We
present a new data structure: double orthogonal list in hash table (Dolha)
which is a high speed and high memory efficiency graph structure applicable to
streaming graph. Dolha has constant time cost for single edge and near linear
space cost that we can contain billions of edges information in memory size and
process an incoming edge in nanoseconds. Dolha also has linear time cost for
neighborhood queries, which allow it to support most algorithms in graphs
without extra cost. We also present a persistent structure based on Dolha that
has the ability to handle the sliding window update and time related queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08639"><span class="datestr">at January 28, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08628">Fair k-Center Clustering for Data Summarization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleindessner:Matth=auml=us.html">Matthäus Kleindessner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morgenstern:Jamie.html">Jamie Morgenstern</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08628">PDF</a><br /><b>Abstract: </b>In data summarization we want to choose k prototypes in order to summarize a
data set. We study a setting where the data set comprises several demographic
groups and we are restricted to choose k_i prototypes belonging to group i. A
common approach to the problem without the fairness constraint is to optimize a
centroid-based clustering objective such as k-center. A natural extension then
is to incorporate the fairness constraint into the clustering objective.
Existing algorithms for doing so run in time super-quadratic in the size of the
data set. This is in contrast to the standard k-center objective that can be
approximately optimized in linear time. In this paper, we resolve this gap by
providing a simple approximation algorithm for the k-center problem under the
fairness constraint with running time linear in the size of the data set and k.
If the number of demographic groups is small, the approximation guarantee of
our algorithm only incurs a constant-factor overhead. We demonstrate the
applicability of our algorithm on both synthetic and real data sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08628"><span class="datestr">at January 28, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/012">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/012">TR19-012 |  Multi-pseudodeterministic algorithms | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, dedicated to Shafi Goldwasser, we consider a relaxation of the notion of pseudodeterministic algorithms, which was put forward by Gat and Goldwasser ({\em ECCC}, TR11--136, 2011). 


Pseudodeterministic algorithms are randomized algorithms that solve search problems by almost always providing the same canonical solution (per each input). 
Multi-pseudodeterministic algorithms relax the former notion by allowing the algorithms to output one of a bounded number  of canonical solutions (per each input). 
We show that efficient multi-seudodeterministic algorithms can solve natural problems that are not solveable by efficient pseudodeterministic algorithms, present a composition theorem regarding multi-pseudodeterministic algorithms,
and relate them to other known notions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/012"><span class="datestr">at January 27, 2019 05:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/011">TR19-011 |  Sampling Graphs without Forbidden Subgraphs and Almost-Explicit Unbalanced Expanders | 

	Benny Applebaum, 

	Eliran Kachlon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the study of the following hypergraph sampling problem: Sample a $d$-uniform hypergraph over $n$ vertices and $m$ hyperedges from some pseudorandom distribution $\mathcal{G}$ conditioned on not having some small predefined $t$-size hypergraph $H$ as a subgraph. The algorithm should run in $\mathrm{poly}(n)$-time even when the size of the subgraph $H$ is super-constant.

We solve the problem by carefully designing a sampling algorithm for $k$-wise independent hypergraphs $\mathcal{G}$ that supports efficient testing for subgraph-freeness. We use our algorithm to obtain the first probabilistic construction of constant-degree polynomially-unbalanced expander graphs whose failure probability is negligible in $n$ (i.e., $n^{-\omega(1)}$). In particular, given constants $d&gt;c$, we output a bipartite graph that has $n$ left nodes, $n^c$ right nodes with right-degree of $d$ so that any right set of size at most $n^{\Omega(1)}$ expands by factor of $\Omega(d)$. This result is extended to the setting of unique expansion as well.

We argue that such an ``almost-explicit'' construction can be employed in many useful settings, and present applications in coding theory (batch codes and LDPC codes), pseudorandomness (low-bias generators and randomness extractors) and cryptography. Notably, we show that our constructions yield a collection of polynomial-stretch locally-computable cryptographic pseudorandom generators based on Goldreich's one-wayness assumption resolving a long-standing open problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/011"><span class="datestr">at January 27, 2019 05:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/010">TR19-010 |   Stoquastic PCP vs. Randomness | 

	Alex Bredariol Grilo, 

	Dorit Aharonov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The derandomization of MA, the probabilistic version of NP, is a long standing open question. In this work, we connect this problem to a variant of another major problem: the quantum PCP conjecture. Our connection goes through the surprising quantum characterization of MA by Bravyi and Terhal. They proved the MA-completeness of the problem of deciding whether the groundenergy of a uniform stoquastic local Hamiltonian is zero or inverse polynomial. We show that the gapped version of this problem, i.e. deciding if a given uniform stoquastic local Hamiltonian is frustration-free or has energy at least some constant $\varepsilon$, is in NP. Thus, if there exists a gap-amplification procedure for uniform stoquastic Local Hamiltonians (in analogy to the gap amplification procedure for constraint satisfaction problems in the original PCP theorem), then MA = NP (and vice versa). Furthermore, if this gap amplification procedure exhibits some additional (natural) properties, then P = RP. We feel this work opens up a rich set of new directions to explore, which might lead to progress on both quantum PCP and derandomization. As a small side result, we also show that deciding if commuting stoquastic Hamiltonian is frustration free is in NP.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/010"><span class="datestr">at January 27, 2019 12:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/009">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/009">TR19-009 |  The Fine-Grained Complexity of Strengthenings of First-Order Logic | 

	Jiawei Gao, 

	Russell Impagliazzo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The class of model checking for first-order formulas on sparse graphs has a complete problem with respect to fine-grained reductions, Orthogonal Vectors (OV) [GIKW17]. This paper studies extensions of this class or more lenient parameterizations. We consider classes obtained by allowing function symbols;
first-order on ordered structures; adding various notions of transitive closure operations; and stratifications of first-order properties by quantifier depth and variable complexity, rather than number of quantifiers. For some of these classes, OV is still a complete problem, in that significant improvement for the entire class is equivalent to significant improvement for OV algorithms.  For these classes, we can also use the improved OV algorithm of [AWY16, CW16] to get moderate improvements on algorithms for the entire class. For other classes, we show that model checking becomes harder than for first-order, under well-studied conjectures such as SETH.  For other classes, we show hardness follows from weaker assumptions than SETH. 

Surprisingly, whether an extension increases the complexity of model checking seems independent of whether it increases the expressive power of the logic. For example, adding function symbols does not change which problems are expressible by first-order, but does increase the time for model checking under SETH. On the other hand, adding an ordering does not change the fine-grained complexity
of model checking, although it increases the logic's expressive power.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/009"><span class="datestr">at January 27, 2019 12:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-2559156841800187715">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html">Algorithmic Unfairness Without Any Bias Baked In</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Discussion of (un)fairness in machine learning hit mainstream political discourse this week, when Representative Alexandria Ocasio-Cortez discussed the possibility of algorithmic bias, and was clumsily "called out" by Ryan Saavedra on twitter: <br /><center><blockquote class="twitter-tweet"><div lang="en" dir="ltr">Socialist Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist <a href="https://t.co/X2veVvAU1H">pic.twitter.com/X2veVvAU1H</a>— Ryan Saavedra (@RealSaavedra) <a href="https://twitter.com/RealSaavedra/status/1087627739861897216?ref_src=twsrc%5Etfw">January 22, 2019</a></div></blockquote></center>It was gratifying to see the number of responses pointing out how wrong he was --- awareness of algorithmic bias has clearly become pervasive! But most of the pushback focused on the possibility of bias being "baked in" by the designer of the algorithm, or because of latent bias embedded in the data, or both:  <br /><center><blockquote class="twitter-tweet"><div lang="en" dir="ltr">You know algorithms are written by people right? And that the data they are trained on is made and selected by people? And that the problems algorithms solve are decided...again...by people? And that people can be and many times are racist? Ok now you do the math</div>— kade (@onekade) <a href="https://twitter.com/onekade/status/1087853353000939521?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote></center>Bias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is "unfair".   Here is the (toy) scenario -- the specifics aren't important. High school students are applying to college, and each student has some innate "talent" $I$, which we will imagine is normally distributed, with mean 100 and standard deviation 15: $I \sim N(100,15)$. The college would like to admit students who are sufficiently talented --- say one standard deviation above the mean (so, it would like to admit students with $I \geq 115$). The problem is that talent isn't directly observable. Instead, the college can observe <i>grades</i> $g$ and <i>SAT scores $s$</i>, which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student's talent level, and also with standard deviation 15: $g \sim N(I, 15)$, $s \sim N(I, 15)$.<br /><br />In this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose <i>predicted </i>talent is at least 115. This is indeed "driven by math" --- since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college.<br /><br />Ok. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population --- the Blues's only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above.<br /><br />But there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds. Here are the grades and exam scores for the two populations, plotted:<br /><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s1600/gradesexams.png"><img width="400" src="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s400/gradesexams.png" border="0" height="261" /></a></div><div style="clear: both; text-align: center;" class="separator"></div>So what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don't see much difference:<br /><br />The Red classifier makes errors approximately 11% of the time. The Blue classifier does about the same --- it makes errors about 10.4% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate!<br /><br />And since we are interested in fairness, lets think about the <i>false negative rate</i> of our classifiers. "False Negatives" in this setting are the people who are qualified to attend the college ($I &gt; 115$), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier's mistakes. And the False Negative <i>Rate</i> is the probability that a randomly selected qualified person is mistakenly rejected from college --- i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier's mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness --- <a href="http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning">sometimes referred to as "equal opportunity."</a><br /><br />So how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 50% on the blues, and the Red model has a false negative rate of 47% on the reds --- so the difference between these two is a satisfyingly small 3%.<br /><br />But you might reasonably object: because we have learned separate models for the Blues and the Reds, we are <i>explicitly </i>making admissions decisions as a function of a student's color! This might sound like a form of discrimination, baked in by the algorithm designer --- and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending.<br /><br />So what happens if we don't allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.5%, and the overall error rate ticks up. This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population.<br /><br />What happened? There wasn't any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population --- which contributed much more to the <i>average</i>. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them --- all in their favor.<br /><br /><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto; text-align: center;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s1600/classifier.png"><img width="400" src="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s400/classifier.png" border="0" height="262" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">The combined admissions rule takes everyone above the black line. Since the Blues are shifted up relative to the Reds, they are admitted at a disproportionately higher rate. </td></tr></tbody></table><div style="clear: both; text-align: center;" class="separator"></div><br /><br />This is the kind of thing that happens <i>all the time</i>: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, <i>it is exacerbated if we artificially force the algorithm to be group blind</i>. Well intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time.<br /><br /><br /><br /><br /></div>







<p class="date">
by Aaron Roth (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html"><span class="datestr">at January 26, 2019 10:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/">Introduction to AMP and the Replica Trick</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(This post from the lecture by <span class="qu"><span class="gD">Yueqi </span></span><span class="qu"><span class="gD">Sheng)</span></span></em></p>
<p>In this post, we will talk about detecting phase transitions using<br />
Approximate-Message-Passing (AMP), which is an extension of<br />
Belief-Propagation to “dense” models. We will also discuss the Replica<br />
Symmetric trick, which is a heuristic method of analyzing phase<br />
transitions. We focus on the Rademacher spiked Wigner model (defined<br />
below), and show how both these methods yield the same phrase transition<br />
in this setting.</p>
<p>The Rademacher spiked Wigner model (RSW) is the following. We are given<br />
observations <img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" title="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> (sampled uniformly) is the true signal and <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> is a<br />
Gaussian-Orthogonal-Ensemble (GOE) matrix:<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, j} \sim \mathbb{N}(0, 1)" class="latex" title="W_{i, j} \sim \mathbb{N}(0, 1)" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \neq j" class="latex" title="i \neq j" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, i} \sim \mathbb{N}(0, 2)" class="latex" title="W_{i, i} \sim \mathbb{N}(0, 2)" />. Here <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> is the signal to noise<br />
ratio. The goal is to approximately recover <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />.</p>
<p>The question here is: how small can <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> be such that it is<br />
impossible to recover anything reasonably correlated with the<br />
ground-truth <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />? And what do the approximate-message-passing algorithm<br />
(or the replica method) have to say about this?</p>
<p>To answer the first question, one can think of the task here is to<br />
distinguish <img src="https://s0.wp.com/latex.php?latex=Y+%5Csim+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" title="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" /> vs<br />
<img src="https://s0.wp.com/latex.php?latex=Y+%5Csim+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \sim W" class="latex" title="Y \sim W" />. One approach to distinguishing these distributions is to<br />
look at the spectrum of the observation matrix <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" />. (In fact, it turns<br />
out that this is an asymptotically optimal distinguisher [1]). The spectrum of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> behaves as ([2]):</p>
<ul>
<li>When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda \leq 1" class="latex" title="\lambda \leq 1" />, the empirical distribution of eigenvalues in<br />
spiked model still follows the semicircle law, with the top<br />
eigenvalues <img src="https://s0.wp.com/latex.php?latex=%5Capprox+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\approx 2" class="latex" title="\approx 2" /><p></p>
</li>
<li>
<p>When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &gt; 1" class="latex" title="\lambda &gt; 1" />, we start to see an eigenvalue <img src="https://s0.wp.com/latex.php?latex=%3E+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="&gt; 2" class="latex" title="&gt; 2" /> in the<br />
planted model.</p>
</li>
</ul>
<h1>Approximate message passing</h1>
<p>This section approximately follows the exposition in [3].</p>
<p>First, note that in the Rademacher spiked Wigner model, the posterior<br />
distribution of the signal <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" /> conditioned on the observation <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><br />
is: <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D+%5Cpropto+%5CPr%5BY+%7C+%5Csigma%5D+%5Cpropto+%5Cprod_%7Bi+%5Cneq+j%7D+%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i+%5Csigma_j+%2F2+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" class="latex" title="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" /> This<br />
defines a graphical-model (or “factor-graph”), over which we can perform<br />
Belief-Propogation to infer the posterior distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" />.<br />
However, in this case the factor-graph is dense (the distribution is a<br />
product of potentials <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i%5Csigma_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" class="latex" title="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" /> for all<br />
pairs of <img src="https://s0.wp.com/latex.php?latex=i%2C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i, j" class="latex" title="i, j" />).</p>
<p>In the previous <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">blog post</a>, we saw belief propagation works great when the underlying interaction<br />
graph is sparse. Intuitively, this is because <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is locally tree like,<br />
which allows us to assume each messages are independent random<br />
variables. In dense model, this no longer holds. One can think of dense<br />
model as each node receive a weak signal from all its neighbors.</p>
<p>In the dense model setting, a class of algorithms called Approximate<br />
message passing (AMP) is proposed as an alternative of BP. We will<br />
define AMP for RWM in terms of its state evolution.</p>
<h2>State evolution of AMP for Rademacher spiked Wigner model</h2>
<p>Recall that in BP, we wish to infer the posterior distributon of<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" />, and the messages we pass between nodes correspond to marginal<br />
probability distribution over values on nodes. In our setting, since the<br />
distributions are over <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}" class="latex" title="\{\pm 1\}" />, we can represent distributions by<br />
their expected values. Let <img src="https://s0.wp.com/latex.php?latex=m%5Et_%7Bu+%5Cto+v%7D+%5Cin+%5B-1%2C+1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t_{u \to v} \in [-1, 1]" class="latex" title="m^t_{u \to v} \in [-1, 1]" /> denote the<br />
message from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" /> to <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />. That is, <img src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{u \to v}" class="latex" title="m_{u \to v}" /> corresponds<br />
to the expected value <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_u]" class="latex" title="{{\mathbb{E}}}[\sigma_u]" />.</p>
<p>To derive the BP update rules, we want to compute the expectation<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_v]" class="latex" title="{{\mathbb{E}}}[\sigma_v]" /> of a node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />, given the<br />
messages <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_u]" class="latex" title="{{\mathbb{E}}}[\sigma_u]" /> for <img src="https://s0.wp.com/latex.php?latex=u+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u \neq v" class="latex" title="u \neq v" />. We can<br />
do this using the posterior distribution of the RWM, <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma | Y]" class="latex" title="\Pr[\sigma | Y]" />,<br />
which we computed above.<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5B%5Csigma_v+%3D+1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D+%3D+%5Cfrac%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+-+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%2B+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" class="latex" title="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" /></p>
<p>And similarly for <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma_v+%3D+-1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" class="latex" title="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" />.<br />
From the above, we can take expectations over <img src="https://s0.wp.com/latex.php?latex=%5Csigma_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_u" class="latex" title="\sigma_u" />, and express<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_v]" class="latex" title="{{\mathbb{E}}}[\sigma_v]" /> in terms of<br />
<img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D%5C%7D_%7Bu+%5Cneq+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" class="latex" title="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" />. Doing this (and<br />
using the heuristic assumption that the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" /> is a<br />
product distribution), we find that the BP state update can be written<br />
as:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%28%5Csum_%7Bw+%5Cneq+v%7Df%5E%7B-1%7D%28A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" class="latex" title="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" /><br />
where the interaction matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D+%3D+%5Clambda+Y_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{w, u} = \lambda Y_{w, u}" class="latex" title="A_{w, u} = \lambda Y_{w, u}" />, and<br />
<img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+tanh%28x%29+%3D+%5Cfrac%7B%5Cexp%28x%29+-+%5Cexp%28-x%29%7D%7B%5Cexp%28x%29+%2B+%5Cexp%28x%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" class="latex" title="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" />.</p>
<p>Now, Taylor expanding <img src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f^{-1}" class="latex" title="f^{-1}" /> around <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />, we find<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%5Cleft%28+%28%5Csum_%7Bw+%5Cneq+v%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29+%2B+O%281%2F%5Csqrt%7Bn%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" class="latex" title="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" /><br />
since the terms <img src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{w, u}" class="latex" title="A_{w, u}" /> are of order <img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1/\sqrt{n})" class="latex" title="O(1/\sqrt{n})" />.</p>
<p>At this point, we could try dropping the “non-backtracking” condition<br />
<img src="https://s0.wp.com/latex.php?latex=w+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w \neq v" class="latex" title="w \neq v" /> from the above sum (since the node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> contributes at most<br />
<img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1/\sqrt{n})" class="latex" title="O(1/\sqrt{n})" /> to the sum anyway), to get the state update:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu%7D+%3D+f%5Cleft%28+%5Csum_%7Bw%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" class="latex" title="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" /> (note the messages no longer<br />
depend on receiver – so we write <img src="https://s0.wp.com/latex.php?latex=m_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_u" class="latex" title="m_u" /> in place of <img src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{u \to v}" class="latex" title="m_{u \to v}" />).<br />
However, this simplification turns out not to work for estimating the<br />
signal. The problem is that the “backtracking” terms which we added<br />
amplify over two iterations.</p>
<p>In AMP, we simply perform the above procedure, except we add a<br />
correction term to account for the backtracking issue above. Given <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" />,<br />
for all <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />, the AMP update is:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+m%5E%7Bt%7D_u+%3D+f%28%5Csum_%7Bw%7DA_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%2B+%5B%5Ctext%7Bsome+correction+term%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" class="latex" title="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" /></p>
<p>The correction term corresponds to error introduced by the backtracking<br />
terms. Suppose everything is good until step <img src="https://s0.wp.com/latex.php?latex=t+-+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t - 2" class="latex" title="t - 2" />. We will examine<br />
the influence of backtracking term to a node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> through length 2 loops.<br />
At time <img src="https://s0.wp.com/latex.php?latex=t+-+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t - 1" class="latex" title="t - 1" />, <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> exert <img src="https://s0.wp.com/latex.php?latex=Y_%7Bv%2C+u%7Dm%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{v, u}m^{t - 2}_v" class="latex" title="Y_{v, u}m^{t - 2}_v" /> additional influence to<br />
each of it’s neighbor <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" />. At time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> receive roughly<br />
<img src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{u, v}^2m^{t - 2}_v" class="latex" title="Y_{u, v}^2m^{t - 2}_v" />. Since <img src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{u, v}^2" class="latex" title="Y_{u, v}^2" /> has magnitude<br />
<img src="https://s0.wp.com/latex.php?latex=%5Capprox+%5Cfrac%7B1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\approx \frac{1}{n}" class="latex" title="\approx \frac{1}{n}" /> and we need to sum over all of <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />’s neighbors,<br />
this error term is to large to ignore. To characterize the exact form of<br />
correction, we simply do a taylor expansion</p>
<p><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_v+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+1%7D_u%29+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7D+%5Cleft%28%5Csum_%7Bw%7Df%28Y_%7Bw%2C+u%7Dm%5E%7Bt+-+2%7D_w%29+-+f%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+2%7D_w%29%5Cright%29+%29%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+Y_%7Bu%2C+v%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bu%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" class="latex" title="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" /></p>
<h2>State evolution of AMP</h2>
<p>In this section we attempt to obtain the phase transition of Rademacher<br />
spiked Wigner model via looking at <img src="https://s0.wp.com/latex.php?latex=m%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{\infty}" class="latex" title="m^{\infty}" />.</p>
<p>We assume that each message could be written as a sum of signal term and<br />
noise term. <img src="https://s0.wp.com/latex.php?latex=m%5Et+%3D+%5Cmu_t+x+%2B+%5Csigma_t+g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t = \mu_t x + \sigma_t g" class="latex" title="m^t = \mu_t x + \sigma_t g" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=g+%5Csim+%5Cmathbb%7BN%7D%280%2C+I%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g \sim \mathbb{N}(0, I)" class="latex" title="g \sim \mathbb{N}(0, I)" />. To the dynamics of AMP (and find its phase<br />
transition), we need to look at how the signal <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t" class="latex" title="\mu_t" /> and noise<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t" class="latex" title="\sigma_t" /> evolves with <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />.</p>
<p>We do the following simplification: ignore the correction term and<br />
assume each time we obtain an independent noise <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g" class="latex" title="g" />.</p>
<p><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D+%3D+Yf%28m%5E%7Bt+-+1%7D%29+%3D+%28%5Cfrac%7B%5Clambda%7D%7Bn%7Dx%5ETx+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW%29f%28m%5E%7Bt+-+1%7D%29+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x+%3E+x+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D+Wf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" class="latex" title="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" /></p>
<p>Here, we see that <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" class="latex" title="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" /><br />
and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_t+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DWf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" class="latex" title="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" />.</p>
<p><em>Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{t}" class="latex" title="\mu_{t}" /> is essentially proportional to overlap between<br />
ground truth and current belief</em>, since the function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> keeps the<br />
magnitude of the current beliefs bounded.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28m%5E%7Bt+-+1%7D%29%2C+x%3E%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28%5Cmu_%7Bt+-+1%7Dx+%2B+%5Csigma_%7Bt+-+1%7Dg%29%2C+x%3E+%5Capprox%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX+%5Csim+unif%28%5Cpm+1%29%2C+G%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29%7D%5BX+f%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D+%3D+%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_G%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" class="latex" title="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" /></p>
<p>For the noise term, each coordinate of <img src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t" class="latex" title="\sigma_t" /> is a gaussian random<br />
variable with <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> mean and variance</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_v+f%28m%5E%7Bt+-+1%7D%29_v%5E2+%5Capprox+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX%2C+G%7D%5Bf%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D+%3D+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BG%7D%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" class="latex" title="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" /></p>
<p>It was shown in [4] that we can introduce a new<br />
parameter <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t" class="latex" title="\gamma_t" /> s.t.<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%3D+%5Clambda%5E2+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28%5Cgamma_%7Bt+-+1%7D+%2B+%5Csqrt%7B%5Cgamma_%7Bt+-+1%7D%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" class="latex" title="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" /><br />
As <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, turns out <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Cgamma_t%7D%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t = \frac{\gamma_t}{\lambda}" class="latex" title="\mu_t = \frac{\gamma_t}{\lambda}" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma_t%5E2+%3D+%5Cfrac%7B%5Csigma_t%7D%7B%5Clambda%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" class="latex" title="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" />. To study the behavior of<br />
<img src="https://s0.wp.com/latex.php?latex=m%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t" class="latex" title="m^t" /> as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, it is enough to track the evolution of<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t" class="latex" title="\gamma_t" />.</p>
<p>This heuristic analysis of AMP actually gives a phase transition at<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda = 1" class="latex" title="\lambda = 1" /> (in fact, the analysis of AMP can be done rigorously as in [5]):</p>
<ul>
<li>For <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &lt; 1" class="latex" title="\lambda &lt; 1" />: If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t \approx 0" class="latex" title="\gamma_t \approx 0" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cgamma_t+%2B+%5Csqrt%7B%5Cgamma_t%7DG%7C+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" class="latex" title="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" /> w.h.p., thus we have <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bt+%2B+1%7D+%5Capprox+%5Clambda%5E2+%28%5Cgamma_t%29+%3C+%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" class="latex" title="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" />. Taking <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, we have <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7B%5Cinfty%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_{\infty} = 0" class="latex" title="\gamma_{\infty} = 0" />, which means there AMP solution has no overlap with the ground truth.<p></p>
</li>
<li>
<p>For <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &gt; 1" class="latex" title="\lambda &gt; 1" />: In this case, AMP’s solution has some correlation with the ground truth.</p>
</li>
</ul>
<p><img src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.49.39.png?w=600" alt="screenshot 2019-01-26 13.49.39" class="alignnone size-full wp-image-7422" /></p>
<p>(Figure from [6])</p>
<h1>Replica symmetry trick</h1>
<p>Another way of obtaining the phase transition is via a non-rigorous<br />
analytic method called the replica method. Although non-rigorous, this<br />
method from statistical physics has been used to predict the fixed point<br />
of many message passing algorithms and has the advantage of being easy<br />
to simulate. In our case, we will see that we obtain the same phase<br />
transition temperature as AMP above. The method is non-rigorous due to<br />
several assumptions made during the computation.</p>
<h2>Outline of replica method</h2>
<p>Recall that we are interested in minizing the free energy of a given<br />
system <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%2C+Y%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%2C+Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" class="latex" title="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" /> where <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> is<br />
the partition function as before:<br />
<img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+exp%28-%5Cbeta+H%28Y%2C+x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" class="latex" title="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=H%28Y%2C+x%29+%3D+-%3CY%2C+x%5ETx%3E+%3D+-xYx%5ET+%3D+-%5Csum_%7Bi%2C+j%7D+Y_%7Bi%2C+j%7Dx_ix_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" class="latex" title="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" />.</p>
<p>In replica method, <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> is not fixed but a random variable. The<br />
assumption is that as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" />, free energy doesn’t vary with <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><br />
too much, so we will look at the mean of <img src="https://s0.wp.com/latex.php?latex=f_Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_Y" class="latex" title="f_Y" /> to approximate free<br />
energy of the system.</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BY%7D%5B%5Clog+Z%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" class="latex" title="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is called the free energy density and the goal now is to<br />
compute the free energy density as a function of only <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" /> , the<br />
temperature of the system.</p>
<p>The <strong>replica method</strong> is first proposed as a simplification of the<br />
computation of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /></p>
<p>It is a generally hard problem to compute <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> in a clear way. A<br />
naive attempt of approximate <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is to simply pull the log out<br />
<img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%5Clog+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" class="latex" title="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" /><br />
Unfortunately <img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta)" class="latex" title="g(\beta)" /> and <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> are quite different quantities,<br />
at least when temperature is low. Intuitively, <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is looking at<br />
system with a fixed <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> while in <img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta)" class="latex" title="g(\beta)" />, <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> are allowed to<br />
fluctuate together. When the temperature is high, <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> doesn’t play a big<br />
roll in system thus they could be close. However, when temperature is<br />
low, there could be a problems. Let <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to \infty" class="latex" title="\beta \to \infty" />,<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%5Capprox+%5Cint_Y+%28%5Cbeta+x_Y+Y+x_Y%29%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" class="latex" title="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" />,<br />
<img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%5Capprox+%5Clog+%5Cint_Y+exp%28%5Cbeta+x_J+Y+x_Y%29%5Cmu%28Y%29dY+%5Capprox+%5Cbeta+x%5E%2A+Yx%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" class="latex" title="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" />.</p>
<p>While <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_X%5B%5Clog%28f%28X%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_X[\log(f(X))]" class="latex" title="{{\mathbb{E}}}_X[\log(f(X))]" /> is hard to compute,<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28X%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[f(X)^r]" class="latex" title="{{\mathbb{E}}}[f(X)^r]" /> is a much easier quantity. The<br />
replica trick starts from rewriting <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> with moments of <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" />:<br />
Recall that <img src="https://s0.wp.com/latex.php?latex=x%5Er+%5Capprox+1+%2B+r+%5Clog+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^r \approx 1 + r \log x" class="latex" title="x^r \approx 1 + r \log x" /> for <img src="https://s0.wp.com/latex.php?latex=r+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \approx 0" class="latex" title="r \approx 0" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cln%281+%2B+x%29%5Capprox+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ln(1 + x)\approx x" class="latex" title="\ln(1 + x)\approx x" />, using this we can rewrite <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x)" class="latex" title="f(x)" /> in the following<br />
way:</p>
<p><strong>Claim 1.</strong> <em>Let <img src="https://s0.wp.com/latex.php?latex=f_r%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br+%5Cbeta+n%7D%5Cln%5B%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" class="latex" title="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" /></em><br />
<em>Then, <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Br+%5Cto+0%7Df_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{r \to 0}f_r(\beta)" class="latex" title="f(\beta) = \lim_{r \to 0}f_r(\beta)" /></em></p>
<p>The idea of replica method is quite simple</p>
<ul>
<li>Define a function <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" /> for <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in \mathbb{Z}_+" class="latex" title="r \in \mathbb{Z}_+" /> s.t. <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+f_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta) = f_r(\beta)" class="latex" title="f(r, \beta) = f_r(\beta)" /> for all such <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />.<p></p>
</li>
<li>
<p>Extend <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" /> analytically to all <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%7B%7B%5Cmathbb%7BR%7D%7D%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in {{\mathbb{R}}}_+" class="latex" title="r \in {{\mathbb{R}}}_+" /> and take the limit of <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />.</p>
</li>
</ul>
<p>The second step may sound crazy, but for some unexplained reason, it has<br />
been surprisingly effective at making correct predictions.</p>
<p>The term replica comes from the way used to compute<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[Z^r]" class="latex" title="{{\mathbb{E}}}[Z^r]" /> in Claim 1. We expand the <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />-th moment<br />
in terms of <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> replicas of the system</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29%5Er+%3D+%28%5Csum_x+exp%28-%5Cbeta+H%28Y%2C+x%29%29%29%5Er+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5CPi_%7Bk+%3D+1%7D%5Er+exp%28-%5Cbeta+H%28Y%2C+x%5Ei%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" class="latex" title="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" /></p>
<h2>For Rademacher spiked Wigner model</h2>
<p>In this section, we will see how one can apply the replica trick to<br />
obtain phase transition in the Rademacher spiked Wigner model. Recall<br />
that given a hidden <img src="https://s0.wp.com/latex.php?latex=a+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a \in \{\pm 1\}^n" class="latex" title="a \in \{\pm 1\}^n" />, the observable<br />
<img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Da%5ETa+%2B+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" class="latex" title="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, j} \sim \mathcal{N}(0, 1)" class="latex" title="W_{i, j} \sim \mathcal{N}(0, 1)" /> and <img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, i} \sim \mathcal{N}(0, 2)" class="latex" title="W_{i, i} \sim \mathcal{N}(0, 2)" />.<br />
We are interested in finding the smallest <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> where we can still<br />
recover a solution with some correlation to the ground truth <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" />. <em>Note<br />
that <img src="https://s0.wp.com/latex.php?latex=%5C%7BW_%7Bi%2C+i%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{W_{i, i}\}" class="latex" title="\{W_{i, i}\}" /> is not so important here as <img src="https://s0.wp.com/latex.php?latex=x_i%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i^2" class="latex" title="x_i^2" /> doesn’t carry<br />
any information in this case.</em></p>
<p>Given by the posterior <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BP%7D%7D%7D%5Bx%7CY%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{P}}}[x|Y]" class="latex" title="{{\mathbb{P}}}[x|Y]" />, the system we<br />
set up corresponding to Rademacher spiked Wigner model is the following:</p>
<ul>
<li>the system consists of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> particles and the interactions between<br />
each particle are give by <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><p></p>
</li>
<li>
<p>the signal to noise ratio <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> as the inverse temperature<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />.</p>
</li>
</ul>
<p>Following the steps above, we begin by computing<br />
<img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" class="latex" title="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" /><br />
for <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in \mathbb{Z}_+" class="latex" title="r \in \mathbb{Z}_+" />: Denote <img src="https://s0.wp.com/latex.php?latex=X%5Ek+%3D+%28x%5Ek%29%5ETx%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X^k = (x^k)^Tx^k" class="latex" title="X^k = (x^k)^Tx^k" /> where <img src="https://s0.wp.com/latex.php?latex=x%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^k" class="latex" title="x^k" /> is the<br />
<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />th replica of the system.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%5Csum_k+%3CY%2C+X%5Ek%3E+%5Cmu%28Y%29+dY%5C%5C+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%3CY%2C+%5Csum_k+X%5Ek%3E%29+%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" class="latex" title="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" /></p>
<p>We then simplify the above expression with a technical claim.</p>
<p><strong>Claim 2.</strong><em> Let <img src="https://s0.wp.com/latex.php?latex=Y+%3D+A+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = A + \frac{1}{\sqrt{n}}W" class="latex" title="Y = A + \frac{1}{\sqrt{n}}W" /> where <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> is a fixed matrix and</em><br />
<em><img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> is the GOE matrix defined as above. Then,</em><br />
<em><img src="https://s0.wp.com/latex.php?latex=%5Cint_Y+exp%28%5Cbeta%3CY%2C+X%3E%29+%5Cmu%28Y%29+dY+%3D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%3CA%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" class="latex" title="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" /></em><br />
<em>for some constant <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> depending on distribution of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" />.</em></p>
<p>Denote <img src="https://s0.wp.com/latex.php?latex=X+%3D+%5Csum_k+X%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X = \sum_k X^k" class="latex" title="X = \sum_k X^k" />. Apply Claim 2 with<br />
<img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cfrac%7B%5Cbeta%7D%7Bn%7Da%5ETa&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A = \frac{\beta}{n}a^Ta" class="latex" title="A = \frac{\beta}{n}a^Ta" />, we have<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D+%3Ca%5ETa%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" class="latex" title="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" /><br />
To understand the term inside exponent better, we can rewrite the inner<br />
sum in terms of overlap between replicas:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7DX_%7Bi%2C+j%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%5E2+%3D%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%28%5Csum_%7Bl+%3D+1%7D%5Er+x%5El_ix%5El_j%29%5C%5C+%3D+%5Csum_%7Bk%2C+l%7D+%28%5Csum_%7Bi+%3D+1%7D%5En+x%5Ek_ix%5E%7Bl%7D_i%29%5E2+%3D+%5Csum_%7Bk%2C+l%7D+%3Cx%5Ek%2C+x%5El%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" class="latex" title="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" /></p>
<p>where the last equality follows from rearranging and switch the inner<br />
and outer summations.</p>
<p>Using a similar trick, we can view the other term as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%3Ca%5ETa%2C+X%3E+%3D+%5Csum_%7Bi%2C+j%7D%5Csum_%7Bk+%3D+1%7D%5Erx%5Ek_ix%5Ek_ja_ia_j+%3D+%5Csum_%7Bk+%3D+1%7D%5Er+%28%5Csum_%7Bi+%3D+1%7D%5En+a_ix%5Ek_i%29%5E2+%3D+%5Csum_%7Bk%7D%3Ca%2C+x%5Ek%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" class="latex" title="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" /></p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+%3Cx%5Ek%2C+x%5El%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = &lt;x^k, x^l&gt;" class="latex" title="Q_{k, l} = &lt;x^k, x^l&gt;" /> represents overlaps between the<br />
<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> and <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" />th replicas and <img src="https://s0.wp.com/latex.php?latex=Q_k+%3D+%3Ca%2C+x%5Ek%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_k = &lt;a, x^k&gt;" class="latex" title="Q_k = &lt;a, x^k&gt;" /> represents the<br />
overlaps between the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />th replica and the ground truth vector.</p>
<p>In the end, we get for any integer <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />, (Equation 1):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%28%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29+%5Clabel%7Be%3A1%7D%5C%5C+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D+%5Cln%28%5Csum_%7BQ%7D%5Cnu_%7Bx%5Ek%7D%28Q%29exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" class="latex" title="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" /></p>
<p>Our goal becomes to approximate this quantity. Intuitively, if we think<br />
of <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l}" class="latex" title="Q_{k, l}" /> as indices on a <img src="https://s0.wp.com/latex.php?latex=%28r+%2B+1%29+%5Ctimes+%28r+%2B+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(r + 1) \times (r + 1)" class="latex" title="(r + 1) \times (r + 1)" /> matrices, <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" />,<br />
with <img src="https://s0.wp.com/latex.php?latex=Q%28i%2Ci%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(i,i) = 1" class="latex" title="Q(i,i) = 1" />, then <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" /> is the average of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> i.i.d matrices. So we<br />
expect <img src="https://s0.wp.com/latex.php?latex=Q_%7Bj%2C+k%7D+%5Cin+%5B%5Cpm+%5Cfrac%7B1%7D%7Bn%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{j, k} \in [\pm \frac{1}{n}]" class="latex" title="Q_{j, k} \in [\pm \frac{1}{n}]" /> for <img src="https://s0.wp.com/latex.php?latex=j+%5Cneq+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j \neq k" class="latex" title="j \neq k" /> w.h.p. In the<br />
remaining part, We find the correct <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" /> via rewriting Equation 1.</p>
<p>Observe that by introducing a new variable <img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k, l}" class="latex" title="Z_{k, l}" /> for <img src="https://s0.wp.com/latex.php?latex=k+%5Cneq+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \neq l" class="latex" title="k \neq l" /> and<br />
using the property of gaussian intergal (Equation 4):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A4%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7Bn%7D%7B4%5Cpi%7D%7D%5Cint_%7BZ_%7Bk%2C+l%7D%7D+exp%28-%5Cfrac%7Bn%7D%7B4%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cbeta+Q_%7Bk%2C+l%7DZ_%7Bk%2C+l%7D%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" class="latex" title="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7DQ_k%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7B8%5Cpi+n%7D%7D%5Cint_%7BZ_k%7Dexp%28-%282n%29Z_k%5E2+%2B+2%5Cbeta+Q_%7Bk%7DZ_k%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" class="latex" title="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" /><br />
Replace each <img src="https://s0.wp.com/latex.php?latex=exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="exp(\frac{\beta^2}{n}Q_{k, l}^2)" class="latex" title="exp(\frac{\beta^2}{n}Q_{k, l}^2)" /> by a such integral, we<br />
have (Equation 2):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bgathered%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29+%5Clabel%7Be%3A2%7D%5C%5C+%3D+C%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5Cexp%28%5Cbeta%5E2+n%29%5Cint_%7BZ_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29+dZ+%5C%5C+%3DC%5Cexp%28%5Cbeta%5En%29+%5Cint_%7BY_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kY_k+Q_k%29%29+dY+%5Clabel%7Be%3A2%7D%5Cend%7Bgathered%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" class="latex" title="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> is the constant given by introducing gaussian intergals.</p>
<p>To compute the integral in (Equation 2), we need to cheat a little bit and take<br />
<img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" /> before letting <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />. Note that free energy density<br />
is defined as<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" class="latex" title="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" /><br />
This is the second assumption made in the replica method and it is<br />
commonly believed that switching the order is okay here. Physically,<br />
this is plausible because we believe intrinsic physical quantities<br />
should not depend on the system size.</p>
<p>Now the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace method</a> tells us when <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" />, the integral in (Equation 2) is dominated by the max of the exponent.</p>
<p><strong>Theorem 1 (Laplace Method).</strong> <em>Let <img src="https://s0.wp.com/latex.php?latex=h%28x%29%3A+%7B%7B%5Cmathbb%7BR%7D%7D%7D%5En+%5Cto+%7B%7B%5Cmathbb%7BR%7D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" class="latex" title="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" />, </em><em>then </em></p>
<p><em><img src="https://s0.wp.com/latex.php?latex=%5Cint+e%5E%7Bnh%28x%29%7D+%5Capprox+e%5E%7Bnh%28x%5E%2A%29%7D%28%5Cfrac%7B2%5Cpi%7D%7Bn%7D%29%5E%7B%5Cfrac%7Bd%7D%7B2%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7Bdet%28H%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" class="latex" title="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" /></em></p>
<p><em>where <img src="https://s0.wp.com/latex.php?latex=x%5E%2A+%3D+argmax_x+%5C%7Bh%28x%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^* = argmax_x \{h(x)\}" class="latex" title="x^* = argmax_x \{h(x)\}" /> and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is the Hessian of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" /> evaluated at the point <img src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^*" class="latex" title="x^*" />.</em></p>
<p>Fix a pair of <img src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k, l" class="latex" title="k, l" /> and apply Laplace method with<br />
<img src="https://s0.wp.com/latex.php?latex=h%28Z_%7Bk%2C+l%7D%29+%3D+-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7B0+%5Cleq+k+%3C+l+%5Cleq+r%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" class="latex" title="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" /><br />
what’s left to do is to find the critical point of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" />. Taking the<br />
derivatives gives<br />
<img src="https://s0.wp.com/latex.php?latex=-Y_%7Bk%2C+l%7D+%2B+%5Cfrac%7BA%28Z_%7Bk%2C+l%7D%29%5Cbeta+Q_%7Bk%2C+l%7D%7D%7Bn+A%28Z_%7Bk%2C+l%7D%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" class="latex" title="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" /><br />
where<br />
<img src="https://s0.wp.com/latex.php?latex=A%28Z_%7Bk%2C+l%7D%29+%3D+%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+%5Cbeta%5Csum_kY_k+Q_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" class="latex" title="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" />.</p>
<p>We now need to find a saddle point of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" /> where the hessian is PSD. To<br />
do that, we choose to assume the order of the replicas does not matter,<br />
which is refer to as the replica symmetry case. <sup id="fnref-7420-1"><a href="https://windowsontheory.org/feed/#fn-7420-1" class="jetpack-footnote">1</a></sup> One simplest form<br />
of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> is the following: <img src="https://s0.wp.com/latex.php?latex=%5Cforall+k%2C+l+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall k, l &gt; 0" class="latex" title="\forall k, l &gt; 0" />, <img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k, l} = y" class="latex" title="Z_{k, l} = y" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k} = y" class="latex" title="Z_{k} = y" /> for some <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />. This also implies that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = q" class="latex" title="Q_{k, l} = q" /> for some<br />
<img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and <img src="https://s0.wp.com/latex.php?latex=y+%3D%5Cfrac%7B%5Cbeta%7D%7Bn%7D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y =\frac{\beta}{n} q" class="latex" title="y =\frac{\beta}{n} q" /></p>
<p>Plug this back in to Equation 2 gives: (Equation 3)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A3%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+C%5Cexp%28%5Cbeta+n%29%5Cexp%28-%5Cfrac%7Bn%7D%7B2%7D%28%5Cfrac%7Br%5E2+-+r%7D%7B2%7D%29y%5E2+-+%5Cfrac%7Bn%5E2%7D%7B2%7D+%2B+%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+2y%5Cbeta+%5Csum_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" class="latex" title="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" /></p>
<p>To obtain <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" />, we only need to deal with the last term in<br />
(Equation 3) as <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />. Using the fact that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = y" class="latex" title="Q_{k, l} = y" /> for all<br />
<img src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k, l" class="latex" title="k, l" /> and using the same trick of introducing new gaussain integral as<br />
in (Equation 4) we have<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%7D%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+n%5Cbeta+%5Csum_k+Q_k%29%29+%3D+-%5Cbeta+%2B+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7Bz+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29%7D%5B%5Clog%282cosh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" class="latex" title="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" /></p>
<p>Using the fact that we want the solution to minimizes free energy,<br />
taking the derivative of the current <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> w.r.t. <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> gives<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7By%7D%7B%5Cbeta%7D+%3D+n%7B%7B%5Cmathbb%7BE%7D%7D%7D_z%5Btanh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" class="latex" title="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" /><br />
which matches the fixed point of AMP. Plug in <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> will give us<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" />. The curve of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> looks like the Figure below, where<br />
the solid line is the curve of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> with the given <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and the<br />
dotted line is the curve given by setting all variables <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.54.49.png?w=600" alt="screenshot 2019-01-26 13.54.49" class="alignnone size-full wp-image-7423" /></p>
<p> </p>
<p><strong>References</strong></p>
<div>[1] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-optimality of pca for spiked random matrices and synchronization.</div>
<div>arXiv preprint arXiv:1609.05573, 2016.</div>
<div></div>
<div>[2] D. Feral and S. Pech e. The Largest Eigenvalue of Rank One Deformation of Large Wigner Matrices. Communications in Mathematical Physics, 272:185–228, May 2007.</div>
<div></div>
<div>[3] Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.</div>
<div></div>
<div>[4] Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for thebinary stochastic block model. In</div>
<div>Information Theory (ISIT), 2016 IEEE International Symposium on, pages 185–189. IEEE, 2016.</div>
<div></div>
<div>[5] Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.</div>
<div></div>
<div>[6] A. Perry, A. S. Wein, and A. S. Bandeira. Statistical limits of spiked tensor models.</div>
<div>ArXiv e-prints, December 2016.</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn-7420-1">
Turns out for this problem, replica symmetry is the only case. We<br />
will not talk about replica symmetry breaking here, which<br />
intuitively means we partition replicas into groups and re-curse. <a href="https://windowsontheory.org/feed/#fnref-7420-1">↩</a>
</li>
</ol>
</div></div>







<p class="date">
by preetum <a href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/"><span class="datestr">at January 26, 2019 07:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7413">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/">Quantum circuits and their role in demonstrating quantum supremacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid proof that quantum computers do have an <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">advantage</a> over classical computers. </p>



<p>What’s rather tricky about showing such a result is that, rather than a direct argument about the capability of quantum computers, what we really need to demonstrate is the incapability of classical computers to achieve tasks that can be done with quantum computers. </p>



<p>One of the major leaps forward in demonstrating quantum supremacy was taken by Terhal and DiVincenzo in their 2008 paper “<a href="https://arxiv.org/abs/quant-ph/0205133">Adaptive quantum computation, constant depth quantum circuits and arthur-merlin games</a>“. Their approach was to appeal to a complexity-theoretic argument: they gave evidence that there exists a certain class of quantum circuits that cannot be simulated classically by proving that if a classical simulation existed, certain complexity classes strongly believed to be distinct would collapse to the same class. While this doesn’t quite provide a proof of quantum supremacy – since the statement about the distinction between complexity classes upon which it hinges is not a proven fact – because the complexity statement appears overwhelmingly likely to be true, so too does the proposed existence of non-classically-simulatable quantum circuits. The Terhal and DiVincenzo paper is a complex and highly technical one, but in this post I hope to explain a little bit and give some intuition for the major points. </p>



<p>Now, let’s start at the beginning. What is a <a href="https://en.wikipedia.org/wiki/Quantum_circuit">quantum circuit</a>? I’m going to go ahead and assume you already know what a <a href="https://en.wikipedia.org/wiki/Circuit_(computer_science)">classical circuit</a> is – the extension to a quantum circuit is rather straightforward: it’s a circuit in which all gates are <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate">quantum gates</a>, where a quantum gate can be thought of as a classical gate whose output is, rather than a deterministic function of the inputs, instead a probability distribution over all possible outputs given the size of the inputs. For example, given two single-bit inputs, a classical AND gate outputs 0 or 1 deterministically given the inputs. A quantum AND gate on the analogous single-<a href="https://en.wikipedia.org/wiki/Qubit">qubit</a> inputs would output 0 with some probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and 1 with some probability <img src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1-p" class="latex" title="1-p" />. Similarly, a classical AND gate on two 4-bit inputs outputs the bitwise AND, while the quantum analog has associated with it a 4-qubit output: some probability distribution over all 4-bit binary strings. A priori there is no particular string that is the “output” of the computation by the quantum gate; it’s only after taking a <a href="https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics">quantum measurement</a> of the output that we get an actual string that we can think of as the outcome of the computation done by the gate. The actual string we “observe” upon taking the measurement follows the probability distribution computed by the gate on its inputs. In this way, a quantum circuit can then be thought of as producing, via a sequence of probabilistic classical gates (i.e., quantum gates) some probability distribution over possible outputs given the input lengths. It’s not hard to see that in this way, we can compose circuits: suppose we have a quantum circuit <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> and another quantum circuit <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />. Let <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> have an input of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits and an output of <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> qubits; suppose we measure <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> of the output qubits of <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> – then we can feed the remaining <img src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m-k" class="latex" title="m-k" /> unmeasured qubits as inputs into <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />(assuming that those <img src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m-k" class="latex" title="m-k" /> qubits do indeed constitute a valid input to <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />). </p>



<p>Consider, then, the following sort of quantum circuit: it’s a composition of <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" /> quantum circuits, such that after each <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th circuit we take a measurement some of its output qubits (so that the remaining unmeasured qubits become inputs to the <img src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i+1)" class="latex" title="(i+1)" />-th circuit), and then the structure of the <img src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i+1)" class="latex" title="(i+1)" />-th circuit is dependent on this measurement. That is, it’s as though, given a quantum circuit, we’re checking every so often at intermediate layers over the course of the circuit’s computation what the value of some of the variables are (leaving the rest to keep going along through the circuit to undergo more computational processing), and based on what we measure is the current computed value, the remainder of the circuit “adapts” in a way determined by that measurement. Aptly enough, this is called an “adaptive circuit”. But since the “downstream” structure of the circuit depends on the outcomes of all the measurements made “upstream”, each adaptive circuit actually comprises a family of circuits, each of which is specified by the sequence of intermediate measurement outcomes. That is, we can alternatively characterize an adaptive circuit as a set of ordinary quantum circuits that is parameterized by a list of measurement outcomes. Terhal and DiVincenzo call this way of viewing an adaptive circuit, as a family of circuits parametrized by a sequence of measurement values, a “non-adaptive circuit” – since we replace the idea that the circuit “adapts” to intermediate measurements with the idea that there are just many regular circuits, one for each possible sequence of measurements. It’s this non-adaptive circuit concept that’ll be our main object of study going forward.</p>



<h2>Simulating quantum circuits</h2>



<p>Now, the result we wanted to demonstrate about quantum circuits had to do with their efficient simulatability by classical circuits – and so we should establish some notion of what we mean when we talk about an “efficient simulation”. </p>



<p>Terhal and DiVincenzo offer the following notion of a classical simulation – which in their paper they call an “efficient density computation”: consider a quantum circuit with some output of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits. Recall that to actually obtain an output value, we need to take a measurement of the circuit output – imagine doing this in disjoint subsets of cubits at a time. That is, we can break up the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits into <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> disjoint subsets and consider the entire output measurement as a process of taking <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> measurements, subset by subset. An efficient density computation exists if there’s a classical procedure for computing, in time polynomial in the width and depth of the quantum circuit, the conditional probability distribution over the set of possible measurement outcomes of a particular subset of qubits, given any subset of the <img src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k-1" class="latex" title="k-1" /> other measurement outcomes. Intuitively, this is a good notion of what a classical simulation should consist of, or at least what data it should contain, since if you know the conditional probabilities given any (possibly empty) subset of the other measurements, you can just flip coins for the outputs according to the conditional probabilities as a way of actually exhibiting a “working” simulation.</p>



<p>It’s with this notion of simulation, along with our concept of an adaptive quantum circuit as a family of regular circuits parameterized by a sequence of intermediate measurement outcomes, we may now arrive at the main result of Terhal and DiVincenzo’s paper. Recall that what we wanted to show from the very beginning is that there exists some quantum circuit that can’t be simulated classically. The argument for this proceeds like a proof by contradiction: suppose the contrary, and that all quantum circuits can be simulated classically. We want to show that we can find, then, a quantum circuit which, if it were possible to be simulated classically (as per our assumption), we’d wind up with some strange consequences that we believe are false, leading us to conclude that those circuits probably can’t be simulated classically.</p>



<p>Thus, we shall now exhibit such a quantum circuit whose classical simulatability leads (as far as we believe) to a contradiction. Consider a special case of adaptive quantum circuits, considered as a parameterized family of regular circuits, in which the circuit’s output distribution is independent of the intermediate measurement outcomes; that is, the case in which the entire family of circuits corresponding to an adaptive circuit is logically the same – that is, is the same logical circuit on input qubits independent of intermediate measurements. I’d like to point out, just for clarification’s sake, the subtlety here, which makes this consideration non-redundant, and not simply a reduction of an adaptive quantum circuit (again, thought of as a family) to a single fixed circuit (i.e., a family of one): the situation in which the family is reduced to a single fixed circuit occurs when the<em> structure of the circuit</em> is independent of the intermediate measurement outcomes. If the structure were independent of the measurements, then no matter what we observed in the measurements, we’d get the same circuit – hence a trivial family of one. What we’re considering instead is the case in which the <em>structure</em> of the circuit is still dependent on the intermediate measurements (and so the circuit is still adaptive), but where the <em>distribution over the possible outputs of the circuit</em> is identical no matter what the intermediate measurements are. In this case, the circuit can still be considered as a parameterized and in general non-trivial family of circuits, but for which each member produces the same distribution over outputs – hence, a family of potentially <em>structurally</em> different circuits, but which are <em>logically</em> identical.</p>



<p>Suppose there’s some set of such circuits that’s universal – that is, that’s sufficient to implement all polynomial-time quantum computations. (This is a reasonable assumption to make, since there do in fact exist <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate\#Universal_quantum_gates">universal quantum gate sets.</a> But now if a simulation of the kind we defined (an efficient density computation) existed for every circuit in this set, then we could calculate the outcome probability of any polynomial-depth quantum circuit, since any polynomial-depth quantum circuit could be realized as some composition of circuits in this universal set (and in particular as a composition of particular family members of each adaptive circuit in the universal set), and an efficient density computation, as we mentioned above, precisely gives us a way to compute the output distribution. </p>



<p>But now here is where our believed contradiction lies: </p>



<p><em>Theorem:</em> Suppose there exists a universal set of adaptive quantum circuits whose output distributions are independent of intermediate measurements. If there is an efficient density computation for each family member of each adaptive circuit in this universal set, then for the polynomial hierarchy PH we have PH = BPP = BQP. </p>



<p>The proof goes something like this: if we can do our desired efficient density computations (as we assumed, for the sake of contradiction, we could for all quantum circuits), this is equivalent to being able to determine the acceptance probability of a quantum computation, which was shown in the paper “<a href="https://arxiv.org/abs/quant-ph/9812056">Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy”</a> by Fenner, Green, Homer and Pruim to be equivalent to the class <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{coC=P}" class="latex" title="\text{coC=P}" />. Thus, we have that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D+%5Csubseteq+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{coC=P} \subseteq \text{BPP}" class="latex" title="\text{coC=P} \subseteq \text{BPP}" />. But it’s known that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BcoC%3DP%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" class="latex" title="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" /> and so <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BBPP%7D%7D+%3D+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" class="latex" title="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" />. That is, we have <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%3D+%5Ctext%7BBPP%7D+%3D+%5Ctext%7BBQP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} = \text{BPP} = \text{BQP}" class="latex" title="\text{PH} = \text{BPP} = \text{BQP}" />, and so the polynomial hierarchy would collapse to <img src="https://s0.wp.com/latex.php?latex=%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma^P_2" class="latex" title="\Sigma^P_2" /> since <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBPP%7D+%5Csubseteq+%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{BPP} \subseteq \Sigma^P_2" class="latex" title="\text{BPP} \subseteq \Sigma^P_2" /> (for more on these more obscure complexity classes, see <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo">here</a>). Again, this is our “contradiction”: while it hasn’t been quite proven, it is widely believed, with strong supporting evidence, that the polynomial hierarchy does not collapse as would be the case if all quantum circuits were classically simulatable. Thus this provides a strong argument that not all quantum circuits are classically simulatable, which was precisely what we were looking to demonstrate.</p>



<h2>Conclusion</h2>



<p>Terhal and DiVincenzo actually go even further and show that there is a certain class of constant-depth quantum circuits that are unlikely to be simulatable by classical circuits – this, indeed, seems to provide even stronger evidence for quantum supremacy. This argument, which is somewhat more complex, uses the idea of teleportation and focuses on a particular class of circuits implementable by a certain restricted set of quantum gates. If you’re interested, I highly recommend reading their paper, where this is explained. </p>



<h2>Recommended reading</h2>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li><li>Harrow, Aram Wettroth and Ashley Montanaro. “Quantum computational supremacy.” <em>Nature</em> 549 (2017): 203-209.</li><li>Boixo, Sergio et. al. “Characterizing quantum supremacy in near-term devices.” <em>Nature Physics</em> 14 (2018); 595-600.</li></ul>



<h2>References</h2>



<p>\begin{enumerate}</p>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Fenner, Stephen et. al. “Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy.” <a href="https://arxiv.org/abs/quant-ph/9812056" rel="nofollow">https://arxiv.org/abs/quant-ph/9812056</a>. (1998).</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li></ul></div>







<p class="date">
by hksorens <a href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/"><span class="datestr">at January 25, 2019 11:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/">postdoc at UC San Diego (apply by March 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/"><span class="datestr">at January 25, 2019 07:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7411">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/">Looking a postdoc opportunity?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out <a href="https://cstheory-jobs.org/">https://cstheory-jobs.org/</a> . </p>



<p>(For example, a new ad was just posted on Wednesday by <a href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">Venkat Guruswami and Pravesh Kothari </a> )</p></div>







<p class="date">
by windowsontheory <a href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/"><span class="datestr">at January 25, 2019 05:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7382539649899855346">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/01/the-paradigm-shift-in-fintech.html">The Paradigm Shift in FinTech Computation and the need for a Computational Toolkit (Guest Post by Evangelos Georgiadis)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Paradigm Shift in FinTech Computation and the need for a Computational Toolkit<br />
<br />
(Guest Post by <a href="http://www.mathcognify.com/">Evangelos Georgiadis</a>)<br />
<br />
We are experiencing a paradigm shift in finance as we are entering the era of algorithmic FinTech computation. (**And another yet to come. See **Future** below.)  This era is marked by a shift in the role played by the theoretical computer scientist. In the not so distant past, the (financial) economist had the ultimate stamp of approval  for how to study financial models, pricing models, mechanism design, etc. The economist was the ultimate gatekeeper of ideas and models, whereas the main role of the computer scientist was to turn these ideas or models into working code; in a sense, an obedient beaver/engineer. (In finance, the theoretical computer scientist more often than not wears the hat of the quant.)<br />
<br />
In today's era, the role of the theoretical computer scientist has been elevated from the obedient engineer to the creative architect not only of models and mechanism designs but also of entire ecosystems. One example is blockchain based ecosystems. In the light of this promotion from obedient engineer to architect, we might need to re-hash the notion of 'sharing blame', as originally and elegantly voiced in <a href="https://www.technologyreview.com/s/408851/on-quants/">On Quants</a> by Professor Daniel W. Stroock, when things go wrong.)<br />
<br />
The role change is also coupled by a shift in emphasis of computation that in turn necessitates a deeper understanding of (what this author would refer to as) distributed yet pragmatic complexity based crypto systems' that attempt to redefine 'trust' in terms of distributed computation.<br />
<br />
This change necessitates an ability to think in terms of approximation (and lower/upper bounds)  or other good-enough solutions that work on all inputs,  rather than merely easy instances of  problem types that usually lead to clean, exact formulas or solutions.  Additionally, looking through the lens of approximation algorithms enables a different and often more insightful metric for dealing with intrinsically hard problems (for which often no exact or clean solutions exist.) Computer Scientists are trained in this way; however, financial economists are not.   Might the economists actually get in the way?<br />
<br />
Our tentative response: The economists are valuable and the solution to the dilemma is to equip them with the right 'computational toolkit'. Ideally, such a toolkit comprises computational tools and algorithms that enable automation of certain computational tasks which otherwise would necessitate more granular understanding at the level of a theoretical computer scientist (or mathematician)<br />
OR be too cumbersome to perform by hand even for the expert.<br />
<br />
Essentially, a toolkit even for the theoretical computer scientist that frees her from clerical work and enables computation to scale from clean cases, such as n=1, to pathological (yet far more realistic) cases, such as n=100000, all the way to the advanced and rather important (agnostic case or) symbolic case when n=k -- without much pain or agony.<br />
<br />
The existence of such a toolkit would in turn do justice to the definition of FinTech Computation, which entails applying advanced computational techniques not necessarily information techniques) to financial computation. in fact, this author is part of building such an infrastructure solution which<br />
necessitates the underlying programming language [R-E-CAS-T] to have intrinsic hybrid capabilities -- symbolic as well as numeric.<br />
<br />
One step towards this  "automation" conquest is shown in <a href="https://arxiv.org/pdf/1808.05255v2.pdf">A combinatorial-probabilistic analysis of bitcoin attacks</a> with Doron Zeilberger.  The work illustrates an algorithmic risk analysis of the bitcoin protocol via symbolic computation, as opposed to the meticulous, yet more laborious by hand conquest shown by the European duo in <a href="https://arxiv.org/abs/1702.02867">Double spend races</a> Heavy usage of the "Wilf-Zeilberger algorithmic proof theory" one of the cornerstones in applied symbolic computation, enabled automated recurrence discovery and algorithmic derivation of higher-order asymptotics. For example, in terms of asymptotics tools: the ability to internalize a very dense body of mathematics, such as the G.D. Birkhoff and W.J. Trjitzinsky method, symbolically, automates the process of computing asymptotics of solutions of recurrence equations; a swiss army knife for any user.<br />
<br />
&lt;**Future**&gt;<br />
<br />
What does the future entail for FinTech Computation ?<br />
<br />
[My two <a href="https://en.bitcoin.it/wiki/Satoshi_(unit)">satoshis</a> on this]<br />
<br />
Where are we headed in terms of type of computation ?<br />
<br />
Blockchain based systems, even though some of us (including this author) have noticed fundamental flaws, seem to still have momentum, at least, judging from recent news articles about companies becoming blockchain technology friendly.  Ranging from (of course) exchanges such as our friends at <a href="https://www.binance.com/en">Binance</a> and <a href="https://www.bitmex.com/">BitMEX</a>, we have major smartphone makers such as <a href="https://www.bloomberg.com/news/articles/2018-04-15/samsung-jumps-on-blockchain-bandwagon-to-manage-its-supply-chain">Samsung</a>, <a href="https://www.bloomberg.com/news/articles/2018-03-21/huawei-said-to-be-in-talks-to-build-blockchain-ready-smartphone">Huawei</a>, and <a href="https://www.cnbc.com/2018/10/23/htc-launches-blockchain-phone-exodus-1-to-be-sold-in-cryptocurrency.html">HTC</a>. The favorable sentiment towards blockchain technology is shared even amongst top tier U.S. banks.<br />
 Can one deduce success or failure momentum from the citation count distribution of the paper that laid grounds to this technology ? <a href="https://bitcoin.org/bitcoin.pdf">Bitcoin: A Peer-to-Peer Electronic Cash System</a>)<br />
<br />
If we look at crypto(currencies), one of many challenges for these blockchain based systems is the high maintenance cost.  Certainly in terms of energy consumption when it comes to the process of mining -- whether Proof-of-Work (PoW) is replaced by Proof-of-Stake (PoS) or some other more energy efficient consensus variant. (This author is aware of various types of optimizations that have been used.)<br />
A few questions that have bugged this author every since ...<br />
<br />
a) Is there a natural way to formalize the notion of energy consumption for consensus mechanisms?<br />
<br />
b) What about formalizing an energy-efficient mechanism design ?)<br />
<br />
(The idea of savings when PoW is replaced by PoS as intended by our friends at the <a href="https://spectrum.ieee.org/computing/networks/ethereum-plans-to-cut-its-absurd-energy-consumption-by-99-percent">Ethereum Foundation</a> has been around for some time but the point of this author is, the value of 0.99*X (where X is a <a href="https://link.springer.com/chapter/10.1007/978-1-4684-6686-7_28">supernatural number</a>  [a la Don E. Knuth style]), is still a big quantity; too big for an environmentalist ?)<br />
<br />
So, what comes next ?<br />
<br />
[... the satoshis are still on the table.]<br />
<br />
Daniel Kane has brought to my attention that quantum computation -- the seemingly next paradigm shift in which again the role of TCS seem  inextricably interwoven --  may lead to blockchain based systems being replaced by less expensive (at least in terms of energy consumption) quantum based systems. (Crypto might get replaced by Quantum (money). :-)) One such pioneering approach is masterfully articulated by Daniel Kane in "<a href="https://arxiv.org/abs/1809.05925">Quantum Money from Modular Forms</a>.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/01/the-paradigm-shift-in-fintech.html"><span class="datestr">at January 25, 2019 04:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
