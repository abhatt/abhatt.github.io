<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.blogger.com/feeds/25562705/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://benjamin-recht.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://www.blogger.com/feeds/21224994/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 14, 2020 11:42 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/105">TR20-105 |  Automating Regular or Ordered Resolution is NP-Hard | 

	Zoë Bell</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that is hard to find regular or even ordered (also known as Davis-Putnam) Resolution proofs, extending the breakthrough result for general Resolution from Atserias and Müller to these restricted forms. Namely, regular and ordered Resolution are automatable if and only if P = NP. Specifically, for a CNF formula $F$ the problem of distinguishing between the existence of a polynomial-size ordered Resolution refutation of $F$ and an at least exponential-size general Resolution proof being required to refute $F$ is NP-complete.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/105"><span class="datestr">at July 14, 2020 01:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06242">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06242">Settling the Price of Fairness for Indivisible Goods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barman:Siddharth.html">Siddharth Barman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhaskar:Umang.html">Umang Bhaskar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Nisarg.html">Nisarg Shah</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06242">PDF</a><br /><b>Abstract: </b>In the allocation of resources to a set of agents, how do fairness guarantees
impact the social welfare? A quantitative measure of this impact is the price
of fairness, which measures the worst-case loss of social welfare due to
fairness constraints. While initially studied for divisible goods, recent work
on the price of fairness also studies the setting of indivisible goods.
</p>
<p>In this paper, we resolve the price of two well-studied fairness notions for
the allocation of indivisible goods: envy-freeness up to one good (EF1), and
approximate maximin share (MMS). For both EF1 and 1/2-MMS guarantees, we show,
via different techniques, that the price of fairness is $O(\sqrt{n})$, where
$n$ is the number of agents. From previous work, it follows that our bounds are
tight. Our bounds are obtained via efficient algorithms. For 1/2-MMS, our bound
holds for additive valuations, whereas for EF1, our bound holds for the more
general class of subadditive valuations. This resolves an open problem posed by
Bei et al. (2019).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06242"><span class="datestr">at July 14, 2020 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06167">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06167">Local Editing in LZ-End Compressed Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Daniel Roodt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Speidel:Ulrich.html">Ulrich Speidel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Vimal.html">Vimal Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ko:Ryan_K=_L=.html">Ryan K. L. Ko</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06167">PDF</a><br /><b>Abstract: </b>This paper presents an algorithm for the modification of data compressed
using LZ-End, a derivate of LZ77, without prior decompression. The performance
of the algorithm and the impact of the modifications on the compression ratio
is evaluated. Finally, we discuss the importance of this work as a first step
towards local editing in Lempel-Ziv compressed data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06167"><span class="datestr">at July 14, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06110">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06110">Streaming Algorithms for Online Selection Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Correa:Jos=eacute=.html">José Correa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=tting:Paul.html">Paul Dütting</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Felix.html">Felix Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewior:Kevin.html">Kevin Schewior</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziliotto:Bruno.html">Bruno Ziliotto</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06110">PDF</a><br /><b>Abstract: </b>The model of streaming algorithms is motivated by the increasingly common
situation in which the sheer amount of available data limits the ways in which
the data can be accessed. Streaming algorithms are typically allowed a single
pass over the data and can only store a sublinear fraction of the data at any
time. We initiate the study of classic online selection problems in a streaming
model where the data stream consists of two parts: historical data points that
an algorithm can use to learn something about the input; and data points from
which a selection can be made. Both types of data points are i.i.d. draws from
an unknown distribution. We consider the two canonical objectives for online
selection---maximizing the probability of selecting the maximum and maximizing
the expected value of the selection---and provide the first performance
guarantees for both these objectives in the streaming model.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06110"><span class="datestr">at July 14, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06105">Efficient Labeling for Reachability in Digraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Maciej Dulęba, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janczewski:Wojciech.html">Wojciech Janczewski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06105">PDF</a><br /><b>Abstract: </b>We consider labeling nodes of a directed graph for reachability queries. A
reachability labeling scheme for such a graph assigns a binary string, called a
label, to each node. Then, given the labels of nodes $u$ and $v$ and no other
information about the underlying graph, it should be possible to determine
whether there exists a directed path from $u$ to $v$. By a simple information
theoretical argument and invoking the bound on the number of partial orders, in
any scheme some labels need to consist of at least $n/4$ bits, where $n$ is the
number of nodes. On the other hand, it is not hard to design a scheme with
labels consisting of $n/2+O(\log n)$ bits. In the classical centralised
setting, Munro and Nicholson designed a data structure for reachability queries
consisting of $n^2/4+o(n^2)$ bits (which is optimal, up to the lower order
term). We extend their approach to obtain a scheme with labels consisting of
$n/3+o(n)$ bits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06105"><span class="datestr">at July 14, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06098">Graph Connectivity and Single Element Recovery via Linear and OR Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assadi:Sepehr.html">Sepehr Assadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06098">PDF</a><br /><b>Abstract: </b>We study the problem of finding a spanning forest in an undirected,
$n$-vertex multi-graph under two basic query models. One is the Linear query
model which are linear measurements on the incidence vector induced by the
edges; the other is the weaker OR query model which only reveals whether a
given subset of plausible edges is empty or not. At the heart of our study lies
a fundamental problem which we call the {\em single element recovery} problem:
given a non-negative real vector $x$ in $N$ dimension, return a single element
$x_j &gt; 0$ from the support. Queries can be made in rounds, and our goals is to
understand the trade-offs between the query complexity and the rounds of
adaptivity needed to solve these problems, for both deterministic and
randomized algorithms. These questions have connections and ramifications to
multiple areas such as sketching, streaming, graph reconstruction, and
compressed sensing. Our main results are:
</p>
<p>* For the single element recovery problem, it is easy to obtain a
deterministic, $r$-round algorithm which makes $(N^{1/r}-1)$-queries per-round.
We prove that this is tight: any $r$-round deterministic algorithm must make
$\geq (N^{1/r} - 1)$ linear queries in some round. In contrast, a $1$-round
$O(\log^2 N)$-query randomized algorithm which succeeds 99% of the time is
known to exist.
</p>
<p>* We design a deterministic $O(r)$-round, $\tilde{O}(n^{1+1/r})$-OR query
algorithm for graph connectivity. We complement this with an
$\tilde{\Omega}(n^{1 + 1/r})$-lower bound for any $r$-round deterministic
algorithm in the OR-model.
</p>
<p>* We design a randomized, $2$-round algorithm for the graph connectivity
problem which makes $\tilde{O}(n)$-OR queries. In contrast, we prove that any
$1$-round algorithm (possibly randomized) requires $\tilde{\Omega}(n^2)$-OR
queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06098"><span class="datestr">at July 14, 2020 11:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06060">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06060">Recognizing $k$-Clique Extendible Orderings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francis:Mathew.html">Mathew Francis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neogi:Rian.html">Rian Neogi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raman:Venkatesh.html">Venkatesh Raman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06060">PDF</a><br /><b>Abstract: </b>A graph is $k$-clique-extendible if there is an ordering of the vertices such
that whenever two $k$-sized overlapping cliques $A$ and $B$ have $k-1$ common
vertices, and these common vertices appear between the two vertices $a,b\in
(A\setminus B)\cup (B\setminus A)$ in the ordering, there is an edge between
$a$ and $b$, implying that $A\cup B$ is a $(k+1)$-sized clique. Such an
ordering is said to be a $k$-C-E ordering. These graphs arise in applications
related to modelling preference relations. Recently, it has been shown that a
maximum sized clique in such a graph can be found in $n^{O(k)}$ time when the
ordering is given. When $k$ is $2$, such graphs are precisely the well-known
class of comparability graphs and when $k$ is $3$ they are called
triangle-extendible graphs. It has been shown that triangle-extendible graphs
appear as induced subgraphs of visibility graphs of simple polygons, and the
complexity of recognizing them has been mentioned as an open problem in the
literature.
</p>
<p>While comparability graphs (i.e. $2$-C-E graphs) can be recognized in
polynomial time, we show that recognizing $k$-C-E graphs is NP-hard for any
fixed $k \geq 3$ and co-NP-hard when $k$ is part of the input. While our
NP-hardness reduction for $k \geq 4$ is from the betweenness problem, for
$k=3$, our reduction is an intricate one from the $3$-colouring problem. We
also show that the problems of determining whether a given ordering of the
vertices of a graph is a $k$-C-E ordering, and that of finding an $\ell$-sized
(or maximum sized) clique in a $k$-C-E graph, given a $k$-C-E ordering, are
complete for the parameterized complexity classes co-W[1] and W[1]
respectively, when parameterized by $k$. However we show that the former is
fixed-parameter tractable when parameterized by the treewidth of the graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06060"><span class="datestr">at July 14, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.06052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.06052">City Guarding with Limited Field of View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daescu:Ovidiu.html">Ovidiu Daescu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Malik:Hemant.html">Hemant Malik</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06052">PDF</a><br /><b>Abstract: </b>Drones and other small unmanned aerial vehicles are starting to get
permission to fly within city limits. While video cameras are easily available
in most cities, their purpose is to guard the streets at ground level. Guarding
the aerial space of a city with video cameras is a problem that so far has been
largely ignored.
</p>
<p>In this paper, we present bounds on the number of cameras needed to guard the
city's aerial space (roofs, walls, and ground) using cameras with 180-degree
range of vision (the region in front of the guard), which is common for most
commercial cameras. We assume all buildings are vertical and have a rectangular
base. Each camera is placed at a top corner of a building.
</p>
<p>We considered the following two versions: (i) buildings have an axis-aligned
ground base and, (ii) buildings have an arbitrary orientation. We give
necessary and sufficient results for (i), necessary results for (ii), and
conjecture sufficiency results for (ii). Specifically, for (i) we prove a
sufficiency bound of 2k + k/4 +4 on the number of vertex guards, while for (ii)
we show that 3k + 1 vertex guards are sometimes necessary, where k is the total
number of buildings in the city.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.06052"><span class="datestr">at July 14, 2020 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05912">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05912">Robust Learning of Mixtures of Gaussians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05912">PDF</a><br /><b>Abstract: </b>We resolve one of the major outstanding problems in robust statistics. In
particular, if $X$ is an evenly weighted mixture of two arbitrary
$d$-dimensional Gaussians, we devise a polynomial time algorithm that given
access to samples from $X$ an $\eps$-fraction of which have been adversarially
corrupted, learns $X$ to error $\poly(\eps)$ in total variation distance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05912"><span class="datestr">at July 14, 2020 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05870">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05870">A subquadratic algorithm for the simultaneous conjugacy problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brodnik:Andrej.html">Andrej Brodnik</a>, Aleksander Malnič, Rok Požar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05870">PDF</a><br /><b>Abstract: </b>The $d$-Simultaneous Conjugacy problem in the symmetric group $S_n$ asks
whether there exists a permutation $\tau \in S_n$ such that $b_j = \tau^{-1}a_j
\tau$ holds for all $j = 1,2,\ldots, d$, where $a_1, a_2,\ldots , a_d$ and
$b_1, b_2,\ldots , b_d$ are given sequences of permutations in $S_n$. The time
complexity of existing algorithms for solving the problem is $O(dn^2)$. We show
that for a given positive integer $d$ the $d$-Simultaneous Conjugacy problem in
$S_n$ can be solved in $o(n^2)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05870"><span class="datestr">at July 14, 2020 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05852">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05852">Submodular Meta-Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adibi:Arman.html">Arman Adibi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mokhtari:Aryan.html">Aryan Mokhtari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassani:Hamed.html">Hamed Hassani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05852">PDF</a><br /><b>Abstract: </b>In this paper, we introduce a discrete variant of the meta-learning
framework. Meta-learning aims at exploiting prior experience and data to
improve performance on future tasks. By now, there exist numerous formulations
for meta-learning in the continuous domain. Notably, the Model-Agnostic
Meta-Learning (MAML) formulation views each task as a continuous optimization
problem and based on prior data learns a suitable initialization that can be
adapted to new, unseen tasks after a few simple gradient updates. Motivated by
this terminology, we propose a novel meta-learning framework in the discrete
domain where each task is equivalent to maximizing a set function under a
cardinality constraint. Our approach aims at using prior data, i.e., previously
visited tasks, to train a proper initial solution set that can be quickly
adapted to a new task at a relatively low computational cost. This approach
leads to (i) a personalized solution for each individual task, and (ii)
significantly reduced computational cost at test time compared to the case
where the solution is fully optimized once the new task is revealed. The
training procedure is performed by solving a challenging discrete optimization
problem for which we present deterministic and randomized algorithms. In the
case where the tasks are monotone and submodular, we show strong theoretical
guarantees for our proposed methods even though the training objective may not
be submodular. We also demonstrate the effectiveness of our framework on two
real-world problem instances where we observe that our methods lead to a
significant reduction in computational complexity in solving the new tasks
while incurring a small performance loss compared to when the tasks are fully
optimized.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05852"><span class="datestr">at July 14, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05647">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05647">Finding Equilibrium in Multi-Agent Games with Payoff Uncertainty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Wenshuo.html">Wenshuo Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Curmei:Mihaela.html">Mihaela Curmei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Serena.html">Serena Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Recht:Benjamin.html">Benjamin Recht</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05647">PDF</a><br /><b>Abstract: </b>We study the problem of finding equilibrium strategies in multi-agent games
with incomplete payoff information, where the payoff matrices are only known to
the players up to some bounded uncertainty sets. In such games, an ex-post
equilibrium characterizes equilibrium strategies that are robust to the payoff
uncertainty. When the game is one-shot, we show that in zero-sum polymatrix
games, an ex-post equilibrium can be computed efficiently using linear
programming. We further extend the notion of ex-post equilibrium to stochastic
games, where the game is played repeatedly in a sequence of stages and the
transition dynamics are governed by an Markov decision process (MDP). We
provide sufficient condition for the existence of an ex-post Markov perfect
equilibrium (MPE). We show that under bounded payoff uncertainty, the value of
any two-player zero-sum stochastic game can be computed up to a tight value
interval using dynamic programming.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05647"><span class="datestr">at July 14, 2020 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05637">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05637">Dynamic Graph Streaming Algorithm for Digital Contact Tracing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahapatra:Gautam.html">Gautam Mahapatra</a>, Priodyuti~Pradhan, Ranjan Chattaraj, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banerjee:Soumya.html">Soumya Banerjee</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05637">PDF</a><br /><b>Abstract: </b>Digital contact tracing of an infected person, testing the possible infection
for the contacted persons, and isolation play a crucial role in alleviating the
outbreak. Here, we design a dynamic graph streaming algorithm that can trace
the contacts under the control of the Public Health Authorities (PHA). The
algorithm can work as the augmented part of the PHA for the crisis period. Our
algorithm receives proximity data from the mobile devices as contact data
streams and uses a sliding window model to construct a dynamic contact graph
sketch. Prominently, we introduce the edge label of the contact graph as a
contact vector, which acts like a sliding window and holds the latest D days of
social interactions. Importantly, the algorithm prepares the direct and
indirect (multilevel) contact list from the contact graph sketch for a given
set of infected persons. The algorithm also uses a disjoint set data structure
to construct the infectious trees for the trace list. The present study offers
the design of algorithms with underlying data structures for digital contact
trace relevant to the proximity data produced by Bluetooth enabled mobile
devices. Our analysis reveals that for COVID-19 close contact parameters, the
storage space requires maintaining the contact graph of ten million users
having 14 days close contact data in PHA server takes 55 Gigabytes of memory
and preparation of the contact list for a given set of the infected person
depends on the size of the infected list.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05637"><span class="datestr">at July 14, 2020 11:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05634">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05634">Vector Balancing in Lebesgue Spaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reis:Victor.html">Victor Reis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothvoss:Thomas.html">Thomas Rothvoss</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05634">PDF</a><br /><b>Abstract: </b>A tantalizing conjecture in discrete mathematics is the one of Koml\'os,
suggesting that for any vectors $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_2^m$
there exist signs $x_1, \dots, x_n \in \{ -1,1\}$ so that $\|\sum_{i=1}^n
x_i\mathbf{a}_i\|_\infty \le O(1)$. It is a natural extension to ask what
$\ell_q$-norm bound to expect for $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_p^m$.
We prove that, for $2 \le p \le q \le \infty$, such vectors admit fractional
colorings $x_1, \dots, x_n \in [-1,1]$ with a linear number of $\pm 1$
coordinates so that $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_q \leq
O(\sqrt{\min(p,\log(2m/n))}) \cdot n^{1/2-1/p+ 1/q}$, and that one can obtain a
full coloring at the expense of another factor of $\frac{1}{1/2 - 1/p + 1/q}$.
In particular, for $p \in (2,3]$ we can indeed find signs $\mathbf{x} \in \{
-1,1\}^n$ with $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_\infty \le O(n^{1/2-1/p} \cdot
\frac{1}{p-2})$. Our result generalizes Spencer's theorem, for which $p = q =
\infty$, and is tight for $m = n$.
</p>
<p>Additionally, we prove that for any fixed constant $\delta&gt;0$, in a centrally
symmetric body $K \subseteq \mathbb{R}^n$ with measure at least $e^{-\delta n}$
one can find such a fractional coloring in polynomial time. Previously this was
known only for a small enough constant -- indeed in this regime classical
nonconstructive arguments do not apply and partial colorings of the form
$\mathbf{x} \in \{ -1,0,1\}^n$ do not necessarily exist.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05634"><span class="datestr">at July 14, 2020 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05580">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05580">A Strong XOR Lemma for Randomized Query Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brody:Joshua.html">Joshua Brody</a>, Jae Tak Kim, Peem Lerdputtipongporn, Hari Srinivasulu <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05580">PDF</a><br /><b>Abstract: </b>We give a strong direct sum theorem for computing $xor \circ g$.
Specifically, we show that for every function g and every $k\geq 2$, the
randomized query complexity of computing the xor of k instances of g satisfies
$\overline{R}_\eps(xor\circ g) = \Theta(k \overline{R}_{\eps/k}(g))$. This
matches the naive success amplification upper bound and answers a conjecture of
Blais and Brody (CCC19).
</p>
<p>As a consequence of our strong direct sum theorem, we give a total function g
for which $R(xor \circ g) = \Theta(k \log(k)\cdot R(g))$, answering an open
question from Ben-David et al.(arxiv:<a href="http://export.arxiv.org/abs/2006.10957">2006.10957v1</a>).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05580"><span class="datestr">at July 14, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.05557">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.05557">Learning Entangled Single-Sample Gaussians in the Subset-of-Signals Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liang:Yingyu.html">Yingyu Liang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Hui.html">Hui Yuan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05557">PDF</a><br /><b>Abstract: </b>In the setting of entangled single-sample distributions, the goal is to
estimate some common parameter shared by a family of $n$ distributions, given
one single sample from each distribution. This paper studies mean estimation
for entangled single-sample Gaussians that have a common mean but different
unknown variances. We propose the subset-of-signals model where an unknown
subset of $m$ variances are bounded by 1 while there are no assumptions on the
other variances. In this model, we analyze a simple and natural method based on
iteratively averaging the truncated samples, and show that the method achieves
error $O \left(\frac{\sqrt{n\ln n}}{m}\right)$ with high probability when
$m=\Omega(\sqrt{n\ln n})$, matching existing bounds for this range of $m$. We
further prove lower bounds, showing that the error is
$\Omega\left(\left(\frac{n}{m^4}\right)^{1/2}\right)$ when $m$ is between
$\Omega(\ln n)$ and $O(n^{1/4})$, and the error is
$\Omega\left(\left(\frac{n}{m^4}\right)^{1/6}\right)$ when $m$ is between
$\Omega(n^{1/4})$ and $O(n^{1 - \epsilon})$ for an arbitrarily small
$\epsilon&gt;0$, improving existing lower bounds and extending to a wider range of
$m$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.05557"><span class="datestr">at July 14, 2020 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=3843">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/">Gradient descent for wide two-layer neural networks – II: Generalization and implicit bias</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">previous post</a>, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The content is mostly based on our recent joint work [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</p>



<h2>1. Generalization with weight decay regularization</h2>



<p class="justify-text">Let us start our journey with the comfortable case where the training objective includes an explicit <em>weight decay</em> regularization (i.e. \(\ell_2\)-regularization on the parameters). Using the notations of the previous post, this consists in the following objective function on the space of probability measures on \(\mathbb{R}^{d+1}\):  $$ \underbrace{R\Big(\int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w)\Big)}_{\text{Data fitting term}} + \underbrace{\frac{\lambda}{2} \int_{\mathbb{R}^{d+1}} \Vert w \Vert^2_2d\mu(w)}_{\text{Regularization}} \tag{1}$$ where \(R\) is the loss and \(\lambda&gt;0\) is the regularization strength. Remember that a  neural network of finite width with \(m\) neurons is recovered with an empirical measure \(\mu = \frac1m \sum_{j=1}^m\delta_{w_j}\), in which case this regularization is proportional to the sum of the squares of all the parameters \(\frac{\lambda}{2m}\sum_{j=1}^m \Vert w_j\Vert^2_2\).</p>



<p class="justify-text"><strong>Variation norm.</strong> In the previous post, we have seen that the Wasserstein gradient flow of this objective function — an idealization of the gradient descent training dynamics in the large width limit — converges to a global minimizer \(\mu^*\) when initialized properly. An example of an admissible initialization is the hidden weights \(b_j\) distributed according to the uniform distribution \(\tau\) on the unit sphere \(\mathbb{S}^{d-1}\subset \mathbb{R}^d\) and the output weights \(a_j\) uniform in \(\{-1,1\}\). What does this minimizer look like in predictor space when the objective function is as in Eq. (1) ? </p>



<p class="justify-text">To answer this question, we define for a predictor \(h:\mathbb{R}^d\to \mathbb{R}\), the quantity $$ \Vert h \Vert_{\mathcal{F}_1} := \min_{\mu \in \mathcal{P}(\mathbb{R}^{d+1})} \frac{1}{2} \int_{\mathbb{R}^{d+1}} \Vert w\Vert^2_2 d\mu(w) \quad \text{s.t.}\quad h = \int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w).\tag{2} $$ As the notation suggests, \(\Vert \cdot \Vert_{\mathcal{F}_1}\) is a norm in the space of predictors. It is known as the <em>variation norm</em> [<a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">2</a>, <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">3</a>]. We call \(\mathcal{F}_1\) the space of functions with finite norm, which is a (non-Hilbertian) Banach space. By construction, the learnt predictor \(h^* = \int \Phi(w)d\mu^*(w)\) is a minimizer of the \(\mathcal{F}_1\)-regularized regression: $$ \min_{h:\mathbb{R}^d\to \mathbb{R}} R(h) + \lambda \Vert h \Vert_{\mathcal{F}_1} \tag{3}.$$ This \(\mathcal{F}_1\)-norm shares similarity with \(\ell_1\) regularization [<a href="https://arxiv.org/pdf/1412.6614.pdf">4</a>]. To see this, observe that the “magnitude” \(\vert a\vert \Vert b\Vert_2\) of a relu function \(x\mapsto a(b^\top x)_+\) with parameter \(w=(a,b)\) equals \(\Vert w\Vert^2_2/2\) if \(\vert a\vert = \Vert b\Vert_2\) and is smaller otherwise. Thus parameterizing the relus by their direction \(\theta = b/\Vert b\Vert_2\) and optimizing over their signed magnitude \(r(\theta) = a\Vert b\Vert_2\)  we have $$ \Vert h \Vert_{\mathcal{F}_1} = \inf_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).\tag{4}$$</p>



<p class="justify-text"><strong>Conjugate RKHS norm.</strong> The regression in the space \(\mathcal{F}_1\) is best understood when compared with the regression obtained by only training the output weights. We consider the same training dynamics with weight decay except that we fix the hidden weights to their initial value, where they are distributed according to the uniform distribution \(\tau\) on the sphere. In that case, the Wasserstein gradient flow also converges to the solution of a regularized regression as in Eq. (3) – this is in fact a convex problem –  but the regularizing norm is different and now defined as $$ \Vert h \Vert_{\mathcal{F}_2}^2 := \min_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert^2 d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).$$ We call \(\mathcal{F}_2\) the set of functions with finite norm. It can be shown to be a <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Space</a> (RKHS), with kernel  $$ K(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+ d\tau(\theta),$$ which has a closed form expression [<a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">5</a>]. In this context, taking a finite width neural network corresponds to a random feature approximation of the kernel [<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">6</a>, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines">7</a>].</p>



<p class="justify-text">Let us informally compare the properties of these spaces \(\mathcal{F}_1\) and \(\mathcal{F}_2\) (see [<a href="https://arxiv.org/abs/1412.8690">2</a>] for details):</p>



<ul class="justify-text"><li><strong>Approximation power.</strong> In high dimension, only very smooth functions have small \(\mathcal{F}_2\)-norm (in rough terms, the \((d+1)/2\) first derivatives should be small). In contrast, there exists non-smooth functions with small \(\mathcal{F}_1\)-norm, an example being the relu function \(x\mapsto (\theta^\top x)_+\). Remarkably, if we define \(f(x)=g(Ux)\) where \(U\) is an orthogonal projection then \(\Vert f\Vert_{\mathcal{F}_1} \leq  \Vert g\Vert_{\mathcal{F}_2}\). This shows in particular that \(\mathcal{F}_1\) contains \(\mathcal{F}_2\) and that \(\mathcal{F}_1\) is <em>adaptive</em> to lower dimensional structures.</li><li><strong>Statistical complexity.</strong> It could be feared that the good approximation properties of \(\mathcal{F}_1\) come at the price of being “too large” as a hypothesis space, making it difficult to estimate a predictor in \(\mathcal{F}_1\) from few samples. But, as measured by bounds on their Rademacher complexities, the unit ball of \(\mathcal{F}_1\) is only \(O(\sqrt{d})\) larger than that of \(\mathcal{F}_2\). By going from \(\mathcal{F}_2\) to \(\mathcal{F}_1\), we thus add some nicely structured predictors to our hypothesis space, but not too much garbage that could fit unstructured noise.</li><li><strong>Generalization guarantees.</strong> By combining the two previous points, it is possible to prove that supervised learning in \(\mathcal{F}_1\) breaks the curse of dimensionality when the output depends on a lower dimensional projection of the input: the required number of training samples only depends mildly on the dimension \(d\).</li><li><strong>Optimization guarantees.</strong> However \(\mathcal{F}_1\) has a strong drawback : there is no known algorithm that solves the problem of Eq. (3) in polynomial time. On practical problems, gradient descent seems to behave well, but in general only qualitative results such as presented in the previous post are known. In contrast, various provably efficient algorithms can solve regression in \(\mathcal{F}_2\), which is a classical kernel ridge regression problem [Chap. 14.4.3, <a href="https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf">8</a>].</li></ul>



<p class="justify-text">In the plot below, we compare the predictor learnt by gradient descent for a 2-D regression with the square loss and weight decay, after training (a) both layers — which is regression in \(\mathcal{F}_1\) — or (b) just the output layer — which is regression in \(\mathcal{F}_2\). This already illustrates some distinctive features of both spaces, although the differences become more stringent in higher dimensions. In particular, observe that in (a) the predictor is the combination of few relu functions, which illustrates  the sparsifying effect of the \(L^1\)-norm in Eq. (4). To simplify notations, we do not include a bias/intercept in the formulas but our numerical experiments include it, so in this plot the input is of the form \(x=(x_1,x_2,1)\) and \(d=3\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="564" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/regularized-2.png" class="wp-image-4231" height="293" />Predictor learnt by the gradient flow on the square loss with weight decay, when training (a) both layers (b) only the output layer. The markers indicate the location of the training samples  \(x_i\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_weightdecay.jl">[code]</a></figure></div>



<p class="justify-text">The qualitative picture is quite clear so far, but something is a bit unsettling: weight decay is often not needed to obtain a good performance in practice. Our line of reasoning however completely falls apart without such a regularization: if the objective function depends on the predictor only via its values on the training set, being a minimizer does not guarantee anything about generalization outside of the training set (remember that wide relu neural networks are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>). Why does it still work in the unregularized case? There must be something in the algorithm…</p>



<h2>2. Implicit bias: linear classification</h2>



<p class="justify-text">This something is called the <em>implicit bias</em> : when there are several minimizers, the optimization algorithm makes a specific choice. In the unregularized case, the “quality” of this choice is a crucial property of an algorithm; much more crucial than, say, its convergence speed on the training objective. To gradually build our intuition of the implicit bias of gradient flows, let us put neural networks aside for a moment and consider, following Soudry, Hoffer, Nacson, Gunasekar and Srebro [<a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">9</a>], a linear classification task.</p>



<p class="justify-text"><strong>Gradient flow of the smooth-margin.</strong> Let \((x_i,y_i)_{i=1}^n\) be a training set of \(n\) pairs of inputs \(x_i\in \mathbb{R}^d\) and outputs \(y_i\in \{-1,1\}\). Let us choose the exponential loss. The analysis that follows also apply to the logistic loss (which is the same as the cross-entropy loss after a sigmoid non-linearity) because only the “tail” of the loss matters, but it is more straightforward with the exponential loss. In order to give a natural “scale” to the problem, we  renormalize the empirical risk by taking minus its logarithm and consider the concave objective $$ F_\beta(a) = -\frac{1}{\beta}\log\Big( \frac1n \sum_{i=1}^n \exp(-\beta y_i \ x_i^\top a) \Big).\tag{5}$$ </p>



<p class="justify-text">Here \(\beta&gt;0\) is a parameter that will be useful in a moment. For now, we take \(\beta=1\) and we note \(F(a)=F_1(a)\).  In this context, the <em>margin</em> of a vector \(a\in \mathbb{R}^d\) is the quantity \(\min_{i} y_i\ x_i^\top a\) which quantifies how far this linear predictor is from making a wrong prediction on the training set.  </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="386" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/max_margin-3.png" class="wp-image-4201" height="329" />The margin of the linear predictor \(x \mapsto a^\top x\) with parameters \(a \in \mathbb{S}^d\) is the smallest distance of a training point to the decision boundary. We show here the max-margin predictor.</figure></div>



<p class="justify-text">Obtained via simple manipulations, the inequalities  $$ \min_i y_i\ x_i^\top a \leq F_\beta(a) \leq \min_i y_i\ x_i^\top a +\frac{\log(n)}{\beta}, \tag{6}$$ suggest to call \(F_\beta\) the <em>smooth-margin</em> because, well, it is smooth and converges to the margin \(F_\infty(a) := \min_i y_i x_i^\top a\) as \(\beta\to \infty\). Let us look at the gradient flow in the ascent direction that maximizes the smooth-margin: $$ a'(t) = \nabla F(a(t))$$ initialized with \(a(0)=0\) (here the initialization does not matter so much). The path followed by this gradient flow is exactly the same as the gradient flow on the empirical risk: taking the logarithm only changes the time parameterization or, in practice, the step-size.</p>



<p class="justify-text"><strong>Convergence to the max-margin.</strong> Assume that the data set is linearly separable, which means that the \(\ell_2\)-max-margin $$ \gamma := \max_{\Vert a\Vert_2 \leq 1} \min_i y_i x_i^\top a$$ is positive. In this case \(F\) is unbounded (indeed \(\lim_{\alpha \to \infty} F(\alpha a) =\infty\) whenever \(a\)  has a positive margin) and thus \(a(t)\) diverges. This is not an issue as such, since for classification, only the sign of the prediction matters.  This just means that the relevant question is not “where does \(a(t)\) converge?” but rather “towards which direction does it diverge?”. In other words, we are interested in the limit of \(\bar a(t):= a(t)/\Vert a(t)\Vert_2\) (in convex analysis, this is called the <em>cosmic limit</em> of \(a(t)\) [Chap. 3, <a href="https://www.springer.com/gp/book/9783540627722">10</a>], isn’t it beautiful ?).</p>



<p class="justify-text">It can be shown with a duality argument (see the end of the blog post) that \(\Vert \nabla F(a)\Vert_2\geq \gamma\) for all \(a\in \mathbb{R}^d\). By the inequality of Eq. (6) and the gradient flow property \(\frac{d}{dt}F(a(t))=\Vert \nabla F(a(t))\Vert_2^2\), it follows $$\begin{aligned}\min_i y_i x_i^\top a(t) \geq F(a(t)) \  – \log(n) \geq \gamma \int_0^t \Vert \nabla F(a(s))\Vert_2ds -\log (n).\end{aligned}$$  For \(t&gt; \log(n)/\gamma^2\), this lower bound is positive. We can then divide the left-hand side by \(\Vert a(t)\Vert_2\) and the right-hand side by the larger quantity \(\int_0^t \Vert\nabla F(a(s))\Vert_2ds\), and we get $$\min_i y_i x_i^\top \bar a(t) \geq \gamma -\frac{\log(n)}{\int_0^t \Vert\nabla F(a(s))\Vert_2ds} \geq \gamma -\frac{\log(n)}{\gamma t}.$$ This shows that the margin of \(\bar a(t) := a(t)/\Vert a(t)\Vert_2\) converges to the \(\ell_2\)-max-margin at a rate \(\log(n)/\gamma t\). That’s it, the implicit bias of this gradient flow is exposed!</p>



<p class="justify-text"><strong>Stability to step-size choice.</strong> To translate this argument to discrete time, we need decreasing step-sizes of order \(1/\sqrt{t}\) which deteriorates the convergence rate to \(\tilde O(1/\sqrt{t})\), see [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>]. In [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>], we proposed a different proof strategy (based on an online optimization interpretation of \(\bar a(t)\), as below) which recovers the same convergence rate \(O(1/\sqrt{t})\) with <em>exponentially larger</em> step-sizes. This suggests that these diverging trajectories are extremely robust to the choice of step-size.</p>



<p class="justify-text"><strong>Illustration. </strong>In the figure below, we plot on the left the evolution of the parameter \(a(t)\) and on the right the predictor \(x\mapsto (x,1)^\top a(t)\) with \(x\in \mathbb{R}^2\). In parameter space, we apply the hyperbolic tangent to the radial component which allows to easily visualize diverging trajectories. This way, the unit sphere represents the <em>horizon</em> of \(\mathbb{R}^d\), i.e., the set of directions at infinity [Chap. 3 in <a href="https://www.springer.com/gp/book/9783540627722">9</a>]. We will use the same convention in the other plots below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="586" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/linear.gif" class="wp-image-4106" height="288" />Implicit bias of gradient descent for a linear classification task with the exponential loss: (left) parameter space, (right) predictor space.</figure></div>



<h2>3. Implicit bias:  training only the output layer</h2>



<p class="justify-text">Despite its apparently restrictive setting, the previous result already tells us something about wide neural networks. Consider the situation touched upon earlier where we only train the output weights \(a_j\) and the hidden weights \(b_j\) are initialized uniformly at random on the sphere. This corresponds to learning a linear classifier on top of the random feature \([(b_j^\top x)_+]_{j=1}^m\). </p>



<p class="justify-text">As we have just shown, if the training set is separable, the normalized gradient flow of the unregularized exponential loss (or logistic loss) converges to a solution to  $$ \max_{\Vert a\Vert_2 \leq 1}\min_i y_i \sum_{j=1}^m  a_j (b_j^\top x_i)_+.$$ </p>



<p class="justify-text">This is a random feature approximation for the unregularized kernel support vector machine problem in the RKHS \(\mathcal{F}_2\), which is recovered in the large width limit \(m\to \infty\):  $$\max_{\Vert h\Vert_{\mathcal{F}_2}\leq 1} \min_i y_i h(x_i).$$ Notice that if \(m\) is large enough, the linear separability assumption is not even needed anymore, because any training set is separable in \(\mathcal{F}_2\) (at least if all \(x_i\)s are distinct and if we include the bias/intercept).</p>



<p class="justify-text"><strong>Illustration.</strong> In the animation below, we plot on the left the evolution of the parameters and on the right the predictor for a 2-D classification task. In parameter space, each particle represents a neuron: their direction is fixed and their weight is learnt (do not pay attention to the colors yet). As above, the unit sphere is at infinity and the particles diverge. In predictor space, +/- are the training samples of both classes, the color shows the predictor and the black line is the decision boundary. The fact that the predictor has a smooth decision boundary is in accordance with the properties of \(\mathcal{F}_2\) given above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_output_comp.gif" alt="" class="wp-image-4192" />Gradient descent on the output layer of a two-layer relu neural network with the exponential loss: (left) parameter space, (right) predictor space. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_output.jl">[code]</a></figure></div>



<h2>4. Implicit bias: 2-homogeneous linear classifiers</h2>



<p class="justify-text">Although the analyses where neural networks behave like kernel methods are pleasant for us theoreticians because we are in conquered territory, they miss essential aspects of neural networks such as their adaptivity and their ability to learn a representation. Let us see if we can characterize the implicit bias of the gradient flow of the unregularized exponential loss when training <em>both</em> layers of the neural network.</p>



<p class="justify-text"><strong>A 2-homogeneous linear model.</strong> From an optimization point of view, an important property of two layer relu neural networks is that \(\Phi(\alpha w)= \alpha^2 \Phi(w)\) for all \(\alpha&gt;0\), i.e., they are positively 2-homogeneous in the training parameters. In contrast, a linear model is 1-homogeneous in the parameter. This seemingly little difference leads to drastic changes in the gradient flow dynamics. </p>



<p class="justify-text">Let us again build our intuition with a simplified model that captures key aspects of the dynamics, namely the linear classification setting of above. This time, we take any initialization \(r(0)\in \mathbb{R}^d\) with positive entries and the gradient flow in the ascent direction of the function \( F(r\odot r)\) where \(\odot\) is the pointwise product between two vectors and \(F\) is defined in Eq. (5). This is just a trick to obtain a 2-homogeneous parameterization of a linear model. This gradient flow satisfies $$ r'(t) = 2 r(t)\odot \nabla F(r(t)\odot r(t)).$$ </p>



<p class="justify-text"><strong>Normalized dynamics.</strong> Let us define \(\bar a(t):=(r(t)\odot r(t))/\Vert r(t)\Vert_2^2\) the normalized predictor associated to our dynamics which, by definition, belongs to the simplex \(\Delta_d\), i.e., the set of nonnegative vectors in \(\mathbb{R}^d\) that sum to one. Using the fact that \(\nabla F(\beta a) = \nabla F_\beta (a)\) for all \(\beta&gt;0\), we obtain $$\begin{aligned} \bar a'(t) &amp;= 2\frac{r(t)\odot r'(t)}{\Vert r(t)\Vert_2^2} -2 (r(t)^\top r'(t))\frac{r(t)\odot r(t)}{\Vert r(t)\Vert_2^4}\\ &amp;=4\bar a(t) \nabla F_{\Vert r(t)\Vert_2^2}(\bar a(t))\ – \alpha(t) \bar a(t)\end{aligned}$$ where \(\alpha(t)\) is the scalar such that \(\sum_{i=1}^d a’_i(t) =0\). Online optimization experts might have recognized that this is (continuous time) <em>online mirror ascent in the simplex</em> for the sequence of smooth-margin functions \(F_{\Vert r(t)\Vert_2^2}\). Notice in particular the multiplicative updates: they correspond to the entropy mirror function, and they are particularly well suited for optimization in the high dimensional simplex [Chap.4, <a href="https://arxiv.org/pdf/1405.4980.pdf">12</a>].</p>



<p>What do we learn from this reformulation? </p>



<ul class="justify-text"><li>We can prove (by similar means) that if the data set is linearly separable then \(\Vert r(t)\Vert_2^2\) diverges. So the sequence of functions \(F_{\Vert r\Vert_2^2}\) converges to the margin \(F_\infty\) which means that \(\bar a(t)\) just ends up optimizing the function \(F_\infty\). As a consequence, we have $$\lim_{t\to \infty} y_i x_i^\top \bar a(t) = \max_{a\in \Delta_d} \min_{i} y_i x_i^\top a.$$ This exposes another implicit bias of gradient flow. Notice the key difference with the implicit bias obtained with a linear parameterization: we obtain here the \(\ell_1\)-max-margin (over classifiers with non-negative entries) instead of the \(\ell_2\)-max-margin.  </li><li>Beyond exposing the implicit bias, this reformulation shows that \(\bar a(t)\) implicitly optimizes a sequence of smooth objectives which converge to the margin \(F_\infty\). Unknowingly, we have recovered the well-principled optimization method that consists in approximating a non-smooth objective with smooth functions [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">13</a>].</li><li>While the conclusion above was only formal, this point of view leads to rigorous proofs of convergence and convergence rates in discrete time in \(\tilde O(1/\sqrt{t})\) with a step-size in \(O(1/\sqrt{t})\), by  exploiting tools from online optimization, see [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</li></ul>



<h2>5. Implicit bias: fully trained 2-layer neural networks</h2>



<p class="justify-text">Once again this argument about linear predictors applies to neural networks: if we train both layers but only the magnitude of the hidden weights and not their direction, then this is equivalent to learning a 2-homogeneous linear model on top of the random feature \([  a_j(0) (x_i^\top b_j(0))_+]_{j=1}^m\). If each feature appears twice with opposite signs — which is essentially the case in the large width limit — then the simplex constraint can be equivalently replaced by an \(\ell_1\)-norm constraint on the weights. Recalling the definition of the \(\mathcal{F}_1\)-norm from Eq. (4), we thus obtain that, in the infinite-width limit, the normalized predictor converges to a solution to $$ \max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">This result is correct, but it is not relevant. In contrast to functions in \(\mathcal{F}_2\), functions in \(\mathcal{F}_1\) <em>can not</em> in general be approximated with few <em>random</em> features in high dimension. In fact, lower bounds that are exponential in the dimension exist in certain settings [Sec. X, <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">14</a>]. They can be approximated with a small number of features but those need to be data-dependent: in that sense, it is necessary to learn a representation – here,  a distribution over the hidden weights — in order to learn in \(\mathcal{F}_1\). </p>



<p class="justify-text">This raises the following question: do we obtain the same implicit bias when training both layers of the neural network ? In the following result which is the main theorem of our paper [<a href="https://arxiv.org/abs/2002.04486">1</a>], we answer by the affirmative.</p>



<p class="justify-text"><strong>Theorem</strong> (C. and Bach [<a href="https://arxiv.org/abs/2002.04486">1</a>], informal). Assume that for some \(\sigma&gt;0\), the hidden weights \(b_j\) are initialized uniformly on the sphere of radius \(\sigma\) and the output weights \(a_j\) are uniform in \(\{-\sigma,\sigma\}\). Let \(\mu_t\) be the Wasserstein gradient flow for the unregularized exponential loss and \(h_t = \int \Phi(w)d\mu_t(w)\) be the corresponding dynamics in predictor space. Under some technical assumptions, the normalized predictor \(h_t/\Vert h_t\Vert_{\mathcal{F}_1}\) converges to a solution to the \(\mathcal{F}_1\)-max-margin problem: $$\max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">Giving an idea of proof would be a bit too technical for this blog post, but let us make some remarks:</p>



<ul class="justify-text"><li>The strength of this result is that although this dynamics could get trapped towards limit directions which are not optimal, this choice of initialization allows to avoid them all and to only converge to <em>global</em> minimizers of this max-margin problem. The principle behind this is similar to the global convergence result in the previous blog post. </li><li>The fact that optimizing on the direction of the hidden weights is compatible with the global optimality conditions of the \(\mathcal{F}_1\)-max-margin problem is very specific to the structure of positively 2-homogeneous problems, and should not be taken for granted for other architectures of neural networks.</li><li>Although at a formal level this result works for any initialization that is diverse enough (such as the standard Gaussian initialization), the initialization proposed here yields dynamics with a better behavior for relu networks: by initializing the hidden and output weights with equal norms – a property preserved by the dynamics – we avoid some instabilities in the gradient. Also notice that this result applies to any scale \(\sigma\) of the initialization (we’ll see an intriguing consequence of this in the next section).</li></ul>



<p class="justify-text"><strong>Illustration.</strong> In the figure below, we plot the training dynamics when both layers are trained. In parameter space (left), each particle represents a neuron: its position is \(\vert a_j\vert b_j\) and its color depends on the sign of \(a_j\).  Here again the unit sphere is at infinity. The inactive neurons at the bottom correspond to those with a bias that is “too negative” at initialization. We observe that all the other neurons gather into few clusters: this is the sparsifying effect of the \(L^1\)-norm in Eq. (4). In predictor space, we obtain a polygonal classifier, as expected for a \(\mathcal{F}_1\)-max-margin classifier. See the paper [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>] for experiments that illustrate the strengths of this classifier in terms of generalization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_both_comp.gif" alt="" class="wp-image-4194" />Training both layers of a wide relu neural network with the exponential loss: (left) space of parameters, (right) space of predictors. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_bothlayers.jl">[code]</a></figure></div>



<h2>6. Lazy regime and the neural tangent kernel</h2>



<p class="justify-text">This blog post would not be complete without mentioning the <em>lazy regime</em>. This is yet another kind of implicit bias which, in our context, takes place when at initialization the weights have a large magnitude and the step-size is small. It was first exhibited in [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">15</a>] for deep neural networks.</p>



<p class="justify-text"><strong>Lazy training via scaling.</strong> This phenomenon is in fact very general so let us present it with a generic parametric predictor \(h(W)\) with differential \(Dh(W)\). We introduce a scaling factor \(\alpha&gt;0\) and look at the gradient flow of \(F(W) := R(\alpha h(W))\) with a step-size \(1/\alpha^2\), that is $$ W'(t) = \ – \frac{1}{\alpha}Dh(W(t))^\top \nabla R(\alpha h(W(t))),$$ with initialization \(W(0)\). In terms of the predictor \(\alpha h(W)\), this yields the dynamics $$\frac{d}{dt} \alpha h(W(t)) = \ – Dh(W(t))Dh(W(t))^\top \nabla R(\alpha h(W(t)).$$ </p>



<p class="justify-text">Lazy training [<a href="https://arxiv.org/pdf/1812.07956.pdf">16</a>] happens when we take \(\alpha\) large while making sure that \(\alpha h(W(0))\) stays bounded. In this case, we see that the parameters change at a rate \(O(1/\alpha)\), while the predictor changes at a rate independent of \(\alpha\). In the limit of  a large \(\alpha\), the parameters only move infinitesimally on any given time interval, while the predictor still makes significant progress, hence the name <em>lazy training</em>.</p>



<p class="justify-text"><strong>Equivalent linear model.</strong> Since the parameters hardly move, we can replace the map \(h\) by its linearization \(\bar h(w) = h(w(0))+Dh(w(0))(w-w(0))\) if we assume that \(Dh(w(0))\neq 0\). This means that the dynamics essentially follows the gradient flow of $$ R\big ( \alpha h(W(0)) + \alpha Dh(W(0))(W-W(0)) \big)$$ which is convex as soon as \(R\) is convex.</p>



<p class="justify-text">If this linearized objective admits a minimizer that is not too far away from \(W(0)\), then \(W(t)\) converges to this minimizer. If in contrast all  the minimizers are too far away (think of the exponential loss), then the lazy regime is just a transient regime in the early phase of training.  Of course, all these behaviors can be quantified and made more precise, because this puts us back to the realm of linear models. But what all of this has to do with two-layer neural networks? As it happens, this scale factor appears implicit in various situations for these models; let us detail two of them. </p>



<p class="justify-text"><strong>Neural networks with \(1/\sqrt{m}\) scaling.</strong> For two-layer neural networks, lazy training occurs if we define \(h = \frac{1}{\sqrt{m}} \sum_{i=1}^n \Phi(w_i)\) instead of \(h=\frac{1}{m} \sum_{i=1}^m \Phi(w_i)\) before taking the infinite width limit. Indeed:</p>



<ul class="justify-text"><li>This induces a scaling factor \(\alpha = \sqrt{m} \to \infty\) compared to \(1/m\) which, as we have already seen, is the “correct” scaling that leads to a non-degenerate dynamics in parameter space as \(m\) increases. </li><li>Moreover, by the central limit theorem,  \(\frac{1}{\sqrt{m}} \sum_{i=1}^m \Phi(w_i(0)) = O(1)\) for typical random initializations of the parameters. So the initial predictor stays bounded.</li></ul>



<p class="justify-text">To take the Wasserstein gradient flow limit, the step-size was of order \(m\) (see previous blog post). So here we should take a step-size of order \(m/\alpha^2 = 1\). With such a step-size, all the conditions for lazy training are gathered when \(m\) is large. Intuitively, each neuron only moves infinitesimally, but they collectively produce a significant movement in predictor space.</p>



<p class="justify-text"><strong>Neural networks with large initialization.</strong> Coming back to our scaling in \(1/m\) and our Wasserstein gradient flow that is obtained in the large width limit, there is another way to enter the lazy regime: by increasing the variance of the initialization. </p>



<p class="justify-text">To see this, assume that \(h\) is a positively \(p\)-homogeneous parametric predictor, which means that \(h(\sigma W)=\sigma^p h(W)\) for all \(\sigma&gt;0\) and some \(p&gt;1\) (remember that this is true with \(p=2\) for our two-layer relu neural network). Take an initialization of the form \(W(0) = \sigma \bar W_0\) where \(h(\bar W_0)=0\) and \(\sigma&gt;0\) and consider the gradient flow of \(R(h(W))\) with step-size \(\sigma^{2-2p}\).   By defining \(\bar W(t) = W(t)/\sigma\) and using the fact that the differential of a p-homogeneous function <a href="https://en.wikipedia.org/wiki/Homogeneous_function#Positive_homogeneity">is (p-1)-homogeneous</a>, we have, on the one hand $$ \bar W'(t) = -\sigma^{1-2p} Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))), $$ and on the other hand $$\frac{d}{dt} \sigma^p h(\bar W(t)) =\  – Dh(\bar W(t))Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))).$$ So in terms of the dynamics \(\bar W(t)\), the situation is quite similar than if we had a scaling factor \(\alpha=\sigma^p\): as the magnitude \(\sigma\) of the initialization increases, we enter the lazy regime, provided the step-size is of order \(\sigma^{2-2p}\).</p>



<p class="justify-text"><strong>Neural tangent kernel. </strong>What does the lazy regime tell us about the learnt predictor for two-layer neural networks? Assuming for simplicity that the predictor at initialization is \(0\), this regime amounts to learning a linear model on top of the feature \([(b_j^\top x)_+]_{j=1}^m\) — the derivative with respect to the output weights — concatenated with the feature \([x a_j 1_{b_j^\top x &gt; 0} ]_{i=1}^m\)  — the derivative with respect to the input weights. Compared to training only the output layer, this simply adds some features. </p>



<p class="justify-text">Assume for concreteness, that at initialization the hidden weights \(b_j\) are uniform on a sphere of large radius \(\sigma&gt;0\) and the output weights are uniform on \(\{-\kappa\sigma, \kappa\sigma\}\) where \(\kappa\geq 0\). In the large width and large \(\sigma\) limit we enter the lazy regime which amounts to learning in a RKHS — let us call it \(\mathcal{F}_{2,\kappa}\) — that is slightly different from \(\mathcal{F}_2 = \mathcal{F}_{2,0}\), with kernel $$ K_\kappa(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+d\tau(\theta) + \kappa^2 \int_{\mathbb{S}^{d-1}} (x^\top x’) 1_{\theta^\top x &gt; 0}1_{\theta^\top x’ &gt; 0}d\tau(\theta). $$</p>



<p class="justify-text">This kernel is called the Neural Tangent Kernel [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">15</a>] and the properties of the associated RKHS have been studied in [<a href="https://arxiv.org/pdf/1904.12191.pdf">17</a>, <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels.pdf">18</a>], where it is shown to include functions that are slightly less smooth than those of \(\mathcal{F}_2\) when \(\kappa\) increases. This is illustrated in the plot below, obtained by training a wide neural network with \(\sigma\) large (to reach the lazy regime) on the square loss, and various values of \(\kappa\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="574" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/interp-4.png" class="wp-image-4213" height="287" />1-D regression with a wide two-layer relu neural network (gradient descent on square loss, in the lazy regime) with 4 training samples (black dots). At initialization, output weights have \(\kappa\) times the (large) magnitude of the hidden weights. This implicitly solves kernel ridgeless regression for a kernel that depends on \(\kappa\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_NTK.jl">[code]</a></figure></div>



<p class="justify-text"><strong>Two implicit biases in one shot.</strong> The attentive reader might have notice that for large initialization scale \(\sigma\gg 1\), when training both layers on the unregularized exponential loss, two of our analyses apply:  lazy training — that leads to a max-margin predictor in \(\mathcal{F}_{2,\kappa}\) — and the asymptotic implicit bias — that leads to a max-margin predictor in \(\mathcal{F}_{1}\). Is there a contradiction ?</p>



<p class="justify-text">No : since the minimizers of this loss are at infinity, the lazy regime is just a transient phase, so we will observe both implicit bias along the training dynamics! Take a look at the video below: we observe that in early phases of training, the neurons do not move while learning a smooth classifier — this is the lazy regime. In later stages of training, the neurons start moving and the predictor converges to a \(\mathcal{F}_1\)-max-margin classifier as stated by the main theorem. The predictor jitters a little bit during training because I have chosen rather aggressive step-sizes.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_lazy2sparse_ns_comp-1.gif" alt="" class="wp-image-4219" />Training both layers with gradient descent for the unregularized logistic loss. The only difference with the previous video is that at initialization the variance \(\sigma^2\) is larger and the step-size smaller \(\approx \sigma^{-2}\). First the network learns a classifier in the lazy regime (a kernel max-margin classifier) and eventually converges to the \(\mathcal{F}_1\)-max-margin.</figure></div>



<h2>Discussion</h2>



<p class="justify-text">In this blog post, I described how analyses of the training dynamics can help us understand the properties of the predictor learnt by neural networks even in the absence of an explicit regularization. Already for the simplest algorithm one can think of — gradient descent — we have found a variety of behaviors depending on the loss, the initialization or the step-size. </p>



<p class="justify-text">To achieve this description, the infinite width limit was of great help. It allowed us to obtain synthetic and simple characterizations of the learnt predictor, that lead to generalization bounds. Yet, there are many interesting non-asymptotic effects caused by having a finite width.  In that sense, we were only concerned with the end of the curve of double descent [<a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">18</a>].</p>



<h2>References</h2>



<p class="justify-text">[1] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss.</a> <em>To appear in Conference On Learning Theory</em>, 2020.<br />[2] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks.</a> <em>The Journal of Machine Learning Research</em>, <em>18</em>(1), 629-681, 2017.<br />[3]  Vera Kurková, Marcello Sanguineti. <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">Bounds on rates of variable-basis and neural-network approximation.</a> <em>IEEE Transactions on Information Theory</em>, 47(6):2659-2665, 2001.  <br />[4] Behnam Neyshabur, Ryota Tomioka, Nathan Srebro. <a href="https://arxiv.org/pdf/1412.6614.pdf">In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.</a> <em>ICLR (Workshop)</em>. 2015.<br />[5] Youngmin Cho, Lawrence K. SAUL.  <a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel methods for deep learning.</a> <em>Advances in neural information processing systems</em>. 342-350, 2009.<br />[6] Radford M. Neal. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf"><em>Bayesian learning for neural networks</em>.</a> Springer Science &amp; Business Media, 2012.<br />[7] Ali Rahimi, Benjamin Recht. <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines.</a> <em>Advances in neural information processing systems</em>. 1177-1184, 2008.<br />[8] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. <em>The MIT Press</em>, 2012<br />[9] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro. <a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data.</a><em> The Journal of Machine Learning Research</em>, <em>19</em>(1), 2822-2878, 2018.<br />[10] R. Tyrrell Rockafellar, Roger J-B. Wets. <a href="https://www.springer.com/gp/book/9783540627722"><em>Variational analysis</em>.</a> Springer Science &amp; Business Media, 2009.<br />[11] Suriya Gunasekar,  Jason D. Lee, Daniel Soudry, Nathan Srebro.  <a href="https://par.nsf.gov/servlets/purl/10107856">Characterizing implicit bias in terms of optimization geometry.</a> <em>International Conference on Machine Learning</em>, 2018.<br />[12] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity.</a> Foundations and Trends in Machine Learning, 8(3-4):231-357, 2015.<br />[13] Yuri Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions.</a> <em>Mathematical programming</em>, 103(1):127-152, 2005.<br />[14] Anrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function.</a> <em>IEEE Transactions on Information theory. </em>39(3), 930-945, 1993.<br />[15] Jacot, Arthur, Franck Gabriel, Clément Hongler. <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Neural tangent kernel: Convergence and generalization in neural networks.</a> <em>Advances in neural information processing systems.</em> 8571-8580, 2018.<br />[16] Lénaïc Chizat, Édouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On lazy training in differentiable programming.</a> <em>Advances in Neural Information Processing Systems.</em> 2937-2947, 2019.<br />[17] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari. <a href="https://arxiv.org/pdf/1904.12191.pdf">Linearized two-layers neural networks in high dimension.</a> To appear in <em>Annals of Statistics</em>. 2019.<br />[18] Alberto Bietti, Julien Mairal. <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">On the Inductive Bias of Neural Tangent Kernels</a>. <em>Advances in Neural Information Processing Systems.</em> p. 12893-12904, 2019.<br />[19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off.</a> <em>Proceedings of the National Academy of Sciences.</em> <em>116</em>(32), 15849-15854, 2019.</p>



<h3>Lower bound on the gradient norm for linear classification with the exponential loss</h3>



<p class="justify-text">In the context of Section 2, we want to prove that \(\Vert \nabla F(a)\Vert_2\geq \gamma\). For this, let \(Z\in \mathbb{R}^{n\times d}\) be the matrix with rows \(y_i x_i\) and let \(\Delta_n\) be the simplex in \(\mathbb{R}^n\). We have by duality $$ \gamma = \max_{\Vert a\Vert_2\leq 1}\min_{p\in \Delta_n} p^\top Z a =   \min_{p\in \Delta_n} \max_{\Vert a\Vert_2\leq 1} a^\top Z^\top p = \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 .$$  Also, notice that \(\nabla F(a) = Z^\top p\) with \(p_i = \frac{e^{-y_ix_i^\top a}}{\sum_{j=1}^n e^{-y_{j}x_{j}^\top a}}\). Since \(p \in \Delta_n\), we conclude that \(\Vert \nabla F(a)\Vert_2\geq \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 = \gamma\).</p></div>







<p class="date">
by Lénaïc Chizat <a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/"><span class="datestr">at July 13, 2020 07:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7770">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/">Simons institute lectures on analysis of Boolean functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(Does it still make sense to blog such announcements or is these days <a href="https://twitter.com/boazbaraktcs/status/1282443765224017920">Twitter</a> the only way to go about this? Asking for a friend <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> )</em></p>



<p>Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. </p>



<p><a href="https://simons.berkeley.edu/events/boolean" target="_blank" rel="noreferrer noopener">Lecture Series: Advances in Boolean Function Analysis</a><br /></p>



<p>The Simons Institute is organizing a series of lectures on Advances in Boolean Function Analysis, that will highlight a few major developments in the area. The series will feature weekly two-hour lectures from July 15th to Aug 18th.  The lectures aim to address both the broad context of the results and their technical details. Though closely related in theme, each lecture will be self-contained.  The schedule is attached below (more info at <a href="https://simons.berkeley.edu/events/boolean" target="_blank" rel="noreferrer noopener">link</a>). </p>



<p>Talks take place on Wednesdays at 10am Pacific time (1pm Eastern). If you can’t catch them live, they will be redcorded.</p>



<p><strong>Zoom Link: </strong><a href="https://berkeley.zoom.us/j/93086371156" target="_blank" rel="noreferrer noopener">https://berkeley.zoom.us/j/93086371156</a><br /></p>



<p><strong><u>Talk Schedule:</u></strong></p>



<p>July 15, Wednesday  10:00am PDT (1pm EDT) <em>Dor Minzer (Institute of Advanced Study)<a href="https://simons.berkeley.edu/events/boolean-1" target="_blank" rel="noreferrer noopener">On the Fourier-Entropy Influence Conjecture</a></em></p>



<p><br />July 22, Wednesday, 10:00am PDT (1pm EDT) <em>Hao Huang (Emory University) &amp; Avishay Tal (UC Berkeley)<a href="https://simons.berkeley.edu/events/boolean-3" target="_blank" rel="noreferrer noopener">Sensitivity Conjecture and Its Applications</a></em></p>



<p><br />August 3rd, Monday, 10:00am PDT (1pm EDT)<em>  Shachar Lovett (UC San Diego)<a href="https://simons.berkeley.edu/events/boolean-2" target="_blank" rel="noreferrer noopener">Improved Bounds for the Sunflower Lemma</a></em></p>



<p><br />August 5, Wednesday, 10:00am PDT (1pm EDT) <em>Ronen Eldan (Weizmann Institute)</em><a href="https://simons.berkeley.edu/events/boolean-4" target="_blank" rel="noreferrer noopener"><em>Concentration on the Boolean Hypercube via Pathwise Stochastic Analysis</em></a></p>



<p><br />August 12, Wednesday, 10:00am <em>Esty Kelman (Tel Aviv University) <a href="https://simons.berkeley.edu/events/boolean-5" target="_blank" rel="noreferrer noopener">KKL via Random Restrictions</a></em></p>



<p><br />August 18, Tuesday, 10:00am <em>Pooya Hatami (Ohio State University)<a href="https://simons.berkeley.edu/events/boolean-6" target="_blank" rel="noreferrer noopener">Pseudorandom Generators from Polarizing Random Walks</a></em></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/"><span class="datestr">at July 13, 2020 04:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/">ADFOCS 2020 (Market Design and Computational Fair Division)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Via Pieter Kleer:</p>
<hr />
<p>21st Max Planck Summer School:<br />
Advanced Course on the Foundations of Computer Science (ADFOCS 2020)</p>
<p>August 24 – 28, 2020</p>
<p>Saarbruecken, Germany</p>
<p>THIS IS A VIRTUAL EVENT<a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer"></a></p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">
</a><p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">http://www.mpi-inf.mpg.de/conference/adfocs</a><br />
—————————————————————————————————</p>
<p><b>About ADFOCS</b><br />
ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute for Informatics (MPII) in Saarbruecken, Germany. It is organized as part of the activities of the MPII, in particular the International Max Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school is to introduce young researchers to topics which are the focus of current research in theoretical computer science. We bring together leading researchers in the field and international participants at the graduate level and above. This year’s focus is on:</p>
<p><b>*** Market Design and Computational Fair Division ***</b><br />
<b>Program</b><br />
Our invited speakers give five 60-min lectures with subsequent exercise and discussion sessions. These sessions will take place daily from 14:30 to 18:30 UTC+2 (CEST) in the week of August 24-28. On some days there will be a social event after the regular schedule. This year’s speakers are:</p>
<p>* Nicole Immorlica, Microsoft Research Lab, New York City, USA<br />
* Jugal Garg and Ruta Mehta, University of Illinois at Urbana-Champaign, USA<br />
<b>Registration</b><br />
This year registration is free as the event takes place virtually. Nevertheless, registration is MANDATORY and can be done through the website (at the latest August 10)</p>
<p><b>Contact</b><br />
The homepage of ADFOCS, including forms for registration, can be found at <a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">http://www.mpi-inf.mpg.de/conference/adfocs</a></p>
<p>If you have further questions, please do not hesitate to contact the ADFOCS team by sending an email to <a href="mailto:adfocs@mpi-inf.mpg.de" target="_blank" rel="noopener">adfocs@mpi-inf.mpg.de</a></p>
<p>Organizers: Cosmina Croitoru, Sandor Kisfaludi-Bak and Pieter Kleer</p></div>







<p class="date">
by timroughgarden <a href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/"><span class="datestr">at July 13, 2020 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-127536327837647663">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html">Ronald Graham: A summary of blog Posts We had about his work</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
To Honor Ronald Graham I summarize the blog posts we had about his work.<br />
<br />
1) Blog post <a href="https://blog.computationalcomplexity.org/2016/05/new-ramsey-result-that-will-be-hard-to.html">New Ramsey Result that will be hard to verify but Ronald Graham thinks its right which is good enough for me</a>.<br />
<br />
Wikipedia (see <a href="https://en.wikipedia.org/wiki/Boolean_Pythagorean_triples_problem">here</a>) says that in the early 1980's (can't Wikipedia be more precise than that?) Ronald Graham conjectured the following:<br />
<br />
For all 2-colorings of N, there exists x,y,z all the same color such that (x,y,z) form a Pythagorean triple.<br />
<br />
I cannot imagine he did not also conjecture this to be true for all finite colorings.<br />
<br />
I suspect that when he conjectured it, the outcomes thought to be likely were:<br />
<br />
a) A purely combinatorial (my spell check says that combinatorial  is not a word. Really? It gets 14,000,000 hits) proof. Perhaps a difficult one. (I think Szemeredi's proof of his density theorem is a rather difficult but purely combinatorial proof).<br />
<br />
b) A proof that uses advanced mathematics, like Roth's proof of the k=3 case of Sz-density, or Furstenberg's proof of Sz theorem.<br />
<br />
c) The question stays open though with some progress over the years, like R(5).<br />
<br />
What actually happened was<br />
<br />
d) A SAT Solver solves it AND gets exact bounds:<br />
<br />
For all 2-colorings of {1,...,7285} there is a mono Pythag triple.<br />
<br />
There exists a 2-coloring of {1,...,7284} with no mono Pythag triple.<br />
<br />
I wonder if this would have been guessed as the outcome back in the early 1980's.<br />
<br />
-------------------------------------------------------------------------------<br />
2) Blog Post <a href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html">Ronald Graham's Other Large Number- well it was large in 1964 anyway</a><br />
<br />
Let<br />
<br />
a(n) = a(n-1) + a(n-2)<br />
<br />
I have not given a(0) and a(1). Does there exists rel prime values of a(0) and a(1) such that for all n, a(n) is composite.<br />
<br />
In 1964 Ronald Graham showed yes, though the numbers he found (with the help of 1964-style computing) were<br />
<br />
a(0) = 1786772701928802632268715130455793<br />
<br />
a(1) = 2059683225053915111058164141686995<br />
<br />
I suspect it is open to get smaller numbers, though I do not know.<br />
<br />
<br />
------------------------------------------------------------------------------<br />
3) Blog Post <a href="https://blog.computationalcomplexity.org/2011/12/solution-to-reciprocals-problem.html">Solution to the reciprocals problem</a><br />
<br />
Prove or disprove that there exists 10 natural numbers a,...,j such that<br />
<br />
2011= a+ ... + j<br />
1 = 1/a + ... + 1/j<br />
<br />
I had pondered putting this on a HS math competition in 2011; however, the committee thought it was too hard. I blogged on the problem asking for solutions, seeing if there was one that a HS student could have gotten. The following post (this one) gave those solutions. My conclusion is that it could have been put on the competition, but its a close call.<br />
<br />
All of the answers submitted had some number repeated.<br />
<br />
So I wondered if there was a way to do this with distinct a,...,j.<br />
<br />
 I was told about Ronald Grahams result:<br />
<br />
For all n at least 78, n can be written as the sum of DISTINCT naturals, where the sum of<br />
the reciprocals is 1.<br />
<br />
This is tight: 77 cannot be so written.<br />
<br />
Comment on that blog DID include solutions  to my original problem with all distinct numbers<br />
<br />
----------------------------------------------------------------------<br />
4) Blog Post <a href="https://blog.computationalcomplexity.org/2013/04/a-nice-case-of-interdisciplinary.html">A nice case of interdisplanary research</a> tells the story of how the study of history lead to R(5) being determined (see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/ramseykings.pdf">here</a> for the actual paper on the subject). One of the main players in the story is the mathematician<br />
<br />
Alma Grand-Rho.<br />
<br />
Note that this is an anagram of<br />
<br />
Ronald Graham.<br />
<br />
What is the probability that two mathematicians have names that are anagrams. I suspect very small. However, see <a href="https://blog.computationalcomplexity.org/2013/04/post-mortem-on-april-fools-day-joke.html">this</a> blog post to see why the probability is not as small as it might be.<br />
<br />
-----------------------------------------------------------------------<br />
5) Blog Post <a href="https://blog.computationalcomplexity.org/search?q=Meme">Winner of Ramsey Meme Contest</a> This post didn't mention Ronald Graham; however I think he would have liked it.<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html"><span class="datestr">at July 12, 2020 08:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html">Graham–Pollak partitions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up <a href="https://en.wikipedia.org/wiki/Ronald_Graham">his Wikipedia article</a> after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in <a href="https://www.ics.uci.edu/~eppstein/pix/bellairs18/index.html">2018 in Barbados</a>, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on <a href="https://en.wikipedia.org/wiki/Graham%E2%80%93Pollak_theorem">the Graham–Pollak theorem</a>, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least  subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</p>

<p>In <em>Proofs from THE BOOK</em>, Aigner and Ziegler describe a simple construction for an -subgraph partition: just order the vertices of the complete graph, and make a star connecting each vertex (except the last) to its later neighbors.
But there are a lot more partitions than that. For instance, you can take any rooted binary tree whose leaves are the vertices of the complete graph, and form a partition in which each complete bipartite subgraph connects the left and right descendants of one of the interior nodes of the tree. The ordered star partition is the special case of this where each internal node has one leaf child.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/graham-pollak-hierarchy.svg" alt="Graham–Pollak partitions from binary trees" /></p>

<p>Even these are not the only possibilities. For instance, a four-vertex complete graph can be partitioned into  subgraphs in this triskelion pattern:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/graham-pollak-triskelion.svg" alt="Graham–Pollak partitions from binary trees" /></p>

<p>More generally, whenever one has a partition of , one can form a partition of a larger complete graph by partitioning its vertices into  subsets, applying the partition of  to the edges that go from one subset to another, and then recursively partitioning the edges within each subset. This is already enough to show that there is a rapidly growing number of these partitions, but not enough to count them more precisely.</p>

<p>This still leaves many questions. How many Graham–Pollak partitions does  have, as a function of ? How complicated can they be? If we define a state space whose states are Graham–Pollak partitions, and whose state transitions correspond to re-partitioning the subgraph formed by two of the complete bipartite graphs, is it connected? Can a graph traversal of this state space list all the Graham–Pollak partitions faster than a brute force search? What does a random partition look like?</p>

<p>It’s too bad Ron’s no longer around to help answer some of them.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104503441875881282">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html"><span class="datestr">at July 12, 2020 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/104">TR20-104 |  On Counting $t$-Cliques Mod 2 | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For a constant integer $t$, we consider the problem of counting the number of $t$-cliques $\bmod 2$ in a given graph. 
We show that this problem is not easier than determining whether a given graph contains a $t$-clique, and present a simple worst-case to average-case reduction for it. The reduction runs in linear time when graphs are presented by their adjacency matrices, and average-case is with respect to the uniform distribution over graphs with a given number of vertices.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/104"><span class="datestr">at July 12, 2020 03:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/103">TR20-103 |  One-Tape Turing Machine and Branching Program Lower Bounds for MCSP | 

	Mahdi Cheraghchi, 

	Shuichi Hirahara, 

	Dimitrios Myrisiotis, 

	Yuichi Yoshida</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For a size parameter $s\colon\mathbb{N}\to\mathbb{N}$, the Minimum Circuit Size Problem (denoted by ${\rm MCSP}[s(n)]$) is the problem of deciding whether the minimum circuit size of a given function $f \colon \{0,1\}^n \to \{0,1\}$ (represented by a string of length $N := 2^n$) is at most a threshold $s(n)$. A recent line of work exhibited ``hardness magnification'' phenomena for MCSP: A very weak lower bound for MCSP implies a breakthrough result in complexity theory. For example, McKay, Murray, and Williams (STOC 2019) implicitly showed that, for some constant $\mu_1 &gt; 0$, if ${\rm MCSP}[2^{\mu_1\cdot n}]$ cannot be computed by a one-tape Turing machine (with an additional one-way read-only input tape) running in time $N^{1.01}$, then ${\rm P}\neq{\rm NP}$.
    
    In this paper, we present the following new lower bounds against one-tape Turing machines and branching programs:
    \begin{enumerate}
        \item  A randomized two-sided error one-tape Turing machine (with an additional one-way read-only input tape) cannot compute ${\rm MCSP}[2^{\mu_2\cdot n}]$ in time $N^{1.99}$, for some constant $\mu_2 &gt; \mu_1$.  
        \item A non-deterministic (or parity) branching program of size $o(N^{1.5}/\log N)$ cannot compute MKTP, which is a time-bounded Kolmogorov complexity analogue of MCSP. This is shown by directly applying the Nechiporuk method to MKTP, which previously appeared to be difficult.
    \end{enumerate}
    These results are the first non-trivial lower bounds for MCSP and MKTP against one-tape Turing machines and non-deterministic branching programs, and essentially match the best-known lower bounds for any explicit functions against these computational models.
    
    The first result is based on recent constructions of pseudorandom generators for read-once oblivious branching programs (ROBPs) and combinatorial rectangles (Forbes and Kelley, FOCS 2018; Viola 2019). En route, we obtain several related results:
    \begin{enumerate}
        \item There exists a (local) hitting set generator with seed length $\widetilde{O}(\sqrt{N})$ secure against read-once polynomial-size non-deterministic branching programs on $N$-bit inputs.
        \item Any read-once co-non-deterministic branching program computing MCSP must have size at least $2^{\widetilde{\Omega}(N)}$.
    \end{enumerate}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/103"><span class="datestr">at July 11, 2020 05:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 27-31, 2020 Telluride CO (virtual) https://sites.google.com/view/telluride2020/home We are happy to announce a Virtual Telluride Neuromorphic Cognition Engineering Workshop 2020 (https://tellurideneuromorphic.org/) this year in replacement of our usual Workshop in Telluride. The workshop will take place from July 27 to July 31 (8am to 10am PDT, or 17:00 to 19:00 CET). The format will be … <a href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/" class="more-link">Continue reading <span class="screen-reader-text">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/"><span class="datestr">at July 11, 2020 05:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/">International Conference on Neuromorphic Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 28-30, 2020 Oak Ridge National Laboratory (virtual) https://icons.ornl.gov ICONS 2020 will be held as a virtual conference. The goal of this conference is to bring together leading researchers in neuromorphic computing to present new research, develop new collaborations, and provide a forum to publish work in this area. Our focus will be on architectures, … <a href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/" class="more-link">Continue reading <span class="screen-reader-text">International Conference on Neuromorphic Systems</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/"><span class="datestr">at July 11, 2020 05:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/">Neuromorphic Computing: Opportunities, Challenges, and Perspectives</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 19, 2020 Virtual https://teuscher-lab.com/dac2020_neuromorphic_workshop/program/ The objective of this workshop is to bring together researchers from multiple disciplines, ranging from physical to biological sciences, to discuss the most promising approaches and overarching goals of neuromorphic computing technologies and paradigms that have the potential to drastically improve conventional approaches. The neuromorphic computing workshop aims to establish … <a href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/" class="more-link">Continue reading <span class="screen-reader-text">Neuromorphic Computing: Opportunities, Challenges, and Perspectives</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/"><span class="datestr">at July 11, 2020 05:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/">Encrypted Blockchain Databases (Part II)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this second part of the series on Encrypted Blockchain Databases, we are going to describe three schemes to store dynamic encrypted multi-maps (EMMs) on blockchains, each of which achieves different tradeoffs between query, add and delete efficiency. A List-Based Scheme (LSX) Recall that a multi-map is a collection of...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/"><span class="datestr">at July 10, 2020 08:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/">Encrypted Blockchain Databases (Part I)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Blockchain databases are storage systems that combine properties of both blockchains and databases like decentralization, tamper-resistance, low query latency, and support for complex queries. As they gain wider adoption, concerns over the confidentiality of the data they manage will increase. Already, several projects use blockchains to store sensitive data like...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/"><span class="datestr">at July 10, 2020 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7764">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/07/10/tcs-book-call-for-github-issues/">TCS book: Call for GitHub issues</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I originally planned this summer to finish the work on my <a href="https://introtcs.org/">Introduction to Theoretical Computer Science</a> book, and in particular write the two missing chapters on space complexity and interactive proof systems. Needless to say, this summer did not go as planned and I won’t be able to write these chapters. However, I still intend to go over the existing chapters, fixing typos, adding examples, exercises, and generally making it friendlier to beginning undergraduate students. </p>



<p>Toward this end, I would be grateful for people posting bugs, typos, and suggestions as <a href="https://github.com/boazbk/tcs/issues">GitHub issues</a> (I currently have 267 closed and 14 open issues which I hope to get to soon). Of course, if you are technically inclined and there’s a simple local fix, you can also make  a <a href="https://github.com/boazbk/tcs/pulls">pull request</a>.</p>



<p>Aside from these fixes, I am making two more “global” changes to the book. First, I am adding a “non mathy overview” for each chapter. While some students got a lot from reading the book prior to lectures, others were intimidated by the mathematical notation, and so I hope this more gentle introduction will be helpful. I am also adding more examples &amp; solved exercises toward this end. </p>



<p>Another change is that I now follow the more traditional way of presenting deterministic finite automata <em>before </em>Turing machines – DFAs are still optional and can be skipped without missing anything, but some instructors find them as a good introduction to Turing Machines. Thus the order of presentation of materials in the book is roughly as follows:<br /></p>



<ol><li><strong>Introduction, representing objects as strings</strong> –  Representing numbers, lists, etc. Specifying computational tasks as functions mapping binary strings to binary strings,  Cantor’s theorem.</li><li><strong>Finite functions and Boolean circuits</strong> – Every function can be computed by some circuit, circuits as straightline programs, representing circuits as strings, universal circuit evaluator, counting lower bound.</li><li><strong>Computing on unbounded inputs</strong> – DFAs (optional), Turing Machines, equivalence between Turing machines, RAM machines and programming languages, λ calculus (optional), cellular automata (optional)</li><li><strong>Uncomputability</strong> – Universal Turing machine, Halting problem, reductions, Rice’s Theorem. Optional: Gödel’s incompleteness theorem, uncomputability of quantified arithmetic statements, context free grammars.</li><li><strong>Efficient computation</strong> – Modeling running time, time hierarchy theorem,  <strong>P</strong> and <strong>EXP</strong></li><li><strong>NP and NP completeness</strong> – Polynomial-time reductions, Cook-Levin Theorem (using circuits), definition of <strong>NP</strong> using “proof system”/”verifying algorithms” (no non-deterministic TMs), <strong>NP</strong> completeness, consequences of <strong>P</strong>=<strong>NP</strong>: search to decision, optimal machine learning, etc..</li><li><strong>Randomized computation:</strong> Worst-case randomized computation, defining <strong>BPP</strong>,  Sipser-Gács, does <strong>BPP</strong>=<strong>P</strong>? (a little on derandomization)</li><li><strong>Cryptography:</strong> One time pad, necessity of long keys for information theoretic crypto,  pseudorandom generators and stream ciphers, taste of public key and “magic” (ZKP, FHE, MPC)</li><li><strong>Quantum computing:</strong> Some quantum mechanics background – double slit experiment,  Bell’s inequality. Modeling quantum computation. Bird’s eye view of Shor’s algorithm and quantum Fourier transform.</li></ol>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/07/10/tcs-book-call-for-github-issues/"><span class="datestr">at July 10, 2020 05:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17276">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/">Ron Graham, 1935–2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Ron Graham passed away, but he lives on…</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg"><img width="175" alt="" src="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg?w=175&amp;h=128" class="alignright wp-image-17278" height="128" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://securityboulevard.com/2020/07/ronald-graham-and-the-magic-of-math/">tribute</a> by Tom Leighton</font></td>
</tr>
</tbody>
</table>
<p>
Ron Graham just passed away Monday at the age of <img src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{84}" class="latex" title="{84}" /> in La Jolla near UCSD. </p>
<p>
Today Ken and I wish to say a few words about Ron.</p>
<p>
Tributes are being written as we write, including <a href="https://www.simonsfoundation.org/2016/01/11/ronald-graham/">this</a> from the Simons Foundation. Here is the American Mathematical Society <a href="https://www.ams.org/news?news_id=6244">announcement</a>, which we saw first: </p>
<blockquote><p><b> </b> <em> Ron Graham, a leader in discrete mathematics and a former president of both the AMS (1993-1994) and the MAA (2003-2004), died on July 6. He was 84. Graham published more than 350 papers and books with many collaborators, including more than 90 with his wife, Fan Chung, and more than 30 with Paul Erdős. He was known for his infectious enthusiasm, his originality, and his accessibility to anyone who had a mathematics question. </em>
</p></blockquote>
<p></p><p>
A <a href="https://www.bradyharanblog.com/blog/the-day-i-met-ron-graham">tribute</a> by Brady Haran embeds several short videos of Ron and his work. Fan’s own <a href="http://www.math.ucsd.edu/~fan/ron/">page</a> for Ron has much more. We have made a collage of images from his life:</p>
<p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg"><img src="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg?w=600" alt="" class="aligncenter size-full wp-image-17279" /></a></p>
<p></p><p><br />
Ron was special and will be greatly missed by all. We at GLL send our thoughts to his dear wife, Fan. Ken and I knew Ron for many years. Ken knew Ron since a visit to Bell Labs in the 1980s and meeting Fan too at STOC 1990. I knew Ron since I was at Yale in the 1970’s—a long time ago. I recall fondly meeting him for the first time when he was at Bell Labs.</p>
<p>
</p><p></p><h2> Some Stories </h2><p></p>
<p></p><p>
Ken and I thought we would give some personal stories about Graham. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> Ken’s story is told <a href="https://rjlipton.wordpress.com/2013/03/28/happy-100th-birthday-paul-erdos/">here</a>. In breaking a confidence by telling Erdős the secret about Bobby Fischer recounted there, Ken hoped that it would spread behind the scenes to enough people that Fischer would be less blamed for failing to play Anatoly Karpov in 1975. Since Erdős was staying with the Grahams, presumably it would have emerged there. The social excursion during STOC 1990 was a dinner cruise in Baltimore’s harbor. Ron and Fan and Ken found each other right away, and some questions to Ken about chess quickly went to the Fischer topic. At least Ken knows the secret was retold at least once. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> Ron told me once that he was the accountant for Erdős. One of Ron’s jobs was to keep track of the prize money that Erdős owed. Ron would send out the checks to whoever solved the next problem. One of the brilliant insights of Erdős was to make the problems hard, but at least some where solvable. Ron told me that for years no one would actually cash the checks. They would frame them and proudly display them.</p>
<p></p><p>
<a href="https://rjlipton.files.wordpress.com/2020/07/check.png"><img width="220" alt="" src="https://rjlipton.files.wordpress.com/2020/07/check.png?w=220&amp;h=102" class="aligncenter wp-image-17280" height="102" /></a></p>
<p></p><p><br />
Ron said that he liked this for the obvious reason—less cash for Erdős to have to pay. But the advent of color xerox machines in the 1970’s changed this. He told me that people began cashing the checks and displaying the color copy. Bummer.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> My first talk at Bell Labs was on my work on the planar separator theorem—joint work with Bob Tarjan. At the beginning of the talk I saw that Ron had a pile of papers on his desk. He was a manager and I guessed he had some paper work to do. I gave my talk. At the end I when up to Ron in the back and he said:</p>
<blockquote><p><b> </b> <em> I did not get any work done. </em>
</p></blockquote>
<p></p><p>
I still fondly remember that as high praise. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> Graham loved to do hand stands. I recall walking around Bell Labs one day when out of the blue Ron did a full handstand. He said that he liked to do these on the hand rail of the stairs. The trick he said was: “To not fall down.” </p>
<p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2020/07/well.jpg"><img src="https://rjlipton.files.wordpress.com/2020/07/well.jpg?w=600" alt="" class="aligncenter size-full wp-image-17281" /></a></p>
<p></p><p><br />
I searched for him doing handstands and found out he and Fan lived in a modern beautiful <a href="http://www.math.ucsd.edu/~fan/home/">house</a>. </p>
<blockquote><p><b> </b> <em> When two mathematicians found a circular home designed by architect Kendrick Bangs Kellogg in La Jolla, they treasured their unique discovery. </em>
</p></blockquote>
<p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2020/07/home1.jpg"><img src="https://rjlipton.files.wordpress.com/2020/07/home1.jpg?w=600" alt="" class="aligncenter size-full wp-image-17283" /></a></p>
<p>
</p><p></p><h2> Fun and Games </h2><p></p>
<p></p><p>
Ron kept a simply organized <a href="http://www.math.ucsd.edu/~ronspubs/">page</a> of all his papers. They are not sorted by subject or kind, but the titles are so descriptive that you can tell at a glance where the fun is. A number of them are expositions in the popular magazines of the AMS and MAA. </p>
<p>
Among them, we’ll mention this <a href="http://www.math.ucsd.edu/~ronspubs/16_02_insert_and_add.pdf">note</a> from 2016, titled “Inserting Plus Signs and Adding.” It is joint with Steve Butler, who penned his own <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">reminiscence</a> for Lance and Bill’s blog, and Richard Strong. </p>
<p>
Say that a number <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is “reducible” to a number <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> in one step (in base <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />) if there is a way to insert one or more <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" /> signs into the base-<img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> representation of <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> so that the resulting numbers add up to <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" />. For example, 1935 is reducible to 99 via <img src="https://s0.wp.com/latex.php?latex=%7B1+%2B+93+%2B+5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 + 93 + 5}" class="latex" title="{1 + 93 + 5}" />. The number 99 reduces only to 18 via <img src="https://s0.wp.com/latex.php?latex=%7B9%2B9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{9+9}" class="latex" title="{9+9}" />, and 18 reduces only to 9, which cannot be reduced further. Thus Ron’s birth year took <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> reduction steps to become a single digit. However, doing <img src="https://s0.wp.com/latex.php?latex=%7B1%2B9%2B3%2B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1+9+3+5}" class="latex" title="{1+9+3+5}" /> gives 18 straightaway and thus saves a step. The paper gives cases where inserting <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" /> everywhere is <em>not</em> a quickest way to reduce to a single digit.</p>
<blockquote><p><b>Definition 1</b> <em> For any base <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> and number <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \geq 1}" class="latex" title="{n \geq 1}" /> denoting an input <b>length</b>, not magnitude, define <img src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_b(n)}" class="latex" title="{f_b(n)}" /> to be the least integer <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> such that all base-<img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> numbers of length <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> can be reduced to a single digit within <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> steps. </em>
</p></blockquote>
<p></p><p>
The question—of a complexity theoretic nature—is:</p>
<blockquote><p><b> </b> <em> Given <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />, what is the growth rate of <img src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_b(n)}" class="latex" title="{f_b(n)}" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \rightarrow \infty}" class="latex" title="{n \rightarrow \infty}" />? </em>
</p></blockquote>
<p></p><p>
Here are some possible answers—which would you expect to be correct in the case where <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> is base 10?</p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{10}(n) = \Theta(\sqrt{n})}" class="latex" title="{f_{10}(n) = \Theta(\sqrt{n})}" />. <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28n%5E%7B1%2F10%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{10}(n) = \Theta(n^{1/10})}" class="latex" title="{f_{10}(n) = \Theta(n^{1/10})}" />. <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{10}(n) = O(\log n)}" class="latex" title="{f_{10}(n) = O(\log n)}" />. <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{10}(n) = O(\log\log n)}" class="latex" title="{f_{10}(n) = O(\log\log n)}" />. <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Calpha%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{10}(n) = O(\alpha(n))}" class="latex" title="{f_{10}(n) = O(\alpha(n))}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" /> is the inverse Ackermann <a href="https://en.wikipedia.org/wiki/Ackermann_function#Inverse">function</a>.
</li></ul>
<p>
Your expectation might be wrong—see the paper for the answer and its nifty proof. For a warmup, if you want to answer without looking at the paper, prove that the final reduced digit is the same regardless of the sequence of reductions.</p>
<p>
Ron is also known for very big integers, including <a href="https://en.wikipedia.org/wiki/Graham's_number">one</a> that held the record for largest to appear in a published mathematical proof. You can find it among the above tributes and also on a <a href="https://www.zazzle.com/store/grahamsnumber">T-shirt</a>.  We could also mention his role in the largest <a href="https://news.slashdot.org/story/16/05/30/2241225/computer-generates-largest-math-proof-ever-at-200tb-of-data">proof</a> known to date—at 200 terabytes it almost doubles the size of the <a href="http://tb7.chessok.com/">tables</a> for proving results of seven-piece chess endgames.</p>
<p>
If you desire serious fun, look also to Ron’s books. He wrote several, including co-authoring the nonpareil <a href="https://en.wikipedia.org/wiki/Concrete_Mathematics">textbook</a> <em>Concrete Mathematics</em> with Don Knuth and Oren Patashnik.</p>
<p>
</p><p></p><h2> Some Prizes </h2><p></p>
<p></p><p>
Ron, in the tradition famously followed by Erdős, liked to put <a href="https://www.quantamagazine.org/cash-for-math-the-erdos-prizes-live-on-20170605/">money</a> on problems. A $10 dollar problem was much easier than a $100 one. A $1,000 one is extremely hard, and so on. In Ron’s paper on his favorite <a href="http://www.math.ucsd.edu/~ronspubs/20_02_favorite.pdf">problems</a> he stated this one: </p>
<blockquote><p><b> </b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BH_%7Bn%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+%5Cfrac%7B1%7D%7Bj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}" class="latex" title="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}" />. Challenge: prove the inequality for all <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \ge 1}" class="latex" title="{n \ge 1}" />, </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bd+%7C+n%7D+d+%5Cle+H_%7Bn%7D+%2B+%5Cexp%28H_%7Bn%7D%29%5Clog%28H_%7Bn%7D%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). " class="latex" title="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). " /></p>
</em><p><em>	 </em>
</p></blockquote>
<p></p><p>
And he put the prize at $1,000,000. He added:</p>
<blockquote><p><b> </b> <em></em></p><em>
</em><p><em>
Why is this reward so outrageous? Because this <a href="https://arxiv.org/pdf/math/0008177.pdf">conjecture</a> is equivalent to the Riemann Hypothesis! A single <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> violating would imply there are infinitely many zeroes of the Riemann zeta function off the critical line <img src="https://s0.wp.com/latex.php?latex=%7BR%28z%29+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{R(z) = 1}" class="latex" title="{R(z) = 1}" />. Of course, the $1,000,000 prize is not from me but rather is offered by the Clay Mathematics Institute since the Riemann Hypothesis is one of their six remaining Millennium Prize Problems. We hope to live to see progress in the Challenges and Conjectures mentioned in this note, especially the last one! </em>
</p></blockquote>
<p></p><p>
Alas Ron did not get to see this resolved. Nor of course did Erdős, nor may any of us. But Ron is prominently mentioned on another Simons <a href="https://www.simonsfoundation.org/2015/12/10/new-erdos-paper-solves-egyptian-fraction-problem/">page</a> where Erdős lives on, and so may Ron.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Ron died at age <img src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{84}" class="latex" title="{84}" />. Perhaps he liked that it is the sum of a twin prime <img src="https://s0.wp.com/latex.php?latex=%7B41+%2B+43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{41 + 43}" class="latex" title="{41 + 43}" />, and also three times a perfect number. We will always remember <img src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{84}" class="latex" title="{84}" /> because of Ron.  <b>Added 7/10:</b> <img src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{84}" class="latex" title="{84}" /> is also his current h-index <a href="https://scholar.google.com/citations?user=qrPaF3QAAAAJ&amp;hl=en&amp;oi=sra&amp;fbclid=IwAR2Tx1GkRQ6-6K-hlumvpBqWUku2Msea6_dybwrYK8tVeNUuYOD6czZ24ZY">according to</a> Google Scholar.  HT in <a href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/#comment-111482">comment</a>.</p>
<p>
[some word changes, update about h-index]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/"><span class="datestr">at July 10, 2020 04:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/102">TR20-102 |  Notes on Hazard-Free Circuits | 

	Stasys Jukna</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The problem of constructing hazard-free Boolean circuits (those avoiding electronic glitches) dates back to the 1940s and is an important problem in circuit design. Recently, Ikenmeyer et al. [J. ACM, 66:4 (2019), Article 25] have shown that the hazard-free circuit complexity of any Boolean function $f(x)$ is lower-bounded by the monotone circuit complexity of the monotone Boolean function which accepts an input $x$ iff $f(z)=1$ for some vector $z\leq x$. We give a short and amazingly simple proof of this interesting result. We also show that a circuit is hazard-free if and only if the circuit and its dual produce (purely syntactically) all prime implicants of the functions they compute. This extends a classical result of Eichelberger [IBM J. Res. Develop., 9 (1965)] showing this property for depth-two circuits producing no terms containing a variable together with its negation. Finally, we give a very simple non-monotone Boolean function whose hazard-free circuit complexity is super-polynomially larger than its unrestricted circuit complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/102"><span class="datestr">at July 09, 2020 07:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-713901807945793095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">Reflections on Ronald Graham by Steve Butler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>
<i>Ronald Graham passed away on July 6 at the age of 84. We present reflections on Ronald Graham by </i><i>Steve Butler.</i></div>
<div>
<i><br /></i></div>
<hr />
<div>
<br /></div>
<div>
Getting to work with Ron Graham</div>
<div>
<br /></div>
<div>
Ron Graham has helped transform the mathematics community and in particular been a leader in discrete mathematics for more than 50 years. It is impossible to fully appreciate the breadth of his work in one sitting, and I will not try to do so here. Ron has put his papers online and made them <a href="http://www.math.ucsd.edu/~ronspubs/">freely available</a>, a valuable treasure; and there are still many a hidden gem inside of these papers that are waiting to be picked up, polished, and pushed further.</div>
<div>
<br /></div>
<div>
I want to share about how I got to know and work with Ron. To be fair I knew about Ron long before I ever knew Ron. He was that rare pop-star mathematician who had managed to reach out and become visible outside of the mathematical community. And so as a teenager I read about Ron in a book about Erdos. I thought to myself that this guy sounds really cool and someday I might even get to see him give a talk (if I was lucky).</div>
<div>
<br /></div>
<div>
I went to UC San Diego for graduate school and after a series of near-misses ended up studying under Fan Chung. I passed Ron in the stairwell once, and then also helped them move some furniture between their two adjoining homes (graduate students are great for manual labor). But I became determined to try and find a way to start a conversation with Ron and maybe work up to working on a problem. So I took the usual route: I erased the chalkboards for him.</div>
<div>
<br /></div>
<div>
Before his class on discrete mathematics would start, I would come in and clean the chalkboards making them pristine. It also gave me time to occasionally engage in some idle chat, and he mentioned that his papers list was far from complete. I jumped on it and got to work right away and put his papers online and have been maintaining that list for the last fifteen years. This turned out to be no small feat and required about six months of work.  Many papers had no previous online version, and there were even a few papers that Ron had written that he had forgotten about! But this gave me a reason to come to Ron and talk with him about his various papers and then he would mention some problems he was working on with others and where they were stuck and thought I might give them a try.</div>
<div>
<br /></div>
<div>
So I started to work on these problems and started to make progress. And Ron saw what I was able to do and would send me more problems that fit my abilities and interests, and I would come back and show him partial solutions, or computations, and then he would often times fill in the gaps. He was fun to work with, because we almost always made progress; even when we didn't make progress we still understood things more fully. Little by little our publications (and friendship) grew and we now have 25+ joint publications, and one more book that will be coming out in the next few years about the enumerating juggling patterns.</div>
<div>
<br /></div>
<div>
After all of that though, I discovered something. I could have just gone to Ron's door and knocked and he would have talked to me, and given me problems (though our friendship would not become so deep if I had chosen the forthright method). But almost no graduate students in math were brave enough to do it; they were scared off by his reputation. As a consequence, Ron had far fewer math graduate students than you would expect. (To any math graduate student out there, don't let fear stop you from talking with professors; many of them are much nicer than you think, and the ones that are not nice are probably not that great to work with.)</div>
<div>
<br /></div>
<div>
So one of the most important lessons I learned from Ron was the importance of kindness. Ron was generous and kind to everyone (and I really stress the word everyone) that he met. It didn't matter what walk of life you were in, what age you were, or what level of math (if any) that you knew, he was kind and willing to share his time and talents. He always had something in reach in his bag or pocket that he could pull out and show someone and give them an unexpected sense of wonder.</div>
<div>
<br /></div>
<div>
Richard Hamming <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">once said</a> "you can be a nice guy or you can be a great scientist", the implication being that you cannot do both. Ron showed that you can be a nice guy and a great scientist. And I believe that a significant portion of his success is owed to his being kind; all of us should learn from his examples and show more kindness towards others.</div>
<div>
<br /></div>
<div>
This is only one of many lessons I learned from Ron. Another thing I learned from Ron is the importance of data. I have seen multiple times when we would work on a problem and generate data resulting in what I thought were hopeless numbers to understand. But Ron looked at that same data and with a short bit of trial and error was able to make a guess of what the general form was. And almost inevitably he would be right! One way that Ron could do this was to start by factoring the values, and if all the prime factors were small he could guess that the expression was some combination of factorials and powers and then start to play with expressions until things worked out. Even when I knew what he did, I still am amazed that he was able to do it.</div>
<div>
<br /></div>
<div>
I will miss Ron, I will never have a collaboration as deep, as meaningful, and as personal. I am better for having worked with him, and learning from him about how to be a better mathematician and a better person.</div>
<div>
<br /></div>
<div>
Thank you, Ron.</div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html"><span class="datestr">at July 09, 2020 03:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1361">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1361">Policy on reporting papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>While we at PTReview always look through the posted papers, we do not check for correctness. We make a serious attempt to make sure the paper is reasonable. In a few instances, we have decided not to post a (topically relevant) paper, because it looks absolutely wrong. Our position is: the benefit of doubt goes to the author, and a borderline paper should be posted. We are only curating relevant tech reports, not passing judgment on results. </p>



<p>In some borderline cases, readers familiar with the subject complained to us that the paper should be not be considered a scientific contribution (because of, say, unspecified algorithms, blatantly incorrect or unverifiable central claims). These are cases where we were also unsure of the paper. We have usually removed/not posted such papers.</p>



<p><strong>If the paper author(s) feels that his/her paper should nonetheless be posted, then they should email us at little.oh.of.n@gmail.com.</strong> As long as the paper is not complete nonsense and appears to cite relevant history, we will defer to the authors’ wishes.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1361"><span class="datestr">at July 09, 2020 12:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/">Silver linings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>To put it mildly, 2020 is not shaping up to be a great year, so it is worthwhile to emphasize the good news, wherever we may find them.</p>
<p>Karlin, Klein, and Oveis Gharan have just <a href="https://arxiv.org/abs/2007.01409">posted a paper</a> in which, at long last, they improve over the 1.5 approximation ratio for metric TSP which was achieved, in 1974, by Christofides. For a long time, it was suspected that the Held-Karp relaxation of metric TSP had an approximation ratio better than 1.5, but there was no viable approach to prove such a result. In 2011, two different approaches were developed to improve 1.5 in the case of shortest-path metrics on unweighted graphs: one by Oveis Gharan, Saberi and Singh and one by Momke and Svensson. The algorithm of Karlin, Klein and Oveis Gharan (which does not establish that the Held-Karp relaxation has an integrality gap better than 1.5) takes as a starting point ideas from the work of Oveis Gharan, Saberi and Singh. </p>
<p><span id="more-4399"></span></p>
<p>Yesterday, Bloom and Sisask <a href="https://arxiv.org/abs/2007.03528">posted a paper</a> in which they show that there is a constant <img src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c&gt;0" class="latex" title="c&gt;0" /> such that, for every sufficiently large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" />, if <img src="https://s0.wp.com/latex.php?latex=A+%5Csubseteq+%5C%7B1%2C%5Cldots%2C+N+%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A \subseteq \{1,\ldots, N \}" class="latex" title="A \subseteq \{1,\ldots, N \}" /> has cardinality at least <img src="https://s0.wp.com/latex.php?latex=N+%2F+%28%5Clog+N%29%5E%7B1%2Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N / (\log N)^{1+c}" class="latex" title="N / (\log N)^{1+c}" />, then <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> contains a non-trivial length-3 arithmetic progression. Without context, this may seem like a strange result to get excited about, but it sits at the nexus of a number of fundamental results and open questions in combinatorics. Gil Kalai <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">has written an excellent post</a> telling the story of this problem, so instead of writing a worse version of it I will refer the reader to Gil’s blog.</p>
<p>Back to bad news, the day after Harvard announced that it would deliver courses online only in 2020-21, the Trump administration announced that it would void student visas of students who are not attending in-person classes in 2020-21. Back to good news, Harvard and MIT announced that they will sue the federal government over this, and other universities, including the University of California system, are planning similar responses. Apart from the action, I was really heartened to read MIT’s President <a href="http://news.mit.edu/2020/mit-and-harvard-file-suit-against-new-ice-regulations-0708">statement on the matter</a> (thanks to Vinod Vaikuntanathan for bringing it my attention) which is worth reproducing:</p>
<blockquote><p>
To the members of the MIT community,</p>
<p>On Monday, in a surprising development, a division of Immigration and Customs Enforcement announced that it will not permit international students on F-1 visas to take a full online course load this fall while studying in the United States. As I wrote yesterday, this ruling has potentially serious implications for MIT’s international students and those enrolled at institutions across the country.</p>
<p>This morning, in response, MIT and Harvard jointly filed suit against ICE and the US Department of Homeland Security in federal court in Massachusetts. In the lawsuit, we ask the court to prevent ICE and DHS from enforcing the new guidance and to declare it unlawful.</p>
<p>The announcement disrupts our international students’ lives and jeopardizes their academic and research pursuits. ICE is unable to offer the most basic answers about how its policy will be interpreted or implemented. And the guidance comes after many US colleges and universities either released or are readying their final decisions for the fall – decisions designed to advance their educational mission and protect the health and safety of their communities.</p>
<p>Our international students now have many questions – about their visas, their health, their families and their ability to continue working toward an MIT degree. Unspoken, but unmistakable, is one more question: Am I welcome?</p>
<p>At MIT, the answer, unequivocally, is yes.</p>
<p>MIT’s strength is its people – no matter where they come from. I know firsthand the anxiety of arriving in this country as a student, excited to advance my education, but separated from my family by thousands of miles. I also know that welcoming the world’s brightest, most talented and motivated students is an essential American strength.</p>
<p>While we pursue legal protections for our international students, we will continue to stay in close touch with them through email and updates on the International Students Office’s website. If you have questions, you may write to the ISO at iso-help@mit.edu.</p>
<p>Sincerely,</p>
<p>L. Rafael Reif
</p></blockquote>
<p>This way of talking like a human being, and like you actually care about the matter at hand, is a big contrast with the robotic statements that usually come out of campus leadership. The corresponding message from UC Berkeley’s Chancellor is the way such statements usually are like:</p>
<blockquote><p>
Dear campus community,</p>
<p>Yesterday, the Department of Homeland Security issued new guidance to universities related to international students and fall instruction requirements. The guidance is deeply concerning: it could potentially force the return of many international students to their home countries if they are unable to find the appropriate balance of in-person and remote classes. These requirements run counter to our values of being an inclusive community and one that has a long tradition of welcoming international students from around the globe. International students enrich campus life immeasurably, through their participation in classes, research collaborations and extracurricular activities.</p>
<p>We will explore all of our options, legal and otherwise, to counter the deleterious effects of these policies that imp act the ability for international students to achieve their academic goals. It is not only important for UC Berkeley but for all of higher education across the U.S. to take every step possible to mitigate these policies that send a message of exclusion to our international community of scholars. We will partner with our professional associations to advocate for sound legislation that continues to support international educational exchange.</p>
<p>More immediately, we are working with colleagues across our campus to identify a path that will allow us to comply with these requirements while ensuring a healthy learning environment, and paying attention to the needs of our international students. We recognize the concern and anxiety these new rules have created, and we are moving quickly to ensure that we offer the proper balance of online and in-person classes so that our students can remain in the U.S. and satisfy their visa requirements, and that those students residing outside the U.S. can maintain their enrollment status.</p>
<p>We expect to announce more details soon. Should you have any questions, please contact the Berkeley International Office at internationaloffice@berkeley.edu.</p>
<p>Sincerely,</p>
<p>Carol Christ<br />
Chancellor</p>
<p>Lisa Alvarez-Cohen<br />
Vice Provost for Academic Planning and Senior International Officer
</p></blockquote>
<p>It is interesting to think about where this difference in tone is coming from. Carol Christ is a renown humanities scholar who, I suppose, writes well. She comes across as charismatic and caring, and she is definitely straight-talking in person. Probably, as for everything else, Berkeley has a byzantine process to create announcements and press releases, and if Stephen Colbert was the Chancellor of UC Berkeley, after a couple of weeks on the job he would sound just as <i>deeply concerned</i> and just as into <i>exploring all options</i>, while meanwhile <i>working to identify a path</i> and <i>paying attention</i> about something that is totally fucked up and needs action <i>today</i>.</p>
<p>Which brings me to all the statements in support of Black Lives Matter that have been coming out of every scholarly institution in the last few days. While their messages are generally unobjectionable, there is a certain sameness to their form (“we say their names…”, “we will do the work…”, “we see you…”) and they don’t sound at all like the way the people putting them out speak. This has complicated causes, including the fact that many such statements came out of letter-writing campaigns that demanded statements in a very specific way, without leaving a lot of room for individual expression. The association of American Poets, for example, put out a statement of solidarity with the Black community; in response, a <a href="https://www.nytimes.com/2020/06/09/books/poetry-foundation-black-lives-matter.html">letter with 1800 signatories</a> claimed that it was too weak a statement and that it was, in fact, itself an act of violence against Black people; several resignations followed. The Board of the National Book Critics Circle was working on such a statement, and the work devolved into acrimony and several rounds of “I am outraged and I resign,” “no <i>I</i> am outraged at your outrage and <i>I</i> resign, “well then <i>I</i> am outraged that you are outraged at her outrage” until almost the whole board was gone in a “<a href="https://www.vulture.com/2020/06/national-book-critics-circle-resignations.html">sequence of events [that] was bizarre and bloody in an end-of-a-Tarantino-movie way</a>.”</p>
<p>Also, people in America talk about race the way UC Berkeley administrators talk about anything, that is extremely carefully and vacuously. But, back to the statements about foreign students, the difference between the administrative cultures at MIT and Berkeley is not the only difference between the statements of Reif and Christ: clearly a big difference is that Reif is an immigrant himself. When Trayvon Martin was killed, Obama talked about the killing in a way that was very different, and much more meaningful, than other politicians: if I had a son, Obama said, he would look a lot like Trayvon. If there were more people of color in positions of academic leadership, I think that we would have seen an academic response to Black Lives Matter that would have been less fearful, dogmatic and robotic and more meaningful and productive. Or perhaps we would have all ended up like the National Book Critic Circle, it’s hard to say.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/"><span class="datestr">at July 08, 2020 09:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=14203">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">To cheer you up in difficult times 7: Bloom and Sisask just broke the logarithm barrier for Roth’s theorem!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img src="https://gilkalai.files.wordpress.com/2020/07/rothnew.png?w=640" alt="rothnew" class="alignnone size-full wp-image-19954" /></p>
<h3 class="title mathjax">Thomas Bloom and Olof Sisask: <a href="https://arxiv.org/abs/2007.03528">Breaking the logarithmic barrier in Roth’s theorem on arithmetic progressions,    arXiv:200703528</a></h3>
<p> </p>
<p>Once again Extraordinary news regarding Roth Theorem! (I thank Ryan <span class="qu"><span class="gD">Alweiss for telling me about it and </span></span>Rahul Santhanam for telling me about Thomas and Olof’s earlier attempts.)</p>
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=R_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_n" class="latex" title="R_n" />  is a subset of <img src="http://l.wordpress.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2C+n+%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{1,2,\dots, n \}" title="\{1,2,\dots, n \}" /> of maximum cardinality not containing an arithmetic progression of length 3. Let <img src="https://s0.wp.com/latex.php?latex=r_3%28n%29%3D%7CR_n%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n)=|R_n|" class="latex" title="r_3(n)=|R_n|" />. Roth proved that <img src="https://s0.wp.com/latex.php?latex=r_3%28n%29%3Do%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n)=o(n)" class="latex" title="r_3(n)=o(n)" />.</p>
<p>A few days ago Thomas Bloom and Olof Sisask proved that for some <img src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c&gt;0" class="latex" title="c&gt;0" /></p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=r_3%28n%29+%5Cle+%5Cfrac+%7Bn%7D%7B%5Clog%5E%7B1%2Bc%7D+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n) \le \frac {n}{\log^{1+c} n}" class="latex" title="r_3(n) \le \frac {n}{\log^{1+c} n}" /></p>
<p>This is an extraordinary result!!! I will tell you a little more about it below.</p>
<h2>Ron Graham</h2>
<p>I just heard yesterday the sad news that <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ron Graham</a> passed away. Ron was an extraordinary mathematician and an extraordinary person. I first met Ron in Montreal in 1978 and we met many times since then. Ron will be dearly missed.</p>
<h3>Back to the new bounds on Roth’s theorem</h3>
<p>From an abstract of a lecture by Thomas and Olof: “This is the integer analogue of a result of Bateman and Katz for the model setting of vector spaces over a finite field, and the proof follows a similar structure.”</p>
<p>A catchy (weaker) formulation which goes back to Erdos and Turan is:</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=a_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a_n" class="latex" title="a_n" /> be a sequence of integers so that <img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cfrac%7B1%7D%7Ba_n%7D+%3D+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum \frac{1}{a_n} = \infty" class="latex" title="\sum \frac{1}{a_n} = \infty" />, then the sequence contains an arithmetic progression of length three!!</p>
<p>Bloom and Sisask’s result implies, of course, Van der Korput’s result that the primes contain infinitely many 3-terms arithmetic progression as well as Green’s 2005 result asserting it for every  dense subset of primes.</p>
<p>Szemeredi’s celabrated result extended Roth’s theorem to arithmetic progression of any fixed size, and Green-Tao celebrated 2008 result asserts that the primes (or a dense subsets of primes) contain arithmetic progression of any length. (The case of 3-term AP is so far much simpler for all the results mentioned below.)</p>
<p> </p>
<p>A little more about the history of the problem below the fold</p>
<p><span id="more-14203"></span></p>
<h2>Roth, Szemeredi, Heath-Brown, and Bourgain; Salem-Spencer and Behrend.</h2>
<p>Let’s wrire $r_3(n)=n/g(n)$. Roth proved that <img src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(n) \ge \log\log n" class="latex" title="g(n) \ge \log\log n" />. Szemeredi and Heath-Brown improved it to <img src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Ec+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(n) \ge \log^c n" class="latex" title="g(n) \ge \log^c n" /> for some <img />$latex c&gt;0$ (Szemeredi’s argument gave <img src="http://l.wordpress.com/latex.php?latex=c%3D1%2F4&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c=1/4" title="c=1/4" />.) Jean Bourgain improved the bound in 1999 to <img src="https://s0.wp.com/latex.php?latex=c%3D1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c=1/2" class="latex" title="c=1/2" /> and in 2008 to <img src="https://s0.wp.com/latex.php?latex=c%3D3%2F4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c=3/4" class="latex" title="c=3/4" /> (up to lower order terms).</p>
<p>Erdös and Turan who posed the problem in 1936 described a set not containing an arithmetic progression of size <img src="http://l.wordpress.com/latex.php?latex=n%5Ec&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n^c" title="n^c" />.  Salem and Spencer improved this bound to <img src="http://l.wordpress.com/latex.php?latex=g%28n%29+%5Cle+e%5E%7Blogn%2F+loglogn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="g(n) \le e^{logn/ loglogn}" title="g(n) \le e^{logn/ loglogn}" />. Behrend’s upper bound from 1946 is of the form <img src="http://l.wordpress.com/latex.php?latex=g%28n%29+%5Cle+e%5E%7BC%5Csqrt+%7B%5Clog+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="g(n) \le e^{C\sqrt {\log n}}" title="g(n) \le e^{C\sqrt {\log n}}" />. A small improvement was achieved  by Elkin and is discussed <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/" target="_blank" rel="noopener" title="Elkin's result">here</a>.  (Look also at the remarks following that post.)</p>
<h2>Sanders</h2>
<p>In 2010 Tom Sanders was able to refine Bourgain’s argument and proved that <img src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%28%5Clog+n%29%5E%7B3%2F4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(n) \ge (\log n)^{3/4}" class="latex" title="g(n) \ge (\log n)^{3/4}" />. A few month later  <a href="http://arxiv.org/abs/1011.0104">Tom have managed to reach the logarithmic barrier and to prove </a>that</p>
<p><img src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%28%5Clog+n%29%2F%28%5Clog+%5Clog+n%29%5E%7B6%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(n) \ge (\log n)/(\log \log n)^{6}." class="latex" title="g(n) \ge (\log n)/(\log \log n)^{6}." /></p>
<p>We reported about this outstanding achievement in <a href="https://gilkalai.wordpress.com/2010/11/24/roths-theorem-sanders-reaches-the-logarithmic-barrier/">this blog post</a> and quoted from his paper: “There are two main new ingredients in the present work: the first is a way of transforming sumsets introduced by <a href="http://front.math.ucdavis.edu/0802.4371">Nets Katz and Paul Koester</a> in 2008, and the second is a result on the <img src="https://s0.wp.com/latex.php?latex=L_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_p" class="latex" title="L_p" />-invariance of convolutions due to <a href="http://front.math.ucdavis.edu/1003.2978">Ernie Croot and Olof Sisask</a> (2010).”</p>
<p>The exponent 6 for the loglog term was improved to 4 by Thomas Bloom and recently to 3 by Thomas Schoen in his paper: <a href="https://arxiv.org/abs/2005.01145">Improved bound in Roth’s theorem on arithmetic progressions.</a> Schoen uses ingredients from Bateman and Katz’s work (see below).</p>
<h2>Cap sets  – Meshulam</h2>
<p>A closely related problem  in <img src="http://l.wordpress.com/latex.php?latex=%5CGamma%3D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Gamma=" title="\Gamma=" /><img src="http://l.wordpress.com/latex.php?latex=%5C%7B0%2C1%2C2%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{0,1,2\}^n" title="\{0,1,2\}^n" />. It is called the <a href="http://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/" target="_blank" rel="noopener" title="Cap sets at Tao">cap set problem</a>. A subset of <img src="http://l.wordpress.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Gamma" title="\Gamma" /> is called a cap set if it contains no arithmetic progression of size three or, alternatively, no three vectors that sum up to 0(modulo 3). If $latex <img src="http://l.wordpress.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A" title="A" />_n$  is a cap set of maximum size in <img src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Gamma" class="latex" title="\Gamma" /> we can ask how the function <img src="https://s0.wp.com/latex.php?latex=f%28n%29%3D%7CA_n%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(n)=|A_n|" class="latex" title="f(n)=|A_n|" /> behaves. In 1995 Roy Meshulam proved, using Roth’s argument, that <img src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+3%5En%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(n) \le 3^n/n" class="latex" title="f(n) \le 3^n/n" /> . Edell found an example of a cap set of size <img src="http://l.wordpress.com/latex.php?latex=2.2%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="2.2^n" title="2.2^n" />.  Again the gap is exponential.  What is the truth? Improving Meshulam’s result may be closely related to crossing the <img src="https://s0.wp.com/latex.php?latex=%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log n" class="latex" title="\log n" /> barrier for Roth’s theorem. In 2007 Tom Sanders  <a href="http://arxiv.org/abs/0807.5101">managed to achieve it </a>, not for the cup problem, but for a related problem over Z/4Z.</p>
<h2>Bateman and Katz</h2>
<p>In 2011, <a href="http://front.math.ucdavis.edu/1101.5851" target="_blank" rel="noopener">Michael Bateman and Nets Katz</a> improved, after many years of attempts by many, the Roth-Meshulam bound.  They proved using Fourier methods that <img src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+3%5En%2Fn%5E%7B1%2Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(n) \le 3^n/n^{1+c}" class="latex" title="f(n) \le 3^n/n^{1+c}" /> for some <img src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c&gt;0" class="latex" title="c&gt;0" />! This was very exciting.   See these two posts on Gowers’s blog (<a href="http://gowers.wordpress.com/2011/01/11/what-is-difficult-about-the-cap-set-problem/" target="_blank" rel="noopener">I</a>,<a href="http://gowers.wordpress.com/2011/01/18/more-on-the-cap-set-problem/" target="_blank" rel="noopener">II</a>). This raised the question if the new method allows breaking the  logarithmic barrier for Roth’s theorem.</p>
<h3>Polymath 6</h3>
<p>Tim Gowers <a href="https://gowers.wordpress.com/2011/02/05/polymath6-a-is-to-b-as-c-is-to/">proposed in 2011 polymath6 </a> to try to break the logarithmic barrier for Roth based on the Bateman-Katz breakthrough. (Here is<a href="http://michaelnielsen.org/polymath1/index.php?title=Improving_the_bounds_for_Roth%27s_theorem"> the wiki</a>; and a <a href="https://polymathprojects.org/2011/02/05/polymath6-improving-the-bounds-for-roths-theorem/">related post by Sanders</a>, and a <a href="https://polymathprojects.files.wordpress.com/2011/02/polymath-3.pdf">document by Katz</a>) This project did not get off the ground. We can regard the news as giving support that the polymath6 project was timely and of an appropriate level, and also as giving some support to an advantage of the conventional way of doing mathematics compared to the polymath way.</p>
<h2> Croot-Lev-Pach-Ellenberg-Gijswijt solution to the Cap set problem via the polynomial method</h2>
<p>Next and quite recently came a startling development  – the Croot-Lev-Pach-Ellenberg-Gijswijt capset bound <img src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+2.756%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(n) \le 2.756^n" class="latex" title="f(n) \le 2.756^n" />. (Croot, Lev, and Pach gave an exponential improvement for the Z/4Z case (see <a href="https://gilkalai.wordpress.com/2016/05/10/math-from-facebook/">this post</a>) and a few weeks later Ellenberg and Gijswijt used the method for the Z/3Z case (see <a href="https://gilkalai.wordpress.com/2016/05/15/mind-boggling-following-the-work-of-croot-lev-and-pach-jordan-ellenberg-settled-the-cap-set-problem/">this post</a>).)</p>
<p>A natural question that many people asked was how this development relates to improving the bounds for Roth perhaps even towards the Behrend bound. We discussed it a little over here and in other places. This is still an interesting possibility.</p>
<h2>The new result: Bloom and Sisask</h2>
<p>However, just a <del>few months</del> few years after the Bateman-Katz result have become obsolete for the cap-set problem, the Bateman-Katz method prevailed in this wonderful breakthrough of Bloom and Sisask giving <img src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Ec+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(n) \ge \log^c n" class="latex" title="g(n) \ge \log^c n" /> for <img src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c&gt;1" class="latex" title="c&gt;1" />.</p>
<h3 style="text-align: center;"><span style="color: #ff0000;">Congratulations!!!</span></h3>
<h2>An old post and poll</h2>
<p>In <a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">an old post we asked</a>: “How does <img src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n)" class="latex" title="r_3(n)" /> behave? Since we do not really know, will it help talking about it? Can we somehow look beyond the horizon and try to guess what the truth is? (I still don’t know if softly discussing this or other mathematical problems is a fruitful idea, but it can be enjoyable.)” We even had a poll collecting people’s predictions about <img src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n)" class="latex" title="r_3(n)" />.  Somewhat surprisingly 18.18% of answerers predicted that <img src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_3(n)" class="latex" title="r_3(n)" /> behaves like <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%28%5Clog+n%29%5Ec%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{(\log n)^c}" class="latex" title="\frac{1}{(\log n)^c}" /> for some <img src="https://s0.wp.com/latex.php?latex=c%3C1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c&lt;1" class="latex" title="c&lt;1" />.</p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/"><span class="datestr">at July 08, 2020 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2020/07/08/gain-margin/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2020/07/08/gain-margin/">Margin Walker</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I want to dive into some classic results in robust control and try to relate them to our current data-driven mindset. I’m going to try to do this in a modern way, avoiding any frequency domain analyses.</p>

<p>Suppose you want to solve some optimal control problem: you spend time modeling the dynamics of your system, how it responds to stimuli, and which objectives you’d like to maximize and constraints you must adhere to. Each of these modeling decisions explicitly encodes both your beliefs about reality and your mental criteria of success and failure. <em>Robustness</em> aims to quantify the effects of oversight on your systems behavior. Perhaps your model wasn’t accurate enough, or perhaps you forgot to include some constraint in your objective. What are the downstream consequences?</p>

<p>In the seventies, it was believed that optimization-based frameworks for control had “natural robustness.” The solutions of optimal control problems were often robust to phenomena not explicitly modeled by the engineer. As a simple example, suppose you have an incorrect model of the dynamical system you are trying to steer. How accurate do you need to be in order for this policy to be reasonably successful?</p>

<p>To focus in on this, let’s study the continuous-time linear quadratic regulator (LQR). I know I’ve been arguing that we should  be moving away from LQR in order to understand the broader challenges in learning and control, but the LQR baseline has so many lessons to teach us. Please humor me again for a few additional reasons: First, most of the history I want to tell arises from studying continuous-time LQR in the 1970s. It’s worth understanding that history with a modern perspective. Second, LQR does admit elegant closed form formulae that are helpful for pedagogy, and they are particularly nice in continuous time.</p>

<h2 id="lqr-in-continuous-time">LQR in Continuous Time</h2>

<p>Suppose we have a dynamical system that we model as an ODE:</p>



<p>Here, as always, $x_t$ is the state, $u_t$ is the control input signal, and $A$ and $B$ are matrices of appropriate dimensions. The goal of the continuous-time LQR problem is to minimize the cost functional</p>



<p>over all possible control inputs $u_t$. Let’s assume for simplicity that $Q$ is a positive semidefinite matrix and $R$ is positive definite.</p>

<p>The optimal LQR policy is <em>static state feedback</em>: there is some matrix $K$ such that</p>



<p>for all time. $K$ has a closed form solution that can be found by solving a <em>continuous algebraic Riccatti equation</em> (CARE) for a matrix $P$:</p>



<p>and then setting</p>



<p>Importantly, we take the solution of the CARE where $P$ that is positive definite. If a positive definite solution of the CARE exists, then it is optimal for continuous time LQR. There are a variety of ways to prove this condition is sufficient, including an appeal to dynamic programming in continuous time. A simple argument I like uses the quadratic structure of LQR to derive the necessity of the CARE solution. (I found this argument in <a href="https://www.ece.ucsb.edu/~hespanha/linearsystems/">Joao Hespansha’s book</a>).</p>

<p>Regardless, showing a positive definite CARE solution exists takes considerably more work. It suffices to assume that the pair $(A,B)$ is controllable and the pair $(Q,A)$ is detectable. But proving these conditions are sufficient requires a lot of manipulation of linear algebra, and I don’t think I could cleanly distill a proof into a blog post. I mention this just to reiterate that while LQR is definitely the simplest problem to study, its analysis in continuous time on an infinite time horizon is nontrivial. LQR is not really “easy.” It’s merely the easiest problem in a space of rather hard problems.</p>

<h2 id="gain-margins">Gain margins</h2>

<p>Let’s now turn to robustness. Suppose there is a mismatch between our modeled dynamics and reality. For example, what if the actual system is</p>



<p>for some matrix $B_\star$. Such model mismatches occur all the time. For example, in robotics, we can send a signal “u” to the joint of some robot. This would be some voltage that would need to be linearly transformed into some torque by a motor. It requires a good deal of calibration to make sure that the output of the motor is precisely the force dictated by the voltage output from our controller. Is there a way to guarantee some leeway in the mapping from voltage to torque?</p>

<p>An attractive feature of LQR is that we can quantify precisely how much slack we have directly from the CARE solution. We can use the solution of the CARE to build a <em>Lyapunov function</em> to guarantee stability of the system. Recall that a Lyapunov function is a function $V$ that maps states to real numbers, is nonnegative everywhere, is equal to $0$ only when $x=0$, and whose value is strictly decreasing along any trajectory of a dynamical system. In equations:</p>



<p>If you have a Lyapunov function, then all trajectories must converge to $x=0$: if you are at any nonzero state, the value of $V$ will decrease. If you are at $0$, then you will be at a global minimum of $V$ and hence can’t move to any other state.</p>

<p>Let $P$ be the solution of the CARE and let’s posit that $V(x) = x^\top  P x$ is a Lyapunov function. Since $P$ is positive definite, we have $V(x)\geq 0$ and $V(x)=0$ if and only if $x=0$. To prove that the derivative of the Lyapunov function is negative, we can first compute the derivative:</p>



<p>Note that it is sufficient to show that  $(A-B_\star K)^\top P + P(A-B_\star K)$ is a negative definite matrix as this would prove that the derivative is negative for all nonzero $x_t$. To prove that this expression is negative definite, let’s apply a bit of algebra to generate some sufficient conditions. Using the definition of $K$ and the fact that $P$ solves the CARE gives the following chain of equalities:</p>



<p>Here, the first equality is simply expanding the matrix product. The second equation uses the fact that $P$ is a solution to the CARE. The third equality uses the definition of $K$. The final equation is an algebraic rearrangement.</p>

<p>With this final expression, we can cook up a huge number of conditions under which we get “robustness for free.” First, consider the base case where $B=B_\star$. Since $R$ is positive definite and $Q$ is positive semidefinite, the entire expression is negative definite, and hence we have proven the system is stable.</p>

<p>Second, there is a famous result that LQR has “large gain margins.” The gain margin of a control system is an interval $(t_0,t_1)$ such that for all $t$ in this interval, our control system is stable with the controller $tK$. Another way of thinking about the gain margin is to assume that $B_\star = tB$, and to find the largest interval such that the system $(A,B_\star)$ is stabilized by a control policy $K$. For LQR, there are very large margins: if we plug in the identity $B_\star=tB$, we find that $x^\top  P x$ is a Lyapunov function provided that $t \in (\tfrac{1}{2},\infty)$. LQR control turns out to be robust to a wide range of perturbations to the matrix $B$. Intuitively, it makes sense that if we would like to drive a signal to zero and have more control authority than we anticipated then our policy will still drive the system to zero. This is the range of $t \in [1,\infty)$. The other part of the interval is perhaps more interesting: for the LQR problem, even if we only have half of the control authority we had planned for, we still will successfully stabilize our system from any initial condition.</p>

<p>In discrete time, you can derive similar formulae with essentially the same argument. Unfortunately, the expressions are not as elegant. Also, note that you cannot expect infinite gain margins in discrete time. In continuous time a differential equation $\dot{x}_t = M x_t$ is stable if all of the eigenvalues of $M$ have negative real parts. In discrete time, you need all of the eigenvalues to have magnitude less than $1$. For almost any random set triple $(A,B,K)$, $A-t B K$ is going to have large eigenvalues for $t$ large enough. Nonetheless, you can certainly derive analogous conditions as to which errors are tolerable.</p>

<p>There are a variety of other conditions that can be derived from our matrix expression. Most generally, the control system will be stable provided that</p>



<p>The LQR gain margins fall out naturally from this expression when we assume $B_\star = t B$. However, we can guarantee much more general robustness using this inequality. For example, if we assume that $B_\star = BM$ for some square matrix $M$, then $K$ stabilizes the pair $(A,B_\star)$ if all of the eigenvalues of $M+M^\top $ are greater than $1$.</p>

<p>Perhaps more in line with what we do in machine learning, suppose we are able to collect a lot of data, do some uncertainty quantification, and guarantee a bound $|B-B_\star|_2&lt;\epsilon$. Then as long as</p>



<p>we will be guaranteed stable execution. This expression depends on the matrices $P$, $Q$, and $R$, so it has a different flavor of the infinite gain margin conditions which held irrespective of the dynamics or the cost. Moreover, if $P$ has large eigenvalues, then we are only able to guarantee safe execution for small perturbations to $B$. This foreshadows issues I’ll dive into in later posts. I want to flag here that these calculations reveal some fragilities of LQR: While the controller is always robust to perturbations along the direction of the matrix $B$, you can construct examples where the system is highly sensitive to tiny perturbations orthogonal to $B$. I’ll return in the next post to start to unpack how optimal control has some natural robustness, but it has natural fragility as well.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2020/07/08/gain-margin/"><span class="datestr">at July 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/07/07/postdoc-at-uc-san-diego-apply-by-august-7-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/07/07/postdoc-at-uc-san-diego-apply-by-august-7-2020/">postdoc at UC San Diego (apply by August 7, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The UCSD CS department created a new postdoc program, modeled after the CI fellows program. To apply, you need to identify a UCSD theory faculty as a mentor, contact them and see if they are interested. If so, both you and the mentor need to apply. The deadline for both applications is Aug 7, so time is of the essence.</p>
<p>Website: <a href="https://forms.gle/7mMKS6xmCjWoMT817">https://forms.gle/7mMKS6xmCjWoMT817</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/07/07/postdoc-at-uc-san-diego-apply-by-august-7-2020/"><span class="datestr">at July 07, 2020 09:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/101">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/101">TR20-101 |  Lower Bounds for XOR of Forrelations | 

	Uma Girish, 

	Ran Raz, 

	Wei Zhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Forrelation problem, first introduced by Aaronson [AA10] and Aaronson and Ambainis [AA15], is  a well studied computational problem in the context of separating quantum and classical computational models. Variants of this problem were used to give tight separations between quantum and classical query complexity [AA15]; the first separation between poly-logarithmic quantum query complexity and bounded-depth circuits of super-polynomial size, a result that also implied an oracle separation of the classes BQP and PH [RT19]; and improved separations between quantum and classical communication complexity [GRT19]. In all these separations, the lower bound for the classical model only holds when the advantage of the protocol (over a random guess) is more than $\approx 1/\sqrt{N}$, that is, the success probability is larger than $\approx 1/2 + 1/\sqrt{N}$. This is unavoidable as $\approx 1/\sqrt{N}$ is the correlation between two coordinates of an input that is sampled from the Forrelation distribution, and hence there are simple classical protocols that achieve advantage $\approx 1/\sqrt{N}$, in all these models.

To achieve separations when the classical protocol has smaller advantage, we study in this work the XOR of $k$ independent copies of (a variant of) the Forrelation function (where $k\ll N$). We prove a very general result that shows that any family of Boolean functions that is closed under restrictions, whose Fourier mass at level $2k$ is bounded by $\alpha^k$ (that is, the sum of the absolute values of all Fourier coefficients at level $2k$ is bounded by $\alpha^k$), cannot compute the XOR of $k$ independent copies of the Forrelation function with advantage better than $O\left(\frac{\alpha^k}{{N^{k/2}}}\right)$. This is a strengthening of a result of [CHLT19], that gave a similar statement for $k=1$, using the technique of [RT19]. We give several applications of our result. In particular, we obtain the following separations:

Quantum versus Classical Communication Complexity: We give the first example of a partial Boolean function that can be computed by a simultaneous-message quantum protocol with communication complexity $\mbox{polylog}(N)$ (where Alice and Bob also share $\mbox{polylog}(N)$ EPR pairs), and such that, any classical randomized protocol of communication complexity at most $\tilde{o}(N^{1/4})$, with any number of rounds,  has quasipolynomially small advantage over a random guess. Previously, only separations where the classical protocol has polynomially small advantage were known between these models [G16, GRT19].

Quantum Query Complexity versus Bounded Depth Circuits: We give the first example of a partial Boolean function that has a quantum query algorithm with query complexity $\mbox{polylog}(N)$, and such that, any constant-depth circuit of quasipolynomial size has quasipolynomially small advantage over a random guess. Previously, only separations where the constant-depth circuit has polynomially small advantage were known [RT19].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/101"><span class="datestr">at July 07, 2020 04:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4892">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4892">My Enlightenment fanaticism</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If there were ever a time for liberals and progressives to put aside their internal squabbles, you’d think it was now.  The President of the United States is a racist gangster, who might not leave if he loses the coming election—all the more reason to ensure he loses in a landslide.  Due in part to that gangster’s breathtaking incompetence, 130,000 Americans are now dead, and the economy tanked, from a pandemic that the rest of the world has under much better control.  The gangster’s latest “response” to the pandemic has been to disrupt the lives of thousands of foreign scientists—including several of my students—by threatening to cancel their visas.  (American universities will, of course, do whatever they legally can to work around this act of pure spite.)</p>



<p>So how is the left responding to this historic moment?</p>



<p>This weekend, 536 people did so by … <a href="https://docs.google.com/document/d/17ZqWl5grm_F5Kn_0OarY9Q2jlOnk200PvhM5e3isPvY/preview?pru=AAABc0ugms8*_1VPq2TCPXlcaha9KVY3_Q">trying to cancel Steven Pinker</a>, stripping him of “distinguished fellow” and “media expert” status (whatever those are) in the Linguistics Society of America for ideological reasons.</p>



<p>Yes, Steven Pinker: the celebrated linguist and cognitive scientist, author of <em>The Language Instinct</em> and <em>How the Mind Works</em> (which had a massive impact on me as a teenager) and many other books, and academic torch-bearer for the Enlightenment in our time.  For years, I’d dreaded the day they’d <em>finally</em> come for Steve, even while friends assured me my fears must be inflated since, after all, they hadn’t come for him yet.</p>



<p>I concede that the cancelers’ logic is impeccable.  If they can get Pinker, everyone will quickly realize that there’s no longer any limit to who they can get—including me, including any writer or scientist who crosses them.  If you’ve ever taken, or aspire to take, any public stand riskier than “waffles are tasty,” then don’t delude yourself that you’ll be magically spared—<em>certainly</em> not by your own progressive credentials.</p>



<p>I don’t know if the “charges” against Pinker merit a considered response  (Pinker <a href="https://twitter.com/sapinker/status/1279934082210816003">writes</a> that some people wondered if they were satire).  For those who care, though, <a href="https://whyevolutionistrue.com/2020/07/05/the-purity-posse-pursues-pinker/">here’s</a> a detailed and excellent takedown by the biologist and blogger Jerry Coyne, and <a href="https://medium.com/@bhpartee/my-response-to-the-pinker-petition-open-letter-to-the-linguistics-community-80e2e4d9dbe2">here’s another</a> by Barbara Partee.</p>



<p>So, it seems Pinker once used the term “urban crime,” which can be a racist dogwhistle—except that in this case, it literally meant “urban crime.”  Pinker once referred to <a href="https://en.wikipedia.org/wiki/1984_New_York_City_Subway_shooting">Bernie Goetz</a>, whose 1984 shooting of four robbers in the NYC subway polarized the US at the time, as a “mild-mannered engineer,” in a sentence whose purpose was to <em>contrast</em> that description with the ferocity of Goetz’s act.  Pinker “appropriated” the work of a Black scholar, Harvard Dean Lawrence Bobo, which apparently meant <a href="https://twitter.com/sapinker/status/1268180637418164224?fbclid=IwAR1drpt4R2khSEEyiKiQMXEYloxy_6YzDTIvUEhb_FEkxL-KAPe9XvPYurg">approvingly citing him</a> in a tweet.  Etc.  Ironically, it occurred to me that the would-be Red Guards could’ve built a much stronger case against Pinker had they seriously engaged with his decades of writing—writing that really <em>does</em> take direct aim at their whole worldview, they aren’t wrong about that—rather than superficially collecting a few tweets.</p>



<p>What Coyne calls the “Purity Posse” sleazily gaslights its readers as follows:</p>



<blockquote class="wp-block-quote"><p>We want to note here that we have no desire to judge Dr. Pinker’s actions in moral terms, or claim to know what his aims are. Nor do we seek to “cancel” Dr. Pinker, or to bar him from participating in the linguistics and LSA communities (though many of our signatories may well believe that doing so would be the right course of action).</p></blockquote>



<p>In other words: many of us “may well believe” that Pinker’s scientific career should be ended entirely.  But magnanimously, <em>for now</em>, we’ll settle for a display of our power that leaves the condemned heretic still kicking.  So don’t accuse us of wanting to “cancel” anyone!</p>



<p>In that same generous spirit:</p>



<blockquote class="wp-block-quote"><p>Though no doubt related, we set aside questions of Dr. Pinker’s tendency to move in the proximity of what The Guardian called a revival of “scientific racism”, his public support for David Brooks (who has been argued to be a proponent of “gender essentialism”), his expert testimonial in favor of Jeffrey Epstein (which Dr. Pinker now regrets), or his dubious past stances on rape and feminism.</p></blockquote>



<p>See, even while we make these charges, we disclaim all moral responsibility for making them.  (For the record, Alan Dershowitz asked Pinker for a linguist’s opinion of a statute, so Pinker provided it; Pinker didn’t know at the time that the request had anything to do with Epstein.)</p>



<p>Again and again, spineless institutions have responded to these sorts of ultimatums by capitulating to them.  So I confess that the news about Pinker depressed me all weekend.  The more time passed, though, the more it looked like the Purity Posse might have <em>actually</em> overplayed its hand this time.  Steven Pinker is not weak prey.</p>



<p>Let’s start with what’s missing from the petition: Noam Chomsky <a href="https://twitter.com/ZaidJilani/status/1279505236181356544">pointedly refused to sign</a>.  How that must’ve stung his comrades!  For that matter, virtually all of the world’s well-known linguists refused to sign.  <a href="https://en.wikipedia.org/wiki/Ray_Jackendoff">Ray Jackendoff</a> and <a href="https://en.wikipedia.org/wiki/Michel_DeGraff">Michel DeGraff</a> were originally on the petition, but their names turned out to have been forged (were others?).</p>



<p>But despite the flimsiness of the petition, suppose the Linguistics Society of America caved.  OK, I mused, how many people have even <em>heard</em> of the Linguistics Society of America, compared to the number who’ve heard of Pinker or read his books?  If the LSA expelled Pinker, wouldn’t they be forever known to the world <em>only</em> as the organization that had done that?</p>



<p>I’m tired of the believers in the Enlightenment being constantly on the defensive.  “No, I’m not a racist or a misogynist … on the contrary, I’ve spent decades advocating for … yes, I did say that, but you completely misunderstood my meaning, which in context was … <em>please, I’m begging you</em>, can’t we sit and discuss this like human beings?”</p>



<p>It’s time for more of us to stand up and say: yes, I am a center-left extremist.  Yes, I’m an Enlightenment fanatic, a radical for liberal moderation and reason.  If liberalism is the vanilla of worldviews, then I aspire to be the most intense vanilla anyone has ever tasted.  I’m not a closeted fascist.  I’m not a watered-down leftist.  I’m something else.  I consider myself ferociously anti-racist and anti-sexist and anti-homophobic and pro-downtrodden, but I don’t cede to any ideological faction the right to dictate what those terms mean.  The world is too complicated, too full of ironies and surprises, for me to outsource my conscience in that way.</p>



<p>Enlightenment liberalism at least has the virtue that it’s not some utopian dream: on the contrary, it’s already led to most of the peace and prosperity that this sorry world has ever known, wherever and whenever it’s been allowed to operate.  And while “the death of the Enlightenment” gets proclaimed every other day, liberal ideals have by now endured for centuries.  They’ve outlasted kings and dictators, the Holocaust and the gulag.  They certainly have it within them to outlast some online sneerers.</p>



<p>Yes, sometimes martyrdom (or at least career martyrdom) is the only honorable course, and yes, the childhood bullies <em>did</em> gift me with a sizeable persecution complex—I’ll grant the sneerers that.  But on reflection, no, I don’t want to be a martyr for Enlightenment values.  I want Enlightenment values to <em>win</em>, and not by vanquishing their opponents but by persuading them.  As Pinker <a href="https://twitter.com/sapinker/status/1279936590236790784">writes</a>:</p>



<blockquote class="wp-block-quote"><p>A final comment: I feel sorry for the signatories. Moralistic dudgeon is a shallow and corrosive indulgence, &amp; policing the norms of your peer group a stunting of the intellect. Learning new ideas &amp; rethinking conventional wisdom are deeper pleasures … and ultimately better for the world. Our natural state is ignorance, fallibility, &amp; self-deception. Progress comes only from broaching &amp; evaluating ideas, including those that feel unfamiliar and uncomfortable.</p></blockquote>



<p>Spend a lot of time on Twitter and Reddit and news sites, and it <em>feels like</em> the believers in the above sentiment are wildly outnumbered by the self-certain ideologues of all sides.  But just like the vanilla in a cake can be hard to taste, so there are more Enlightenment liberals than it seems, even in academia—especially if we include all those who never explicitly identified that way, because they were too busy building or fixing or discovering or teaching, and because they mistakenly imagined that if they just left the Purity Posse alone then the Posse would do likewise.  If that’s you, then please ask yourself now: <em>what is my personal break-point for speaking up?</em></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4892"><span class="datestr">at July 07, 2020 02:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1352">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1352">News for June 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Sublinear algorithms in times of social distancing…always something exciting. This month we have a slew of results on sublinear algorithms for classic graph problems.</p>



<p><em>(Ed: We have removed a previously posted paper due to correctness concerns raised by our readers. Please look at the <a href="https://ptreview.sublinear.info/?p=1361">post on our paper policy</a>.)</em></p>



<p><strong>Palette Sparsification Beyond (∆ + 1) Vertex Coloring</strong> by Noga Alon and Sepehr Assadi (<a href="https://arxiv.org/pdf/2006.10456.pdf">arXiv</a>). A basic fact from graph theory is that any graph has a \((\Delta+1)\)-coloring, where \(\Delta\) is the maximum degree. Followers of property testing are likely familiar with a fantastic result of <a href="https://www.cs.rutgers.edu/~sa1497/pages/sublinear_vertex-coloring_2019.html">Assadi-Chen-Khanna</a> (ACK) on sublinear algorithms, that gives a sublinear algorithm for \((\Delta+1)\)-coloring. (The running time is \(\widetilde{O}(n^{3/2})\), where \(n\) is the number of vertices.) The key tool is a palette sparsification theorem: suppose each vertex is given a “palette” of \((\Delta+1)\) colors. Each vertex randomly sparsifies its palette by sampling \(O(\log n)\) colors, and is constrained to only use these colors. Remarkably, whp the graph can still be properly colored. This tool is at the heart of sublinear time/space algorithms for coloring. This paper gives numerous extensions to this theorem, where one can tradeoff a larger initially palette for a smaller final sample. Another extension is for triangle-free graphs, where the initial palette is of size \(O(\Delta/\ln \Delta)\) and the sample is of size \(O(\Delta^\gamma + \sqrt{\ln n})\) (for parameter \(\gamma &lt; 1\). This leads to an \(O(n^{3/2 + \gamma})\) time algorithm for \(O(\Delta/\ln \Delta)\) coloring of triangle-free graphs.</p>



<p><strong>When Algorithms for Maximal Independent Set and Maximal Matching Run in Sublinear-Time</strong> by Sepehr Assadi and Shay Solomon (<a href="https://arxiv.org/pdf/2006.07628.pdf">arXiv</a>). Taking off from sublinear coloring algorithms, one can ask if there are sublinear time algorithms for Maximal Independent Set (MIS) and Maximal Matching (MM). Alas, ACK prove that this is impossible. This paper investigates when one can get a sublinear time algorithm for these problems. For graph \(G\), let \(\beta(G)\) be the “neighborhood independence number”, the size of the largest independent set contained in a vertex neighborhood. This papers shows that both problems can be solved in \(\widetilde{O}(n \beta(G))\) time. Examples of natural classes of graphs where \(\beta(G)\) is constant: line graphs and unit-disk graphs. An interesting aspect is that MIS algorithm is actually deterministic! It’s the simple marking algorithm that rules out neighborhoods of chosen vertices; the analysis shows that not much time is wasted in remarking the same vertex. </p>



<p><strong>Sublinear Algorithms and Lower Bounds for Metric TSP Cost Estimation</strong> by Yu Chen, Sampath Kannan, and Sanjeev Khanna (<a href="https://arxiv.org/pdf/2006.05490.pdf">arXiv</a>). This paper studies sublinear algorithms for the metric TSP problem. The input is an \(n \times n\) distance matrix. One can 2-approximate the TSP by computing the MST, and a result of <a href="http://wrap.warwick.ac.uk/2416/">Czumaj-Sohler</a> gives a \((1+\varepsilon)\)-approximation algorithm for the latter, running in \(O(n\varepsilon^{-O(1)})\) time. The main question is: can one beat the 2-factor approximation in sublinear time? This paper considers the graphic TSP setting, where the distance matrix corresponds to the shortest path metric of an unweighted graph. One result is a \((2-\varepsilon_0)\)-approximation algorithm (for an explicit constant \(\varepsilon_0\)) that runs in \(\widetilde{O}(n)\) time. For the important \((1,2)\) TSP setting (all distances are either 1 or 2), the paper gives a \(O(n^{1.5})\) time 1.63-approximation algorithm. Interestingly, there is a lower bound showing that \((1+\varepsilon)\)-approximations, for arbitrarily small \(\varepsilon\), cannot be achieved in \(o(n^2)\) time. One of the key tools is sublinear algorithms for estimating the maximum matching size, itself a well-studied problem in the community.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1352"><span class="datestr">at July 07, 2020 06:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
