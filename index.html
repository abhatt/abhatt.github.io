<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 11, 2020 02:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/154">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/154">TR20-154 |  A Structural Theorem for Local Algorithms with Applications to Coding, Testing, and Privacy | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Oded Lachish</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a general structural theorem for a wide family of local algorithms, which includes property testers, local decoders, and PCPs of proximity. Namely, we show that the structure of every algorithm that makes $q$ adaptive queries and satisfies a natural robustness condition admits a sample-based algorithm with $n^{1- 1/O(q^2 \log^2 q)}$ sample complexity, following the definition of Goldreich and Ron (TOCT 2016). We prove that this transformation is nearly optimal. Our theorem also admits a scheme for constructing privacy-preserving local algorithms. 

Using the unified view that our structural theorem provides, we obtain results regarding various types of local algorithms, including the following.


- We strengthen the state-of-the-art lower bound for relaxed locally decodable codes, obtaining an exponential improvement on the dependency in query complexity; this resolves an open problem raised by Gur and Lachish (SODA 2020).
- We show that any (constant-query) testable property admits a sample-based tester with sublinear sample complexity; this resolves a problem left open in a work of Fischer, Lachish, and Vasudev (FOCS 2015) by extending their main result to adaptive testers.
- We prove that the known separation between proofs of proximity and testers is essentially maximal; this resolves a problem left open by Gur and Rothblum (ECCC 2013, Computational Complexity 2018) regarding sublinear-time delegation of computation.

Our techniques strongly rely on relaxed sunflower lemmas and the Hajnal–Szemerédi theorem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/154"><span class="datestr">at October 10, 2020 11:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.04157">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.04157">Online and Distribution-Free Robustness: Regression and Contextual Bandits with Huber Contamination</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koehler:Frederic.html">Frederic Koehler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yau:Morris.html">Morris Yau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.04157">PDF</a><br /><b>Abstract: </b>In this work we revisit two classic high-dimensional online learning
problems, namely regression and linear contextual bandits, from the perspective
of adversarial robustness. Existing works in algorithmic robust statistics make
strong distributional assumptions that ensure that the input data is evenly
spread out or comes from a nice generative model. Is it possible to achieve
strong robustness guarantees even without distributional assumptions
altogether, where the sequence of tasks we are asked to solve is adaptively and
adversarially chosen?
</p>
<p>We answer this question in the affirmative for both regression and linear
contextual bandits. In fact our algorithms succeed where convex surrogates fail
in the sense that we show strong lower bounds categorically for the existing
approaches. Our approach is based on a novel way to use the sum-of-squares
hierarchy in online learning and in the absence of distributional assumptions.
Moreover we give extensions of our main results to infinite dimensional
settings where the feature vectors are represented implicitly via a kernel map.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.04157"><span class="datestr">at October 10, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.04135">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.04135">Isometric and affine copies of a set in volumetric Helly results</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>John A. Messina, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sober=oacute=n:Pablo.html">Pablo Soberón</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.04135">PDF</a><br /><b>Abstract: </b>We show that for any compact convex set $K$ in $\mathbb{R}^d$ and any finite
family $\mathcal{F}$ of convex sets in $\mathbb{R}^d$, if the intersection of
every sufficiently small subfamily of $\mathcal{F}$ contains an isometric copy
of $K$ of volume $1$, then the intersection of the whole family contains an
isometric copy of $K$ scaled by a factor of $(1-\varepsilon)$, where
$\varepsilon$ is positive and fixed in advance. Unless $K$ is very similar to a
disk, the shrinking factor is unavoidable. We prove similar results for affine
copies of $K$. We show how our results imply the existence of randomized
algorithms that approximate the largest copy of $K$ that fits inside a given
polytope $P$ whose expected runtime is linear on the number of facets of $P$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.04135"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.04108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.04108">Succinct Permutation Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsakalidis:Konstantinos.html">Konstantinos Tsakalidis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wild:Sebastian.html">Sebastian Wild</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zamaraev:Viktor.html">Viktor Zamaraev</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.04108">PDF</a><br /><b>Abstract: </b>We present a succinct, i.e., asymptotically space-optimal, data structure for
permutation graphs that supports distance, adjacency, neighborhood and
shortest-path queries in optimal time; a variant of our data structure also
supports degree queries in time independent of the neighborhood's size at the
expense of an $O(\log n/\log \log n)$-factor overhead in all running times. We
show how to generalize our data structure to the class of circular permutation
graphs with asymptotically no extra space, while supporting the same queries in
optimal time. Furthermore, we develop a similar compact data structure for the
special case of bipartite permutation graphs and conjecture that it is succinct
for this class. We demonstrate how to execute algorithms directly over our
succinct representations for several combinatorial problems on permutation
graphs: Clique, Coloring, Independent Set, Hamiltonian Cycle, All-Pair Shortest
Paths, and others.
</p>
<p>Moreover, we initiate the study of semi-local graph representations; a
concept that "interpolates" between local labeling schemes and standard
"centralized" data structures. We show how to turn some of our data structures
into semi-local representations by storing only $O(n)$ bits of additional
global information, beating the lower bound on distance labeling schemes for
permutation graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.04108"><span class="datestr">at October 10, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03983">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03983">Online Allocation of Reusable Resources via Algorithms Guided by Fluid Approximations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vineet.html">Vineet Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iyengar:Garud.html">Garud Iyengar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Udwani:Rajan.html">Rajan Udwani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03983">PDF</a><br /><b>Abstract: </b>We consider the problem of online allocation (matching and assortments) of
reusable resources where customers arrive sequentially in an adversarial
fashion and allocated resources are used or rented for a stochastic duration
that is drawn independently from known distributions. Focusing on the case of
large inventory, we give an algorithm that is $(1-1/e)$ competitive for general
usage distributions. At the heart of our result is the notion of a relaxed
online algorithm that is only subjected to fluid approximations of the
stochastic elements in the problem. The output of this algorithm serves as a
guide for the final algorithm. This leads to a principled approach for
seamlessly addressing stochastic elements (such as reusability, customer
choice, and combinations thereof) in online resource allocation problems, that
may be useful more broadly.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03983"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03910">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03910">An Experimental Analysis of Indoor Spatial Queries: Modeling, Indexing, and Processing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Tiantian.html">Tiantian Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Huan.html">Huan Li</a>, Hua Lu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheema:Muhammad_Aamir.html">Muhammad Aamir Cheema</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shou:Lidan.html">Lidan Shou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03910">PDF</a><br /><b>Abstract: </b>Indoor location-based services (LBS), such as POI search and routing, are
often built on top of typical indoor spatial queries. To support such queries
and indoor LBS, multiple techniques including model/indexes and search
algorithms have been proposed. In this work, we conduct an extensive
experimental study on existing proposals for indoor spatial queries. We survey
five model/indexes, compare their algorithmic characteristics, and analyze
their space and time complexities. We also design an in-depth benchmark with
real and synthetic datasets, evaluation tasks and performance metrics. Enabled
by the benchmark, we obtain and report the performance results of all
model/indexes under investigation. By analyzing the results, we summarize the
pros and cons of all techniques and suggest the best choice for typical
scenarios.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03910"><span class="datestr">at October 10, 2020 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03894">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03894">Intrinsic Hierarchical Clustering Behavior Recovers Higher Dimensional Shape Information</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ignacio:Paul_Samuel_P=.html">Paul Samuel P. Ignacio</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03894">PDF</a><br /><b>Abstract: </b>We show that specific higher dimensional shape information of point cloud
data can be recovered by observing lower dimensional hierarchical clustering
dynamics. We generate multiple point samples from point clouds and perform
hierarchical clustering within each sample to produce dendrograms. From these
dendrograms, we take cluster evolution and merging data that capture clustering
behavior to construct simplified diagrams that record the lifetime of clusters
akin to what zero dimensional persistence diagrams do in topological data
analysis. We compare differences between these diagrams using the bottleneck
metric, and examine the resulting distribution. Finally, we show that
statistical features drawn from these bottleneck distance distributions detect
artefacts of, and can be tapped to recover higher dimensional shape
characteristics.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03894"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03870">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03870">Improved approximation ratios for two Euclidean maximum spanning tree problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03870">PDF</a><br /><b>Abstract: </b>We study the following two maximization problems related to spanning trees in
the Euclidean plane. It is not known whether or not these problems are NP-hard.
We present approximation algorithms with better approximation ratios for both
problems. The improved ratios are obtained mainly by employing the Steiner
ratio, which has not been used in this context earlier.
</p>
<p>(i) Longest noncrossing spanning tree: Given a set of points in the plane,
the goal is to find a maximum-length noncrossing spanning tree. Alon,
Rajagopalan, and Suri (SoCG 1993) studied this problem for the first time and
gave a $0.5$-approximation algorithm. Over the years, the approximation ratio
has been successively improved to $0.502$, $0.503$, and to $0.512$ which is the
current best ratio, due to Cabello et al.. We revisit this problem and improve
the ratio further to $0.519$. The improvement is achieved by a collection of
ideas, some from previous works and some new ideas (including the use of the
Steiner ratio), along with a more refined analysis.
</p>
<p>(ii) Longest spanning tree with neighborhoods: Given a collection of regions
(neighborhoods) in the plane, the goal is to select a point in each
neighborhood so that the longest spanning tree on selected points has maximum
length. We present an algorithm with approximation ratio $0.524$ for this
problem. The previous best ratio, due to Chen and Dumitrescu, is $0.511$ which
is in turn the first improvement beyond the trivial ratio $0.5$. Our algorithm
is fairly simple, its analysis is relatively short, and it takes linear time
after computing a diametral pair of points. The simplicity comes from the fact
that our solution belongs to a set containing three stars and one double-star.
The shortness and the improvement come from the use of the Steiner ratio.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03870"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03850">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03850">An Improved Exact Algorithm for the Exact Satisfiability Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoi:Gordon.html">Gordon Hoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03850">PDF</a><br /><b>Abstract: </b>The Exact Satisfiability problem, XSAT, is defined as the problem of finding
a satisfying assignment to a formula $\varphi$ in CNF such that exactly one
literal in each clause is assigned to be "1" and the other literals in the same
clause are set to "0". Since it is an important variant of the satisfiability
problem, XSAT has also been studied heavily and has seen numerous improvements
to the development of its exact algorithms over the years.
</p>
<p>The fastest known exact algorithm to solve XSAT runs in $O(1.1730^n)$ time,
where $n$ is the number of variables in the formula. In this paper, we propose
a faster exact algorithm that solves the problem in $O(1.1674^n)$ time. Like
many of the authors working on this problem, we give a DPLL algorithm to solve
it. The novelty of this paper lies on the design of the nonstandard measure, to
help us to tighten the analysis of the algorithm further.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03850"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03820">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03820">A Matroid Generalization of the Super-Stable Matching Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamiyama:Naoyuki.html">Naoyuki Kamiyama</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03820">PDF</a><br /><b>Abstract: </b>A super-stable matching, which was introduced by Irving, is a solution
concept in a variant of the stable matching problem in which the preferences
may contain ties. Irving proposed a polynomial-time algorithm for the problem
of finding a super-stable matching if a super-stable matching exists. In this
paper, we consider a matroid generalization of a super-stable matching. We call
our generalization of a super-stable matching a super-stable common independent
set. This can be considered as a generalization of the matroid generalization
of a stable matching for strict preferences proposed by Fleiner. We propose a
polynomial-time algorithm for the problem of finding a super-stable common
independent set if a super-stable common independent set exists.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03820"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03817">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03817">Efficient Sampling from Feasible Sets of SDPs and Volume Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalkis:Apostolos.html">Apostolos Chalkis</a>, Ioannis Emiris, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fisikopoulos:Vissarion.html">Vissarion Fisikopoulos</a>, Panagiotis Repouskos, Elias Tsigaridas <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03817">PDF</a><br /><b>Abstract: </b>We present algorithmic, complexity, and implementation results on the problem
of sampling points from a spectrahedron, that is the feasible region of a
semidefinite program. Our main tool is geometric random walks. We analyze the
arithmetic and bit complexity of certain primitive geometric operations that
are based on the algebraic properties of spectrahedra and the polynomial
eigenvalue problem. This study leads to the implementation of a broad
collection of random walks for sampling from spectrahedra that experimentally
show faster mixing times than methods currently employed either in theoretical
studies or in applications, including the popular family of Hit-and-Run walks.
The different random walks offer a variety of advantages , thus allowing us to
efficiently sample from general probability distributions, for example the
family of log-concave distributions which arise in numerous applications. We
focus on two major applications of independent interest: (i) approximate the
volume of a spectrahedron, and (ii) compute the expectation of functions coming
from robust optimal control. We exploit efficient linear algebra algorithms and
implementations to address the aforemen-tioned computations in very high
dimension. In particular, we provide a C++ open source implementation of our
methods that scales efficiently, for the first time, up to dimension 200. We
illustrate its efficiency on various data sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03817"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.03653">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.03653">Efficient Temporal Pattern Mining in Big Time Series Using Mutual Information</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Van_Long.html">Van Long Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Nguyen.html">Nguyen Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedersen:Torben_Bach.html">Torben Bach Pedersen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.03653">PDF</a><br /><b>Abstract: </b>Very large time series are increasingly available from an ever wider range of
IoT-enabled sensors deployed in different environments. Significant insights
can be obtained through mining temporal patterns from these time series. Unlike
traditional pattern mining, temporal pattern mining (TPM) adds additional
temporal aspect into extracted patterns, thus making them more expressive.
However, adding the temporal dimension into patterns results in an exponential
growth of the search space, significantly increasing the mining process
complexity. Current TPM approaches either cannot scale to large datasets, or
typically work on pre-processed event sequences rather than directly on time
series. This paper presents our comprehensive Frequent Temporal Pattern Mining
from Time Series (FTPMfTS) approach which provides the following contributions:
(1) The end-to-end FTPMfTS process that directly takes time series as input and
produces frequent temporal patterns as output. (2) The efficient Hierarchical
Temporal Pattern Graph Mining (HTPGM) algorithm that uses efficient data
structures to enable fast computations for support and confidence. (3) A number
of pruning techniques for HTPGM that yield significantly faster mining. (4) An
approximate version of HTPGM which relies on mutual information to prune
unpromising time series, and thus significantly reduce the search space. (5) An
extensive experimental evaluation on real-world datasets from the energy and
smart city domains which shows that HTPGM outperforms the baselines and can
scale to large datasets. The approximate HTPGM achieves up to 3 orders of
magnitude speedup compared to the baselines and consumes significantly less
memory, while obtaining high accuracy compared to the exact HTPGM.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.03653"><span class="datestr">at October 10, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.02264">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.02264">Subspace Embeddings Under Nonlinear Transformations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Aarshvi Gajjar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.02264">PDF</a><br /><b>Abstract: </b>We consider low-distortion embeddings for subspaces under \emph{entrywise
nonlinear transformations}. In particular we seek embeddings that preserve the
norm of all vectors in a space $S = \{y: y = f(x)\text{ for }x \in Z\}$, where
$Z$ is a $k$-dimensional subspace of $\mathbb{R}^n$ and $f(x)$ is a nonlinear
activation function applied entrywise to $x$. When $f$ is the identity, and so
$S$ is just a $k$-dimensional subspace, it is known that, with high
probability, a random embedding into $O(k/\epsilon^2)$ dimensions preserves the
norm of all $y \in S$ up to $(1\pm \epsilon)$ relative error. Such embeddings
are known as \emph{subspace embeddings}, and have found widespread use in
compressed sensing and approximation algorithms. We give the first
low-distortion embeddings for a wide class of nonlinear functions $f$. In
particular, we give additive $\epsilon$ error embeddings into $O(\frac{k\log
(n/\epsilon)}{\epsilon^2})$ dimensions for a class of nonlinearities that
includes the popular Sigmoid SoftPlus, and Gaussian functions. We strengthen
this result to give relative error embeddings under some further restrictions,
which are satisfied e.g., by the Tanh, SoftSign, Exponential Linear Unit, and
many other `soft' step functions and rectifying units. Understanding embeddings
for subspaces under nonlinear transformations is a key step towards extending
random sketching and compressing sensing techniques for linear problems to
nonlinear ones. We discuss example applications of our results to improved
bounds for compressed sensing via generative neural networks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.02264"><span class="datestr">at October 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/153">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/153">TR20-153 |  Total Functions in the Polynomial Hierarchy | 

	Robert Kleinberg, 

	Daniel Mitropolsky, 

	Christos Papadimitriou</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify several genres of search problems beyond NP for which existence of solutions is guaranteed.  One class that seems especially rich in such problems is PEPP (for "polynomial empty pigeonhole principle"), which includes problems related to existence theorems proved through the union bound, such as finding a bit string that is far from all codewords, finding an explicit rigid matrix, as well as a problem we call Complexity, capturing Complexity Theory's quest.  When the union bound is generous, in that solutions constitute at least a polynomial fraction of the domain, we have a family of seemingly weaker classes $\alpha$-PEPP, which are inside FP}$^{\text{NP}}|$poly.  Higher in the hierarchy, we identify the constructive version of the Sauer-Shelah lemma and the appropriate generalization of PPP that contains it.  The resulting total function hierarchy turns out to be more stable than the polynomial hierarchy: it is known that, under oracles, total functions within FNP may be easy, but total functions a level higher may still be harder than FP$^{\text{NP}}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/153"><span class="datestr">at October 08, 2020 10:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/152">TR20-152 |  Variants of the Determinant polynomial and VP-completeness | 

	Prasad Chaugule, 

	Nutan Limaye, 

	Shourya Pandey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The determinant is a canonical VBP-complete polynomial in the algebraic complexity setting. In this work, we introduce two variants of the determinant polynomial which we call $StackDet_n(X)$ and $CountDet_n(X)$ and show that they are VP and VNP complete respectively under $p$-projections. The definitions of the polynomials are inspired by a combinatorial characterisation of the determinant developed by Mahajan and Vinay (SODA 1997). We extend the combinatorial object in their work, namely clow sequences, by introducing additional edge labels on the edges of the underlying graph. The idea of using edge labels is inspired by the work of Mengel (MFCS 2013).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/152"><span class="datestr">at October 08, 2020 10:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=1380">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/hermite-polynomials/">Polynomial magic III : Hermite polynomials</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">After two blog posts earlier this year on <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> and <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. </p>



<p class="justify-text">This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. </p>



<h2>Definition and first properties</h2>



<p class="justify-text">There are many equivalent ways to define Hermite polynomials. A natural one is through the so-called <a href="https://en.wikipedia.org/wiki/Rodrigues%27_formula">Rodrigues’ formula</a>: $$H_k(x) = (-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big],$$ from which we can deduce \(H_0(x) = 1\), \(H_1(x) =\   – e^{x^2} \big[ -2x e^{-x^2} \big] = 2x\), \(H_2(x) = e^{x^2} \big[ (-2x)^2e^{-x^2} -2 e^{-x^2}  \big] =  4x^2 – 2\), etc.</p>



<p class="justify-text">Other simple properties which are consequences of the definition (and can be shown by recursion) are that \(H_k\) is a polynomial of degree \(k\), with the same parity as \(k\), and with a leading coefficient equal to \(2^k\).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> Using integration by parts, one can show (see end of the post) that for \(k \neq \ell\), we have $$\int_{-\infty}^{+\infty}  \!\!\!H_k(x) H_\ell(x) e^{-x^2} dx =0, $$ and that for \(k=\ell\), we have $$\int_{-\infty}^{+\infty} \!\!\! H_k(x)^2 e^{-x^2}dx = \sqrt{\pi} 2^k k!.$$ </p>



<p class="justify-text">In other words, the Hermite polynomials are orthogonal for the Gaussian distribution with mean \(0\) and variance \(\frac{1}{2}\). Yet in other words, defining the <em>Hermite functions</em> as \( \displaystyle \psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), we obtain an orthonormal basis of \(L_2(dx)\). As illustrated below, the Hermite functions, as the index \(k\) increases, have an increasing “support” (the support is always the entire real line, but most of the mass is concentrated in centered balls of increasing sizes, essentially at \(\sqrt{k}\)) and, like cosines and sines, an increasingly oscillatory behavior.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="349" alt="" src="https://francisbach.com/wp-content/uploads/2020/08/hermite.gif" class="wp-image-4579" height="305" />Plot of Hermite functions \(\psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), from \(k=0\) to \(k=20\).</figure></div>



<p class="justify-text">Among such orthonormal bases, the Hermite functions happen to be diagonalizing the Fourier tranform operator.  In other words, the Fourier transform of \(\psi_k\) (for the definition making it an isometry of \(L_2(dx)\)) is equal to $$ \mathcal{F}(\psi_k)(\omega)  =  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} \psi_k(x) e^{- i \omega x} dx = (-i)^k \psi_k(\omega).$$ (note that the eigenvalues are all of unit modulus as we have an isometry). See a proof at the end of the post. I am not aware of any applications of this property in machine learning or statistics (but there are probably some).</p>



<p class="justify-text"><strong>Recurrence.</strong> In order to compute Hermite polynomials, the following recurrence relation is the most useful $$ H_{k+1}(x) = 2x H_k(x) \ – 2k H_{k-1}(x). \tag{1}$$  Such recursions are always available for orthogonal polynomials (see [4]), but it takes here a particularly simple form (see a proof at the end of the post).</p>



<p class="justify-text"><strong>Generating function.</strong> The following property is central in many proofs of properties of Hermite polynomials: for all \(t \in \mathbb{R}\), we have $$\sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =e^{ 2xt \ – \ t^2}, \tag{2}$$ with a proof at the end of the post based on the residue theorem.</p>



<h2>Further (less standard) properties</h2>



<p class="justify-text">For the later developments, we need other properties which are less standard (there are many other interesting properties, which are not useful for this post, see <a href="https://en.wikipedia.org/wiki/Hermite_polynomials">here</a>).</p>



<p class="justify-text"><strong>Mehler formula. </strong>For \(|\rho| &lt; 1\), it states: $$ \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sqrt{1-\rho^2} \sum_{k=0}^\infty \frac{\rho^k}{2^k k!} H_k(x) H_k(y) \exp \Big( – \frac{\rho}{1+\rho} (x^2 + y^2) \Big).$$ The proof is significantly more involved; see [<a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">1</a>] for details (with a great last sentence: “Prof. Hardy tells me that he has not seen his proof in print, though the inevitability of the successive steps makes him think that it is unlikely to be new”). Note that we will in fact obtain a new proof from the relationship with kernel methods (see below).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions. </strong>We will need this property for \(|\rho|&lt;1\) (see proof at the end of the post), which corresponds to the expectation of \(H_k(x)\) for \(x\) distributed as a non-centered Gaussian distribution: $$\int_{-\infty}^\infty H_k(x) \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big)dx= \sqrt{\pi} \rho^k \sqrt{1-\rho^2} H_k (y). \tag{3}$$</p>



<p class="justify-text">Given the relationship with the Gaussian distribution, it is no surprise that Hermite polynomials pop up whenever Gaussians are used, as distributions or kernels. Before looking into it, let’s first give a brief review of kernel methods.</p>



<h2>From positive-definite kernel to Hilbert spaces</h2>



<p class="justify-text">Given a prediction problem with inputs in a set \(\mathcal{X}\), a traditional way of parameterizing real-valued functions on \(\mathcal{X}\) is to use <em>positive-definite kernels</em>.</p>



<p class="justify-text">A positive-definite kernel is a function \(K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) such that for all sets \(\{x_1,\dots,x_n\}\) of \(n\) elements of \(\mathcal{X}\), the “kernel matrix” in \(\mathbb{R}^{n \times n}\) composed of pairwise evaluations is symmetric positive semi-definite. This property happens to be equivalent to the existence of a Hilbert feature space \(\mathcal{H}\) and a feature map \(\varphi: \mathcal{X} \to \mathcal{H}\) such that $$K(x,x’) = \langle \varphi(x), \varphi(x’) \rangle_{\mathcal{H}},$$ with an elegant constructive proof [<a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">15</a>].</p>



<p class="justify-text">This allows to define the space of linear functions on the features, that is, functions of the form $$f(x) = \langle f, \varphi(x) \rangle_{\mathcal{H}},$$ for \(f \in \mathcal{H}\). </p>



<p class="justify-text">This space is often called the <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space" class="">reproducing kernel Hilbert space</a> (RKHS) associated to the kernel \(K\) (we can prove that it is indeed uniquely defined). In such a space, we can also define the squared norm of the function \(f\), namely \(\| f\|_{\mathcal{H}}^2\), which can be seen as a specific regularization term in kernel methods.</p>



<p class="justify-text">The space satisfies the so-called reproducing property (hence its name): \(f(x) = \langle f, K(\cdot,x) \rangle_{\mathcal{H}}\). In other words, the feature \(\varphi(x)\) is the kernel function evaluated at \(x\), that is,  \(\varphi(x) = K(\cdot,x)\). These spaces have been a source of many developments in statistics [5] and machine learning [6, 7].</p>



<p class="justify-text"><strong>Orthonormal basis.</strong> A difficulty in working with infinite-dimensional Hilbert spaces of functions is that it is sometimes hard to understand what functions are actually considered. One simple way to enhance understanding of the regularization property is to have an orthonormal basis (in very much the same way as the Fourier basis), as we can then identify \(\mathcal{H}\) to the space of squared-integrable sequences.</p>



<p class="justify-text">For kernel-based Hilbert spaces, if we have an orthonormal basis \((g_k)_{k \geqslant 0}\) of the Hilbert space \(\mathcal{H}\), then, by decomposing \(\varphi(x)\) in the basis, we have $$\varphi(x) = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} g_k,$$ we get $$K(x,y) = \langle \varphi(y), \varphi(x) \rangle = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} \langle  \varphi(y), g_k \rangle_\mathcal{H} =\sum_{k=0}^\infty g_k(x) g_k(y), \tag{4}$$ that is, we have an expansion of the kernel as an infinite sum (note here, that we ignore summability issues).</p>



<p class="justify-text">Among orthonormal bases, some are more interesting than others. The ones composed of eigenfunctions for particular operators are really more interesting, in particular for the covariance operator that we now present, and their use in statistical learning theory.</p>



<h2>Analyzing ridge regression through covariance operators</h2>



<p class="justify-text">The most classical problem where regularization by RKHS norms occurs is <em>ridge regression</em>, where, given some observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathcal{X} \times \mathbb{R}\), one minimizes with respect to \(f \in \mathcal{H}\): $$ \frac{1}{n} \sum_{i=1}^n \big( y_i \ – \langle f, \varphi(x_i) \rangle_{\mathcal{H}} \big)^2 +  \lambda \| f\|_{\mathcal{H}}^2.$$</p>



<p class="justify-text">In finite dimensions, the convergence properties are characterized by the (non-centered) covariance matrix \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\), where the expectation is taken with respect to the underlying distribution of the observations \(x_1,\dots,x_n\) (which are assumed independently and identically distributed for simplicity). If \(\mathcal{H} = \mathbb{R}^d\), then \(\Sigma\) is a \(d \times d\) matrix. </p>



<p class="justify-text">For infinite-dimensional \(\mathcal{H}\), the same expression \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\) defines a linear <em>operator</em> from \(\mathcal{H}\) to  \(\mathcal{H}\), so that for \(f,g \in \mathcal{H}\), we have $$\langle f, \Sigma g \rangle_{\mathcal{H}} = \mathbb{E} \big[ \langle f, \varphi(x)\rangle_{\mathcal{H}}\langle g, \varphi(x)\rangle_{\mathcal{H}}\big] = \mathbb{E} \big[ f(x) g(x) \big].$$</p>



<p class="justify-text">The generalization property of ridge regression has been thoroughly studied (see, e.g., [8, 9]), and if there exists \(f_\ast \in \mathcal{H}\) such that \(y_i = \langle f_\ast, \varphi(x_i) \rangle + \varepsilon_i\) for a noise \(\varepsilon_i\) which is independent of \(x_i\), with zero mean and variance equal to \(\sigma^2\), then the expected error on unseen data is asymptotically upper-bounded by $$\sigma^2 + \lambda \| f_\ast\|_{\mathcal{H}}^2 + \frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big].$$ The first term \(\sigma^2\) is the best possible expected performance, the term \(\lambda \| f_\ast\|_{\mathcal{H}}^2\) is usually referred to as the <em>bias</em> term and characterizes the bias introduced by regularizing towards zero, while the third term \(\frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is the <em>variance</em> term, which characterizes the loss in performance due to the observation of only \(n\) observations.</p>



<p class="justify-text">The quantity \({\rm df}(\lambda) = {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is often referred to as the degrees of freedom [10]. When \(\lambda\) tends to infinity, then \({\rm df}(\lambda)\) tends to zero; when \(\lambda\) tends to zero, then \({\rm df}(\lambda)\) tends to the number of non-zero eigenvalues of \(\Sigma\). Thus, in finite dimension, this typically leads to the underlying dimension. Given the usual variance term in \(\sigma^2 \frac{d}{n}\) for ordinary least-squares with \(d\)-dimensional features, \({\rm df}(\lambda)\) is often seen as an implicit number of parameters for kernel ridge regression.</p>



<p class="justify-text">In infinite dimensions, under mild assumptions, there are infinitely many eigenvalues for \(\Sigma\), which form a decreasing sequence \((\lambda_i)_{i \geqslant 0}\) that tends to zero (and is summable, with a sum equal to the trace of \(\Sigma\)). The rate of such a decay is key to understanding the generalization capabilities of kernel methods. With the following classical types of decays:</p>



<ul class="justify-text"><li><em>Polynomial decays</em>: If \(\lambda_i \leqslant \frac{C}{(i+1)^{\alpha}}\) for \(\alpha &gt; 1\), then one can upper bound the sum by an integral as $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big] = \sum_{i=0}^\infty \frac{\lambda_i}{\lambda_i + \lambda} \leqslant \sum_{i=1}^\infty \frac{1}{1  + \lambda i^\alpha / C} \leqslant \int_0^\infty \frac{1}{1+\lambda t^\alpha / C} dt.$$ With the change of variable \(u = \lambda t^\alpha / C\), we get that \({\rm df}(\lambda) = O(\lambda^{-\alpha})\). We can then balance bias and variance with \(\lambda \sim n^{-\alpha/(\alpha+1)}\) and an excess risk proportional to \(n^{-\alpha/(\alpha+1)}\). This type of decay is typical of <a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a>.</li><li><em>Exponential decays</em>: If \(\lambda_i \leqslant {C}e^{-\alpha i}\), for some \(\alpha &gt;0\), we have $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]  \leqslant \sum_{i=0}^\infty \frac{{C}e^{-\alpha i}}{  \lambda + {C}e^{-\alpha i}} \leqslant \int_{0}^\infty \frac{{C}e^{-\alpha t}}{ \lambda + {C}e^{-\alpha t}}dt.$$ With the change of variable \(u = e^{-\alpha t}\), we get an upper bound $$\int_{0}^1 \frac{C}{\alpha}\frac{1}{ \lambda + {C}u}du = \frac{1}{\alpha}\big[ \log(\lambda + C) \ – \log (\lambda) \big] = \frac{1}{\alpha} \log \big( 1+\frac{C}{\lambda} \big).$$ We can then balance bias and variance with \(\lambda \sim 1/n \) and an excess risk proportional to \((\log n) / n \), which is very close to the usual parametric (finite-dimensional) rate in \(O(1/n)\). We will see an example of this phenomenon for the Gaussian kernel.</li></ul>



<p class="justify-text">In order to analyze the generalization capabilities, we consider a measure \(d \mu\) on \(\mathcal{X}\), and the following (non-centered) <em>covariance operator</em> defined above as $$\mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big],$$ which is now an self-adjoint operator from \(\mathcal{H}\) to \(\mathcal{H}\) with a finite trace. The traditional empirical estimator \(\hat\Sigma = \frac{1}{n} \sum_{i=1}^n \varphi(x_i) \otimes \varphi(x_i)\), whose eigenvalues are the same as the eigenvalues of \(1/n\) times the \(n \times n\) kernel matrix of pairwise kernel evaluations (see simulation below).</p>



<p class="justify-text"><strong>Characterizing eigenfunctions.</strong> If \((g_k)\) is the eigenbasis associated to the eigenfunctions of \(\Sigma\), then it has to be an orthogonal family that span the entire space \(\mathcal{H}\) and such that \(\Sigma g_k = \lambda_k g_k\). Applying it to \(\varphi(y) = K(\cdot,y)\), we get $$ \langle K(\cdot,y), \Sigma g_k \rangle_{\mathcal{H}} = \mathbb{E} \big[ K(x,y) g_k(x) \big] = \lambda_k \langle g_k, \varphi(y)\rangle_\mathcal{H} =  \lambda_k g_k(y),$$ which implies that the functions also have to be eigenfunctions of the self-adjoint so-called <em>integral operator</em> \(T\) defined on \(L_2(d\mu)\) as \(T f(y) = \int_{\mathcal{X}} K(x,y) f(y) d\mu(y)\). Below, we will check this property. Note that this other notion of integral operator (defined on \(L_2(d\mu)\) and not in \(\mathcal{H}\)), which has the same eigenvalues and eigenfunctions, is important to deal with mis-specified models (see [9]). Note that the eigenfunctions \(g_k\) are orthogonal for both dot-products in \(L_2(d\mu)\) and \(\mathcal{H}\), but that the normalization to unit norm differs. If \(\| g_k \|_{L_2(d\mu)}=1\) for all \(k \geqslant 0\), then we have \( \| g_k \|^2_\mathcal{H}=  \lambda_k^{-1} \langle g_k, \Sigma g_k \rangle_\mathcal{H} = \lambda_k^{-1}\mathbb{E} [ g_k(x)^2] =\lambda_k^{-1}\) , and thus, \(\| \lambda_k^{1/2} g_k \|_{\mathcal{H}}=1\), and we have the kernel expansion from an orthonormal basis of \(\mathcal{H}\): $$K(x,y) = \sum_{k=0}^\infty\lambda_k g_k(x) g_k(y),$$ which will lead to a new proof for Mehler formula.</p>



<h2>Orthonormal basis for the Gaussian kernel</h2>



<p class="justify-text">Hermite polynomials naturally lead to orthonormal basis of some reproducing kernel Hilbert spaces (RKHS). For simplicity, I will focus on one-dimensional problems, but this extends to higher dimension. </p>



<p class="justify-text"><strong>Translation-invariant kernels.</strong> We consider a function \(q: \mathbb{R} \to \mathbb{R}\) which is integrable, with <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> (note the different normalization than before) which is defined for all \(\omega \in \mathbb{R}\) because of the integrability: $$\hat{q}(\omega) = \int_{\mathbb{R}} q(x) e^{-i \omega x} dx.$$ We consider the kernel $$K(x,y) = q(x-y).$$ It can be check that as soon as  \(\hat{q}(\omega) \in \mathbb{R}_+\)  for all \(\omega \in \mathbb{R}\), then the kernel \(K\) is positive-definite.</p>



<p class="justify-text">For a translation-invariant kernel, we can write using the inverse Fourier transform formula: $$K(x,y) = q(x-y) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{q}(\omega) e^{i \omega ( x- y)} d \omega = \int_{\mathbb{R}} \varphi_\omega(x)^* \varphi_\omega(y) d \omega,$$ with \(\varphi_\omega(x) = \sqrt{\hat{q}(\omega) / (2\pi) } e^{i\omega x}\). Intuitively, for a function \(f: \mathbb{R} \to \mathbb{R}\), with \(\displaystyle f(x) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{f}(\omega)e^{i\omega x} d\omega = \int_{\mathbb{R}} \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\varphi_\omega(x) d\omega\), which is a “dot-product” between the family \((\varphi_\omega(x))_\omega\) and \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big)_\omega\), the squared norm \(\| f\|_{\mathcal{H}}^2\) is equal to the corresponding “squared norm” of \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\Big)_\omega\), and we thus have $$ \| f\|_{\mathcal{H}}^2 = \int_{\mathbb{R}} \Big| \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big|^2 d\omega =  \frac{1}{2\pi} \int_{\mathbb{R}} \frac{ | \hat{f}(\omega) |^2}{\hat{q}(\omega)} d\omega,$$ where \(\hat{f}\) is the Fourier transform of \(f\). While the derivation above is not rigorous, the last expression is.</p>



<p class="justify-text">In this section, I will focus on the Gaussian kernel defined as \(K(x,y) = q(x-y) =  \exp \big( – \alpha ( x- y )^2 \big)\), for which \(\displaystyle \hat{q}(\omega)= \sqrt{\frac{\pi}{\alpha}} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\).</p>



<p class="justify-text">Given that \(\displaystyle \frac{1}{\hat{q}(\omega)} = \sqrt{\frac{\alpha}{\pi}} \exp\big(  \frac{\omega^2}{4 \alpha} \big)= \sqrt{\frac{\alpha}{\pi}} \sum_{k=0}^\infty  \frac{\omega^{2k}}{(4 \alpha)^k k!} \), the penalty \(\|f\|_\mathcal{H}^2\) is a linear combination of squared \(L_2\)-norm of \(\omega^k \hat{f}(\omega)\), which is the squared \(L_2\)-norm of the \(k\)-th derivative of \(f\). Thus, functions in the RKHS are infinitely differentiable, and thus very smooth (this implies that to have the fast rate \((\log n) / n \) above, the optimal regression function has to be very smooth).</p>



<p class="justify-text"><strong>Orthonormal basis of the RKHS</strong>. As seen in Eq. (4), an expansion in an infinite sum is necessary to obtain an orthonormal basis. We have: $$K(x,y) = e^{-\alpha x^2} e^{-\alpha y^2} e^{2 \alpha x y} = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$ Because of Eq. (4), with \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} x^k \exp \big( – \alpha x^2 \big)\), we have a good candidate for an orthonornal basis. Let us check that this is the case. Note that the expansion above alone cannot be used as a proof that \((g_k)\) is an orthonormal basis of \(\mathcal{H}\).</p>



<p class="justify-text">Given the function \(f_k(x) = x^k \exp \big( – \alpha x^2 \big)\), we can compute its Fourier transform as $$ \hat{f}_k(\omega) = i^{-k} ( 4 \alpha)^{-k/2} \sqrt{\frac{\pi}{\alpha}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) .$$ Indeed, we have, from Rodrigues’ formula, $$H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) =(-1)^k (4 \alpha)^{k/2} \frac{d^k}{d \omega^k}\big[ \exp\big( – \frac{\omega^2}{4 \alpha} \big) \big],$$ and thus its inverse Fourier transform is equal to \((ix)^k\) times the one of \((-1)^k (4 \alpha)^{k/2} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\), which is thus equal to \((-i)^k (4 \alpha)^{k/2} \sqrt{ \alpha / \pi }e^{-\alpha x^2} \), which leads to the Fourier transform formula above.</p>



<p class="justify-text">We can now compute the RKHS dot products, to show how to obtain the orthonormal basis described in [11]. This leads to $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2}  \int_{\mathbb{R}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) H_\ell \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big)  d\omega,$$ which leads to, with a change of variable $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2} \sqrt{4 \alpha} \int_{\mathbb{R}} H_k (u) H_\ell (u)  \exp(-u^2) du,$$ which is equal to zero if \(k \neq \ell\), and equal to \(\frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k} \sqrt{4 \alpha} \sqrt{\pi} 2^k k!   = ( 2 \alpha)^{-k} k!\) if \(k = \ell\). Thus the sequence \((f_k)\) is an orthogonal basis of the RKHS, and the sequence \((g_k)\) defined as \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} f_k(x)\) is an orthonormal basis of the RKHS, from which, using the expansion as in Eq. (4), we recover the expansion: $$K(x,y) = \sum_{k=0}^\infty g_k(x) g_k(y) = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$</p>



<p class="justify-text">This expansion can be used to approximate the Gaussian kernel by finite-dimensional explicit feature spaces, by just keeping the first basis elements (see an application to optimal transport in [<a href="https://arxiv.org/pdf/1810.10046">12</a>], with an improved behavior using an adaptive low-rank approximation through the Nyström method in [<a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">13</a>]).</p>



<h2>Eigenfunctions for the Gaussian kernels</h2>



<p class="justify-text">In order to obtain explicit formulas for the eigenvalues of the covariance operator, we need more than a mere orthonormal basis, namely an eigenbasis.</p>



<p class="justify-text">An orthogonal basis will now be constructed with arguably better properties as it is also an orthonormal basis for both the RKHS and \(L_2(d\mu)\) for a Gaussian measure, that diagonalizes the integral operator associated to this probability measure, as well as the covariance operator.</p>



<p class="justify-text">As seen above, we simply need an orthogonal family \((f_k)_{k \geqslant 0}\), such that given a distribution \(d\mu\),  \((f_k)_{k \geqslant 0}\) is a family in \(L_2(d\mu)\) such that $$\int_{\mathbb{R}} f_k(x) K(x,y) d\mu(x) = \lambda_k f_k(y), \tag{5}$$ for eigenvalues \((\lambda_k)\). In the next paragraph, we will do exactly this for the Gaussian kernel \(K(x,y) = e^{-\alpha (x-y)^2}\) for \(\alpha = \frac{\rho}{1- \rho^2}\) for some \(\rho \in (0,1)\); this particular parameterization in \(\rho\) is to make the formulas below not (too) complicated.</p>



<p class="justify-text">With \(f_k(x) = \frac{1}{\sqrt{N_k}} H_k(x) \exp \Big( – \frac{\rho}{1+\rho} x^2 \Big)\), where \(N_k = {2^k k!} \sqrt{ \frac{1-\rho}{1+\rho}}\), then \((f_k)_{k \geqslant 0}\) is an <em>orthonormal</em> basis for \(L_2(d\mu)\) for \(d\mu\) the Gaussian distribution with mean zero and variance \(\frac{1}{2} \frac{1+\rho}{1-\rho}\) (this is a direct consequence of the orthogonality property of Hermite polynomials).</p>



<p class="justify-text">Moreover, the moment of the Hermite polynomial in Eq. (3) exactly leads to Eq. (5) for the chosen kernel and \(\lambda_k = (1-\rho) \rho^k\). Since the eigenvalues sum to one, and the trace of \(\Sigma\) is equal to one (as a consequence of \(K(x,x)=1\)), the family \((f_k)\) has to be a basis of \(\mathcal{H}\).</p>



<p>From properties of the eigenbasis, since \((f_k)\) is an orthonormal eigenbasis of \(L_2(d\mu)\) and the eigenvalues are \(\lambda_k = (1-\rho)\rho^k\),  we get: $$ K(x,y) = \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sum_{k=0}^\infty (1-\rho)\rho^k f_k(x) f_k(y),$$ which is exactly the Mehler formula, and thus we obtain an alternative proof.</p>



<p class="justify-text">We then get an explicit basis and the exponential decay of eigenvalues, which was first outlined by [2]. See an application to the estimation of the Poincaré constant in [<a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">14</a>] (probably a topic for another post in a few months).</p>



<p class="justify-text"><strong>Experiments.</strong> In order to showcase the exact eigenvalues of the expectation \(\Sigma\) (for the correct combination of Gaussian kernel and Gaussian distribution), we compare the eigenvalues with the ones of the empirical covariance operator \(\hat\Sigma\), for various values of the number of observations. We see that as \(n\) increases, the empirical eigenvalues match the exact ones for higher \(k\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="363" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/gaussian_kernel-1.gif" class="wp-image-4889" height="307" />Eigenvalues of the covariance operator \(\Sigma\) (“expectation”) compared to the ones of the empirical covariance operator \(\hat\Sigma\), averaged over 20 replications (“empirical”), for several values of \(n\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I only presented applications of Hermite polynomials to the Gaussian kernel, but these polynomials appear in many other areas of applied mathematics, for other types of kernels within machine learning such as dot-product kernels [3], in random matrix theory (see <a href="https://terrytao.wordpress.com/2011/02/20/topics-in-random-matrix-theory/">here</a>), in statistics for <a href="https://en.wikipedia.org/wiki/Edgeworth_series">Edgeworth expansions</a>, and of course for <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature">Gauss-Hermite quadrature</a>.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien and Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] George Neville Watson. <a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">Notes on Generating Functions of Polynomials: (2) Hermite Polynomials</a>. <em>Journal of the London Mathematical Soc</em>iety, 8, 194-199, 1933.<br />[2] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. <a href="https://publications.aston.ac.uk/id/eprint/38366/1/NCRG_97_011.pdf">Gaussian regression and optimal finite dimensional linear models</a>. In <em>Neural Networks and Machine Learning</em>. Springer-Verlag, 1998.<br />[3] A. Daniely, R. Frostig, and Y. Singer. <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</a>. In Advances In Neural Information Processing Systems, 2016.<br />[4] Gabor Szegö. <em>Orthogonal polynomials</em>. American Mathematical Society, 1939.<br />[5] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br />[6] Bernhard Schölkopf, Alexander J. Smola. <a href="https://mitpress.mit.edu/books/learning-kernels">Learning with kernels: support vector machines, regularization, optimization, and beyond</a>. MIT Press, 2002.<br />[7] John Shawe-Taylor, Nello Cristianini. <em>Kernel methods for pattern analysis</em>. Cambridge University Press, 2004.<br />[8] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/content/pdf/10.1007/s10208-006-0196-8.pdf">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7.3: 331-368, 2007.<br />[9] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>. Vol. 1. No. 10. Springer series in statistics, 2001.<br />[10] Trevor Hastie and Robert Tibshirani. <em>Generalized Additive Models</em>. Chapman &amp; Hall, 1990.<br />[11] Ingo Steinwart, Don Hush, and Clint Scovel. <a href="http://[PDF] ieee.org">An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels</a>. <em>IEEE Transactions on Information Theory</em>, 52.10:4635-4643, 2006.<br />[12] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://arxiv.org/pdf/1810.10046">Approximating the quadratic transportation metric in near-linear time</a>. Technical report arXiv:1810.10046, 2018.<br />[13] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">Massively scalable Sinkhorn distances via the Nyström method</a>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.<br />[14] Loucas Pillaud-Vivien, Francis Bach, Tony Lelièvre, Alessandro Rudi, Gabriel Stoltz. <a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">Statistical Estimation of the Poincaré constant and Application to Sampling Multimodal Distributions</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),</em> 2020.<br />[15] Nachman Aronszajn. <a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">Theory of Reproducing Kernels</a>. <em>Transactions of the American Mathematical Society</em>, 68(3): 337–404, 1950.</p>



<h2>Proof of properties of Hermite polynomials</h2>



<p class="justify-text">In this small appendix, I give “simple” proofs (that sometimes require knowledge of <a href="https://en.wikipedia.org/wiki/Complex_analysis">complex analysis</a>) to the properties presented above.</p>



<p class="justify-text"><strong>Generating function.</strong> We have, using <a href="https://en.wikipedia.org/wiki/Residue_theorem">residue theory</a>, $$H_k(x)=(-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big] = (-1)^k \frac{k!}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{(z-x)^{k+1}}dz, $$ where \(\gamma\) is a contour in the complex plane around \(x\). This leads to, for any \(t\) (here, we ignore on purpose the summability issues, for more details, see [4, Section 5.5]): $$ \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \sum_{k=0}^\infty \frac{t^k} {(x-z)^{k}}dz, $$ which can be simplified using the sum of the geometric series, leading to $$\frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \frac{z-x}{z-x- t} dz =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}} {z-x- t} dz.$$ Using the first-order residue at \(x+t\). This is thus equal to \(e^{x^2-(t+x)^2} = e^{-t^2 + 2tx}\), which is exactly the generating function statement from Eq. (2).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> We can prove through integration by parts, but there is a nicer proof through the generating function. Indeed, with $$ a_{k \ell} = \int_{-\infty}^{+\infty} e^{-x^2} H_k(x) H_\ell(x) dx, $$ for \(k, \ell \geqslant 0\), we get $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} = \int_{-\infty}^{+\infty}e^{-x^2}\Big(  \sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!}  H_k(x) H_\ell(x) \Big) dx.$$ Using the generating function, this leads to $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} =  \int_{-\infty}^{+\infty} e^{-x^2 + 2xu-u^2 + 2xt – t^2} dx= e^{2uv} \int_{-\infty}^{+\infty} e^{-(x-u-v)^2}dx, $$ which can be computed explicitly using normalization constants of the Gaussian distribution, as \( \sqrt{\pi} e^{2uv} = \sqrt{\pi} \sum_{k=0}^\infty \frac{ (2  u v)^k}{k!},\) leading to all desired orthogonality relationships using the uniqueness of all coefficients for factors \(t^k u^\ell\).</p>



<p class="justify-text"><strong>Recurrence relationship.</strong> Taking the derivative of the generating function with respect to \(t\), one gets \( \displaystyle (2x-2t) e^{2tx-t^2} = \sum_{k=0}^\infty \frac{t^{k-1}}{(k-1)!} H_k(x),\) which is equal to (using again the generating function) \(\displaystyle \sum_{k=0}^\infty \frac{t^{k}}{k!} 2x H_k(x) \ – \sum_{n=0}^\infty \frac{t^{k+1}}{k!} 2 H_k(x).\) By equating the coefficients for all powers of \(t\), this leads to the desired recursion in Eq. (1).</p>



<p class="justify-text"><strong>Fourier transform.</strong> Again using the generating function, written $$ e^{-x^2/2 + 2xt – t^2} = \sum_{k=0}^\infty \frac{t^k}{k!} e^{-x^2/2} H_k(x), $$ we can take Fourier transforms and use the fact that the Fourier transform of \(e^{-x^2/2}\) is itself (for the chosen normalization), and then equate coefficients for all powers of \(t\) to conclude (see more details <a href="https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions_as_eigenfunctions_of_the_Fourier_transform">here</a>).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions.</strong> We finish the appendix by proving Eq. (3). We consider computing for any \(t\), $$\sum_{k=0}^\infty \rho^k \frac{t^k}{k!} H_k (y) = e^{2\rho t y – \rho^2 t^2},$$ using the generating function from Eq. (2). We then compute $$A=\int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) dx = \int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \exp( 2tx – t^2) dx.$$ We then use \( \frac{(x-\rho y)^2}{1-\rho^2} – 2tx + t^2 = \frac{x^2}{1-\rho^2}  – \frac{2x[ t(1-\rho^2) + \rho y]}{1-\rho^2}  + t^2 + \frac{\rho^2 y^2}{1-\rho^2}\), leading to $$A = \sqrt{\pi} \sqrt{1-\rho^2} \exp\Big( -t^2 – \frac{\rho^2 y^2}{1-\rho^2} +(1-\rho^2) \big( t + \frac{\rho y}{1-\rho^2} \big)^2 \Big) = \sqrt{\pi} \sqrt{1-\rho^2} e^{2\rho t y – \rho^2 t^2}.$$ By equating powers of \(t\), this leads to Eq. (3).</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/hermite-polynomials/"><span class="datestr">at October 08, 2020 07:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/151">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/151">TR20-151 |  Pseudobinomiality of the Sticky Random Walk | 

	Venkatesan Guruswami, 

	Vinayak Kumar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Random walks on expanders are a central and versatile tool in pseudorandomness.  If an arbitrary half of the vertices of an expander graph are marked, known Chernoff bounds for expander walks imply that the number $M$ of marked vertices visited in a long $n$-step random walk strongly concentrates around the expected $n/2$ value. Surprisingly, it was recently shown that the parity of $M$ also has exponentially small bias.

Is there a common unification of these results? What other statistics about $M$ resemble the binomial distribution (the Hamming weight of a random $n$-bit string)? To gain insight into such questions, we analyze a simpler model called the sticky random walk. This model is a natural stepping stone towards understanding expander random walks, and we also show that it is a necessary step. The sticky random walk starts with a random bit and then each subsequent bit independently equals the previous bit with probability $(1+\lambda)/2$. Here $\lambda$ is the proxy for the expander's (second largest) eigenvalue.
    
Using Krawtchouk expansion of functions, we derive several probabilistic results about the sticky random walk. We show an asymptotically tight $\Theta(\lambda)$ bound on the total variation distance between the (Hamming weight of the) sticky walk and the binomial distribution. We prove that the correlation between the majority and parity bit of the sticky walk is bounded by $O(n^{-1/4})$. This lends hope to unifying Chernoff bounds and parity concentration, as well as establishing other interesting statistical properties, of expander random walks.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/151"><span class="datestr">at October 08, 2020 02:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/150">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/150">TR20-150 |  Almost-Everywhere Circuit Lower Bounds from Non-Trivial Derandomization | 

	Lijie Chen, 

	Xin Lyu, 

	Ryan Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In certain complexity-theoretic settings, it is notoriously difficult to prove complexity separations which hold almost everywhere, i.e., for all but finitely many input lengths. For example, a classical open question is whether $\mathrm{NEXP} \subset \mathrm{i.o.-}\mathrm{NP}$; that is, it is open whether nondeterministic exponential time computations can be simulated on infinitely many input lengths by $\mathrm{NP}$ algorithms. This difficulty also applies to Williams' algorithmic method for circuit lower bounds [Williams, J. ACM 2014]. In particular, although [Murray and Williams, STOC 2018] proved $\mathrm{NTIME}[2^{\mathrm{polylog}(n)}] \not\subset \mathrm{ACC}^0$, it has remained an open problem to show that $\mathrm{E}^{\mathrm{NP}}$ ($2^{O(n)}$ time with an $\mathrm{NP}$ oracle) is not contained in $\mathrm{i.o.-}\mathrm{ACC}^0$. 
	
In this paper, we show how many infinitely-often circuit lower bounds proved by the algorithmic method can be adapted to establish almost-everywhere lower bounds. 

- We show there is a function $f \in \mathrm{E}^{\mathrm{NP}}$ such that for all sufficiently large input lengths $n$ and $\varepsilon \leq o(1)$, $f$ cannot be $(1/2+2^{-n^{\varepsilon}})$-approximated by $2^{n^\varepsilon}$-size $\mathrm{ACC}^0$ circuits on inputs of length $n$, improving lower bounds in [Chen and Ren, STOC 2020] and [Viola, ECCC 2020]. 

- We construct rigid matrices in $\mathrm{P}^{\mathrm{NP}}$ for all but finitely many inputs, rather than infinitely often as in [Alman and Chen, FOCS 2019] and [Bhangale et al., FOCS 2020]. 

- We show there are functions in $\mathrm{E}^{\mathrm{NP}}$ requiring constant-error probabilistic degree at least $\Omega(n/\log^2 n)$ for all large enough $n$, improving an infinitely-often separation of [Viola, ECCC 2020].
	
Our key to proving almost-everywhere worst-case lower bounds is a new ``constructive'' proof of an NTIME hierarchy theorem proved by [Fortnow and Santhanam, CCC 2016], where we show for every ``weak'' nondeterminstic algorithm (with smaller running-time and short witness), a ``refuter algorithm'' exists that can construct ``bad'' inputs for the hard language. We use this refuter algorithm to construct an almost-everywhere hard function. To extend our lower bounds to the average case, we prove a new XOR Lemma based on approximate linear sums, and combine it with the PCP-of-proximity applications developed in [Chen and Williams, CCC 2019] and [Chen and Ren, STOC 2020]. As a byproduct of our new XOR Lemma, we obtain a nondeterministic pseudorandom generator for poly-size $\mathrm{ACC}^0$ circuits with seed length $\mathrm{polylog}(n)$, which resolves an open question in [Chen and Ren, STOC 2020].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/150"><span class="datestr">at October 08, 2020 01:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7346788885933681031">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html">Revisiting the Continuum Hypothesis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I have been thinking about CH lately for two reasons</p><p>1) I reread the article</p><p>Hilbert's First Problem: The Continuum Hypothesis by Donald Martin from <b>Proceedings of Symposia </b> i<b>n Pure Mathematics: Mathematical developments arising from Hilbert Problems</b>. 1976. (For a book review of the symposia and, <b>The Honor Class</b>, also about Hilbert's problems, see <a href="https://www.cs.umd.edu/users/gasarch/bookrev/44-4.pdf">here</a>.)</p><p>The article takes the point of view that CH CAN have an answer. He discusses large cardinals (why assuming they exist is plausible, but alas, that assumption does not seem to resolve CH) and Projective Det.  (why assuming it is true is plausible, but alas, that assumption does not seem to resolve CH).</p><p>(A set A \subseteq {0,1}^omega is DETERMINED if either Alice or Bob has a winning strategy in the following non-fun game: they alternate picking bits a_1, b_1, a_2, b_2, ... with Alice going first. If a_1 b_1 a_2 b_2... IS IN A then Alice wins, IF NOT then Bob wins. Martin showed that all Borel sets are determined. Proj Det is the statement that all projections of Borel sets are determined. AD is the axiom that ALL sets A are determined. It contradicts AC.)</p><p>But what really inspired this post is the last paragraph:</p><p><i>Throughout the latter part of my discussion, I have been assuming a naive and uncritical attitude towards CH. While this <b>is</b> in fact my attitude, I by no means wish to dismiss the opposite viewpoint.  Those that argue that the concept of set is not sufficiently clear to fix the truth-value of CH have a position that is at present difficult to assail. As long as no new axiom is found which decides CH, their case will continue to grow stronger, and our assertions that the meaning of CH is clear will sound more and more empty.</i></p><p>2) Scott Aaronson mentioned in a blog post (see <a href="https://www.scottaaronson.com/blog/?p=4962">here</a>) that  he has read and understood the proof that CH is independent of set theory.</p><p>SO, this seemed like a good time to revisit thoughts on CH.</p><p> I took a very short poll, just two people, about CH: Stephen Fenner (in a perfect world he would be a set theorists) and Scott Aaronson (having JUST read the proof that CH is ind.  he has thought about it recently).</p><p>Here are some thoughts of theirs and mine</p><p>1) All three of us are Platonists with regard to the Naturals (I was surprised to find recently that there are people who are not!) but not with regard to the reals.  So we would be OKAY with having CH have no answer.</p><p>2) All three of us  agree that it would be nice if SOME axiom was both</p><p>a) Intuitively appealing or aesthetically appealing ,  and</p><p>b) resolved CH.</p><p>I always thought that (a) would be the hard part-- or at least getting everyone (not sure who we are talking about) to AGREE on a new axiom. But even getting an axiom to resolve CH seems hard.  Large cardinals don't seem to do it, and various forms of Determinacy don't seem to do it.</p><p>Scott reminded me of Freiling's Axiom of Symmetry (see <a href="https://en.wikipedia.org/wiki/Freiling%27s_axiom_of_symmetry">here</a>) which IS intuitive and DOES resolve CH (its false) though there are problems with it--- a minor variant   of it contradicts AC (I am QUITE FINE with that since AC implies Banach-Tarski which Darling says shows `Math is broken'.)</p><p>Stephen recalled some of Hugh Woodin's opinions of CH, but Hugh seems to have changed his mind from NOT(CH): 2^{aleph_0} = aleph_2, to CH:  2^{aleph_0} = aleph_1.(See <a href="https://www.jstor.org/stable/24569622?seq=1#metadata_info_tab_contents">here</a>.)</p><p>3) All three of would be okay with V=L, though note that this would put many set theorists out of work. All the math that applies to the real world would still be intact.  I wonder if in an alternative history the reaction to Russell's paradox would be a formulation of set theory where V=L. We would KNOW that CH is true, KNOW that AC is true. We would know a lot about L but less about forcing.</p><p>4) Which Geometry is true: Euclidian, Riemannian, others? This is now regarded as a silly question: Right Tool, Right Job! If you build a bridge use Euclid. If you are doing astronomy use Riemann. Might Set Theory go the same way? It would be AWESOME if Scott Aaronson found some quantum thing where assuming 2^{aleph_0} = aleph_2 was the right way to model it.</p><p>5) If I was more plugged into the set theory community I might do a poll of set theorists, about CH. Actually, someone sort-of already has. Penelope Maddy has two excellent and readable articles where she studies what set theorists believe and why.</p><p><b>Believing The Axioms I</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms1.pdf">here</a></p><p><b>Believing The Axioms II</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms2.pdf">here</a></p><p>Those articles were written in 1988. I wonder if they need an update.</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html"><span class="datestr">at October 08, 2020 01:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1420">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1420">News for September 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Apologies dear readers for the late posting. The beginning of the school year is always frenzied, and the pandemic has only added to that frenzy. We have an exciting September, with four papers on graph property testing, <s>one</s> two papers on distribution testing, and one paper that connects both topics.</p>



<p><em>(Ed: we normally scan through ECCC and arXiv, but are happy to post about papers that appear elsewhere. Thanks to the reader who pointed out a relevant COLT 2020 paper.)</em></p>



<p><strong>Estimation of Graph Isomorphism Distance in the Query World</strong> by Sourav Chakraborty, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://eccc.weizmann.ac.il/report/2020/135/">ECCC</a>). Graph isomorphism is about as fundamental as it gets, and this papers studies approximating the graph isomorphism distance for dense graphs. There is a known graph \(G_k\) (with \(n\) vertices). The algorithm is given query access to an input graph \(G_u\) and needs to approximate the number of edge inserts/deletes required to make the graphs isomorphic. This is the tolerant testing version; the property testing version is known to be doable in \(\widetilde{O}(\sqrt{n})\) queries (<a href="https://epubs.siam.org/doi/abs/10.1137/070680795?journalCode=smjcat">Matsliah-Fischer</a>). The main insight of this paper is to relate the tolerant testing complexity to a distribution testing problem. Consider distributions over the \(\{0,1\}^n\) defined by multisets of \(n\) hypercube points. Our aim is to estimate the earthmover distance between a known distribution and an unknown distribution. Interestingly, the query model is different: one can sample the underlying multisets <em>without</em> replacement. It turns out that the optimal complexity of this problem is (upto polylog factors) is the same as the optimal complexity of tolerant testing of graph isomorphism. A direct corollary is that the isomorphism distance can be approximated upto additive \(\epsilon n^2\) using \(\widetilde{O}(n)\) samples. This equivalence also gives an alternate proof for lower bounds for property testing graph isomorphism.</p>



<p><strong>Robustly Self-Ordered Graphs: Constructions and Applications to Property Testing </strong>by Oded Goldreich and Avi Wigderson (<a href="https://eccc.weizmann.ac.il/report/2020/149/">ECCC</a>). Let’s start from the application. The aim is to prove the following property testing lower bounds for the bounded-degree graph setting: an exponential separation between tolerant and vanilla testing, and finding an efficiently decidable property (in polynomial time) that cannot be property tested in sublinear time. For binary strings, results of this form are known. Can these be “ported” to the bounded-degree graph world? Can we construct graphs such that adjacency queries reduce to bit queries in strings? Naturally, one can simply represent the adjacency list as a string and treat graph queries as bit queries. But the problem is that of isomorphisms: different bit strings could represent the same graph and therefore, the different bit strings must have the same status with respect to the underlying property. The key insight in this paper is to introduce <em>robustly self-ordered graph</em>s, as a tool to port bit string property testing lower bounds to bounded-degree graphs. Such graphs essentially have a unique (identity) automorphism, even after a few edge insert/deletes. The actual definition is more technical, but that is the essence. The main result is an explicit construction of such graphs, from which the lower bound can be ported directly through a convenient lemma.</p>



<p><strong>Modifying a Graph’s Degree Sequence and the Testablity of Degree Sequence Properties </strong>by Lior Gishboliner (<a href="https://arxiv.org/pdf/2009.12697.pdf">arXiv</a>). A sequence of numbers \(D = (d_1, d_2, \ldots, d_n)\) is graphic if there exists an undirected graph on \(n\) vertices whose degrees are precisely the numbers of the sequence. Graphical sequences have been characterized by classic results of <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem">Erdös-Gállai</a> and <a href="https://en.wikipedia.org/wiki/Havel%E2%80%93Hakimi_algorithm">Havel-Hakimi</a>. This paper first proves the following theorem. Suppose a graphic sequence \(D’\) has \(l_1\)-distance at most \(\delta\) from the degree sequence \(D\) of a graph \(G\). Then, there exists a graph \(G’\) with degree sequence \(D’\) such that the (dense graph) distance between \(G\) and \(G’\) is \(O(\sqrt{\delta})\). This theorem is used to prove an interesting property testing result. Let \(\mathcal{D}\) be a subset of graphic sequences that are closed under permutation. Let \(\mathcal{G}\) be the set of graphs that have a degree sequence in \(\mathcal{D}\). Then \(\mathcal{G}\) can be tested in \(poly(1/\epsilon)\) queries.</p>



<p><strong>Sampling an Edge Uniformly in Sublinear Time</strong> by Jakub Têtek (<a href="https://arxiv.org/pdf/2009.11178.pdf">arXiv</a>). In the general model for sublinear algorithms on graphs, an important choice is whether one allows uniform random edge queries. A natural question is whether such queries can simulated efficiently, using only random vertex, degree, and neighbor queries. This problem appears somewhat implicitly in previous sublinear subgraph counting algorithms, and <a href="http://drops.dagstuhl.de/opus/volltexte/2019/10628/pdf/LIPIcs-ICALP-2019-52.pdf">Eden-Ron-Rosenbaum</a> study it explicitly. They prove that one can sample from an \(\epsilon\)-approximate uniform distribution (over edges) using \(O(n/\sqrt{\epsilon m})\) samples. The problem of sampling from exactly the uniform distribution is left open. Until this paper. The main result shows that by modifying the Eden-Ron-Rosenbaum algorithm parameters, one can generate edge samples from an \(\epsilon\)-approximate uniform distribution using \(O((n/\sqrt{m})\log \epsilon^{-1})\) samples. The exact uniform distribution is achieved by setting \(\epsilon = 1/n\), to get a sample complexity of \(O((n\log n)/\sqrt{m})\).</p>



<p><strong>Faster Property Testers in a Variation of the Bounded Degree Model </strong>by Isolde Adler and Polly Fahey (<a href="https://arxiv.org/pdf/2009.07770.pdf">arXiv</a>). The setting of bounded-degree graph property testing naturally extends to bounded-degree relational databases, which can be thought of as “directed” hypergraphs. This is an interesting new direction of research, that combines property testing with database theory (see <a href="https://core.ac.uk/download/pdf/157699299.pdf">Adler-Harwath</a> and <a href="https://dl.acm.org/doi/10.1145/3294052.3319679">Chen-Yoshida</a>). One of the main contributions of this work is to consider another notion of distance: edge and vertex inserts/deletes. This is a natural extension, and we can now compare distances between graphs/databases with different numbers of vertices. The main result is that, under this notion of distance, a large class of properties can be tested in constant running time on databases with bounded degree and treewidth. Specifically, any property expressible in Counting Monadic Second-Order Logic (CMSO) can be tested in constant time. Previous results by Alder-Harwath showed that such properties can be tested (under the standard distance notion) in constant queries, but polylogarithmic time. </p>



<p><strong>Optimal Testing of Discrete Distributions with High Probability</strong> by Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, John Peebles, and Eric Price (<a href="https://arxiv.org/abs/2009.06540">arXiv</a>, <a href="https://eccc.weizmann.ac.il/report/2020/140/">ECCC</a>). The focus of this paper is distribution testing in the “high probability” regime, where we wish the error of the tester to be \(&lt; \delta\). Typically, most results just get an error of at most \(1/3\), from which standard probabilistic boosting would tack on an extra \(O(\log 1/\delta)\) factor. In standard TCS settings, one doesn’t focus on optimizing this dependence, but in statistics, there is significant focus on the optimal sample complexity. And indeed, for practical applications, it is crucial to have sharp bounds on the right number of samples required for hypothesis testing. The paper also argues that getting the optimal sample complexity requires new algorithms, even for uniformity testing. There are optimal results given for closeness and independence testing. The optimal sample complexity only pays a multiplicative factor of \(\log^{1/3} (1/\delta)\) or \(\log^{1/2}(1/\delta)\) over the optimal bound for constant error (with other additive terms depending on \(\log(1/\delta)\)).</p>



<p><strong>Bessel Smoothing and Multi-Distribution Property Estimation</strong> by Yi Hao and Ping Li (<a href="http://proceedings.mlr.press/v125/hao20a.html">COLT 2020</a>). Let us consider some standard (tolerant) distribution testing questions, phrases as approximation algorithms. Given sample access to two distributions \(p\) and \(q\) over \([n]\), we may wish to estimate the \(l_1\)-distance, \(l_2\)-distance, relative entropy, etc. between these distributions. One can phrases this problem abstractly as estimating \(\sum_{i \in [n]} f(p_i, q_i)\), where \(f\) is some explicit function. This papers shows that for any 1-Lipschitz function \(f\) that satisfies some “regularity” property, the sum \(\sum_{i \in [n]} f(p_i, q_i)\) can be \(\epsilon\)-approximated with \(O(\epsilon^{-3}n/\sqrt{\log n})\) samples (apologies to the authors to replacing their \(k\) with the more familiar \(n\) for our readers). Thus, we can get sublinear sampling complexity for a very general class of estimation problems. Moreover, this was actually the simplest setting consider in the paper. One can deal with such functions of \(d\) distributions, not just two distributions. One of the corollaries of the theorems is a sublinear tolerant tester for the property of being a mixture of distributions.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1420"><span class="datestr">at October 07, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=488">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/10/07/tcs-talk-wednesday-october-14-jayadev-acharya-cornell-university/">TCS+ talk: Wednesday, October 14 — Jayadev Acharya, Cornell University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Jayadev Acharya</strong> from Cornell University will speak about  “<em>Distributed Statistical Inference under Local Information Constraints</em> ” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: We consider statistical inference tasks in a distributed setting where access to data samples is subjected to strict “local constraints,” through a unified framework that captures communication limitations and (local) privacy constraints as special cases. We study estimation (learning) and goodness-of-fit (testing) for both discrete and high-dimensional distributions. Our goal is to understand how the sample complexity increases under the information constraints.<br /><br />In this talk we will provide an overview of this field and a sample of some of our results. We will discuss the role of (public) randomness  and interactivity in information-constrained inference, and make a case for thinking about randomness and interactivity as resources.<br /><br />The work is part of a long-term ongoing collaboration with Clément Canonne (IBM Research) and Himanshu Tyagi (IISc), and includes works done with Cody Freitag (Cornell), Yanjun Han (Stanford), Yuhan Liu (Cornell), and Ziteng Sun (Cornell). </p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/10/07/tcs-talk-wednesday-october-14-jayadev-acharya-cornell-university/"><span class="datestr">at October 07, 2020 08:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/neurips2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/neurips2020/">Conference Digest - NeurIPS 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://neurips.cc/Conferences/2020">NeurIPS 2020</a> is the biggest conference on machine learning, with tons of content on differential privacy in many different forms.
We were able to find two workshops, a competition, and 31 papers. 
This was just going off the preliminary <a href="https://nips.cc/Conferences/2020/AcceptedPapersInitial">accepted papers list</a>, so it’s possible that we might have missed some papers on differential privacy – please let us know!
We will update this post later, once all the conference material (papers and videos) are publicly available.</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li>
    <p><a href="https://ppml-workshop.github.io/">Privacy Preserving Machine Learning - PriML and PPML Joint Edition</a></p>
  </li>
  <li>
    <p><a href="http://icfl.cc/SpicyFL/2020">International Workshop on Scalability, Privacy, and Security in Federated Learning (SpicyFL 2020)</a></p>
  </li>
</ul>

<h2 id="competitions">Competitions</h2>

<ul>
  <li><a href="https://www.vanderschaar-lab.com/privacy-challenge/">Hide-and-Seek Privacy Challenge: Synthetic Data Generation vs. Patient Re-identification with Clinical Time-series Data</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05665">A Computational Separation between Private Learning and Online Learning</a><br />
<a href="https://cs-people.bu.edu/mbun/">Mark Bun</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.05975">Adversarially Robust Streaming Algorithms via Differential Privacy</a><br />
<a href="http://u.cs.biu.ac.il/~avinatan/">Avinatan Hasidim</a>, <a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://research.google/people/YossiMatias/">Yossi Matias</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.07709">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a><br />
<a href="https://www.ccis.northeastern.edu/home/jagielski/">Matthew Jagielski</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://www.ccs.neu.edu/home/alina/">Alina Oprea</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.11707">Breaking the Communication-Privacy-Accuracy Trilemma</a><br />
<a href="https://web.stanford.edu/~wnchen/index.html">Wei-Ning Chen</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://web.stanford.edu/~aozgur/">Ayfer Ozgur</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06618">CoinPress: Practical Private Mean and Covariance Estimation</a><br />
<a href="https://sravb.github.io/">Sourav Biswas</a>, <a href="https://yihedong.me/">Yihe Dong</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.08007">Differentially Private Clustering: Tight Approximation Ratios</a><br />
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a></p>
  </li>
  <li>
    <p><a href="http://web.mit.edu/dubeya/www/files/dp_linucb_20.pdf">Differentially-Private Federated Contextual Bandits</a><br />
<a href="http://web.mit.edu/dubeya/www/">Abhimanyu Dubey</a>, <a href="https://www.media.mit.edu/people/sandy/overview/">Alex Pentland</a></p>
  </li>
  <li>
    <p>Faster Differentially Private Samplers via Rényi Divergence Analysis of Discretized Langevin MCMC<br />
<a href="https://people.eecs.berkeley.edu/~arunganesh/">Arun Ganesh</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.08265">GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators</a><br />
<a href="https://cispa.de/en/people/dingfan.chen">Dingfan Chen</a>, <a href="https://tribhuvanesh.github.io/">Tribhuvanesh Orekondy</a>, <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a></p>
  </li>
  <li>
    <p>Improving Sparse Vector Technique with Renyi Differential Privacy<br />
<a href="https://jeremy43.github.io/">Yuqing Zhu</a>, <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.10630">Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms</a><br />
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="https://web.stanford.edu/~jduchi/">John Duchi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.13660">Learning discrete distributions: user vs item-level privacy</a><br />
<a href="https://www.ece.cornell.edu/research/grad-students/yuhan-liu">Yuhan Liu</a>, <a href="http://theertha.info/">Ananda Theertha Suresh</a>, <a href="http://felixyu.org/">Felix Xinnan Yu</a>, <a href="https://research.google/people/author11555/">Sanjiv Kumar</a>, <a href="https://research.google/people/author125/">Michael D Riley</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.00331">Learning from Mixtures of Private and Public Populations</a><br />
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, <a href="http://web.cse.ohio-state.edu/~nandi.10/">Anupama Nandi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.00701">Locally Differentially Private (Contextual) Bandits Learning</a><br />
<a href="https://scholar.google.com/citations?user=Bw-WdyUAAAAJ">Kai Zheng</a>, <a href="https://tianle.website/">Tianle Cai</a>, <a href="https://www.weiranhuang.com/">Weiran Huang</a>, <a href="http://www.ee.columbia.edu/~zgli/">Zhenguo Li</a>, <a href="http://www.liweiwang-pku.com/">Liwei Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.12601">Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms</a><br />
<a href="https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/berrett/">Thomas Berrett</a>, <a href="http://cbutucea.perso.math.cnrs.fr/">Cristina Butucea</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.01980">On the Equivalence between Online and Private Learnability beyond Binary Classification</a><br />
<a href="https://scholar.google.com/citations?user=ajqlbHUAAAAJ">Young Hun Jung</a>, <a href="https://scholar.google.com/citations?user=5xt0ba0AAAAJ&amp;hl=en">Baekjin Kim</a>, <a href="https://ambujtewari.github.io/">Ambuj Tewari</a></p>
  </li>
  <li>
    <p>Optimal Private Median Estimation under Minimal Distributional Assumptions<br />
<a href="https://tzamos.com/">Christos Tzamos</a>, <a href="http://www.cs.columbia.edu/~emvlatakis/">Emmanouil-Vasileios Vlatakis-Gkaragkounis</a>, <a href="http://www.mit.edu/~izadik/">Ilias Zadik</a></p>
  </li>
  <li>
    <p>Permute-and-Flip: A new mechanism for differentially-private selection<br />
<a href="https://people.cs.umass.edu/~rmckenna/">Ryan McKenna</a>, <a href="https://people.cs.umass.edu/~sheldon/">Daniel Sheldon</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.06605">Privacy Amplification via Random Check-Ins</a><br />
<a href="https://borjaballe.github.io/">Borja Balle</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://scholar.google.com/citations?user=iKPWydkAAAAJ">Brendan McMahan</a>, <a href="https://scholar.google.com/citations?user=iKPWydkAAAAJ">Om Thakkar</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.11947">Private Identity Testing for High-Dimensional Distributions</a><br />
<a href="http://www.cs.columbia.edu/~ccanonne/">Clement Canonne</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://audramarymcmillan.wixsite.com/mysite">Audra McMillan</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://www.ccs.neu.edu/home/lydiazak/">Lydia Zakynthinou</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07839">Private Learning of Halfspaces: Simplifying the Construction and Reducing the Sample Complexity</a><br />
<a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a>, <a href="https://dblp.org/pid/146/9658.html">Eliad Tsfadia</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.10129">Smoothed Analysis of Online and Differentially Private Learning</a><br />
<a href="https://www.cs.cornell.edu/~nika/">Nika Haghtalab</a>, <a href="http://timroughgarden.org/">Tim Roughgarden</a>, <a href="https://ashettyv.github.io/">Abhishek Shetty</a></p>
  </li>
  <li>
    <p>Smoothly Bounding User Contributions in Differential Privacy<br />
<a href="https://www.epasto.org/">Alessandro Epasto</a>, <a href="https://research.google/people/MohammadMahdian/">Mohammad Mahdian</a>, <a href="https://sites.google.com/view/jieming-mao">Jieming Mao</a>, <a href="https://people.csail.mit.edu/mirrokni/Welcome.html">Vahab Mirrokni</a>, <a href="https://www.linkedin.com/in/lijie-ren-57162633/">Lijie Ren</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06914">Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses</a><br />
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://sites.google.com/view/cguzman/">Cristobal Guzman</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p>Synthetic Data Generators – Sequential and Private<br />
<a href="https://research.google/people/OlivierBousquet/">Olivier Bousquet</a>, <a href="https://www.tau.ac.il/~rlivni/">Roi Livni</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.00010">The Discrete Gaussian for Differential Privacy</a><br />
<a href="http://www.cs.columbia.edu/~ccanonne/">Clement Canonne</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a></p>
  </li>
  <li>
    <p>The Flajolet-Martin Sketch Itself Preserves Differential Privacy: Private Counting with Minimal Space<br />
<a href="https://cs-people.bu.edu/ads22/">Adam Smith</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.08598">Towards practical differentially private causal graph discovery</a><br />
<a href="https://wanglun1996.github.io/">Lun Wang</a>, Qi Pang, <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15429">Understanding Gradient Clipping in Private SGD: A Geometric Perspective</a><br />
<a href="https://scholar.google.com/citations?user=M0ki5ZgAAAAJ">Xiangyi Chen</a>, <a href="https://zstevenwu.com/">Steven Wu</a>, <a href="https://people.ece.umn.edu/~mhong/mingyi.html">Mingyi Hong</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/neurips2020/"><span class="datestr">at October 07, 2020 04:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=63">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/">Friday, Oct 09 — Alexandr Andoni from Columbia University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next Foundations of Data Science virtual talk will take place on Friday, Oct 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Alexandr Andoni </strong>from Columbia University will speak about “<em><strong>Approximating Edit Distance in Near-Linear Time</strong></em>”.</p>



<p><strong>Abstract</strong>: Edit distance is a classic measure of similarity between strings, with applications ranging from computational biology to coding. Computing edit distance is also a classic dynamic programming problem, with a quadratic run-time solution, often taught in the “Intro to Algorithms” classes. Improving this runtime has been a decades-old challenge, now ruled likely-impossible using tools from the modern area of fine-grained complexity. We show how to approximate the edit distance between two strings in near-linear time, up to a constant factor. Our result completes a research direction set forth in the breakthrough paper of [Chakraborty, Das, Goldenberg, Koucky, Saks; FOCS’18], which showed the first constant-factor approximation algorithm with a (strongly) sub-quadratic running time.</p>



<p>Joint work with Negev Shekel Nosatzki, available at<a href="https://arxiv.org/abs/2005.07678"> https://arxiv.org/abs/2005.07678</a>.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/"><span class="datestr">at October 07, 2020 03:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17368">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/06/knowledge-is-good/">Knowledge is Good</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Science is good too</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/08/seal.png"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/08/seal.png?w=150&amp;h=150" class="alignright size-thumbnail wp-image-17660" height="150" /></a></p>
<p>
Emil Faber is the pretend founder of the pretend Faber College. The 1978 movie <a href="https://en.wikipedia.org/wiki/Animal_House">Animal House</a> starts with a close-up of Faber’s statue, which has the inscription, <b>Knowledge Is Good</b>.</p>
<p>
Today, Ken and I thought we might talk about knowledge, science, mathematics, proofs, and more.</p>
<p>
The phrase on Faber’s pedestal is meant to be a joke, as is the subtitle we added saying the same about science. But there is some truth to both of them. From the cause of climate change to the best response to the current pandemic to sports predictions there is much interest in science. Science is good, indeed.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/08/faberpedestal.jpg"><img src="https://rjlipton.files.wordpress.com/2020/08/faberpedestal.jpg?w=150&amp;h=114" alt="" class="aligncenter size-thumbnail wp-image-17661" /></a></p>
<p>
</p><p></p><h2> Science </h2><p></p>
<p></p><p>
What is science and what are methods of creating knowledge via science? There is a whole world on the philosophy of <a href="https://en.wikipedia.org/wiki/Science">science</a>. The central questions are: What is science? What methods are used to create new science? Is science good?—just kidding. </p>
<p>
We are not experts on the philosophy of science. But there seem to be three main ways to create scientific knowledge.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Experiments:</i> This is the classic one. Think about the testing of a candidate vaccine to stop the pandemic. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Computational Experiments:</i> This is relatively new. Think computer simulations of how climate change is effected by the methods of creating energy—for example. wind vs. coal. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Mathematical Proofs:</i> This is the one we focus on here at GLL. Think proofs that some algorithm works or that there is no algorithm that can work unless… </p>
<p>
</p><p></p><h2> Mathematical Proofs </h2><p></p>
<p></p><p>
We are interested in creating knowledge via proving new theorems. This is how we try to create knowledge. Our science is based not on experiments and not on simulations but mostly on the theorem-proof method. Well not exactly. We do use experiments and simulations. For example, the field of quantum algorithms uses both of these.</p>
<p>
However, math proofs are the basis of complexity theory. This means that we need to create proofs and then check that they are correct. The difficulty of checking a proof is based on who created them: </p>
<ul>
<li>
You did—checking your own work. <p></p>
</li><li>
Someone else did—refereeing for a journal. <p></p>
</li><li>
Someone in your class did—grading exams. <p></p>
</li><li>
Some graduate student did—mentoring. <p></p>
</li><li>
Someone on the web who claims a major result like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" title="{\mathsf{P &lt; NP}}" /> did—debugging. <p></p>
</li><li>
And so on.
</li></ul>
<p>
</p><p></p><h2> My Favorite Checking Method </h2><p></p>
<p></p><p>
My favorite tool for checking is this trick: Suppose that we have a proof <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> that demonstrates <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A \implies X}" class="latex" title="{A \implies X}" /> is true. Sometimes it is possible to show that there is a proof <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q}" class="latex" title="{Q}" /> that proves <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A \implies Y}" class="latex" title="{A \implies Y}" /> where: </p>
<ol>
<li>
The proof <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q}" class="latex" title="{Q}" /> is based on changing the claimed proof <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" />. <p></p>
</li><li>
The proof <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q}" class="latex" title="{Q}" /> demonstrates <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A \implies Y}" class="latex" title="{A \implies Y}" />, and; <p></p>
</li><li>
The statement <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" title="{Y}" /> does not follow from <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" />.
</li></ol>
<p>
One way this commonly arises is when <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> as a proof did not use all of the assumptions in <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> really proves more that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" title="{X}" /> and it proves <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" title="{Y}" />. But we note that <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" title="{Y}" /> is not a consequence of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" />.</p>
<p>
For example, consider the Riemann hypothesis. Suppose that we claim that we have a proof that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7Bn%5E%7Bs%7D%7D+%5Cneq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} \neq 0 " class="latex" title="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} \neq 0 " /></p>
<p>follows from the usual axioms of math plus <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt; 1/2}" class="latex" title="{\Re(s) &gt; 1/2}" />. Sounds great. But suppose this is based on an argument that assumes that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7Bn%5E%7Bs%7D%7D+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} = 0 " class="latex" title="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} = 0 " /></p>
<p>and manipulates the summation, eventually yielding a contradiction, without using the condition <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt; 1/2}" class="latex" title="{\Re(s) &gt; 1/2}" />. This is a problem, since there are <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" title="{s}" /> with <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3D+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) = 1/2}" class="latex" title="{\Re(s) = 1/2}" /> so that the sum is zero. This is an example of the above method of checking. </p>
<p>
</p><p></p><h2> A New Checking Method </h2><p></p>
<p></p><p>
From time to time claims are made of resolutions to famous conjectures. Think <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" title="{\mathsf{P = NP}}" />. These claims have all been wrong to date. So most researchers are reluctant to take time to check any new claims. Why would you take the effort to try and find the bug that is likely there? </p>
<p>
I wonder if there could be a method that is based on competition. For concreteness, suppose Alice and Bob are two researchers who both claim a resolution to the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" title="{\mathsf{P}}" /> versus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> problem. Alice has a lower bound argument that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" title="{\mathsf{P &lt; NP}}" /> and Bob has an upper bound that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" title="{\mathsf{P = NP}}" />. Could we have them play a “game”? </p>
<blockquote><p><b> </b> <em> Give their papers to each other. Have them try to find a flaw in each other’s paper. </em>
</p></blockquote>
<p></p><p>
They are highly motivated. Could we argue that if they cannot find any flaw then we would be slightly more motivated to look at the papers? </p>
<p>
This might work even if they both claim <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" title="{\mathsf{P = NP}}" />. Ken and I, personally, have had more claims of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" title="{\mathsf{P = NP}}" /> brought to our attention. Even in this case they would be highly motivated: the awards, the prizes, the praise will go to the one who is correct. </p>
<p>
</p><p></p><h2> Possible Extensions </h2><p></p>
<p></p><p>
One difference in our situation from classic empirical science is the nature of gaps in knowledge. For example, one of the big current controversies in physics is over the existence of <a href="https://en.wikipedia.org/wiki/Dark_matter">dark matter</a>. The Wikipedia article we just linked seems to date mostly to years around 2012 when dark matter was more widely accepted than <a href="https://medium.com/futuresin/doubting-dark-matter-f3e400c7dcd7">strikes</a> us <a href="https://www.scientificamerican.com/article/is-dark-matter-real/">today</a> (see also <a href="http://backreaction.blogspot.com/2019/10/dark-matter-nightmare-what-if-we-just.html">this</a> and <a href="http://backreaction.blogspot.com/2020/03/are-dark-energy-and-dark-matter.html">this</a>). There are cases where two competing theories are incompatible yet the available data do not suffice to find a fault in either. </p>
<p>
Whereas, with claimed proofs of incompatible statements, such as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" title="{\mathsf{P &lt; NP}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" title="{\mathsf{P = NP}}" />, at least one must have a demonstrable error. The statements themselves may have barriers all the way up to undecidability, but that does not matter to judging the proffered proofs.</p>
<p>
The method may be more applicable in life sciences where the gap is gathering sufficient field or lab observations. For a topical example, consider claims about the risk or safety of human gatherings amid the pandemic. One extreme is represented by the extraordinary <a href="http://ftp.iza.org/dp13670.pdf">claim</a>, which is <a href="https://www.wired.com/story/how-much-do-crowds-contribute-to-covid-its-complicated/">evidently</a> quite <a href="https://bgr.com/2020/09/14/sturgis-bike-rally-paper-impact-on-covid-19-coronavirus-cases/">excessive</a>, that the Sturgis motorcycle rally in August led to over 250,000 Covid-19 cases. The other extreme would be analyses used to justify gatherings with minimal precautions. The extremes cannot coexist. The means to arbitrate between them are available in principle but require costly social effort for contact tracing and testing as well as resolving mathematical issues between epidemiological models.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do you think of our new checking method? Should it be more widely employed for evaluating claims and hypotheses?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/10/06/knowledge-is-good/"><span class="datestr">at October 07, 2020 04:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/?p=389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/">Blackwell approachability meets Online Convex Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>David Blackwell was still roaming the corridors of UC Berkeley’s stat department during my postdoc years, circa 2009. Jake Abernethy, Sasha Rakhlin, Peter Bartlett and myself were discussing his results, and his seminal contributions to prediction theory were already well known.</p>



<figure class="wp-block-image size-large is-resized"><img width="275" alt="" src="https://minimizingregret.files.wordpress.com/2020/10/download.jpeg?w=275" class="wp-image-395" height="183" />David Blackwell</figure>



<figure class="wp-block-image size-large is-resized"><img width="204" alt="" src="https://minimizingregret.files.wordpress.com/2020/10/16-figure11-1-1.png?w=662" class="wp-image-397" height="200" />James Hannan</figure>



<p>At that time, still the early days of online convex optimization, we were contemplating everything from adaptive gradient methods to bandit convex optimization. Blackwell’s famed approachability theorem was looming in the background, considered to be one of the strongest theorems in the ML-theorists toolkit. </p>



<p>I’ve recently added a new draft chapter to to V2.0 of <a href="http://ocobook.cs.princeton.edu">Introduction to Online Convex Optimization</a>, about the connection between OCO and Blackwell approachability: they are algorithmically equivalent! More precisely, an efficient algorithm for approachability gives rise to an efficient algorithm for OCO with sublinear regret, and vice versa. </p>



<p>Details are in the chapter draft, and  I recommend this music for reading it: (the song is called “together we won despite everything”, dedicated to music and science)</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"></div>
</div></figure>



<p>.</p>



<p></p></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/"><span class="datestr">at October 06, 2020 12:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1794">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/10/05/forc-2021-is-on-its-way/">FORC 2021 Is on Its Way</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>After <a href="https://theorydish.blog/2020/03/30/forc-2020-going-strong-going-virtual/">a powerful launch</a> at 2021, the second meeting of  <a href="https://responsiblecomputing.org/">The Symposium on Foundations of Responsible Computing (FORC)</a> is on its way. The <a href="https://responsiblecomputing.org/forc-2021-call-for-papers/">call for papers</a> for FORC 2021 is out, with a wonderful PC, headed by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>. Please consider sending your strong submissions down our way.</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/10/05/forc-2021-is-on-its-way/"><span class="datestr">at October 05, 2020 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2360146425812493374">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/mip-re-redux.html">MIP* = RE Redux</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Everything pre-covid seems at least five years ago to me, so it's hard to believe that <a href="https://arxiv.org/abs/2001.04383">MIP* = RE</a> is a 2020 result. To remind you, consider the model where we have two powerful provers put in separate rooms where they can't communicate (think suspects of a crime put in separate interrogation rooms). An computationally efficient verifier can ask them questions based on random coins. Thirty years ago, Laszlo Babai, Carsten Lund and myself <a href="https://doi.org/10.1007/BF01200056">showed</a> that every language in nondeterministic exponential time has proofs in this model.<div><br /></div><div>Now suppose the provers have entangled quantum bits. This question has a long history that culminates in the <a href="https://arxiv.org/abs/2001.04383">MIP* = RE paper</a> earlier this year by Zhengfeng Ji, Anand Natarajan, Thomas Vidick, John Wright, Henry Yuen showing that every proof of any length can be proven in this model. Incredible!</div><div><br /></div><div>But wait, why is there a new version 2 dated last week? Turns out the MIP* = RE paper relied on a 2016 paper by Vidick which was later discovered to have a bug. No worries, as the authors of the MIP* = RE paper got around this issue by a <a href="https://arxiv.org/abs/2009.12982">quantum analysis</a> of a low-degree test from that old Babai-Fortnow-Lund paper.</div><div><br /></div><div>Vidick's <a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">blog post</a> explains it all, including the angst of a major result falling apart temporarily. He has <a href="https://blog.computationalcomplexity.org/2017/01/babai-strikes-back.html">good</a> <a href="http://nautil.us/issue/24/error/how-maths-most-famous-proof-nearly-broke">company</a>. Glad it all worked out. </div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/mip-re-redux.html"><span class="datestr">at October 05, 2020 12:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4979">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4979">On the destruction of America’s best high school</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<blockquote class="wp-block-quote"><p>[C]hildren with special abilities and skills need to be nourished and encouraged.  They are a national treasure.  Challenging programs for the “gifted” are sometimes decried as “elitism.”  Why aren’t intensive practice sessions for varsity football, baseball, and basketball players and interschool competition deemed elitism?  After all, only the most gifted athletes participate.  There is a self-defeating double standard at work here, nationwide.<br />—Carl Sagan, <a href="https://www.amazon.com/Demon-Haunted-World-Science-Candle-Dark/dp/0345409469"><em>The Demon-Haunted World</em></a> (1996)</p></blockquote>



<p>I’d like you to feel about the <a href="https://quillette.com/2020/09/23/rallying-to-protect-admissions-standards-at-americas-best-public-high-school/">impending destruction</a> of Virginia’s <a href="https://en.wikipedia.org/wiki/Thomas_Jefferson_High_School_for_Science_and_Technology">Thomas Jefferson High School for Science and Technology</a>, the same way you might’ve felt when the Taliban threatened to blow up the <a href="https://en.wikipedia.org/wiki/Buddhas_of_Bamyan">Bamyan Buddhas</a>, and then days later actually did blow them up.  Or the way you felt when human negligence caused wildfires that incinerated half the koalas in Australia, or turned the San Francisco skyline into an orange hellscape.  For that matter, the same way most of us felt the day Trump was elected.  I’d like you to feel in the bottom of your stomach the <em>avoidability</em>, and yet the finality, of the loss.</p>



<p>For thousands of kids in the DC area, especially first- or second-generation immigrants, TJHS represented a lifeline.  Score high enough on an entrance exam—something hard but<em> totally within your control</em>—and you could attend a school where, instead of the other kids either tormenting or ignoring you, they might teach you Lisp or the surreal number system.  Where you could learn humility instead of humiliation.</p>



<p>When I visited TJHS back in 2012 to give a quantum computing talk, I toured the campus, chatted with students, fielded their questions, and thought: so <em>this</em> is the teenagerhood—the ironically <em>normal</em> teenagerhood—that I was denied by living someplace else.  I found myself wishing that a hundred more TJHS’s, large and small, would sprout up across the country.  I felt like if I could further that goal then, though the universe return to rubble, my life would’ve had a purpose.</p>



<p>Instead, of course, our sorry country is destroying the few such schools that exist.  <a href="https://www.nytimes.com/2019/06/24/nyregion/specialized-schools-nyc-deblasio.html">Stuyvesant and Bronx Science</a> in New York, and the Liberal Arts and Science Academy here in Austin, are also under mortal threat right now.  The numerous parents who moved, who arranged their lives, specifically so that these schools might later be available for “high-risk” kids were suckered.</p>



<p>Assuming you haven’t just emerged from 30 years in a Tibetan cave, you presumably know why this is happening.  As the <em>Washington Post</em>‘s Jay Matthews <a href="https://www.washingtonpost.com/local/education/americas-top-sat-school-makes-another-disputed-attempt-at-diversity/2020/10/03/1bef3db2-0261-11eb-897d-3a6201d6643f_story.html?utm_campaign=wp_post_most&amp;utm_medium=email&amp;utm_source=newsletter&amp;wpisrc=nl_most&amp;carta-url=https%3A%2F%2Fs2.washingtonpost.com%2Fcar-ln-tr%2F2bf01fe%2F5f78a5d19d2fda0efb3d8031%2F5974cf76ade4e21a8497eeb1%2F42%2F62%2F2b179e7390f19bdba6f66f626ab37646">explains</a>, the Fairfax County School Board is “embarrassed” to have a school that, despite all its outreach attempts, remains only 5% Black and Latino—even though, crucially, the school <em>also</em> happens to be only 19% White (it’s now ~75% Asian).</p>



<p>You might ask: so then why doesn’t TJHS just institute affirmative action, like almost every university does?  It seems there’s an extremely interesting answer: <a href="https://www.washingtonian.com/2017/04/26/is-the-no-1-high-school-in-america-thomas-jefferson-fairfax-discrimination/">they did in the 1990s</a>, and Black and Hispanic enrollment surged.  But then the verdicts of court cases, brought by right-wing groups, made the school district fear that they’d be open to lawsuits if they continued with affirmative action, so they dropped it.  Now the boomerang has returned, and the School Board has decided on a more drastic remedy: namely, to eliminate the TJHS entrance exam entirely, and replace it by a lottery for anyone whose GPA exceeds 3.5.</p>



<p>The trouble is, TJHS without an entrance exam is no longer TJHS.  More likely than not, such a place would simply converge to become another of the thousands of schools across the US where success is based on sports, networking, and popularity.  And if by some miracle it avoided that fate, <em>still</em> it would no longer be available to most of the kids who‘d most need it.</p>



<p>So yes, the district is <strong>embarrassed</strong>—note that the<em> Washington Post</em> writer explains it as if that’s the most obvious, natural reaction in the world—to host a school that’s regularly ranked #1 in the US, with the highest average SATs and a distinguished list of alumni.  To avoid this embarrassment, the solution is (in effect) to burn the school to the ground.</p>



<p>In a world-historic irony, the main effect of this “solution” will be to <strong>drastically limit the number of Asian students, while drastically <em>increasing</em> (!!!) the number of White students</strong>.  The proportion of Black and Hispanic students is projected to increase a bit but remain small.  Let me say that one more time: in practice, TJHS’s move from a standardized test to a lottery will be<strong> overwhelmingly pro-White, anti-Asian, and anti-immigrant</strong>; only as a much smaller effect will it be pro-underrepresented-minority.</p>



<p>In spite of covid and everything else going on, hundreds of students and parents have been <a href="https://quillette.com/2020/09/23/rallying-to-protect-admissions-standards-at-americas-best-public-high-school/">protesting in front of TJHS</a> to try to prevent the school’s tragic and pointless destruction.  But it sounds like TJHS’s fate might be sealed.  The school board tolerated excellence for 35 more years than it wanted to; now its patience is at an end.</p>



<p>Some will say: sure, the end of TJHS is unfortunate, Scott, but why do you let this stuff <em>weigh</em> on you so heavily?  This is merely another instance of friendly fire, of good people fighting the just war against racism, and in <em>one</em> case hitting a target that, yeah, OK, probably should’ve been spared.  On reflection, though, I can accept that only insofar as I accept that it was “friendly fire” when Bolsheviks targeted the kulaks, or (much more comically, less importantly, and less successfully) when Arthur Chu, Amanda Marcotte, and a thousand other woke-ists targeted me.  With friendly fire like that, who needs enemy fire?</p>



<p>If you care about the gifted Black and Hispanic kids of Fairfax County, then like me, you should demand a change in the law to allow the reinstatement of affirmative action for them.  You should acknowledge that the issue lies there and not with TJHS itself.</p>



<p>I don’t see how you reach the point of understanding all the facts and still wanting to dismantle TJHS, over the desperate pleas of the students and parents, without a decent helping of <em>resentment</em> toward the kind of student who flourishes there—without a wish to see those uppity,  “fresh off the boat” Chinese and Indian grinds get dragged down to where they belong.  And if you tell me that such magnet programs need to end even though you yourself once benefitted from them—well, isn’t that more contemptible still?  Aren’t you knowingly burning a bridge you crossed so that a younger generation can’t follow you, basically reassuring the popular crowd that if they’ll only accept <em>you</em>, then there won’t be a hundred more greasy nerds in your tow?  And if, on some level, you already know these things about yourself, then the only purpose of this post has been to remind you of them.</p>



<hr class="wp-block-separator" />



<p>As for the news that dominates the wires and inevitably preempts what I’ve written: <strong>I wish for his successful recovery, followed by his losing the election and spending the rest of his life in New York State prison</strong>.  (And I look forward to seeing how woke Twitter summarizes the preceding statement—e.g., “Aaronson, his mask finally off, conveys well-wishes to Donald Trump”…)</p>



<hr class="wp-block-separator" />



<p>See <a href="https://news.ycombinator.com/item?id=24682119">further discussion of this post</a> on Hacker News.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4979"><span class="datestr">at October 04, 2020 06:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/open-problem-avoid-union/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/open-problem-avoid-union/">Open Problem - Avoiding the Union Bound for Multiple Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><strong>Background:</strong> Perhaps the best-studied problem in differential privacy is answering multiple counting queries.
The standard approach is to add independent, appropriately-calibrated (Laplace or Gaussian) noise to each query result and apply a composition theorem.
To bound the maximum error over the query answers, one takes a union bound over the independent noise samples.
However, this is <em>not</em> optimal.
The problem is to identify the optimal method (up to constant factors).</p>

<p><strong>Problem 1:</strong> Is there a randomized algorithm \(M : \{0,1\}^{n \times k} \rightarrow \mathbb{R}^k\) that is differentially private<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote">1</a></sup> and satisfies
\[
\forall x \in \{0, 1\}^{n \times k} \quad \mathbb{E}\left[ \left\|M(x) - \sum_{i=1}^n x_i \right\|_\infty \right] \leq c \sqrt{k}
\]
for some constant \( c &gt; 0\) depending only on the privacy parameters?<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote">2</a></sup></p>

<p>Adding independent Gaussian noise to each coordinate/query yields \(c \sqrt{k \log k}\) in place of \(c \sqrt{k}\) above. 
Steinke and Ullman [<a href="https://arxiv.org/abs/1501.06095">SU17</a>] showed that the union bound can <em>almost</em> be avoided and obtained \(c \sqrt{k \log \log k}\) using correlated noise.
The algorithm is nonetheless based on independent Gaussian noise, with the added step of using the exponential mechanism to identify high-error answers and correct them.</p>

<p>Note that a \(\Omega(\sqrt{k})\) lower bound is known [<a href="https://arxiv.org/abs/1311.3158">BUV18</a>, <a href="https://arxiv.org/abs/1501.06095">SU17</a>]. By [<a href="http://www.cs.utah.edu/~bhaskara/files/privacy.pdf">BDKT12</a>] it suffices to consider mechanisms \(M\) that add <em>instance-independent noise</em>. That is, \(M(x) = \sum_{i=1}^n x_i + Z\) where \(Z\) is some fixed noise distribution over \(\mathbb{R}^k\) that is independent of \(x\).</p>

<p><strong>Reward:</strong> For a positive solution, an all-you-can-eat sushi dinner at a sushi restaurant of your choice.
If the solution is an efficiently-sampleable distribution with a closed-form density, alcohol will be included.
For a negative solution, alcohol only.<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote">3</a></sup></p>

<p><strong>Other related work:</strong> [<a href="https://privacytools.seas.harvard.edu/files/privacytools/files/robust.pdf">DSSUV15</a>, <a href="https://privacytools.seas.harvard.edu/files/privacytools/files/complexityprivacy_1.pdf">Vad17</a>, <a href="https://arxiv.org/abs/1801.09236">AS18</a>]. 
A very recent work by Ganesh and Zhao [<a href="https://people.eecs.berkeley.edu/~arunganesh/papers/generalizedgaussians.pdf">GZ20</a>] improves the best upper bound from \(c\sqrt{k \log \log k}\) to \(c\sqrt{k \log \log \log k}\).</p>

<p><em>Submitted by <a href="http://www.thomas-steinke.net/">Thomas Steinke</a> and <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a> on April 9, 2019.</em></p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Specifically, \(M\) is either 1-zCDP [<a href="https://arxiv.org/abs/1605.02065">BS16</a>] with \(c\) an absolute constant or, for every \(\delta &gt; 0\), there is an \(M_\delta\) that is \((1, \delta)\)-DP with \(c = c(\delta) = c’ \cdot \sqrt{\log (1/\delta)}\) for an absolute constant \(c’\). <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Here \(x_i \in \{0, 1\}^k \subset \mathbb{R}^k\) denotes the data vector of individual \(i\) and \(x = (x_1, x_2, \dots, x_n) \in \{0,1\}^{n \times k}\). For simplicity, we only consider expected error; high-probability error bounds are an immediate consequence. <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Restaurant need not necessarily be all-you-can-eat. Maximum redeemable value US$500. <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/open-problem-avoid-union/"><span class="datestr">at October 03, 2020 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/02/positions-in-differential-privacy-open-source-software-project-at-opendp-apply-by-october-23-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/02/positions-in-differential-privacy-open-source-software-project-at-opendp-apply-by-october-23-2020/">Positions in Differential Privacy Open-Source Software Project  at OpenDP (apply by October 23, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The new open-source software project OpenDP is hiring 1-2 scientists to work on the theory and practice of differential privacy.</p>
<p>Website: <a href="https://projects.iq.harvard.edu/opendp/blog/opendp-hiring-scientific-staff">https://projects.iq.harvard.edu/opendp/blog/opendp-hiring-scientific-staff</a><br />
Email: privacytools-info@seas.harvard.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/02/positions-in-differential-privacy-open-source-software-project-at-opendp-apply-by-october-23-2020/"><span class="datestr">at October 02, 2020 09:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/10/02/cheerful-test-your-intuition-45-survey-about-sisters-and-brothers/">Cheerful Test Your Intuition (#45):  Survey About Sisters and Brothers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>You survey many many school children and ask each one:</p>



<p>Do you have more brothers than sisters? or more sisters than brothers? or the same number?</p>



<p>Then you separate the boys’s answers from the girls’s answers</p>



<p>Which of the following is true?</p>



<ol><li>The answers from boys will be tilted toward having MORE sisters and LESS brothers</li><li>The answers from boys will be tilted toward having MORE brothers and LESS sisters</li><li>There will be no (statistically significant) difference.      </li></ol>



<p><strong>The reason for answer 1 is</strong>: Since the ratio of boys/girls is 50/50, and you don’t count yourself in the answer, boys will tend to have more sisters and girls will tend to have more brothers</p>



<p><strong>The reason for answer 2 </strong>is: Some families are uneven and have more boys. In these families you will see more boys having more brothers than sisters and less girls having more brothers than sisters. Some families are uneven and have more girls. In these families you will see more girls having more sisters than brothers and less boys having more sisters than brothers. </p>



<p>I thank <a href="https://securityintelligence.com/author/oded-margalit/">Oded</a> Margalit for this question given to me as a birthday gift.<br /><br /></p>



<p></p>



<p></p>



<p></p>


<div class="align crowdsignal-poll-wrapper wp-block-crowdsignal-forms-poll"></div></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/10/02/cheerful-test-your-intuition-45-survey-about-sisters-and-brothers/"><span class="datestr">at October 02, 2020 12:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/02/postdocs-at-irif-cnrs-u-de-paris-paris-france-apply-by-november-2-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/02/postdocs-at-irif-cnrs-u-de-paris-paris-france-apply-by-november-2-2020/">postdocs at IRIF (CNRS &amp; U. de Paris), Paris, France (apply by November 2, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IRIF (Institute for Research in Foundations of Computer Science, CNRS &amp; U. de Paris) is seeking excellent candidates for postdoctoral positions in all areas of the Foundations of Computer Science.</p>
<p>Candidates must hold a Ph.D. in Computer Science or a related area before the starting date of the position. Knowledge of French is not required, and applications can be sent in French or English.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc">https://www.irif.fr/postes/postdoc</a><br />
Email: adiro@irif.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/02/postdocs-at-irif-cnrs-u-de-paris-paris-france-apply-by-november-2-2020/"><span class="datestr">at October 02, 2020 07:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/01/tenure-track-faculty-and-teaching-faculty-at-university-of-michigan-ann-arbor-apply-by-april-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/01/tenure-track-faculty-and-teaching-faculty-at-university-of-michigan-ann-arbor-apply-by-april-30-2021/">Tenure Track Faculty, and Teaching Faculty at University of Michigan, Ann Arbor (apply by April 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Computer Science and Engineering (CSE) at the University of Michigan invites applications for multiple tenure-track and teaching faculty (lecturer) positions. We seek exceptional candidates in all areas of CSE, with special emphasis on candidates at the early stages; we also have a targeted search for an endowed professorship in CS theory (the Fischer Chair). Positions remain open until filled.</p>
<p>Website: <a href="https://cse.engin.umich.edu/about/faculty-hiring/">https://cse.engin.umich.edu/about/faculty-hiring/</a><br />
Email: <a href="mailto:cpeikert@umich.edu" target="_blank" rel="noopener">cpeikert@umich.edu</a></p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/01/tenure-track-faculty-and-teaching-faculty-at-university-of-michigan-ann-arbor-apply-by-april-30-2021/"><span class="datestr">at October 01, 2020 09:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=479">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/09/30/479/">TCS+ talk: Wednesday, October 7 — Susanna F. de Rezende, Mathematical Institute of the Czech Academy of Sciences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 7th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Susanna F. de Rezende</strong> from Mathematical Institute of the Czech Academy of Sciences will speak about “<em>Lifting with Simple Gadgets and Applications to Circuit and Proof Complexity</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: Lifting theorems in complexity theory are a method of transferring lower bounds in a weak computational model into lower bounds for a more powerful computational model, via function composition. There has been an explosion of lifting theorems in the last ten years, essentially reducing communication lower bounds to query complexity lower bounds. These theorems only hold for composition with very specific “gadgets” such as indexing and inner product. <br /><br /> In this talk, we will present a generalization of the theorem lifting Nullstellensatz degree to monotone span program size by Pitassi and Robere (2018) so that it works for any gadget with high enough rank, in particular, for useful gadgets such as equality and greater-than. We will then explain how to apply our generalized theorem to solve three open problems: <br />– We present the first result that demonstrates a separation in proof power for cutting planes with unbounded versus polynomially bounded coefficients. Specifically, we exhibit CNF formulas that can be refuted in quadratic length and constant line space in cutting planes with unbounded coefficients, but for which there are no refutations in subexponential length and subpolynomial line space if coefficients are restricted to be of polynomial magnitude.<br />– We give the strongest separation to-date between monotone Boolean formulas and monotone Boolean circuits. Namely, we show that the classical GEN problem, which has polynomial-size monotone Boolean circuits, requires monotone Boolean formulas of size <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5COmega%28n+%2F+%5Ctextrm%7Bpolylog%7D+n%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="2^{\Omega(n / \textrm{polylog} n)}" class="latex" title="2^{\Omega(n / \textrm{polylog} n)}" />.<br />– We give the first explicit separation between monotone Boolean formulas and monotone real formulas. Namely, we give an explicit family of functions that can be computed with monotone real formulas of nearly linear size but require monotone Boolean formulas of exponential size. Previously only a non-explicit separation was known.<br /><br />This talk is based on joint work with Or Meir, Jakob Nordström, Toniann Pitassi, Robert Robere, and Marc Vinyals, available at <a href="https://arxiv.org/abs/2001.02144" rel="nofollow">https://arxiv.org/abs/2001.02144</a> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/09/30/479/"><span class="datestr">at September 30, 2020 11:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/a-new-algorithm-for-graph-crossings-hiding-in-plain-sight-20200915/">A new algorithm for graph crossings, hiding in plain sight</a> (<a href="https://mathstodon.xyz/@11011110/104876209471911167">\(\mathbb{M}\)</a>). Dynamic graph planarity testing, in <em>Quanta</em>. The original papers are <a href="https://arxiv.org/abs/1910.09005">arXiv:1910.09005, in SODA 2020</a> and <a href="https://arxiv.org/abs/1911.03449">arXiv:1911.03449, in STOC 2020</a>, by Jacob Holm and Eva Rotenberg.</p>
  </li>
  <li>
    <p><a href="https://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16159">Fun with Algorithms proceedings, now online</a> (<a href="https://mathstodon.xyz/@11011110/104884749945714543">\(\mathbb{M}\)</a>). So if you want to read about robot bamboo trimmers, phase transitions in the mine density of minesweeper, applications of the Blaschke–Lebesgue inequality to the game of battleship, multiplication of base-Fibonacci numbers, or trains that can jump gaps in their tracks, you know where to go. The conference itself has been rescheduled to next May. Maybe by then we can actually get a trip to an Italian resort island out of it.</p>
  </li>
  <li>
    <p><a href="https://fractalkitty.com/2020/07/02/week-37-cantor-set-kirigami/">Cantor set kirigami</a> (<a href="https://mathstodon.xyz/@11011110/104889995571500186">\(\mathbb{M}\)</a>). One of many many mathy-craft blog posts at Fractal Kitty, which I found via <a href="https://blogs.ams.org/blogonmathblogs/2020/08/24/fractal-kitty-blog-a-tour/">the AMS math blog tour</a>.</p>
  </li>
  <li>
    <p>Probability theorist <a href="https://en.wikipedia.org/wiki/Nina_Holden">Nina Holden</a>, quantum complexity theorist <a href="https://en.wikipedia.org/wiki/Urmila_Mahadev">Urmila Mahadev</a>, and knot theorist <a href="https://en.wikipedia.org/wiki/Lisa_Piccirillo">Lisa Piccirillo</a> win the <a href="https://breakthroughprize.org/News/60">2021 Maryam Mirzakhani New Frontiers Prizes</a> (<a href="https://mathstodon.xyz/@11011110/104894368463336926">\(\mathbb{M}\)</a>). For more on them their work, see <a href="https://johncarlosbaez.wordpress.com/2020/09/19/the-brownian-map/">Baez on the Brownian map</a>, <a href="https://www.quantamagazine.org/graduate-student-solves-quantum-verification-problem-20181008/"><em>Quanta</em> on quantum verification</a>, and <a href="https://www.quantamagazine.org/graduate-student-solves-decades-old-conway-knot-problem-20200519/"><em>Quanta</em> on Conway’s knot problem</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/channel/UCIyDqfi_cbkp-RU20aBF-MQ/videos">Richard Borcherd’s YouTube channel</a> (<a href="https://mathstodon.xyz/@jsiehler/104870496696544903">\(\mathbb{M}\)</a>), “a trove of mathematical lectures at various levels”.</p>
  </li>
  <li>
    <p><em><a href="https://archive.org/details/gri_33125012889602">Perspectiva corporum regularium</a></em> (1568), by Wenzel Jamnitzer (<a href="https://mathstodon.xyz/@11011110/104907440380152299">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Perspectiva_corporum_regularium">see also</a>). I don’t know how readable the brief medieval German text connecting the regular polyhedra to Plato’s theory of the four elements is, but the pictures of elaborated variations of the regular polyhedra can be understood in any language.</p>
  </li>
  <li>
    <p>Two new Wikipedia articles inspired by papers at Graph Drawing 2020 (<a href="https://mathstodon.xyz/@11011110/104911250878365185">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Geodetic_graph">Geodetic graph</a>, a graph in which all shortest paths are unique, inspired by “<a href="https://arxiv.org/abs/2008.07637">Drawing shortest paths in geodetic graphs</a>”</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Kirchberger%27s_theorem">Kirchberger’s theorem</a>, that if every  points in a red-blue point set are linearly separable then all of them are, inspired by “<a href="https://arxiv.org/abs/2005.12568">Topological drawings meet classical theorems from convex geometry</a>”</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.europeanwomeninmaths.org/ewm-open-letter-on-the-covid-19-pandemic/">European Women in Mathematics have written an open letter advocating proactive support of temporary employees, applicants, women, and parents in academia</a> (<a href="https://mathstodon.xyz/@11011110/104918402781146229">\(\mathbb{M}\)</a>, <a href="https://twitter.com/hollykrieger/status/1308375574285606913">via</a>), to forestall disproportionate losses in diversity in the wake of the covid pandemic. It’s addressed to European authorities but most of the same concerns apply more globally.</p>
  </li>
  <li>
    <p><a href="https://wikimediafoundation.org/news/2020/09/24/china-blocks-wikimedia-foundations-accreditation/">China blocks Wikimedia Foundation from being an observer to the World Intellectual Property Organization</a> (<a href="https://mathstodon.xyz/@11011110/104926568710157787">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24588913">via</a>), apparently because it has a chapter in Taiwan.</p>
  </li>
  <li>
    <p><a href="https://www.iqoqi-vienna.at/blog/article/dishonesty-in-academia-the-deafening-silence-of-the-royal-society-open-science-journal-on-an-accept/">Royal Society Open Science journal publishes crank quantum paper despite negative referee reports, and has not responded to two-year-old open letter from two of the referees and several other quantum heavy hitters requesting its retraction</a> (<a href="https://mathstodon.xyz/@11011110/104927678937136375">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24593465">via</a>).</p>
  </li>
  <li>
    <p>Michael Wehar posted a nice algorithms / fine grained complexity question on the CS theory stack exchange: <a href="https://cstheory.stackexchange.com/q/47588/95">how quickly can we test whether a 2d matrix has a square of four non-zero entries</a> (<a href="https://mathstodon.xyz/@11011110/104934034382351929">\(\mathbb{M}\)</a>)? The obvious method, looping over nonzeros and testing the squares each might be part of, is cubic when there are many nonzeros. And there can be many nonzeros without forcing a square to exist. Is there a standard hardness assumption under which strongly subcubic is impossible?</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2020/10/247584-bouncing-balls-and-quantum-computing/fulltext">The connection between the unexpected appearance of \(\pi\) in counting the bounces of billiard balls of different sizes and Grover’s algorithm for quantum search</a> (<a href="https://mathstodon.xyz/@11011110/104941135715864349">\(\mathbb{M}\)</a>): hidden constraints that keep things on a unit circle. Based on <a href="https://arxiv.org/abs/1912.02207">a preprint by Adam Brown</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2020/09/22/puzzle-reviews-by-a-puzzle-writer/">Puzzle reviews by a puzzle writer</a> (<a href="https://mathstodon.xyz/@11011110/104949202213901273">\(\mathbb{M}\)</a>). Lipton and Regan look at a few puzzles from the book <em>Bicycles or Unicycles: A Collection of Intriguing Mathematical Puzzles</em>, by Velleman and Wagon, concentrating on one that places a pebble at the origin of the positive quadrant and asks to clear a \(3\times 3\) square by moves that replace a pebble by one above and one to its left. The puzzle writer is Jason Rosenhouse, who <a href="https://www.ams.org/journals/notices/202009/rnoti-p1382.pdf">reviewed <em>Bicycles or Unicycles</em> in the <em>Notices</em></a>.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/journals/notices/202009/rnoti-p1397.pdf">Otto Neugebauer, famous as a historian of mathematics, also championed internationalism and diversity during Nazi times</a> (<a href="https://mathstodon.xyz/@11011110/104952629031738190">\(\mathbb{M}\)</a>, <a href="https://blogs.ams.org/beyondreviews/2020/09/28/otto-neugebauer-redux/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-020-02746-y"><em>Nature</em> covers the stories of five international students and postdocs whose plans to join US academia were disrupted by Trumpist visa restrictions</a> (<a href="https://mathstodon.xyz/@11011110/104955593693842251">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24634486">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/30/linkage.html"><span class="datestr">at September 30, 2020 05:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/30/postdoc-graduate-student-at-ben-gurion-university-at-ben-gurion-university-apply-by-december-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/30/postdoc-graduate-student-at-ben-gurion-university-at-ben-gurion-university-apply-by-december-30-2020/">POSTDOC, GRADUATE STUDENT AT BEN-GURION UNIVERSITY at Ben Gurion University (apply by December 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Excellent students with a strong background in Theoretical Computer Science and Mathematics, interested to conduct research in Error-Correcting Codes, Information Theory, or Arithmetic Circuits, are welcome to apply to postdoctoral and graduate student positions. The position is funded by the ERC, and it includes a generous salary, as well as funding for equipment and travel.</p>
<p>Website: <a href="https://www.cs.bgu.ac.il/~klim/Links/Call">https://www.cs.bgu.ac.il/~klim/Links/Call</a><br />
Email: klimefrem@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/30/postdoc-graduate-student-at-ben-gurion-university-at-ben-gurion-university-apply-by-december-30-2020/"><span class="datestr">at September 30, 2020 10:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/149">TR20-149 |  Robustly Self-Ordered Graphs: Constructions and Applications to Property Testing | 

	Oded Goldreich, 

	Avi Wigderson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A graph $G$ is called {\em self-ordered}\/ (a.k.a asymmetric) if the identity permutation is its only automorphism.
Equivalently, there is a unique isomorphism from $G$ to any graph that is isomorphic to $G$. 
We say that $G=(V,E)$ is {\em robustly self-ordered}\/ if the size of the symmetric difference between $E$ and the edge-set of the graph obtained by permuting $V$ using any permutation $\pi:V\to V$ is proportional to the number of non-fixed-points of $\pi$.

We show that robustly self-ordered bounded-degree graphs exist (in abundance), and that they can be constructed efficiently, in a strong sense.
Specifically, given the index of a vertex in such a graph, it is possible to find all its neighbors in polynomial-time (i.e., in time that is poly-logarithmic in the size of the graph).

We provide two very different constructions, in tools and structure. 
The first, a direct construction, is based on proving a sufficient condition for robust self-ordering, 
which requires that an auxiliary graph, on {\em pairs|}\/ of vertices of the original graph, is expanding. 
In this case the original graph is (not only robustly self-ordered but) also expanding.
The second construction proceeds in three steps: It boosts the mere existence of robustly self-ordered graphs, 
which provides explicit graphs of sublogarithmic size, to an efficient construction of polynomial-size graphs, 
and then, repeating it again, to exponential-size(robustly self-ordered) graphs that are locally constructible.
This construction can yield robustly self-ordered graphs that are either expanders or highly disconnected, having logarithmic size connected components. 

We also consider graphs of unbounded degree, seeking correspondingly unbounded robustness parameters.
We again demonstrate that such graphs (of linear degree) exist (in abundance), and give an explicit construction. 
This turns out to require very different tools, and the definition and constructions of new pseudo-random objects. 
Specifically, we show that the construction of such graphs reduces to the construction of non-malleable two-source extractors 
with very weak parameters but with an additional natural feature. 
Next, we reduce the construction of such non-malleable two-source extractors to the construction of ``relocation-detecting'' codes. Loosely speaking, in such code permuting arbitrarily the coordinates of a random codeword yields a string that is far any other codeword. We conclude by showing how to construct relocation-detecting codes (of various types, including ones with constant rate).  

We demonstrate that robustly self-ordered bounded-degree graphs are useful towards obtaining lower bounds on the query complexity of testing graph properties both in the bounded-degree and the dense graph models.  
Indeed, their robustness offers efficient, local and distance preserving reductions from testing problems on ordered structures (like sequences) to the unordered (effectively unlabeled) graphs. 
One of the results that we obtain, via such a reduction, is a subexponential separation 
between the complexity of testing and tolerant testing of graph properties in the bounded-degree graph model.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/149"><span class="datestr">at September 29, 2020 07:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/29/postdoctoral-research-associate-high-performance-parallel-graph-based-machine-learning-at-david-r-cheriton-school-of-computer-science-university-of-waterloo-apply-by-january-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/29/postdoctoral-research-associate-high-performance-parallel-graph-based-machine-learning-at-david-r-cheriton-school-of-computer-science-university-of-waterloo-apply-by-january-1-2021/">Postdoctoral Research Associate, High-Performance Parallel Graph-Based Machine Learning at David R. Cheriton School of Computer Science, University of Waterloo (apply by January 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for a postdoctoral research associate to join our research group (opallab.ca) at the Computer Science department at the University of Waterloo. Our goal is to develop parallel and communication efficient algorithms for large-scale graph-based machine learning.</p>
<p>Website: <a href="https://jobs.siam.org/job/postdoctoral-research-associate/54755436/">https://jobs.siam.org/job/postdoctoral-research-associate/54755436/</a><br />
Email: kfountou@uwaterloo.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/29/postdoctoral-research-associate-high-performance-parallel-graph-based-machine-learning-at-david-r-cheriton-school-of-computer-science-university-of-waterloo-apply-by-january-1-2021/"><span class="datestr">at September 29, 2020 07:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/148">TR20-148 |  Simple and fast derandomization from very hard functions: Eliminating randomness at almost no cost | 

	Roei Tell, 

	Lijie Chen</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Extending the classical ``hardness-to-randomness'' line-of-works, Doron et al. (FOCS 2020) recently proved that derandomization with near-quadratic time overhead is possible, under the assumption that there exists a function in $\mathcal{DTIME}[2^n]$ that cannot be computed by randomized SVN circuits of size $2^{(1-\epsilon)\cdot n}$ for a small $\epsilon$.

In this work we extend their inquiry and answer several open questions that arose from their work. Our main result is that *derandomization with almost no time overhead is possible*, under a plausible hypothesis. Specifically, we show that probabilistic algorithms that run in time $T(n)$ can be deterministically simulated in time $n\cdot T(n)^{1+\epsilon}$, under a hypothesis that is formally incomparable to the one of Doron et al., but is arguably more standard: We assume that there exist non-uniformly secure one-way functions, and that for $\delta=\delta(\epsilon)$ and $k=k_T(\epsilon)$ there exists a problem in $\mathcal{DTIME}[2^{k\cdot n}]$ that is hard for algorithms that run in time $2^{(k-\delta)\cdot n}$ and use $2^{(1-\delta)\cdot n}$ bits of advice. We also show that the latter hypothesis (or, more accurately, a relaxation of it that we use) is in fact necessary to obtain the derandomization conclusion if one relies on a PRG construction (as is indeed our approach).

For sub-exponential time functions $T(n)=2^{n^{o(1)}}$ we further improve the derandomization time to $n^{1+\epsilon}\cdot T(n)$, under a mildly stronger hypothesis. We also show that *the multiplicative time overhead of $n$ is essentially optimal*, conditioned on a counting version of the non-deterministic strong exponential-time hypothesis (i.e., on $\# NSETH$). Nevertheless, we show that *in the average-case setting a faster derandomization is possible*: Under hypotheses similar to the ones in our main result, we show that for every $L\in\mathcal{BPTIME}[n^k]$ there exists a deterministic algorithm $A_L$ running in time $n^{\epsilon}\cdot n^{k}$ such that for every distribution $\mathcal{D}$ over $\{0,1\}^n$ samplable in time $n^k$ it holds that $\Pr_{x\sim\mathcal{D}}[A_L(x)=L(x)]\ge1-n^{-\omega(1)}$. 

Lastly, we present an alternative proof for the result of Doron et al. using a *proof paradigm that is both considerably simpler and more general*; in fact, we show how to simplify the analysis of any construction that ``extracts randomness from a pseudoentropic string''. We use this simpler proof to extend their result, deducing a mildly slower derandomization (i.e., with cubic or quadratic overhead) from weaker hardness assumptions (i.e., for SVN circuits that do not use randomness).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/148"><span class="datestr">at September 29, 2020 05:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1252">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">It happens to everyone…but it’s not fun</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h1></h1>



<p>A <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">recent post</a> on this blog concerned the posting of our paper <a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> on the arXiv and gave a personal history of the the sequence of works that led to the result. Quite unfortunately (dramatically?) a few weeks after initial posting of the paper (and the blog post) John Wright discovered an important error in the proof of a key result in this sequence: my paper <a href="https://doi.org/10.1137/140956622">Three-player entangled XOR games are NP-hard to approximate</a>, published in 2016 in a special issue of the SIAM journal on computing dedicated to selected papers from the FOCS’13 conference. While I did not mention this paper directly in the previous blog post, its main result, a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, is a key ingredient in the proof of the <a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a>, itself a key ingredient in the MIP*=RE paper. (Strictly speaking the latter paper relies on an extension of my result to two-player games obtained in a <a href="https://arxiv.org/abs/1710.03062">follow-up</a> with Natarajan. Since that paper re-used the flawed part of my earlier analysis in a black-box manner it is similarly impacted.) So then…?</p>



<h2 id="scientific-aspects">Scientific aspects</h2>



<p>I’ll start with the science. The result MIP*=RE, to the best of our knowledge, remains correct. In order to remove the dependence of the proof on the flawed paper we extended the soundness analysis of Babai, Fortnow and Lund (BFL)’s multilinearity test against entangled provers from my <a href="https://arxiv.org/abs/1207.0550">paper with Ito</a> to the case of multivariate polynomials of low individual degree. (This extension, for the case of classical provers, is already mentioned in the BFL paper.) We just posted a self-contained analysis of that test on the arXiv <a href="https://arxiv.org/abs/2009.12982">here</a> and updated the MIP*=RE paper to account for the replacement (see v2.). The latter paper is currently under review; on this I will simply say that, as for all mathematical works, it is advised to wait until the outcome of the refereeing process is complete before declaring confidence in the validity of the result. For a more in-depth description of the changes made I refer to the introduction of the <a href="https://arxiv.org/abs/2009.12982">new paper</a>.</p>



<p>Our analysis of the “low individual-degree test” mentioned in the preceding paragraph can be used to recover the main result of my SICOMP’16 paper in a weakened form. Since the proof is different and does not directly fix the error I have decided to withdraw the paper from SICOMP. For more details on the error itself and consequences to other works, such as the quantum low-degree test and the quantum games PCP, I refer to the <a href="http://users.cms.caltech.edu/~vidick/errata.pdf">short note</a> I wrote to accompany the withdrawal of the paper. The one-sentence summary is that essentially all subsequent results expressed in terms of “high” complexity classes such as QMA-EXP, NEEXP, etc., still hold, while “scaled-down” results on the hardness of entangled games can only be recovered by allowing a substantial weakening of parameters. In particular, the <a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a> holds in its scaled-up version (testing exp(n) EPR pairs using poly(n) resources), but the scaled-down version requires polylog(n) communication to test n EPR pairs, instead of O(log n) as claimed.</p>



<h2 id="personal-aspects">Personal aspects</h2>



<p>In addition to notifying researchers in the area of the bug, my goal in writing this blog post is to help me exorcise the demon of having a large mistake in one of my papers. In doing so I was inspired by Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=2854">blog post</a> on a similar topic. (I’ll admit that even just linking explicitly to his post helps reassure myself, a power which I believe was one of Scott’s aims in writing the post. So, thanks Scott, and allow me to pass it on!) The faulty paper is not based on a minor back-of-the-envelope observation; in fact it is one that I was quite proud of. The mistake in it is not small either; it’s a mistake that I cannot find any excuse for having made. Yet here I am: after having spent the past 6 months trying to find an alternative proof, I now strongly believe that the problem cannot be solved using the kind of techniques that I had imagined could do so. Whether the theorem statement is true or not, I don’t know; but at the moment I am unable to prove it. I have to accept that there is a bug.</p>



<p>As painful as it is I realize that I am writing this post from a relatively comfortable position. Who knows if I would have been able to do the same had we not been able to recover a full proof of MIP*=RE. Moreover, after having banged my head against the problem for 6 months straight (COVID helping, walls were never far) I am now able to see my failure in a more positive light: the story I told in the previous blog post is not yet closed; there is an open challenge for me to solve. It is a very personal challenge; having spent the past 6 months delineating it I have accumulated sufficient grounds on which to believe that it is an interesting one. I feel grateful for this.</p>



<p>Getting there wasn’t easy. So, even though I am writing from a place of comfort, I want to share the pain that the whole adventure has caused me. This simple acknowledgment is especially directed at younger readers: so that when it happens to you, you will remember this post and know that you’re not the only one. That it happens to others as well and that it is possible to face, accept, and move away from such errors. Of course you will try to fix it first. Here are some quick tips. While banging your head on the problem, make sure that your understanding increases every day. To start with, do you really understand why there is a mistake? Of course some step doesn’t go through, but what is the simplest form of the incriminated statement that fails? Can you write it down? Can you formulate and prove a weaker form of it? Probe the issue with examples. Try to isolate it as much as you can: take it outside of the paper and formulate an entirely self-contained version of it, stripped of all the baggage. Place it in as many different contexts that you can think of: do you still believe it, does it stand on its own? Again, make sure that you learn. Even if you’re not able to fix the claim, are you exploring a new technique, discovering a new perspective? If it didn’t work yesterday it probably won’t work today either: make sure that you always find something new to inject. When you can no longer do this, it is time to stop. So make sure to set yourself some near-term (how much to think about this on any given day) and long-term (when to admit defeat) limits. Always remember that problems are much more often solved in the shower or while walking the dog than at the desk. Finally, be ready to move on. Realize that as bad as it may seem to you, there are more important things in life. You can’t reduce yourself to this one problem: you’ll be stronger for accepting what happened than trying to bury it at all costs. If you don’t see this by yourself, try to talk about it. Explain the situation you’re in to your close non-academia friends, to your parents; practice on your pet first if it helps. You will realize, as I eventually did (although it took quite a while) that <em>it is ok</em>.</p>



<h2 id="social-aspects">Social aspects</h2>



<p>After the scientific and the personal aspects, let me end with the sociological. This is a semi-tangent but it is a good opportunity to discuss a topic that we scientists, possibly even more so us working in the “hard sciences” (as the French call “proof-based” disciplines), are insufficiently sensitized to. This is the topic of how science is made, and what is the reality of this “absolute truth” that we claim to discover and establish in our mathematical results.</p>



<p>My paper was posted on arXiv in 2013, it was accepted to the FOCS conference and published in its proceedings the same year, and it appeared in the journal SICOMP in 2016. Both publications were refereed. Since its posting the paper has been cited 47 times (google scholar) and its main result is used in an essential way in at least half a dozen papers (my best guess). 7 years later a big hole has been found in the proof. How did the “truth value of my result evolve in the process? Was it always wrong or was there a time where it had truth, in whatever appropriate sense of the word?</p>



<p>I realize that these questions can be given trivial answers—I know what is an axiom and what is a proof system. Yet I am trying to push myself, and my reader, to look a little deeper. An analogy might help. The situation brought to mind a book by French philosopher of science Bruno Latour, called (in its English translation) <a href="https://www.amazon.com/Laboratory-Life-Construction-Scientific-Facts/dp/069102832X">Laboratory Life: The Construction of Scientific Facts</a>. This is a wonderful book, which goes well beyond the classic misconceptions from Popper or even Kuhn; it should be mandatory reading for every scientist. In one of the early chapters of the book Latour makes a detailed study of how subsequent citations can collectively enshrine an initial claim based entirely on the citer’s conscious or unconscious biases in making use of the citation (i.e. in complete independence from any ground “truth” or “importance” of the cited work). An entertaining example of this can be found in <a href="https://journals.sagepub.com/doi/full/10.1177/0306312714535679">this article</a>, which dissects the claim that “The myth from the 1930s that spinach is a rich source of iron was due to misleading information in the original publication: a malpositioned decimal point gave a 10-fold overestimate of iron content.” The example, pursued in great depth in the article, shows very well how one citation at a time the (spoiler: unjustified) claim is given more and more credibility until it eventually becomes a fact: from initial citations written in a tentative tone “according to Z, it could be that…” to more assertive citations “Z has shown that” by more and more well-known researchers in highly-read journals to pure fact (citation above). I highly recommend the article!</p>



<p>It is easy to dismiss this story as being the result of “sloppy” authors misrepresenting a “soft” claim whose truth value is not well-determined in the first place, being a statement about the world rather than about some hypothetical mathematical universe. Yet I believe that it is worth taking the time to examine with an open mind what exactly, if anything, distinguishes a claim about the iron content of spinach from the main “theorem” of my paper. From its initial posting on the arXiv to its presentation in a conference and its journal publication to the multiple citations it received through its use in subsequent works, and including multiple other considerations such as my own credibility (itself the result of so many other considerations) and the results base “believability”, when was the logical statement itself evaluated? Does it matter? Did the unchallenged existence of the result for 7 years impact the course of science? Or was it a mistake that was bound to be discovered and has no lasting consequences?</p>



<p>These are questions for the reader, that can be (and are probably better) asked in other contexts than the limited one of my result. Indeed there is a much broader point to all this, that I only meant to raise in an indirect manner. It is impossible to disregard the fact that our scientific work is grounded in cultural and societal effects, but we may disagree on the impact that this grounding has. We owe it to ourselves and to our readers (broadly interpreted—from colleagues to funding agencies to the broader public) to refuse to hide behind the thin veil of “hard science”, mathematics or logic, and educate ourselves to what it is that we really are doing.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/"><span class="datestr">at September 29, 2020 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
