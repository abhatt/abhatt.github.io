<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 22, 2019 09:24 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/21/report-from-socg">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/21/report-from-socg.html">Report from SoCG</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As I mentioned in <a href="https://11011110.github.io/blog/2019/06/20/portland-street-art.html">my previous post</a>, I just finished attending the Symposium on Computational Geometry in Portland. The <a href="http://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16108">conference proceedings are open access through LIPIcs</a>.</p>

<p>The conference started on Tuesday (after some earlier social activities) with the best paper talk by Arnaud de Mesmay, “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.27">Almost tight lower bounds for hard cutting problems in embedded graphs</a>” (with V. Cohen-Addad, É. Colin de Verdière, and D. Marx) proving that, to find the shortest set of cuts to slice a genus- surface into a disk, the exponent must depend linearly on  (assuming the exponential time hypothesis).</p>

<p>Several of the contributed talks from throughout the conference particularly caught my attention:</p>

<ul>
  <li>
    <p>Shay Moran’s paper with A. Yehudayoff, “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.51">On weak -nets and the Radon number</a>” concerned abstract convexity spaces where the convex sets are any set family closed under intersection. A space has Radon number  if every  points can be partitioned into two subsets whose convex hulls (smallest containing convex sets) intersect, and a point in the intersection is called a Radon point. Having bounded Radon number turns out to be equivalent to having weak -nets, subsets of points that (for a given measure on the space) intersect every large convex set.</p>
  </li>
  <li>
    <p>Mitchell Jones’ “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.41">Journey to the center of the point set</a>” (with Sariel Har-Peled) improved an old paper of mine on using Radon points to find points of high <a href="https://en.wikipedia.org/wiki/Centerpoint_(geometry)">Tukey depth</a> in high-dimensional point sets in time polynomial in the dimension, improving both the polynomial and the depth of the resulting points.</p>
  </li>
  <li>
    <p>Sariel Har-Peled spoke on his work with Timothy Chan, “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.23">Smallest -enclosing rectangle revisited</a>”. As well as shaving logs from the time bounds for finding rectangles that enclose a given number of points and minimize their area or perimeter, they found a general reduction from problems on  points to  problems on  points, allowing one to turn factors of  in the time bounds into factors of . Previously such reductions were only known for -point subset problems where the optimal solution lies among the  nearest neighbors of some input point, true for rectangle perimeter but not rectangle area.</p>
  </li>
  <li>
    <p>Timothy Chan’s “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.20">Computing Shapley values in the plane</a>” concerns an interesting combination of game theory and computational geometry. The <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley value</a> is a method for assigning credit when multiple people collaborate to produce some value (given as input a function from subsets of people to the value of a subset). It can be defined by adding contributors one-by-one in a random order and setting each contributor’s Shapley value to the expected increase in value when that contributor is added. It sums to the total value and is the unique function with this property that obeys several other natural and desirable axioms for credit assignment. For arbitrary functions from subsets of  contributors to subset values, it takes time  to compute, but Timothy described polynomial time algorithms for cases when the value function has some geometric meaning, such as when it measures the area of the convex hull of a subset of  points. There’s probably a lot more to be done in the same direction.</p>
  </li>
  <li>
    <p>Patrick Schnider’s “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.56">Ham-Sandwich cuts and center transversals in subspaces</a>” has some partial results towards a conjectured generalization of the <a href="https://en.wikipedia.org/wiki/Ham_sandwich_theorem">ham sandwich theorem</a>: given  probability distributions in -dimensional Euclidean space, it should be possible to find  hyperplanes whose checkerboard partition of space simultaneously bisects all of the distributions.</p>
  </li>
  <li>
    <p>Jie Xue spoke on “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.60">Near-optimal algorithms for shortest paths in weighted unit-disk graphs</a>” (with Haitao Wang). Here “weighted” means that overlapping unit disks are connected by an edge whose length is the Euclidean distance between their centers. Previous methods obtained near-linear algorithms by using bichromatic closest pair data structures to simulate Dijkstra’s algorithm. Instead, Wang and Xue use a related relaxation algorithm that partitions the plane into a grid and at each step relaxes all edges between certain pairs of grid squares, using data structures for additively weighted nearest neighbors. By avoiding the overhead of closest pairs they shave several logs from the runtime.</p>
  </li>
  <li>
    <p>Arnaud de Mesmay spoke again towards the end of the conference on “<a href="https://doi.org/10.4230/LIPIcs.SoCG.2019.49">The unbearable hardness of unknotting</a>”, with Yo’av Rieck, Eric Sedgwick, and Martin Tancer. They used a reduction from 3-satisfiability, with <a href="https://en.wikipedia.org/wiki/Hopf_link">Hopf links</a> for variables and <a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean links</a> for clauses (connected to each other by ribbons), to prove that it’s NP-complete to find an unlinked subset of a link with a maximum number of components. By a second level of replacement that doubles the strands of each link, they also showed that it’s NP-complete to find the minimum number of crossing changes or of Reidemeister moves to unlink a link or to unknot a knot.</p>
  </li>
</ul>

<p>On Tuesday afternoon I went to the workshop on open problems. After the obligatory open problem session, we debated whether we should collect problems on <a href="https://cs.smith.edu/~jorourke/TOPP/">The Open Problems Project</a>, <a href="http://www.openproblemgarden.org/">The Open Problem Garden</a>, or some new system that, since it doesn’t exist, can be more perfect than anything that does. Then I switched to the Young Researcher’s Forum (unfortunately missing the open problem implementation challenge); at the YRF, my student Daniel Frishberg spoke about <a href="https://arxiv.org/abs/1902.06875">our work on using nearest neighbor chains to speed up greedy algorithms</a>.</p>

<p>The invited talks on Wednesday and Friday were by Sanjoy Dasgupta and Bruce Donald, respectively. Dasgupta spoke on using concepts from geometric data structures to interpret the neural structure of the olfactory system in fruit flies (and hopefully, eventually, vice versa: to use our understanding of neural structures to develop new data structures). For instance, the first three layers of neurons in this system appear to implement an “expand-and-sparsify” data structure that represents low-dimensional normalized vectors by randomly mapping them to much higher dimensional vectors and then listing as a set a smaller number of high-value coordinates. This appears to be closely analogous to known methods for <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">locality-sensitive hashing</a>.  Donald spoke on using geometric methods to represent and infer the shapes of proteins, and to design proteins with desired shapes and functions.</p>

<p>Wednesday evening was the conference banquet, at the famed <a href="https://en.wikipedia.org/wiki/Crystal_Ballroom_(Portland,_Oregon)">Crystal Ballroom</a>, conveniently located near Powell’s Books. Matthew Dickerson showed up (after many years absence from SoCG) and brought some musical family members for a live concert.</p>

<p style="text-align: center;"><img src="http://www.ics.uci.edu/~eppstein/pix/portland/ShakySituation-m.jpg" alt="Marquee for Michael Dickerson and Shaky Situation" style="border-style: solid; border-color: black;" /></p>

<p>On Thursday afternoon I stopped by the workshop on algebraic methods where I had the pleasure of seing Josh Zahl give his talk on a whiteboard instead of using prepared slides. It was on methods for proving bounds on the number of incidences among geometric objects by combining naive bounds based on forbidden complete bipartite subgraphs of the incidence graphs, cuttings of space into smaller regions within which one applies these naive bounds, and cuttings of the objects into pieces in order to make the forbidden subgraphs smaller.</p>

<p>The conference business meeting was also Thursday, and ran very smoothly.
Next year SoCG will be in Zurich; the year after that, in Buffalo. We now have an officially incorporated society, <a href="http://www.computational-geometry.org/society/index.html">The Society for Computational Geometry</a>, so that we can maintain buffer funds for the conference from one year to the next; it will be supported by an increase of $30-$35 in non-student registration fees. And the attendees voted overwhelmingly in favor of both <a href="https://www.ics.uci.edu/~irani/safetoc.html">the SafeTOC anti-harassment guidelines</a> and ensuring and better advertising the availability of childcare at future conferences.</p>

<p>The conference was dedicated to the memory of <a href="https://en.wikipedia.org/wiki/Richard_M._Pollack">Ricky Pollack</a>, but it also included a celebration on Friday of a living computational geometer: <a href="https://en.wikipedia.org/wiki/John_Hershberger">John Hershberger</a>, who led the effort to bring SoCG to Oregon, and turned 60 just before the conference began. Happy birthday, John!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102313609521577006">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/21/report-from-socg.html"><span class="datestr">at June 21, 2019 10:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08656">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08656">Stochastic One-Sided Full-Information Bandit</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Haoyu.html">Haoyu Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wei.html">Wei Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08656">PDF</a><br /><b>Abstract: </b>In this paper, we study the stochastic version of the one-sided full
information bandit problem, where we have $K$ arms $[K] = \{1, 2, \ldots, K\}$,
and playing arm $i$ would gain reward from an unknown distribution for arm $i$
while obtaining reward feedback for all arms $j \ge i$. One-sided full
information bandit can model the online repeated second-price auctions, where
the auctioneer could select the reserved price in each round and the bidders
only reveal their bids when their bids are higher than the reserved price. In
this paper, we present an elimination-based algorithm to solve the problem. Our
elimination based algorithm achieves distribution independent regret upper
bound $O(\sqrt{T\cdot\log (TK)})$, and distribution dependent bound $O((\log T
+ \log K)f(\Delta))$, where $T$ is the time horizon, $\Delta$ is a vector of
gaps between the mean reward of arms and the mean reward of the best arm, and
$f(\Delta)$ is a formula depending on the gap vector that we will specify in
detail. Our algorithm has the best theoretical regret upper bound so far. We
also validate our algorithm empirically against other possible alternatives.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08656"><span class="datestr">at June 21, 2019 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08484">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08484">Coresets for Clustering with Fairness Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Lingxiao.html">Lingxiao Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Shaofeng_H==C=.html">Shaofeng H.-C. Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vishnoi:Nisheeth_K=.html">Nisheeth K. Vishnoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08484">PDF</a><br /><b>Abstract: </b>In a recent work, Chierichetti et al. studied the following "fair" variants
of classical clustering problems such as $k$-means and $k$-median: given a set
of $n$ data points in $\mathbb{R}^d$ and a binary type associated to each data
point, the goal is to cluster the points while ensuring that the proportion of
each type in each cluster is roughly the same as its underlying proportion.
Subsequent work has focused on either extending this setting to when each data
point has multiple, non-disjoint sensitive types such as race and gender, or to
address the problem that the clustering algorithms in the above work do not
scale well. The main contribution of this paper is an approach to clustering
with fairness constraints that involve multiple, non-disjoint types, that is
{\em also scalable}. Our approach is based on novel constructions of coresets:
for the $k$-median objective, we construct an $\varepsilon$-coreset of size
$O(\Gamma k^2 \varepsilon^{-d})$ where $\Gamma$ is the number of distinct
collections of groups that a point may belong to, and for the $k$-means
objective, we show how to construct an $\varepsilon$-coreset of size $O(\Gamma
k^3\varepsilon^{-d-1})$. The former result is the first known coreset
construction for the fair clustering problem with the $k$-median objective, and
the latter result removes the dependence on the size of the full dataset as in
Schmidt et al. and generalizes it to multiple, non-disjoint types. Plugging our
coresets into existing algorithms for fair clustering such as Backurs et al.
results in the fastest algorithms for several cases. Empirically, we assess our
approach over the Adult and Bank dataset, and show that the coreset sizes are
much smaller than the full dataset; applying coresets indeed accelerates the
running time of computing the fair clustering objective while ensuring that the
resulting objective difference is small.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08484"><span class="datestr">at June 21, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08455">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08455">Push-pull direct CAD modeling with movable neighboring faces for preserving G^1 connections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Qiang.html">Qiang Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Hsi=Yung.html">Hsi-Yung Feng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08455">PDF</a><br /><b>Abstract: </b>B-rep model editing plays an essential role in CAD and motivates the very
recent direct modeling CAD paradigm, which features intuitive push-pull
manipulation of the model geometry. Boundary faces in a B-rep model could be
connected in a smooth manner, typically G1 continuous, for functional,
manufacturing or aesthetic reasons. Quite often, it is desirable to preserve
such smooth connections during push-pull moves. This is, however, no trivial
matter and introduces additional challenges. To preserve the G1 connections,
neighboring faces of push-pulled faces need to be made movable, but their
motions are not known explicitly. Consequently, it becomes challenging to track
the geometry-topology inconsistency caused by these movable faces and to attain
a robust update for push-pulled solid models. No effective ways exist in the
literature to deal with the challenges; the industrial state of the art has
implemented this function, but the challenges are not addressed satisfactorily
and robustness issues are observed. This paper proposes a novel reverse
tracking method to solve the above challenges, and then, based on it, presents
a systematic method for push-pull direct modeling while preserving G1
connections. The developed method has been validated with a series of case
studies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08455"><span class="datestr">at June 21, 2019 11:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08448">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08448">Extensions of Self-Improving Sorters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Siu=Wing.html">Siu-Wing Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Kai.html">Kai Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yan:Lie.html">Lie Yan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08448">PDF</a><br /><b>Abstract: </b>Ailon et al. (SICOMP 2011) proposed a self-improving sorter that tunes its
performance to an unknown input distribution in a training phase. The input
numbers $x_1,x_2,\ldots,x_n$ come from a product distribution, that is, each
$x_i$ is drawn independently from an arbitrary distribution ${\cal D}_i$. We
study two relaxations of this requirement. The first extension models hidden
classes in the input. We consider the case that numbers in the same class are
governed by linear functions of the same hidden random parameter. The second
extension considers a hidden mixture of product distributions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08448"><span class="datestr">at June 21, 2019 11:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08433">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08433">A decision-support method for information inconsistency resolution in direct modeling of CAD models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Qiang.html">Qiang Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Hsi=Yung.html">Hsi-Yung Feng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08433">PDF</a><br /><b>Abstract: </b>Direct modeling is a very recent CAD paradigm that can provide unprecedented
modeling flexibility. It, however, lacks the parametric capability, which is
indispensable to modern CAD systems. For direct modeling to have this
capability, an additional associativity information layer in the form of
geometric constraint systems needs to be incorporated into direct modeling.
This is no trivial matter due to the possible inconsistencies between the
associativity information and geometry information in a model after direct
edits. The major issue of resolving such inconsistencies is that there often
exist many resolution options. The challenge lies in avoiding invalid
resolution options and prioritizing valid ones. This paper presents an
effective method to support the user in making decisions among the resolution
options. In particular, the method can provide automatic information
inconsistency reasoning, avoid invalid resolution options completely, and guide
the choice among valid resolution options. Case studies and comparisons have
been conducted to demonstrate the effectiveness of the method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08433"><span class="datestr">at June 21, 2019 11:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08371">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08371">Fine-grained complexity of graph homomorphism problem for bounded-treewidth graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okrasa:Karolina.html">Karolina Okrasa</a>, Paweł Rzążewski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08371">PDF</a><br /><b>Abstract: </b>For graphs $G$ and $H$, a \emph{homomorphism} from $G$ to $H$ is an
edge-preserving mapping from the vertex set of $G$ to the vertex set of $H$.
For a fixed graph $H$, by \textsc{Hom($H$)} we denote the computational problem
which asks whether a given graph $G$ admits a homomorphism to $H$. If $H$ is a
complete graph with $k$ vertices, then \textsc{Hom($H$)} is equivalent to the
$k$-\textsc{Coloring} problem, so graph homomorphisms can be seen as
generalizations of colorings. It is known that \textsc{Hom($H$)} is
polynomial-time solvable if $H$ is bipartite or has a vertex with a loop, and
NP-complete otherwise [Hell and Ne\v{s}et\v{r}il, JCTB 1990].
</p>
<p>In this paper we are interested in the complexity of the problem,
parameterized by the treewidth of the input graph $G$. If $G$ has $n$ vertices
and is given along with its tree decomposition of width $\mathrm{tw}(G)$, then
the problem can be solved in time $|V(H)|^{\mathrm{tw}(G)} \cdot
n^{\mathcal{O}(1)}$, using a straightforward dynamic programming. We explore
whether this bound can be improved. We show that if $H$ is a \emph{projective
core}, then the existence of such a faster algorithm is unlikely: assuming the
Strong Exponential Time Hypothesis (SETH), the \homo{H} problem cannot be
solved in time $(|V(H)|-\epsilon)^{\mathrm{tw}(G)} \cdot n^{\mathcal{O}(1)}$,
for any $\epsilon &gt; 0$. This result provides a full complexity characterization
for a large class of graphs $H$, as almost all graphs are projective cores.
</p>
<p>We also notice that the naive algorithm can be improved for some graphs $H$,
and show a complexity classification for all graphs $H$, assuming two
conjectures from algebraic graph theory. In particular, there are no known
graphs $H$ which are not covered by our result.
</p>
<p>In order to prove our results, we bring together some tools and techniques
from algebra and from fine-grained complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08371"><span class="datestr">at June 21, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08320">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08320">Scalable and Differentially Private Distributed Aggregation in the Shuffled Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghazi:Badih.html">Badih Ghazi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagh:Rasmus.html">Rasmus Pagh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velingker:Ameya.html">Ameya Velingker</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08320">PDF</a><br /><b>Abstract: </b>Federated learning promises to make machine learning feasible on distributed,
private datasets by implementing gradient descent using secure aggregation
methods. The idea is to compute a global weight update without revealing the
contributions of individual users.
</p>
<p>Current practical protocols for secure aggregation work in an "honest but
curious" setting where a curious adversary observing all communication to and
from the server cannot learn any private information assuming the server is
honest and follows the protocol.
</p>
<p>A more scalable and robust primitive for privacy-preserving protocols is
shuffling of user data, so as to hide the origin of each data item. Highly
scalable and secure protocols for shuffling, so-called mixnets, have been
proposed as a primitive for privacy-preserving analytics in the
Encode-Shuffle-Analyze framework by Bittau et al. Recent papers by Cheu et al.
and Balle et al. have formalized the "shuffled model" and suggested protocols
for secure aggregation that achieve differential privacy guarantees. Their
protocols come at a cost, though: Either the expected aggregation error or the
amount of communication per user scales as a polynomial $n^{\Omega(1)}$ in the
number of users $n$.
</p>
<p>In this paper we propose simple and more efficient protocol for aggregation
in the shuffled model, where communication as well as error increases only
polylogarithmically in $n$. Our new technique is a conceptual "invisibility
cloak" that makes users' data almost indistinguishable from random noise while
introducing zero distortion on the sum.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08320"><span class="datestr">at June 21, 2019 11:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08308">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08308">The Complexity of Online Bribery in Sequential Elections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hemaspaandra:Edith.html">Edith Hemaspaandra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hemaspaandra:Lane_A=.html">Lane A. Hemaspaandra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothe:Joerg.html">Joerg Rothe</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08308">PDF</a><br /><b>Abstract: </b>Prior work on the complexity of bribery assumes that the bribery happens
simultaneously, and that the briber has full knowledge of all voters' votes.
But neither of those assumptions always holds. In many real-world settings,
votes come in sequentially, and the briber may have a use-it-or-lose-it moment
to decide whether to bribe/alter a given vote, and at the time of making that
decision, the briber may not know what votes remaining voters are planning on
casting.
</p>
<p>In this paper, we introduce a model for, and initiate the study of, bribery
in such an online, sequential setting. We show that even for election systems
whose winner-determination problem is polynomial-time computable, an online,
sequential setting may vastly increase the complexity of bribery, in fact
jumping the problem up to completeness for high levels of the polynomial
hierarchy or even PSPACE. On the other hand, we show that for some natural,
important election systems, such a dramatic complexity increase does not occur,
and we pinpoint the complexity of their bribery problems in the online,
sequential setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08308"><span class="datestr">at June 21, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.08256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.08256">Jaccard Filtration and Stable Paths in the Mapper</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Dustin L. Arendt, Matthew Broussard, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnamoorthy:Bala.html">Bala Krishnamoorthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saul:Nathaniel.html">Nathaniel Saul</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.08256">PDF</a><br /><b>Abstract: </b>The contributions of this paper are two-fold. We define a new filtration
called the cover filtration built from a single cover based on a generalized
Jaccard distance. We provide stability results for the cover filtration and
show how the construction is equivalent to the Cech filtration under certain
settings. We then develop a language and theory for stable paths within this
filtration, inspired by ideas of persistent homology. We demonstrate how the
filtration and paths can be applied to a variety of applications in which
defining a metric is not obvious but a cover is readily available.
</p>
<p>We demonstrate the usefulness of this construction by employing it in the
context of recommendation systems and explainable machine learning. We
demonstrate a new perspective for modeling recommendation system data sets that
does not require manufacturing a bespoke metric. This extends work on
graph-based recommendation systems, allowing a topological perspective. For an
explicit example, we look at a movies data set and we find the stable paths
identified in our framework represent a sequence of movies constituting a
gentle transition and ordering from one genre to another.
</p>
<p>For explainable machine learning, we apply the Mapper for model induction,
providing explanations in the form of paths between subpopulations or
observations. Our framework provides an alternative way of building a
filtration from a single mapper that is then used to explore stable paths. As a
direct illustration, we build a mapper from a supervised machine learning model
trained on the FashionMNIST data set. We show that the stable paths in the
cover filtration provide improved explanations of relationships between
subpopulations of images.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.08256"><span class="datestr">at June 21, 2019 11:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blogs.princeton.edu/imabandit/?p=1373">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/bubeck.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blogs.princeton.edu/imabandit/2019/06/20/optimal-bound-for-stochastic-bandits-with-corruption/">Optimal bound for stochastic bandits with corruption</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><em>Guest post by <a href="http://web.stanford.edu/~msellke/main" class="liexternal">Mark Sellke</a>.</em></p>
<p>In the <a href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/#comments" class="liinternal">comments</a> of the previous blog post we asked if the new viewpoint on best of both worlds can be used to get clean “interpolation” results. The context is as follows: in a <a href="https://arxiv.org/abs/1803.09353" class="liinternal">STOC 2018 paper</a> followed by a <a href="https://arxiv.org/abs/1902.08647" class="liinternal">COLT 2019 paper</a>, the following corruption model was discussed: stochastic bandits, except for <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eda88fce4ab12a676aa4baf036291115_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula " /> rounds which are adversarial. The state of the art bounds were of the form: optimal (or almost optimal) stochastic term plus <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-189063850ed2dd251e3453bcdf72bb1f_l3.png?resize=30%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="30" alt="K C" class="ql-img-inline-formula " />, and it was mentioned as an open problem whether <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a11b6676b92ebe2d33985ebf5d9107fe_l3.png?resize=30%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="30" alt="KC" class="ql-img-inline-formula " /> could be improved to <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eda88fce4ab12a676aa4baf036291115_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula " /> (there is a lower bound showing that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eda88fce4ab12a676aa4baf036291115_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula " /> is necessary — when <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7a63a8eb07aa9c2f39df62272d6a867e_l3.png?resize=92%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="92" alt="C = O(\sqrt{T})" class="ql-img-inline-formula " />). As was discussed in the comment section, it seemed that indeed this clean best of both worlds approach should certainly shed light on the corruption model. It turns out that this is indeed the case, and a one-line calculation resolves positively the open problem from the COLT paper. The formal result is as follows (recall the notation/definitions from <a href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/" class="liinternal">the previous blog post</a>):</p>
<blockquote><p><strong>Lemma:</strong> Consider a strategy whose regret with respect to the optimal action <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9af0b76a90462b68c1d83fca9cc6604d_l3.png?resize=12%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="12" alt="i^*" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 56px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2ed2f41c6df060684cac5f6b54464432_l3.png?resize=122%2C56&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="56" width="122" alt="\begin{equation*} c \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>Then in the <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eda88fce4ab12a676aa4baf036291115_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula " />-corruption stochastic bandit model one has that the regret is bounded by:</p>
<p style="line-height: 50px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c48324ec76e014154981894b80b05577_l3.png?resize=219%2C50&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="50" width="219" alt="\[ C + 2 c \sqrt{K C} + c^2 \sum_{i \neq i^*} \frac{\log(T)}{\Delta_i} \]" class="ql-img-displayed-equation " /></p>
<p> </p></blockquote>
<p>Note that by the previous blog post we know strategies that satisfy (1) with <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac5b38b4377a6b37c2b5319ca167d4c1_l3.png?resize=49%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="49" alt="c=10" class="ql-img-inline-formula " /> (see Lemma 2 in the <a href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/" class="liinternal">previous post</a>).</p>
<p><em>Proof: In equation (1) let us apply Jensen over the corrupt rounds, this yields a term <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-aec148c42036e82bad7bbc28ce4df79f_l3.png?resize=53%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="53" alt="c \sqrt{K C}" class="ql-img-inline-formula " />. For the non-corrupt rounds, let us use that</em></p>
<p style="line-height: 45px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-85bba8f5ccf6308ab72f723bb5aa19b6_l3.png?resize=213%2C45&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="45" width="213" alt="\[ c \sqrt{\frac{x_{i,t}}{t}} \leq \frac{1}{2} \left( \Delta_i x_{i,t} + \frac{c^2}{t \Delta_i} \right) \]" class="ql-img-displayed-equation " /></p>
<p>The sum of the second term on the right hand side is upper bounded by <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b9e1680c083d81bc4a40e096a27bda7b_l3.png?resize=109%2C28&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="28" width="109" alt="c^2 \sum_{i \neq i^*} \frac{\log(T)}{2 \Delta_i}" class="ql-img-inline-formula " />. On the other hand the sum (over non-corrupt rounds) of the first term is equal to <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1c652ece8cc629e4e659c41eeed4d410_l3.png?resize=25%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="25" alt="1/2" class="ql-img-inline-formula " /> of the regret over the non-corrupt rounds, which is certainly smaller than <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1c652ece8cc629e4e659c41eeed4d410_l3.png?resize=25%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="25" alt="1/2" class="ql-img-inline-formula " /> of the total regret plus <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eda88fce4ab12a676aa4baf036291115_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula " />. Thus we obtain (denoting <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-dfd80738ac64385be5b381ea59d7fe55_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="R" class="ql-img-inline-formula " /> for the total regret):</p>
<p style="line-height: 50px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-936baa25f1189bd3993ce4dba58242c5_l3.png?resize=290%2C50&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="50" width="290" alt="\[ R \leq c \sqrt{K C} + c^2 \sum_{i \neq i^*} \frac{\log(T)}{2 \Delta_i} + \frac{C}{2} + \frac{R}{2} \]" class="ql-img-displayed-equation " /></p>
<p>which concludes the proof.</p>
<p> </p></div>







<p class="date">
by Sebastien Bubeck <a href="https://blogs.princeton.edu/imabandit/2019/06/20/optimal-bound-for-stochastic-bandits-with-corruption/"><span class="datestr">at June 20, 2019 06:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=34">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2019/06/20/theory-and-practice-of-differential-privacy-2019/">Theory and Practice of Differential Privacy 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>While I’m a relative newcomer to differential privacy (my <a href="https://arxiv.org/abs/1703.10127">first paper</a> on it was only in 2017), I’ve found the community to be a pleasure to interact with: paradoxically, simultaneously tight-knit yet highly welcoming to newcomers. I partially credit this culture to the number of workshops and programs which bring people together, including, but not limited to, <a href="https://www.birs.ca/events/2018/5-day-workshops/18w5189">a BIRS workshop</a>, <a href="https://privacytools.seas.harvard.edu/">the Privacy Tools project at Harvard</a>, <a href="https://simons.berkeley.edu/programs/privacy2019">a semester at the Simons Institute</a>, <a href="https://shonan.nii.ac.jp/seminars/164/">the forthcoming Shonan workshop</a>, and <a href="https://tpdp.cse.buffalo.edu/">the Theory and Practice of Differential Privacy (TPDP) Workshop</a>.</p>



<p>I’m writing this post to draw attention to the imminent deadline of <a href="https://tpdp.cse.buffalo.edu/2019/">TPDP 2019</a>, co-located with CCS 2019 in London. I’ll spare you the full details (click the link for more information), but most pressing is the deadline tomorrow, June 21, 2019, anywhere on Earth (let me know if this presents hardship for you, and I can pass concerns on to the chair). Essentially anything related to the theory or practice of differential privacy is welcome. Submissions are limited to four pages in length and are lightly refereed, based on originality, relevance, interest, and clarity. There are no published proceedings, and previously published results are welcome. If you’ve been looking to get to know the community, consider either submitting or attending the workshop! </p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2019/06/20/theory-and-practice-of-differential-privacy-2019/"><span class="datestr">at June 20, 2019 06:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/20/portland-street-art">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/20/portland-street-art.html">Portland street art</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I’ve been in Portland, Oregon this week for <a href="http://eecs.oregonstate.edu/socg19/">Computational Geometry Week</a>. Here are a few photos of local street art, the first set I’ve taken with my new Pixel 3 XL cellphone
(I neglected to bring an actual camera for this trip):</p>

<div><table style="margin-left: auto; margin-right: auto;">
<tbody><tr style="text-align: center; vertical-align: middle;">
<td style="padding: 10px;"><a href="http://www.ics.uci.edu/~eppstein/pix/portland/Clay-2nd-1.html"><img width="400" alt="Portland Oregon Street art: Clay &amp; 2nd" style="border-style: solid; border-color: black;" src="http://www.ics.uci.edu/~eppstein/pix/portland/Clay-2nd-1-m.jpg" /></a></td>
<td style="padding: 10px;"><a href="http://www.ics.uci.edu/~eppstein/pix/portland/Clay-2nd-2.html"><img width="382" alt="Portland Oregon Street art: Clay &amp; 2nd" style="border-style: solid; border-color: black;" src="http://www.ics.uci.edu/~eppstein/pix/portland/Clay-2nd-2-m.jpg" /></a></td>
</tr><tr style="text-align: center; vertical-align: middle;">
<td style="padding: 10px;"><a href="http://www.ics.uci.edu/~eppstein/pix/portland/Hall-5th-2.html"><img width="335" alt="Portland Oregon Street art: Hall &amp; 5th" style="border-style: solid; border-color: black;" src="http://www.ics.uci.edu/~eppstein/pix/portland/Hall-5th-2-m.jpg" /></a></td>
<td style="padding: 10px;"><a href="http://www.ics.uci.edu/~eppstein/pix/portland/College-4th.html"><img width="382" alt="Portland Oregon Street art: College &amp; 4th" style="border-style: solid; border-color: black;" src="http://www.ics.uci.edu/~eppstein/pix/portland/College-4th-m.jpg" /></a></td>
</tr></tbody></table></div>

<p><a href="http://www.ics.uci.edu/~eppstein/pix/portland/index.html">Here are the rest of my photos</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102306697775168322">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/20/portland-street-art.html"><span class="datestr">at June 20, 2019 05:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4207">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4207">Quanta of Solace</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In <em>Quanta</em> magazine, Kevin Hartnett has a recent article entitled <a href="https://www.quantamagazine.org/does-nevens-law-describe-quantum-computings-rise-20190618/">A New Law to Describe Quantum Computing’s Rise?</a>  The article discusses “Neven’s Law”—a conjecture, by Hartmut Neven (head of Google’s quantum computing effort), that the number of integrated qubits is now increasing exponentially with time, so that the difficulty of simulating a state-of-the-art QC on a fixed classical computer is increasing <em>doubly</em> exponentially with time.  (Jonathan Dowling tells me that he expressed the same thought years ago.)</p>



<p>Near the end, the <em>Quanta</em> piece quotes some UT Austin professor whose surname starts with a bunch of A’s as follows:</p>



<blockquote class="wp-block-quote"><p>“I think the undeniable reality of this progress puts the ball firmly in the court of those who believe scalable quantum computing can’t work.   They’re the ones who need to articulate where and why the progress will stop.”</p></blockquote>



<p>The quote is perfectly accurate, but in context, it might give the impression that I’m endorsing Neven’s Law.  In reality, I’m reluctant to fit a polynomial or an exponential or any other curve through a set of numbers that so far hasn’t exceeded about 50.  I say only that, regardless of what anyone believes is the ultimate rate of progress in QC, what’s already happened today puts the ball firmly in the skeptics’ court.</p>



<p>Also in <em>Quanta</em>, Anil Ananthaswamy has a new article out on <a href="https://www.quantamagazine.org/how-to-turn-a-quantum-computer-into-the-ultimate-randomness-generator-20190619/">How to Turn a Quantum Computer Into the Ultimate Randomness Generator</a>.  This piece covers two schemes for using a quantum computer to generate “certified random bits”—that is, bits you can <em>prove</em> are random to a faraway skeptic.  one due to me, the other due to <a href="https://arxiv.org/abs/1804.00640">Brakerski et al</a>.  The article cites my <a href="https://arxiv.org/abs/1612.05903">paper with Lijie Chen</a>, which shows that under suitable computational assumptions, the outputs in my protocol are hard to spoof using a classical computer.  The randomness aspect will be addressed in a paper that I’m currently writing; for now, see <a href="https://www.scottaaronson.com/talks/certrand2.ppt">these slides</a>.</p>



<p>As long as I’m linking to interesting recent <em>Quanta</em> articles, Erica Klarreich has <a href="https://www.quantamagazine.org/mathematician-disproves-hedetniemis-graph-theory-conjecture-20190617/">A 53-Year-Old Network Coloring Conjecture is Disproved</a>.  Briefly, <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Hedetniemi’s Conjecture</a> stated that, given any two finite, undirected graphs G and H, the chromatic number of the tensor product G⊗H is just the minimum of the chromatic numbers of G and H themselves.  This reasonable-sounding conjecture has now been <a href="https://arxiv.org/abs/1905.02167">falsified</a> by Yaroslav Shitov.  For more, see also <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">this post by Gil Kalai</a>—who appears here <em>not</em> in his capacity as a quantum computing skeptic.</p>



<p>In interesting math news beyond <em>Quanta</em> magazine, the Berkeley alumni magazine has a piece about the crucial, neglected topic of <a href="https://alumni.berkeley.edu/california-magazine/just-in/2019-06-14/chalk-market-where-mathematicians-go-get-good-stuff">mathematicians’ love for Hagoromo-brand chalk</a> (hat tip: Peter Woit).  I can personally vouch for this.  When I moved to UT Austin three years ago, most offices in CS had whiteboards, but I deliberately chose one with a blackboard.  I figured that chalk has its problems—it breaks, the dust gets all over—but I could live with them, much more than I could live with the Fundamental Whiteboard Difficulty, of all the available markers always being dry whenever you want to explain anything.  With the Hagoromo brand, though, you pretty much get all the benefits of chalk with none of the downsides, so it just strictly dominates whiteboards.</p>



<p>Jan Kulveit asked me to advertise the <a href="https://espr-camp.org/">European Summer Program on Rationality</a> (ESPR), which will take place this August 13-23, and which is aimed at students ages 16-19.  I’ve <a href="https://www.scottaaronson.com/blog/?p=2410">lectured</a> both at ESPR and at a similar summer program that ESPR was modeled after (called SPARC)—and while I was never there as a student, it looked to me like a phenomenal experience.   So if you’re a 16-to-19-year-old who reads this blog, please consider applying!</p>



<p>I’m now at the end of my annual family trip to Tel Aviv, returning to the Eastern US tonight, and then on to <a href="http://acm-stoc.org/stoc2019/stocprogram.html">STOC’2019</a> at the <a href="https://fcrc.acm.org/">ACM Federated Computing Research Conference</a> in Phoenix (which I can blog about if anyone wants me to).  It was a good trip, although marred by my two-year-old son Daniel falling onto sun-heated metal and suffering a second-degree burn on his leg, and then by the doctor botching the treatment.  Fortunately Daniel’s now healing nicely.  For future reference, whenever bandaging a burn wound, <em>be sure</em> to apply lots of Vaseline to prevent the bandage from drying out, and also to change the bandage daily.  Accept no fancy-sounding substitute.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4207"><span class="datestr">at June 20, 2019 03:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7528">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/06/20/tcs-women-at-stoc-guest-post-by-virginia-williams/">TCS Women at STOC (guest post by Virginia Williams)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><em>[Guest post by Virgi Vassilevska Williams on the TCS women program at STOC. In particular the TCS Women Spotlight workshop has a great program and is open to all. –Boaz]</em></div>
<div></div>
<div>Dear all,</div>
<div></div>
<div>The TCS Women 2019 program is finalized: <a href="https://sigact.org/tcswomen/tcs-women-2019/" target="_blank" rel="noopener">https://sigact.org/tcswomen/tcs-women-2019/</a>. Here are some details:</div>
<div></div>
<div>On June 23rd, we have our TCS Women Spotlight workshop from 2 pm to 5 pm. This will feature an inspirational talk by Ronitt Rubinfeld (MIT &amp; Tel Aviv University)–“Comedy of Errors”, and rising stars talks by Naama Ben-David (CMU), Debarati Das (Charles University), Andrea Lincoln (MIT) and Oxana <span style="color: #000000;">Poburinnaya (Boston University). The workshop is open to all and we would love to see you all there.</span></div>
<div><span style="color: #000000;"> </span></div>
<div><span style="color: #000000;">On June 24th, we have the TCS Women lunch and panel of distinguished researchers from academia and industries. This year’s panelists include Shuchi Chawla (University of Wisconsin), Julia Chuzhoy (TTIC), Edith Cohen (Google), Faith Ellen (University of Toronto), Rachel Lin (University of Washington) and Nutan Limaye (Indian Institute of Technology, Mumbai). The event is open to all women participants of FCRC.</span></div>
<div><span style="color: #000000;"> </span></div>
<div><span style="color: #000000;">In addition to these, there will be a TCS Women poster session held jointly with the STOC poster session on June 24th. Fourteen women researchers will be presenting posters on their research. More information is available on our website. All are welcome. </span></div>
<div><span style="color: #000000;"> </span></div>
<div><span style="color: #000000;">We are providing travel scholarships to more than forty women students this year to attend STOC and FCRC. We secured funding from the NSF, Akamai, Amazon, Google and Microsoft. Thank you!<br />
</span></div>
<div><span style="color: #000000;"> </span></div>
<div><span style="color: #000000;">Looking forward to seeing you soon!</span></div>
<div><span style="color: #000000;"> </span></div>
<div><span style="color: #000000;">Best wishes,</span></div>
<div><span style="color: #000000;">The TCS Women organizers</span></div></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/06/20/tcs-women-at-stoc-guest-post-by-virginia-williams/"><span class="datestr">at June 20, 2019 02:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8718880846893877693">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/the-urbanrural-collegiality-divide.html">The Urban/Rural Collegiality Divide</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>
Just a reminder that Grigory Yaroslavtsev has <a href="http://grigory.us/blog/theory-jobs-2019/">taken over</a> the <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">Theory Jobs Spreadsheet</a>. Check out who is moving where and let everyone know where you will continue your research career.</div>
<div>
<br /></div>
In 1999 when I considered leaving the University of Chicago for NEC Research I had a conversation with Bob Zimmer, then VP of Research and current U of C President. Zimmer said it was a shame I didn't live in Hyde Park, the Chicago South Side neighborhood where the university resides, and thus not fully connected with the school. At the time I didn't fully understand his point and did leave for NEC, only to return in 2003 and leave again in 2008. I never did live in Hyde Park.<br />
<div>
<br /></div>
<div>
Recently I served on a review committee for a computer science department in a rural college town. You couldn't help but notice the great collegiality among the faculty. Someone told me their theory that you generally get more faculty collegiality in rural versus urban locations. Why?</div>
<div>
<br /></div>
<div>
In urban locations faculty tend to live further from campus, to get bigger homes and better schools, and face more traffic. They are likely to have more connections with people unconnected with the university. There are more consulting opportunities in large cities, a larger variety of coffee houses to hang out in and better connected airports make leaving town easier. Particularly in computer science, where you can do most of your research remotely, faculty will find themselves less likely to come in every day and you lose those constant informal connections with the rest of the faculty. </div>
<div>
<br /></div>
<div>
This is a recent phenomenon, even going back to when I was a young faculty you needed to come to the office for access to research papers, better computers to write papers, good printers. Interactions with students and colleagues is always better in person but in the past the methods of electronic communication proved more limiting.</div>
<div>
<br /></div>
<div>
The University of Chicago helped create and promote its own neighborhood and ran a very strong private school with reduced tuition for faculty children. Maybe my life would have been different had I immersed myself in that lifestyle. </div>
<div>
<br /></div>
<div>
Or maybe we should go the other extreme. If we can find great ways to do on-line meetings and teaching, why do we need the physical university at all?</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/the-urbanrural-collegiality-divide.html"><span class="datestr">at June 20, 2019 01:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-778501013924768672">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/06/postdoc-position-at-gssi-in-algorithms.html">Postdoc position at the GSSI in algorithms for modern networks, formal methods for reactive systems and model-based software engineering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"></b></span><br /><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"></b></span></span></b></span></div><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Postdoctoral position financed by the <a href="http://www.gssi.it/">GSSI</a></span></b></span></b></span></div><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><br /><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="background-color: white; color: #222222; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><b>Scientific profile:</b> The successful candidate will have a strong research record in one or more of the research areas within the <a href="https://sites.google.com/gssi.it/csgssi">Computer Science group at the GSSI,</a> namely algorithms for modern networks, formal methods for reactive systems and model-based software engineering. We welcome applications in any of those areas. </span></div></b></span><br /><br /><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The position is for two years (</span><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span style="font-size: medium;"><b style="font-weight: normal;" id="m_2299630027166989820gmail-m_-1686136991101628107gmail-docs-internal-guid-fe2365bd-7fff-efca-8269-ca7515fa84d9"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">renewable for a third year, if this is mutually agreeable)</span></b></span> and the salary is 36,000€ per year before taxes. </span></div><br /><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">How to apply: </span><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The application must be submitted through the online form available at</span><a style="text-decoration: none;" href="https://applications.gssi.it/postdoc/" target="_blank"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; color: #1155cc; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">https://applications.gssi.it/postdoc/</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> by </span><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Thursday, 11 July 2019 at 6 p.m. (Central European Time)</span><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">. For more information, please consult the Call for Applications at</span><a style="text-decoration: none;" href="https://applications.gssi.it/postdoc/" target="_blank"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; color: #1155cc; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">https://applications.gssi.it/postdoc/</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">. </span></div><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;" dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> </span></div><div style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: justify;" dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Requirements:</span><span style="background-color: transparent; color: black; font-family: Arial; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> Candidates are expected to have a PhD degree in Computer Science or related disciplines. </span></div><br /></b></span></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/06/postdoc-position-at-gssi-in-algorithms.html"><span class="datestr">at June 20, 2019 10:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-2613037482816339108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/06/postdoctoral-position-at-gssi-within.html">Postdoctoral position at the GSSI within the project “ALGADIMAR: Algorithms, Games, and Digital Markets”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Computer Science group at the Gran Sasso Science Institute (GSSI, <a href="http://cs.gssi.it/">http://cs.gssi.it</a>) --- an international PhD school and centre for advanced studies located in L’Aquila, Italy --- has one available postdoctoral position  in the context of the PRIN projects  “ALGADIMAR: Algorithms, Games, and Digital Markets” (Gianlorenzo D’Angelo, PI at GSSI),  funded by the Italian Ministry for University and Research.<br /><br />The position is briefly described below. It is for two years and the salary is 36,000€ per year before taxes. The position is renewable for a third year, if this is mutually agreeable.<br /><br /><b>How to apply:</b> The application must be submitted through the online form available at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/</a> by Thursday, 11 July 2019 at 6 p.m. (Central European Time). For more information, please consult the Call for Applications at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/</a> or write an email to <a href="http://www.gssi.it/people/professors/lectures-computer-science/item/456-d-angelo-gianlorenzo">Gianlorenzo D’Angelo</a>.<br /><br /><b>Requirements:</b> Candidates are expected to have a PhD degree in Computer Science or related disciplines. Preference will be given to applicants who have previous research experience on topics related to the theme of the project.<br /><br /><b>Postdoctoral position within the project “ALGADIMAR: Algorithms, Games, and Digital Markets”, supervised by <a href="http://www.gssi.it/people/professors/lectures-computer-science/item/456-d-angelo-gianlorenzo">Gianlorenzo D’Angelo</a>. </b><br /><br />Brief description of the project: Digital markets form an increasingly important component of the global economy. The Internet has enabled new markets with previously unknown features (e-commerce, web-based advertisement, viral marketing, sharing economy, real-time trading), and algorithms play a central role in many decision processes involved in these markets. For example, algorithms run electronic auctions, adjust prices dynamically, trade stocks, harvest big data to decide market strategies and to provide recommendations to customers. The focus of the proposal ALGADIMAR is on the development of new methods and tools in research areas that are critical to the understanding of digital markets: algorithmic game theory, market and mechanism design, machine learning, algorithmic data analysis, optimization in strategic settings. We plan to apply these methods so as to solve fundamental algorithmic problems motivated by Internet advertisement, sharing economy, mechanism design for social good, security games. While our research is focused on foundational work —with rigorous design and analysis of algorithms, mechanisms and games— it will also include empirical validation on large-scale datasets from real-world applications.<br /><br />Main activities:  The research activity will be on one or more of the following topics: design and analysis of algorithms; algorithms and data structures for big data; approximation algorithms; combinatorial optimization; algorithms for graphs and networks; autonomous agents and mobile computing; distributed algorithms; algorithmic game theory, algorithmic aspects of internet and social networks.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/06/postdoctoral-position-at-gssi-within.html"><span class="datestr">at June 20, 2019 10:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/19/diophantine-equations/">Diophantine Equations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Complexity of solving polynomial equations</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/19/diophantine-equations/jeff/" rel="attachment wp-att-16022"><img width="240" alt="" class="alignright  wp-image-16022" src="https://rjlipton.files.wordpress.com/2019/06/jeff.jpg?w=240" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Jeff ]</font></td>
</tr>
</tbody>
</table>
<p>
Jeff Lagarias is a mathematician <b>or</b> a professor at the University of Michigan.</p>
<p>
Today I wish to discuss Diophantine equations.</p>
<p>
Note, the “or” is a poor joke: For mathematicians <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ctext%7B+or+%7D+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \text{ or } B}" class="latex" title="{A \text{ or } B}" /> is true also when both <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> are true.</p>
<p>
Jeff has a paper titled <i>Complexity of Diophantine Equations</i> and related <a href="http://www.math.lsa.umich.edu/~lagarias/TALK-SLIDES/dioph-cplx-icerm2011aug.pdf">talk</a> version. It is a nice review of some of the issues around Diophantine equations. </p>
<p>
He has worked on number theory, on complexity theory, and on many other <a href="https://rjlipton.wordpress.com/2013/09/05/eulers-constants/">problems</a>. Some are applied and some theoretical. He with Peter Shor solved an open problem years ago that was first stated by Ott-Heinrich Keller in 1930. Our friends at Wikipedia <a href="https://en.wikipedia.org/wiki/Keller%27s_conjecture">state</a>: </p>
<blockquote><p><b> </b> <em> In geometry, Keller’s conjecture is the conjecture that in any tiling of Euclidean space by identical hypercubes there are two cubes that meet face to face. </em>
</p></blockquote>
<p>For dimensions ten or more it is now proved to be false thanks to Jeff and Peter. I am amazed by such geometric results, since I have no geometric intuition. Ten dimensions is way beyond me—although curiously I am okay in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> dimensions. Strange? Jeff has a relevant <a href="https://www.azquotes.com/quote/765002">quote</a>:</p>
<blockquote><p>Every dimension is special.</p></blockquote>
<p>
</p><p></p><h2> Complexity of Diophantine Equations </h2><p></p>
<p></p><p>
Recall the main problem is to find solutions to equations usually restricted to be integers or rationals. This restriction makes the problems hard as in <em>open</em>, and hard as in <em>computationally difficult</em>. </p>
<p>
</p><p></p><h3>  Factoring-Hard </h3><p></p>
<p>Jeff mentions the following hardness result in his talk: Solving in nonnegative integers for <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2B2%29%28y%2B2%29+%3D+N%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x+2)(y+2) = N, " class="latex" title="\displaystyle  (x+2)(y+2) = N, " /></p>
<p>is as hard as integer factoring. This follows since <img src="https://s0.wp.com/latex.php?latex=%7Bx%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x+2}" class="latex" title="{x+2}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y+2}" class="latex" title="{y+2}" /> must be non-trivial factors of <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />. Note this relies on the fact that <img src="https://s0.wp.com/latex.php?latex=%7Bx%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x+2}" class="latex" title="{x+2}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y+2}" class="latex" title="{y+2}" /> are both greater than or equal to <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />.</p>
<p>
We can delete “nonnegative” in the above by the following simple idea. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> has two factors that are both congruent to <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> modulo <img src="https://s0.wp.com/latex.php?latex=%7B8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8}" class="latex" title="{8}" />. If not, then we can still factor, but I believe the idea is clearer without adding extra complications. Then solve in integers 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%288x+%2B+3%29%288y%2B3%29+%3D+N.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (8x + 3)(8y+3) = N. " class="latex" title="\displaystyle  (8x + 3)(8y+3) = N. " /></p>
<p>By assumption on <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> there is a solution of this equation. Moreover suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> are some solution. The key is that <img src="https://s0.wp.com/latex.php?latex=%7B8x+%2B+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8x + 3}" class="latex" title="{8x + 3}" /> cannot be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 1}" class="latex" title="{\pm 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B8y%2B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8y+3}" class="latex" title="{8y+3}" /> also cannot be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 1}" class="latex" title="{\pm 1}" />. Then it follows that each is also not equal to <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> in absolute value. Thus 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%3C+%7C8x%2B3%7C+%3C+N%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1 &lt; |8x+3| &lt; N, " class="latex" title="\displaystyle  1 &lt; |8x+3| &lt; N, " /></p>
<p>and we have factored <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />.</p>
<p>
</p><p></p><h3>  NP-hard </h3><p></p>
<p></p><p>
Jeff points out that it is unlikely that Diophantine problems are going to be classified by the NP-hard machinery. He says </p>
<blockquote><p><b> </b> <em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> shows the (possible) mismatch of “natural” Diophantine problems with the P versus NP question. </em>
</p></blockquote>
<p>Factoring is one of those in between problems that could be in P, but most believe that it could not be NP-hard.</p>
<p>
</p><p></p><h2> Rational Case </h2><p></p>
<p></p><p>
The problem of deciding whether a polynomial has a <i>rational</i> root is still open. That is given a polynomial with integer coefficients, does the equation 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%2Cy%2Cz%2C%5Cdots%29%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x,y,z,\dots)= 0, " class="latex" title="\displaystyle  F(x,y,z,\dots)= 0, " /></p>
<p>have a rational solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z,\dots}" class="latex" title="{x,y,z,\dots}" />? Of course the famous Hilbert’s Tenth problem asks for integer solutions of polynomials and is undecidable. The rational case is open. We have discussed it before <a href="https://rjlipton.wordpress.com/2010/08/07/hilberts-tenth-over-the-rationals/">here</a>. See <a href="https://sites.math.washington.edu/~morrow/336_16/2016papers/peter%20gc.pdf">this</a> for a survey. </p>
<p>
I had tried to see if we could at least prove the following: The decision problem over the rationals is at NP-hard. I thought for a while that I could show that solving equations over the rationals is at least NP-hard. My idea was to try to replace <img src="https://s0.wp.com/latex.php?latex=%28x%2B2%29%28y%2B2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x+2)(y+2)" class="latex" title="(x+2)(y+2)" /> by a equation that works over the rationals. The attempt failed. I was also unable to find a reference for such a result either. So perhaps it is open.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is it undecidable to determine whether a polynomial has a root over the rationals? Can we at least get that it is factoring hard?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/19/diophantine-equations/"><span class="datestr">at June 19, 2019 10:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/06/19/the-spring-season-is-behind-us/">The Spring season of TCS+ is behind us!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>With <a href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/">John Wright</a>‘s talk last week, the Spring ’19 season of TCS+ concluded. Thanks to all our followers who tuned in, everyone who <a href="https://sites.google.com/site/plustcs/suggest">suggested a talk</a> or spread the word, and, of course, thanks to <a href="https://sites.google.com/site/plustcs/past-talks">all our speakers</a>!</p>
<p>For those who missed a talk, or would like to watch them again in the comfort of your home, institution, or on the seaside: all past talks are now uploaded and available, along with the speakers’ slides.</p>
<ul>
<li>
<ul>
<li>Ran Canetti (Boston University and Tel Aviv University) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190206rancanettibostonuniversityandtelavivuniversity">Fully Bideniable Interactive Encryption</a></em></li>
<li>Sepehr Assadi (Princeton) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190220sepehrassadiprincetonuniversity">A Simple Sublinear-Time Algorithm for Counting Arbitrary Subgraphs via Edge Sampling</a></em></li>
<li>Shayan Oveis Gharan (University of Washington) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190306shayanoveisgharanuniversityofwashington">Strongly log concave polynomials, high dimensional simplicial complexes, and an FPRAS for counting Bases of Matroids </a></em></li>
<li>Aleksandar Nikolov (University of Toronto) on <em><a href="https://sites.google.com/site/plustcs/past-talks/03202019aleksandarnikolovuniversityoftoronto">Sticky Brownian Rounding and its Applications to Constraint Satisfaction Problems</a><br />
</em></li>
<li>Richard Peng (Georgia Tech) on <em><a href="https://sites.google.com/site/plustcs/past-talks/04032019richardpenggeorgiatech">Fully Dynamic Spectral Vertex Sparsifiers and Applications</a><br />
</em></li>
<li>Thatchaphol Saranurak (TTIC) on <em><a href="https://sites.google.com/site/plustcs/past-talks/04172019thatchapholsaranurakttic">Breaking Quadratic Time for Small Vertex Connectivity</a><br />
</em></li>
<li>Chris Peikert (University of Michigan) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190501chrispeikertuniversityofmichigan">Noninteractive Zero Knowledge for NP from Learning With Errors</a><br />
</em></li>
<li>Ewin Tang (University of Washington) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190515ewintanguniversityofwashington">Quantum-inspired classical linear algebra algorithms: why and how?</a><br />
</em></li>
<li>Lior Kamma (Aarhus University) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190529liorkammaaarhusuniversity">Lower Bounds for Multiplication via Network Coding</a><br />
</em></li>
<li>John Wright (MIT) on <em><a href="https://sites.google.com/site/plustcs/past-talks/20190612johnwrightmit">NEEXP in MIP*</a><br />
</em></li>
</ul>
</li>
</ul>
<p>Have a great summer, and see you in the Fall!</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/06/19/the-spring-season-is-behind-us/"><span class="datestr">at June 19, 2019 06:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-819529627944448924">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/06/postdoctoral-positions-at-gssi-on.html">Postdoctoral positions at the GSSI on modelling and verification for cyber-physical and smart systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The <a href="http://cs.gssi.it/">Computer Science group at the Gran Sasso Science Institute</a> (GSSI, an international PhD school and centre for advanced studies located in L’Aquila, Italy) has two available postdoctoral positions on topics related to modelling and verification for cyber-physical and smart systems. The positions are in the context of two PRIN projects funded by the Italian Ministry for University and Research, namely  "Designing Spatially Distributed Cyber-Physical Systems under Uncertainty (SEDUCE)" (Catia Trubiani, PI at the GSSI) and "IT MATTERS: Methods and Tools for Trustworthy Smart Systems" (Luca Aceto, PI at the GSSI).  Each position is for two years and the salary is 36,000€ per year before taxes. The positions are renewable for a third year if this is mutually agreeable.<br /><br />The positions are briefly described below. We foresee a close collaboration between the members of the teams working on the two projects.   The application must be submitted through the online form available at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/</a> by <b>Thursday, 11 July 2019 at 6 p.m. (Central European Time)</b>. For more information, please consult the Call for Applications at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/ </a>or write an email to the PIs of the respective projects.<br /><br />Note that the Computer Science group at the GSSI also has a further position financed by the institute, which can be filled in any of the research themes within the group. See <a href="http://cs.gssi.it/">http://cs.gssi.it</a> for more information.<br /><br />Requirements: Candidates are expected to have a PhD degree in Computer Science or related disciplines. Preference will be given to applicants who have previous research experience on topics related to the themes of the project.<br /><br />P<b>ostdoctoral position within the project "Designing Spatially Distributed Cyber-Physical Systems under Uncertainty"</b>, supervised by Catia Trubiani: <a href="https://cs.gssi.it/catia.trubiani">https://cs.gssi.it/catia.trubiani</a>.<br /><br />Brief description of the project: Emerging scenarios such as autonomous vehicles and the Internet-of-Things require large-scale cyber-physical systems (CPS), i.e., computing devices that interact with the physical world. To cope with their complexity, model-based design has long been advocated as a prominent approach for their rigorous development. However, the state of the art does not adequately account for two major issues: space, to capture the distribution of CPS devices as well as their mobility; and uncertainty, e.g., to reflect lack of knowledge about the environment, the accuracy of the model, or errors occurring in the real world. Our goal is to develop modelling and analysis techniques for CPS where space and uncertainty are first-class citizens. We envisage a component-based framework where digital and physical components have locality and mobility features, and where uncertainty is captured by means of probabilistically distributed activities to describe their dynamics. We devise a system to specify spatio-temporal CPS requirements, turning them into probabilistic spatio-temporal logical specifications that will be at the basis of efficient algorithms for the analysis, verification, and synthesis. We will apply our techniques to real case studies on smart buildings and crowd-navigating robots.<br /><br />Main activities: The successful candidate will develop a component-based modelling language for the specification of cyber physical systems, where digital components (i.e., the computational devices), need to co-exist with physical components (i.e., the dynamical models of the physical world). Each component will feature an appropriate location and mobility model. Uncertainty will be specified with either probabilistically distributed activities, or nondeterministically, leading to reasoning that will take into account worst- and best-case scenarios under any possible value of such uncertain actions. The modelling framework will allow the specification of spatio-temporal requirements, which establish the behavioural guarantees to be satisfied by the CPS under analysis.   The postdoctoral researcher will work with Catia Trubiani at the GSSI, and she/he will also collaborate with the project partners at IMT Lucca (Mirco Tribastone, coordinator of the project), University of Trieste (Luca Bortolussi and Stefano Seriani), and University of Camerino (Francesco Tiezzi).<br /><br /><br /><b>Postdoctoral position within the project "IT MATTERS: Methods and Tools for Trustworthy Smart Systems"</b> supervised by Luca Aceto: <a href="http://www.gssi.it/people/professors/lectures-computer-science/item/225-aceto-luca">http://www.gssi.it/people/professors/lectures-computer-science/item/225-aceto-luca</a>.<br /><br />Brief description of the project: The goal of the project is the development and the application of a novel methodology for the specification, implementation and validation of trustworthy smart systems based on formal methods. We envisage system development in three steps by first providing and analysing system models to find design errors, then moving from models to executable code by translation into domain-specific programming languages and, finally, monitoring runtime execution to detect anomalous behaviours and to support systems in taking context-dependent decisions autonomously. Scientifically, the research will yield new, fundamental insights on the general properties of large scale, physically located, smart systems, leading to an end-to-end, principled approach to their design, implementation and deployment. The developed software tools and the work on the case studies will show the effectiveness of our proposed approach in three practical scenarios at different application scales.<br /><br />Main activities: The successful candidate will develop runtime-monitoring and software-model-checking techniques for “smart systems”, that is, systems that take context-dependent decisions autonomously. The research activities will involve developing frameworks that can deal with the distributed nature of smart systems and will  build on existing work on specification-based monitoring and software model checking of cyber-physical systems. The postdoctoral researcher will also devise techniques to use information derived from the runtime analysis to guide the software-model-checking effort and to refine the models of the runtime environment used in model checking.    The postdoctoral researcher will work with Luca Aceto, Omar Inverso, Ludovico Iovino and Emilio Tuosto at the GSSI. He/she will also collaborate with the project partners at IMT Lucca, CNR Pisa, University of Camerino, University of Pisa and University of Udine. Moreover, he/she will also interact with the concurrency group at ICE-TCS (<a href="http://icetcs.ru.is/">http://icetcs.ru.is/</a>), Reykjavik University, led by Luca Aceto and Anna Ingólfsdóttir.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/06/postdoctoral-positions-at-gssi-on.html"><span class="datestr">at June 19, 2019 11:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2592965474602859939">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/why-does-nevalina-prize-now-abacus-got.html">Why does the Nevalina Prize (now Abacus) got to Algorithms/Complexity people</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In my post about the Nevanlinna prize  name change (see <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">here</a>) one of my readers raised a different question about the prize:<br />
<br />
BEGIN QUOTE<br />
<br />
<div style="text-align: start;">
<br /></div>
<span>So there's one of my main questions about the prize answered (or at least resolved). The second remains. The IMU's website(which still refers to the Nevanlinna Prize) says that it is awarded "for outstanding contributions in Mathematical Aspects of Information Sciences including:"</span><br />
<br />
<span>1)All mathematical aspects of computer science, including complexity theory, logic of programming languages, analysis of algorithms, cryptography, computer vision, pattern recognition, information processing and modelling of intelligence.</span><br />
<br />
<span>2)Scientific computing and numerical analysis. Computational aspects of optimization and control theory. Computer algebra.</span><br />
<br />
<span>Correct me if I'm wrong, but it seems that the recognized work of the ten winners of the award all fits into two or three of the possible research areas for which the prize may be rewarded. Why do people think that this is the case?</span><br />
<br />
<br />
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">END QUOTE</span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">First off, lets see if this is true. Here is a list of all the winners:</span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">Tarjan, Valiant, Razborov, Wigderson, Shor, Sudan, Kleinberg, Spielman, Khot, Daskalakis </span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">Yup, they all seem to be in Algorithms or Complexity.</span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">Speculation as to why:</span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">1) Algorithms and Complexity have problems with short descriptions that can easily be understood: Tarjan proved Planarity was in O(n) time. Valiant defined Sharp-P and showed the Permanent was Sharp-P complete. Hence it is easy to see what they have done. In many of the fields listed, while people have done great work, it may be harder to tell since the questions are not as clean.  If my way to do Vision is better than your way to do Vision, that may be hard to prove. And the proof  may need to be non-rigorous.</span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">2) If someone does great work in (for example) Logic of Programming Languages, it may not be recognized until she is past 40 years old. </span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">3) This one I am less sure of (frankly I'm not that sure of any of these and invite polite commentary): areas that are more practical may take longer to build and get to work.</span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">But there is still a problem with this. Numerical Analysis and Cryptography would seem to not have these problems. </span></span><br />
<span><span style="background-color: #ccff99; font-size: 14px;"><br /></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">I invite the reader to speculate</span></span></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/why-does-nevalina-prize-now-abacus-got.html"><span class="datestr">at June 18, 2019 12:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/">Robustness in Learning and Statistics: Past and Future</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 12-15, 2019 UC San Diego http://cseweb.ucsd.edu/~slovett/workshops/robust-statistics-2019/ Registration deadline: July 31, 2019 Robust statistics and related topics offer ways to stress test estimators to the assumptions they are making. It offers insights into what makes some estimators behave well in the face of model misspecification, while others do not. In this summer school, we will … <a href="https://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/" class="more-link">Continue reading <span class="screen-reader-text">Robustness in Learning and Statistics: Past and Future</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/"><span class="datestr">at June 18, 2019 12:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=23">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2019/06/17/hello-world/">Hello World!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Welcome to my blog! My name is Gautam Kamath, and I just started as an assistant professor in computer science at the University of Waterloo (for more info, see <a href="https://kamathematics.wordpress.com/about/">About</a>).</p>



<p>This blog will, broadly speaking, be about topics relevant to those interested in the the theory of computer science, statistics, and machine learning. Posts will range from technical, to informational, to meta (read: basically whatever I want to write about, but I’ll do my best to keep it topical). Stay tuned!</p>



<p>The unofficial theme song of this blog is <a href="https://www.youtube.com/watch?v=8Ir-zFC9nFE">“Mathematics” by Mos Def</a>.</p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2019/06/17/hello-world/"><span class="datestr">at June 17, 2019 11:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/">3rd School on Foundations of Programming and Software Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 10-15, 2019 Warsaw, Poland https://www.mimuw.edu.pl/~fopss19/ The Summer School on Foundations of Programming and Software Systems (FoPSS) was jointly created by EATCS, ETAPS, ACM SIGLOG and ACM SIGPLAN. It was first organised in 2017. The goal is to introduce the participants to various aspects of computation theory and programming languages. The school, spread over a … <a href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/" class="more-link">Continue reading <span class="screen-reader-text">3rd School on Foundations of Programming and Software Systems</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/"><span class="datestr">at June 17, 2019 12:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16001">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/17/contraction-and-explosion/">Contraction and Explosion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Different ways of recursing on graphs</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<img width="120" alt="TutteBletchley" src="https://rjlipton.files.wordpress.com/2019/06/tuttebletchley.jpg?w=120&amp;h=200" class="alignright wp-image-16003" height="200" />
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Bletchley Park 2017 <a href="https://bletchleypark.org.uk/news/codebreaker-bill-tutte-to-be-celebrated-in-centenary-exhibition">source</a></font></td>
</tr>
</tbody>
</table>
<p>
William Tutte was a British combinatorialist and codebreaker. He worked in a different group at Bletchley Park from that of Alan Turing. He supplied several key insights and algorithms for breaking the Lorenz cipher <a href="https://en.wikipedia.org/wiki/Lorenz_cipher">machine</a>. His algorithms were implemented alongside Turing’s on <a href="https://en.wikipedia.org/wiki/Colossus_computer">Colossus</a> code-breaking computers.</p>
<p>
Today we discuss graph recursions discovered by Tutte and Hassler Whitney.</p>
<p>
Tutte wrote a doctoral thesis after the war on graph theory and its generalization into <em>matroid theory</em>. We will follow the same arc in this and a followup post. He joined the faculty of the universities of Toronto and then Waterloo, where he was active long beyond his retirement. </p>
<p>
For more on Tutte and his work, see this <a href="https://theconversation.com/remembering-bill-tutte-another-brilliant-codebreaker-from-world-war-ii-77556">article</a> and <a href="http://thelaborastory.com/stories/william-thomas-tutte/">lecture</a> by Graham Farr, who is a professor at Monash University and a longtime friend of Ken’s from their Oxford days. We covered some of Tutte’s other work <a href="https://rjlipton.wordpress.com/2010/06/02/the-tensor-trick-and-tuttles-flow-conjectures/">here</a>.</p>
<p></p><h2> Deletion and Contraction </h2><p></p>
<p></p><p>
The two most basic recursion operations are <em>deleting</em> and <em>contracting</em> a chosen edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> in a given graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G = (V,E)}" class="latex" title="{G = (V,E)}" />:</p>
<p><img width="400" alt="deletion_contraction" src="https://rjlipton.files.wordpress.com/2019/06/deletion_contraction.png?w=400&amp;h=118" class="aligncenter wp-image-16004" height="118" /></p>
<p>
These operations produce graphs denoted by <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />, respectively. A motive for them harks back to Gustav Kirchhoff’s counting of spanning trees:</p>
<ul>
<li>
A spanning tree of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> avoids using edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> if and only if it is a spanning tree of the graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> with <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> deleted.<p></p>
</li><li>
A spanning tree of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> uses edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> if and only if the rest of it is a spanning tree of the graph <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" /> after contracting <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />.
</li></ul>
<p>
Well, this is not how Kirchhoff counted trees. Counting via the recursion would take exponential time. Our whole object will be telling which cases of the recursions can be computed more directly.</p>
<p>Note that contracting one edge of the triangle graph <img src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_3}" class="latex" title="{C_3}" /> produces a <em>multi-</em>graph <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" /> with one double-edge. Then contracting one edge of <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" /> yields the loop graph <img src="https://s0.wp.com/latex.php?latex=%7BC_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_1}" class="latex" title="{C_1}" />.</p>
<p></p><p><br />
<img width="250" alt="triangle" src="https://rjlipton.files.wordpress.com/2019/06/triangle.png?w=250&amp;h=73" class="aligncenter wp-image-16005" height="73" /></p>
<p></p><p><br />
Thus contraction yields non-simple undirected graphs, but the logic of counting their spanning trees remains valid.</p>
<p>
The order of edges does not matter as long as one avoids disconnecting the graph, and the base case is a tree (ignoring any loops) which contributes <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />.</p>
<p></p><h2> Tutte’s Polynomial </h2><p></p>
<p></p><p>
A similar recursion counts colorings <img src="https://s0.wp.com/latex.php?latex=%7Bc%3A+V+%5Crightarrow+%5C%7B1%2C...%2Ck%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c: V \rightarrow \{1,...,k\}}" class="latex" title="{c: V \rightarrow \{1,...,k\}}" /> that are <em>proper</em>, meaning that for each edge <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) \neq c(v)}" class="latex" title="{c(u) \neq c(v)}" />.</p>
<ul>
<li>
A proper coloring <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) \neq c(v)}" class="latex" title="{c(u) \neq c(v)}" /> iff it is a proper coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.<p></p>
</li><li>
A proper coloring <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%3D+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) = c(v)}" class="latex" title="{c(u) = c(v)}" /> iff it induces a proper coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />.
</li></ul>
<p>
This leads to the recursive definition of the <em>chromatic polynomial</em>:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++P_G%28x%29+%3D+P_%7BG%5Csetminus+e%7D%28x%29+-+P_%7BG%2Fe%7D%28x%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   " class="latex" title="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   " /></p>
<p>
The base cases are that an isolated vertex contributes <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, whereas an isolated loop contributes <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> since its single edge is never properly colored. The final rule is that <img src="https://s0.wp.com/latex.php?latex=%7BP_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_G(x)}" class="latex" title="{P_G(x)}" /> is always the product of <img src="https://s0.wp.com/latex.php?latex=%7BP_H%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_H(x)}" class="latex" title="{P_H(x)}" /> over all connected components <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7BP_G%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_G(k)}" class="latex" title="{P_G(k)}" /> counts the number of proper <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-colorings.</p>
<p>
This is like the recursion for coutning spanning terees except for the minus sign. Tutte’s brilliant insight, which was anticipated by Whitney in less symbolic form, was that the features can be combined by using two variables <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. Call an edge a <a href="https://en.wikipedia.org/wiki/Bridge_(graph_theory)">bridge</a> if it is not part of any cycle. If <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is not a bridge, the recursion is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++T_G%28x%2Cy%29+%3D+T_%7BG+%5Csetminus+e%7D%28x%2Cy%29+%2B+T_%7BG%2Fe%7D%28x%2Cy%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   " class="latex" title="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   " /></p>
<p>
The base case is now a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> with some number <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> of bridges and some number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> of loops, which gives <img src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2Cy%29+%3D+x%5Ek+y%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(x,y) = x^k y^\ell}" class="latex" title="{T_G(x,y) = x^k y^\ell}" />. An important feature is that all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex trees have the same Tutte polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{n-1}}" class="latex" title="{x^{n-1}}" />, since there are <img src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n-1}" class="latex" title="{n-1}" /> edges and they are all bridges. The following are just some of the beautiful <a href="https://en.wikipedia.org/wiki/Tutte_polynomial#Specialisations">rules</a> that <img src="https://s0.wp.com/latex.php?latex=%7BT_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G}" class="latex" title="{T_G}" /> follows. Let <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+c_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = c_G}" class="latex" title="{c = c_G}" /> stand for the number of connected components of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.</p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1,1)}" class="latex" title="{T_G(1,1)}" /> counts the number of spanning trees forests. This counts the number of spanning trees if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is connected.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281-x%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1-x,0)}" class="latex" title="{T_G(1-x,0)}" />, when multiplied by <img src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5E%7Bn-c%7D+x%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(-1)^{n-c} x^c}" class="latex" title="{(-1)^{n-c} x^c}" />, yields the chromatic polynomial.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1,2)}" class="latex" title="{T_G(1,2)}" /> counts the number of spanning subgraphs.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%282%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(2,2)}" class="latex" title="{T_G(2,2)}" /> is just <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B%7CE%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{|E|}}" class="latex" title="{2^{|E|}}" />.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2C%5Cfrac%7B1%7D%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(x,\frac{1}{x})}" class="latex" title="{T_G(x,\frac{1}{x})}" /> gives the <a href="https://en.wikipedia.org/wiki/Jones_polynomial">Jones polynomial</a> of a knot related to <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.
</li></ul>
<p>
There are many further relations. The Jones polynomial has many applications including in quantum physics.</p>
<p></p><h2> Contraction With a Twist </h2><p></p>
<p>
Recall our definition of the “amplitude” <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> of an undirected <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> from the “Net-Zero Graphs” <a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/">post</a>:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+%5Cfrac%7Bc_0+-+c_1%7D%7B2%5En%7D%2C+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   " class="latex" title="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   " /></p>
<p>
where <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0}" class="latex" title="{c_0}" /> is the number of black-and-white 2-colorings that make an even number of edges have both nodes colored black, and <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1}" class="latex" title="{c_1}" /> for an odd number. </p>
<p>
There does not seem to be a simple recursion for <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> from <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />. We can, however, obtain one by using another kind of contraction that adds a loop at the combined vertex:</p>
<p><img width="250" alt="inchworm_contraction" src="https://rjlipton.files.wordpress.com/2019/06/inchworm_contraction.png?w=250&amp;h=128" class="aligncenter wp-image-16006" height="128" /></p>
<p>
We denote this by <img src="https://s0.wp.com/latex.php?latex=%7BG%2F%5C%21%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/\!/e}" class="latex" title="{G/\!/e}" />. We have not found a simple reference for this. We obtain the following recursive formula:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G%5Cbackslash+e%29+%2B+%5Cfrac%7B1%7D%7B2%7Da%28G%2F%5C%21%2Fe%29+-+%5Cfrac%7B1%7D%7B2%7Da%28G%2Fe%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   " class="latex" title="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   " /></p>
<p>
This recursion allows <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> to be a bridge, so the base cases are <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for an isolated vertex and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for a loop. More generally, the basis is <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for a node with an even number of loops, <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for odd. Here is an example for the ‘star graph’ <img src="https://s0.wp.com/latex.php?latex=%7BS_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_4}" class="latex" title="{S_4}" /> on 4 vertices:</p>
<p></p><p><br />
<img width="520" alt="starGraph" src="https://rjlipton.files.wordpress.com/2019/06/stargraph.png?w=520&amp;h=294" class="aligncenter wp-image-16007" height="294" /></p>
<p></p><p><br />
The diagram would need another layer to get down to (products of) base cases, which we have shortcut by putting values of <img src="https://s0.wp.com/latex.php?latex=%7Ba%28H%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(H)}" class="latex" title="{a(H)}" /> for each graph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> at a leaf. Adding the products over all branches gives <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" />. For the star graph,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28S_4%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B8%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+%3D+%5Cfrac%7B1%7D%7B2%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   " class="latex" title="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   " /></p>
<p></p><p><br />
Clearly this brute-force recursion grows as <img src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3^n}" class="latex" title="{3^n}" />. This is slower than the order-<img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" /> time of using the coloring definition directly, but what all this underscores is how singular it is to be able to compute <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> in polynomial time, indeed <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. The search for a more-efficient recursion, one that might apply to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-hard quantities, leads us to consider a more-drastic operation on edges.</p>
<p></p><h2> Exploding Edges </h2><p></p>
<p>
The new recursion operation is well illustrated by this figure:</p>
<p></p><p><br />
<img width="300" alt="explosionSolo" src="https://rjlipton.files.wordpress.com/2019/06/explosionsolo.png?w=300&amp;h=203" class="aligncenter wp-image-16008" height="203" /></p>
<p></p><p><br />
Two vertices disappear, not just one. Not only does the edge <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" /> disappear, but any other edge incident to <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> from a vertex <img src="https://s0.wp.com/latex.php?latex=%7Bw+%5Cneq+u%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w \neq u,v}" class="latex" title="{w \neq u,v}" /> gets “recoiled” into a loop at <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. We denote this operation by <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> to connote that <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is not just deleted but “exploded.” </p>
<p>
Properly speaking, we need to specify what happens if there are other edges between <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> or loops at <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" />. In an upcoming post we will see that those become <em>circles</em> in a <em>graphical polymatroid</em> which generalizes the notion of a graph. For now, however, it suffices to let <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> be the total number of vaporized edges, including <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />. Then we obtain a two-term recursive formula:</p>
<p><a name="expl"></a></p><a name="expl">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Cbackslash+e%29+%2B+%5Cfrac%7B%28-1%29%5Er%7D%7B2%7D+a%28G%5Cbackslash+%5C%21%5Cbackslash+e%29.+++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)" class="latex" title="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)" /></p>
</a><p><a name="expl"></a></p>
<p>
The base cases for isolated vertices are the same as before, but explosion also needs a base case for pure emptiness. This contributes <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. In the following example diagram, for the path graph <img src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_4}" class="latex" title="{P_4}" /> on four nodes, we denote such base cases by `w’ for “wisp”:</p>
<p><img width="450" alt="P3explosion" src="https://rjlipton.files.wordpress.com/2019/06/p3explosion.png?w=450&amp;h=291" class="aligncenter wp-image-16009" height="291" /></p>
<p>
Note again the rule that when the recursion disconnects the graph, the component values multiply together. Thus the value is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28P_4%29+%3D+%281+-+%5Cfrac%7B1%7D%7B2%7D%29%281+-+%5Cfrac%7B1%7D%7B2%7D%29+-+0+%3D+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   " class="latex" title="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   " /></p>
<p>
This is different from the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> of the star graph. What this means is that <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> does not obey the rules of the Tutte polynomial, which is the same for both of these 4-vertex trees. </p>
<p>
To prove the recursion equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>), for <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" />, note that every coloring has the same odd/even parity of black-black edges for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G\setminus e}" class="latex" title="{G\setminus e}" /> as for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> except those that color both <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> black. Let <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0^{uv}}" class="latex" title="{c_0^{uv}}" /> denote the colorings among the latter that make an even number of black-black edges (including <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />) overall, <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1^{uv}}" class="latex" title="{c_1^{uv}}" /> for an odd number. Then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Csetminus+e%29+%2B+%5Cfrac%7B2%7D%7B2%5En%7D%28c_1%5E%7Buv%7D+-+c_0%5E%7Buv%7D%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   " class="latex" title="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   " /></p>
<p>
Now if there are no other edges between or loops at <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" />, then <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1^{uv}}" class="latex" title="{c_1^{uv}}" /> is the same as the number of colorings of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> that make an even number of black-black edges, and <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0^{uv}}" class="latex" title="{c_0^{uv}}" /> becomes the odd case in <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> again because we subtracted <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />. Considering the sign change from other <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(u,v)}" class="latex" title="{(u,v)}" /> edges or loops and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%7D%7B2%5En%7D+%3D+%5Cfrac%7B1%2F2%7D%7B2%5E%7Bn-2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}" class="latex" title="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}" /> yields equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>).  It is also possible to “explode” a loop, and our readers may enjoy figuring out how to define it.</p>
<p></p><h2> The Amplitude Polynomial </h2><p></p>
<p></p><p>
We can expand on this by defining a polynomial <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+Q_G%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G) = Q_G(1)}" class="latex" title="{a(G) = Q_G(1)}" />. The base cases are <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for an isolated vertex but still <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for a “wisp” and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for a loop. The basis extends to give <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for an isolated node with an even number of loops and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for odd. Another way to put it is that two edges with the same endpoints, or two loops at the same node, can be removed. The above diagram shows that for the path graph <img src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_4}" class="latex" title="{P_4}" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++Q_%7BP_4%7D%28x%29+%3D+%28x%5E2+-+%5Cfrac%7B1%7D%7B2%7D%29%5E2+%3D+x%5E4+-+x%5E2+%2B+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   " class="latex" title="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   " /></p>
<p>
Whereas, the recursion for the star graph—noting that the “star” <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> on two nodes is just a single edge—gives:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++Q_%7BS_4%7D%28x%29+%26%3D%26+xQ_%7BS_3%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E2+Q_%7BS_2%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E4+-+x%5E2+%5Cfrac%7B1%7D%7B2%7D%281%5C%21%5Ccdot%5C%21+1%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0+%3D+x%5E4+-+%5Cfrac%7B1%7D%7B2%7Dx%5E2.+++%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} " /></p>
<p>
This is not the same polynomial as <img src="https://s0.wp.com/latex.php?latex=%7BQ_%7BP_4%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_{P_4}(x)}" class="latex" title="{Q_{P_4}(x)}" />, again implying that <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> is not a specialization of the Tutte polynomial. We will show in the last post in this series that <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> does specialize the polynomial <img src="https://s0.wp.com/latex.php?latex=%7BS_G%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_G(x,y)}" class="latex" title="{S_G(x,y)}" /> introduced in this 1993 <a href="http://homepages.mcs.vuw.ac.nz/~whittle/pubs/Tutte_invariants_of_2-polymatroids.pdf">paper</a> titled, “A Characterization of Tutte Invariants of 2-Polymatroids” and covered further in this 2006 <a href="https://www.researchgate.net/publication/49399603_Evaluating_the_Rank_Generating_Function_of_a_Graphic_2-Polymatroid">paper</a>.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
What other rules does our “amplitude polynomial” follow?  We will explore this in the mentioned upcoming post.  What other quantities can it be made to count?</p>
<p>
What we called “explosion” is in fact attested as the natural form of <em>contraction</em> for the <b>polymatroids</b> considered in these papers. What further uses might “explosion” have in graph theory apart from polymatroids?</p></font></font></div>







<p class="date">
by Chaowen Guan and K.W. Regan <a href="https://rjlipton.wordpress.com/2019/06/17/contraction-and-explosion/"><span class="datestr">at June 17, 2019 08:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/06/16/modeconnectivity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/06/16/modeconnectivity/">Landscape Connectivity of Low Cost Solutions for Multilayer Nets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A big mystery about deep learning is how, in a highly nonconvex loss landscape, gradient descent often finds near-optimal solutions —those with training cost almost zero— even starting from a random initialization. This conjures an image of a landscape filled with deep pits.  Gradient descent started at a random point falls easily to the bottom of the nearest pit. In this mental image the pits are disconnected from each other, so there is no way to go from the bottom of one pit to bottom of another without going through regions of high cost.</p>

<p>The current post is about our <a href="https://arxiv.org/abs/1906.06247">new paper with Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Wei Hu, Zhiyuan Li and Sanjeev Arora</a> which provides a mathematical explanation of  the following surprising phenomenon reported last year.</p>

<blockquote>
  <p><strong>Mode connectivity</strong> (<a href="https://arxiv.org/abs/1611.01540">Freeman and Bruna, 2016</a>, <a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00885">Draxler et al. 2018</a>) All pairs of low-cost solutions found via gradient descent  can actually be connected by simple paths in the parameter space, such that every point on the path is another solution of almost the same cost. In fact the low-cost path connecting two near-optima  can be <em>piecewise linear</em> with two line-segments, or a Bezier curve.</p>
</blockquote>

<p>See Figure 1 below from <a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a> for an illustration. Solutions A and B have low cost but the line connecting them goes through solutions with high cost. But we can find C of low cost such that paths AC and CB only pass through low-cost region.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/modes.PNG" style="width: 300px;" />
<br />
<b>Figure 1</b> Mode Connectivity. Warm colors represent low loss. 
</div>

<p>Using a very simple example let us see that this phenomenon is highly counterintuitive. Suppose we’re talking about 2-layer nets with linear activations and a real-valued output. Let the two nets $\theta_A$ and 
$\theta_B$  with zero loss be
<br />
respectively where $x, U_1, U_2 \in \Re^n$
and matrices $W_1, W_2$ are $n\times n$. Then the straight line connecting them in parameter space corresponds to nets of the type $(\alpha U_1 + (1-\alpha)U_2)^\top(\alpha W_1 + (1-\alpha)W_2)$ which can be rewritten as</p>
<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/hybridnet.jpg" style="width: 650px;" /> 
</div>

<p>Note that the middle terms correspond to putting the top layer of one net on top of the bottom of the other, which in general is a nonsensical net (reminiscent of a <em>centaur</em>, a mythical half-man half-beast) that in general would be expected to have high loss.</p>

<p>Originally we figured mode connectivity would not be mathematically understood for a long time, because of the seeming difficulty of proving any mathematical theorems  about, say, $50$-layer nets trained on ImageNet data, and in particular dealing with such “centaur-like” nets in the interpolation.</p>

<p>Several authors (<a href="https://arxiv.org/abs/1611.01540">Freeman and Bruna, 2016</a>, <a href="https://arxiv.org/abs/1802.06384">Venturi et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00909">Liang et al. 2018</a>, <a href="https://arxiv.org/abs/1809.10749">Nguyen et al. 2018</a>, <a href="https://arxiv.org/abs/1901.07417">Nguyen et al. 2019</a>) did try to explain the phenomenon of mode connectivity in simple settings (the first of these demonstrated mode connectivity empirically for multi-layer nets). But these explanations only work for very unrealistic 2-layer nets (or multi-layer nets with special structure) which are highly redundant e.g., the number of neurons may have to be larger than the number of training samples.</p>

<p>Our paper starts by clarifying an important point: redundancy with respect to a ground truth neural network is  insufficient for mode connectivity, which we show via a simple counterexample sketched below.</p>

<p>Thus to explain mode connectivity for multilayer nets we  will need to leverage some stronger property of <em>typical</em> solutions discovered via gradient-based training, as we will see below.</p>

<h2 id="mode-connectivity-need-not-hold-for-2-layer-overparametrized-nets">Mode Connectivity need not hold for 2-layer overparametrized nets</h2>

<p>We show that the strongest version of mode connectivity (every two minimizers are connected) does not hold even for a simple two-layer setting, where $f(x) = W_2\sigma(W_1x)$, even where the net is vastly overparametrized than it needs to be for the dataset in question.</p>

<blockquote>
  <p><strong>Theorem</strong> For any $h&gt;1$ there exists a data set which is perfectly fitted by  a ground truth neural network with $2$ layers and only $2$ hidden neurons, but if we desire to train neural network with $h$ hidden units on this dataset then the set of global minimizers are not connected.</p>
</blockquote>

<h2 id="stability-properties-of-typical-nets">Stability properties of typical nets</h2>

<p>Since mode connectivity has been found to hold for a range of architectures and datasets, any explanation probably should only rely upon properties that <em>generically</em> seem to hold for deep net standard training. Our explanation relies upon properties that were discovered in recent years in the effort to understand the generalization properties of deep nets.  These properties say that the output of the final net is stable to various kinds of added noise.
The properties imply that the loss function does not change much when the net parameters are perturbed; this is informally described as the net being a <em>flat minimum</em> (<a href="https://www.cs.toronto.edu/~hinton/absps/colt93.html">Hinton and Van Camp 1993</a>).</p>

<p>Our explanation of mode connectivity will involve the following two properties.</p>

<h3 id="noise-stability-and-dropout-stability">Noise stability and Dropout Stability</h3>

<p><em>Dropout</em> was introduced by <a href="https://arxiv.org/abs/1207.0580">Hinton et al. 2012</a>: during gradient-based training, one  zeroes out the output of $50\%$ of the nodes, and doubles the output of the remaining nodes. The gradient used in the next update is computed for this net. While dropout may not be as popular these days, it can be added to any existing net training without loss of generality. We’ll say a net is “$\epsilon$-dropout stable” if applying dropout to $50\%$ of the nodes increases its loss by at most $\epsilon$. Note that unlike dropout training where nodes are <em>randomly</em> dropped out, in our definition a network is dropout stable as long as there <em>exists</em> a way of dropping out $50\%$ of the nodes that does not increase its loss by too much.</p>

<blockquote>
  <p><strong>Theorem 1:</strong> If two trained multilayer ReLU nets with the same architecture  are $\epsilon$-dropout stable, then they can be connected in the loss landscape via a piece-wise linear path in which the number of linear segments is linear in the number of layers, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<p><em>Noise stability</em> was discovered by <a href="https://arxiv.org/abs/1802.05296">Arora et al. ICML18</a>; this   was described in a <a href="http://www.offconvex.org/2018/02/17/generalization2/">previous blog post</a>. They found that trained nets are very stable to noise injection: if one adds a fairly large Gaussian noise vector to the output of a layer, then this has only a small effect on the output of higher layers. In other words, the network <em>rejects</em> the injected noise. That paper showed that noise stability can be used to prove that the net is compressible. Thus noise stability is indeed a form of redundancy in the net.</p>

<p>In the new paper we show that a minor variant of the noise stability property (which we empirically find to still hold in trained nets) implies dropout stability. More importantly, solutions satisfying this property can be connected using a piecewise linear path with at most $10$ segments.</p>

<blockquote>
  <p><strong>Theorem 2:</strong> If two trained multilayer ReLU nets with the same architecture are $\epsilon$-noise stable, then they can be connected in the loss landscape via a piece-wise linear path with at most 10 segments, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<h2 id="proving-mode-connectivity-for-dropout-stable-nets">Proving mode connectivity for dropout-stable nets</h2>
<p>We exhibit the main ideas by proving mode connectivity for  fully connected nets that are dropout-stable, meaning training loss is stable to dropping out $50\%$ of the nodes.</p>

<p>Let $W_1,W_2,…,W_p$ be the weight matrices of the neural network, so the function that is computed by the network is $f(x) = W_p\sigma(\cdots \sigma(W_2(\sigma(W_1x)))\cdots)$. Here $\sigma$ is the ReLU activation (our result in this section works for any activations). We use $\theta = (W_1,W_2,…,W_p)\in \Theta$ to denote the parameters for the neural network. Given a set of data points $(x_i,y_i)~i=1,2,…,n$, the empirical loss $L$ is just an average of the losses for the individual samples $L(\theta) = \frac{1}{n}\sum_{i=1}^n l(y_i, f_\theta(x_i))$. The function $l(y, \hat{y})$ is a loss function that is convex in the second parameter (popular loss functions such as cross-entropy or mean-squared-error are all in this category).</p>

<p>Using this notation, Theorem 1 can be restated as:</p>

<blockquote>
  <p><strong>Theorem 1 (restated)</strong> Let $\theta_A$ and $\theta_B$ be two solutions that are both $\epsilon$-dropout stable, then there exists a path $\pi:[0,1]\to \Theta$ such that $\pi(0) = \theta_A$, $\pi(1) = \theta_B$ and for any $t\in(0,1)$ the loss $L(\pi(t)) \le \max{L(\theta_A), L(\theta_B)} + \epsilon$.</p>
</blockquote>

<p>To prove this theorem, the major step is to connect a network with its dropout version where half of the neurons are not used (see next part). Then intuitively it is not too difficult to connect two dropout versions as they both have a large number of inactive neurons.</p>

<p>As we discussed before, directly interpolating between two networks may not work as it give rise to <em>centaur-like</em> networks.  A key idea in this simpler theorem is that each linear segment in the path involves varying the parameters of only one layer, which allows careful control of this issue. (Proof of Theorem 2 is more complicated because the number of layers in the net are allowed to exceed the number of path segments.)</p>

<p>As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:
As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/path.png" style="width: 400px;" /> <br />
<b>Figure 2</b> Path from a 3-layer neural network to its dropout version.
</div>

<p>We use two types of steps to construct the path: (a) Since the loss function is convex in the weight of the top layer, we can interpolate between two different networks that only differ in top layer weights; (b) if a set of neurons already has 0 output weights, then we can set its input weights arbitrarily.</p>

<p>Figure 2 shows how to alternate between these two types of steps to connect a 3-layer network to its dropout version. The red color highlights weights that have changed. In the case of type (a) steps, the red color only appears in the top layer weights; in the case of type (b) steps, the 0 matrices highlighted by the green color are the 0 output weights, where because of these 0 matrices setting the red blocks to any matrix will not change the output of the neural network.</p>

<p>The crux of this construction appears in steps (3) and (4). When we are going from (2) to (3), we changed the bottom rows of $W_2$ from $[D_2, R_2]$ to $[2L_2, 0]$. This is a type (b) step, and because currently the top-level weight is $[2L_3, 0]$, changing the bottom row of $W_2$ has no effect on the output of the neural network. However, making this change allows us to do the interpolation between (3) and (4), as now the two networks only differ in the top layer weights. The loss is bounded because the weights in (3) are equivalent to $(2[L_3, 0], W_2, W_1)$ (weights with dropout applied to top hidden layer), and the weights in (4) are equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ (weights with dropout applied to both hidden layers). The same procedure can be repeated if the network has more layers.</p>

<p>The number of line segments in the path is linear in the number of layers. As mentioned, the paper also gives stronger results assuming noise stability, where we can actually consruct a path with constant number of line segments.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Our results are a first-cut explanation for how mode connectivity can arise in realistic deep nets. Our methods do not answer all mysteries about mode connectivity. In particular, in many cases (especially when the number of parameters is not as large) the solutions found in practice are not as robust as we require in our theorems (either in terms of dropout stability or noise stability), yet empirically it is still possible to find simple paths connecting the solutions. Are there other properties satisfied by these solutions that allow them to be connected? Also, our results can be extended to convolutional neural networks via <em>channel-wise dropout</em>, where one randomly turn off half of the channels (this was considered before in <a href="https://arxiv.org/abs/1411.4280">Thompson et al. 2015</a>,<a href="https://arxiv.org/abs/1812.03965">Keshari et al.2018</a>). While it is possible to train networks that are robust to channel-wise dropout, standard networks or even the ones trained with standard dropout do not satisfy this property.</p>

<p>It would also be interesting to utilize the insights into the landscape given by our explanation to design better training algorithms.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/06/16/modeconnectivity/"><span class="datestr">at June 16, 2019 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/088">TR19-088 |  On the Complexity of Estimating the Effective Support Size | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Loosely speaking, the effective support size of a distribution is the size of the support of a distribution that is close to it (in totally variation distance). 
We study the complexity of estimating the effective support size of an unknown distribution when given samples of the distributions as well as an evaluation oracle (which returns the probability that the queried element appears in the distribution).
In this context, we present several algorithms that exhibit a trade-off between the quality of the approximation and the complexity of obtaining it, and leave open the question of their optimality.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/088"><span class="datestr">at June 16, 2019 03:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e20240a466212d200c">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/erickson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html">Buy My Free Book!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-popup" class="asset-img-link"><img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-500wi" alt="Al-khwarizmi" style="display: block; margin-left: auto; margin-right: auto;" class="asset  asset-image at-xid-6a00d83452383469e20240a48f6b37200d img-responsive" title="Al-khwarizmi" /></a></p>
<p>I'm happy to finally announce the publication of an actual dead-tree paperback edition of my <em>Algorithms</em> textbook.  The book can be purchased from Amazon (<a href="https://www.amazon.com/dp/1792644833">US</a>, <a href="https://www.amazon.co.uk/dp/1792644833">UK</a>, <a href="https://www.amazon.de/dp/1792644833">DE</a>, <a href="https://www.amazon.es/dp/1792644833">ES</a>, <a href="https://www.amazon.fr/dp/1792644833">FR</a>, <a href="https://www.amazon.it/dp/1792644833">IT</a>, <a href="https://www.amazon.co.jp/dp/1792644833">JP</a>), for the <a href="https://www.youtube.com/watch?v=ygE01sOhzz0">ludicrous</a> price of $27.50 (or the equivalent in pounds, euros, and yen).</p>
<p>I've also updated the freely available, full-color electronic version at <a href="http://algorithms.wtf/">http://algorithms.wtf</a>; this electronic version will remain free indefinitely.  The same site includes several additional lecture notes and other course materials.</p>
<p>Thanks to everyone who reported dozens of errors in the 0th and ½th edition on the <a href="https://github.com/jeffgerickson/algorithms">Github issue-tracker</a>.  Please keep the bug reports and feature requests coming!</p>
<p>Enjoy!</p></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html"><span class="datestr">at June 15, 2019 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/15/linkage.html">Linkage for the end of an academic year</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The Spring term just ended at UCI (we’re on a quarter system, so we run later into June and then start up again later in September than most other US universities). I haven’t yet turned in my grades, but I can already feel summer setting in.</p>

<ul>
  <li>
    <p><a href="https://hackaday.com/2019/06/01/paper-strandbeest-is-strong-enough-to-walk/">Remote-controlled papercraft steerable Strandbeest</a> (<a href="https://mathstodon.xyz/@11011110/102197633382208469"></a>, <a href="https://news.ycombinator.com/item?id=20068166">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.instagram.com/p/BaSVpX_nMpU/">Voronoi origami</a> (<a href="https://mathstodon.xyz/@11011110/102206289543572358"></a>, <a href="http://orderinspace.blogspot.com/2015/07/voronoi.html">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://archive.org/details/@mirtitles">Mir Books</a> (<a href="https://mathstodon.xyz/@11011110/102212071495683563"></a>, <a href="https://mathstodon.xyz/@jarban/102209797748141088">via</a>). A big collection of interesting-looking Soviet-era mathematics and science books and booklets, translated into English and free to read.</p>
  </li>
  <li>
    <p><a href="https://mathenchant.wordpress.com/2017/09/17/how-do-you-write-one-hundred-in-base-32/">Chip-firing games and sesquinary notation</a> (<a href="https://mathstodon.xyz/@11011110/102217258990536291"></a>). Jim Propp writes a monthly long-form math blog and somehow I hadn’t encountered it before; this is one of its many interesting entries. One of the oddities about base  is that you calculate it bottom-up (by starting from a ones digit that’s too big and then carrying things higher) instead of top-down (by greedily subtracting powers).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@JordiGH/102226037720885699">A discussion on whether proofs in Wikipedia articles need references</a>, and what those references are for.</p>
  </li>
  <li>
    <p><a href="https://theinnerframe.wordpress.com/2018/07/16/death-of-proof-the-pleasures-of-failure-i/">The tale of Horgan’s surface</a> (<a href="https://mathstodon.xyz/@11011110/102228263549241096"></a>, <a href="https://mathenchant.wordpress.com/2019/06/06/carnival-of-mathematics-170/">via</a>, <a href="http://www.indiana.edu/~minimal/essays/horgan/index.html">see also</a>, <a href="https://www.scottaaronson.com/blog/?p=4133">see also</a>), a nonexistent minimal surface whose existence was incorrectly predicted by numerical experiments, named sarcastically after a journalist who incautiously suggested that proof was a dead concept.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.02231">Compact packings of the plane with three sizes of discs</a> (<a href="https://mathstodon.xyz/@11011110/102240130646118434"></a>), Thomas Fernique, Amir Hashemi, and Olga Sizova. Here, “compact packing” means interior-disjoint disks forming only 3-sided gaps. The circle packing theorem constructs these for any finite maximal planar graph, with little control over disk size. Instead this paper seeks packings of the whole plane by infinitely many disks, with few sizes. 9 pairs of sizes and 164 triples work.</p>
  </li>
  <li>
    <p>Luca Trevisan posts a series of tutorials on online convex optimization, where you want to approximately minimize a sequence of convex functions before discovering what the functions are (parts <a href="https://lucatrevisan.wordpress.com/2019/04/17/online-optimization-for-complexity-theorists/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/"></a>; <a href="https://mathstodon.xyz/@11011110/102244702633232612"></a>). It’s a hot topic in TCS with connections to regularity lemmas, fast SDP approximation, and spectral sparsifiers.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Cross_sea">Squared patterns of ocean waves</a> (<a href="https://mathstodon.xyz/@11011110/102251731269228854"></a>), and wave patterns in social media: search for “cross sea” and note its appearance on gizmodo in 2014, amusingplanet in 2015, azula in 2017, providr in 2018, sciencealert in 2019…all repeating the same somewhat garbled explanation of mathematical wave models and danger to shipping.</p>
  </li>
  <li>
    <p><a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">The SIGACT Committee for the Advancement of Theoretical Computer Science is planning a Wikipedia edit-a-thon, in Phoenix on June 24 as part of STOC</a> (<a href="https://mathstodon.xyz/@11011110/102262460312659446"></a>). You can help, and you don’t even have to brave the desert heat to do so! There’s <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">a shared spreadsheet</a> where CATCS is crowdsourcing TCS topics on Wikipedia that need help. Add your favorite missing algorithm, theorem, complexity class, etc, and it’s likely it’ll get some attention.</p>
  </li>
  <li>
    <p>While I’m publicizing activities associated with STOC and FCRC next week in Phoenix, here’s another: <a href="https://sigact.org/tcswomen/tcs-women-2019/">the TCS Women Spotlight Workshop</a> (<a href="https://mathstodon.xyz/@11011110/102266092255364896"></a>). It features an inspirational talk from Ronitt Rubinfeld (in my experience a great speaker), four “rising star” talks by Naama Ben-David, Debarati Das, Andrea Lincoln, and Oxana Poburinnaya, a panel/lunch for women at STOC, and a poster session of recent theoretical computer science research by women.</p>
  </li>
  <li>
    <p>Two colleagues from my department, Alex Nicolau and Alex Veidenbaum, are participating in <a href="http://artdaily.com/news/114313/University-of-California--Irvine-computer-scientists-breathe-life-into-Venice-Biennale-installations">a Venice Biennale project</a> (<a href="https://mathstodon.xyz/@11011110/102272037129033816"></a>) in which viewers converse with computerized simulations of poet <a href="https://en.wikipedia.org/wiki/Paul_Celan">Paul Celan</a> and politician <a href="https://en.wikipedia.org/wiki/Nicolae_Ceau%C8%99escu">Nicolae Ceaușescu</a>.
The Alexes usually work on the more technical side of CS (parallelizing compilers, computer architecture, and embedded systems) so it’s interesting to me to see this softer direction from them.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-019-01796-1"><em>Nature</em> on how word processors and text editors have been shifting from roll-your-own equation editing to LaTeX</a> (<a href="https://mathstodon.xyz/@11011110/102277832022967684"></a>, <a href="https://news.ycombinator.com/item?id=20191348">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/15/linkage.html"><span class="datestr">at June 15, 2019 05:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/">3rd Symposium on Simplicity in Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 6-7, 2020 Salt Lake City, UT https://www.siam.org/Conferences/CM/Conference/sosa20 Submission deadline: August 9, 2019 Symposium on Simplicity in Algorithms is a new conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding … <a href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">3rd Symposium on Simplicity in Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/"><span class="datestr">at June 14, 2019 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/">5th Algorithmic and Enumerative Combinatorics Summer School 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 29 – August 2, 2019 Linz, Austria https://www3.risc.jku.at/conferences/aec2019/local.html Registration deadline: June 16, 2019 The goal of this summer school is to put forward the interplay between the fields of Enumerative Combinatorics, Analytic Combinatorics, and Algorithmics. This is a very active research area, which, aside from the three fields fueling each other mutually, receives as … <a href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/" class="more-link">Continue reading <span class="screen-reader-text">5th Algorithmic and Enumerative Combinatorics Summer School 2019</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/"><span class="datestr">at June 13, 2019 10:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/">ADFOCS 2019 – Games, Brains, and Distributed Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 19-23, 2019 Saarbrücken, Germany http://resources.mpi-inf.mpg.de/conferences/adfocs/ Registration deadline: July 19, 2019 ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). It is organized as part of the activities of the MPII, in particular the International Max-Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school … <a href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/" class="more-link">Continue reading <span class="screen-reader-text">ADFOCS 2019 – Games, Brains, and Distributed Computing</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/"><span class="datestr">at June 13, 2019 10:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentdescent.org/smoothadv">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dd.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentdescent.org/smoothadv.html">Provably Robust Deep Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<blockquote>
  <p>Recently, several works proposed the convolution of a neural network with a Gaussian as a smoothed classifier for provably robust classification. We show that adversarially training this <strong>smoothed</strong> classifier significantly increases its provable robustness through extensive experiments, achieving state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> <strong>provable robustness</strong> on CIFAR10 and Imagenet, as shown in the tables below.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="introduction">Introduction</h1>

<p>It is now well-known that deep neural networks suffer from the brittleness problem: A small change in an input image imperceptible to humans can cause dramatic change in a neural network’s classification of the image. Such a perturbed input is known as an <em>adversarial example</em> and is by now immortalized in the famous picture below from <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al.</a></p>

<p><img src="https://decentdescent.org/assets/smoothadv/panda_gibbon_adv.png" alt="A small carefully crafted noise can change a panda to a gibbon --- at least to a neural network!" /></p>

<p>As deep neural networks enter consumer and enterprise products of various forms, this brittleness can possibly have devastating consequences (<a href="https://arxiv.org/abs/1712.09665">Brown et al. 2018</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1707.089450">Evtimov &amp; Eykholt et al. 2018</a>,
<a href="https://arxiv.org/abs/1904.00759">Li et al. 2019</a>).
Most strikingly, Tencent Keen Security Lab recently <a href="https://keenlab.tencent.com/en/2019/03/29/Tencent-Keen-Security-Lab-Experimental-Security-Research-of-Tesla-Autopilot/">demonstrated</a> that the neural network underlying Tesla Autopilot can be fooled by an adversarially crafted marker on the ground into swerving into the opposite lane.</p>

<h2 id="adversarial-attack-and-defense">Adversarial Attack and Defense</h2>

<p>Given the importance of the problem, many researchers have formulated security models of adversarial attacks, along with ways to defend against adversaries in such models. In the most popular security model in the academic circle today, the adversary is allowed to perturb an input by a small noise bounded in <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-norm, in order to cause the network to misclassify it. Thus, given a loss function <span class="katex"><span class="katex-mathml">LL</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">L</span></span></span></span>, a norm bound <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, an input <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, its label <span class="katex"><span class="katex-mathml">yy</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">y</span></span></span></span>, and a neural network <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span>, the adversary tries to find an input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span>, within <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-distance <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span> of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, that maximizes the loss <span class="katex"><span class="katex-mathml">L(F(x),y)L(F(x), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, i.e. it solves the following optimization problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥p≤ϵL(F(x′),y).\hat x = \argmax_{\|x' - x\|_p \le \epsilon} L(F(x'), y).</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43055999999999994em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.16454285714285716em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.2818857142857143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.18276em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>If <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> has trainable parameters <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">θ</span></span></span></span>, then the defense needs to find the parameters that minimizes <span class="katex"><span class="katex-mathml">L(F(x^),y)L(F(\hat x), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, for <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> sampled from the data distribution <span class="katex"><span class="katex-mathml">DD</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">D</span></span></span></span>, i.e. it solves the following minimax problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">min⁡θE(x,y)∼DL(F(x^),y).\min_{\theta} \underset{(x, y) \sim D}{\mathbb{E}} L(F(\hat x), y).</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.66786em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span><span style="height: 2.7em;" class="pstrut"></span><span><span class="mop">min</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.7521079999999999em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathdefault mtight">D</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Empirically, during an attack, the adversarial input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be obtained approximately by solving the max problem using gradient descent, making sure to project back to the <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>-ball after each step.
This is known as the PGD attack (<a href="https://arxiv.org/abs/1607.02533">Kurakin et al.</a>, <a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), short for “project gradient descent.”
During training by the defense, for every sample <span class="katex"><span class="katex-mathml">(x,y)∼D(x, y) \sim D</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">D</span></span></span></span>, this estimate of <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be plugged into the min problem for gradient descent of <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">θ</span></span></span></span>. 
This is known as <strong>Adversarial Training</strong>, or <strong>PGD training</strong> specifically when PGD is used for finding <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span>.</p>

<h2 id="empirical-robust-accuracy">Empirical Robust Accuracy</h2>

<p>Currently, the standard benchmark for measuring the strength of a model’s adversarial defense is the model’s <em>(empirical) robust accuracy</em> on various standard datasets like CIFAR-10 and Imagenet.
This accuracy is calculated by attacking the model with a strong empirical attack (like PGD) for every sample of the test set.
The percentage of the test set that the model is still able to correctly classify is the empirical robust accuracy.</p>

<p>For example, consider an adversary allowed to perturb an input by <span class="katex"><span class="katex-mathml">ϵ=8255\epsilon = \frac{8}{255}</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.845108em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">5</span><span class="mord mtight">5</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.345em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> in <span class="katex"><span class="katex-mathml">ℓ∞\ell_\infty</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.151392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> norm.
On an image, this means that the adversary can change the color of each pixel by at most 8 units (out of 255 total) in each color channel — a rather imperceptible perturbation.
Currently, the state-of-the-art empirical robust accuracy against such an adversary on CIFAR-10 hovers around 55% (<a href="https://arxiv.org/abs/1901.08573">Zhang et al. 2019</a>, <a href="https://arxiv.org/abs/1901.09960">Hendrycks et al. 2019</a>), meaning that the best classifier can only withstand a strong attack on about 55% of the samples in CIFAR-10.
Contrast this with the <a href="https://paperswithcode.com/sota/image-classification-on-cifar-10">state-of-the-art nonrobust accuracy on CIFAR-10 of &gt;95%</a>.
Thus it’s clear that adversarial robustness research still has a long way to go.</p>

<h1 id="provable-robustness-via-randomized-smoothing">Provable Robustness via Randomized Smoothing</h1>

<p>Note that the empirical robust accuracy is only an upper bound on the <strong>true robust accuracy</strong>.
This is defined by hypothetically replacing the <em>strong empirical attack</em> used in empirical robust accuracy with the <em>ideal attack</em> able to find <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> exactly for every <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>.
Thus, nothing in principle prevents a stronger empirical attack from further lowering the empirical robust accuracy of a model.
Indeed, except a few notable cases like PGD (<a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), we have seen most claims of adversarial robustness broken down by systematic and thorough attacks (as examples, see <a href="https://arxiv.org/abs/1607.04311">Carlini &amp; Wagner 2016</a>,
<a href="https://arxiv.org/pdf/1705.07263">Carlini &amp; Wagner 2017</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1802.05666">Uesato et al. 2018</a>,
<a href="https://arxiv.org/abs/1802.00420">Athalye et al. 2018</a>,
<a href="https://arxiv.org/abs/1807.10272">Engstrom et al. 2018</a>,
<a href="https://arxiv.org/abs/1902.02322">Carlini 2019</a>).</p>

<p>This has motivated researchers into developing defenses that can <em>certify the absence of adversarial examples</em> (as prominent examples, see
<a href="https://arxiv.org/abs/1711.00851">Wong &amp; Kolter 2018</a>,
<a href="https://arxiv.org/abs/1702.01135">Katz et al. 2017</a>,
and see <a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a> for a thorough overview of these techniques). 
Such a defense is afforded a <strong>provable (or certified) robust accuracy</strong> on each dataset, defined as the percentage of the test set that can be proved to have no adversarial examples in its neighborhood.
In contrast with empirical robust accuracy, provable robust accuracy is a <em>lower bound</em> on the true robust accuracy, and therefore cannot be lowered further by more clever attacks.
The tables in the beginning of our blog post, for example, display provable robust accuracies on CIFAR-10 and Imagenet.</p>

<p>Until recently, most such certifiable defenses have not been able to scale to large networks and datasets (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>), but a new technique called <em>randomized smoothing</em> (<a href="https://arxiv.org/abs/1802.03471">Lecuyer et al.</a>, <a href="https://arxiv.org/abs/1809.03113">Li et al.</a>, <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>) was shown to bypass this limitation, obtaining highly-nontrival <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> certified robust accuracy on Imagenet (<a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>). We now briefly review randomized smoothing.</p>

<h2 id="definition">Definition</h2>

<p>Consider a classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> from <span class="katex"><span class="katex-mathml">Rd\mathbb{R}^d</span><span class="katex-html"><span class="base"><span style="height: 0.849108em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.849108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span> to classes <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>. Randomized smoothing is a method that constructs a new, <em>smoothed</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> from the <em>base</em> classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>. The smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> assigns to a query point <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> the class which is most likely to be returned by the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> under isotropic Gaussian noise perturbation of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, i.e.,</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">g(x)=arg max⁡c∈Y  P(f(x+δ)=c)g(x) = \argmax_{c \in \mathcal{Y}} \; \mathbb{P}(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43055999999999983em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight">Y</span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.006825em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord"><span class="mord mathbb">P</span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">δ</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and the variance <span class="katex"><span class="katex-mathml">σ2\sigma^2</span><span class="katex-html"><span class="base"><span style="height: 0.8141079999999999em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> is a hyperparameter of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> (it can be thought to control a robustness/accuracy tradeoff).
In <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>, <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> is a neural network.</p>

<h2 id="prediction">Prediction</h2>

<p>To estimate <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>, one simply has to</p>

<ol>
  <li>Sample a collection of Gausian samples <span class="katex"><span class="katex-mathml">δi\delta_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>.</li>
  <li>Predict the class <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> of each <span class="katex"><span class="katex-mathml">x+δix + \delta_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> using the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>.</li>
  <li>Take the majority vote of the <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>’s as the final prediction of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>.</li>
</ol>

<h2 id="certification">Certification</h2>

<p>The robustness guarantee presented by <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> is as follows: suppose that when the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> classifies  <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, the (most popular) class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is returned with probability  <span class="katex"><span class="katex-mathml">pA=Pδ(f(x+δ)=cA)p_A =  \mathbb{P}_\delta(f(x+\delta) = c_A)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, and the <em>runner-up</em> class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is returned with probability <span class="katex"><span class="katex-mathml">pB=max⁡c≠cAPδ(f(x+δ)=c)p_B = \max_{c \neq c_A} \mathbb{P}_\delta(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.3361079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="rlap mtight"><span class="strut"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.3448em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.14329285714285717em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span>. We estimate <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> using Monte Carlo sampling and confidence intervals<sup id="fnref:1"><a href="https://decentdescent.org/smoothadv.html#fn:1" class="footnote">1</a></sup>. Then the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is robust around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> within the radius</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">σ2(Φ−1(pA)−Φ−1(pB)),\frac{\sigma}{2} \left(\Phi^{-1}(p_A) - \Phi^{-1}(p_B)\right),</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.10756em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">2</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord mathdefault">σ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.864108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mbin">−</span><span class="mspace"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.864108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">Φ−1\Phi^{-1}</span><span class="katex-html"><span class="base"><span style="height: 0.8141079999999999em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> is the inverse of the standard Gaussian CDF. Thus, the bigger <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is and the smaller <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is, the more provably robust <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is.</p>

<h2 id="training">Training</h2>

<p><a href="https://arxiv.org/abs/1906.04584">Cohen et al.</a> simply trained the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> under Gaussian noise data augmentation with cross entropy loss, i.e. for each data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, sample <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">δ</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> and train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> on the example <span class="katex"><span class="katex-mathml">(x+δ,y)(x+\delta, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>.
With this simple training regime applied to a <a href="https://arxiv.org/abs/1512.03385">Resnet-110</a> base classifier, they were able to obtain significant certified robustness on CIFAR-10 and Imagenet, as shown in our tables.</p>

<h2 id="an-illustration">An Illustration</h2>

<p>The following figures modified from <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> illustrate randomized smoothing.
The base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> partitions the input space into different regions with different classifications, colored differently in the left figure.
The regions’ Gaussian measures (under the Gaussian <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> whose level curves are shown as dashed lines) are shown as a histogram on the right.
The class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> corresponding to the blue region is the output of the smoothed classifier <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>; the class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> corresponding to the cyan region is the runner-up class.
If <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is large enough and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is small enough, then we can prove that <span class="katex"><span class="katex-mathml">g(x′)=cAg(x') = c_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> for all <span class="katex"><span class="katex-mathml">∥x′−x∥2≤ϵ\|x' - x\|_2 \le \epsilon</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord">∥</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">−</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">x</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">≤</span><span class="mspace"></span></span><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, i.e. <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is robust at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>.</p>

<p><img src="https://decentdescent.org/assets/smoothadv/randomized_smoothing_simple_light.png" class="limit-height" /></p>

<h1 id="adversarially-training-the-smoothed-classifier">Adversarially Training the Smoothed Classifier</h1>

<p>Intuitively, adversarial training attempts to make a classifier locally flat around input sampled from a data distribution.
Thus it would seem that adversarial training should make it easier to <em>certify</em> the lack of adversarial examples, despite having no provable guarantees itself.
Yet historically, it has been difficult to execute this idea (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>, and folklore), with the closest being <a href="https://arxiv.org/abs/1809.03008">Xiao et al.</a></p>

<p>It is hence by no means a foregone conclusion that adversarial training should improve certified accuracy of randomized smoothing.
<em>A priori</em> there could also be many ways these two techniques can be combined, and it is not clear which one would work best:</p>

<ol>
  <li>Train the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> to be adversarially robust, simultaneous with the Gaussian data augmentation training prescribed in <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>.</li>
  <li>Find an adversarial example of the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>, then add Gaussian noise and train.</li>
  <li>Add Gaussian noise and find an adversarial example of <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> in the neighborhood of this Gaussian perturbation. Train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> on this adversarial example.</li>
  <li>Find an adversarial example of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span>, then train <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> on this example.</li>
</ol>

<p>It turns out that certified accuracies of these methods follow the order (1) &lt; (2) &lt; (3) &lt; (4), with (4) achieving the highest certified accuracies (see <a href="https://arxiv.org/abs/1906.04584">our paper</a>).
Indeed, in hindsight, if <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is the classifer doing the prediction, then we should be adversarially training <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span>, and not <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>.
In the rest of the blog post, we lay out the details of (4).</p>

<h2 id="randomized-smoothing-for--soft-classifiers">Randomized Smoothing for  <em>Soft</em> Classifiers</h2>
<p>Neural networks typically learn <em>soft</em> classifiers, namely, functions <span class="katex"><span class="katex-mathml">F:Rd→P(Y)F: \mathbb{R}^d \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span><span class="mspace"></span><span class="mrel">:</span><span class="mspace"></span></span><span class="base"><span style="height: 0.849108em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.849108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">→</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml">P(Y)P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is the set of probability distributions over <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>.
During prediction, the soft classifier is argmaxed to return the final hard classification.
We therefore consider a generalization of randomized smoothing to soft classifiers. Given a soft classifier <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span>, its associated <em>smoothed</em> soft classifier  <span class="katex"><span class="katex-mathml">G:Rn→P(Y)G: \mathbb{R}^n \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span><span class="mspace"></span><span class="mrel">:</span><span class="mspace"></span></span><span class="base"><span style="height: 0.68889em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.664392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">→</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is defined as</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">G(x)=Eδ∼N(0,σ2I)F(x+δ).G (x) = \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F(x + \delta).</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Let <span class="katex"><span class="katex-mathml">f(x)f(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml">F(x)F (x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> denote the hard and soft classifiers learned by the neural network, respectively, and let <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> and <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> denote the associated smoothed hard and smoothed soft classifiers. Directly finding adversarial examples for the smoothed <em>hard</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is a somewhat ill-behaved problem because of the argmax, so we instead propose to <em>find adversarial examples for the smoothed soft classifier</em> <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>. Empirically we found that doing so will also find good adversarial examples for the smoothed hard classifier.</p>

<h2 id="finding-adverarial-examples-for-smoothed-soft-classifier">Finding Adverarial Examples for Smoothed Soft Classifier</h2>
<p>Given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, we wish to find a point <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> which maximizes the loss of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> in an <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> ball around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> for some choice of loss function.
As is canonical in the literature, we focus on the cross entropy loss <span class="katex"><span class="katex-mathml">LCEL_{\mathrm{CE}}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>.
Thus, given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> our (ideal) adversarial perturbation is given by the formula:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥2≤ϵLCE(G(x′),y)=arg max⁡∥x′−x∥2≤ϵ(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y).\begin{aligned}
    \hat x &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} L_{\mathrm{CE} } (G (x'), y)\\ 
    &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon}  \left( - \log \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}}  F (x' + \delta)_y \right).
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.8554399999999998em;" class="vlist"><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.3554399999999998em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.8554399999999998em;" class="vlist"><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"></span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.3554399999999998em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>We will refer to the above as the <strong>SmoothAdv</strong> objective. The <em>SmoothAdv</em> objective is highly non-convex, so as is common in the literature, we will optimize it via projected gradient descent (PGD), and variants thereof. It is hard to find exact gradients for <em>SmoothAdv</em>, so in practice we must use some estimator based on random Gaussian samples.</p>

<h2 id="estimating-the-gradient-of-smoothadv">Estimating the Gradient of <em>SmoothAdv</em></h2>

<p>If we let <span class="katex"><span class="katex-mathml">J(x′)=LCE(G(x′),y)J(x') = L_{\mathrm{CE} } (G (x'), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> denote the <em>SmoothAdv</em> objective, then</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)=∇x′(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y)  .\nabla_{x'} J(x') = \nabla_{x'} \left( - \log \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F (x' + \delta)_y \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord">.</span></span></span></span></span>

<p>However, it is not clear how to evaluate the expectation inside the log exactly, as it takes the form of a complicated high dimensional integral.
Therefore, we will use Monte Carlo approximations.
We sample i.i.d. Gaussians <span class="katex"><span class="katex-mathml">δ1,…,δm∼N(0,σ2I)\delta_1, \ldots, \delta_m \sim \mathcal{N} (0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="minner">…</span><span class="mspace"></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.151392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and use the plug-in estimator for the expectation:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)≈∇x′(−log⁡(1m∑i=1mF(x′+δi)y))  .\nabla_{x'} J(x') \approx \nabla_{x'} \left( - \log \left( \frac{1}{m} \sum_{i = 1}^m  F (x' + \delta_i)_y \right) \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">≈</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.32144em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord mathdefault">m</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.6513970000000002em;" class="vlist"><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.277669em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord">.</span></span></span></span></span>

<p>It is not hard to see that if <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> is smooth, this estimator will converge to <span class="katex"><span class="katex-mathml">∇x′J(x′)\nabla_{x'} J(x')</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> as we take more samples.</p>

<h2 id="smoothadv-is-not-the-naive-objective"><em>SmoothAdv</em> is not the <em>Naive</em> Objective</h2>

<p>We note that <em>SmoothAdv</em> should not be confused with the similar-looking objective</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">=arg max⁡∥x′−x∥2≤ϵEδ∼N(0,σ2I)LCE(F(x′+δ),y)=arg max⁡∥x′−x∥2≤ϵ Eδ∼N(0,σ2I)[−log⁡F(x′+δ)y]  ,\begin{aligned}
&amp;\phantom{ {}={}} \argmax_{\|x' - x\|_2 \leq \epsilon}
    \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        L_{\mathrm{CE} } (F (x' + \delta), y) \\
&amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} 
    \ \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        \left[-\log  F(x' + \delta)_y\right] \; ,
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.55044em;" class="vlist"><span><span style="height: 2.84em;" class="pstrut"></span><span class="mord"></span></span><span><span style="height: 2.84em;" class="pstrut"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.05044em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.55044em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span style="color: transparent;" class="mord"></span><span class="mspace"></span><span style="color: transparent;" class="mrel">=</span><span class="mspace"></span><span style="color: transparent;" class="mord"></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mspace"> </span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter">[</span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter">]</span></span><span class="mspace"></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.05044em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>where the <span class="katex"><span class="katex-mathml">log⁡\log</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mop">lo<span>g</span></span></span></span></span> and <span class="katex"><span class="katex-mathml">E\mathbb{E}</span><span class="katex-html"><span class="base"><span style="height: 0.68889em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord mathbb">E</span></span></span></span></span> have been swapped compared to <em>SmoothAdv</em>, as suggested in section G.3 of <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a>.
This objective, which we shall call <strong>naive</strong>, is the one that corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> that is robust to Gaussian noise.
In contrast, <em>SmoothAdv</em> directly corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>.
From this point of view, <em>SmoothAdv</em> is the right optimization problem that should be used to find adversarial examples of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>. 
This distinction turns out to be crucial in practice: empirically,  <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a> found attacks based on the <em>naive</em> objective not to be effective.
In <a href="https://arxiv.org/abs/1906.04584">our paper</a>, we perform <em>SmoothAdv</em>-attack on <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>’s smoothed model and find, indeed, that it works better than the <em>Naive</em> objective, and it performs better with more Gaussian noise samples used to estimate its gradient.</p>

<h2 id="adversarially-training-smoothed-classifiers">Adversarially Training Smoothed Classifiers</h2>

<p>We now wish to use our new <em>SmoothAdv</em> attack to boost the adversarial robustness of smoothed classifiers.
As described in the beginning of this blog post, in (ordinary) adversarial training, given a current set of model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and a labeled data point <span class="katex"><span class="katex-mathml">(xt,yt)(x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, one finds an adversarial perturbation <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">xtx_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> for the current model, and then takes a gradient step for the model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>, evaluated at the point <span class="katex"><span class="katex-mathml">(x^t,yt)(\hat x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.
Intuitively, this encourages the network to learn to minimize the worst-case loss over a neighborhood around the input.</p>

<p>What is different in our proposed algorithm is that <em>we are finding the adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with respect to the smoothed classifier <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> using the <strong>SmoothAdv</strong> objective</em>, and <em>we are training <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> at this adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with respect to the <strong>SmoothAdv</strong> objective, estimated by the plug-in estimator.</em></p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">θt+1=θt+η∇θlog⁡(1m′∑i=1m′F(x^t+δi)y),\begin{aligned}
\theta_{t+1} &amp;= \theta_t + \eta \nabla_\theta \log\left(\frac{1}{m'} \sum_{i=1}^{m'} F(\hat x_t + \delta_i)_y\right),
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.2000199999999994em;" class="vlist"><span><span style="height: 4.05002em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.301108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.208331em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.70002em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.2000199999999994em;" class="vlist"><span><span style="height: 4.05002em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.05002em;" class="vlist"><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.55002em;" class="vlist"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.32144em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6778919999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.9294850000000003em;" class="vlist"><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8278285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.277669em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.05002em;" class="vlist"><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.55002em;" class="vlist"><span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.70002em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> are the parameters of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> at time <span class="katex"><span class="katex-mathml">tt</span><span class="katex-html"><span class="base"><span style="height: 0.61508em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">t</span></span></span></span>,  <span class="katex"><span class="katex-mathml">δi∼N(0,σ2I)\delta_i \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and <span class="katex"><span class="katex-mathml">η\eta</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">η</span></span></span></span> is a learning rate.</p>

<h2 id="results">Results</h2>

<p>Over the course of the blog post, we have introduced several hyperparameters, such as 1) <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, the radius of perturbation used for adversarial training, 2) <span class="katex"><span class="katex-mathml">mm</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">m</span></span></span></span>, the number of Gaussian noise samples, 3) <span class="katex"><span class="katex-mathml">σ\sigma</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">σ</span></span></span></span>, the standard deviation of the Gaussian noise.
We also did not mention other hyperparameters like <span class="katex"><span class="katex-mathml">TT</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">T</span></span></span></span>, the number of iterations used for PGD iterations, or the usage of DDN, an alternative attack to PGD that has been shown to be effective for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-perturbations (<a href="https://arxiv.org/abs/1811.09600">Rony et al.</a>).
In <a href="https://arxiv.org/abs/1906.04584">our paper</a> we do extensive analysis of the effects of these hyperparameters, to which we refer interested readers.</p>

<p>Taking the max over all such hyperparameter combinations for each <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> perturbation radius, we obtain the <em>upper envelopes</em> of the certified accuracies of our method vs the <em>upper envelopes</em> of <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> in the tables in the beginning of this post, which we also replicate here for convenience.</p>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we reviewed adversarial training and randomized smoothing, a recently proposed provable defense.
By adversarially training the smoothed classifier — and carefully getting all the details right — we obtained the state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> provable robustness on CIFAR-10 and Imagenet, demonstrating significant improvement over randomized smoothing alone.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>This blog post presented work done by Hadi Salman, Greg Yang, Jerry Li, Huan Zhang, Pengchuan Zhang, Ilya Razenshteyn, and Sebastien Bubeck.
We would like to thank Zico Kolter, Jeremy Cohen, Elan Rosenfeld, Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar, Jacob Steinhardt for comments and discussions during the making of this paper.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>We actually estimate a lower bound <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="underline-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.39443999999999996em;" class="vlist"><span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and an upper bound <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.63056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="overline-line"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with high probability, and substitute <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="underline-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.39443999999999996em;" class="vlist"><span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.63056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="overline-line"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span></span></span> for <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> everywhere. This is an overestimate, so our guarantee holds except for a small probability that the estimates are wrong. See <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> or <a href="https://arxiv.org/abs/1906.04584">our paper</a> for more details. <a href="https://decentdescent.org/smoothadv.html#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
<a href="https://decentdescent.org/smoothadv.html"><span class="datestr">at June 13, 2019 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15989">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/">How To Make A Polynomial Map Nicer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Stability theory and polynomials</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/essen/" rel="attachment wp-att-15990"><img src="https://rjlipton.files.wordpress.com/2019/06/essen.jpeg?w=600" alt="" class="alignright size-full wp-image-15990" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Essen ]</font></td>
</tr>
</tbody>
</table>
<p>
Arno van den Essen is the author of <b>the</b> book on the Jacobian Conjecture.</p>
<p>
Today I want to highlight one of the ideas he presents in his <a href="https://www.springer.com/gp/book/9783764363505">book</a>.<br />
<span id="more-15989"></span></p>
<p>
The theory is sometimes called stabilization methods. Or K-theory methods. It is often used in connection with the famous Jacobian conjecture (JC). I will not say any more about JC now—see <a href="https://rjlipton.wordpress.com/2014/01/29/progress-on-the-jacobian-conjecture/">this</a> for some comments we made a while ago. </p>
<p>
</p><p></p><h2> The Stability Philosophy </h2><p></p>
<p></p><p>
Essen states that the philosophy of stability theory is: </p>
<blockquote><p><b> </b> <em> <i>It is possible to change a map <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> and make it “nicer” provided we allow <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> to be increased</i>.<img src="https://s0.wp.com/latex.php?latex=%7B%5E%7B%5Cdagger%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{^{\dagger}}" class="latex" title="{^{\dagger}}" /> </em>
</p></blockquote>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%5Crule%7B0.4%5Ctextwidth%7D%7B0.4pt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rule{0.4\textwidth}{0.4pt}" class="latex" title="\rule{0.4\textwidth}{0.4pt}" /></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cdagger%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\dagger)}" class="latex" title="{(\dagger)}" /> Not a direct quote.</p>
<p>
That is provided we can change <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D+%3A+%5Cmathbb%7BK%7D%5E%7BN%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7BN%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} " class="latex" title="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> is larger than <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. The whole method is based on a simple observation. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> and define <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> by 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%2Cu_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29+%3D+%28F%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%29%2C+u_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). " class="latex" title="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). " /></p>
<p>Then <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is injective if and only if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is injective. This is trivial—really trivial. From trivial observations sometimes important methods are created. </p>
<p>
I will now explain how why this is useful by presenting an example. </p>
<p>
</p><p></p><h2> The Method: By Example </h2><p></p>
<p></p><p>
Suppose that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%3A+%5Cmathbb%7BK%7D%5E%7B2%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} " class="latex" title="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} " /></p>
<p>is a polynomial mapping where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%2Cy%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x,y) = (f(x,y),g(x,y)). " class="latex" title="\displaystyle  F(x,y) = (f(x,y),g(x,y)). " /></p>
<p>We wish to show that we can replace <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> by another polynomial map <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> that has degree at most <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. Moreover, the new polynomial map is injective if and only if the original polynomial map is injective. The method can be used to preserve other properties of the polynomial mapping, but being injective is a important example. </p>
<p>
There seems to be no way to lower the degree of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> without destroying its structure. But if we use the stability philosophy and allow extra dimensions we can succeed. That is we replace <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x,y)}" class="latex" title="{F(x,y)}" /> by the function 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D%28x%2Cy%2Cu%2Cv%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). " class="latex" title="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). " /></p>
<p>Why does this work? The idea is that the extra two dimensions can be used as extra <i>registers</i>. These registers can be used to simplify the computation, and reduce the degree of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. </p>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x,y)}" class="latex" title="{f(x,y)}" /> have one term that we wish to remove. To be concrete, let’s assume the term is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B3%7Dy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x^{3}y. " class="latex" title="\displaystyle  x^{3}y. " /></p>
<p>Start with the input 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x,y,u,v). " class="latex" title="\displaystyle  (x,y,u,v). " /></p>
<p>Now change this to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2BP%2Cv%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x,y,u+P,v+Q), " class="latex" title="\displaystyle  (x,y,u+P,v+Q), " /></p>
<p>where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%3Dx%5E%7B2%7D+%5Ctext%7B+and+%7D+Q%3Dxy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  P=x^{2} \text{ and } Q=xy. " class="latex" title="\displaystyle  P=x^{2} \text{ and } Q=xy. " /></p>
<p>This is an invertible transformation. Note this is only possible because we have two extra dimensions or registers. Otherwise, we could not compute <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> without messing up the rest of the computation. Now map this to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2BP%2Cv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). " class="latex" title="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). " /></p>
<p>This is nothing more than computing the original function and ignoring the new registers. </p>
<p>
The next step is to go to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f-%28u%2BP%29%28v%2BQ%29%2Cg%2Cu%2BPv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). " class="latex" title="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). " /></p>
<p>The last point is that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- (u+P)(v+Q), " class="latex" title="\displaystyle  f- (u+P)(v+Q), " /></p>
<p>cancels the term <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B3%7Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{3}y}" class="latex" title="{x^{3}y}" /> we wished to remove. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP-uv+%3D+f-x%5E%7B2%7Dxy+-uxy+-+vx%5E%7B2%7D-uv.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. " class="latex" title="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. " /></p>
<p>The price we pay is that new terms have been added, but they have at most degree <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. </p>
<p>
</p><p></p><h2> The Method: General Case </h2><p></p>
<p></p><p>
We can prove by induction the following general theorem: </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> is polynomial map where <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is a field. Then we can construct a polynomial map of degree at most <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> denoted by <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> so that it is injective precisely when <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is injective. </em>
</p></blockquote>
<p></p><p>
Even stronger theorems are possible. For example, the polynomial map <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> can be required to be cubic linear: </p>
<blockquote><p><b>Definition 2</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrix over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BK%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{K}}" class="latex" title="{\mathbb{K}}" />. The <img src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%28X%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F_{A}(X)}" class="latex" title="{F_{A}(X)}" /> is the <b>cubic linear</b> map for the matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is defined to be the map <img src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%5Crightarrow+X+%2B+%28AX%29%5E%7B%2A3%7D%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  X \rightarrow X + (AX)^{*3}, " class="latex" title="\displaystyle  X \rightarrow X + (AX)^{*3}, " /></p>
</em><p><em>where <img src="https://s0.wp.com/latex.php?latex=%7BY%5E%7B%2A3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Y^{*3}}" class="latex" title="{Y^{*3}}" /> is defined to be the vector <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BZ_%7Bk%7D+%3D+Y_%7Bk%7D%5E%7B3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Z_{k} = Y_{k}^{3}}" class="latex" title="{Z_{k} = Y_{k}^{3}}" /> for all coordinates <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />. </em>
</p></blockquote>
<p></p><p>
See Essen’s book for more details. Note a cubic linear map when <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> is of the form: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28+x+%2B+%28+ax+%2B+by%29%5E%7B3%7D%2C+y+%2B+%28cx+%2B+dy%29%5E%7B3%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) " class="latex" title="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2Cd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b,c,d}" class="latex" title="{a,b,c,d}" /> are constants. This reduction to cubic linear maps is quite pretty, and requires a clever application of the stabilization method.</p>
<p>
</p><p></p><h2> A Limit of The Method </h2><p></p>
<p></p><p>
The reduction in degree is possible only to degree <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. It cannot be reduced to degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> in general. Let’s look at the intuition why this is true. The last step is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- (u+P)(v+Q) " class="latex" title="\displaystyle  f- (u+P)(v+Q) " /></p>
<p>which is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- PQ -uQ -vP. " class="latex" title="\displaystyle  f- PQ -uQ -vP. " /></p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> has a leading term of degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />. Also suppose that <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> has degree <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}}" class="latex" title="{d_{P}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> has degree <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}}" class="latex" title="{d_{Q}}" />. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d+%3D+d_%7BP%7D+%2B+d_%7BQ%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  d = d_{P} + d_{Q} " class="latex" title="\displaystyle  d = d_{P} + d_{Q} " /></p>
<p>since the leading term goes away. But <img src="https://s0.wp.com/latex.php?latex=%7BuQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{uQ}" class="latex" title="{uQ}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BvP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{vP}" class="latex" title="{vP}" /> have degrees <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}+1}" class="latex" title="{d_{P}+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}+1}" class="latex" title="{d_{Q}+1}" /> respectively. So to keep <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}+1}" class="latex" title="{d_{P}+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}+1}" class="latex" title="{d_{Q}+1}" /> both <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> or less, it follows that <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> can be at most <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. However, in this case a term of degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> is removed and other terms of degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> are added. This is not a formal proof that the method cannot reduce the degree to <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. I do believe that formalized properly it is a theorem that reduction to degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> is in general impossible. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I like this technology. I wonder if it might be possible to use it on some of our favorite problems. I do like that it conserves invertibility. This seems like it could be related to quantum computing, because of the reversible nature of quantum computing. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/"><span class="datestr">at June 12, 2019 08:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8019500166163846173">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html">Compressing in Moscow</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator">
</div>
<div style="clear: both; text-align: center;" class="separator">
<a style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;" href="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg"><img src="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg" border="0" /></a><a style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg"><img src="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg" border="0" /></a></div>
<br />
This week finds me in Moscow for a pair of workshops, the <a href="https://mipt.ru/education/chairs/dm/conferences/workshop-june-9-11-moscow-2019.php">Russian Workshop on Complexity and Model Theory</a> and a workshop on <a href="https://www.poncelet.ru/conference/ric">Randomness, Information and Complexity</a>. The latter celebrates the lives of Alexander Shen and Nikolay Vereshchagin on their 60th birthdays.<br />
<br />
Alexander Shen might be best known in computational complexity for his <a href="https://doi.org/10.1145/146585.146613">alternate proof</a> of IP = PSPACE. In 1989, Lund, Fortnow, Karloff and Nisan gave an interactive proof for the permanent, which got the entire polynomial-time hierarchy by Toda's theorem. But we didn't know how to push the protocol to PSPACE, we had a problem keeping degrees of polynomials low. Shamir had the first proof by looking at a specific protocol for PSPACE. Shen had the brilliant but simple idea to use a degree reducing operator, taking the polynomial modulo x<sup>2</sup>-x. The three papers appeared <a href="https://dl.acm.org/citation.cfm?id=146585#prox">back-to-back-to-back</a> in JACM.<br />
<br />
Shen and Vereshchagin though made their careers with their extensive work on Kolmogorov complexity and entropy, often together. Vereshchagin and I have co-authored some papers together during our mutual trips to Amsterdam, including on <a href="http://doi.org/10.1007/11672142_10">Kolmogorov Complexity with Errors</a> and how to <a href="http://doi.org/10.1007/b106485">increase Kolmogorov Complexity</a>. My <a href="https://doi.org/10.1006/jcss.1999.1677">favorite work</a> of Shen and Vereshchagin, which they did with Daniel Hammer and Andrei Romashchenko showed that every linear inequality that holds for entropy also holds for Kolmogorov complexity and vice-versa, the best argument that the two notions of information, one based on distributions, the other based on strings, share strong connections.<br />
<br />
Today is <a href="https://en.wikipedia.org/wiki/Russia_Day">Russia Day</a> that celebrates the reestablishment of Russia out of the Soviet Union in 1990. Just like how the British celebrate their succession from the US in 1776 I guess. But I'm celebrating Russia day by honoring these two great Russians. Congrats Sasha and Kolya!</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html"><span class="datestr">at June 12, 2019 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals.html">Dancing arc-quadrilaterals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Several of my past papers concern Lombardi drawing, which I and my coauthors named after conspiracy-theory artist <a href="https://en.wikipedia.org/wiki/Mark_Lombardi">Mark Lombardi</a>. In this style of drawing, edges are drawn as circular arcs, and must meet at equal angles around every vertex. Not every graph has such a drawing, but many symmetrical graphs do (example below: the smallest <a href="https://en.wikipedia.org/wiki/Zero-symmetric_graph">zero-symmetric graph</a> with only two edge orbits).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Two-edge-orbit_GRR.svg" alt="The smallest zero-symmetric graph with only two edge orbits" /></p>

<p>All 2-degenerate graphs do as well; these are the graphs that can be reduced to nothing by removing vertices of degree at most two. And all 3-regular planar graphs have planar drawings in this style; I drew the one below to illustrate <a href="https://en.wikipedia.org/wiki/Grinberg%27s_theorem">Grinberg’s theorem</a>, a necessary condition for Hamiltonicity of planar graphs.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Grinberg_5CEC_Nonhamiltonian_graph.svg" alt="Grinberg's non-Hamiltonian planar cubic graph with cyclic edge-connectivity five" /></p>

<p>So anyway, my newest arXiv preprint is “Bipartite and series-parallel graphs without planar Lombardi drawings” (<a href="https://arxiv.org/abs/1906.04401">arXiv:1906.04401</a>, to appear at <a href="https://sites.ualberta.ca/~cccg2019/">CCCG</a>). It is about some families of planar graphs that have Lombardi drawings (because they are 2-degenerate) but do not have planar Lombardi drawings. They include planar bipartite graphs like the one below (but with more edges and vertices):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nested-K2n.svg" alt="Construction for a family of planar bipartite graphs with no planar Lombardi drawing " /></p>

<p>and embedded series-parallel graphs like the one below (again, with more edges and vertices):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonlom-serpar.svg" alt="Construction for a family of embedded series-parallel graphs with no planar Lombardi drawing" /></p>

<p>The key structures that makes both of these graphs hard to draw in Lombardi style are their yellow-blue quadrilateral faces.
The red parts of the graph are just filler to make these faces have the right angles. The yellow-blue quadrilaterals all share the same two yellow vertices,
and I like to think of them as forming a ring dancing hand-to-hand around these two yellow vertices like <a href="https://en.wikipedia.org/wiki/Dance_(Matisse)">Matisse’s dancers</a>.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Matisse-Dance.jpg" alt="_La Danse_, Henri Matisse, 1910, from https://en.wikipedia.org/wiki/File:Matissedance.jpg" /></p>

<p>When I wrote about <a href="https://11011110.github.io/blog/2018/12/22/circles-crossing-equal.html">quadrilaterals with circular-arc edges meeting at equal angles at each vertex</a> last December, it was these rings of quadrilaterals I was thinking about. As I wrote in my previous post, each of these quadrilaterals has a circumscribing circle. It’s not hard to make one arc polygon with four equal angles, as sharp as you like. But when the angles get sharp and two of these polygons share two opposite vertices and are packed at a tight angle next to each other (as all the yellow-blue quadrilateral faces in these graphs do) each of the two adjacent quadrilaterals must have a deep pocket into which its neighbor reaches to touch its circumcircle, and this forces them to become quite distorted looking.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/reacharound.svg" alt="Two sharp circular-arc quadrilaterals reach into each others' pockets to touch their circumscribing circles" /></p>

<p>If we think of these two quadrilaterals as dancers, with their heads and toes at the shared points and with their hands free, then both of them have their right hands (from the perspective of the viewer) held high and their left hands held low. Extending the same picture to a complete ring of dancers, each dancer in the ring must be holding their right hand higher than their neighbor to the left. But this obviously can’t extend all the way around the ring, because when you came back to the dancer you started with their hand should be the same height as it was when you started.</p>

<p>This is all very metaphorical but fortunately this intuition can be turned into a mathematical proof that the drawing doesn’t exist. The correct tools for measuring the heights of the quadrilateral vertices turn out to be <a href="https://en.wikipedia.org/wiki/M%C3%B6bius_transformation">Möbius transformations</a> and <a href="https://en.wikipedia.org/wiki/Bipolar_coordinates">bipolar coordinates</a>, a system for assigning coordinates to points in the plane by the angle they make with two fixed points (the two yellow shared vertices of all the quadrilateral faces of our graph) and the ratio of their distances to the fixed points.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/apollo.svg" alt="Level sets for bipolar coordinates" /></p>

<p>Möbius transformations preserve the circular-arc shape of our quadrilateral sides, and the angles at which they meet. When we restrict them to the transformations that leave the two poles of the bipolar coordinate system fixed, they act very nicely on the two coordinates, essentially adding fixed amounts to each coordinate. We can use them to transform our quadrilaterals into a more canonical shape and prove that the radius-ratio coordinate increases from one quadrilateral to the next around our ring of quadrilaterals, getting the same contradiction described above and proving that no drawing exists.</p>

<p>The most obvious questions in the same direction that this paper leaves unsolved are: what about series-parallel graphs where you do not have a fixed planar embedding for the graph? And what about outerplanar graphs (either with the outerplanar embedding or without fixing an embedding)? We neither have a method for finding planar Lombardi drawings of all graphs of these types, nor a proof that these drawings do not exist.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102256946045372078">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals.html"><span class="datestr">at June 11, 2019 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamdsmith.wordpress.com/?p=602">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/smith.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamdsmith.wordpress.com/2019/06/11/tpdp-2019/">TPDP 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The call for submissions for the latest edition of TPDP (Theory and Practice of Differential Privacy) is out! https://tpdp.cse.buffalo.edu/2019/  The workshop covers work on differential privacy (of course), and more generally on rigorous modeling of, and attacks on, statistical data privacy. The intention is to be inclusive. Submissions are due June 21, 2019. Advertisements</div>







<p class="date">
by adamdsmith <a href="https://adamdsmith.wordpress.com/2019/06/11/tpdp-2019/"><span class="datestr">at June 11, 2019 08:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/087">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/087">TR19-087 |  Coin Theorems and the Fourier Expansion | 

	Rohit Agrawal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this note we compare two measures of the complexity of a class $\mathcal F$ of Boolean functions studied in (unconditional) pseudorandomness: $\mathcal F$'s ability to distinguish between biased and uniform coins (the coin problem), and the norms of the different levels of the Fourier expansion of functions in $\mathcal F$ (the Fourier growth). We show that for coins with low bias $\varepsilon = o(1/n)$, a function's distinguishing advantage in the coin problem is essentially equivalent to $\varepsilon$ times the sum of its level $1$ Fourier coefficients, which in particular shows that known level $1$ and total influence bounds for some classes of interest (such as constant-width read-once branching programs) in fact follow as a black-box from the corresponding coin theorems, thereby simplifying the proofs of some known results in the literature. For higher levels, it is well-known that Fourier growth bounds on all levels of the Fourier spectrum imply coin theorems, even for large $\varepsilon$, and we discuss here the possibility of a converse.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/087"><span class="datestr">at June 11, 2019 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1267">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">Wikipedia edit-a-thon at STOC’19</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>CATCS is organizing a Wikipedia edit-a-thon at <a href="http://acm-stoc.org/stoc2019/">STOC</a> in Phoenix this year. The goal is to create/edit Wikipedia articles on TCS topics that need improvement. (A crowdsourced list of such topics is maintained <a href="https://docs.google.com/spreadsheets/d/1zVUdxKk9nqR5Itwc37v26aRHMqgV9qI8XexssoFq_CE/edit">here</a>.) The event will be held on June 24th, 2019 in West 104A, starting right after the STOC business meeting around 9-9:30 pm.</p>
<p>We invite members of the community to participate. Prior experience with Wikipedia is a plus, but is not necessary. If you are interested in participating, please fill out <a href="https://forms.gle/ZnPkHN1Wc2edAWhB6">this form</a>. Participants are asked to bring their own laptop or other device. Power outlets will be available. Light refreshments will be provided.</p>
<p>If you are interested in helping improve TCS coverage on Wikipedia but are unable to attend this event, please see <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">this post</a> for how you can help.</p>
<p> </p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/"><span class="datestr">at June 11, 2019 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
