<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 25, 2021 11:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8402768071991338392">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html">Squaring the circle is mentioned in a Gilbert and Sullivan comic Opera.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The problem of <i>squaring the circle</i>: Given a circle, construct (with ruler and compass) a square with the same area. While browsing the web for more information on this problem (for the blog entry on problems that might be similar to P vs NP: <a href="https://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html">here</a>)  I came across the following:</p><p>In the Gilbert and Sullivan comic opera <i>Princess Ida</i>, in the song <i>Gently, Gently</i>  is the line:</p><p>                                <i>    ... and the circle they will square it one fine day.</i></p><p>(To hear the song see <a href="https://www.youtube.com/watch?v=1r2hEcDmeIY">here</a>. The line is towards the end.) </p><p>They lyrics are <a href="https://www.gsarchive.net/princess_ida/webop/pi_12.html">here</a>. That website begins  <i>gsarchive.ne</i>t which made me wonder<i> Did I at one time set</i> u<i>p a website of math refs in Gilbert and Sullivan plays (gsarch is very close to gasarch) ? </i>which IS the kind of thing I would do. The answer is no: <i> gsarch</i> stands for <i>Gilbert and Sullivan archive.</i> They could have called it <i>gasarch </i>if they used the <i>and </i>in <i>Gilbert and Sullivan</i> but abbreviated <i>archive </i>as arch. Then I would have been far more confused. </p><p>Moving on...</p><p>In 1884<i> Princess Ida</i> opened in 1884. For more on this comic opera see <a href="https://en.wikipedia.org/wiki/Princess_Ida">here</a>.</p><p>In 1882 pi was proven  transcendental and hence one cannot square the circle. For more on pi being transcendental see <a href="https://en.wikipedia.org/wiki/Squaring_the_circle">here</a>.</p><p>Kolmogorov Random Thoughts on all of this</p><p>0) The song is sung my three men who are making fun of the notion of a women's college. The song is about all the things the women are trying to do that are absurd such as squaring the circle. They also mention perpetual motion machines. </p><p>1) Did G and S know that the squaring the circle had been proven impossible, or just that it was thought to be impossible, or just that it was thought to be hard?</p><p>2) Was it known that perpetual motion machines were impossible? Or just very hard? </p><p>3) G and S used Mathematics in at least one other song: <i> I am the very model of a modern major general, </i>from<i> The Pirates of Penzance  </i>has the lines:</p><p><br /></p><p>                                       <i>I'm very well acquainted too with matters mathematical</i></p><p><i>                                       I understand equations, both the simple and quadratical,</i></p><p><i>                                       About binomial theorems I'm teeming with the a lot o' news---</i></p><p><i>                                       With many cheerful facts about the square of the hypotenuse</i></p><p><br /></p><p>and later </p><p><i>                                        I'm very good at integral and differential calculus</i></p><p>See <a href="https://naic.edu/~gibson/poems/gilbert1.html">here</a> for all the lyrics. The website mentioned in the next point has a pointer to a YouTube video of people singing it. </p><p>4) There are many parodies of <i>Modern Major General. </i>The earliest ones I know of is Tom Lehrer's  <i>The Elements. </i>Since making a website of them IS the kind of thing I would do,  while writing this post I did it (Are we compelled to do things that fit our image of ourselves? Yup.) The website is <a href="http://www.cs.umd.edu/~gasarch/FUN//modmajgen.html">here</a>. It has 36 parodies (as of Oct 17, 2021 when I wrote this blog--- it may have more if you read this later.) That may seem like a lot, but it pales in comparison  to the most satirized song of all time: <i>The 12 days of Christmas </i>which I did an ugly lyrics-only website for back before html had nice tools, see <a href="http://www.cs.umd.edu/~gasarch/12days.html">here</a>. It has 143 songs on it but I am sure there are many more. (Note to self: redo that website when you have time. Maybe when I retire.) </p><p>4) I suspect that G and S knew more math, or perhaps knew of more math,  than Broadway composers know now. I suspect this is a more general trend: people are more specialized now. Having said that, I need to mention the off-Broadway musical <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Tango">Fermat's last Tango</a> which I liked more than Lance (see his post on it <a href="https://blog.computationalcomplexity.org/2004/02/fermats-last-tango.html">here</a>). </p><p>5) How much math would you need to know in order to insert some into your play or movie? With Wikipedia and other web sources you could find out some things, but you would have to have some idea what you are looking for. And perhaps you would need some math background in order to even want to insert some math into your work in the first place. </p><p>6)  Here's hoping someone will make a musical about William Rowan Hamilton using this song <a href="https://www.youtube.com/watch?v=SZXHoWwBcDc">here</a> as a starting point. I blogged rather optimistically about that possibility <a href="https://blog.computationalcomplexity.org/2017/04/william-rowan-hamilton-musical.html#comment-form">here</a>.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html"><span class="datestr">at October 24, 2021 06:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/24/new-computational-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html">New computational geometry journal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Over the past year or so I’ve been working with Marc van Kreveld and Wolfgang Mulzer to set up a new <a href="https://en.wikipedia.org/wiki/Diamond_open_access">diamond open access</a> computational geometry journal, <em><a href="https://www.cgt-journal.org/index.php/cgt">Computing in Geometry and Topology</a></em>, sponsored by the <a href="https://www.computational-geometry.org/">Society for Computational Geometry</a>, the organization set up to run the annual <em>Symposium on Computational Geometry</em>. It is the second such journal to be established, after the <em><a href="https://jocg.org/index.php/jocg">Journal of Computational Geometry</a></em>. This week the new journal went live and it is now available for submissions. Below is the official announcement we sent out to several mailing lists:</p>

<hr />

<p>Dear all,</p>

<p>As of this week, the new diamond open access journal</p>

<p style="text-align: center;"><em>Computing in Geometry and Topology</em></p>

<p>is welcoming submissions. The website of the journal is <a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<p>Here you can also find the editorial board, the submission guidelines, the scope, and a style file.</p>

<p>If you have any questions about the journal, please send them to info@cgt-journal.org</p>

<p>Best regards,
David Eppstein, Marc van Kreveld, and Wolfgang Mulzer</p>

<hr />

<p><strong>Purpose and scope</strong></p>

<p><em>Computing in Geometry and Topology</em> aims to support the broader computational geometry and topology community by being a peer-reviewed scientific journal that provides diamond open access. <em>Computing in Geometry and Topology</em> is sponsored by the Society for Computational Geometry.</p>

<p>With the broader computational geometry and topology community, we include researchers in discrete and combinatorial geometry, and any application area of computational geometry and topology. We also include algorithm engineering for geometric computations.</p>

<p>The journal publishes two types of papers. Firstly, the journal publishes original research of sufficient depth and interest. Secondly, the journal publishes high-quality survey papers. Every paper has been thoroughly reviewed by experts in the area.</p>

<p>To emphasize the breadth of the interpretation of computational geometry and topology, the editorial board has different sections that represent the algorithmic and mathematical aspects, the applied aspects, and the engineering aspects.</p>

<p><a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<hr />

<p>(<a href="https://mathstodon.xyz/@11011110/107159503144164449">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html"><span class="datestr">at October 24, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/147">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/147">TR21-147 |  Extractors for Sum of Two Sources | 

	Jyun-Jie Liao, 

	Eshan Chattopadhyay</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the problem of extracting randomness from \textit{sumset sources}, a general class of weak sources introduced by Chattopadhyay and Li (STOC, 2016). An $(n,k,C)$-sumset source $\mathbf{X}$ is a distribution on $\{0,1\}^n$ of the form $\mathbf{X}_1 + \mathbf{X}_2 + \ldots + \mathbf{X}_C$, where $\mathbf{X}_i$'s are independent sources on $n$ bits with min-entropy at least $k$. Prior extractors either required the number of sources $C$ to be a large constant or the min-entropy $k$ to be at least $0.51 n$. 

As our main result, we construct an explicit extractor for sumset sources in the setting of $C=2$ for min-entropy $\mathrm{poly}(\log n)$ and polynomially small error. We can further improve the min-entropy requirement  to $(\log n) \cdot (\log \log n)^{1 + o(1)}$ at the expense of worse error parameter of our extractor. We find applications of our sumset extractor for extracting randomness from other well-studied models of weak sources such as affine sources, small-space sources,  and interleaved sources.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/147"><span class="datestr">at October 24, 2021 11:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10824">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10824">Dynamic Bipartite Matching Market with Arrivals and Departures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kakimura:Naonori.html">Naonori Kakimura</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Donghao.html">Donghao Zhu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10824">PDF</a><br /><b>Abstract: </b>In this paper, we study a matching market model on a bipartite network where
agents on each side arrive and depart stochastically by a Poisson process. For
such a dynamic model, we design a mechanism that decides not only which agents
to match, but also when to match them, to minimize the expected number of
unmatched agents. The main contribution of this paper is to achieve theoretical
bounds on the performance of local mechanisms with different timing properties.
We show that an algorithm that waits to thicken the market, called the
$\textit{Patient}$ algorithm, is exponentially better than the
$\textit{Greedy}$ algorithm, i.e., an algorithm that matches agents greedily.
This means that waiting has substantial benefits on maximizing a matching over
a bipartite network. We remark that the Patient algorithm requires the planner
to identify agents who are about to leave the market, and, under the
requirement, the Patient algorithm is shown to be an optimal algorithm. We also
show that, without the requirement, the Greedy algorithm is almost optimal. In
addition, we consider the $\textit{1-sided algorithms}$ where only an agent on
one side can attempt to match. This models a practical matching market such as
a freight exchange market and a labor market where only agents on one side can
make a decision. For this setting, we prove that the Greedy and Patient
algorithms admit the same performance, that is, waiting to thicken the market
is not valuable. This conclusion is in contrast to the case where agents on
both sides can make a decision and the non-bipartite case by [Akbarpour et
al.,$~\textit{Journal of Political Economy}$, 2020].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10824"><span class="datestr">at October 24, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10759">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10759">Balanced Allocations: Caching and Packing, Twinning and Thinning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Los:Dimitrios.html">Dimitrios Los</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sauerwald:Thomas.html">Thomas Sauerwald</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sylvester:John.html">John Sylvester</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10759">PDF</a><br /><b>Abstract: </b>We consider the sequential allocation of $m$ balls (jobs) into $n$ bins
(servers) by allowing each ball to choose from some bins sampled uniformly at
random. The goal is to maintain a small gap between the maximum load and the
average load. In this paper, we present a general framework that allows us to
analyze various allocation processes that slightly prefer allocating into
underloaded, as opposed to overloaded bins. Our analysis covers several natural
instances of processes, including:
</p>
<p>The Caching process (a.k.a. memory protocol) as studied by Mitzenmacher,
Prabhakar and Shah (2002): At each round we only take one bin sample, but we
also have access to a cache in which the most recently used bin is stored. We
place the ball into the least loaded of the two.
</p>
<p>The Packing process: At each round we only take one bin sample. If the load
is below some threshold (e.g., the average load), then we place as many balls
until the threshold is reached; otherwise, we place only one ball.
</p>
<p>The Twinning process: At each round, we only take one bin sample. If the load
is below some threshold, then we place two balls; otherwise, we place only one
ball.
</p>
<p>The Thinning process as recently studied by Feldheim and Gurel-Gurevich
(2021): At each round, we first take one bin sample. If its load is below some
threshold, we place one ball; otherwise, we place one ball into a
$\textit{second}$ bin sample.
</p>
<p>As we demonstrate, our general framework implies for all these processes a
gap of $\mathcal{O}(\log n)$ between the maximum load and average load, even
when an arbitrary number of balls $m \geq n$ are allocated (heavily loaded
case). Our analysis is inspired by a previous work of Peres, Talwar and Wieder
(2010) for the $(1+\beta)$-process, however here we rely on the interplay
between different potential functions to prove stabilization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10759"><span class="datestr">at October 24, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10729">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10729">Part-X: A Family of Stochastic Algorithms for Search-Based Test Generation with Probabilistic Guarantees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedrielli:Giulia.html">Giulia Pedrielli</a>, Tanmay Kandhait, Surdeep Chotaliya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thibeault:Quinn.html">Quinn Thibeault</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Hao.html">Hao Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Castillo=Effen:Mauricio.html">Mauricio Castillo-Effen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fainekos:Georgios.html">Georgios Fainekos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10729">PDF</a><br /><b>Abstract: </b>Requirements driven search-based testing (also known as falsification) has
proven to be a practical and effective method for discovering erroneous
behaviors in Cyber-Physical Systems. Despite the constant improvements on the
performance and applicability of falsification methods, they all share a common
characteristic. Namely, they are best-effort methods which do not provide any
guarantees on the absence of erroneous behaviors (falsifiers) when the testing
budget is exhausted. The absence of finite time guarantees is a major
limitation which prevents falsification methods from being utilized in
certification procedures. In this paper, we address the finite-time guarantees
problem by developing a new stochastic algorithm. Our proposed algorithm not
only estimates (bounds) the probability that falsifying behaviors exist, but
also it identifies the regions where these falsifying behaviors may occur. We
demonstrate the applicability of our approach on standard benchmark functions
from the optimization literature and on the F16 benchmark problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10729"><span class="datestr">at October 24, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10725">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10725">An Invariance Principle for the Multi-slice, with Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Mark.html">Mark Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khot:Subhash.html">Subhash Khot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lifshitz:Noam.html">Noam Lifshitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Minzer:Dor.html">Dor Minzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10725">PDF</a><br /><b>Abstract: </b>Given an alphabet size $m\in\mathbb{N}$ thought of as a constant, and
$\vec{k} = (k_1,\ldots,k_m)$ whose entries sum of up $n$, the
$\vec{k}$-multi-slice is the set of vectors $x\in [m]^n$ in which each symbol
$i\in [m]$ appears precisely $k_i$ times. We show an invariance principle for
low-degree functions over the multi-slice, to functions over the product space
$([m]^n,\mu^n)$ in which $\mu(i) = k_i/n$. This answers a question raised by
Filmus et al.
</p>
<p>As applications of the invariance principle, we show:
</p>
<p>1. An analogue of the "dictatorship test implies computational hardness"
paradigm for problems with perfect completeness, for a certain class of
dictatorship tests. Our computational hardness is proved assuming a recent
strengthening of the Unique-Games Conjecture, called the Rich $2$-to-$1$ Games
Conjecture. Using this analogue, we show that assuming the Rich $2$-to-$1$
Games Conjecture, (a) there is an $r$-ary CSP $\mathcal{P}_r$ for which it is
NP-hard to distinguish satisfiable instances of the CSP and instances that are
at most $\frac{2r+1}{2^r} + o(1)$ satisfiable, and (b) hardness of
distinguishing $3$-colorable graphs, and graphs that do not contain an
independent set of size $o(1)$.
</p>
<p>2. A reduction of the problem of studying expectations of products of
functions on the multi-slice to studying expectations of products of functions
on correlated, product spaces. In particular, we are able to deduce analogues
of the Gaussian bounds from \cite{MosselGaussian} for the multi-slice.
</p>
<p>3. In a companion paper, we show further applications of our invariance
principle in extremal combinatorics, and more specifically to proving removal
lemmas of a wide family of hypergraphs $H$ called $\zeta$-forests, which is a
natural extension of the well-studied case of matchings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10725"><span class="datestr">at October 24, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10701">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10701">Optimizing Strongly Interacting Fermionic Hamiltonians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hastings:Matthew_B=.html">Matthew B. Hastings</a>, Ryan O'Donnell <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10701">PDF</a><br /><b>Abstract: </b>The fundamental problem in much of physics and quantum chemistry is to
optimize a low-degree polynomial in certain anticommuting variables. Being a
quantum mechanical problem, in many cases we do not know an efficient classical
witness to the optimum, or even to an approximation of the optimum. One
prominent exception is when the optimum is described by a so-called "Gaussian
state", also called a free fermion state. In this work we are interested in the
complexity of this optimization problem when no good Gaussian state exists. Our
primary testbed is the Sachdev--Ye--Kitaev (SYK) model of random degree-$q$
polynomials, a model of great current interest in condensed matter physics and
string theory, and one which has remarkable properties from a computational
complexity standpoint. Among other results, we give an efficient classical
certification algorithm for upper-bounding the largest eigenvalue in the $q=4$
SYK model, and an efficient quantum certification algorithm for lower-bounding
this largest eigenvalue; both algorithms achieve constant-factor approximations
with high probability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10701"><span class="datestr">at October 24, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10685">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10685">Predicting parameters for the Quantum Approximate Optimization Algorithm for MAX-CUT from the infinite-size limit</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boulebnane:Sami.html">Sami Boulebnane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montanaro:Ashley.html">Ashley Montanaro</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10685">PDF</a><br /><b>Abstract: </b>Combinatorial optimization is regarded as a potentially promising application
of near and long-term quantum computers. The best-known heuristic quantum
algorithm for combinatorial optimization on gate-based devices, the Quantum
Approximate Optimization Algorithm (QAOA), has been the subject of many
theoretical and empirical studies. Unfortunately, its application to specific
combinatorial optimization problems poses several difficulties: among these,
few performance guarantees are known, and the variational nature of the
algorithm makes it necessary to classically optimize a number of parameters. In
this work, we partially address these issues for a specific combinatorial
optimization problem: diluted spin models, with MAX-CUT as a notable special
case. Specifically, generalizing the analysis of the Sherrington-Kirkpatrick
model by Farhi et al., we establish an explicit algorithm to evaluate the
performance of QAOA on MAX-CUT applied to random Erdos-Renyi graphs of expected
degree $d$ for an arbitrary constant number of layers $p$ and as the problem
size tends to infinity. This analysis yields an explicit mapping between QAOA
parameters for MAX-CUT on Erdos-Renyi graphs of expected degree $d$, in the
limit $d \to \infty$, and the Sherrington-Kirkpatrick model, and gives good
QAOA variational parameters for MAX-CUT applied to Erdos-Renyi graphs. We then
partially generalize the latter analysis to graphs with a degree distribution
rather than a single degree $d$, and finally to diluted spin-models with
$D$-body interactions ($D \geq 3$). We validate our results with numerical
experiments suggesting they may have a larger reach than rigorously
established; among other things, our algorithms provided good initial, if not
nearly optimal, variational parameters for very small problem instances where
the infinite-size limit assumption is clearly violated.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10685"><span class="datestr">at October 24, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10494">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10494">Deep Point Cloud Normal Estimation via Triplet Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Weijia.html">Weijia Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Xuequan.html">Xuequan Lu</a>, Dasith de Silva Edirimuni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xiao.html">Xiao Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Robles=Kelly:Antonio.html">Antonio Robles-Kelly</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10494">PDF</a><br /><b>Abstract: </b>Normal estimation on 3D point clouds is a fundamental problem in 3D vision
and graphics. Current methods often show limited accuracy in predicting normals
at sharp features (e.g., edges and corners) and less robustness to noise. In
this paper, we propose a novel normal estimation method for point clouds. It
consists of two phases: (a) feature encoding which learns representations of
local patches, and (b) normal estimation that takes the learned representation
as input and regresses the normal vector. We are motivated that local patches
on isotropic and anisotropic surfaces have similar or distinct normals, and
that separable features or representations can be learned to facilitate normal
estimation. To realise this, we first construct triplets of local patches on 3D
point cloud data, and design a triplet network with a triplet loss for feature
encoding. We then design a simple network with several MLPs and a loss function
to regress the normal vector. Despite having a smaller network size compared to
most other methods, experimental results show that our method preserves sharp
features and achieves better normal estimation results on CAD-like shapes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10494"><span class="datestr">at October 24, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10437">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10437">A unifying framework for $n$-dimensional quasi-conformal mappings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Daoping.html">Daoping Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choi:Gary_P=_T=.html">Gary P. T. Choi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Jianping.html">Jianping Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lui:Lok_Ming.html">Lok Ming Lui</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10437">PDF</a><br /><b>Abstract: </b>With the advancement of computer technology, there is a surge of interest in
effective mapping methods for objects in higher-dimensional spaces. To
establish a one-to-one correspondence between objects, higher-dimensional
quasi-conformal theory can be utilized for ensuring the bijectivity of the
mappings. In addition, it is often desirable for the mappings to satisfy
certain prescribed geometric constraints and possess low distortion in
conformality or volume. In this work, we develop a unifying framework for
computing $n$-dimensional quasi-conformal mappings. More specifically, we
propose a variational model that integrates quasi-conformal distortion,
volumetric distortion, landmark correspondence, intensity mismatch and volume
prior information to handle a large variety of deformation problems. We further
prove the existence of a minimizer for the proposed model and devise efficient
numerical methods to solve the optimization problem. We demonstrate the
effectiveness of the proposed framework using various experiments in two- and
three-dimensions, with applications to medical image registration, adaptive
remeshing and shape modeling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10437"><span class="datestr">at October 24, 2021 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10357">Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single object allocations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Dhruv Matani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Menghani:Gaurav.html">Gaurav Menghani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10357">PDF</a><br /><b>Abstract: </b>Applications making excessive use of single-object based data structures
(such as linked lists, trees, etc...) can see a drop in efficiency over a
period of time due to the randomization of nodes in memory. This slow down is
due to the ineffective use of the CPU's L1/L2 cache. We present a novel
approach for mitigating this by presenting the design of a single-object memory
allocator that preserves memory locality across randomly ordered memory
allocations and deallocations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10357"><span class="datestr">at October 24, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10283">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10283">Fine-Grained Complexity Theory: Conditional Lower Bounds for Computational Geometry</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10283">PDF</a><br /><b>Abstract: </b>Fine-grained complexity theory is the area of theoretical computer science
that proves conditional lower bounds based on the Strong Exponential Time
Hypothesis and similar conjectures. This area has been thriving in the last
decade, leading to conditionally best-possible algorithms for a wide variety of
problems on graphs, strings, numbers etc. This article is an introduction to
fine-grained lower bounds in computational geometry, with a focus on lower
bounds for polynomial-time problems based on the Orthogonal Vectors Hypothesis.
Specifically, we discuss conditional lower bounds for nearest neighbor search
under the Euclidean distance and Fr\'echet distance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10283"><span class="datestr">at October 24, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.10099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.10099">Matrix Discrepancy from Quantum Communication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Samuel_B=.html">Samuel B. Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghavendra:Prasad.html">Prasad Raghavendra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shetty:Abhishek.html">Abhishek Shetty</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.10099">PDF</a><br /><b>Abstract: </b>We develop a novel connection between discrepancy minimization and (quantum)
communication complexity. As an application, we resolve a substantial special
case of the Matrix Spencer conjecture. In particular, we show that for every
collection of symmetric $n \times n$ matrices $A_1,\ldots,A_n$ with $\|A_i\|
\leq 1$ and $\|A_i\|_F \leq n^{1/4}$ there exist signs $x \in \{ \pm 1\}^n$
such that the maximum eigenvalue of $\sum_{i \leq n} x_i A_i$ is at most
$O(\sqrt n)$. We give a polynomial-time algorithm based on partial coloring and
semidefinite programming to find such $x$.
</p>
<p>Our techniques open a new avenue to use tools from communication complexity
and information theory to study discrepancy. The proof of our main result
combines a simple compression scheme for transcripts of repeated (quantum)
communication protocols with quantum state purification, the Holevo bound from
quantum information, and tools from sketching and dimensionality reduction. Our
approach also offers a promising avenue to resolve the Matrix Spencer
conjecture completely -- we show it is implied by a natural conjecture in
quantum communication complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.10099"><span class="datestr">at October 24, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/">Tenure-Track Faculty Positions at Simon Fraser University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html">https://www.sfu.ca/computing/job-opportunities.html</a><br />
Email: cs_faculty_affairs@sfu.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/"><span class="datestr">at October 23, 2021 07:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/10/23/tcs-talk-wednesday-october-27-shravas-rao-northwestern-university/">TCS+ talk: Wednesday, October 27 — Shravas Rao, Northwestern University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://cims.nyu.edu/~rao/"><strong>Shravas Rao</strong></a> from Northwestern University will speak about “<em>Degree vs. Approximate Degree and Quantum Implications of Huang’s Sensitivity Theorem</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />,</p>
<ul>
<li>The degree of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is at most quadratic in the approximate degree of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />. This is optimal as witnessed by the OR function.</li>
<li>The deterministic query complexity of f is at most quartic in the quantum query complexity of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017).</li>
</ul>
<p>We apply these results to resolve the quantum analogue of the Aanderaa–Karp–Rosenberg conjecture. We show that if <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is a nontrivial monotone graph property of an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertex graph specified by its adjacency matrix, then <img src="https://s0.wp.com/latex.php?latex=Q%28f%29%3D%5COmega%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="Q(f)=\Omega(n)" class="latex" />, which is also optimal. We also show that the approximate degree of any read-once formula on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" /> variables is <img src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Csqrt%7Bn%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\Theta(\sqrt{n})" class="latex" />.</p>
<p>Based on joint work with Scott Aaronson, Shalev Ben-David, Robin Kothari, and Avishay Tal.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/10/23/tcs-talk-wednesday-october-27-shravas-rao-northwestern-university/"><span class="datestr">at October 23, 2021 07:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19240">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/">An Annoying Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>It’s the stupid questions that have some of the most surprising and interesting answers — Cory Doctorow</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/bt/" rel="attachment wp-att-19242"><img width="120" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/bt.jpg?resize=120%2C180&amp;ssl=1" class="alignright wp-image-19242" height="180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">ACM Turing Award <a href="https://amturing.acm.org/photo/tarjan_1092048.cfm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Robert Tarjan is well known to most—a Turing award winner in 1986 with <a href="https://amturing.acm.org/award_winners/tarjan_1092048.cfm">John Hopcroft</a>. Bob is a professor of Computer Science at Princeton University, and one of the world experts on linear time graph algorithms. I always thought if some NP-complete graph problem had a linear time algorithm, then P=NP would have long been solved by Bob. </p>
<p>
Today we talk about a problem that hasn’t been solved by Bob.</p>
<p>
The problem I mean was <em>quasi-</em>solved by Bob. That is, he gave a quasi-polynomial time algorithm. The problem is <em>group isomorphism</em> (GpI). Laci Babai discusses its relation as a special case of <em>graph isomorphism</em> (GI) in section 13.2 of his famous 2016 <a href="https://arxiv.org/pdf/1512.03547.pdf">paper</a> giving a quasi-polynomial time algorithm for GI. He says that the annoyance of GpI should temper expectations for getting GI into polynomial time, because:</p>
<blockquote><p><b> </b> <em> [I]n spite of considerable effort and the availability of powerful algebraic machinery, Group Isomorphism is still not known to be in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />. </em>
</p></blockquote>
<p>
</p><p></p><h2> Bob’s Least Result </h2><p></p>
<p></p><p>
Bob has many great and deep results. For example, Donald Knuth <a href="https://www.informit.com/articles/article.aspx?p=2213858">described</a> Bob’s strong connected component algorithm for directed graphs in these words:</p>
<blockquote><p><b> </b> <em> The data structures that he devised for this problem fit together in an amazingly beautiful way, so that the quantities you need to look at while exploring a directed graph are always magically at your fingertips. </em>
</p></blockquote>
<p></p><p>
Here is Ken’s slight editing of Wikipedia’s <a href="https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm">presentation</a>:</p>
<p><code><br />
<font size="-1"><br />
Input: Graph G = (V,E) whose nodes have fields int index, int lowlink, bool onStack<br />
Output: The set of subsets of V denoting the strongly connected components</font></code></p><font size="-1">
<p>Stack S := empty stack;<br />
int index := 0;  //global variable with invariant: index = smallest unused index</p>
<p>proc strongconnect(v):<br />
   v.index := index; v.lowlink := index; index++;<br />
   S.push(v); v.onStack := true;<br />
   for each w such that (v,w) is in E: </p>
<p>      if w.index is undefined then:  //w not yet visited so recurse on it </p>
<p>         strongconnect(w);<br />
         v.lowlink = min{v.lowlink,w.lowlink}  //lowest-indexed node reached</p>
<p>      else if w.onStack then: </p>
<p>         v.lowlink = min(v.lowlink, w.index);<br />
         //clever: w.index was lowest unused index at the time </p>
<p>      else:<br />
         pass;   //(v,w) goes to already-found component, so ignore.<br />
      end if;<br />
   end for </p>
<p>   if v.lowlink == v.index then: //we've completed a component so pop and output it<br />
      repeat:<br />
         Node w = S.pop();<br />
         w.onStack = false;<br />
         output w as part of the current strongly connected component;<br />
      until w == v;<br />
   end if;<br />
end proc;</p>
</font><p><font size="-1">for each v in V do:<br />
   if v.index is undefined then:<br />
      strongconnect(v);<br />
   end if;<br />
end for;<br />
</font><br />
</p>
<p></p><h2> The Annoying Problem </h2><p></p>
<p></p><p>
Just over ten years ago we talked about: How do we tell if two groups are isomorphic? Precisely on October 8, 2011 we talked about <a href="https://rjlipton.wpcomstaging.com/2011/10/08/an-annoying-open-problem/">group isomorphism</a>. We called it an annoying problem. It remains annoying. </p>
<p>
In the group isomorphism problem (GpI), we are given two finite groups <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> in terms of their multiplication tables and must decide if <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" />.</p>
<p>
Bob Tarjan famously noted that this could be done <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Clog_2+n+%2B+O%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{\log_2 n + O(1)}}" class="latex" /> time. He never published this result and just orally told others about it. The insight was that every group of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> had a generator set of size at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 n}" class="latex" />. Since his insight the group isomorphism problem remains roughly the same complexity. The best is the <a href="https://rjlipton.wpcomstaging.com/2013/05/11/advances-on-group-isomorphism/">improvement</a> due to David Rosenbaum. His algorithm is still of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7Bc+%5Clog+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{c \log n}}" class="latex" />. See <a href="https://arxiv.org/pdf/1304.3935.pdf">here</a> for details. </p>
<blockquote><p><b>Theorem 1</b> <em> General group isomorphism is in <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%281%2F2%29+log_p+n%2BO%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{(1/2) log_p n+O(1)}}" class="latex" /> deterministic time where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> is the smallest prime dividing the order of the group. </em>
</p></blockquote>
<p></p><p>
Better than Bob’s insight, but still not polynomial time. </p>
<p>
</p><p></p><h2> Order Restriction </h2><p></p>
<p></p><p>
The results to date on GpI have relied mainly on basic group theory. Bob’s result uses an elementary fact about finite groups. David uses the same insight and adds a deep fact about isomorphism type problems. Neither use any of the “millions” of theorems known about finite groups. </p>
<p>
I wondered if there is some hope to start to exploit more of the papers on group structure? The trouble I think is that these results are quite difficult for those not in group theory to understand. But perhaps there is some hope. What do you think?</p>
<p>
I still wonder if there is a polynomial time algorithm for the general group isomorphism problem? Can we eliminate the <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log n}" class="latex" /> exponent somehow? This lead us to note the following simple theorem:</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> be finite groups of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> that is square free.Then <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" /> can be determined in time polynomial in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> time. </em>
</p></blockquote>
<p></p><p>
That is, <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> has no prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^2}" class="latex" /> divides <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />.</p>
<p>
</p><p></p><h2> But Wait… </h2><p></p>
<p></p><p>
As I worked on this I found a nice <a href="https://arxiv.org/pdf/1806.08872.pdf">paper</a> by Heiko Dietrich and James Wilson, titled “Polynomial Time Isomorphism Tests Of Black-Box Type Groups Of Most Orders.” They reference another <a href="https://arxiv.org/pdf/1810.03467.pdf">paper</a> that proved a stronger theorem than our above one:</p>
<blockquote><p><b>Theorem 3</b> <em> There is an algorithm that given groups <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of permutations on finitely many points, decides whether they are of cube-free order, and if so, decides that <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" /> or constructs an isomorphism <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Crightarrow+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \rightarrow H}" class="latex" />. The algorithm runs in time polynomial in the input size. </em>
</p></blockquote>
<p>
</p><p></p><h2> Proof of Our Theorem </h2><p></p>
<p>
</p><blockquote><p><b>Definition 4</b> <em> A finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is <b>metacyclic</b> provided it has a normal subgroup <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2FN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G/N}" class="latex" /> are both cyclic. </em>
</p></blockquote>
<p></p><p>
Every finite group of square-free order (i.e. the order is not divisible by the square of a natural number) is metacyclic.</p>
<p>
<em>Proof:</em>  We claim that <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> are both metacyclic. This follows from the <a href="https://en.wikipedia.org/wiki/Metacyclic_group">fact</a> that their order is square free. But then we know that both <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> are generated by at most two elements—see <a href="https://math.stackexchange.com/questions/1734406/prove-metacyclic-group-is-generated-by-two-elements">this</a>. This yields an isomorphism algorithm based on the above generator trick. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Box" class="latex" /></p>
<p>
Their stronger proof uses quite a bit more group theory. But our simple proof may give you some intuition why the restriction on the order of the group helps.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
The above result fits well with the belief that the worst case for isomorphism is when the order of the groups is a power of a prime. Some possible conjectures are:</p>
<ul>
<li>
Can we generalize “<img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is square free” to “<img src="https://s0.wp.com/latex.php?latex=%7Bp%5E4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^4}" class="latex" /> does not divide <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />“? Or to “<img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{5}}" class="latex" /> does not divide <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />?” And so on? <p></p>
</li><li>
Let <img src="https://s0.wp.com/latex.php?latex=%7BZ%28G%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z(G)=1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CAut%28G%29%7C+%5Cle+%7CG%7C%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|Aut(G)| \le |G|^{O(1)}}" class="latex" />. Then GpI is in polynomial time.
</li></ul>
<blockquote><p><b>Definition 5</b> <em> A finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is <b>metabelian</b> provided it has a normal subgroup <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2FN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G/N}" class="latex" /> are both abelian. </em>
</p></blockquote>
<ul>
<li>
What if we replace metacyclic by metabelian?
</li></ul></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/"><span class="datestr">at October 22, 2021 08:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6088">An Orthodox rabbi and Steven Weinberg walk into an email exchange…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Ever since I posted my <a href="https://scottaaronson.blog/?p=5566">obituary for the great Steven Weinberg</a> three months ago, I’ve gotten a steady trickle of emails—all of which I’ve appreciated enormously—from people who knew Steve, or were influenced by him, and who wanted to share their own thoughts and memories.  Last week, I was contacted by one Moshe Katz, an Orthodox rabbi, who wanted to share a long email exchange that he’d had with Steve, about Steve’s reasons for rejecting his birth-religion of Judaism (along with every other religion).  Even though Rabbi Katz, rather than Steve, does most of the talking in this exchange, and even though Steve mostly expresses the same views he’d expressed in many of his public writings, I knew immediately on seeing this exchange that it could be of broader interest—so I secured permission to share it here on <em>Shtetl-Optimized</em>, both from Rabbi Katz and from Steve’s widow Louise.</p>



<p>While longtime readers can probably guess what <em>I</em> think about most of the topics discussed, I’ll refrain from any editorial commentary in this post—but of course, feel free to share your own thoughts in the comments, and maybe I’ll join in.  Mostly, reading this exchange reminded me that someone at some point should write a proper book-length biography of Steve, and someone should also curate and publish a selection of his correspondence, much like <a href="https://www.amazon.com/Perfectly-Reasonable-Deviations-Letters-Richard/dp/141934322X"><em>Perfectly Reasonable Deviations from the Beaten Track</em></a> did for Richard Feynman.  There must be a lot more gems to be mined.</p>



<p>Anyway, without further ado, <a href="https://www.scottaaronson.com/weinberg.pdf"><strong>here’s the exchange</strong></a> (10 pages, PDF).</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6088"><span class="datestr">at October 22, 2021 03:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6086">Welcome to scottaaronson.blog !</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If you’ve visited <em>Shtetl-Optimized</em> lately — which, uh, I suppose you have — you may have noticed that your URL was redirected from www.scottaaronson.com/blog to scottaaronson.blog.  That’s because Automattic, makers of <a href="https://wordpress.com">WordPress.com</a>, volunteered to move my blog there from Bluehost, free of charge.  If all goes according to plan, you should notice faster loading times, less downtime, and <em>hopefully</em> nothing else different.  Please let me know if you encounter any problems.  And <em>huge</em> thanks to the WordPress.com Special Projects Team, especially Christopher Jones and Mark Drovdahl, for helping me out with this.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6086"><span class="datestr">at October 21, 2021 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/">Postdoc Positions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The<a href="https://jobmarket.unibocconi.eu/include/dwload.php?a=NTUwXjQ0M2QwY2Y4N2FhMjA2MDcxN2MwMzc3NGZjODM4NGU0Xi9kMC9qb2JtYXJrZXQudW5pYm9jY29uaS5ldS91cGxvYWQvQklEL3Nlc3Npb25fMTM3LTIwMjExMDE4Xmpta19zZXNfZmlsZV5qbWZfXmptZl9maWxlXjM2OQ=="> call is out</a> for two postdoctoral positions at Bocconi to work in my group [<a href="https://jobmarket.unibocconi.eu/application.php?f=MDJkYTc3YjUzZTNlMmNlMGIxMmNjOGQ0ZjRhNTc3MzBeMTM3">application link</a>]. If you are interested and you have any questions, feel free to email me (L.Trevisan at Unibocconi dot it)</p>



<p>The negotiable start date is September 1st, 2022. Each position is for one year, renewable for a second. The positions offer an internationally competitive salary (up to 65,000 Euro per year, tax-free, plus relocation assistance and travel allowance), in a wonderful location that, at long last, is back to more or less normal life. The application deadline is <strong>December 17, 2021</strong>.</p>



<p>Among the topics that I am interested in are spectral graph theory, average-case complexity, “applications” of semidefinite programming, random processes on networks, approximation algorithms, pseudorandomness and combinatorial constructions.</p>



<p>Bocconi Computer Science is building up a theory group: besides me, we have <a href="https://www.alonrosen.net/">Alon Rosen</a>, <a href="https://elias.ba30.eu/">Marek Elias</a>, a tenured person that will join next Fall, and more hires are on the horizon. Now that traveling is ok again, and considering that Alon and I both have ERC grants, we should expect a big stream of theory visitors coming and going through Bocconi from week-long visits to semester or year long sabbaticals.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/"><span class="datestr">at October 21, 2021 05:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/">postdoc at IRIF, DIENS, LIP6 (Paris, France) (apply by November 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking to recruit 1 to 3 postdocs for 1 to 3 years to work on the algorithmic theory of new data models (Theory, Algorithms), at IRIF (Université de Paris), LIP6 (Sorbonne Université), and DIENS (Université PSL) in Paris, France, with three years of funding by the ANR grant Algoridam (<a href="https://www.irif.fr/~algoridam/">https://www.irif.fr/~algoridam/</a>) and a flexible starting date.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc">https://www.irif.fr/postes/postdoc</a><br />
Email: postdoc.algoridam@listes.irif.fr.</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/"><span class="datestr">at October 20, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/10/20/opportunities-at-harvard/">Opportunities at Harvard!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Computer Science at Harvard, and in particular theoretical computer science and machine learning, is growing fast, see my 21-Tweet thread:</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/21 Banner year for Harvard CS! <br /><br />New hires include Sham Kakade <a href="https://twitter.com/ShamKakade6?ref_src=twsrc%5Etfw">@ShamKakade6</a> and Fernanda Viegas <a href="https://twitter.com/viegasf?ref_src=twsrc%5Etfw">@viegasf</a> (joining <a href="https://twitter.com/wattenberg?ref_src=twsrc%5Etfw">@wattenberg</a>), as well as David Alvarez-Melis, Anurag Anshu <a href="https://twitter.com/AnuragAnshu4?ref_src=twsrc%5Etfw">@AnuragAnshu4</a>, Sitan Chen, and Jonathan Frankle <a href="https://twitter.com/jefrankle?ref_src=twsrc%5Etfw">@jefrankle</a> <a href="https://t.co/mDR4bi0kSD">https://t.co/mDR4bi0kSD</a> <a href="https://t.co/bXwMbQK7s8">pic.twitter.com/bXwMbQK7s8</a></p>— Boaz Barak (@boazbaraktcs) <a href="https://twitter.com/boazbaraktcs/status/1450171218070495238?ref_src=twsrc%5Etfw">October 18, 2021</a></blockquote></div>
</div></figure>



<p>Please consider applying for <a href="https://www.seas.harvard.edu/computer-science/graduate-programs/how-apply"><strong>graduate studies in computer science</strong></a> (or encourage others to apply if like me, your grad-school days are behind you). </p>



<p>In recent years, I’ve taken a special interest in the <a href="https://mltheory.org/"><strong>theory of machine learning</strong></a>, and there are <a href="https://mltheory.org/#opportunities"><strong>several opportunities at Harvard</strong></a> in this area. In particular, I’ve been collaborating with people both in and outside CS, including incoming CS faculty <a href="https://homes.cs.washington.edu/~sham/"><strong>Sham Kakade</strong></a>, as well as <a href="http://www.demba-ba.org/" target="_blank" rel="noreferrer noopener"><strong>Demba Ba</strong></a> (Electrical Engineering and Bioengineering), <a href="http://lucasjanson.fas.harvard.edu/" target="_blank" rel="noreferrer noopener"><strong>Lucas Janson</strong></a> (Statistics), and  <a href="https://pehlevan.seas.harvard.edu/" target="_blank" rel="noreferrer noopener"><strong>Cengiz Pehlevan</strong></a> (Applied Mathematics). I’m very much open to co-advising students outside computer science as well. </p>



<p>Students interested in <a href="https://quantum.harvard.edu/"><strong>quantum computation and information</strong></a> can apply to any of our programs (including computer science, physics, and others) as well as to the new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering"><strong>quantum science and engineering degree</strong></a>. There are a number of Harvard faculty interested in <a href="https://quantum.harvard.edu/hqi-members">quantum computing</a>  including new incoming faculty member <a href="https://people.eecs.berkeley.edu/~anuraganshu/"><strong>Anurag Anshu</strong></a>. In this area as well I am open to co-advising, including students outside CS.</p>



<p>If you are applying to graduate studies and are potentially interested in working with me, you can indicate this in the application form and statement of interest. This is the best way to ensure that I read your application (and in particular better than emailing me separately: for fairness sake I read all the applications together in batch, regardless of whether the candidate emailed me or not).</p>



<p>We are also looking for <strong>postdocs</strong>! We have the general <strong>Rabin postdoc position</strong>, as well as specific positions in <strong>privacy</strong>, <strong>fairness</strong>, and <strong>theory of machine learning</strong>, all <a href="https://academicpositions.harvard.edu/postings/10730"><strong>described in the following ad</strong></a>. There is also a separate <a href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html"><strong>networking+theory</strong></a> postdoc position. Several other machine-learning theory related postdocs are linked in our <a href="https://mltheory.org/#opportunities"><strong>ML theory opportunities page</strong></a>. See also the <strong><a href="https://quantum.harvard.edu/external-candidates">quantum initiative postdoctoral fellowship</a></strong>. There may be more CS, ML and quantum related opportunities that I am missing. If I hear of more relevant opportunities then I will post them here and/or <a href="https://twitter.com/boazbaraktcs"><strong>on Twitter</strong></a>. </p>



<p>Finally, Harvard welcomes applications from candidates of all backgrounds, regardless of disability, gender identity and expression, physical appearance, race, religion, or sexual orientation. I would like to <strong>especially encourage</strong> applications from <strong>members of under-represented groups</strong>. Computer Science, and in particular the areas I work in, have a significant diversity problem. We are trying at Harvard to create an environment that is welcoming and inclusive for people from all backgrounds. I am sincerely grateful and appreciative of people placing their trust in us by applying, and we will do our best to live up to that trust.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/10/20/opportunities-at-harvard/"><span class="datestr">at October 20, 2021 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-3394824620518911343">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html">Networking+Theory Postdoc at Harvard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The last couple of years one aspect of research I've greatly enjoyed is getting back into networking, which is really due to my excellent (and patient) collaborators Ran Ben Basat (now at UCL, was a postdoc at Harvard) and Minlan Yu (at Harvard).  Minlan and I are working to establish a larger-scale Networking+Theory (hopefully broadening to an even larger Systems+Theory) group at Harvard, working on algorithmic problems in the context of real (or at least very real-ish) systems.  We have funding, and are looking for a postdoc, the basic description is below.  Ideally we're looking for people comfortable with the theory side and the systems side.  The website link for applying is <a href="https://academicpositions.harvard.edu/postings/10748">https://academicpositions.harvard.edu/postings/10748</a> .  We have preliminary website for the group at <a href="https://projects.iq.harvard.edu/theosys">https://projects.iq.harvard.edu/theosys</a> (it's just a start, Minlan and I are both on sabbatical, but you can see some of our publications).  We look forward to finding another member of the team!</p><p><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;"><span style="font-family: Avenir-Heavy, Helvetica, Arial, sans-serif; font-weight: bolder;">Networking + Theory Postdoctoral Position</span></span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">The John A. Paulson School of Engineering and Applied Sciences at Harvard University (SEAS) seeks applicants for a postdoctoral position in networking and theory. The postdoc is intended for one year but there will be funding to potentially extend it to a second year. The postdoc will receive a generous salary as well as an allocation for research and travel expenses.</span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">We are looking for junior scientists who are especially interested in working at the intersection of networking and algorithmic theory, in areas such as programmable network architectures, data center network management, cloud computing, and algorithms for the Internet.  Example topics of interest include but are not limited to the design and analysis of sketches and filters for use in real systems, network security, network compression methods, and optimizing network performance for machine learning applications.  The ideal candidate will be interested in both building real systems and either developing algorithms and data structures or using existing, underutilized results from the theoretical literature in system design.  </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">The postdoc is intended to work with closely with Minlan Yu and Michael Mitzenmacher, and others involved in the group focused on Systems + Theory work that they are developing, as well as possibly other Harvard faculty.   </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">Candidates should have backgrounds in networking and/or theoretical computer science.  Candidates should demonstrate experience in working at the intersection of these areas, or otherwise demonstrate how they will be able to contribute at the intersection. The candidate will be expected to publish scholarly papers, attend internal, domestic, and international conferences and meetings, and take on a mentorship role for undergraduate and graduate students.  </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">Harvard SEAS is dedicated to building a diverse community that is welcoming for everyone, regardless of disability, gender identity and expression, physical appearance, race, religion, or sexual orientation.  We strongly encourage applications from members of underrepresented groups.</span></p><p><br /></p><div><br /></div></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html"><span class="datestr">at October 20, 2021 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4572">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Online Optimization Post 6: The Impagliazzo Hard-Core Set Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(This is the sixth in a series of posts on online optimization techniques and their “applications” to complexity theory, combinatorics and pseudorandomness. The plan for this series of posts is to alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application.” The first two posts were about the technique of multiplicative weight updates and its application to “derandomizing” probabilistic arguments based on combining a Chernoff bound and a union bound. The third and fourth post were about the Follow-the-Regularized-Leader framework, and how it unifies multiplicative weights and gradient descent, and a “gradient descent view” of the Frieze-Kannan Weak Regularity Lemma. The fifth post was about the constrained version of the Follow-the-Regularized-Leader framework, and today we shall see how to apply that to a proof of the Impagliazzo Hard-Core Lemma.)</em></p>
<p><span id="more-4572"></span></p>
<p><b>1. The Impagliazzo Hard-Core Lemma </b></p>
<p>The Impagliazzo Hard-Core Lemma is a striking result in the theory of average-case complexity. Roughly speaking, it says that if <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" /> is a function that is “weakly” hard on average for a class <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> of “efficiently computable” functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f}" class="latex" />, that is, if, for some <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta&gt;0}" class="latex" />, we have that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f+%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+%5C%7B+0%2C1%5C%7D%5En%7D+%5Bf%28x%29+%3D+g%28x%29+%5D+%5Cleq+1+-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f \in {\cal F}: \ \ \Pr_{x\sim \{ 0,1\}^n} [f(x) = g(x) ] \leq 1 -\delta " class="latex" /></p>
<p>then there is a subset <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" /> of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+2%5Cdelta+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq 2\delta 2^n}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is “strongly” hard-on-average on <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />, meaning that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f+%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5Bf%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f \in {\cal F}: \ \ \Pr_{x\sim H} [f(x) = g(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
<p>for a small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt;0}" class="latex" />. Thus, the reason why functions from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> make a mistake in predicting <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> at least a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" /> fraction of the times is that there is a “hard-core” set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of inputs such that every function from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> makes a mistake about 1/2 of the times for the <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" /> fraction of inputs coming from <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />.</p>
<p>The result is actually not literally true as stated above, and it is useful to understand a counterexample, in order to motivate the correct statement. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> contains just <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\delta}" class="latex" /> functions, and that each function <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in \cal F}" class="latex" /> differs from <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> in exactly a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" /> fraction of inputs from <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" />, and that the set of mistakes are <em>disjoint</em>. Thus, for every set <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" />, no matter its size, there is a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in \cal F}" class="latex" /> that agrees with <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> on at least a <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1-\delta}" class="latex" /> fraction of inputs from <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />. The reason is that the sets of inputs on which the functions of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> differ from <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> form a partition of <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" />, and so their intersections with <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> form a partition of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />. By an averaging argument, one of those intersections must then contain at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%7CH%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta |H|}" class="latex" /> elements of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />.</p>
<p>In the above example, however, if we choose any three distinct functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2Cf_2%2Cf_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,f_2,f_3}" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+x%5Cin+%5C%7B+0%2C1+%5C%7D%5En%3A+%5C+%5C+%5C+g%28x%29+%3D+%7B%5Crm+majority%7D+%28f_1%28x%29%2C+f_2%28x%29%2Cf_3%28x%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall x\in \{ 0,1 \}^n: \ \ \ g(x) = {\rm majority} (f_1(x), f_2(x),f_3(x)) " class="latex" /></p>
<p>So, although <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is weakly hard on average with respect to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, we have that <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not even worst-case hard for a slight extension of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> in which we allow functions obtained by simple compositions of a small number of functions of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />.</p>
<blockquote><p><b>Theorem 1 (Impagliazzo Hard-Core Lemma)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Then at least one of the following conditions is true: </em></p>
<ul>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not weakly hard-on-average over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" /> with respect to a slight extension of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />) There is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in \{ 0,1 \}^n} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</li>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is strongly hard-on-average over a set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of density <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" />) There is a set <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BH+%5Cgeq+2%5Cdelta+%5Ccdot+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H \geq 2\delta \cdot 2^n}" class="latex" /> and
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Cin+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\in H} [ g(x) = f(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
</li>
</ul>
</blockquote>
<p>Where <img src="https://s0.wp.com/latex.php?latex=%7BI+%5C%7B+%7B%5Crm+boolean%5C+expression%7D+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{I \{ {\rm boolean\ expression} \}}" class="latex" /> is equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> depending on whether the boolean expression is true or false (the letter “<img src="https://s0.wp.com/latex.php?latex=%7BI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{I}" class="latex" />” stands for “indicator” function of the truth of the expression).</p>
<p><b>2. Proving the Lemma </b></p>
<p>Impagliazzo’s proof had <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> polynomial in both <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\epsilon}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\delta}" class="latex" />, and an alternative proof discovered by Nisan has a stronger bound on <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cepsilon%5E%7B-1%7D+%5Cdelta%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon^{-2} \log \epsilon^{-1} \delta^{-1}}" class="latex" />. The proofs of Impagliazzo and Nisan did not immediately give a set of size <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta2^n}" class="latex" /> (the set had size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta 2^n}" class="latex" />), although this could be achieved by iterating their argument. An idea of Holenstein allows to prove the above statement in a more direct way.</p>
<p>Today we will see how to obtain the Impagliazzo Hard-Core Lemma from online optimization, as done by Barak, Hardt and Kale. Their proof achieves all the parameters claimed above, once combined with Holenstein’s ideas.</p>
<p></p>
<p>We say that a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> (here “<img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />” stands for probability <em>measure</em>; we use this letter since we have already used <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> last time to denote the Bregman divergence) has min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K}" class="latex" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29+%5Cleq+2%5E%7B-K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x) \leq 2^{-K}}" class="latex" />. In other words, the min-entropy of a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> over a sample space <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+H_%7B%5Cinfty%7D+%28M%29+%3A%3D+%5Cmin_%7Bx%5Cin+X%7D+%5Clog_2+%5Cfrac+1+%7BM%28x%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle H_{\infty} (M) := \min_{x\in X} \log_2 \frac 1 {M(x)} " class="latex" /></p>
<p>The uniform distribution over a set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> has min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CH%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 |H|}" class="latex" />, and all distributions of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K}" class="latex" /> can be realized as a convex combination of distributions that are each uniform over a set of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq K}" class="latex" />, thus uniform distributions over large sets and large-min-entropy distributions are closely-related concepts. We will prove the following version of the hard-core lemma:</p>
<blockquote><p><b>Theorem 2 (Impagliazzo Hard-Core Lemma — Min-Entropy Version)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> be a finite set, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: X \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: X \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Then at least one of the following conditions is true: </em></p>
<ul>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not weakly hard-on-average over <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with respect to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />) There is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+X%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in X} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</li>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is strongly hard-on-average on a distribution of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta |X|}" class="latex" />) There is a distribution <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\sim H} [ g(x) = f(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
</li>
</ul>
</blockquote>
<p>Under minimal assumptions on <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> (that it contains <img src="https://s0.wp.com/latex.php?latex=%7B%3C%3C+2%5E%7B%7CX%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;&lt; 2^{|X|}}" class="latex" /> functions), the min-entropy version implies the set version, and the min-entropy version can be used as-is to derive most of the interesting consequences of the set version.</p>
<p>Let us restate it one more time.</p>
<blockquote><p><b>Theorem 3 (Impagliazzo Hard-Core Lemma — Min-Entropy Version)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> be a finite set, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: X \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: X \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Suppose that for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> we have </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%3E+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\sim H} [ g(x) = f(x) ] &gt; \frac 12 + \epsilon " class="latex" /></p>
<p>Then there is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+X%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in X} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</blockquote>
<p>As in previous posts, we are going to think about a game between a “builder” that works toward the construction of <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> and an “inspector” that looks for defects in the construction. More specifically, at every round <img src="https://s0.wp.com/latex.php?latex=%7Bi+%3D+1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i = 1,\ldots,T}" class="latex" />, the inspector is going to pick a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> and the builder is going to pick a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i\in {\cal F}}" class="latex" />. The loss function, that the inspector wants to minimize, is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i+%28M%29+%3A%3D+%5Cmathop%7B%5Cmathbb+P%7D_%7Bx%5Csim+M%7D+%5Bf_i+%28x%29+%3D+g%28x%29%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i (M) := \mathop{\mathbb P}_{x\sim M} [f_i (x) = g(x)] " class="latex" /></p>
<p>The inspector runs the agile online mirror descent algorithm with the constraint of picking distributions of the required min-entropy, and using the entropy regularizer; the builder always chooses a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i}" class="latex" /> such that that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i+%28M%29+%3A%3D+%5Cmathop%7B%5Cmathbb+P%7D_%7Bx%5Csim+M%7D+%5Bf_i+%28x%29+%3D+g%28x%29%5D+%3E+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i (M) := \mathop{\mathbb P}_{x\sim M} [f_i (x) = g(x)] &gt; \frac 12 + \epsilon " class="latex" /></p>
<p>which is always a possible choice given the assumptions of our theorem.</p>
<p>Just by plugging the above setting into the analysis from the previous post, we get that if we play this online game for <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T = O(\epsilon^{-2} \log \delta^{-1})}" class="latex" /> steps, the builder picks functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_T}" class="latex" /> such that, <em>for every distribution</em> <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log 2\delta |X|}" class="latex" />, we have</p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+%5C+i+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%3E+%5Cfrac+12+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, \ i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] &gt; \frac 12 \ \ \ \ \ (1)" class="latex" /></p>
<p><a name="game"></a><a name="game"></a><a name="game"></a><a name="game"></a></p>
<p>We will prove that <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a> holds in the next section, but we emphasize again that it is just a matter of mechanically using the analysis from the previous post. Impagliazzo’s proof relies, basically, on playing the game using lazy mirror descent with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_2^2}" class="latex" /> regularization, and he obtains a guarantee like the one above after <img src="https://s0.wp.com/latex.php?latex=%7BT%3DO%28%5Cepsilon%5E%7B-2%7D+%5Cdelta%5E%7B-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T=O(\epsilon^{-2} \delta^{-1})}" class="latex" /> steps.</p>
<p>What do we do with <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a>? Impagliazzo’s original reasoning was to define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+majority+%28f_1%28x%29%2C%5Cldots%2Cf_T%28x%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := majority (f_1(x),\ldots,f_T(x)) " class="latex" /></p>
<p>and to consider the set <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> of “bad” inputs <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) \neq g(x)}" class="latex" />. We have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+x+%5Cin+B+%5C+%5C+%5CPr_%7Bi+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall x \in B \ \ \Pr_{i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] \leq \frac 12 " class="latex" /></p>
<p>and so</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+B%2C+%5C+i+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim B, \ i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] \leq \frac 12 " class="latex" /></p>
<p>The min-entropy of the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CB%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 |B|}" class="latex" />, and this needs to be less than <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" />, so we conclude that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) \neq g(x)}" class="latex" /> happens for at most a <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" /> fraction of elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />.</p>
<p>This is qualitatively what we promised, but it is off by a factor of 2 from what we stated above. The factor of 2 comes from a subsequent idea of Holenstein. In Holenstein’s analysis, we sort elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> according to</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bi+%5Csim+%5C%7B+1%2C%5Cldots%2C+T+%5C%7D%7D+%5B+f_i%28x%29+%3D+g_i+%28x%29+%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{i \sim \{ 1,\ldots, T \}} [ f_i(x) = g_i (x) ] " class="latex" /></p>
<p>and he lets <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> be the set of <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta |X|}" class="latex" /> elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> for which the above quantity is smallest, and he shows that if we properly pick an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> and define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I%5C%7B+f_1%28x%29+%2B+%5Ccdots+%2B+f_T%28x%29+%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I\{ f_1(x) + \cdots + f_T(x) \geq b \} " class="latex" /></p>
<p>then <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x)}" class="latex" /> will be equal to <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g(x)}" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cnot%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\not\in B}" class="latex" /> and also for at least half the <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in B}" class="latex" />, meaning that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%3D+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) = g(x)}" class="latex" /> for at least a <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1-\delta}" class="latex" /> fraction of the input. Since this is a bit outside the scope of this series of posts, we will not give an exposition of Holenstein’s argument.</p>
<p><b>3. Analysis of the Online Game </b></p>
<p>It remains to show that we can achieve <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a> with <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1+%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1 {\delta}}" class="latex" />. As we said, we play a game in which, at every step <img src="https://s0.wp.com/latex.php?latex=%7Bi%3D1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i=1,\ldots,T}" class="latex" /></p>
<ul>
<li>The “inspector” player picks a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> of min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" />, that is, it picks a number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B2%5Cdelta+%7CX%7C%7D+%5Cgeq+M_i%28x%29%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {2\delta |X|} \geq M_i(x)\geq 0}" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in X}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+M_i%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_x M_i(x) = 1}" class="latex" />.</li>
<li>The “builder” player picks a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i+%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i \in {\cal F}}" class="latex" />, whose existence is guaranteed by the assumption of the theorem, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+M_i%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%5Cgeq+%5Cfrac+12+%2B%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim M_i} [f_i(x) = g(x) ] \geq \frac 12 +\epsilon " class="latex" /></p>
<p>and defines the loss function</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i%28M%29+%3A%3D+%5CPr_%7Bx%5Csim+M_i%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%3D+%5Csum_%7Bx%5Cin+X%7D+M%28x%29+%5Ccdot+I%5C%7B+f_i%28x%29+%3D+g%28x%29+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i(M) := \Pr_{x\sim M_i} [f_i(x) = g(x) ] = \sum_{x\in X} M(x) \cdot I\{ f_i(x) = g(x) \} " class="latex" /></p>
</li>
<li>The “inspector” is charged the loss <img src="https://s0.wp.com/latex.php?latex=%7BL_i%28M_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_i(M_i)}" class="latex" />.</li>
</ul>
<p>We analyze what happens if the inspector plays the strategy defined by agile mirror descent with negative entropy regularizer. Namely, we define the regularizer</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28M%29+%3D+c+%5Csum_x+M%28x%29+%5Clog+M%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle R(M) = c \sum_x M(x) \log M(x) " class="latex" /></p>
<p>for a choice of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> that we will fix later. The corresponding Bregman divergence is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M%2C%5Chat+M%29+%3D+c+KL%28M%2C%5Chat+M%29+%3D+c+%5Ccdot+%5Cleft%28+%5Csum_x+M%28x%29+%5Clog+%5Cfrac+%7BM%28x%29%7D%7B%5Chat+M%28x%29%7D+-+%5Csum_x+M%28x%29+%2B+%5Csum_x+%5Chat+M%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M,\hat M) = c KL(M,\hat M) = c \cdot \left( \sum_x M(x) \log \frac {M(x)}{\hat M(x)} - \sum_x M(x) + \sum_x \hat M(x) \right) " class="latex" /></p>
<p>and we work over the space <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal K}" class="latex" /> of distributions of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta |X|}" class="latex" />.</p>
<p>The agile online mirror descent algorithm is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M_1+%3D+%5Carg%5Cmin_%7BM%5Cin+%7B%5Ccal+K%7D%7D+R%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle M_1 = \arg\min_{M\in {\cal K}} R(M) " class="latex" /></p>
<p>so that <img src="https://s0.wp.com/latex.php?latex=%7BM_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_1}" class="latex" /> is the uniform distribution, and for <img src="https://s0.wp.com/latex.php?latex=%7Bi%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i\geq 1}" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat+M_%7Bi%2B1%7D+%3D+%5Carg%5Cmin_%7BM%3A+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D+%5C+%5C+D%28M_i%2CM%29+%2B+L_i+%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \hat M_{i+1} = \arg\min_{M: X \rightarrow {\mathbb R}} \ \ D(M_i,M) + L_i (M) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M_%7Bi%2B1%7D+%3D+%5Carg%5Cmin_%7BM+%5Cin+%7B%5Ccal+K%7D%7D+%5C+%5C+D%28M%2C+%5Chat+M_%7Bi%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle M_{i+1} = \arg\min_{M \in {\cal K}} \ \ D(M, \hat M_{i+1} )" class="latex" /></p>
<p>Solving the first step of agile online mirror descent, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat+M_%7Bi%2B1%7D+%28x%29+%3D+M_i%28x%29+e%5E%7B-%5Cfrac+1c+I+%5C%7B+f%28x%29+%3D+g%28x%29+%5C%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \hat M_{i+1} (x) = M_i(x) e^{-\frac 1c I \{ f(x) = g(x) \} } " class="latex" /></p>
<p>Using the analysis from the previous post, for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal K}" class="latex" />, and every number <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of steps, we have the regret bound</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5ET+L_i%28M_i%29+-+L_i%28M%29+%5Cleq+D%28M%2CM_1%29+%2B+%5Csum_%7Bi%3D1%7D%5ET+D%28M_i%2C+%5Chat+M_%7Bi%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \sum_{i=1}^T L_i(M_i) - L_i(M) \leq D(M,M_1) + \sum_{i=1}^T D(M_i, \hat M_{i+1} ) " class="latex" /></p>
<p>and we can bound</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M%2CM_1%29+%3D+c+%5Csum_x+M%28x%29+%5Ccdot+%5Cln+%7CX%7C+%5Ccdot+M%28x%29+%5Cleq+c+%5Cln+%5Cfrac+1+%7B2%5Cdelta%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M,M_1) = c \sum_x M(x) \cdot \ln |X| \cdot M(x) \leq c \ln \frac 1 {2\delta} " class="latex" /></p>
<p>and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M_i%2C%5Chat+M_%7Bi%2B1%7D%29+%3D+c%5Ccdot+%5Cleft%28+%5Csum_x+M_i%28x%29+%5Cln+%5Cfrac%7BM_i%28x%29%7D%7B%5Chat+M_%7Bi%2B1%7D+%28x%29+%7D+%2B+%5Csum_x+%5Chat+M_%7Bi%2B1%7D%28x%29+-+M_i%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M_i,\hat M_{i+1}) = c\cdot \left( \sum_x M_i(x) \ln \frac{M_i(x)}{\hat M_{i+1} (x) } + \sum_x \hat M_{i+1}(x) - M_i(x) \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+c+%5Ccdot+%5Csum_x+M_i%28x%29+%5Ccdot+%5Cleft%28+%5Cfrac+1c+I%5C%7B+f%28x%29+%3D+g%28x%29+%5C%7D+%2B+e%5E%7B-%5Cfrac+1+cI+%5C%7B+f%28x%29+%3D+g%28x%29%5C%7D%7D+-1+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle = c \cdot \sum_x M_i(x) \cdot \left( \frac 1c I\{ f(x) = g(x) \} + e^{-\frac 1 cI \{ f(x) = g(x)\}} -1 \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cleq+O+%5Cleft%28+%5Cfrac+1c+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \leq O \left( \frac 1c \right) " class="latex" /></p>
<p>where, in the last step, we used the fact the quantity in parenthesis is either 0 or <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fc+%2B+e%5E%7B-1%2Fc%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/c + e^{-1/c} - 1}" class="latex" /> which is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2Fc%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(1/c^2)}" class="latex" />, and that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+M_i%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_x M_i(x) = 1}" class="latex" /> because <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> is a distribution.</p>
<p>Overall, the regret is bounded by</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5ET+L_i%28M_i%29+-+L_i%28M%29+%5Cleq+O+%5Cleft%28+c%5Clog+%5Cfrac+1%5Cdelta+%2B+%5Cfrac+Tc+%5Cright%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7BT+%5Clog+%5Cfrac+1%5Cdelta%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \sum_{i=1}^T L_i(M_i) - L_i(M) \leq O \left( c\log \frac 1\delta + \frac Tc \right) \leq O \left( \sqrt{T \log \frac 1\delta}\right) " class="latex" /></p>
<p>where the last inequality comes from an optimized choice of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>Recall that we choose the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BL_i%28M_i%29+%5Cgeq+1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_i(M_i) \geq 1/2 + \epsilon}" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" />, so for every <img src="https://s0.wp.com/latex.php?latex=%7BM%5Cin+%7B%5Ccal+K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M\in {\cal K}}" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%5Cgeq+%5Cfrac+12+%2B+%5Cepsilon+-+O+%28%5Cleft%28+%5Csqrt%7B%5Cfrac+1+T+%5Clog+%5Cfrac+1%5Cdelta%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \frac 1T \sum_{i=1}^T L_i (M) \geq \frac 12 + \epsilon - O (\left( \sqrt{\frac 1 T \log \frac 1\delta}\right) " class="latex" /></p>
<p>and by choosing <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1 \delta}" class="latex" /> we get</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+M+%5Cin+%7B%5Ccal+K%7D+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall M \in {\cal K} : \ \ \ \frac 1T \sum_{i=1}^T L_i (M) &gt; \frac 12 " class="latex" /></p>
<p>It remains to observe that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%3D+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+%5CPr_%7Bx%5Csim+M%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%3D+%5CPr_%7Bi+%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%2C+%5C+x%5Csim+M%7D+%5B+f_i+%28x%29+%3D+g%28x%29+%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \frac 1T \sum_{i=1}^T L_i (M) = \frac 1T \sum_{i=1}^T \Pr_{x\sim M} [f_i(x) = g(x) ] = \Pr_{i \sim \{1,\ldots,T\}, \ x\sim M} [ f_i (x) = g(x) ] " class="latex" /></p>
<p>so we have that for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> of min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" /> it holds that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bi+%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%2C+%5C+x%5Csim+M%7D+%5B+f_i+%28x%29+%3D+g%28x%29+%5D%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{i \sim \{1,\ldots,T\}, \ x\sim M} [ f_i (x) = g(x) ]&gt; \frac 12 " class="latex" /></p>
<p>which is the statement that we promised and from which the Impagliazzo Hard-Core Lemma follows.</p>
<p><b>4. Some Final Remarks </b></p>
<p>After Impagliazzo circulated a preliminary version of his paper, Nisan had the following idea: consider the game that we define above, in which a builder picks an <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in {\cal F}}" class="latex" />, an inspector picks a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Cin+%7B%5Ccal+K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M \in {\cal K}}" class="latex" /> of the prescribed min-entropy, and the loss for the inspector is given by <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr+%5B+f%28x%29+%3D+g%28x%29+%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Pr [ f(x) = g(x) ]}" class="latex" />. We can think of it as a zero-sum game if we also assign a gain <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr+%5B+f%28x%29+%3D+g%28x%29%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Pr [ f(x) = g(x)]}" class="latex" /> to the builder.</p>
<p>If the builder plays second, there is a strategy that guarantees a gain that is at least <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2 + \epsilon}" class="latex" />, and so there must be a mixed strategy, that is, a distribution <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+DF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal DF}}" class="latex" /> over functions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, that guarantees such a gain even if the builder plays first. In other words, for all distributions <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of the prescribed min-entropy we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+f%5Csim+%7B%5Ccal+DF%7D%7D+%5B+f%28x%29+%3D+g%28x%29+%5D+%5Cgeq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, f\sim {\cal DF}} [ f(x) = g(x) ] \geq \frac 12 + \epsilon " class="latex" /></p>
<p>Nisan then observes that we can sample <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5Cfrac+1%7B%5Cepsilon%5E2%7D+%5Clog+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T = \frac 1{\epsilon^2} \log |X|}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_T}" class="latex" /> and have, with high probability</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+i%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, i\sim \{1,\ldots,T\}} [ f_i(x) = g(x) ] &gt; \frac 12 " class="latex" /></p>
<p>and the sampling bound on <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> can be improved to order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1%7B%5Cepsilon+%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1{\epsilon \delta}}" class="latex" /> with the same conclusion.</p>
<p>Basically, what we have been doing today is to come up with an algorithm that finds an approximate solution for the LP that defines the optimal mixed strategy for the game, and to design the algorithm is such a way that the solution is very sparse.</p>
<p>This is a common feature of other applications of online optimization techniques to find “sparse approximations”: one sets up an optimization problem whose objective function measures the “approximation error” of a given solution. The object we want to approximate is the optimum of the optimization problem, and we use variants of mirror descent to prove the existence of a sparse solution that is a good approximation.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/"><span class="datestr">at October 20, 2021 04:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/">Department Chair, Computer Science and Engineering at NYU Tandon (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>NYU’s Dept. of Computer Science and Engineering within the Tandon School of Engineering is hiring a department chair. The position is open to all research areas, but applications from theory candidates are very welcome. Our department has a growing Algorithms and Foundations Group (csefoundations.engineering.nyu.edu), with several recent hires working on theory of algorithms and machine learning.</p>
<p>Website: <a href="https://apply.interfolio.com/96495">https://apply.interfolio.com/96495</a><br />
Email: cmusco@nyu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/"><span class="datestr">at October 20, 2021 03:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/">Assistant Professor at Georgetown University (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Provost’s Distinguished Faculty Fellow and Assistant Professor of Computer Science position available at Georgetown University. For information on how to apply, please visit <a href="https://cs.georgetown.edu/jobs/">https://cs.georgetown.edu/jobs/</a></p>
<p>Website: <a href="http://apply.interfolio.com/96143">http://apply.interfolio.com/96143</a><br />
Email: nitin.vaidya@georgetown.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/"><span class="datestr">at October 20, 2021 02:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/10/20/highleyman/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/10/20/highleyman/">The Saga of Highleyman's Data.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The first machine learning benchmark dates back to the late 1950s. Few used it and even fewer still remembered it by the time benchmarks became widely used in machine learning in the late 1980s.</p>

<p>In 1959 at Bell Labs, Bill Highleyman and Louis Kamenstky designed a <a href="https://dl.acm.org/doi/10.1145/1457838.1457894">scanner to evaluate character recognition techniques</a>. Their goal was “to facilitate a systematic study of character-recognition techniques and an evaluation of methods prior to actual machine development.” It was not clear at the time which part of the computations should be done in special purpose hardware and which parts should be done with more general computers. Highleyman later <a href="https://patents.google.com/patent/US2978675A/en">patented an OCR scheme</a> that we recognize today as a convolutional neural network with convolutions optically computed as part of the scanning.</p>

<p>Highleyman and Kamentsky used their scanner to create a dataset of 1800 alphanumeric characters. They gathered the 26 letters of the alphabet and 10 digits from 50 different writers. Each character in their corpus was scanned in binary at a resolution of 12 x 12 and stored on punch cards that were compatible with the <a href="https://en.wikipedia.org/wiki/IBM_704">IBM 704</a>, the GPGPU of the era.</p>

<p class="center"><img width="95%" alt="A look at Highleyman’s digits" src="http://www.argmin.net/assets/highleyman-data.png" /></p>

<p>With the data in hand, Highleyman and Kamenstky began studying various proposed techniques for recognition. In particular, they analyzed a method of Woody Bledsoe and published an analysis claiming to be <a href="https://ieeexplore.ieee.org/document/5219829">unable to reproduce Bledsoe’s results</a>. Bledsoe found their numbers to be considerably lower than he had expected, and asked Highleyman to send him the data. Highleyman obliged, mailing the package of punch cards across the country to Sandia Labs.</p>

<p>Upon receiving the data, Bledsoe conducted a new experiment. In what may be the first implementation of a train-test split, he divided the characters up, using 40 writers for training and 10 for testing. By tuning the hyperparameters, <a href="https://ieeexplore.ieee.org/document/5219162">Bledsoe was able to achieve approximately 60% error</a>. Bledsoe also suggested that the high error rates were to be expected as Highleyman’s data was too small. Prophetically, he declared that 1000 alphabets might be needed for good performance.</p>

<p>By this point, Highleyman had also shared his data with Chao Kong “C.K.” Chow at the Burroughs Corporation (a precursor to Unisys). A pioneer of <a href="https://ieeexplore.ieee.org/document/5222035">using decision theory for pattern recognition</a>, Chow built a pattern recognition system for characters. Using the same train-test split as Bledsoe, <a href="https://ieeexplore.ieee.org/document/5219431">Chow obtained an error rate of 41.7%</a> using a convolutional neural network.</p>

<p class="center"><img width="75%" alt="Chow’s architecture" src="http://www.argmin.net/assets/chownet.png" /></p>

<p>Highleyman made at least six additional copies of the data he had sent to Bledsoe and Chow, and many researchers remained interested. He thus decided to <a href="https://ieeexplore.ieee.org/document/4037813">publicly offer to send a copy to anyone</a> willing to pay for the duplication and shipping fees. An interested party would simply have to mail him a request. Of course, the dataset was sent by US Postal Service. Electronic transfer didn’t exist at the time, resulting in sluggish data transfer rates on the order of a few bits per minute.</p>

<p>Highleyman not only created the first machine learning benchmark. He authored the the first formal study of <a href="https://ieeexplore.ieee.org/document/6768949">train-test splits</a> and proposed <a href="https://ieeexplore.ieee.org/document/4066882">empirical risk minimization for pattern classification</a> as part of his 1961 dissertation.
By 1963, however, Highleyman had left his research position at Bell Labs and abandoned pattern recognition research.</p>

<p>We don’t know how many people requested Highleyman’s data. The total number of copies may have been less than twenty. Based on citation surveys, we determined there were at least another six copies made after Highleyman’s public offer for duplication, sent to  <a href="https://ieeexplore.ieee.org/abstract/document/1671536">CMU</a>, <a href="https://ieeexplore.ieee.org/document/1671257">Honeywell</a>, <a href="https://ieeexplore.ieee.org/document/5008873">SUNY Stony Brook</a>, <a href="https://spiral.imperial.ac.uk/bitstream/10044/1/16132/2/Ullmann-JR-1968-PhD-Thesis.pdf">Imperial College</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/0031320371900045">UW Madison</a>, and Stanford Research Institute (SRI).</p>

<p>The SRI team of John Munson, Richard Duda, and Peter Hart performed some of the most <a href="https://ieeexplore.ieee.org/document/1687355">extensive experiments with Highleyman’s data</a>. A 1-nearest-neighbors baseline achieved an error rate of 47.5%. With a more sophisticated approach, they were able to do significantly better. They used a multi-class, piecewise linear model, trained using Kesler’s multi-class version of the perceptron algorithm (what we’d now call “one-versus all classification”). Their feature vectors were 84 simple pooled edge detectors in different regions of the image at different orientations. With these features, they were able to get a test error of 31.7%, 10 percentage points better than Chow. When restricted only to digits, this method recorded 12% error. The authors concluded that they needed more data, and that the error rates were “still far too high to be practical.” They concluded that “larger and higher-quality datasets are needed for work aimed at achieving useful results.” They suggested that such datasets “may contain hundreds, or even thousands, of samples in each class.”</p>

<p>Munson, Duda, and Hart also performed informal experiments with humans to gauge the readability of Highleyman’s characters. On the full set of alphanumeric characters, they found an average error rate of 15.7%, about 2x better than their pattern recognition machine. But this rate was still quite high and suggested the data needed to be of higher quality. They (again prophetically) concluded that “an array size of at least 20X20 is needed, with an optimum size of perhaps 30X30.”</p>

<p>Decades passed until such a dataset appeared. Thirty years later, with 125 times as much training data, 28x28 resolution, and with grayscale scans, a neural net achieved 0.7% test error on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST digit recognition task</a>. In fact, a similar model to Munson’s architecture consisting of kernel ridge regression trained on pooled edged detectors also achieves 0.6% error. Intuition from the 1960s proved right. The resolution was higher and the number of examples per digit was now in the thousands, just as Bledsoe, Munson, Duda, and Hart predicted would be sufficient. Reasoning heuristically that the test error should be inversely proportional to the square root of the number of training examples, we would expect an 11x improvement over Munson’s approaches. The actual recorded improvement from 12% to 0.7% was closer to 17x, not far from what the back of the envelope calculation predicts.</p>

<p>Unlike Highleyman’s data, MNIST featured only digits, no letters. Only recently, in 2017, researchers from Western Sydney University <a href="https://arxiv.org/abs/1702.05373">extracted alphanumeric characters from the NIST-19 repository</a>. The resulting <em>EMNIST_Balanced</em> dataset has 2400 examples in each of the 47 classes, with a class for all upper case letters, all digits, and some of the non-ambiguous lower case letters. Currently, the best performing <a href="https://www.mdpi.com/2076-3417/9/15/3169">model achieves a test error rate of 9.4%</a>. While the dataset is still fairly new, this is only a 3x improvement over the methods of Munson, Duda, and Hart. Applying the same naive scaling argument as above, the increase in dataset size would predict a 7x improvement if such an improvement was achievable. Considering that the SRI team observed a human-error rate of 11% on Highleyman’s data, it is quite possible that an accuracy of 90% is close to the best that we can expect for recognizing handwritten digits without context.</p>

<p>The story of Highleyman’s data foreshadows many of the later waves of machine learning research. A desire for better evaluation inspired the creation of novel data. Dissemination of the experimental results on this data led to sharing in order for researchers to be content that the evaluation was fair. Once the dataset was distributed, others requested the data to prove their methods were superior. And then the dataset itself became enshrined as a benchmark for competitive testing.  Such comparative testing led to innovations in methods, theory, and data collection and curation itself. We have seen this pattern time and time again in machine learning, from <a href="https://archive.ics.uci.edu/ml/index.php">the UCI repository</a>, to <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, to <a href="https://www.image-net.org/">ImageNet</a>, to <a href="https://predictioncenter.org/">CASP</a>. The nearly forgotten history of Highleyman’s data marks the beginning of this pattern recognition research paradigm.</p>

<p><em>We are, as always, deeply indebted to Chris Wiggins for sending us Munson et al.’s paper after watching a talk by BR on the history of ML benchmarking. We also thank Ludwig Schmidt for pointing us to EMNIST.</em></p>

<h2 id="addendum-on-our-protagonist-bill-highleyman">Addendum on our protagonist Bill Highleyman.</h2>

<p>After posting this blog, we found <a href="https://availabilitydigest.com/public_articles/1208/thesis.pdf">some lovely recollections by Bill Highleyman about his thesis</a>. It is remarkable how Bill invented so many powerful machine learning primitives—finding linear functions that minimize empirical risk, gradient descent to minimize the risk, train-test splits, convolutional neural networks—all as part of his PhD dissertation project. That said,
Bill considered the project to be a failure. He (and Bell Labs) realized the computing of 1959 was not up to the task of character recognition.</p>

<p>After he finished his thesis, Bill abandoned pattern recognition and moved on to work on other cool and practical computer engineering projects that interested him, never once looking back. By the mid sixties Bill had immersed himself in data communication and transmission, and patented novel approaches to electrolytic printing and financial transaction hardware. He eventually ended up specializing in high-reliability computing. Though he developed many of the machine learning techniques we use today, he was content to leave the field and work to advance general computing to catch up with his early ideas.</p>

<p>It’s odd but not surprising that while every machine learning class mentions Rosenblatt, Minsky, and Papert, almost everyone we’ve spoken with so far has never heard of Bill Highleyman.</p>

<p>We worry Bill is no longer reachable as he seems to have no online presence after 2019 and would be 88 years old today. If anyone out there on has met Bill, we’d love to hear more about him. Please drop us a note.</p>

<p>And if anyone has any idea of where we can get a copy of his 1800 characters from 1959, please let us know about that too…</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/10/20/highleyman/"><span class="datestr">at October 20, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/146">TR21-146 |  Sample-Based Proofs of Proximity | 

	Guy Goldberg, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose we have random sampling access to a huge object, such as a graph or a database.
Namely, we can observe the values of \emph{random} locations in the object, say random records in the database or random edges in the graph.
We cannot, however, query locations of our choice. Can we verify complex properties of the object using only this restricted sampling access?

In this work, we initiate the study of \emph{sample-based} proof systems, where the verifier is extremely constrained; Given an input, the verifier can only obtain samples of uniformly random and i.i.d. locations in the input string, together with the values at those locations. The goal is verifying complex properties in sublinear time, using only this restricted access.
Following the literature on Property Testing and on Interactive Proofs of Proximity (IPPs), we seek proof systems where the verifier accepts every input that has the property, and with high probability rejects every input that is \emph{far} from the property.

We study both interactive and non-interactive sample-based proof systems, showing:

	- On the positive side, our main result is that rich families of properties / languages have sub-linear sample-based interactive proofs of proximity (SIPPs).
	We show that every language in $\mathcal{NC}$ has a SIPP, where the sample and communication complexities, as well as the verifier's running time, are $\widetilde{O}(\sqrt{n})$, and with polylog(n) communication rounds.
	We also show that every language that can be computed in polynomial-time and bounded-polynomial space has a SIPP, where the sample and communication complexities of the protocol, as well as the verifier's running time are roughly $\sqrt{n}$, and with a constant number of rounds.
	
	This is achieved by constructing a reduction protocol from SIPPs to IPPs.
	With the aid of an untrusted prover, this reduction enables a restricted, sample-based verifier to simulate an execution of a (query-based) IPP, even though it cannot query the input.
	Applying the reduction to known query-based IPPs yields SIPPs for the families described above.
	
	- We show that every language with an adequate (query-based) property tester has a 1-round SIPP with \emph{constant} sample complexity and logarithmic communication complexity.
	One such language is equality testing, for which we give an explicit and simple SIPP.
	
	- On the negative side, we show that \emph{interaction} can be essential:
	we prove that there is no \emph{non}-interactive sample-based proof of proximity for equality testing.
	
	- Finally, we prove that \emph{private coins} can dramatically increase the power of SIPPs.
	We show a strong separation between the power of public-coin SIPPs and private-coin SIPPs for Equality Testing.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/146"><span class="datestr">at October 19, 2021 08:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/">Some Statistical Gamma Fun</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Nothing takes place in the world whose meaning is not that of some maximum or minimum. — Leonhard Euler</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/kaspermueller/" rel="attachment wp-att-19221"><img width="125" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/KasperMueller-150x150.jpg?resize=125%2C125&amp;ssl=1" class="alignright wp-image-19221" height="125" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cantor’s Paradise <a href="https://www.cantorsparadise.com/inventing-mathematics-a33cc9d2732b">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Kasper Müller is a mathematics and data science writer for <a href="https://medium.com/">Medium</a>, where he contributes primarily to the blogs <a href="https://www.cantorsparadise.com/">Cantor’s Paradise</a> and <a href="https://towardsdatascience.com/">Towards Data Science</a>. He wrote a nice <a href="https://www.cantorsparadise.com/the-beautiful-gamma-function-and-the-genius-who-discovered-it-8778437565dc">article</a> last April titled, “The Beautiful Gamma Function and the Genius Who Discovered It.”</p>
<p>
Today we discuss the relevance of the gamma function to statistics and use statistics to suggest a new kind of estimate for it.<br />
<span id="more-19219"></span></p>
<p>
The “Genius” that Müller refers to is Leonhard Euler. Euler proved that for all integers <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 0}" class="latex" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%21+%3D+%5Cint_0%5E1+%28-%5Cln+s%29%5En+ds+%3D+%5Cint_0%5E%5Cinfty+t%5En+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n! = \int_0^1 (-\ln s)^n ds = \int_0^\infty t^n e^{-t} dt, " class="latex" /></p>
<p>where the latter equation uses the substitution <img src="https://s0.wp.com/latex.php?latex=%7Bs+%3D+e%5E%7B-t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s = e^{-t}}" class="latex" />. The right-hand side produces a value for any complex number <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+x+%2B+iy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = x + iy}" class="latex" /> in place of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> provided <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3E+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x &gt; -1}" class="latex" />. This leads to the formal definition </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28z%29+%3D+%5Cint_0%5E%5Cinfty+t%5E%7Bz-1%7D+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma(z) = \int_0^\infty t^{z-1} e^{-t} dt, " class="latex" /></p>
<p>whose analytic extension is defined everywhere except for <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+0%2C+-1%2C+-2%2C+-3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = 0, -1, -2, -3,\dots}" class="latex" />. Because <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> has no zeroes, its reciprocal is an entire function. One neat value is <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" />. We will be mainly concerned with ratios of two values of <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" />.</p>
<p>
</p><p></p><h2> What is Gamma For? </h2><p></p>
<p></p><p>
For all <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> except the non-positive integers, <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> obeys the formula </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+1)}{\Gamma(z)} = z. " class="latex" /></p>
<p>Of course, this follows from <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29+%3D+%28z-1%29%21%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z) = (z-1)!}" class="latex" /> for positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" />. Also </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%29%7D+%3D+%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%2B1%29%7D%5Ccdot%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+%28z%2B1%29z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+2)}{\Gamma(z)} = \frac{\Gamma(z+2)}{\Gamma(z+1)}\cdot\frac{\Gamma(z+1)}{\Gamma(z)} = (z+1)z. " class="latex" /></p>
<p>In general, for all <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a &gt; 0}" class="latex" />, <a name="approx"></a></p><a name="approx">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2Ba%29%7D%7B%5CGamma%28z%29%7D+%5Csim+z%5Ea+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+a)}{\Gamma(z)} \sim z^a \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="approx"></a> but there is a discrepancy. This and the lack of a simple explicit formula for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> at all have always made the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function seem opaque to me. Two notable values are <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B3%7D%7B2%7D%29+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2}}" class="latex" />.</p>
<p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function is not even the only uniformly continuous interpolation of the factorial function. It is the unique one whose logarithm is a convex function. This is the first of many reasons given in Müller’s article for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> to be <em>salient</em> and beautiful, culminating in its relation to the Riemann zeta function given by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28%5Cfrac%7Bs%7D%7B2%7D%29%5Czeta%28s%29%7D%7B%5Cpi%5E%7Bs%2F2%7D%7D+%3D+%5Cfrac%7B%5CGamma%28%5Cfrac%7B1-s%7D%7B2%7D%29%5Czeta%281-s%29%7D%7B%5Cpi%5E%7B%281-s%29%2F2%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(\frac{s}{2})\zeta(s)}{\pi^{s/2}} = \frac{\Gamma(\frac{1-s}{2})\zeta(1-s)}{\pi^{(1-s)/2}}. " class="latex" /></p>
<p>Yet the log-convex uniqueness was proved only 99 years ago, and none of these tell me at a flash what the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function <b>is</b>. </p>
<p>
What is the simplest label for its corner of the sky? The leading example is the formula for the volume of a sphere of radius <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> dimensions: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++V_n+%3D+%5Cfrac%7B%5Cpi%5E%7Bn%2F2%7D%7D%7B%5CGamma%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7Dr%5En.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  V_n = \frac{\pi^{n/2}}{\Gamma(n + \frac{1}{2})}r^n. " class="latex" /></p>
<p>But I wonder whether a different application is more fundamental. Since we are dealing with <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a = \frac{1}{2}}" class="latex" /> already here, let us define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%3D+%5Cfrac%7B%5CGamma%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D%7B%5CGamma%28z%29%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) = \frac{\Gamma(z+\frac{1}{2})}{\Gamma(z)}. " class="latex" /></p>
<p>Noting <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%29+%5Csim+z%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z) \sim z^{1/2}}" class="latex" /> via (<a href="https://rjlipton.wpcomstaging.com/feed/#approx">1</a>), this is a tweak of the square-root function. Here are some values of it:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5CGamma_%7B1%2F2%7D%281%29+%26%3D%26+%5Cfrac%7B%5CGamma%281.5%29%7D%7B%5CGamma%281%29%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%2F2%7D%7B1%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%5C%5C+%5CGamma_%7B1%2F2%7D%282%29+%26%3D%26+%5Cfrac%7B%5CGamma%282.5%29%7D%7B%5CGamma%282%29%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%2F4%7D%7B1%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%7D%7B4%7D%5C%5C+%5CGamma_%7B1%2F2%7D%283%29+%26%3D%26+%5Cfrac%7B%5CGamma%283.5%29%7D%7B%5CGamma%283%29%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%2F8%7D%7B2%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%7D%7B16%7D%5C%5C+%5CGamma_%7B1%2F2%7D%284%29+%26%3D%26+%5Cfrac%7B%5CGamma%284.5%29%7D%7B%5CGamma%284%29%7D+%3D+%5Cfrac%7B105%5Csqrt%7B%5Cpi%7D%2F16%7D%7B6%7D+%3D+%5Cfrac%7B35%5Csqrt%7B%5Cpi%7D%7D%7B32%7D%5C%5C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  \Gamma_{1/2}(1) &amp;=&amp; \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}/2}{1} = \frac{\sqrt{\pi}}{2}\\ \Gamma_{1/2}(2) &amp;=&amp; \frac{\Gamma(2.5)}{\Gamma(2)} = \frac{3\sqrt{\pi}/4}{1} = \frac{3\sqrt{\pi}}{4}\\ \Gamma_{1/2}(3) &amp;=&amp; \frac{\Gamma(3.5)}{\Gamma(3)} = \frac{15\sqrt{\pi}/8}{2} = \frac{15\sqrt{\pi}}{16}\\ \Gamma_{1/2}(4) &amp;=&amp; \frac{\Gamma(4.5)}{\Gamma(4)} = \frac{105\sqrt{\pi}/16}{6} = \frac{35\sqrt{\pi}}{32}\\ \end{array} " class="latex" /></p>
<p>
Here is the significance:</p>
<blockquote><p><b> </b> <em> For integer <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 1}" class="latex" />, the expected Euclidean norm of a vector of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent samples from the standard Gaussian distribution is <a name="gammahalf"></a></em></p><em><a name="gammahalf">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csqrt%7B2%7D%5Ccdot%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sqrt{2}\cdot\Gamma_{1/2}(\frac{n}{2}). \ \ \ \ \ (2)" class="latex" /></p>
</a></em><p><em><a name="gammahalf"></a> </em>
</p></blockquote>
<p></p><p>
That’s it: Gamma gives the norm of Gaussians. The norm is of order <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{n}}" class="latex" /> but not exactly. The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function gives it <em>exactly</em>.</p>
<p>
</p><p></p><h2> An Inferior But Curious Estimate </h2><p></p>
<p></p><p>
The norm of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent Gaussians is called the <a href="https://en.wikipedia.org/wiki/Chi_distribution">chi distribution</a>. Its square is the better-known <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>. This idea is used in the statistical <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared</a> test, but what follows is simpler.</p>
<p>
We let <img src="https://s0.wp.com/latex.php?latex=%7BX%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X^2}" class="latex" /> stand for the square norm divided by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> stands for the Euclidean norm divided by <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{n}}" class="latex" />. From (<a href="https://rjlipton.wpcomstaging.com/feed/#gammahalf">2</a>) we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BX%5D+%3D+%5Csqrt%7B%5Cfrac%7B2%7D%7Bn%7D%7D%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E[X] = \sqrt{\frac{2}{n}}\Gamma_{1/2}(\frac{n}{2}). " class="latex" /></p>
<p>
We will estimate <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> a different way and use that to estimate <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}}" class="latex" />. First we note that since the vector entries <img src="https://s0.wp.com/latex.php?latex=%7Bz_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_i}" class="latex" /> are independent and normally distributed, we have the exact values</p>
<p><br /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++E%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+E%5Bz_i%5E2%5D+%3D+1%5C%5C+Var%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%5E2%7D+%5Csum_%7Bi%3D1%7D%5En+Var%5Bz_i%5E2%5D+%3D+%5Cfrac%7B1%7D%7Bn%5E2%7D%5Csum_%7Bi%3D1%7D%5En+%28E%5Bz_i%5E4%5D+-+E%5Bz_i%5D%5E2%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%283+-+1%29+%3D+%5Cfrac%7B2%7D%7Bn%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  E[X^2] &amp;=&amp; \frac{1}{n}\sum_{i=1}^n E[z_i^2] = 1\\ Var[X^2] &amp;=&amp; \frac{1}{n^2} \sum_{i=1}^n Var[z_i^2] = \frac{1}{n^2}\sum_{i=1}^n (E[z_i^4] - E[z_i]^2) = \frac{1}{n}(3 - 1) = \frac{2}{n}. \end{array} " class="latex" /></p>
<p></p><p><br />
Since we have <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X^2]}" class="latex" />, computing either <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> suffices to get the other, by the relation <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+E%5BX%5E2%5D+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = E[X^2] - E[X]^2}" class="latex" />. Our also having <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X^2]}" class="latex" /> enables estimating <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> via the <a href="http://www.phidot.org/software/mark/docs/book/pdf/app_2.pdf">delta</a> <a href="https://en.wikipedia.org/wiki/Delta_method">method</a>, in a particular form I noticed <a href="https://stats.stackexchange.com/questions/10337/operatornamevarx2-if-operatornamevarx-sigma2/383603">here</a>. The derivation requires no special properties of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />: <a name="est2"></a></p><a name="est2">
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Var%5BX%5E2%5D+%5Capprox+4E%5BX%5D%5E2+Var%5BX%5D+-+Var%5BX%5D%5E2.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Var[X^2] \approx 4E[X]^2 Var[X] - Var[X]^2. \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="est2"></a></p><p><br />
For our particular <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+1+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = 1 - E[X]^2}" class="latex" />, this yields a quadratic equation in <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y = E[X]^2}" class="latex" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D+%5Cdoteq+4y%281+-+y%29+-+%281-y%29%5E2%2C+%5Cquad%5Ctext%7Bso%7D%5Cquad+5y%5E2+-+6y+%2B+1+%2B+%5Cfrac%7B2%7D%7Bn%7D+%3D+0+%5Cquad%5Ctext%7Bso%7D%5Cquad+y+%3D+%5Cfrac%7B6+%2B+%5Csqrt%7B16+-+%5Cfrac%7B40%7D%7Bn%7D%7D%7D%7B10%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n} \doteq 4y(1 - y) - (1-y)^2, \quad\text{so}\quad 5y^2 - 6y + 1 + \frac{2}{n} = 0 \quad\text{so}\quad y = \frac{6 + \sqrt{16 - \frac{40}{n}}}{10}. " class="latex" /></p>
<p>This yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D%5CGamma_%7B1%2F2%7D%5E2%28%5Cfrac%7Bn%7D%7B2%7D%29+%5Cdoteq+%5Cfrac%7B3%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4%7D%7B25%7D+-+%5Cfrac%7B2%7D%7B5n%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n}\Gamma_{1/2}^2(\frac{n}{2}) \doteq \frac{3}{5} + \sqrt{\frac{4}{25} - \frac{2}{5n}}. " class="latex" /></p>
<p>
Changing variables to <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+%5Cfrac%7Bn%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = \frac{n}{2}}" class="latex" /> and rearranging, we get the estimate </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%5Cdoteq+%5Csqrt%7B%5Cfrac%7B3z%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4z%5E2%7D%7B25%7D+-+%5Cfrac%7Bz%7D%7B5%7D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) \doteq \sqrt{\frac{3z}{5} + \sqrt{\frac{4z^2}{25} - \frac{z}{5}}}. " class="latex" /></p>
<p>
It has been traditional to estimate what we would call <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z+\frac{1}{2})}" class="latex" /> instead, so putting <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+z+%2B+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = z + \frac{1}{2}}" class="latex" /> we finally get:</p>
<p>
<a name="estimate"></a></p><a name="estimate">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28x%2B1%29%7D%7B%5CGamma%28x%2B%5Cfrac%7B1%7D%7B2%7D%29%7D+%5Csim+%5Csqrt%7B0.6x+%2B+0.3+%2B+0.2%5Csqrt%7B8x%5E2+-+2x+-+1.5%7D%7D+%7E.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(x+1)}{\Gamma(x+\frac{1}{2})} \sim \sqrt{0.6x + 0.3 + 0.2\sqrt{8x^2 - 2x - 1.5}} ~. \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="estimate"></a></p>
<p>
As an estimate, this is barely competitive with the simple <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bx+%2B+0.25%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{x + 0.25}}" class="latex" /> and far inferior to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%5E2+%2B+0.5x+%2B+0.125%29%5E%7B1%2F4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  (x^2 + 0.5x + 0.125)^{1/4}, " class="latex" /></p>
<p>which is the first of several estimates of the form <img src="https://s0.wp.com/latex.php?latex=%7Bp_k%28x%29%5E%7B1%2F2k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p_k(x)^{1/2k}}" class="latex" /> <a href="https://www.sciencedirect.com/science/article/pii/S0895717710001317">given</a> by Cristinel Mortici in 2010. But it is curious that we got a formula with nested radicals and non-dyadic coefficients from a simple statistical estimate. It makes us wonder whether formulas with nested radicals can be tuned for greater accuracy, and whether this might knock back to statistical estimation.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can vectors of Gaussian variables be leveraged to say further interesting things about the gamma function and its applications? What are your favorite properties of the gamma function?</p>
<p>
[fixed missing “ds” in intro, typo n–&gt;sqrt(n) at end of sentence with “divided by”]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/"><span class="datestr">at October 19, 2021 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/145">TR21-145 |  Revisiting a Lower Bound on the Redundancy of Linear Batch Codes | 

	Omar Alrabiah, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent work of Li and Wootters (2021) shows a redundancy lower bound of $\Omega(\sqrt{Nk})$ for systematic linear $k$-batch codes of block length $N$ by looking at the $O(k)$ tensor power of the dual code. In this note, we present an alternate proof of their result via a linear independence argument on a collection of polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/145"><span class="datestr">at October 19, 2021 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=102">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/">Thursday Oct 21st — Maxim Raginsky from UIUC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, Oct</strong> <strong>21</strong>st at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong><a href="http://maxim.ece.illinois.edu/" target="_blank" rel="noreferrer noopener">Maxim Raginsky </a></strong>from<strong> University of Illinois, Urbana-Champaign</strong> will speak about “Neural SDEs: Deep Generative Models in the Diffusion Limit”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: In deep generative models, the latent variable is generated by a time-inhomogeneous Markov chain, where at each time step we pass the current state through a parametric nonlinear map, such as a feedforward neural net, and add a small independent Gaussian perturbation. In this talk, based on joint work with Belinda Tzen, I will discuss the diffusion limit of such models, where we increase the number of layers while sending the step size and the noise variance to zero. I will first provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. Then I will show how we can quantify the expressiveness of diffusion-based generative models. Specifically, I will prove that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback-Leibler divergence to the target distribution. Finally, I will briefly discuss a scheme for unbiased, finite-variance simulation in such models. This scheme can be implemented as a deep generative model with a random number of layers.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/"><span class="datestr">at October 19, 2021 02:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">Theory Group Postdoc at UC Berkeley (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Postdoc inquiries will be viewable by all our group’s faculty. Individual faculty may then reach out in the case of matched interests. Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line.</p>
<p>Website: <a href="http://theory.cs.berkeley.edu/">http://theory.cs.berkeley.edu/</a><br />
Email: tcs-postdoc-inquiries@lists.eecs.berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/"><span class="datestr">at October 18, 2021 09:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/">Multiple Open Tenure-Track Faculty Positions in Computer Science and Engineering at The Ohio State University (apply by October 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science and Engineering at The Ohio State University invites applications for tenure-track faculty appointments at all ranks and in all research areas. Theory and algorithms is one of the areas with particular emphasis in this faculty search. Review of applications will continue until the positions are filled.</p>
<p>Website: <a href="https://www.cse.ohio-state.edu/faculty-recruiting/tenuredtenure-track-faculty-positions">https://www.cse.ohio-state.edu/faculty-recruiting/tenuredtenure-track-faculty-positions</a><br />
Email: rountev.1@osu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/"><span class="datestr">at October 17, 2021 09:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2010994775517576408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html">Is MATH Ready for P=NP? Is Alexandra Fahrenthold Ready for P=NP?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>(This post was inspired by Harry Lewis emailing me about his granddaughter.)</div><div><br /></div><br /><div style="clear: both; text-align: center;" class="separator"><div style="clear: both; text-align: center;" class="separator"><a style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;" href="https://1.bp.blogspot.com/-EaezFO7EQKg/YWbi3-_1hKI/AAAAAAAB-8o/S76g-orqsg48R9MHQqPf1SZi8M7GFchswCLcBGAsYHQ/s4032/pnp.jpg"><img width="240" src="https://1.bp.blogspot.com/-EaezFO7EQKg/YWbi3-_1hKI/AAAAAAAB-8o/S76g-orqsg48R9MHQqPf1SZi8M7GFchswCLcBGAsYHQ/s320/pnp.jpg" border="0" height="320" /></a></div><a style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-xlbZstxaM_0/YWbibmPgZWI/AAAAAAAB-8Y/TDBhMXf7D2sQ8ZV3M3tlyWYeaJhNzC5dACLcBGAsYHQ/s4032/singing.jpg"><img width="240" src="https://1.bp.blogspot.com/-xlbZstxaM_0/YWbibmPgZWI/AAAAAAAB-8Y/TDBhMXf7D2sQ8ZV3M3tlyWYeaJhNzC5dACLcBGAsYHQ/s320/singing.jpg" border="0" height="320" /></a><br /></div><div style="clear: both; text-align: center;" class="separator"><br /></div><div>Harry Lewis's grand daughter Alexandra Fahrenthold (see both pictures) wants information</div><div>on how to claim the Millennial prize, so she will be ready.</div><div><br /></div><div>This raises the question: How likely is it that Alexandra will resolve P vs NP (or perhaps some other Millennium problem if she wants to rebel against her grandfather)?</div><div><br /></div><div>And more seriously:</div><div><br /></div><div>1) Have we made progress on P vs NP? (I think not.)</div><div>(By <i>we</i>  I mean the community, not <i>Harry and I </i>or <i>Harry and I and Alexandra</i>,</div><div>for which the answer is a more definite NO.)</div><div><br /></div><div>2) If not then why not?</div><div><br /></div><div>3) How does this compare historically to other open problems in Math?</div><div><br /></div><div>We will refer to progress made in solving an open problem, though that is a tricky notion since only after a problem is solved can you look back and say what was progress.  One might also count subcases (e.g., n=4 case of FLT) as progress even if they don't help lead to the final proof. I quote a letter from Harry Lewis to me upon reading a first draft of this post:</div><div><blockquote><i>The one larger point I would suggest adding is to add my operational definition of progress: Progress is being made on a problem if, when the solution is published, it will cite work being published today. Of course that is “operational” only after the fact. Demillo Lipton Perlis at the end have a nice riff on this. The alchemists thought they were making progress on turning lead to gold but they weren’t, even though we know that was actually a solvable problem. Likewise jumping off of higher and higher buildings was not making progress toward heavier than air flight.</i></blockquote></div><div><br /></div><div>---------------------------------------------------------</div><div><br /></div><div><br /></div><div><div>1) Have we made progress on P vs NP?</div><div><br /></div><div>a) I tell my students that we have made progress on ruling out certain techniques.</div><div>They laugh at that, at which point I decide to<i> not </i>tell them that my PhD thesis was about that sort of thing (oracles). I could say</div><div><br /></div><div><i>Once you know what's not going to work you can concentrate one what is going to work.</i></div><div><br /></div><div>But that sounds hollow since very few people are working on techniques that</div><div>might work (The Geometric Complexity Program, see <a href="https://en.wikipedia.org/wiki/Geometric_complexity_theory">here</a>, is the only exception I know of.)</div><div><br /></div><div>b) Are there any partial results? Ryan Williams showed that SAT (and also counting mod versions of it) cannot be done in time n^c and space n^{o(1)} where c is 2cos(2pi/7) (see <a href="https://link.springer.com/content/pdf/10.1007/s00037-008-0248-y.pdf">here</a>).  That is the only concrete lower bound on SAT that I know of.  Is it progress? Sam Buss and Ryan Williams later showed (see <a href="https://link.springer.com/content/pdf/10.1007/s00037-015-0104-9.pdf">here</a>) that, using current techniques, this cannot be improved. If that inspires new techniques that push it further, that would be great. So it is progress? Hard to know now. </div><div><br /></div><div>c) There are some circuit lower bounds. One can debate if this is progress.</div><div>It will be a much better informed debate once the problem is solved.</div><div><br /></div><div>So I would say VERY LITTLE PROGRESS.</div><div><br /></div><div>------------------------------------------------</div><div>2) If not then why not?</div><div><br /></div><div>a) It's only been open for 50 years. A drop in the mathematical bucket.</div><div><i>Counterargument</i>: 50 years of 20th and 21st century mathematics is A LOT.</div><div><br /></div><div>b) Sociology: The academic computer science conference-model induces us to get out a paper in time for the next conference deadline, and not think deeply about a problem.  Carl Smith thought that P vs NP would be solved by the son of a member of the communist party in the USSR (when there was a USSR) who did not have the pressure to get tenure and grants and such. He may be right.</div><div><i>Counterargumen</i>t: there are some (1) mavericks who buck the system, and (2) people like Carl's son-of-a-party-member who are allowed to think deeply for years.</div><div><br /></div><div>c) It's just dang hard! That's the real question. Paul Erdos said of the Collatz Conjecture:</div><div>       <i> Mathematics may not be ready for such problems.</i></div><div>Is that true of P vs NP as well?</div><div><br /></div></div><div><div><br /></div><div>----------------------------------</div><div>3) History and Philosophy.</div><div>(In college I once took the following four courses in one semester: History of Philosophy, Philosophy of History, Philosophy of Philosophy, History of History.)</div><div><br /></div><div>Let's look at problems that were open and then solved:</div><div><br /></div><div>a) The Three Greek Problems of Antiquity: Squaring the circle (given a circle, construct a square with the same area), doubling the cube (given a line that is the edge of cube, construct another line that is the edge of a cube with twice the volume), trisecting an angle (given an angle, construct two lines whose angle is 1/3 of the given angle), with a straightedge and compass. (When I first heard of this problem I wondered how knowing what direction was North would help trisect an angle.) Posed in roughly 400BC. Not clear what <i>posed</i> means in this context. Did the ask for a construction OR did they ask for EITHER a construction OR a proof that there wasn't one?</div><div><br /></div><div>This might be the closest analogy to P vs NP: At the time the problem was stated</div><div> <i>MATHEMATICS WAS NOT READY FOR SUCH PROBLEMS. </i></div><div>It took lots of new math, a better notation, and a different way of looking at numbers, to show that they  could not be done: Pierre Wantzel--doubling the cube (1837),Pierre Wantzel--trisection (1837), Lindemann-Weierstrass--squaring the circle (1882).</div><div>NOTE: Some sources list a fourth problem: constructing every regular polygon. Pierre Watnzel proved, in 1837, that a regular n-gon is constructible iff n is the product of a power of 2 and distinct Fermat  primes. (Why isn't Wantzel better known?) </div><div><br /></div><div>b) Fermat's Last Theorem. Given its origin, not quite clear when it was posed but 1640's seems fair. This could not be solved when it was posed (On an episode of Dr. Who they claim that Fermat had a simple proof. Note that Dr. Who is fictional and their PhD (if they has one) is probably not in mathematics.) </div><div><i>MATHEMATICS WAS NOT READY FOR SUCH PROBLEMS</i>, </div><div>but not as much as the three Greek problems. Very steady progress on it, see  <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/progressflt.pdf">here</a>. One of the real milestone was connecting it to other problems in Math. And then Wiles proved it in the 1990's. While the solution was a surprise when it happened it was not that much of a surprise.</div><div><br /></div><div>QUESTION: Is P vs NP more similar to Greek3 or to FLT? </div><div><br /></div><div>c) Peano Arithmetic (and similar systems) are incomplete. Hilbert's 2nd problem (1900) asked to show the axioms of PA were consistent. Godel (1931) showed this could not be done.  Moreover, there are TRUE statements about numbers that PA cannot prove. I think people mostly thought PA was complete so one of the innovations was to think it was incomplete.  </div><div><i>MATHEMATICS WAS READY FOR SUCH PROBLEMS </i></div><div>but it took the boldness to think PA was incomplete to solve it.  The math needed was known when Hilbert posed the problem. But of course, how to put it together was still quite a challenge.</div></div><div><br /></div><div><div><br /></div><div>d) The Continuum Hypothesis, CH, is that there is no cardinality between N and R. Cantor in 1878 asked for a proof that CH was true. It was Hilbert's first problem in 1900.</div><div>When Hilbert posed this problem in 1900</div><div><i>MATHEMATICS WAS NOT QUITE  READY FOR SUCH PROBLEMS.</i></div><div>The math to solve it wasn't quite there, but wasn't so far off (of course, that's in hindsight). Godel's model L (1940) was brilliant, though Lowenhiem-Skolem had constructed models.  A model of set theory that was defined by levels was, I think, though of by Russell (though in a very diff way than L). When Cohen did a model where CH is false (1963) he invented forcing for Set Theory, though forcing had already been used in Recursion theory (The Kleene-Post construction of intermediary Turing degrees.)</div><div><br /></div><div>e) Hilbert's tenth problem (1900): Find an algorithm that will, given a poly in many variables over Z, determine if it has a solution in Z.</div><div><i>MATHEMATICS WAS ALMOST READY FOR SUCH PROBLEMS.</i></div><div>I turns out that there is no such algorithm. Similar to CH: Once it was thought that it was unsolvable, the proof that it was unsolvable just took a few decades. However, it did need  the definition of computable to be pinned down.  Davis-Putnam-Robinson outlined what was needed in the 1950's,and Matiyasevich finished it in 1970.  While it required just the right combination of ideas, and lots of cleverness, the math needed was known.</div><div>CAVEAT: There are many restrictions of H10 that are still open. My favorite: is the following solvable: given k, does x^3 + y^3 + z^3 = k have a solution in Z? (See my blog post on this problem <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">here</a>.) For a survey of what is known about subcases see (1) my paper <a href="http://export.arxiv.org/pdf/2104.07220">here</a>, though it is has been argued that I am looking at the wrong subcases (see my blog post on this <a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">here</a>), and (2) Bogdan Grechuk's paper <a href="https://arxiv.org/abs/2108.08705">here</a></div><div>CAVEAT: Matiyasevich has suggested that Hilbert really meant to ask about equations and solutions over  Q. That problem is still open. If it is unsolvable, that might be proven reasonably soon. If it is solvable, then</div><div><i>MATHEMATICS IS NOT READY FOR SUCH PROBLEMS.</i></div><div><br /></div><div>f) The four color theorem. Posed in 1852 by Francis Guthrie, proven in 1976. Haken, Appel, and Koch (more on that last name later) did do some very impressive math to set the problem up, and the computer program to finish it off. When the problem was posed (1852) the computing power was not up to the task. So </div><div><i>COMPUTER SCIENCE WAS NOT READY FOR SUCH PROBLEMS.</i></div><div>Could the ideas to set it up have been done earlier? Maybe, but not much earlier. The result is often attributed to Haken and Appel, but actually there are two papers, and Koch is an author on the second one. Note that (1) Robertson, Sanders, Seymour, Thomas had a simpler, though still computer proof (1996), and (2) Werner Gonthier formalized the proof inside the Coq proof assistant in 2005.</div><div>CAVEAT: An open problem that is hard to state precisely is to come up with a non-computer proof.</div></div><div>CAVEAT: There is a non-computer proof that every planar graph is 4.5-colorable, see my blog post in this <a href="https://blog.computationalcomplexity.org/search?q=chromatic+number">here</a>. (No, this is not a joke. If it was I would make if funnier and claim there is a non-computer proof that every planar graph is 4 + 1/e colorable.)</div><div><br /></div><div><div>g) Poincare Conjecture. Conjectured in 1904 and solved in 2002. To bad---if it was solved in 2004 it would be exactly 100 years. There was some progress on this all along so I don't know which step was <i>the hard one </i>though probably they were all hard. This one is harder for me to speculate on. When it was solved and Darling wanted to know why it was worth $1,000,000 I told her that it says <i>if something</i> <i>tastes and smells and feels like a sphere, its a sphere</i>. She was unimpressed.  But back to our story:  in hindsight,</div><div><i>MATH WAS READY FOR SUCH PROBLEMS</i></div><div> since there was steady progress. I think of NOT READY as meaning NO progress, NO plan.</div><div><br /></div><div>h) The Erdos Distance Problem: Show that for any n points in the plane the number of distinct distances is Omega(n/\sqrt{log n}). Not quite solved, but a big milestone was Gutz and Katz proof of Omega(n/log n). For that result</div><div><i>MATH WAS READY FOR SUCH PROBLEMS</i></div><div><i>Steady progress</i>:  see the Wikipedia entry <a href="https://en.wikipedia.org/wiki/Erd%C5%91s_distinct_distances_problem#:~:text=Erd%C5%91s%20distinct%20distances%20problem%20From%20Wikipedia%2C%20the%20free,by%20Larry%20Guth%20and%20Nets%20Katz%20in%202015.">here</a>. What's of interest to us is that there was a barrier result of Omega(n^{8/9}) by Ruzsa (apparently unpublished) that said the techniques being used could not do better-- so people, in short order, found new techniques.  Here is hoping that happens with P vs NP.</div><div><br /></div><div>--------------------------------------------------------------------------------</div><div>Let's look at problems that are open and unsolved.</div><div><br /></div><div>a) Collatz Conjecture (also called the 3x+1 conjecture). I asked</div><div>Jeff Lagarias, who is an expert on the problem:</div><div><br /></div><div><i>Is it true? When will it be resolved?</i> He said <i>Yes</i> and <i>Never</i>.</div><div><br /></div><div>I once heard there has been NO progress on this problem, though I later  heard that Terry Tao has made some progress. In any case, not much progress has been made. Maybe Erdos was right.</div><div><br /></div><div>QUESTION: Why does my spell checker think that<i> Collatz</i> is not a word? </div><div><br /></div><div>b) Small Ramsey Numbers. I asked Stanislaw Radziszowski, who is an expert on Small Ramsey Numbers (he has a dynamic survey on small Ramsey numbers <a href="https://www.combinatorics.org/files/Surveys/ds1/ds1v15-2017.pdf">here</a>) </div><div><br /></div><div><i>What is R(5)?  When will we know? </i>He said <i>43</i> and <i>Never</i>.</div><div><br /></div><div>Worse than being hard, I don't think any nice math has come out of trying to find R(5,5). Too bad. The coloring that gives the lower bound for R(4) and some (perhaps all) of the R(i,j) where i,j\le 4 can be derived from group theory. YEAH! But then connections to interesting math just... stopped. For now? Forever? Joel Spencer told me this is an example of the <i>law</i> <i>of small numbers</i>: patterns that hold for small numbers stop holding when the numbers get too big. (I've seen other things  called <i>the law of small numbers</i> as well.) </div></div><div><div><i>MATH MAY NEVER BE READY FOR SUCH PROBLEMS</i> </div><div>If no interesting math comes out of the attempt to find the exact values of the Ramsey Numbers, then it is not a good problem. </div><div><br /></div><div><i>Note:  </i>The conversations about Collatz and R(5) were within 10 minutes of each other. Depressing day!</div><div><br /></div><div>c) The Twin Primes Conjecture. Sieve methods have been used to get partial result. YEAH! Yitang Zhang showed there exists infinite x such that x and x + 70million (something like that are prime. YEAH. Its been gotten down to x, x+246 and with various assumptions x,x+12 or x, x+6). YEAH! but Sieve methods are known to NOT be able to prove the  conjecture. Dang it!</div><div><i>DO NOT KNOW IF MATH IS READY FOR SUCH PROBLEMS</i>.</div><div>I think people are kind of stuck here. Much like P vs NP, though at least they have some partial results. By contrast, with regard to P vs NP we don't even have that (unless you count Ryan's lower bound on SAT---maybe you do).</div><div><br /></div><div><i>Note</i>: I found that information <a href="https://www.britannica.com/science/twin-prime-conjecture">here</a> which seems to be an Encyclopedia Britannica  website. I would have thought that, with the web and Wikipedia, they would be out of business. Good for them to still be in business! </div><div><br /></div><div>d) I am not qualified to write about any of the Millennium prizes except P vs NP (am I even qualified for that?)  so I ask my readers to leave opinions (informed or not) about, for which of them, </div><div><i>MATH IS NOT READY FOR SUCH PROBLEMS</i></div><div>One of the people who worked on the Riemann Hypothesis said: </div><div><br /></div><div><i>I do not recommend spending half your life on the Riemann Hypothesis. </i></div><div><i><br /></i></div><div>That raises a different question: When do you give up? (topic for a different blog post). </div><div><i><br /></i></div><div>e) I am also not qualified to write about the Hilbert Problems where are still unsolved. Note that some of them are not well enough defined  to ever be resolved (H6: Make Physics rigorous) and some are either solved or unsolved depending on who you ask (H4: Construct ALL metrics where lines are geodesics-- <i>surely,</i> he didn't mean ALL metrics. Probably right, but  stop calling me <i>Shirley</i>!) For a byte more about Hilbert's problems, including a few paragraphs on H4,  see my reviews of two books on them, <a href="http://www.cs.umd.edu/~gasarch/bookrev/44-4.pdf">here</a>. Same as the last item- if you have an opinion (informed or not) about, for which of them that are though to be sort-of open, is math ready for them, leave a comment. </div><div><br /></div><div>CODA: Alexandra will be working on Collatz this summer!</div><div>Let's wish her luck!</div></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html"><span class="datestr">at October 17, 2021 07:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/144">TR21-144 |  Towards Uniform Certification in QBF | 

	Leroy Chew, 

	Friedrich Slivovsky</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We pioneer a new technique that allows us to prove a multitude of previously open simulations in QBF proof complexity. In particular, we show that extended QBF Frege p-simulates clausal proof systems such as IR-Calculus, IRM-Calculus, Long-Distance Q-Resolution, and Merge Resolution.
  These results are obtained by taking a technique of Beyersdorff et al. (JACM 2020) that turns strategy extraction into simulation and combining it with new local strategy extraction arguments.



This approach leads to simulations that are carried out mainly in propositional logic, with minimal use of the QBF rules. Our proofs therefore provide a new, largely propositional interpretation of the simulated systems.
We argue that these results strengthen the case for uniform certification in QBF solving, since many QBF proof systems now fall into place underneath extended QBF Frege.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/144"><span class="datestr">at October 17, 2021 03:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/copriors/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/copriors/">Combining Diverse Feature Priors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2110.08220" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/madrylab/copriors" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br /></p>

<p><i>
    In <a href="https://arxiv.org/abs/2110.08220">our new paper</a>, we take a closer look at the design space of so-called feature priors—i.e., priors that bias the features that a model relies on. We show that models trained with diverse sets of such priors have less overlapping failure modes and can correct each other’s mistakes.
</i></p>

<p>At the core of deep learning’s success is its ability to automatically learn useful features from raw data, e.g., image pixels. Consequently, the set of such learned features has a large impact on the deep learning model’s ability to generalize, especially when the deployment conditions deviate from the training environment. For example, a model that relies heavily on the background of an image may have trouble recognizing a cow on a beach if during training it was mostly encountering cows on grass.</p>

<p>So, how can one control the features that a model relies on? This is often accomplished by changing the model’s architecture or training methodology. For example, by choosing to use a convolutional neural network a model designer biases that model towards learning a hierarchy of spatial features. Similarly, by employing data augmentation during training, one biases the model towards features that are invariant to the particular augmentations used. In fact, researchers have recently explored explicitly manipulating the set of features that a model learns, via, for example, <a href="https://arxiv.org/abs/1811.12231">suppressing texture information</a> by training on stylized inputs or by <a href="https://arxiv.org/abs/1706.06083">training with worst-case input perturbations</a> to <a href="https://arxiv.org/abs/1906.00945">avoid brittle features</a>.</p>

<p>Now, all of these design choices can be thought of as imposing feature priors, i.e., biasing a model towards learning a particular type of features. The question thus becomes: how can we explore and leverage this space of feature priors in a more systematic and purposeful manner?</p>

<h2 id="feature-priors-as-distinct-perspectives-on-data">Feature Priors as Distinct Perspectives on Data</h2>
<p>The core idea is to view different feature priors as distinct perspectives on the data. That is, since different sets of features are likely to generalize differently across inputs, by considering multiple such sets in tandem we can obtain a more holistic (and thus, hopefully, reliable) view on our data.</p>

<p><strong>Setting up our case study:</strong> Let us focus our attention on two concrete feature priors: shape and texture. These two priors arise naturally in the context of image classification and will serve as the basis of our study. We impose these priors through deliberate construction of the dataset and architecture:</p>

<ul>
  <li><strong>Shape-based priors</strong>:  We remove texture information from the images with the help of an edge detection algorithm. We use for this purpose two canonical edge detection algorithms from the computer vision literature: <a href="https://en.wikipedia.org/wiki/Canny_edge_detector">Canny</a> and <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel</a>.</li>
  <li><strong>Texture-based priors</strong>: We use a variant of the <a href="https://arxiv.org/abs/1904.00760">BagNet</a> model, which limits the model’s receptive field to prevent the model from relying on global structures like shape.</li>
</ul>

<div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/original.png" />
            <span style="text-align: center;">Original</span>
        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/sobel.png" />
            <span style="text-align: center;">Sobel</span>

        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/canny.png" />
            <span style="text-align: center;">Canny</span>
        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/bagnet.png" />
            <span style="text-align: center;">BagNet</span>

        </div>
    </div>
</div>

<div class="footnote">
Visualizing different feature priors. Sobel and Canny suppress texture through edge detection while BagNet suppresses shape by limiting the receptive field.
</div>

<p>Intuitively, these priors should correspond to very different sets of features. But are the views offered by these priors truly complementary? A simple way to measure this is to quantify the overlap of the failure modes of the models trained with the respective priors. Specifically, we measure the correlation of predicting the correct label for each pair of such models. We perform this analysis on a subset of CIFAR-10.</p>

<p><img src="https://gradientscience.org/assets/copriors/cifar_corr_table.png" class="bigimg" id="pipeline" /></p>
<div class="footnote"> Correlation of predictions between pairs of models with different priors on a subset of CIFAR-10. The shape-biased and texture-biased models have the least correlated predictions.
</div>

<p>It looks like these results match our intuition! Indeed, models corresponding to the same feature priors (but different random initialization) are relatively well correlated with each other. This also includes the case when we use two <em>different</em> shape biases. On the other hand, when we consider a pairing of a shape-biased model and a texture-biased model the predictions are the least correlated, that is, they make different mistakes on the test set.</p>

<h2 id="combining-feature-priors">Combining feature priors</h2>

<p>Since shape- and texture-biased models make different types of mistakes, can we leverage their diversity to improve our predictions?</p>

<h3 id="ensembles">Ensembles</h3>
<p>A natural way to examine that question is combining these models in an ensemble. This ensemble, for a given test input, evaluates both models on that input and then outputs the one prediction that is assigned the highest probability by the respective model. It turns out that, indeed, such an ensemble is significantly more accurate when we combine in this way models with different priors (as opposed to combining two models trained with the same prior, but with different initializations). Clearly, prediction diversity matters!</p>

<div id="anno"> 
<canvas width="100%" id="ensemble_bar" height="35%"></canvas>
</div>
<div class="footnote"> The maximum accuracy achieved when using a single model, an ensemble with two models with the same prior, and an ensemble with two models with different priors on the CIFAR-10 subset. 
</div>

<h3 id="co-training">Co-Training</h3>
<p>So far, we demonstrated that models with diverse feature priors have less overlapping failure modes, and can be combined via ensembling for improved generalization performance. However, is that all? In particular, are there ways of incorporating prior diversity during training (as opposed to ensembling post hoc) in order to improve the learning process itself?</p>

<p>To answer this question, we focus on <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.664.3543">self-training</a>, a methodology often employed when the labeled data is insufficient to learn a well-generalizing model alone, but a large pool of unlabeled data is available. The key idea in self-training is to use a model trained on the labeled data to produce “pseudo-labels” for the unlabeled data and then use these labels for further training. This setting is particularly challenging since: (a) the (original) labelled data points are typically too few to learn reliable prediction rules, and (b) any incorrect prediction rules learned will be reinforced via pseudo-labelling (so-called <a href="https://arxiv.org/abs/1908.02983">“confirmation bias”</a>).</p>

<p>From this perspective, our goal is to jointly train models with different feature priors to mitigate the propagation of such incorrect prediction rules. We will do so by leveraging the well-established framework of <a href="https://www.cs.cmu.edu/~avrim/Papers/cotrain.pdf">co-training</a>, a framework designed for learning from data with multiple independent sets of features. In the context of our study, we can instantiate this framework as follows.</p>

<p>First, we train one model for each prior using the labeled data. Then, we use each of these models to pseudo-label the unlabelled data and add the examples which are assigned the highest predicted probability to a joint pseudo-labelled data pool. We then use these examples to train both models further and keep repeating this process until we eventually use all the unlabeled data for training. In the end, we combine these models into a single classifier by training a standard model from scratch on the combined pseudo-labels.</p>

<p>The intuition behind this process is that by jointly training models which rely on different features, these models can learn from each other’s predictions. If one model produces incorrect pseudo-labels, we can hope that the other model will correct them by relying on alternative prediction rules.</p>

<p>So, how well does this approach work? To evaluate it, we extract from the CIFAR-10 dataset a small, labeled part (100 examples per class) and treat the rest of this dataset unlabeled. We then compare how different training methods—specifically, self-training a model with a single prior, and co-training models with different priors—perform in this setting. (For an additional baseline, we also consider ensembling two such models with different priors together.)</p>

<div id="anno"> 
<canvas width="100%" id="summary_bar" height="35%"></canvas>
</div>
<div class="footnote"> Test accuracy of models in pre-trained, self-trained, and co-trained settings. We consider: a single model alone, combining multiple models with the same prior, and combining models with diverse priors.  
</div>

<p>Overall, we find that co-training with shape- and texture-based priors can significantly improve the test accuracy of the final model compared to self-training with any of the priors alone. In fact, co-training models with diverse priors also improves significantly upon simply combining self-trained models in an ensemble. So these models are indeed able to take advantage of each other’s predictions during training.</p>

<h2 id="priors-and-spurious-correlations">Priors and Spurious Correlations</h2>
<p>So far, we were focused on a setting where the training and test data were all sourced from the same distribution. However, a major challenge for the real-world model deployment are spurious correlations: associations which are predictive on the training data but not valid for the actual task. For example, an image classification model may predict <a href="https://gradientscience.org/background/">an object’s class based on its location</a>, or <a href="https://www.medrxiv.org/content/10.1101/2020.09.13.20193565v2.full">rely on artifacts of the data collection process</a>.</p>

<p>How can we train models that avoid picking up such spurious correlations? For this problem to be tractable in the first place, we need to rely on some information beyond the training data. Here, we will assume that we have access to an unlabelled dataset where this correlation does not hold (e.g., cows do not always appear on grass and thus the correlation “grass”-&gt;”cow” is not always predictive). This is a rather mild assumption in settings where we can easily collect unlabelled data from a variety of sources—if a correlation is spurious, it is less likely to be uniformly present.</p>

<p>As a concrete example, let us consider a simple gender classification task based on the CelebA dataset. In this task, we will introduce a spurious correlation into the labeled data by only collecting photographs of blond females and non-blond males. This makes hair color a good predictor of gender for the labeled dataset, but will not generalize well beyond that as such correlation does not hold in the real world.</p>

<p>Our goal here will be to harness an unbiased, yet unlabelled dataset, to learn a model that avoids this correlation. We will again attempt to do so by co-training models with diverse feature priors: shape and texture. Notice that since the spurious correlation is color-based, shape-biased models are likely to ignore it. As a result, we anticipate that the prediction of the shape-biased and texture-biased models will differ on inputs where hair color disagrees with gender. Thus, during co-training, these models are intuitively providing each other with counter-examples and are thus likely to steer each other away from incorrect prediction rules.</p>

<div id="anno"> 
<canvas width="100%" id="spurious_bar" height="35%"></canvas>
</div>
<div class="footnote"> Accuracy of models with different feature priors on the (unbiased) CelebA test set. We consider the setting of using only the (biased) labeled data, as well as self-training and co-training using the (unbiased) unlabeled dataset. (For co-training, the combined model is a standard model trained on the combined pseudo-labels of the co-trained Canny and BagNet models.) 
</div>

<p>We find that this is indeed the case! When we co-train a texture-biased model with a shape-biased one, the texture-biased model improves substantially, relying less on the hair color. Moreover, the shape-biased model also improves through co-training. This indicates that even though the texture-biased model relies heavily on the spurious correlation, it also captures non-spurious associations that, through pseudo-labeling, are useful for the shape-based model too.</p>

<h2 id="outlook-exploring-the-design-space-of-priors">Outlook: Exploring the Design Space of Priors</h2>
<p>In this post, we described how models trained with diverse feature priors can be leveraged during training to learn more reliable prediction rules (e.g., in the presence of spurious correlations). However, we view our exploration as only the first step in systematically exploring the design space of feature priors. We believe that this direction will yield an important building block of reliable training and deployment pipelines.</p></div>







<p class="date">
<a href="https://gradientscience.org/copriors/"><span class="datestr">at October 17, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/">The Ideal State Machine Model: Multiple Clients and Linearizability</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduced state machines and state machine replication in an earlier post. In this post, we elaborate on the exact notion of safety and liveness that can be obtained by an ideal state machine when there are multiple clients. First, these definitions highlight the challenges of serving multiple clients. Second,...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/"><span class="datestr">at October 16, 2021 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>I was interested to see a familiar-looking graph drawing as one of the answers to <a href="https://fractalkitty.com/2021/10/02/mathober-2021-begins/">the prompt “multiplicity” for the first entry in Mathober 2021</a> (<a href="https://mathstodon.xyz/@11011110/107030908468774953">\(\mathbb{M}\)</a>). It’s a multigraph formed by a triangle with tripled edges, and looks a lot like <a href="https://commons.wikimedia.org/wiki/File:Multigraph-edge-coloring.svg">the drawing I made</a> for <a href="https://en.wikipedia.org/wiki/Shannon_multigraph">the Wikipedia Shannon multigraph article</a>, prettied up by making an infinitely recessing sequence of these drawings rather than just one. Good choice for multiplicity.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2021/10/03/to-cheer-you-up-in-difficult-times-32-annika-heckels-guest-post-how-does-the-chromatic-number-of-a-random-graph-vary/">Non-concentration of the chromatic number of random graphs</a> (<a href="https://mathstodon.xyz/@11011110/107040518830792599">\(\mathbb{M}\)</a>). Uniformly random graphs, \(G(n,1/2)\) in the Erdős–Rényi–Gilbert model, turn out to have chromatic numbers that, for infinitely many \(n\), are spread over roughly \(\sqrt{n}\) values. But there are weird fluctuations so that, conjecturally, for some \(n\) the concentration is much tighter, more like \(n^{1/4}\).</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/10/04/tenure-under-threat-georgia">University System of Georgia moves to gut tenure</a> (<a href="https://mathstodon.xyz/@11011110/107047579890149970">\(\mathbb{M}\)</a>). The proposed new policy includes procedures for removal of tenure under certain enumerated grounds, including failure to perform their jobs (this is pretty normal) but then adds a massive escape clause in which the board of regents can remove tenured faculty at any time as long as their reason for doing so is not one of the enumerated ones.</p>
  </li>
  <li>
    <p><a href="http://hyperbolic-crochet.blogspot.com/2010/07/story-about-origins-of-model-of.html">The first physical models of the hyperbolic plane, made in 1868 by Beltrami</a> (<a href="https://mathstodon.xyz/@11011110/107052024179327595">\(\mathbb{M}\)</a>, <a href="http://www.open.ac.uk/blogs/is/?p=731">via</a>), blog post by Daina Taimiņa from 2010. Maybe you could make something like this by wrapping and stretching a disk of wet paper in a roll around a pseudosphere (https://en.wikipedia.org/wiki/Pseudosphere)? The rolled-up photo of Beltrami’s model suggests that he did that. The via link shows this as a tangent to a story about triangulated polygons, frieze patterns, and the Farey tessellation.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=QFj-hF8XDQ0">Why do bees make rhombic dodecahedrons</a> (<a href="https://mathstodon.xyz/@11011110/107058516442920053">\(\mathbb{M}\)</a>)? Nice video from Matt Parker (Stand-up Maths) on why bees usually end the cells of their honeycombs with rhombic dodecahedron faces, why this isn’t the optimal solution to fitting two layers of cells together (in terms of minimum wax usage), and why it isn’t reasonable to expect bees to find exact optima for this problem. If I have to quibble with something, though, it’s his plural. It’s not wrong, but see <a href="https://books.google.com/ngrams/graph?content=dodecahedrons%2Cdodecahedra">Google ngrams</a>.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-prove-melting-ice-stays-smooth-20211006/">Mathematicians prove melting ice stays smooth</a> (<a href="https://mathstodon.xyz/@11011110/107064697896988128">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Stefan_problem">see also</a>). The headline is a little overstated: you’re probably familiar with thin necks of ice melting to sharp points at the instant they separate. But these singularities are instantaneous: mathematical models of ice stay smooth for all but a measure-zero set of times. Original paper: “<a href="https://arxiv.org/abs/2103.13379">The singular set in the Stefan problem</a>”, Alessio Figalli, Xavier Ros-Oton, and Joaquim Serra.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@JordiGH/107061930434927745">Discussion of the recent meme telling programmers and mathematicians that summation notation and for loops are the same thing</a>. They’re not quite the same, though: summations don’t have an order of evaluation. But which is easier for people who don’t already know what they are to search and find out about? And why do programmers get angry at non-programming notational conventions?</p>
  </li>
  <li>
    <p><a href="http://eugeniacheng.com/wp-content/uploads/2017/02/cheng-morality.pdf">Mathematics, morally</a> (<a href="https://mathstodon.xyz/@11011110/107078314739030894">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=28816050">via</a>), Eugenia Cheng, 2004. Somehow I hadn’t run across this  before. It argues that much philosophy of mathematics is irrelevant to practice (“You can’t tell from somebody’s mathematics if they are a fictionalist, a rationalist, a platonist, a realist, an operationalist, a logicist, a formalist, structuralist, nominalist, intuitionist.”) and instead considerations of the right way of handling certain topics are more central.</p>
  </li>
  <li>
    <p>The SIGACT Committee for the Advancement of Theoretical Computer Science is collecting information on women in theoretical computer science (<a href="https://mathstodon.xyz/@11011110/107084102339408082">\(\mathbb{M}\)</a>). If this is you, please see <a href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/">their announcement</a> for details of how to be counted.</p>
  </li>
  <li>
    <p><a href="https://pratt.duke.edu/about/news/rudin-squirrel-award">Cynthia Rudin wins major award with silly name</a> (<a href="https://mathstodon.xyz/@11011110/107089557928657710">\(\mathbb{M}\)</a>), for her work on machine learning systems that learn to predict behavior using simple, interpretable, and transparent formulas.</p>
  </li>
  <li>
    <p>According to the <a href="https://www.siam.org/conferences/cm/conference/soda22">SODA web site</a>, SIAM has decided that their conferences will be hybrid through July (<a href="https://mathstodon.xyz/@11011110/107090556997921608">\(\mathbb{M}\)</a>). So if (like me) you wanted to participate in SODA/SOSA/ALENEX/APOCS, but were worried about planning a trip to Virginia with another predicted winter wave of Covid, now you can stay home and conference safely. Or, if you feel hybrid conferences are problematic and organizers should do one or the other but not both, now you have another reason to be annoyed.</p>
  </li>
  <li>
    <p>Rarely is the question asked: <a href="https://blogs.ams.org/beyondreviews/2021/10/14/are-math-papers-getting-longer/">Are math papers getting longer?</a> (<a href="https://mathstodon.xyz/@11011110/107104144047349269">\(\mathbb{M}\)</a>). Following earlier work by Nick Trefethen, Edward Dunne provides some data suggesting that (for certain journals, at least, and not the ones with page limits) the answer is yes. I’m not convinced by the suggested explanation that it’s because they are taking more effort to explain “connections with other work”, though: is that really a big enough fraction of most papers?</p>
  </li>
  <li>
    <p>I haven’t been using my office desktop Mac much because I haven’t been into my office much, so it took me a while to pay attention to the fact that much of its networking had recently broken. <a href="https://eclecticlight.co/2021/09/21/el-capitan-and-older-mac-os-x-are-about-to-have-a-security-certificate-problem/">Here’s why</a> (<a href="https://mathstodon.xyz/@11011110/107107828546240163">\(\mathbb{M}\)</a>). It was still running OS X El Capitan (10.11.6) and a crucial top-level certificate expired. The machine is too old (late 2009) for the latest OS X but it looks like I can and should upgrade to High Sierra, 10.13. So much for getting anything else accomplished today…</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/15/linkage.html"><span class="datestr">at October 15, 2021 03:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7816227323646004961">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html">A Young Person's Game?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>When László Babai first announced his graph isomorphism in quasipolynomial time result, I <a href="https://blog.computationalcomplexity.org/2015/11/a-primer-on-graph-isomorphism.html">wrote</a></p><blockquote><p>We think of theory as a young person's game, most of the big breakthroughs coming from researchers early in their careers. Babai is 65, having just won the Knuth Prize for his lifetime work on interactive proofs, group algorithms and communication complexity. Babai uses his extensive knowledge of combinatorics and group theory to get his algorithm. No young researcher could have had the knowledge base or maturity to be able to put the pieces together the way that Babai did.</p></blockquote><p>Babai's proof is an exceptional story, but it is exceptional. Most CS theorists have done their best work early in their career. I got myself into a <a href="https://twitter.com/fortnow/status/1447681436513882112">twitter discussion</a> on the topic. For me, I'm proud of the research I did through my forties, but I'll always be best known, research wise, for my work on interactive proofs around 1990. It would be hard to run a scientific study to determine cause and effect but here are some reasons, based on my own experiences, on why we don't see research dominated by the senior people in theory.</p><p></p><b>The field changes - </b>Computation complexity has moved from a computational-based discipline to one now dominated by combinatorics, algebra and analysis. I'm not complaining, a field should evolve over time but it plays less to my strengths. It's hard to teach this old dog new tricks.<div><b>The fruit hanged lower - </b>there were important problems with easier proofs available then not available now<div><b>Responsibilities </b>- You have fewer as a PhD student, postdoc or assistant professor.</div><div><b>Family - </b>becomes more of a focus.</div><div><b>Taking on new jobs - </b>Many academics, though not all, take on administrative roles at their university or , or leave academics completely. </div><div><b>The young people have the new ideas </b>- And older people get settled in their ways</div><div><b>The thrill is gone or at least decays - </b>Your first theorem, your first talk, your first conference paper gives you a level of excitement that's hard to match.</div><div><b>Existentialism - </b>The realization that while computing has no doubt changed the world, my research, for the most part, hasn't.</div><div><b>Cognitive Decline</b> - Probably the most controversial but for me I find it hard to focus on problems like I used to. Back in the day I prided myself on knowing all the proofs of my theorems, now I can't even remember the theorems.</div><div><br /></div><div>Honestly there is just nothing wrong with taking on new roles, writing books, surveys and blogs, focusing on teaching and mentorship and service and leaving the great research to the next generation.</div><div><p></p></div></div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html"><span class="datestr">at October 15, 2021 01:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/">Multiple faculty positions in theory of computing at York University (apply by November 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>York University in Toronto, Canada is inviting applications for five tenured or tenure-track positions in the theory of computing or data science (both broadly interpreted). The review of applications will begin on November 15, and full applications are due by November 30.</p>
<p>Website: <a href="https://lassonde.yorku.ca/about/careers/faculty-recruitment">https://lassonde.yorku.ca/about/careers/faculty-recruitment</a><br />
Email: eecsjoin@yorku.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/"><span class="datestr">at October 15, 2021 01:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
