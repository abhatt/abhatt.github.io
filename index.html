<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 05, 2019 08:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/135">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/135">TR19-135 |  Doubly-Efficient Pseudo-Deterministic Proofs | 

	Dhiraj Holden, 

	Shafi Goldwasser, 

	Michel Goemans</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In [20] Goldwasser, Grossman and Holden  introduced pseudo-deterministic interactive proofs for search problems where a powerful prover can convince a probabilistic polynomial time verifier that a solution to a search problem is canonical.  They studied  search problems for which polynomial time algorithms are not known and for which many solutions are possible. They showed that whereas there exists a constant round pseudo deterministic proof for graph isomorphism where the canonical solution is the lexicographically smallest isomorphism, the existence of pseudo-deterministic interactive proofs for NP-hard problems would imply the collapse of the polynomial time hierarchy.

In this paper, we turn our attention to studying  doubly-efficient pseudo-deterministic proofs for polynomial time search problems:  pseudo-deterministic proofs with the extra requirement that the prover runtime is polynomial  and the verifier runtime to verify that a solution is canonical is significantly lower  than the complexity of finding any solution, canonical or otherwise. Naturally this question is particularly interesting for search problems for which a lower bound on its worst case complexity  is known or has been widely conjectured. 

We show doubly-efficient pseudo-deterministic algorithms for a host of natural problems whose complexity has long been conjectured. In particular:

We show a doubly efficient pseudo-deterministic proof for linear programming where the canonical solution which the prover will provide is  the lexicographically greatest optimal solution for the LP. To this end, we show how through perturbing the linear program and strong duality this solution can be both  computed efficiently by the prover, and verified by the verifier.  
The time of the verifier is $O(d^2 )$ for a linear program with integer data and at most $d$ variables and constraints, whereas the time to solve such linear program is $\tilde{O}(d^{\omega} )$ by randomized algorithms [11] for $\omega$  the exponent for fast matrix multiplication .


We show a doubly efficient pseudo-deterministic proof for 3-SUM and problems reducible to 3-SUM where the prover is a $O(n^2)$ time algorithm and the verifier takes time $\tilde{O}(n^{1.5})$. 


We show a doubly-efficient pseudo-deterministic proof for the hitting set problem} where the verifier runs in time $\tilde{O}(m)$ and the prover runs in time $\tilde{O}(m^2)$ where $ m = \sum_{S \in \mathcal{S}} |S| + \sum_{T \in \mathcal{T}} |T|$ for inputs  collections of sets $\mathcal{S}, \mathcal{T}$.

We show a doubly-efficient pseudo-deterministic proof for the Zero Weight Triangle problem where the verifier runs in time $\tilde{O}(n^{2 + \omega/3})$ and the prover runs in randomized time $\tilde{O}(n^3)$. The Zero Weight Triangle problem is equivalent to the All-Pairs Shortest Path problem, a well-studied problem that is the foundation of many hardness results in graph algorithms [39,38], under sub-cubic reductions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/135"><span class="datestr">at October 04, 2019 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/134">TR19-134 |  Finding monotone patterns in sublinear time | 

	Omri Ben-Eliezer, 

	Clement Canonne, 

	Shoham Letzter, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the problem of finding monotone subsequences in an array from the viewpoint of sublinear algorithms. For fixed $k \in \mathbb{N}$ and $\varepsilon &gt; 0$, we show that the non-adaptive query complexity of finding a length-$k$ monotone subsequence of $f \colon [n] \to \mathbb{R}$, assuming that $f$ is $\varepsilon$-far from free of such subsequences, is $\Theta((\log n)^{\lfloor \log_2 k \rfloor})$. Prior to our work, the best algorithm for this problem, due to Newman, Rabinovich, Rajendraprasad, and Sohler (2017), made $(\log n)^{O(k^2)}$ non-adaptive queries; and the only lower bound known, of $\Omega(\log n)$ queries for the case $k = 2$, followed from that on testing monotonicity due to Erg\"un, Kannan, Kumar, Rubinfeld, and Viswanathan (2000) and Fischer (2004).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/134"><span class="datestr">at October 04, 2019 03:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/">Postdoc at University of Warwick, UK (apply by December 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for a postdoc position hosted by Tom Gur at the University of Warwick, United Kingdom, in the fields of sublinear-time algorithms, coding theory, interactive and probabilistically checkable proofs, and complexity theory.</p>
<p>Please contact Tom Gur directly with your CV. The start date is flexible.</p>
<p>Website: <a href="https://www.dcs.warwick.ac.uk/~tomgur/">https://www.dcs.warwick.ac.uk/~tomgur/</a><br />
Email: tom.gur@warwick.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/"><span class="datestr">at October 04, 2019 01:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3425">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/10/04/highlights-beyond-ec-call-for-nominations/">Highlights Beyond EC – Call for Nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="http://ec20.sigecom.org/">21st ACM Conference on Economics and Computation</a> (EC’20) will host a special <strong>plenary session</strong> highlighting some of the best work in economics and computation that appears in conferences and journals other than EC, or mature working papers. The intention of this session is to expose EC attendees to related work just beyond the boundary of their current awareness. We seek nominations for papers in Economics and Computation that have made breakthrough advances, opened up new horizons for research, made interesting connections between different scientific areas, or had significant impact on practice. Examples of relevant conferences and journals include STOC/FOCS/SODA/ITCS, AAAI/IJCAI/AAMAS, NIPS/ICML/COLT, WWW/KDD, AER/Econometrica/JPE/QJE/RESTUD/TE/AEJ Micro/JET/GEB, and Math of OR/Management Science/Operations Research.</p>
<p><strong>Who can nominate? </strong>This call is open to everyone (self-nominations are also allowed), but we particularly encourage members of PCs or editorial boards in various venues to submit nominations.</p>
<p><strong>Deadline:</strong> December 23, 2019.</p>
<p><strong>Nomination format:</strong> Nominations should be emailed to <a href="mailto:HighlightsBeyondEC@gmail.com">HighlightsBeyondEC2020@gmail.com</a>, and should include:</p>
<ul>
<li>Paper title and author names.</li>
<li>Publication venue or online working version. Preference will be given to papers that have appeared in a related conference or journal within the past two years, or have a working version circulated within the past two years.</li>
<li>A short (2-3 paragraph) justification letter, explaining the significance of the paper.</li>
<li>Names of 1-3 experts on the area of the paper.</li>
</ul>
<p><strong>Committee members:</strong></p>
<ul>
<li><strong>Michal Feldman </strong>(Tel Aviv University)</li>
<li><strong>Hervé Moulin</strong> (University of Glasgow)</li>
<li><strong>Michael Wellman </strong>(University of Michigan)</li>
<li><strong>Adam Wierman </strong>(California Institute of Technology)</li>
</ul></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2019/10/04/highlights-beyond-ec-call-for-nominations/"><span class="datestr">at October 04, 2019 06:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1192">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1192">News for Sept 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><s>Five</s> Six papers this month: results on testing separations, linearity testing in \(\mathbb{R}^n\), testing for regular languages, graph property testing, topological property testing, and Boolean rank. </p>



<p><strong>Hard properties with (very) short PCPPs and their applications</strong>, by Omri Ben-Eliezer, Eldar Fischer, Amit Levi, and Ron D. Rothblum (<a href="https://eccc.weizmann.ac.il/report/2019/088/">arXiv</a>). Probably, the most significant takeaway from this work is a (largest possible) separation between standard and tolerant property testing. PCPPs (Probabilistically Checkable Proofs of Proximity) are the “NP” variant of property testing, where the tester is aided by a proof string. Consider property \(\mathcal{P}\). If \(x \in \mathcal{P}\), there must be a proof string that makes the tester accept (with probability 1). If \(x\) is far from \(\mathcal{P}\) (in the usual property testing sense), for any proof string, the tester must reject with sufficiently high probability. PCPPs have played a role in the classical constructions of PCPs, but have also found uses in getting a better understanding of property testing itself. And this paper shows how PCPP constructions can be used to get property testing separations. The main result in this paper is a property \(\mathcal{P}\) that (basically) requires \(\Omega(n)\) queries to “property test”, but has a PCPP system where the proof length is \(\widetilde{O}(n)\). (\(n\) is the input length.) The main construction uses collections of random linear codes. Significantly, these constructions show a strong separation between standard vs tolerant property testing, and standard vs erasure-resilient property testing. (The latter is a recent variant by <a href="https://epubs.siam.org/doi/abs/10.1137/16M1075661?journalCode=smjcat">Dixit et al</a>, where certain parts of the input are hidden from the tester.) There is a property that is testable in a constant number of queries, but requires \(\widetilde{\Omega}(n)\) queries to test tolerantly (for any non-trivial choice of parameters). An analogous result holds for erasure-resilient testing.</p>



<p><strong>Distribution-Free Testing of Linear Functions on R^n</strong>, by Noah Fleming and Yuichi Yoshida (<a href="https://arxiv.org/abs/1909.03391">arXiv</a>). Linearity testing is arguably <em>the</em> canonical problem in property testing, yet there is still much to be learned about it. This paper considers functions \(f: \mathbb{R}^n \to \mathbb{R}\), and the <em>distribution-free setting</em>. (In this setting, distance is measured according is an unknown distribution \(\mathcal{D}\) over the input, and the tester can access samples from this distribution. For \(\mathbb{R}^n\), the “standard” distribution would the \(n\)-dimensional Gaussian.) The main result is that linearity testing can be done in the distribution-free setting with \(\widetilde{O}(1/\varepsilon)\) queries, assuming that the distribution is continuous. The primary technical tool, an interesting result in its own right, is that additivity \((f(x+y) = f(x) + f(y))\) can be tested in \(\widetilde{O}(1/\varepsilon)\) queries. The significance of the testing result is cemented by an \(\Omega(n)\) lower bound for sample-based testers.</p>



<p><strong>Sliding window property testing for regular languages</strong> by Moses Ganardi, Danny Hucke, Markus Lohrey, Tatiana Starikovskaya (<a href="https://arxiv.org/abs/1909.10261">arXiv</a>). Fix a regular language \(\mathcal{R}\). Consider the streaming model, and the basic question of recognizing whether the string (being streamed) is in \(\mathcal{R}\). Simple, you will say! Run the DFA recognizing \(\mathcal{R}\) in constant space. Now, suppose there is a sliding window length of \(n\). The aim is to determine if the past \(n\) symbols (the “active window”) form a string in \(\mathcal{R}\). Suprisingly (at least to me), there is a full characterization of the space required for randomized algorithms, and (depending on \(\mathcal{R}\)), it is either \(\Theta(1)\), \(\Theta(\log\log n)\), \(\Theta(\log n)\), or \(\Theta(n)\).  In the interest of beating these lower bounds, suppose we wish to property test on the active window. It turns out the answer is quite nuanced. There are deterministic \(O(\log n)\)-space testers and randomized two-sided \(O(1/\varepsilon)\)-space testers for all regular languages. For randomized one-sided testers, there are multiple possibilities for the optimal space complexity, and there is a full characterization of these regular languages.</p>



<p><strong>A characterization of graph properties testable for general planar graphs with one-sided error (It’s all about forbidden subgraphs)</strong> by Artur Czumaj and Christian Sohler (<a href="https://arxiv.org/pdf/1909.10647.pdf">arXiv</a>). Property testing of sparse graphs has been receiving more attention, but most results focus on the bounded degree setting. Unfortunately, many of these results break quite dramatically on sparse graphs with unbounded degrees. This paper focuses on property testing, within the class of unbounded degree planar graphs. (Meaning, the input is always assumed to be planar.) The results achieve a significant goal: as the title suggests, there is a complete characterization of properties that are constant-query testable with one-sided error. The easier part is in showing that all such properties can be reduced to testing \(H\)-freeness. The harder (remarkable) result is \(H\)-freeness can be tested in general planar queries with constant queries. (This is non-trivial even for triangle-freeness.) And, as is easy to conjecture but hard to prove, these results carry over for all minor-closed families.  As a small indication of the challenge, most testers for bounded-degree graphs work by doing constant depth BFSes. When high degree vertices are present, this method fails, and we really need new ideas to deal with such graphs.</p>



<p><strong>Near Coverings and Cosystolic Expansion – an example of topological property testing</strong> by Irit Dinur and Roy Meshulam (<a href="https://eccc.weizmann.ac.il/report/2019/126/">ECCC</a>). In most algebraic settings, property testing results can be seen as local to global theorems. When do local constraints on a large object imply a global condition? This paper gives a topological instantiation of this phenomenon. We need to define the <em>cover</em> of a simplicial complex \(X\). For concreteness, think of a 2D simplicial complex \(X\), which is a hypergraph with hyperedges of size at most 3, where subsets of hyperedges are also present. A 2-cover is a simplicial complex \(X’\) with the following property. It has two copies of each vertex of \(X\). Each hyperedge of \(X\) must have two “corresponding” disjoint copies in \(X’\). Let the copies of vertex \(v\) be \(v_0, v_1\). Then, for every hyperedge (say) \((u,v,w)\) of \(X\), there must be two disjoint hyperedges in \(X’\) involving copies of the corresponding vertices. One can consider the property testing twist: if the neighborhoods of “most” vertices \(v\) in \(X\) satisfy these condition (with respect to the neighborhoods of the copies of \(v\) in \(X’\)), then is \(X’\) close to being a cover of \(X\)? Indeed, this paper proves that such a “property testing condition” holds iff \(X\) is a high-dimensional expander.</p>



<p><strong>Property testing of the Boolean and binary rank</strong> by Michal Parnas, Dana Ron, and Adi Shraibman (<a href="https://arxiv.org/abs/1908.11632">arXiv</a>). The Boolean rank of a matrix \(M\) is a fundamental quantity that appears in many lower bound constructions. (Recall that an \(n \times m\) Boolean matrix \(M\) has a rank \(r\) if \(M\) can be expressed as \(X \cdot Y\), where \(X \in \mathbb{F}_2^{n \times d}\) and \(Y \in \mathbb{F}_2^{d \times m}\).) In the real-valued setting, results show that one can property test rank in \(poly(d/\varepsilon)\) queries. This paper proves an analogous result for the Boolean rank. There is a surprise element here: over reals, the rank can be computed in polynomial time, and many of the geometric intuitions can be brought over to the property testing problem. On the other hand, the Boolean rank is NP-hard to compute exactly, yet we can still get a tester with \(poly(d)\) query complexity. The paper also gives results for <em>binary rank</em>. For the binary rank, we require the component matrices \(X, Y\) to be Boolean, but algebraic operations are over the reals. In the case, the tester has query complexity \(2^{2d}\) (with varying dependencies on \(\varepsilon\) for adaptive/non-adaptive testers). The intriguing open problem is whether \(poly(d)\)-query testers exist for binary rank.</p>



<p> </p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1192"><span class="datestr">at October 04, 2019 05:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01565">On partisan bias in redistricting: computational complexity meets the science of gerrymandering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Tanima.html">Tanima Chatterjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/DasGupta:Bhaskar.html">Bhaskar DasGupta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01565">PDF</a><br /><b>Abstract: </b>The topic of this paper is "gerrymandering", namely the curse of deliberate
creations of district maps with highly asymmetric electoral outcomes to
disenfranchise voters, and it has a long legal history. Measuring and
eliminating gerrymandering has enormous implications to sustain the backbone of
democratic principles of a society. Although there is no dearth of legal briefs
involving gerrymandering over many years, it is only more recently that
mathematicians and applied computational researchers have started to
investigate this topic. However, it has received relatively little attention so
far from the computational complexity researchers dealing with theoretical
analysis of computational complexity issues, such as computational hardness,
approximability issues, etc. There could be many reasons for this, such as
descriptions of these problem non-CS non-math (often legal or political)
journals that theoretical CS (TCS) people usually do not follow, or the lack of
coverage of these topics in TCS publication venues. One of our modest goals in
writing this article is to improve upon this situation by stimulating further
interactions between the gerrymandering and TCS researchers. To this effect,
our main contributions are twofold: (1) we provide formalization of several
models, related concepts, and corresponding problem statements using TCS
frameworks from the descriptions of these problems as available in existing
non-TCS (perhaps legal) venues, and (2) we also provide computational
complexity analysis of some versions of these problems, leaving other versions
for future research.
</p>
<p>The goal of writing this article is not to have the final word on
gerrymandering, but to introduce a series of concepts, models and problems to
the TCS community and to show that science of gerrymandering involves an
intriguing set of partitioning problems involving geometric and combinatorial
optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01565"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01552">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01552">Extensions of the Algorithmic Lovasz Local Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolmogorov:Vladimir.html">Vladimir Kolmogorov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01552">PDF</a><br /><b>Abstract: </b>We consider recent formulations of the algorithmic Lovasz Local Lemma by
Achlioptas-Iliopoulos-Kolmogorov [2] and by Achlioptas-Iliopoulos-Sinclair [3].
These papers analyze a random walk algorithm for finding objects that avoid
undesired "bad events" (or "flaws"), and prove that under certain conditions
the algorithm is guaranteed to find a "flawless" object quickly. We show that
conditions proposed in these papers are incomparable, and introduce a new
family of conditions that includes those in [2, 3] as special cases.
</p>
<p>Secondly, we extend our previous notion of "commutativity" in [15] to this
more general setting, and prove that it allows to use an arbitrary strategy for
selecting the next flaw to address. In the special case of primary flaws we
prove a stronger property: the flaw selection strategy does not affect at all
the expected number of steps until termination, and also does not affect the
distribution induced by the algorithm upon termination. This applies, in
particular, to the single-clause backtracking algorithm for constraint
satisfaction problems considered in [3].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01552"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01492">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01492">A Grid-based Approach for Convexity Analysis of a Density-based Cluster</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naghavi=Nozad:Sayyed=Ahmad.html">Sayyed-Ahmad Naghavi-Nozad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banaei:Seyed=Mojtaba.html">Seyed-Mojtaba Banaei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saberi:Mohsen.html">Mohsen Saberi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01492">PDF</a><br /><b>Abstract: </b>This paper presents a novel geometrical approach to investigate the convexity
of a density-based cluster. Our approach is grid-based and we are about to
calibrate the value space of the cluster. However, the cluster objects are
coming from an infinite distribution, their number is finite, and thus, the
regarding shape will not be sharp. Therefore, we establish the precision of the
grid properly in a way that, the reliable approximate boundaries of the cluster
are founded. After that, regarding the simple notion of convex sets and
midpoint convexity, we investigate whether or not the density-based cluster is
convex. Moreover, our experiments on synthetic datasets demonstrate the
desirable performance of our method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01492"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01357">Recognizing the Tractability in Big Data Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Xiangyu.html">Xiangyu Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jianzhong.html">Jianzhong Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miao:Dongjing.html">Dongjing Miao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xianmin.html">Xianmin Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01357">PDF</a><br /><b>Abstract: </b>Due to the limitation on computational power of existing computers, the
polynomial time does not works for identifying the tractable problems in big
data computing. This paper adopts the sublinear time as the new tractable
standard to recognize the tractability in big data computing, and the
random-access Turing machine is used as the computational model to characterize
the problems that are tractable on big data. First, two pure-tractable classes
are first proposed. One is the class $\mathrm{PL}$ consisting of the problems
that can be solved in polylogarithmic time by a RATM. The another one is the
class $\mathrm{ST}$ including all the problems that can be solved in sublinear
time by a RATM. The structure of the two pure-tractable classes is deeply
investigated and they are proved $\mathrm{PL^i} \subsetneq \mathrm{PL^{i+1}}$
and $\mathrm{PL} \subsetneq \mathrm{ST}$. Then, two pseudo-tractable classes,
$\mathrm{PTR}$ and $\mathrm{PTE}$, are proposed. $\mathrm{PTR}$ consists of all
the problems that can solved by a RATM in sublinear time after a PTIME
preprocessing by reducing the size of input dataset. $\mathrm{PTE}$ includes
all the problems that can solved by a RATM in sublinear time after a PTIME
preprocessing by extending the size of input dataset. The relations among the
two pseudo-tractable classes and other complexity classes are investigated and
they are proved that $\mathrm{PT} \subseteq \mathrm{P}$, $\sqcap'\mathrm{T^0_Q}
\subsetneq \mathrm{PTR^0_Q}$ and $\mathrm{PT_P} = \mathrm{P}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01357"><span class="datestr">at October 04, 2019 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01331">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01331">Optimal Joint Subcarrier and Power Allocation in NOMA is Strongly NP-Hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salaun:Lou.html">Lou Salaun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chung_Shue.html">Chung Shue Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coupechoux:Marceau.html">Marceau Coupechoux</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01331">PDF</a><br /><b>Abstract: </b>Non-orthogonal multiple access (NOMA) is a promising radio access technology
for 5G. It allows several users to transmit on the same frequency and time
resource by performing power-domain multiplexing. At the receiver side,
successive interference cancellation (SIC) is applied to mitigate interference
among the multiplexed signals. In this way, NOMA can outperform orthogonal
multiple access schemes used in conventional cellular networks in terms of
spectral efficiency and allows more simultaneous users. This paper investigates
the computational complexity of joint subcarrier and power allocation problems
in multi-carrier NOMA systems. We prove that these problems are strongly
NP-hard for a large class of objective functions, namely the weighted
generalized means of the individual data rates. This class covers the popular
weighted sum-rate, proportional fairness, harmonic mean and max-min fairness
utilities. Our results show that the optimal power and subcarrier allocation
cannot be computed in polynomial time in the general case, unless P = NP.
Nevertheless, we present some tractable special cases and we show that they can
be solved efficiently.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01331"><span class="datestr">at October 04, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01327">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01327">Privately detecting changes in unknown distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cummings:Rachel.html">Rachel Cummings</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krehbiel:Sara.html">Sara Krehbiel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lut:Yuliia.html">Yuliia Lut</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Wanrong.html">Wanrong Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01327">PDF</a><br /><b>Abstract: </b>The change-point detection problem seeks to identify distributional changes
in streams of data. Increasingly, tools for change-point detection are applied
in settings where data may be highly sensitive and formal privacy guarantees
are required, such as identifying disease outbreaks based on hospital records,
or IoT devices detecting activity within a home. Differential privacy has
emerged as a powerful technique for enabling data analysis while preventing
information leakage about individuals. Much of the prior work on change-point
detection (including the only private algorithms for this problem) requires
complete knowledge of the pre-change and post-change distributions. However,
this assumption is not realistic for many practical applications of interest.
This work develops differentially private algorithms for solving the
change-point problem when the data distributions are unknown. Additionally, the
data may be sampled from distributions that change smoothly over time, rather
than fixed pre-change and post-change distributions. We apply our algorithms to
detect changes in the linear trends of such data streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01327"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01317">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01317">Orbit Computation for Atomically Generated Subgroups of Isometries of $\mathbb{Z}^n$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Haizi.html">Haizi Yu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mineyev:Igor.html">Igor Mineyev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Varshney:Lav_R=.html">Lav R. Varshney</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01317">PDF</a><br /><b>Abstract: </b>Isometries and their induced symmetries are ubiquitous in the world. Taking a
computational perspective, this paper considers isometries of $\mathbb{Z}^n$
(since values are discrete in digital computers), and tackles the problem of
orbit computation under various isometry subgroup actions on $\mathbb{Z}^n$.
Rather than just conceptually, we aim for a practical algorithm that can
partition any finite subset of $\mathbb{Z}^n$ based on the orbit relation. In
this paper, instead of all subgroups of isometries, we focus on a special class
of subgroups, namely atomically generated subgroups. This newly introduced
notion is key to inheriting the semidirect-product structure from the whole
group of isometries, and in turn, the semidirect-product structure is key to
our proposed algorithm for efficient orbit computation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01317"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01296">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01296">Best-first Search Algorithm for Non-convex Sparse Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakaue:Shinsaku.html">Shinsaku Sakaue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marumo:Naoki.html">Naoki Marumo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01296">PDF</a><br /><b>Abstract: </b>Non-convex sparse minimization (NSM), or $\ell_0$-constrained minimization of
convex loss functions, is an important optimization problem that has many
machine learning applications. NSM is generally NP-hard, and so to exactly
solve NSM is almost impossible in polynomial time. As regards the case of
quadratic objective functions, exact algorithms based on quadratic
mixed-integer programming (MIP) have been studied, but no existing exact
methods can handle more general objective functions including Huber and
logistic losses; this is unfortunate since those functions are prevalent in
practice. In this paper, we consider NSM with $\ell_2$-regularized convex
objective functions and develop an algorithm by leveraging the efficiency of
best-first search (BFS). Our BFS can compute solutions with objective errors at
most $\Delta\ge0$, where $\Delta$ is a controllable hyper-parameter that
balances the trade-off between the guarantee of objective errors and
computation cost. Experiments demonstrate that our BFS is useful for solving
moderate-size NSM instances with non-quadratic objectives and that BFS is also
faster than the MIP-based method when applied to quadratic objectives.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01296"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01293">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01293">A Fast Exponential Time Algorithm for Max Hamming Distance X3SAT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoi:Gordon.html">Gordon Hoi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Sanjay.html">Sanjay Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephan:Frank.html">Frank Stephan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01293">PDF</a><br /><b>Abstract: </b>X3SAT is the problem of whether one can satisfy a given set of clauses with
up to three literals such that in every clause, exactly one literal is true and
the others are false. A related question is to determine the maximal Hamming
distance between two solutions of the instance. Dahll\"of provided an algorithm
for Maximum Hamming Distance XSAT, which is more complicated than the same
problem for X3SAT, with a runtime of $O(1.8348^n)$; Fu, Zhou and Yin considered
Maximum Hamming Distance for X3SAT and found for this problem an algorithm with
runtime $O(1.6760^n)$. In this paper, we propose an algorithm in $O(1.3298^n)$
time to solve the Max Hamming Distance X3SAT problem; the algorithm actually
counts for each $k$ the number of pairs of solutions which have Hamming
Distance $k$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01293"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01251">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01251">Search problems in algebraic complexity, GCT, and hardness of generator for invariant rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makam:Visu.html">Visu Makam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Rafael.html">Rafael Oliveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01251">PDF</a><br /><b>Abstract: </b>We consider the problem of outputting succinct encodings of lists of
generators for invariant rings. Mulmuley conjectured that there are always
polynomial sized such encodings for all invariant rings. We provide simple
examples that disprove this conjecture (under standard complexity assumptions).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01251"><span class="datestr">at October 04, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01147">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01147">Path and Ancestor Queries on Trees with Multidimensional Weight Vectors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Meng.html">Meng He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kazi:Serikzhan.html">Serikzhan Kazi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01147">PDF</a><br /><b>Abstract: </b>We consider an ordinal tree $T$ on $n$ nodes, with each node assigned a
</p>
<p>$d$-dimensional weight vector $\pnt{w} \in \{1,2,\ldots,n\}^d,$ where $d \in
\mathbb{N}$ is a constant.
</p>
<p>We study path queries as generalizations of well-known {\textit{orthogonal
range queries}}, with one of the dimensions being tree topology rather than a
linear order. Since in our definitions $d$ only represents the number of
dimensions of the weight vector without taking the tree topology into account,
a path query in a tree with $d$-dimensional weight vectors generalize the
corresponding $(d+1)$-dimensional orthogonal range query.
</p>
<p>We solve {\textit{ancestor dominance reporting}} problem as a direct
generalization of dominance reporting problem, %in time $\O((\lg^{d-1}
n)/(\lg\lg n)^{d-2}+k)$ in time $\O(\lg^{d-1}{n}+k)$ %and space of $\O(n(\lg
n)^{d-1}/(\lg \lg n)^{d-2})$ words, and space of $\O(n\lg^{d-2}n)$ words, where
$k$ is the size of the output, for $d \geq 2.$
</p>
<p>We also achieve a tradeoff of $\O(n\lg^{d-2+\eps}{n})$ words of space, with
query time of $\O((\lg^{d-1} n)/(\lg\lg n)^{d-2}+k),$ for the same problem,
when $d \geq 3.$
</p>
<p>We solve {\textit{path successor problem}} in $\O(n\lg^{d-1}{n})$ words of
space and time $\O(\lg^{d-1+\eps}{n})$ for $d \geq 1$ and an arbitrary constant
$\eps &gt; 0.$ We propose a solution to {\textit{path counting problem}}, with
$\O(n(\lg{n}/\lg\lg{n})^{d-1})$ words of space and $\O((\lg{n}/\lg\lg{n})^{d})$
query time, for $d \geq 1.$
</p>
<p>Finally, we solve {\textit{path reporting problem}} in
$\O(n\lg^{d-1+\eps}{n})$ words of space and
$\O((\lg^{d-1}{n})/(\lg\lg{n})^{d-2}+k)$ query time, for $d \geq 2.$
</p>
<p>These results match or nearly match the best tradeoffs of the respective
range queries. We are also the first to solve path successor even for $d = 1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01147"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3615744836127152440">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html">Quantum Supremacy: A Guest Post by Abhinav Deshpande</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I am delighted to introduce you to Abhinav Deshpande, who is a graduate student at the University of Maryland, studying Quantum Computing. This will be a guest post on the rumors of the recent Google breakthrough on Quantum Supremacy. For other blog posts on this exciting rumor, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott Aaronson's post</a>, <a href="https://www.scottaaronson.com/blog/?p=4342">Scott Aaronson's second post on it</a>, <a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">John Preskill's quanta article</a>, <a href="https://blog.computationalcomplexity.org/2019/09/quantum-supremacy.html">Fortnow's post</a>,<br />
and there may be others.<br />
<br />
Guest post by Abhinav:<br />
<br />
I (Abhinav) thank Bill Fefferman for help with this post, and Bill Gasarch for inviting me to do a guest post.<br />
<br />
<br />
<b>The quest towards quantum computational supremacy</b><br />
<br />
September saw some huge news in the area of quantum computing, with rumours that the Google AI Lab has achieved a milestone known as 'quantum computational supremacy', also termed 'quantum supremacy' or 'quantum advantage' by some authors. Today, we examine what this term means, the most promising approach towards achieving this milestone, and the best complexity-theoretic evidence we have so far against classical simulability of quantum mechanics. We will not be commenting on details of the purported paper since there is no official announcement or claim from the authors so far.<br />
<br />
<b>What it means</b><br />
<br />
First off, the field of quantum computational supremacy arose from trying to formally understand the differences in the power of classical and quantum computers. A complexity theorist would view this goal as trying to give evidence to separate the complexity classes BPP and BQP. However, it turns out that one can gain more traction from considering the sampling analogues of these classes, SampBPP and SampBQP.  These are classes of distributions that can be efficiently sampled on classical and quantum computers, respectively. Given a quantum circuit U on n qubits, one may define an associated probability distribution over 2^n outcomes as follows: apply U to the fiducial initial state |000...0&gt; and measure the resulting state in the computational basis. This produces a distribution D_U.<br />
<br />
A suitable way to define the task of simulating the quantum circuit is as follows<b style="font-style: italic;">:</b><br />
<br />
Input: Description of a quantum circuit U acting on n qubits.<br />
<br />
Output: A sample from the probability distribution D_U obtained by measuring U|000...0&gt; in the computational basis.<br />
<br />
One of the early works in this field was that of <a href="https://arxiv.org/abs/quant-ph/0205133">Terhal and DiVincenzo</a>, which first considered the complexity of sampling from a distribution (weak simulation) as opposed to that of calculating the exact probability of a certain outcome (strong simulation). Weak simulation is arguably the more natural notion of simulating a quantum system, since in general, we cannot feasibly compute the probability of a certain outcome even if we can simulate the quantum circuit. Subsequent works by <a href="https://arxiv.org/abs/1011.3245">Aaronson and Arkhipov</a>, and by <a href="https://arxiv.org/abs/1005.1407">Bremner, Jozsa, and Shepherd</a> established that if there is a classically efficient weak simulator for different classes of quantum circuits, the polynomial hierarchy collapses to the third level.<br />
<br />
<br />
So far, we have only considered the question of exactly sampling from the distribution D_U. However, any realistic experiment is necessarily noisy, and a more natural problem is to sample from a distribution that is not exactly D_U but from any distribution D_O that is ε-close in a suitable distance measure, say the variation distance.<br />
<br />
The aforementioned work by Aaronson and Arkhipov was the first to consider this problem, and they made progress towards showing that a special class of quantum circuits (linear optical circuits) is classically hard to approximately simulate in the sense above. The task of sampling from the output of linear optical circuits is known as boson sampling. At the   time, it was the best available way to show that quantum computers  may solve some problems that are far beyond the reach of classical computers.<br />
<br />
Even granting that the PH doesn't collapse, one still needs to make an additional conjecture to establish that boson sampling is not classically simulable.  The conjecture is that additively approximating the output probabilities of a random linear optical quantum circuit is #P-hard.  The reason this may be true is that output probabilities of random linear optical quantum circuits are Permanents of a Gaussian random matrix, and the Permanent is as hard to compute on a random matrix as it is on a worst-case matrix. Therefore, the only missing link is to go from average-case hardness of exact computation to average-case hardness of an additive estimation. In addition, if we make a second conjecture known as the "anti-concentration" conjecture, we can show that this additive estimation is non-trivial: it suffices to give us a good multiplicative estimation with high probability.<br />
<br />
So that's what quantum computational supremacy is about: we have a computational task that is efficiently solvable with quantum computers, but which would collapse the polynomial hierarchy if done by a classical computer (assuming certain other conjectures are true). One may substitute "collapse of the polynomial hierarchy" with stronger conjectures and incur a corresponding tradeoff in the likelihood of the conjecture being true.<br />
<br />
<b>Random circuit sampling</b><br />
<br />
In 2016,<a href="https://arxiv.org/abs/1608.00263"> Boixo et al</a>. proposed to replace the class of quantum circuits for which some hardness results were known (commuting circuits and boson sampling) by random circuits of sufficient depth on a 2D grid of qubits having nearest-neighbour interactions. Concretely, the proposed experiment would be to apply random unitaries from a specified set on n qubits arranged on a 2D grid for sufficient depth, and then sample from the resulting distribution. The two-qubit unitaries in the set are restricted to act between nearest neighbours, respecting the geometric This task is called random circuit sampling (RCS).<br />
<br />
At the time, the level of evidence for the hardness of this scheme was not yet the same as the linear optical scheme. However, given the theoretical and experimental interest in the idea of demonstrating a quantum speedup over classical computers, subsequent works by<a href="https://arxiv.org/abs/1803.04402"> Bouland, Fefferman, Nirkhe and Vazirani</a>, and <a href="https://arxiv.org/abs/1809.06957">Harrow and Mehraban</a> bridged this gap (the relevant work by <a href="https://arxiv.org/abs/1612.05903">Aaronson and Chen</a> will be discussed in the following section). Harrow and Mehraban proved anticoncentration for random circuits. In particular, they showed that a 2-dimensional grid of n qubits achieve anticoncentration in depth O(\sqrt{n}), improving upon earlier results with higher depth due to <a href="https://arxiv.org/abs/1208.0692">Brandao, Harrow and Horodeck</a>i. Bouland et al. proved the same supporting evidence for RCS as that for boson sampling, namely a worst-to-average-case reduction for exactly computing most output probabilities, even without the permanent structure possessed by linear optical quantum circuits.<br />
<br />
<b>Verification</b><br />
<br />
So far, we have not discussed the elephant in the room: of verifying that the output distribution supported on 2^n outcomes. It turns out that there are concrete lower bounds such as those due to Valiant and Valiant, showing that verifying whether an empirical distribution is close to a target distribution is impossible if one has few samples.<br />
<br />
Boixo et al. proposed a way of certifying the fidelity of the purported simulation. Their key observation was to note that if their experimental system is well modelled by a noise model called global depolarising noise, estimating the output fidelity is possible with relatively few outcomes. Under global depolarising noise with fidelity f, the noisy distribution takes the form D_N = f D_U + (1-f) I, where I is the uniform distribution over the 2^n outcomes. Together with another empirical observation about the statistics of output probabilities of the ideal distribution D_U, they argued that computing the following cross-entropy score would serve as a good estimator of the fidelity:<br />
<br />
f ~ H(I, D_U) - H(D_exp, D_U), where H(D_A,D_B) is the cross-entropy between the two distributions: H(D_A, D_B) = -\sum_i p_A log (p_B).<br />
<br />
The proposal here was to experimentally collect several samples from D_exp, classically compute using brute-force the probabilities of these outcomes in the distribution D_U, and estimate the cross-entropy using this information. If the test outputs a high score for a computation on sufficiently many qubits and depth, the claim is that quantum supremacy has been achieved.<br />
<br />
Aaronson and Chen gave alternative form of evidence for the hardness of scoring well on a test that aims to certify quantum supremacy similar to the manner above. This sidesteps the issue of whether a test similar to the one above does indeed certify the fidelity. The specific problem considered was "Heavy Output Generation" (HOG), the problem of outputting strings that have higher than median probability in the output distribution. Aaronson and Chen linked the hardness of HOG to a closely related problem called "QUATH", and conjectured that QUATH is hard for classical computers.<br />
<br />
<b>Open questions</b><br />
<br />
Assuming the Google team has performed the impressive feat of both running the experiment outlined before and classically computing the probabilities of the relevant outcomes to see a high score on their cross-entropy test, I discuss the remaining positions a skeptic might take regarding the claim about quantum supremacy.<br />
<br />
"The current evidence of classical hardness of random circuit sampling is not sufficient to conclude that the task is hard". Assuming that the skeptic believes that the polynomial hierarchy does not collapse, a remaining possibility is that there is no worst-to-average-case reduction for the problem of *approximating* most output probabilities, which kills the proof technique of Aaronson and Arkhipov to show hardness of approximate sampling.<br />
<br />
"The cross-entropy proposal does not certify the fidelity." Boixo et al. gave numerical evidence and other arguments for this statement, based on the observation that the noise is of the global depolarising form. A skeptic may argue that the assumption of global depolarising noise is a strong one.<br />
<br />
"The QUATH problem is not classically hard." In order to give evidence for the hardness of QUATH, Aaronson and Chen examined the best existing algorithms for this problem and also gave a new algorithm that nevertheless do not solve QUATH with the required parameters.<br />
<br />
It would be great if the community could work towards strengthening the evidence we already have for this task to be hard, either phrased as a sampling experiment or together with the verification test.<br />
<br />
Finally, I think this is an exciting time for quantum computing and to witness this landmark event. It may not be the first probe of an experiment that is "hard" to classically simulate, since there are many quantum experiments that are beyond the reach of current classical simulations, but the inherent programmability and control present in the experimental system is what enables the tools of complexity theory to be applied to the problem. A thought that fascinates me is the idea that we may be exploring quantum mechanics in a regime never probed this carefully before, the "high complexity regime" of quantum mechanics. One imagines there are important lessons in physics here.<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html"><span class="datestr">at October 03, 2019 07:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=18199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/">Noisy quantum circuits: how do we know that we have robust experimental outcomes at all? (And do we care?)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">a recent post we discussed Google’s claim of achieving “quantum supremacy” and my reasons to think that these claims will not stand.</a> (See also <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/#comment-61043">this comment</a> for necessary requirements from a quantum supremacy experiment.) This debate gives a good opportunity to discuss some conceptual issues regarding sampling, probability distributions, statistics, and computational complexity. This time we will discuss <span style="color: #ff0000;">chaotic behavior vs. robust experimental outcomes.</span></p>
<p>On unrelated matter, I just heard Shachar Lovett’s very beautiful TCS+ lecture on the sunflower conjecture (<a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">see this post</a> on the Alweiss, Lovett, Wu, and Zhang’s breakthrough). You can see the lecture and many others on the <a href="https://www.youtube.com/user/TCSplusSeminars/videos">TCS+ you tube channel</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png"><img src="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png?w=640&amp;h=449" alt="" width="640" class="alignnone size-full wp-image-18240" height="449" /></a></p>
<p style="text-align: center;"><span style="color: #ff0000;"><span style="color: #993366;">Slide 30 from my August, ’19 CERN lecture: predictions of near-term experiments. (Here is the</span> <a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">full powerpoint presentation</a><span style="color: #993366;">.) In this post we mainly</span> <strong>discuss</strong> <strong>point b) about chaotic behavior. </strong><span style="color: #800080;">See also <a href="https://arxiv.org/abs/1908.02499">my paper: The argument against quantum computers</a>.</span></span></p>
<p>Consider an experiment aimed for establishing quantum supremacy: your quantum computer produced a sample <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> which is a 0-1 string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> from a certain distribution <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />. The research assumption is that <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />  is close enough to a fixed distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> (<img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> accounts for the computing process and the noise) which is very hard to be demonstrated on a classical computer. By looking at a large number of samples you can perform a statistical test on the samples to verify that they were (approximately) sampled from <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, or at least that they were sampled from a probability distribution that is very hard to be computed on a classical computer!</p>
<p>But, is it possible that all the distributions <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />‘s are very different? Namely that each sample is taken from a completely different distribution? More formally, is it possible  that under a correct modeling of the device for two different samples <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_j" class="latex" title="x_j" />, <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" /> has a very small correlation with <img src="https://s0.wp.com/latex.php?latex=D_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_j" class="latex" title="D_j" />? In this case we say that the experiment outcomes are <strong>not robust</strong> and that the situation is <strong>chaotic</strong>.</p>
<p>Here are a couple of questions that I propose to think about:</p>
<ul>
<li>How do we test robustness?</li>
<li>Do the supremacy experiments require that the experiment is robust?</li>
<li>If, after many samples, you reach a probability distribution that require exponential time on a classical computer should you worry about the question whether the experiment is robust?</li>
<li><span style="color: #0000ff;">Do the 10,000,000 samples for the Google 53-qubit experiment represent a robust sampling experiment?</span></li>
</ul>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/"><span class="datestr">at October 03, 2019 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/">Faculty position at University of Minnesota-Twin Cities (apply by November 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science &amp; Engineering at the University of Minnesota-Twin Cities is hiring to fill multiple tenure-track positions at the assistant professor level, although higher levels of appointments may be considered when commensurate with experience and accomplishments. One of the areas of interest is theoretical computer science.</p>
<p>Website: <a href="https://www.cs.umn.edu/news/cse-now-hiring-new-faculty">https://www.cs.umn.edu/news/cse-now-hiring-new-faculty</a><br />
Email: csciadmin@umn.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/"><span class="datestr">at October 03, 2019 06:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/">Faculty at Virginia Tech (apply by December 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="http://careers.pageuppeople.com/968/cw/en-us/job/510994">http://careers.pageuppeople.com/968/cw/en-us/job/510994</a></p>
<p>Website: <a href="http://www.cs.vt.edu/">http://www.cs.vt.edu/</a><br />
Email: facdev@cs.vt.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/"><span class="datestr">at October 03, 2019 02:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/10/03/NTK/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/10/03/NTK/">Ultra-Wide Deep Nets and Neural Tangent Kernel (NTK)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(Crossposted <a href="https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/">at CMU ML</a>.)</p>

<p>Traditional wisdom in machine learning holds that there is a careful trade-off between training error and generalization gap. There is a “sweet spot” for the model complexity such that the model (i) is big enough to achieve reasonably good training error, and (ii) is small enough so that the generalization gap - the difference between test error and training error - can be controlled. A smaller model would give a larger training error, while making the model bigger would result in a larger generalization gap, both leading to larger test errors. This is described by the classical U-shaped curve for the test error when the model complexity varies (see Figure 1(a)).</p>

<p>However, it is common nowadays to use highly complex over-parameterized models like deep neural networks. These models are usually trained to achieve near zero error on the training data, and yet they still have remarkable performance on test data. <a href="https://arxiv.org/abs/1812.11118">Belkin et al. (2018)</a> characterized this phenomenon by a “double descent” curve which extends the classical U-shaped curve. It was observed that, as one increases the model complexity past the point where it can perfectly fits the training data (i.e., <em>interpolation</em> regime is reached), test error continues to drop! Interestingly, the best test error is often achieved by the largest model, which goes against the classical intuition about the “sweet spot.” The following figure from <a href="https://arxiv.org/abs/1812.11118">Belkin et al. (2018)</a> illustrates this phenomenon.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/belkinfig.jpg" style="width: 700px;" />
<br />
<b>Figure 1.</b> Effect of increased model complexity on generalization: traditional belief vs actual practice. 
</div>
<p><br /></p>

<p>Consequently one suspects that the training algorithms used in deep learning - (stochastic) gradient descent and its variants - somehow implicitly constrain the complexity of trained networks (i.e., “true number” of parameters), thus leading to a small generalization gap.</p>

<p>Since larger models often give better performance in practice, one may naturally wonder:</p>

<blockquote>
  <p>How does an infinitely wide net perform?</p>
</blockquote>

<p>The answer to this question corresponds to the right end of Figure 1(b). This blog post is about a model that has attracted a lot of attention in the past year:  deep learning in the regime where the width - namely, the number of channels in convolutional filters, or the number of neurons in fully-connected internal layers - goes to infinity. At first glance this approach may seem hopeless for both practitioners and theorists: all the computing power in the world is insufficient to train an infinite network, and theorists already have their hands full trying to figure out finite ones. But in math/physics there is a tradition of deriving insights into questions by studying them in the infinite limit, and indeed here too the infinite limit becomes easier for theory.</p>

<p>Experts may recall the connection between infinitely wide neural networks and kernel methods from 25 years ago by <a href="https://www.cs.toronto.edu/~radford/pin.abstract.html">Neal (1994)</a> as well as the recent extensions by <a href="https://openreview.net/forum?id=B1EA-M-0Z">Lee et al. (2018)</a> and <a href="https://arxiv.org/abs/1804.11271">Matthews et al. (2018)</a>. These kernels correspond to infinitely wide deep networks whose all parameters are chosen randomly, and <em>only the top (classification) layer is trained</em> by gradient descent. Specifically, if $f(\theta,x)$ denotes the output of the network on input $x$ where $\theta$ denotes the parameters in the network, and $\mathcal{W}$ is an initialization distribution over $\theta$ (usually Gaussian with proper scaling), then the corresponding kernel is

where $x,x’$ are two inputs.</p>

<p>What about the more usual scenario when <em>all layers are trained</em>? Recently, <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a> first observed that this is also related to a kernel named <em>neural tangent kernel (NTK)</em>, which has the form
</p>

<p>The key difference between the NTK and previously proposed kernels is that the NTK is defined through the inner product between the gradients of the network outputs with respect to the network parameters. This gradient arises from the use of the gradient descent algorithm. Roughly speaking, the following conclusion can be made for a sufficiently wide deep neural network trained by gradient descent:</p>

<blockquote>
  <p>A properly randomly initialized <strong>sufficiently wide</strong> deep neural network <strong>trained by gradient descent</strong> with infinitesimal step size (a.k.a. gradient flow) is <strong>equivalent to a kernel regression predictor</strong> with a <strong>deterministic</strong> kernel called <em>neural tangent kernel (NTK)</em>.</p>
</blockquote>

<p>This was more or less established in the original paper of <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a>, but they required the width of every layer to go to infinity in a sequential order. In <a href="https://arxiv.org/abs/1904.11955">our recent paper</a> with Sanjeev Arora, Zhiyuan Li, Ruslan Salakhutdinov and Ruosong Wang, we improve this result to the non-asymptotic setting where the width of every layer only needs to be greater than a certain finite threshold.</p>

<p>In the rest of this post we will first explain how NTK arises and the idea behind the proof of the equivalence between wide neural networks and NTKs. Then we will present experimental results showing how well infinitely wide neural networks perform in practice.</p>

<h2 id="how-does-neural-tangent-kernel-arise">How Does Neural Tangent Kernel Arise?</h2>

<p>Now we describe how training an ultra-wide fully-connected neural network leads to kernel regression with respect to the NTK. A more detailed treatment is given in <a href="https://arxiv.org/abs/1904.11955">our paper</a>. We first specify our setup. We consider the standard supervised learning setting, in which we are given $n$ training data points ${(x_i,y_i)}_{i=1}^n \subset \mathbb{R}^{d}\times\mathbb{R}$ drawn from some underlying distribution and wish to find a function that given the input $x$ predicts the label $y$ well on the data distribution. We consider a fully-connected neural network defined by $f(\theta, x)$, where $\theta$ is the collection of all the parameters in the network and $x$ is the input. For simplicity we only consider neural network with a single output, i.e., $f(\theta, x) \in \mathbb{R}$, but the generalization to multiple outputs is straightforward.</p>

<p>We consider training the neural network by minimizing the quadratic loss over training data:

Gradient descent with infinitesimally small learning rate (a.k.a. gradient flow) is applied on this loss function $\ell(\theta)$:                                                                               
where $\theta(t)$ denotes the parameters at time $t$.</p>

<p>Let us define some useful notation. Denote $u_i = f(\theta, x_i)$, which is the network’s output on $x_i$. We let $u=(u_1, \ldots, u_n)^\top \in \mathbb{R}^n$ be the collection of the network outputs on all training inputs. We use the time index $t$ for all variables that depend on time, e.g. $u_i(t), u(t)$, etc. With this notation the training objective can be conveniently written as $\ell(\theta) = \frac12 |u-y|_2^2$.</p>

<p>Using simple differentiation, one can obtain the dynamics of $u(t)$ as follows: (see <a href="https://arxiv.org/abs/1904.11955">our paper</a> for a proof)​

where $H(t)$ is an $n\times n$ positive semidefinite matrix whose $(i, j)$-th entry is $\left\langle \frac{\partial f(\theta(t), x_i)}{\partial\theta}, \frac{\partial f(\theta(t), x_j)}{\partial\theta} \right\rangle$.</p>

<p>Note that $H(t)$ is the <em>kernel matrix</em> of the following (time-varying) kernel $ker_t(\cdot,\cdot)$ evaluated on the training data:

In this kernel an input $x$ is mapped to a feature vector $\phi_t(x) = \frac{\partial f(\theta(t), x)}{\partial\theta}$ defined through the gradient of the network output with respect to the parameters at time $t$.</p>

<p>###The Large Width Limit</p>

<p>Up to this point we haven’t used the property that the neural network is very wide. The formula for the evolution of $u(t)$ is valid in general. In the large width limit, it turns out that the time-varying kernel $ker_t(\cdot,\cdot)$ is (with high probability) always close to a <em>deterministic</em> fixed kernel $ker_{\mathsf{NTK}}(\cdot,\cdot)$, which is the <strong>neural tangent kernel (NTK)</strong>. This property is proved in two steps, both requiring the large width assumption:</p>

<ol>
  <li>
    <p><strong>Step 1: Convergence to the NTK at random initialization.</strong> Suppose that the network parameters at initialization ($t=0$), $\theta(0)$, are i.i.d. Gaussian. Then under proper scaling, for any pair of inputs $x, x’$, it can be shown that the random variable $ker_0(x,x’)$, which depends on the random initialization $\theta(0)$, converges in probability to the deterministic value $ker_{\mathsf{NTK}}(x,x’)$, in the large width limit.</p>

    <p>(Technically speaking, there is a subtlety about how to define the large width limit. <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a> gave a proof for the sequential limit where the width of every layer goes to infinity one by one. Later <a href="https://arxiv.org/abs/1902.04760">Yang (2019)</a> considered a setting where all widths go to infinity at the same rate. <a href="https://arxiv.org/abs/1904.11955">Our paper</a> improves them to the non-asymptotic setting, where we only require all layer widths to be larger than a finite threshold, which is the weakest notion of limit.)</p>
  </li>
  <li>
    <p><strong>Step 2: Stability of the kernel during training.</strong> Furthermore, the kernel <em>barely changes</em> during training, i.e., $ker_t(x,x’) \approx ker_0(x,x’)$ for all $t$. The reason behind this is that the weights do not move much during training, namely $\frac{|\theta(t) - \theta(0)|}{|\theta(0)|} \to 0$ as width $\to\infty$. Intuitively, when the network is sufficiently wide, each individual weight only needs to move a tiny amount in order to have a non-negligible change in the network output. This turns out to be true when the network is trained by gradient descent.</p>
  </li>
</ol>

<p>Combining the above two steps, we conclude that for any two inputs $x, x’$, with high probability we have

As we have seen, the dynamics of gradient descent is closely related to the time-varying kernel $ker_t(\cdot,\cdot)$. Now that we know that $ker_t(\cdot,\cdot)$ is essentially the same as the NTK, with a few more steps, we can eventually establish the equivalence between trained neural network and NTK: the final learned neural network at time $t=\infty$, denoted by $f_{\mathsf{NN}}(x) = f(\theta(\infty), x)$, is equivalent to the <em>kernel regression</em> solution with respect to the NTK. Namely, for any input $x$ we have

where $ker_{\mathsf{NTK}}(x, X) = (ker_{\mathsf{NTK}}(x, x_1), \ldots, ker_{\mathsf{NTK}}(x, x_n))^\top \in \mathbb{R}^n$, and $ker_{\mathsf{NTK}}(X, X) $ is an $n\times n$ matrix whose $(i, j)$-th entry is $ker_{\mathsf{NTK}}(x_i, x_j)$.</p>

<p>(In order to not have a bias term in the kernel regression solution we also assume that the network output at initialization is small: $f(\theta(0), x)\approx0$; this can be ensured by e.g. scaling down the initialization magnitude by a large constant, or replicating a network with opposite signs on the top layer at initialization.)</p>

<h2 id="how-well-do-infinitely-wide-neural-networks-perform-in-practice">How Well Do Infinitely Wide Neural Networks Perform in Practice?</h2>

<p>Having established this equivalence, we can now address the question of how well infinitely wide neural networks perform in practice — we can just evaluate the kernel regression predictors using the NTKs! We test NTKs on a standard image classification dataset, CIFAR-10. Note that for image datasets, one needs to use convolutional neural networks (CNNs) to achieve good performance. Therefore, we derive an extension of NTK, <em>convolutional neural tangent kernels (CNTKs)</em> and test their performance on CIFAR-10. In the table below, we report the classification accuracies of different CNNs and CNTKs:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/cntk_acc.jpeg" style="width: 700px;" />
<br />
</div>

<p>Here CNN-Vs are vanilla practically-wide CNNs (without pooling), and CNTK-Vs are their NTK counterparts. We also test CNNs with global average pooling (GAP), denotes above as CNN-GAPs, and their NTK counterparts, CNTK-GAPs. For all experiments, we turn off batch normalization, data augmentation, etc., and only use SGD to train CNNs (for CNTKs, we use the closed-form formula of kernel regression).</p>

<p>We find that CNTKs are actually very power kernels. The best kernel we find, 11-layer CNTK with GAP, achieves 77.43% classification accuracy on CIFAR-10. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher than methods reported by <a href="https://openreview.net/forum?id=B1g30j0qF7">Novak et al. (2019)</a>. The CNTKs also perform similarly to their CNN counterparts. This means that ultra-wide CNNs can achieve reasonable test performance on CIFAR-10.</p>

<p>It is also interesting to see that the global average pooling operation can significantly increase the classification accuracy for both CNNs and CNTKs. From this observation, we suspect that many techniques that improve the performance of neural networks are in some sense universal, i.e., these techniques might benefit kernel methods as well.</p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>

<p>Understanding the surprisingly good performance of over-parameterized deep neural networks is definitely a challenging theoretical question. Now, at least we have a better understanding of a class of ultra-wide neural networks: they are captured by neural tangent kernels! A hurdle that remains is that the classic generalization theory for kernels is still incapable of giving realistic bounds for generalization. But at least we now know that better understanding of kernels can lead to better understanding of deep nets.</p>

<p>Another fruitful direction is to “translate” different architectures/tricks of neural networks to kernels and to check their practical performance. We have found that global average pooling can significantly boost the performance of kernels, so we hope other tricks like batch normalization, dropout, max-pooling, etc. can also benefit kernels. Similarly, one can try to translate other architectures like recurrent neural networks, graph neural networks, and transformers, to kernels as well.</p>

<p>Our study also shows that there is a performance gap between infinitely wide networks and finite ones. How to explain this gap is an important theoretical question.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/10/03/NTK/"><span class="datestr">at October 03, 2019 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4476874159248982333">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2019/10/harvard-admissions-lawsuit-decision-out.html">Harvard Admissions Lawsuit Decision Out</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As someone who reads a significant number of court documents and decisions (I still do expert witness work), I can recommend for your reading pleasure the <a href="https://admissionscase.harvard.edu/files/adm-case/files/2019-10-30_dkt_672_findings_of_fact_and_conclusions_of_law.pdf">very recent decision on the Harvard admissions case</a>.  For those who want a sense of how Harvard admissions works, you will get a good summary of the information that came out during the trial.  For those who want to see a well-written court decision, in my opinion, this is a good example.  (Whether you agree with the decision or not, you should find the decision well written;  it lays out the issues and challenges in determining the decision clearly, and similarly explains the reasons for the ultimate conclusion clearly.)  And for those who care about the actual underlying issues of discrimination and affirmative action, I think the document provides a lot of food for thought, with a depth beyond what you'll see in the  news coverage.</div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2019/10/harvard-admissions-lawsuit-decision-out.html"><span class="datestr">at October 03, 2019 05:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4342">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4342">From quantum supremacy to classical fallacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Maybe I should hope that people <em>never</em> learn to distinguish for themselves which claimed breakthroughs in building new forms of computation are obviously serious, and which ones are obviously silly.  For as long as they don’t, this blog will always serve at least one purpose.  People will cite it, tweet it, invoke its “authority,” even while from my point of view, I’m offering nothing more intellectually special than my toddler does when he calls out “moo-moo cow! baa-baa sheep!” as we pass them on the road.</p>



<p>But that’s too pessimistic.  Sure, most readers <em>must</em> more-or-less already know what I’ll say about each thing: that <a href="https://www.scottaaronson.com/blog/?p=4317">Google’s quantum supremacy claim</a> is serious, that <a href="https://www.scottaaronson.com/blog/?p=2212">memcomputing to solve NP-complete problems</a> is not, etc.  Even so, I’ve heard from many readers that this blog was at least helpful for double-checking their initial impressions, and for making <a href="https://www.scottaaronson.com/blog/?p=2410">common knowledge</a> what before had merely been known to many.  I’m fine for it to continue serving those roles.</p>



<p>Last week, even as I dealt with fallout from Google’s quantum supremacy leak, I also got several people asking me to comment on a <em>Nature</em> paper entitled <a href="https://www-nature-com.ezproxy.lib.utexas.edu/articles/s41586-019-1557-9">Integer factorization using stochastic magnetic tunnel junctions</a> (warning: paywalled).  See also <a href="https://www.purdue.edu/newsroom/releases/2019/Q3/poor-mans-qubit-can-solve-quantum-problems-without-going-quantum.html">here</a> for a university press release.</p>



<p>The authors report building a new kind of computer based on asynchronously updated “p-bits” (probabilistic bits).  A p-bit is “a robust, classical entity fluctuating in time between 0 and 1, which interacts with other p-bits … using principles inspired by neural networks.”  They build a device with 8 p-bits, and use it to factor integers up to 945.  They present this as another “unconventional computation scheme” alongside quantum computing, and as a “potentially scalable hardware approach to the difficult problems of optimization and sampling.”</p>



<p>A <a href="https://www.nature.com/articles/d41586-019-02742-x">commentary accompanying the </a><em><a href="https://www.nature.com/articles/d41586-019-02742-x">Nature</a></em><a href="https://www.nature.com/articles/d41586-019-02742-x"> paper</a> goes much further still—claiming that the new factoring approach, “if improved, could threaten data encryption,” and that resources should now be diverted from quantum computing to this promising new idea, one with the advantages of requiring no refrigeration or maintenance of delicate entangled states.  (It should’ve added: and how big a number has Shor’s algorithm factored anyway, 21?  Compared to 945, that’s peanuts!)</p>



<p>Since I couldn’t figure out a gentler way to say this, here goes: it’s <strong>astounding</strong> that this paper and commentary made it into <em>Nature</em> in the form that they did.  Juxtaposing Google’s sampling achievement with p-bits, as several of my Facebook friends did last week, is juxtaposing the Wright brothers with some guy bouncing around on a pogo stick.</p>



<p>If you were looking forward to watching me dismantle the p-bit claims, I’m afraid you might be disappointed: the task is over almost the moment it begins.  <strong>“p-bit” devices can’t scalably outperform classical computers, for the simple reason that they <font color="red">are</font> classical computers.</strong>  A little unusual in their architecture, but still well-covered by the classical <a href="https://www.scottaaronson.com/talks/bernays2.ppt">Extended Church-Turing Thesis</a>.  Just like with the <a href="https://en.wikipedia.org/wiki/Adiabatic_quantum_computation">quantum adiabatic algorithm</a>, an energy penalty is applied to coax the p-bits into running a local optimization algorithm: that is, making random local moves that preferentially decrease the number of violated constraints.  Except here, because the whole evolution is classical, there doesn’t seem to be even the <em>pretense</em> that anything is happening that a laptop with a random-number generator couldn’t straightforwardly simulate.  In terms of <a href="https://www.nytimes.com/2019/10/02/opinion/impeachment-trump-nixon.html">this editorial</a>, if adiabatic quantum computing is Richard Nixon—hiding its lack of observed speedups behind subtle arguments about tunneling and spectral gaps—then p-bit computing is Trump.</p>



<p>Even so, I wouldn’t be writing this post if you opened the paper and it immediately said, in effect, “look, <em>we know</em>.  You’re thinking that this is just yet another stochastic local optimization method, which could clearly be simulated efficiently on a conventional computer, thereby putting it into a different conceptual universe from quantum computing.  You’re thinking that factoring an n-bit integer will self-evidently take exp(n) time by this method, as compared to exp(n<sup>1/3</sup>) for the <a href="https://en.wikipedia.org/wiki/General_number_field_sieve">Number Field Sieve</a>, and that no crypto is in even remote danger from this.  But here’s why you should still be interested in our p-bit model: because of other advantages X, Y, and Z.”  Alas, in vain one searches the whole paper, <em>and</em> the lengthy supplementary material, <em>and</em> the commentary, for any acknowledgment of the pachyderm in the pagoda.  Not an asymptotic runtime scaling in sight.  Quantum computing is there, but stripped of the theoretical framework that gives it its purpose.</p>



<p>That silence, in the pages of <em>Nature</em>—<em>that’s</em> the part that convinced me that, while on the negative side this blog seems to have accomplished nothing for the world in 14 years of existence, on the positive side it will likely have a role for decades to come.</p>



<p><strong>Update:</strong> See a <a href="https://www.scottaaronson.com/blog/?p=4342#comment-1820670">response in the comments</a>, which I appreciated, from Kerem Cansari (one of the authors of the paper), and <a href="https://www.scottaaronson.com/blog/?p=4342#comment-1820674">my response to the response</a>.</p>



<p><strong>(Partly) Unrelated Announcement #1:</strong> My new postdoc, <a href="https://andrearocchetto.github.io/">Andrea Rocchetto</a>, had the neat idea of compiling a <a href="https://quantumfactsheet.github.io/">Quantum Computing Fact Sheet</a>: a quick “Cliffs Notes” for journalists, policymakers, and others looking to get the basics right.  The fact sheet might grow in the future, but in the meantime, check it out!  Or at a more popular level, try the <a href="https://quantumatlas.umd.edu/">Quantum Atlas</a> made by folks at the University of Maryland.</p>



<p><strong>Unrelated Announcement #2:</strong> Daniel Wichs asked me to give a shout-out to a new <a href="https://itcrypto.github.io/">Conference on Information-Theoretic Cryptography</a>, to be held June 17-19 in Boston.</p>



<p><strong>Third Announcement:</strong> Several friends asked me to share that <a href="https://peterwittek.com/">Prof. Peter Wittek</a>, quantum computing researcher at the University of Toronto, has <a href="https://www.theglobeandmail.com/canada/article-renowned-ai-expert-university-of-toronto-prof-missing-after-avalanche/?fbclid=IwAR0FTnzQxRL79-oo43xjKaNEA7Oe1rA8A2yVjvhrgodxG1wJzhfhJZt9oJw">gone missing</a> in the Himalayas.  Needless to say we hope for his safe return.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4342"><span class="datestr">at October 03, 2019 03:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1288">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2019/10/02/a-solicitation-for-tcs-job-market-profiles/">A solicitation for TCS job market profiles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>CATCS is piloting an effort this year to collect and disseminate profiles of junior theory researchers who are going on the job market during the 2019-20 academic year, complementing the job postings collected under the <a href="https://cstheory-jobs.org/">Jobs tab</a>. The SIGecom community has run a similar effort very successfully for a number of years and we are following their lead. The goals are two-fold:</p>
<div></div>
<ul>
<li>Provide a platform to job-seekers to advertise their credentials.</li>
<li>Provide an interface for institutions/individuals with open positions to find prospective candidates.</li>
</ul>
<p>Candidates looking for theory jobs can fill out <a href="https://forms.gle/gMDaChCqQocKSXT79" target="_blank" rel="noopener">this form</a>. The form asks for basic personal information, thesis title, graduation date (past or future), research/teaching interests, bibliographic information for three publications, and allows you to add links to publications and a brief CV.</p>
<div></div>
<p>The responses will be reviewed and, if approved, edited and posted on Theory Matters starting in Nov’19. There is no deadline, but for responses received after Nov 1 please allow two weeks for review before your profile appears on the website. Responses received by Dec 15 will have summaries published in the following issue of SIGACT News.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2019/10/02/a-solicitation-for-tcs-job-market-profiles/"><span class="datestr">at October 02, 2019 09:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/133">TR19-133 |  More on $AC^0[\oplus]$ and Variants of the Majority Function | 

	Utkarsh Tripathi, 

	Nutan Limaye, 

	Srikanth Srinivasan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this paper we prove two results about $AC^0[\oplus]$ circuits. 

We show that for $d(N) = o(\sqrt{\log N/\log \log N})$ and $N \leq s(N) \leq 2^{dN^{1/d^2}}$ there is an explicit family of functions $\{f_N:\{0,1\}^N\rightarrow \{0,1\}\}$ such that 
$f_N$ has uniform $AC^0$ formulas of depth $d$ and size at most $s$; 
$f_N$ does not have $AC^0[\oplus]$ formulas of depth $d$ and size $s^{\varepsilon}$, where $\varepsilon$ is a fixed absolute constant. 

This gives a quantitative improvement on the recent result of Limaye, Srinivasan, Sreenivasaiah, Tripathi, and Venkitesh, (STOC, 2019), which proved a similar Fixed-Depth Size-Hierarchy theorem but for $d \ll \log \log N$ and $s \ll \exp(N^{1/2^{\Omega(d)}})$. 

As in the previous result, we use the Coin Problem to prove our hierarchy theorem. Our main technical result is the construction of uniform size-optimal formulas for solving the coin problem with improved sample complexity $(1/\delta)^{d+4}$ (down from $(1/\delta)^{2^{O(d)}}$ in the previous result).

In our second result, we show that randomness buys depth in the $AC^0[\oplus]$ setting. Formally, we show that for any fixed constant $d\geq 2$, there is a family of Boolean functions that has polynomial-sized randomized uniform $AC^0$ circuits of depth $d$ but no polynomial-sized (deterministic) $AC^0[\oplus]$ circuits of depth $d$.

Previously Viola (Computational Complexity, 2014) showed that an increase in depth (by at least $2$) is essential to avoid superpolynomial blow-up while derandomizing randomized $AC^0$ circuits. We show that an increase in depth (by at least $1$) is essential even for $AC^0[\oplus]$. 

As in Viola's result, the separating examples are promise variants of the Majority function on $N$ inputs that accept inputs of weight at least $N/2 + N/(\log N)^{d-1}$ and reject inputs of weight at most $N/2 - N/(\log N)^{d-1}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/133"><span class="datestr">at October 02, 2019 11:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/">QuICS Fellows at Joint Center for Quantum Information and Computer Science (apply by October 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Joint Center for Quantum Information and Computer Science (QuICS) is currently seeking outstanding quantum information researchers to join the Center faculty as QuICS Fellows. QuICS is a research partnership between the University of Maryland and the National Institute of Standards and Technology, with faculty from both institutions.</p>
<p>Website: <a href="http://quics.umd.edu/join-quics/new-faculty">http://quics.umd.edu/join-quics/new-faculty</a><br />
Email: quics-coordinator@umiacs.umd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/"><span class="datestr">at October 02, 2019 03:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/10/02/matching-markets-simons-driven-by-theory-driving-the-economy/">Matching Markets @ Simons:  Driven by Theory, Driving the Economy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Guest post by Sid Banerjee.]</em></p>



<blockquote class="wp-block-quote"><p><em>Divergent Evolution: The formation of new species when populations experience different selective pressures.</em></p></blockquote>



<p>While the canonical example is Darwin’s finches, it could apply as well to matching theorists! A notable feature of the <a href="https://simons.berkeley.edu/workshops/market2019-1" target="_blank" rel="noreferrer noopener">first</a> and <a href="https://simons.berkeley.edu/workshops/market2019-2" target="_blank" rel="noreferrer noopener">second</a> workshops at the <a href="https://simons.berkeley.edu/programs/market2019" target="_blank" rel="noreferrer noopener">Simons Institute program on Matching Markets</a> was how researchers in Economics, Operations Research and TCS all share common antecedents (Fulkerson, Gale, Scarf, Shapley, Walras — to name but a few giants invoked regularly), and yet have taken the theory in  diverse directions. The workshops helped create a healthy dialogue between the communities, as everyone tries to understand each other’s objectives and techniques. </p>



<p>A more notable aspect of matching theory in recent years has been its  impact on the design of real-world marketplaces. Over the two workshops,  a mix of speakers from academia and industry covered a host of markets,  including <a href="https://simons.berkeley.edu/talks/tba-121" target="_blank" rel="noreferrer noopener">payment routing</a>, <a href="https://simons.berkeley.edu/talks/tba-127" target="_blank" rel="noreferrer noopener">online advertising</a>, <a href="https://simons.berkeley.edu/talks/tba-124" target="_blank" rel="noreferrer noopener">kidney exchange</a>, <a href="https://simons.berkeley.edu/talks/tba-128" target="_blank" rel="noreferrer noopener">real-estate</a>, <a href="https://simons.berkeley.edu/talks/tba-131" target="_blank" rel="noreferrer noopener">public housing</a>, <a href="https://simons.berkeley.edu/talks/ridesharing-panel" target="_blank" rel="noreferrer noopener">ride-sharing</a>, <a href="https://simons.berkeley.edu/talks/driving-efficiencies-freight-industry" target="_blank" rel="noreferrer noopener">long-haul trucking</a>, <a href="https://simons.berkeley.edu/talks/ratings-design-and-barriers-entry" target="_blank" rel="noreferrer noopener">restaurant reviews</a>, <a href="https://simons.berkeley.edu/talks/tba-129" target="_blank" rel="noreferrer noopener">school choice</a>, <a href="https://simons.berkeley.edu/talks/unreasonable-effectiveness-artificial-currencies" target="_blank" rel="noreferrer noopener">food-banks</a> and many many others. A common theme that emerged was that online marketplaces, with the support of good algorithm and mechanism designers, are slowly taking over the economy.</p>



<p>And talking of giants of matching theory, another event held in parallel with the program was a <a href="https://simons.berkeley.edu/events/richard-m-karp-distinguished-lecture-inaugural-lecture" target="_blank" rel="noreferrer noopener">celebration</a> of the achievements and contributions of Dick Karp, with Vijay Vazirani giving the <a href="https://simons.berkeley.edu/rmklectures2019-fall-1" target="_blank" rel="noreferrer noopener">inaugural lecture</a> of the Simons Institute Richard M. Karp Distinguished Lecture Series. Vijay’s talk touched on both the above themes, with a sweeping overview of three great threads in matching theory (stable matching, market equilibria, and online matching). He highlighted the critical role of algorithmic thinking in their development, and concluded with a tantalizing 40-year-old open problem connected to finding a polynomial-time algorithm for the Hylland-Zeckhauser market equilibrium. It is an excellent starting point for those interested in the program, or matching markets in general!</p></div>







<p class="date">
by robertkleinberg <a href="https://agtb.wordpress.com/2019/10/02/matching-markets-simons-driven-by-theory-driving-the-economy/"><span class="datestr">at October 02, 2019 12:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-8966138510075761711">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/10/phd-position-at-tue-on-product-line.html">PhD position at TU/e on product line engineering in multidisciplinary cyber-physical systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Model Driven Software Engineering section at Eindhoven University of Technology (TU/e) is searching for a candidate for a fully-funded PhD position on product line engineering in the multidisciplinary context of cyber-physical systems to collaborate with the high-tech company ASML in the context of the EU ECSEL project Arrowhead Tools.<br /><br />See <a href="https://eapls.org/items/3327/">here</a> for the details of the position.<br /><br />TU/e is a dynamic, research-intensive university in the heart of Europe, and in the Brainport region, a leading European technology region, and a centre for innovation and hi-tech industry. TU/e is consistently ranked within the top-100 positions in several world rankings for its research and quality of education.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/10/phd-position-at-tue-on-product-line.html"><span class="datestr">at October 01, 2019 10:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3419">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/10/01/sigecom-dissertation-award-call-for-nominations/">SIGecom Dissertation Award — Call for Nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div>
<div>
<div>Please consider nominating graduating Ph.D. students for the SIGecom Dissertation Award.  If you are a graduating student, consider asking your adviser or other senior mentor to nominate you.</div>
<div></div>
<div>Nominations are due on February 29, 2020.  This award is given to a student who defended a thesis in 2019.  It is a prestigious award and is accompanied by a $1500 prize.  In the past, the prize has been awarded to:</div>
</div>
<div>
<div></div>
<div>2018: Yannai Gonczarowski, “Aspects of Complexity and Simplicity in Economic Mechanisms”</div>
<div>2017: Aviad Rubinstein, “Hardness of Approximation Between P and NP”</div>
<div>2016: Peng Shi, “Prediction and Optimization in School Choice”</div>
<div>2015: Inbal Talgam-Cohen, “Robust Market Design: Information and Computation “</div>
<div>2014: S. Matthew Weinberg, “Algorithms for Strategic Agents”</div>
<div>2013: Balasubramanian Sivan, “Prior Robust Optimization”</div>
<div></div>
<div></div>
<div>And the award has had nine runner-ups: Nika Haghtalab, Haifeng Xu, Rachel Cummings, Christos Tzamos, Bo Waggoner, James Wright, Xi (Alice) Gao, Yang Cai, and Sigal Oren.  You can find detailed information about the nomination process at: <a href="http://www.sigecom.org/awardd.html" target="_blank" rel="noopener">http://www.sigecom.org/awardd.html</a>. We look forward to reading your nominations!</div>
<div></div>
<div></div>
<div>Your Award Committee,</div>
<div></div>
<div>Ozan Candogan</div>
<div>Renato Paes Leme (Chair)</div>
<div>
<div>Yiling Chen</div>
</div>
</div>
</div>
<div class="yj6qo"></div>
<div class="adL"></div></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2019/10/01/sigecom-dissertation-award-call-for-nominations/"><span class="datestr">at October 01, 2019 03:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2892542038915710982">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/09/richard-guy-is-102-years-old-today.html">Richard Guy is 103 years old today</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Richard Guy is a mathematician. He co-authored the classic book <i>Winning Ways for your Mathematical Plays </i> with Elywn Berlekamp and John Conway.<br />
<br />
On Sept 30 (today) he turned 102. According to <a href="https://en.wikipedia.org/wiki/List_of_centenarians_(scientists_and_mathematicians)">this list </a> he is the oldest living mathematician, and he would need to live to 110 to be the oldest mathematician ever. <br />
<br />
I have met him twice. He was at the Gathering for Gardner Conference as a young 98-year old. I told him that his book Winning Ways had a great influence on me. He asked it if was positive or negative. I later saw him at a Math conference where he went to my talk on The Muffin Problem. So he is still active.<br />
<br />
His Wikipedia site says that he says he regards himself as an Amateur Mathematician. While it is awkward to disagree with how someone sees himself, I'll point out that he is an author or co-author of 11 books, has many papers, and has solved Erdos Problems. He has taught some but I couldn't really find out what his source of income is or was. This takes us back to the word `amateur' which has several meanings:<br />
<br />
Amateur: Someone who does X for the love of X (Amor is Love in Spanish), and not for money. This could be true of Richard Guy. This notion of amateur may be lost on my younger readers since this it used to be a thing to NOT take money since it somehow soils what you do. In those days Olympic athletes could not have played professionally beforehand.  We can't imagine that now.<br />
<br />
Amateur: Someone who dabbles in something but is not really that good. This could NOT be true of Richard Guy.<br />
<br />
<br />
<br />
<br />
Aside from games he has also worked in Number Theory. His book <i>Unsolved Problems in Number Theory </i> has inspired many (including me). <br />
<br />
So happy birthday Richard Guy!<br />
<br />
He is the also the oldest living person we have honored on this blog. Second oldest was Katherine Johnson, see <a href="https://blog.computationalcomplexity.org/2018/08/katherine-johnson-1918.html"></a> who is still alive.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/09/richard-guy-is-102-years-old-today.html"><span class="datestr">at October 01, 2019 12:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/09/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/09/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://arxiv.org/html/1909.07013">Proceedings of the 27th International Symposium on Graph Drawing and Network Visualization (GD 2019)</a> (<a href="https://mathstodon.xyz/@11011110/102805229578506057"></a>). As in recent years, Graph Drawing is making an open-access version of their complete proceedings through arXiv, mirroring (except for minor typographic details) the official proceedings to be published through Springer LNCS. One advantage of the arXiv version is that in many cases appendices or longer versions of the papers are also available.</p>
  </li>
  <li>
    <p><a href="https://commons.wikimedia.org/wiki/File:National_Library_of_Kosovo_photo_Arben_Llapashtica.jpg">Look at all the triangulated (hemi)spheres!</a> (<a href="https://mathstodon.xyz/@11011110/102811772067495838"></a>). Sadly this aerial view of the <a href="https://en.wikipedia.org/wiki/National_Library_of_Kosovo">National Library of Kosovo</a> did not pass <a href="https://en.wikipedia.org/wiki/Wikipedia:Featured_picture_candidates/National_Library_of_Kosovo">its nomination as an English Wikipedia featured picture</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/102809304875599612">Christian Lawson-Perfect tries to visualize the concept of non-Hamiltonicity</a> by sending flocks of arrowheads tracing around paths in the <a href="https://en.wikipedia.org/wiki/Herschel_graph">Herschel graph</a> but without ever closing up to form a cycle.</p>
  </li>
  <li>
    <p>Millions of books published from 1923 to 1964 didn’t have their copyright renewed, putting them in the public domain in the US. <a href="https://www.vice.com/en_us/article/a3534j/libraries-and-archivists-are-scanning-and-uploading-books-that-are-secretly-in-the-public-domain">Libraries and the Internet Archive are collaborating to scan them and put them online</a> (<a href="https://mathstodon.xyz/@11011110/102825724023396126"></a>, <a href="https://news.ycombinator.com/item?id=20992397">via</a>). See also <a href="https://www.nypl.org/blog/2019/09/01/historical-copyright-records-transparency">the NYPL blog</a> for more technical details about the efforts to determine the non-renewal status of these books.</p>
  </li>
  <li>
    <p><a href="https://phys.org/news/2019-08-mathematical-framework-sheet-material-kirigami.html">Programmable kirigami</a> (<a href="https://mathstodon.xyz/@11011110/102828568682129281"></a>, <a href="https://news.ycombinator.com/item?id=20792995">via</a>, <a href="https://www.nature.com/articles/s41563-019-0452-y">original research paper</a>). Harvard researchers use numerical optimization to design slit cutting patterns and hinged unfoldings that allow sheets of material to expand from one given shape to another.</p>
  </li>
  <li>
    <p>White nationalists use the abbreviation SPQR (<a href="https://archive.org/stream/handbuchdertheo00hefngoog#page/n122/mode/2up">Sono Pazzi Questi Romani!</a>) thinking it refers to the Roman military. <a href="http://pages.vassar.edu/pharos/2018/07/27/scholars-respond-to-spqr-and-white-nationalism/">Classics scholars set them straight</a> (<a href="https://mathstodon.xyz/@11011110/102834635536818832"></a>). As the same abbreviation is widely used in graph drawing for <a href="https://en.wikipedia.org/wiki/SPQR_tree">a data structure to describe 3-connected components and planar embeddings</a> I think it’s important to pay attention to these darker shifts in  popular usage.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=4317">Scott Aaronson’s Quantum Supremacy FAQ</a> (<a href="https://mathstodon.xyz/@11011110/102845648603908743"></a>). The story Scott’s responding to is not yet properly published in peer-reviewed sources, but the scoop-hungry journalists say that Google has demonstrated their 50-something-qbit machines to be truly quantum. This seems like pretty big news from the quantum computing world even though it’s still a long way from there to breaking RSA.</p>
  </li>
  <li>
    <p>Embedding a Sierpiński tetrahedron onto a king’s graph (<a href="https://mathstodon.xyz/@11011110/102849777026944754"></a>):</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/sierpinski-kings-graph.svg" alt="Embedding a Sierpiński tetrahedron onto a king's graph" /></p>
  </li>
  <li>
    <p><a href="https://nerdcore.de/2019/09/23/eine-fotostudie-der-tafeln-von-mathematikern/">Jessica Wynne’s forthcoming book “Do Not Erase”, her collection of photographs of mathematical blackboards, looks fascinating</a> (<a href="https://mathstodon.xyz/@11011110/102856920767696508"></a>, <a href="https://www.metafilter.com/183291/This-is-what-thought-looks-like">discussion</a>, <a href="https://boingboing.net/2019/09/27/do-not-erase.html">see also</a>). There’s also an NYT article but I’m not linking it because they were too sniffy about reading their site in incognito mode.</p>
  </li>
  <li>
    <p><a href="http://bit-player.org/2019/my-god-its-full-of-dots">My God, It’s Full of Dots!</a> (<a href="https://mathstodon.xyz/@11011110/102868939006126852"></a>). Brian Hayes plays with the fractal circle packings of the plane (or of a region of the plane) that you get from a greedy process of picking random points and using either the maximum radius possible or the next radius on a given sequence of radii (but then only adding a circle if that radius fits).</p>
  </li>
  <li>
    <p><a href="https://blogs.plos.org/absolutely-maybe/2019/09/27/google-scholar-risks-and-alternatives/">Is the increasing use of Google Scholar causing citations to be concentrated more heavily on a smaller number of highly-cited papers?</a> (<a href="https://mathstodon.xyz/@11011110/102871113125694380"></a>, <a href="https://retractionwatch.com/2019/09/28/weekend-reads-jailed-for-publishing-a-paper-pushing-back-on-vaping-research-sugar-daddy-science/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2019-09-28/leading-chinese-american-scholars-decry-fallout-on-them-of-trumps-hardline-policies-against-china">Leading Chinese American scholars decry racial profiling from Trump’s hard-line policies against China</a> (<a href="https://mathstodon.xyz/@11011110/102877049872954133"></a>). You know how the US took over as a leader in mathematics and science from Germany in the 1930s-1940s because the Nazis were already driving away their best Jewish scientists, long before they became completely genocidal? This feels kind of similar.</p>
  </li>
  <li>
    <p><a href="https://royalsociety.org.nz/150th-anniversary/150-women-in-150-words/">150 women in 150 words</a> (<a href="https://mathstodon.xyz/@11011110/102884552172728020"></a>). The Royal Society of New Zealand celebrates its 150th anniversary by highlighting the contributions of women in New Zealand to scientific knowledge.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/09/30/linkage.html"><span class="datestr">at September 30, 2019 06:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/132">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/132">TR19-132 |  Radio Network Coding Requires Logarithmic Overhead | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the celebrated radio network model for abstracting communication in wireless networks. In this model, in any round, each node in the network may broadcast a message to all its neighbors. However, a node is able to hear a message broadcast by a neighbor only if no collision occurred, meaning that it was the only neighbor broadcasting. 


While the (noiseless) radio network model received a lot of attention over the last few decades, the effect of noise on radio networks is still not well understood. In this paper, we take a step forward and show that making radio network protocols resilient to noise may require a substantial performance overhead. Specifically, we  construct a multi-hop network and a communication protocol over this network that works in $T$ rounds when there is no noise. We prove that any scheme that simulates our protocol and is resilient to stochastic noise, requires $\Omega(T \log n)$ rounds. 

This stands in contrast to our previous result (STOC, 2018), showing that protocols over the single-hop (clique) network can be made noise resilient with only a constant overhead. Our result also settles a recent conjecture by Censor{-}Hillel, Haeupler, Hershkowitz, Zuzic (2018).

We complement the above result by giving a scheme to simulate any protocol with a fixed order of transmissions with only an $\mathcal{O}(\log n)$ overhead.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/132"><span class="datestr">at September 30, 2019 05:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16277">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/09/30/writing-33-as-a-sum-of-cubes/">Writing 33 as a Sum of Cubes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Cracking a Diophantine problem for 42 too</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/09/bookerbristolmath.jpg"><img src="https://rjlipton.files.wordpress.com/2019/09/bookerbristolmath.jpg?w=116&amp;h=150" alt="" width="116" class="alignright wp-image-16279" height="150" /></a></p>
<p>
Andrew Booker is a mathematician at the University of Bristol, who works in analytic number theory. For example he has a <a href="https://arxiv.org/pdf/1710.00603.pdf">paper</a> extending a result of Alan Turing on the Riemann zeta function. Yes our Turing. </p>
<p>
Today Ken and I will talk about his successful search for a solution to a 64 year old problem.</p>
<p>
He was inspired by a <a href="https://www.youtube.com/watch?v=wymmCdLdPvM&amp;feature=youtu.be">video</a> on the search problem authored by Tim Browning and Brady Haran. The question was to find a solution to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%2B+z%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  33 = x^{3} + y^{3} + z^{3}. " class="latex" title="\displaystyle  33 = x^{3} + y^{3} + z^{3}. " /></p>
<p>Booker <a href="https://arxiv.org/pdf/1903.04284.pdf">found</a> 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+-+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} - (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " class="latex" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} - (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " /></p>
<p>The search was for all possible solutions with <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z}" class="latex" title="{x,y,z}" /> bounded by <img src="https://s0.wp.com/latex.php?latex=%7B10%5E%7B16%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{10^{16}}" class="latex" title="{10^{16}}" />. Note that this is expensive, and is not even close to polynomial time in the number of bits. But it is feasible today thanks to modern technology: </p>
<blockquote><p><b> </b> <em> The total computation used was approximately <img src="https://s0.wp.com/latex.php?latex=%7B23%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{23}" class="latex" title="{23}" /> core-years over one month of real time. </em>
</p></blockquote>
<p></p><p>
Before we turn to our discussion, note that Booker’s paper on extending Turing is really a result on proof checking. Turing had great intuition, terrible that we lost him so early. He, Turing, essentially proved the first result ever on how to efficiently check a computation. Booker says: </p>
<blockquote><p><b> </b> <em> Turing introduced a method for certifying the completeness of a purported list of zeros of <img src="https://s0.wp.com/latex.php?latex=%7BZ%28t%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Z(t)}" class="latex" title="{Z(t)}" /> that is guaranteed to work (when the list is in fact complete). Turing’s method has remained a small but essential ingredient in all subsequent verifications of RH and its many generalizations. </em>
</p></blockquote>
<p>That is checking the zeroes of the Riemann zeta function <img src="https://s0.wp.com/latex.php?latex=%7BZ%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z(t)}" class="latex" title="{Z(t)}" />.</p>
<p>
Speaking of checking, when I was drafting this I initially had the wrong solution: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+%2B+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " class="latex" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " /></p>
<p>which is <b>wrong</b>. Can you quickly see why this cannot be right? Answer at the end.</p>
<p>
</p><p></p><h2> The Press </h2><p></p>
<p></p><p>
The press love Booker’s result. Not the one on the zeta function, the one on the number <img src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{33}" class="latex" title="{33}" />.</p>
<p>
Part of the excitement is caused by the number <img src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{33}" class="latex" title="{33}" />. In complexity theory we rarely see explicit numbers—more likely to see expressions like 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O%28n%5E%7B%5Clog_2+3%7D%28%5Clog+%5Clog+n%29%5E%7B3%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  O(n^{\log_2 3}(\log \log n)^{3}) " class="latex" title="\displaystyle  O(n^{\log_2 3}(\log \log n)^{3}) " /></p>
<p>and worse. </p>
<p>
The press seem to like the numerology of <img src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{33}" class="latex" title="{33}" />. The number <img src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{33}" class="latex" title="{33}" /> is quite <a href="https://en.wikipedia.org/wiki/33_(number)">neat</a>: </p>
<ul>
<li>
The atomic number of arsenic. <p></p>
</li><li>
It is the code for international direct-dial phone calls to France. <p></p>
</li><li>
It is Kareem Abdul-Jabbar’s old jersey number. <p></p>
</li><li>
And more <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" />
</li></ul>
<p>
Most important is the connection with Rolling-Rock beer:</p>
<p><a href="https://rjlipton.files.wordpress.com/2019/09/roll.jpg"><img src="https://rjlipton.files.wordpress.com/2019/09/roll.jpg?w=143&amp;h=300" alt="" width="143" class="aligncenter size-medium wp-image-16280" height="300" /></a></p>
<p>
The press from <em>Newsweek</em> and other sites talked about Booker’s solution. See <a href="https://www.newsweek.com/uncracked-problem-mathematician-diophantine-puzzle-1384422">here</a> and <a href="https://phys.org/news/2019-04-bristol-mathematician-diophantine-puzzle.html">here</a>. And <a href="https://www.quantamagazine.org/sum-of-three-cubes-problem-solved-for-stubborn-number-33-20190326/">here</a> at the <em>Quanta</em> magazine with a great diagram:</p>
<p><a href="https://rjlipton.files.wordpress.com/2019/09/cube.jpg"><img src="https://rjlipton.files.wordpress.com/2019/09/cube.jpg?w=300&amp;h=127" alt="" width="300" class="aligncenter size-medium wp-image-16281" height="127" /></a></p>
<p>
One <a href="https://science.howstuffworks.com/math-concepts/mathematician-has-just-cracked-33-problem.htm">said</a>: </p>
<blockquote><p><b> </b> <em> To crunch the numbers, he then used a cluster of powerful computers – 512 central processing unit (CPU) cores at the same time – known as Blue Crystal Phase 3. When he returned to his office one morning after dropping his children off at school, he spotted the solution on his screen. “I jumped for joy,” he recalled. </em>
</p></blockquote>
<p></p><p>
Another <a href="https://www.bristol.ac.uk/news/2019/april/number-33-.html">reported</a>, </p>
<blockquote><p><b> </b> <em> Booker said: “This one’s right at the boundary between what we know how to prove and what we suspect might be undecidable.” </em>
</p></blockquote>
<p></p><p>
I hope we will get the same coverage for our big results. </p>
<p>
</p><p></p><h2> More Press </h2><p></p>
<p></p><p>
The press love Booker’s result. Not the one on the zeta function, the one on the number <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" />. This search was jointly led by Andrew Sutherland of MIT.</p>
<p>
Part of the excitement is caused by the number <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" />. In complexity theory we rarely see explicit numbers—more likely to see expressions like 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O%28n%5E%7B%5Clog_%7B3%7D+4%7D%28%5Clog+%5Clog+n%29%5E%7B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  O(n^{\log_{3} 4}(\log \log n)^{2}) " class="latex" title="\displaystyle  O(n^{\log_{3} 4}(\log \log n)^{2}) " /></p>
<p>and worse. </p>
<p>
The press seem to like the numerology of <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" />. The number <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" /> is quite <a href="https://en.wikipedia.org/wiki/42_(number)">neat</a>: </p>
<ul>
<li>
The atomic number of molybdenum. <p></p>
</li><li>
It was the code for international direct-dial phone calls to Czechoslovakia, until the “velvet divorce” split the codes into <img src="https://s0.wp.com/latex.php?latex=%7B420%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{420}" class="latex" title="{420}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B421%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{421}" class="latex" title="{421}" />. <p></p>
</li><li>
It was Jackie Robinson’s old jersey number. Major League Baseball retired the number for all teams. The last player allowed to wear it was Mariano Rivera of the Yankees, himself a Hall-of-Famer, except that all players on all teams wear it on April 15. Rivera is nicknamed “Mo” which is the symbol for molybdenum—are we the first to notice this coincidence?<p></p>
</li><li>
And more <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" />
</li></ul>
<p>
Most important is the <a href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker's_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)">connection</a> with <em>The Hitchhiker’s Guide to the Galaxy</em>:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/09/wiki42snip.jpg"><img src="https://rjlipton.files.wordpress.com/2019/09/wiki42snip.jpg?w=240&amp;h=210" alt="" width="240" class="aligncenter wp-image-16282" height="210" /></a></p>
<p>
The press from <em>New Scientist</em> and other sites talked about Booker’s solution. See <a href="https://www.newscientist.com/article/2215680-mathematicians-crack-elusive-puzzle-involving-the-number-42/">here</a> and <a href="https://science.howstuffworks.com/math-concepts/mathematicians-solved-sum-3-cubes-problem-42.htm">here</a>. But <a href="https://www.quantamagazine.org/search?q[s]=42&amp;q[sort]=newest">here</a> the <em>Quanta</em> magazine seems not to have mentioned the number <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" /> at all in over three months:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/09/quantano42.jpg"><img src="https://rjlipton.files.wordpress.com/2019/09/quantano42.jpg?w=300&amp;h=192" alt="" class="aligncenter size-medium wp-image-16284" /></a></p>
<p>
One <a href="https://www.sciencealert.com/the-sum-of-three-cubes-problem-has-been-solved-for-42">said</a>:</p>
<blockquote><p><b> </b> <em> Of course, it wasn’t simple. The pair had to go large, so they enlisted the aid of the <a href="https://www.charityengine.com/">Charity Engine</a>, an initiative that spans the globe, harnessing unused computing power from over 500,000 home PCs to act as a sort of “planetary supercomputer.”</em></p><em>
<p>
It took over a million hours of computing time, but the two mathematicians found their solution: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28-80538738812075974%29%5E3+%2B+80435758145817515%5E3+%2B+12602123297335631%5E3+%3D+42.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  (-80538738812075974)^3 + 80435758145817515^3 + 12602123297335631^3 = 42. " class="latex" title="\displaystyle  (-80538738812075974)^3 + 80435758145817515^3 + 12602123297335631^3 = 42. " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
Another <a href="https://www.bristol.ac.uk/news/2019/september/sum-of-three-cubes-.html">reported</a>:</p>
<blockquote><p><b> </b> <em> Booker said: “I feel relieved … we might find what we’re looking for with a few months of searching, or it might be that the solution isn’t found for another century.” </em>
</p></blockquote>
<p></p><p>
I hope we will get the same coverage for our big results. </p>
<p>
</p><p></p><h2> Less Press </h2><p></p>
<p></p><p>
Booker and Sutherland also <a href="https://www.newscientist.com/article/2216941-mathematicians-find-a-completely-new-way-to-write-the-number-3/">discovered</a> that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++569936821221962380720%5E3+-+569936821113563493509%5E3+-+472715493453327032%5E3+%3D+3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  569936821221962380720^3 - 569936821113563493509^3 - 472715493453327032^3 = 3. " class="latex" title="\displaystyle  569936821221962380720^3 - 569936821113563493509^3 - 472715493453327032^3 = 3. " /></p>
<p>This is the next-largest solution after <img src="https://s0.wp.com/latex.php?latex=%7B3+%3D+1%5E3+%2B+1%5E3+%2B+1%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 = 1^3 + 1^3 + 1^3}" class="latex" title="{3 = 1^3 + 1^3 + 1^3}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B3+%3D+4%5E3+%2B+4%5E3+%2B+%28-5%29%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 = 4^3 + 4^3 + (-5)^3}" class="latex" title="{3 = 4^3 + 4^3 + (-5)^3}" />. Weird. And the first solution not to duplicate a number. And it uses two numbers that agree to markedly more decimal places than those in the above solutions for <img src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{33}" class="latex" title="{33}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{42}" class="latex" title="{42}" />. Weirder.</p>
<p>
</p><p></p><h2> Smart Search </h2><p></p>
<p></p><p>
Booker wanted to search for a solution to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%2B+z%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  k = x^{3} + y^{3} + z^{3}. " class="latex" title="\displaystyle  k = x^{3} + y^{3} + z^{3}. " /></p>
<p>Actually his main interest was in <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k=33}" class="latex" title="{k=33}" />, but his method is general. How does one do this for <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z}" class="latex" title="{x,y,z}" /> bounded by <img src="https://s0.wp.com/latex.php?latex=%7BB+%3D+10%5E%7B16%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B = 10^{16}}" class="latex" title="{B = 10^{16}}" />. The obvious method is: Try all numbers below <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. </p>
<p>
This is too expensive and requires <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\widetilde{O}(B^{3})}" class="latex" title="{\widetilde{O}(B^{3})}" /> time. Too much, even with a cluster of fast processors.</p>
<p>
An improvement is to try all <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> in the range and then check that <img src="https://s0.wp.com/latex.php?latex=%7Bk-x%5E%7B3%7D-y%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k-x^{3}-y^{3}}" class="latex" title="{k-x^{3}-y^{3}}" /> is cube. This runs in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\widetilde{O}(B^{2})}" class="latex" title="{\widetilde{O}(B^{2})}" /> time. Still too much.</p>
<p>
A key insight is to re-write the equation as 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+-z%5E%7B3%7D+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%3D+%28x+%2B+y%29%28x%5E%7B2%7D+-+xy+%2B+y%5E%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  k -z^{3} = x^{3} + y^{3} = (x + y)(x^{2} - xy + y^{2}). " class="latex" title="\displaystyle  k -z^{3} = x^{3} + y^{3} = (x + y)(x^{2} - xy + y^{2}). " /></p>
<p>Then we note that <img src="https://s0.wp.com/latex.php?latex=%7Bx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x+y}" class="latex" title="{x+y}" /> must be a divisor of <img src="https://s0.wp.com/latex.php?latex=%7Bk-z%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k-z^{3}}" class="latex" title="{k-z^{3}}" />. Since there are few such divisors, we can improve the time greatly. For the divisors <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> of <img src="https://s0.wp.com/latex.php?latex=%7Bk-z%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k-z^{3}}" class="latex" title="{k-z^{3}}" /> some simple algebra and the quadratic formula shows that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+%5Cfrac%7Bd%7D%7B2%7D+%2B+%5Csqrt%7B%5Cfrac%7B4%7Ck-z%5E%7B3%7D%7C+-+d%5E%7B3%7D%7D%7B6d%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x = \frac{d}{2} + \sqrt{\frac{4|k-z^{3}| - d^{3}}{6d}}, " class="latex" title="\displaystyle  x = \frac{d}{2} + \sqrt{\frac{4|k-z^{3}| - d^{3}}{6d}}, " /></p>
<p>and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y+%3D+%5Cfrac%7Bd%7D%7B2%7D+-+%5Csqrt%7B%5Cfrac%7B4%7Ck-z%5E%7B3%7D%7C+-+d%5E%7B3%7D%7D%7B3d%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y = \frac{d}{2} - \sqrt{\frac{4|k-z^{3}| - d^{3}}{3d}}. " class="latex" title="\displaystyle  y = \frac{d}{2} - \sqrt{\frac{4|k-z^{3}| - d^{3}}{3d}}. " /></p>
<p>This shows that the search is now reduced to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\widetilde{O}(B)}" class="latex" title="{\widetilde{O}(B)}" />. Still too much, but close to doable. The next trick is to avoid the factoring step. See Booker’s <a href="https://arxiv.org/pdf/1903.04284.pdf">paper</a> for the rest of the search description. </p>
<p>
I like the progression of time bounds from 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B). " class="latex" title="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B). " /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can one beat <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\widetilde{O}(B)}" class="latex" title="{\widetilde{O}(B)}" />? Could there be an algorithm that runs in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7Ba%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\widetilde{O}(B^{a})}" class="latex" title="{\widetilde{O}(B^{a})}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Ba%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a&lt;1}" class="latex" title="{a&lt;1}" />? Can any of our tricks apply here? A possible observation: Booker is clever but he writes that the methods use not 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B) " class="latex" title="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B) " /></p>
<p>time, but that they use 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7BO%7D%28B%5E%7B3%2B%5Cepsilon%7D%29+%5Crightarrow+%7BO%7D%28B%5E%7B2%2B%5Cepsilon%7D%29+%5Crightarrow+%7BO%7D%28B%5E%7B1%2B+%5Cepsilon%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {O}(B^{3+\epsilon}) \rightarrow {O}(B^{2+\epsilon}) \rightarrow {O}(B^{1+ \epsilon}) " class="latex" title="\displaystyle  {O}(B^{3+\epsilon}) \rightarrow {O}(B^{2+\epsilon}) \rightarrow {O}(B^{1+ \epsilon}) " /></p>
<p>time. Maybe we can help in some manner. What do you think? The next unsolved number, <img src="https://s0.wp.com/latex.php?latex=%7B114%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{114}" class="latex" title="{114}" />, awaits.</p>
<p></p><p align="center">§</p>
<p>			 <i>Answer to the question on checking</i>: Take the numbers modulo <img src="https://s0.wp.com/latex.php?latex=%7B10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{10}" class="latex" title="{10}" />. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+%2B+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3} " class="latex" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3} " /></p>
<p>becomes 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3+%5Cequiv+%288%29%5E%7B3%7D+%2B+%289%29%5E%7B3%7D+-+%280%29%5E%7B3%7D+%5Cequiv+2+%2B+9+%5Cequiv+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  3 \equiv (8)^{3} + (9)^{3} - (0)^{3} \equiv 2 + 9 \equiv 1. " class="latex" title="\displaystyle  3 \equiv (8)^{3} + (9)^{3} - (0)^{3} \equiv 2 + 9 \equiv 1. " /></p>
<p>
</p><p>[Typo fixed]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/09/30/writing-33-as-a-sum-of-cubes/"><span class="datestr">at September 30, 2019 04:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/131">TR19-131 |  A Simple Proof of Vyalyi&amp;#39;s Theorem and some Generalizations | 

	Lieuwe Vinkhuijzen, 

	André Deutz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In quantum computational complexity theory, the class QMA models the set of problems efficiently verifiable by a quantum computer the same way that NP models this for classical computation. Vyalyi proved that if $\text{QMA}=\text{PP}$ then $\text{PH}\subseteq \text{QMA}$. In this note, we give a simple, self-contained proof of the theorem, using only the closure properties of the complexity classes in the theorem statement. We then extend the theorem in two directions: (i) we strengthen the consequent, proving that if $\text{QMA}=\text{PP}$ then $\text{QMA}=\text{PH}^{\text{PP}}$, and (ii) we weaken the hypothesis, proving that if $\text{QMA}=\text{coQMA}$ then $\text{PH}\subseteq \text{QMA}$. Lastly, we show that all the above results hold, without loss of generality, for the class QAM instead of QMA. We also formulate a ``Quantum Toda's Conjecture''.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/131"><span class="datestr">at September 30, 2019 02:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=671">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/09/30/talk-why-do-lower-bounds-stop-just-before-proving-major-results/">Talk: Why do lower bounds stop “just before” proving major results?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I have prepared this talk which is a little unusual and is in part historical and speculative. You can view the slides <a href="http://www.ccs.neu.edu/home/viola/talks/lower-bounds-201909.pdf">here</a>. I am scheduled to give it in about three hours at <a href="http://www.bu.edu/cs/algorithms-and-theory-seminar/">Boston University</a>. And because it’s just another day in the greater Boston area, while I’ll be talking my ex office-mate <span class="field-content">Vitaly Feldman</span> will be speaking <a href="https://toc.seas.harvard.edu/toc-seminar">at Harvard University</a>.  His talk looks quite interesting and attempts to explain why overfitting is actually necessary for good learning. As for mine, well you’ll have to come and see or take a peek at the slides.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2019/09/30/talk-why-do-lower-bounds-stop-just-before-proving-major-results/"><span class="datestr">at September 30, 2019 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/130">TR19-130 |  A moment ratio bound for polynomials and some extremal properties of Krawchouk polynomials and Hamming spheres | 

	Alex Samorodnitsky, 

	Naomi Kirshner</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Let $p \ge 2$. We improve the bound $\frac{\|f\|_p}{\|f\|_2} \le (p-1)^{s/2}$ for a polynomial $f$ of degree $s$ on the boolean cube $\{0,1\}^n$, which comes from hypercontractivity, replacing the right hand side of this inequality by an explicit bivariate function of $p$ and $s$, which is smaller than $(p-1)^{s/2}$ for any $p &gt; 2$ and $s &gt; 0$. We show the new bound to be tight, within a smaller order factor, for the Krawchouk polynomial of degree $s$.

This implies several nearly-extremal properties of Krawchouk polynomials and Hamming spheres (equivalently, Hamming balls). In particular, Krawchouk polynomials have (almost) the heaviest tails among all polynomials of the same degree and $\ell_2$ norm (this has to be interpreted with some care). The Hamming spheres have the following approximate edge-isoperimetric property: For all $1 \le s \le \frac{n}{2}$, and for all even distances $0 \le i \le \frac{2s(n-s)}{n}$, the Hamming sphere of radius $s$ contains, up to a multiplicative factor of $O(i)$, as many pairs of points at distance $i$ as possible, among sets of the same size (there is a similar, but slightly weaker and somewhat more complicated claim for general distances). This also implies that Hamming spheres are (almost) stablest with respect to noise among sets of the same size. In coding theory terms this means that a Hamming sphere (equivalently a Hamming ball) has the maximal probability of undetected error, among all binary codes of the same rate.

We also describe a family of hypercontractive inequalities for functions on $\{0,1\}^n$, which improve on the `usual' ``$q \rightarrow 2$" inequality by taking into account the concentration of a function (expressed as the ratio between its $\ell_r$ norms), and which are nearly tight for characteristic functions of Hamming spheres.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/130"><span class="datestr">at September 30, 2019 07:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/129">TR19-129 |  Fourier and Circulant Matrices are Not Rigid | 

	Zeev Dvir, 

	Allen Liu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The concept of matrix rigidity was first introduced by Valiant in [Val77].  Roughly speaking, a matrix is rigid if its rank cannot be reduced significantly by changing a small number of entries.  There has been extensive interest in rigid matrices as Valiant showed  that rigidity can be used to prove arithmetic circuit lower bounds.  

In a surprising result, Alman and Williams showed that the (real valued) Hadamard matrix, which was conjectured  to be rigid, is actually not very rigid. This line of work was extended by [DE17] to a family of matrices related to the Hadamard matrix, but over finite fields.  In our work, we take another step in this direction and show that for any abelian group $G$ and function $f:G \rightarrow \mathbb C$, the matrix given by $M_{xy} = f(x - y)$ for $x,y \in G$ is not rigid.  In particular, we get that complex valued Fourier matrices, circulant matrices, and Toeplitz matrices are all not rigid and cannot be used to carry out Valiant's approach to proving circuit lower bounds. This complements a recent result of Goldreich and Tal  who showed that Toeplitz matrices are nontrivially rigid (but not enough for Valiant's method).  Our work differs  from previous non-rigidity results in that those works considered matrices whose underlying group of symmetries was of the form ${\mathbb F}_p^n$ with $p$ fixed and $n$ tending to infinity, while in the families  of matrices we study, the underlying group of symmetries can be any abelian group and, in particular,  the cyclic group ${\mathbb Z}_N$, which has very different structure. Our results also suggest natural new candidates for rigidity in the form of matrices whose symmetry groups are highly non-abelian. We are also able to extend these results to matrices with entries in a finite field, assuming sufficiently large dimension.

Our proof has four parts. The first extends the results of [AW16,DE17]  to generalized Hadamard matrices over the complex numbers via a new proof technique. The second part handles the $N \times N$ Fourier matrix when $N$ has a particularly nice factorization that allows us to embed smaller copies of (generalized) Hadamard matrices inside of it. The third part uses results from number theory to bootstrap the non-rigidity for these special values of $N$ and extend to all sufficiently large $N$.  The fourth and final part involves using the non-rigidity of the Fourier matrix to show that the group algebra matrix, given by $M_{xy} = f(x - y)$ for $x,y \in G$, is not rigid for any function $f$ and abelian group $G$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/129"><span class="datestr">at September 27, 2019 09:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1283">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2019/09/27/travel-funding-for-focs-2019/">Travel funding for FOCS 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="http://focs2019.cs.jhu.edu/">FOCS 2019</a> will be held in Baltimore, MA from Nov 9-12, 2019. The early registration deadline is October 9th.</p>
<p>For students interested in attending, Shang-Hua Teng has asked me to relay the message that there is some travel funding available, courtesy of the NSF. For details, see <a href="http://focs2019.cs.jhu.edu/travel_grants/">this website</a>. Ignore the deadline on that page, but apply ASAP for full consideration. Women and minorities are especially encouraged to apply. Having or presenting papers at FOCS’19 is *not* a prerequisite.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2019/09/27/travel-funding-for-focs-2019/"><span class="datestr">at September 27, 2019 08:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=370">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/09/27/tcs-talk-wednesday-october-2-shachar-lovett-ucsd/">TCS+ talk: Wednesday, October 2 — Shachar Lovett, UCSD</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 2th <strong>(next week!)</strong> at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Shachar Lovett</strong> from UCSD will lead us “<em>Towards the sunflower conjecture</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: A sunflower with <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=fff&amp;fg=444444&amp;s=0" alt="r" class="latex" title="r" /> petals is a collection of <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=fff&amp;fg=444444&amp;s=0" alt="r" class="latex" title="r" /> sets so that the intersection of each pair is equal to the intersection of all. Erdos and Rado in 1960 proved the sunflower lemma: for any fixed <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=fff&amp;fg=444444&amp;s=0" alt="r" class="latex" title="r" />, any family of sets of size <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=fff&amp;fg=444444&amp;s=0" alt="w" class="latex" title="w" />, with at least about <img src="https://s0.wp.com/latex.php?latex=w%5Ew&amp;bg=fff&amp;fg=444444&amp;s=0" alt="w^w" class="latex" title="w^w" /> sets, must contain a sunflower. The famous sunflower conjecture is that the bound on the number of sets can be improved to <img src="https://s0.wp.com/latex.php?latex=c%5Ew&amp;bg=fff&amp;fg=444444&amp;s=0" alt="c^w" class="latex" title="c^w" /> for some constant <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=fff&amp;fg=444444&amp;s=0" alt="c" class="latex" title="c" />. Despite much research, the best bounds until recently were all of the order of <img src="https://s0.wp.com/latex.php?latex=w%5E%7Bcw%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="w^{cw}" class="latex" title="w^{cw}" /> for some constant c. In this work, we improve the bounds to about <img src="https://s0.wp.com/latex.php?latex=%28%5Clog+w%29%5Ew&amp;bg=fff&amp;fg=444444&amp;s=0" alt="(\log w)^w" class="latex" title="(\log w)^w" />.</p>
<p>There are two main ideas that underlie our result. The first is a structure vs pseudo-randomness paradigm, a commonly used paradigm in combinatorics. This allows us to either exploit structure in the given family of sets, or otherwise to assume that it is pseudo-random in a certain way. The second is a duality between families of sets and DNFs (Disjunctive Normal Forms). DNFs are widely studied in theoretical computer science. One of the central results about them is the switching lemma, which shows that DNFs simplify under random restriction. We show that when restricted to pseudo-random DNFs, much milder random restrictions are sufficient to simplify their structure.</p>
<p>Joint work with Ryan Alweiss, Kewen Wu and Jiapeng Zhang.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/09/27/tcs-talk-wednesday-october-2-shachar-lovett-ucsd/"><span class="datestr">at September 27, 2019 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://bit-player.org/?p=2177">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hayes.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://bit-player.org/2019/my-god-its-full-of-dots">My God, It’s Full of Dots!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://bit-player.org" title="bit-player">bit-player</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Please click or tap in the gray square below. It will fill with a jolly tableau of colored disks—first big blue ones, then somewhat smaller purple ones, and eventually lots of tiny red dots.</p>
<div class="canvas-container canvas-failure" id="demo">
<p>Sorry. My program and your browser are not getting along. None of the interactive elements of this page will work. Could you try a different browser? Current versions of Chrome, Firefox, and Safari seem to work.</p>
</div>
<p class="indent">The disks are scattered randomly, except that no disk is allowed to overlap another disk or extend beyond the boundary of the square. Once a disk has been placed, it never moves, so each later disk has to find a home somewhere in the nooks and crannies between the earlier arrivals. Can this go on forever?</p>
<p>The search for a vacant spot would seem to grow harder as the square gets more crowded, so you might expect the process to get stuck at some point, with no open site large enough to fit the next disk. On the other hand, because the disks get progressively smaller, later ones can squeeze into tighter quarters. In the specific filling protocol shown here, these two trends are in perfect balance. The process of adding disks, one after another, never seems to stall. Yet as the number of disks goes to infinity, they completely fill the box provided for them. There’s a place for every last dot, but there’s no blank space left over.</p>
<p>Or at least that’s the mathematical ideal. The computer program that fills the square above never attains this condition of perfect plenitude. It shuts down after placing just 5,000 disks, which cover about 94 percent of the square’s area. This early exit is a concession to the limits of computer precision and human patience, but we can still dream of how it would work in a world without such tiresome constraints.</p>
<p>This scheme for filling space with randomly placed objects is the invention of John Shier, a physicist who worked for many years in the semiconductor industry and who has also taught at Normandale Community College near Minneapolis. He explains the method and the mathematics behind it in a recent book, <em>Fractalize That! A Visual Essay on Statistical Geometry</em>. (For bibliographic details see the links and references at the end of this essay.) I learned of Shier’s work from my friend Barry Cipra.</p>
<p>Shier hints at the strangeness of these doings by imagining a set of 100 round tiles in graduated sizes, with a total area approaching one square meter. He would give the tiles to a craftsman with these instructions:</p>
<blockquote><p>“Mark off an area of one square meter, either a circle or a square. Start with the largest tile, and attach it permanently anywhere you wish in the marked-off area. Continue to attach the tiles anywhere you wish, proceeding always from larger to smaller. <em>There will always be a place for every tile regardless of how you choose to place them.</em>” How many experienced tile setters would believe this?</p></blockquote>
<p class="indent">Shier’s own creations go way beyond squares and circles filled with simple shapes such as disks. <img src="http://bit-player.org/wp-content/uploads/2019/07/Shier-book-cover-from-Wolrd-Scientific.jpg" height="" width="300" alt="Shier book cover from WS" border="0" class="alignright" />He has shown that the algorithm also works with an assortment of more elaborate designs, including nonconvex figures and even objects composed of multiple disconnected pieces. We get snow­flakes, nested rings, stars, butter­flies, fish eating lesser fish, faces, letters of the alphabet, and visual salads bringing together multiple ingredients. Shier’s interest in these patterns is aesthetic as well as mathematical, and several of his works have appeared in art exhibits; one of them won a best-of-show award at the 2017 Joint Mathematics Meeting.</p>
<p>Shier and his colleagues have also shown that the algorithm can be made to work in three-dimensional space. The book’s cover is adorned with a jumble of randomly placed toruses filling the volume of a transparent cube. If you look closely, you’ll notice that some of the rings are linked; they cannot be disentangled without breaking at least one ring. (The 3D illustration was created by Paul Bourke, who has <a href="http://paulbourke.net/fractals/linkedrings/">more examples online</a>, including 3D-printed models.)</p>
<p>After reading Shier’s account of his adventures, and admiring the pictures, I had to try it for myself. The experiments I’m presenting in this essay have no high artistic ambitions. I stick with plain-vanilla circular disks in a square frame, all rendered with the same banal blue-to-red color scheme. My motive is merely to satisfy my curiosity—or perhaps to overcome my skepticism. When I first read the details of how these graphics are created, I couldn’t quite believe it would work. Writing my own programs and seeing them in action has helped persuade me. So has a proof by Christopher Ennis, which I’ll return to below.</p>
<hr />
<p>Filling a region of the plane with disks is not in itself such a remarkable trick. <img src="http://bit-player.org/wp-content/uploads/2019/07/apollonian_gasket_detail.svg" height="" width="280" alt="Apollonian gasket detail" border="0" class="alignright" />One well-known way of doing it goes by the name Apollonian circles. Start with three disks that are all tangent to one another, leaving a spiky three-pointed vacancy between them. Draw a new disk in the empty patch, tangent to all three of the original disks; this is the largest disk that can possibly fit in the space. Adding the new disk creates three smaller triangular voids, where you can draw three more triply tangent disks. There’s nothing to stop you from going on in this way indefinitely, approaching a limiting configuration where the entire area is filled.</p>
<p>There are randomized versions of the Apollonian model. For example, you might place zero-diameter seed disks at random unoccupied positions and then allow them to grow until they touch one (or more) of their neighbors. This process, too, is space-filling in the limit. And it can never fail: Because the disks are custom-fitted to the space available, you can never get stuck with a disk that can’t find a home.</p>
<p>Shier’s algorithm is different. You are given disks one at a time in a predetermined order, starting with the largest, then the second-largest, and so on. To place a disk in the square, you choose a point at random and test to see if the disk will fit at that location without bumping into its neighbors or poking beyond the boundaries of the square. If the tests fail, you pick another random point and try again. It’s not obvious that this haphazard search will always succeed—and indeed it works only if the successive disks get smaller according to a specific mathematical rule. But if you follow that rule, you can keep adding disks forever. Furthermore, as the number of disks goes to infinity, the fraction of the area covered approaches \(1\). It’s convenient to have a name for series of disks that meet these two criteria; I have taken to calling them <em>fulfilling</em> series.</p>
<hr />
<p>In exploring these ideas computationally, it makes sense to start with the simplest case: disks that are all the same size. This version of the process clearly <em>cannot</em> be fulfilling. No matter how the disks are arranged, their aggregate area will eventually exceed that of any finite container. Click in the gray square below to start filling it with equal-size disks. The square box has area \(A_{\square} = 4\). The slider in the control panel determines the area of the individual disks \(A_k\), in a range from \(0.0001\) to \(1.0\).</p>
<h2 class="zf">Program 1: Fixed-Size Disks</h2>
<div class="canvas-container canvas-failure" id="fixed-size">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">If you play with this program for a while, you’ll find that the dots bloom quickly at first, but the process invariably slows down and eventually ends in a state labeled “Jammed,” indicating that the program has been unable The program gives up after trying 10 million random locations. to find a place where one more disk will fit. If you move the slider to the right, specifying larger disks, this impasse is reached very quickly, sometimes after placing just one or two disks. If you select very small disks, the program may churn away for five or ten minutes and fill the square with more than 20,000 disks before running out of options. Nevertheless, for any disk size greater than zero, a jammed outcome is inescapable.</p>
<p>The densest possible packing of equal-size disks places the centers on a triangular lattice with spacing equal to the disk diameter. The resulting density (for an infinite number of disks on an infinite plane) is \(\pi \sqrt{3}\, /\, 6 \approx 0.9069\), which means more than 90 percent of the area is covered. A random filling in a finite square is much looser. My first few trials all halted with a filling fraction fairly close to one-half, and so I wondered if that nice round number might be the expectation value of the probabilistic process. Further experiments suggested otherwise. Over a broad range of disk sizes, from \(0.0001\) up to about \(0.01\), the area covered varied from one run to the next, but the average was definitely above one-half—perhaps \(0.54\). After some rummaging through the voluminous literature on circle packing, I think I may have a clue to the exact expectation value: \(\pi / (3 + 2 \sqrt{2}) \approx 0.539012\). Where does that weird number come from? The answer has nothing to do with Shier’s algorithm, but I think  it’s worth a digression.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/06/max-2-packing.svg" height="200" width="200" alt="The two largest equal-size disks that fit in a unit square." border="0" class="alignright" />Consider an adversarial process: Alice is filling a unit square with \(n\) equal-size disks and wants to cover as much of the area as possible. Bob, who wants to minimize the area covered, gets to choose \(n\). If Bob chooses \(n = 1\), Alice can produce a single disk that just fits inside the square and covers about \(79\) percent of the space. Can Bob do better? Yes, if Bob specifies \(n = 2\), Alice’s best option is to squeeze the two disks into diagonally opposite corners of the square as shown in the diagram at right. These disks are bounded by right isosceles triangles, which makes it easy to calculate their radii as \(r = 1 / (2 + \sqrt{2}) \approx 0.2929\). Their combined area works out to that peculiar number \(\pi / (3 + 2 \sqrt{2}) \approx 0.54\).</p>
<p>If two disks are better than one (from Bob’s point of view), could three be better still? Or four, or some larger number? Apparently not. In 2010, <a href="https://arxiv.org/abs/1008.1224">Erik Demaine, Sándor Fekete and Robert Lang</a> conjectured that the two-disk configuration shown above represents the worst case for any number of equal-size disks. In 2017 <a href="https://arxiv.org/abs/1705.00924">Fekete, Sebastian Morr, and Christian Scheffer</a> proved this result. </p>
<p>Is it just a coincidence that the worst-case density for packing disks into a square also appears to be the expected density when equal-size disks are placed randomly until no more will fit? Wish I knew.</p>
<hr />
<p>Let us return to the questions raised in Shier’s <em>Fractalize That!</em> If we want to fit infinitely many disks into a finite square, our only hope is to work with disks that get smaller and smaller as the process goes on. The disk areas must come from some sequence of ever-diminishing numbers. Among such sequences, the one that first comes to mind is \(\frac{1}{1}, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \ldots\) These fractions have been known since antiquity as the harmonic numbers. (They are the wavelengths of the overtones of a plucked string.) </p>
<p>To see what happens when successive disks are sized according to the harmonic sequence, click in the square below.</p>
<h2 class="zf">Program 2: Disk Sizes from Harmonic Series</h2>
<div class="canvas-container canvas-failure" id="harmonic">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p>Again, the process halts when no open space is large enough to accommodate the next disk in the sequence. If you move the slider all the way to the right, you’ll see a sequence of disks with areas drawn from the start of the full harmonic sequence, \(\frac{1}{1} , \frac{1}{2}, \frac{1}{3}, \dots\); at this setting, you’ll seldom get beyond eight or nine disks. Moving the slider to the left omits the largest disks at the beginning of the sequence, leaving the infinite tail of smaller disks. For example, setting the slider to \(1/20\) skips all the disks from \(\frac{1}{1}\) through \(\frac{1}{19}\) and begins filling the square with disks of area \(\frac{1}{20}, \frac{1}{21}, \frac{1}{22}, \dots\) Such truncated series go on longer, but eventually they also end in a jammed configuration.</p>
<p>The slider goes no further than 1/50, but even if you omitted the first 500 disks, or the first 5 million, the result would be the same. This is a consequence of the most famous property of the harmonic numbers: Although the individual terms \(1/k\) dwindle away to zero as \(k\) goes to infinity, the sum of all the terms, </p>
<p>\[\sum_{k = 1}^{\infty}\frac{1}{k} = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \cdots,\]</p>
<p class="undent">does not converge to a finite value. As long as you keep adding terms, the sum will keep growing, though ever more slowly. This curious fact was proved in the 14th century by the French bishop and scholar Nicole Oresme. The proof is simple but ingenious. Oresme pointed out that the harmonic sequence</p>
<p>\[\frac{1}{1} + \frac{1}{2} + \left(\frac{1}{3} + \frac{1}{4}\right) + \left(\frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8}\right) + \cdots\]</p>
<p class="undent">is greater than</p>
<p>\[\frac{1}{1} + \frac{1}{2} + \left(\frac{1}{4} + \frac{1}{4}\right) + \left(\frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8}\right) + \cdots\]</p>
<p class="undent">The latter series is equivalent to \(1 + \frac{1}{2} + \frac{1}{2} + \frac{1}{2} \cdots\), and so it is clearly divergent. Since the grouped terms of the harmonic series are even greater, they too must exceed any finite bound.</p>
<p>The divergence of the harmonic series implies that disks whose areas are generated by the series will eventually overflow any enclosing container. Dropping a finite prefix of the sequence, such as the first 50 disks, does not change this fact.</p>
<p>Let me note in passing that just as the filling fraction for fixed-size disks seems to converge to a specific constant, 0.5390, disks in harmonic series also seem to have a favored filling fraction, roughly 0.71. Can this be explained by some simple geometric argument? Again, I wish I knew.</p>
<hr />
<p>Evidently we need to make the disks shrink faster than the harmonic numbers do. Here’s an idea: Square each element of the harmonic series, yielding this:</p>
<p>\[\sum_{k = 1}^{\infty}\frac{1}{k^2} = \frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \cdots.\]</p>
<p class="undent">Click below (or press the Start button) to see how this one turns out, again in a square of area 4.</p>
<h2 class="zf">Program 3: Disk Sizes from Zeta(2)</h2>
<div class="canvas-container canvas-failure" id="zeta2">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">At last we have a process that won’t get stuck in a situation where there’s no place to put another disk. It <em>could</em> run forever, but of course it doesn’t. It quits when the area of the next disk shrinks down to about a tenth of the size of a single pixel on a computer display. The stopped state is labeled “Exhausted” rather than “Jammed.”This is an algorithm that could truly run forever. And yet the result is still not quite what we were hoping for—it’s not <em>fulfilling</em>. The disks are scattered sparsely in the square, leaving vast open spaces unoccupied. The configuration reminds me of deep-sky images made by large telescopes.</p>
<p>Why does this outcome look so different from the others? Unlike the harmonic numbers, the infinite series \(1 +  \frac{1}{4} + \frac{1}{9} + \frac{1}{16} + \cdots\) converges to a finite sum. In the 18th century the task of establishing this fact (and determining the exact sum) was known as the Basel Problem, after the hometown of the Bernoulli family, who put much effort into the problem but never solved it. The answer came in 1735 from Leonhard Euler (another native of Basel, though he was working in St. Petersburg), who showed that the sum is equal to \(\pi^2 / 6\). This works out to about \(1.645\); since the area of the square we want to fill is \(4\), even an infinite series of disks would cover only about \(41\) percent of the territory.</p>
<p>Given that the numbers \(\frac{1}{1^1}, \frac{1}{2^1}, \frac{1}{3^1}, \dots\) diminish too slowly, whereas \(\frac{1}{1^2}, \frac{1}{2^2}, \frac{1}{3^2}, \dots\) shrink too fast, it makes sense to try an exponent somewhere between \(1\) and \(2\) in the hope of finding a Goldilocks solution. The computation performed below in Program 4 is meant to facilitate the search for such a happy medium. Here the disk sizes are elements of the sequence \(\frac{1}{1^s}, \frac{1}{2^s}, \frac{1}{3^s}, \dots\), where the value of the exponent \(s\) is determined by the setting of the slider, with a range of \(1 \lt s \le 2\). We already know what happens at the extremes of this range. What is the behavior in the middle?</p>
<h2 class="zf">Program 4: Disk Sizes from Zeta(s)</h2>
<div class="canvas-container canvas-failure" id="zeta-adjust">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">If you try the default setting of \(s = 1.5\), you’ll find you are still in the regime where the disks dwindle away so quickly that the box never fills up; if you’re willing to wait long enough, the program will end in an exhausted state rather than a jammed one. Reducing the exponent to \(s = 1.25\) puts you on the other side of the balance point, where the disks remain too large and at some point one of them will not fit into any available space. By continuing to shuttle the slider back and forth, you could carry out a binary search, closing in, step by step, on the “just right” value of \(s\). This strategy can succeed, but it’s not quick.  As you get closer to the critical value, the program will run longer and longer before halting. (After all, running forever is the behavior we’re seeking.) To save you some tedium, I offer a spoiler: the optimum setting is between 1.29 and 1.30.</p>
<p>At this point we have wandered into deeper mathematical waters. A rule of the form \(A_k = 1/k^s\) is called a power law, since each \(k\) is raised to the same power. And series of the form \(\sum 1/k^s\) are known as zeta functions, denoted \(\zeta(s)\). Zeta functions have quite a storied place in mathematics. The harmonic numbers correspond to \(\zeta(1) = \sum 1/k^1\), which does not converge. If \(\zeta(1)\) grows without limit whereas \(\zeta(2)\) converges, you might well wonder where the boundary lies between these two behaviors. As it happens, \(\zeta(1 + \epsilon)\) converges for any \(\epsilon \gt 0\).As mentioned above, Euler found that \(\zeta(2) = \pi^2 / 6\), and he went on to show that \(\zeta(s)\) also converges for all integer values of \(s\) greater than \(1\). Later, Pafnuty Chebyshev extended the domain of the function beyond the integers to all real numbers \(s\) greater than \(1\). And then Bernhard Riemann went further still: He devised a smoke-and-mirrors trick for defining the zeta function over the entire plane of complex numbers, with the single exception of \(s = 1\). </p>
<p>Today, Riemann’s version of the zeta function is the engine (or enigma!) driving a major mathematical industry. Shier’s use of this apparatus in making fractal art is far removed from that heavy-duty research enterprise—but no less fascinating. Think of it as the zeta function on vacation. </p>
<p>If a collection of disks are to fill a square exactly, their aggregate area must equal the area of the square. This is a necessary condition though not a sufficient one. In all the examples I’ve presented so far, the containing square has an area of 4, so what’s needed is to find a value of \(s\) that satisfies the equation:</p>
<p>\[\zeta(s) = \sum_{k = 1}^{\infty}\frac{1}{k^s} = 4\]</p>
<p class="undent">Except for isolated values of \(s\), Those isolated values are the negative integers and the even positive integers.there is no known method for solving this equation exactly. But numerical approximation works well enough for a computer program that draws pictures. The binary search described above is a crude and cumbersome version of this numerical method. <a href="http://www.sagemath.org/">SageMath</a> presumably does something more sophisticated. When I ask it to find a root of the equation \(zeta(s) = 4\), it returns the result \(1.2939615055572438\).</p>
<p>Having this result in hand solves one part of the square-filling problem. It tells us how to construct an infinite set of disks whose total area is just enough to cover a square of area \(4\), with adequate precision for graphical purposes. We assign each disk \(k\) (starting at \(k = 1\)) an area of \(1/k^{1.2939615}.\) This sequence begins 1.000, 0.408, 0.241, 0.166, 0.125, 0.098,…</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/07/zeta_curves_edit1.svg" height="" width="640" alt="Curves showing the convergence of zeta(s) for five different values of s, when the series is summed over a number of terms ranging up to 10^8. The curve for s = 1.29396 converges to a value of 4." border="0" class="centered" /></p>
<p class="undent">In the graph above, the maroon curve with \(s = 1.29396\) converges to a sum very close to 4. Admittedly, the rate of convergence is not quick. More than 3 million terms are needed to get within 1 percent of the target.</p>
<hr />
<p>Our off-label use of the zeta function defines an infinite sequence of disks whose aggregate area is equal to \(4\). The disks in this unique collection will exactly fill our square box (assuming they can be properly arranged). It’s satisfying to have a way of reliably achieving this result, after our various earlier failures. On the other hand, there’s something irksome about that number \(4\) appearing in the equation. It’s so arbitrary! I don’t dispute that \(4\) is a perfectly fine and foursquare number, but there are many other sizes of squares we might want to fill with dots. Why give all our attention to the \(2 \times 2\) variety?</p>
<p>This is all my fault. When I set out to write some square-filling programs, I knew I couldn’t use the unit square—which seems like the obvious default choice—because of the awkward fact that \(\zeta(s) = 1\) has no finite solution. The unit square is also troublesome in the case of the harmonic numbers; the first disk, with area \(A_1 = 1\),  is too large to fit. So I picked the next squared integer for the box size in those first programs. Having made my choice, I stuck with it, but now I feel hemmed in by that decision made with too little forethought.</p>
<p>We have all the tools we need to fill squares of other sizes (as long as the size is greater than \(1\)). Given a square of area \(A_{\square}\), we just solve for \(s\) in \(\zeta(s) = A_{\square}\). A square of area 8 can be covered by disks sized according to the rule \(A_k = 1/k^s\) with \(s = \zeta(8) \approx 1.1349\). For \(A_{\square} = 100\), the corresponding value of \(s\) is \(\zeta(100) \approx 1.0101\). For any \(A_{\square} \gt 1\) there is an \(s\) that yields a fulfilling set of disks, and vice versa for any \(s \gt 1\).</p>
<p>This relation between the exponent \(s\) and the box area \(A_{\square}\) suggests a neat way to evade the whole bother of choosing a specific container size. We can just scale the disks to fit the box, or else scale the box to accommodate the disks. Shier adopts the former method. Each disk in the infinite set is assigned an area of</p>
<p>\[A_k = \frac{A_{\square}}{\zeta(s)} \frac{1}{k^s},\]</p>
<p class="undent">where the first factor is a scaling constant that adjusts the disk sizes to fit the container. In my first experiments with these programs I followed the same approach. Later, however, when I began writing this essay, it seemed easier to think about the scaling—and explain it—if I transformed the size of the box rather than the sizes of the disks. In this scheme, the area of disk \(k\) is simply \(1 / k^s\), and the area of the container is \(A_{\square} = \zeta(s)\). (The two scaling procedures are mathematically equivalent; it’s only the ratio of disk size to container size that matters.)</p>
<p>Program 5 offers an opportunity to play with such scaled zeta functions. I’m not actually changing the physical size of the box—the number of pixels it occupies on-screen remains the same. I’m scaling the units of measure.No matter where you move the \(s\) slider, the area of the square container will adjust to match the total area of the infinite set of disks. As the ratio of disk size to container size shifts, so does the overall texture or appearance of the pattern. At a setting of \(s = 1.35\), the largest disk fills almost a third of the square; at \(s = 1.08\), the first disk occupies only about \(8\) percent of box area. At those lower settings it takes a very long time to reach a high filling percentage, but if you have true faith in mathematical certainties, your patience will be rewarded.</p>
<h2 class="zf">Program 5: Disk Sizes from Scaled Zeta(s)</h2>
<div class="canvas-container canvas-failure" id="zeta-scaled">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">At the other end of the scale, if you push the value of \(s\) up beyond about \(1.40\), you’ll discover something else: The program more often than not halts after placing just a few disks. At \(s = 1.50\) or higher, it seldom gets beyond the first disk. This failure is similar to what we saw with the harmonic numbers, but more interesting. In the case of the harmonic numbers, the total area of the disks is unbounded, making an overflow inevitable. With this new scaled version of the zeta function, the total area of the disks is always equal to that of the enclosing square. In principle, all the disks could all be made to fit, if you could find the right arrangement. I’ll return below to the question of why that doesn’t happen.</p>
<hr />
<p>In <em>Fractalize That!</em> Shier introduces another device for taming space-filling sets. He not only scales the object sizes so that their total area matches the space available; he also adopts a variant zeta function that has two adjustable parameters rather than just one: </p>
<p>A note on notation: Shier writes the Hurwitz zeta function as \(\zeta(c, N)\), whereas most of the mathematical literature seems to favor \(\zeta(s, a)\). I’m going with the majority.\[\zeta(s, a) = \sum_{k=0}^{\infty} \frac{1}{(a + k)^s}\]</p>
<p class="undent">This is the Hurwitz zeta function, named for the German mathematician Adolf Hurwitz (1859–1919). Before looking into the details of the function, let’s play with the program and see what happens. Try a few settings of the \(s\) and \(a\) controls:</p>
<h2 class="zf">Program 6: Disk Sizes from Scaled Hurwitz Zeta Function</h2>
<div class="canvas-container canvas-failure" id="hurwitz">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p>Different combinations of \(s\) and \(a\) produce populations of disks with different size distributions. The separate contributions of the two parameters are not always easy to disentangle, but in general decreasing \(s\) or increasing \(a\) leads to a pattern dominated by smaller disks. Here are snapshots of four outcomes:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/07/four_hurwitz_snapshots.svg" height="" width="640" alt="Four hurwitz snapshots" border="0" class="centered" /></p>
<p class="undent">Within the parameter range shown in these four panels, the filling process always continues to exhaustion, but at higher values of \(s\) it can jam, just as it does with the scaled Riemann zeta function.</p>
<p>Adolf Hurwitz in the 1880s. Photo from <a href="https://en.wikipedia.org/wiki/Adolf_Hurwitz">Wikipedia</a>.<img src="http://bit-player.org/wp-content/uploads/2019/07/Adolf_Hurwitz-1880s-600px.jpg" height="300" width="200" alt="Adolf Hurwitz in the 1880s. Image from Wikimedia" border="0" class="alignleft" />Until I began this project, I knew nothing of Adolf Hurwitz or his work, although he is <a href="http://www-history.mcs.st-and.ac.uk/Biographies/Hurwitz.html">hardly an obscure figure</a> in the history of mathematics. He earned his Ph.D. under Felix Klein and also studied with Karl Weierstrass, Ernst Eduard Kummer, and Leopold Kronecker—key figures in the founding of modern analysis and number theory. Among his own pupils (and close friends) were David Hilbert and Hermann Minkowski. Albert Einstein was another of his students, although apparently he <a href="https://arxiv.org/abs/1205.4335">seldom went to class</a>.</p>
<p>Hurwitz wrote just one paper on the zeta function. It was published in 1882, when he was still quite young and just beginning his first academic appointment, at the University of Göttingen. (The paper is available from the <a href="https://gdz.sub.uni-goettingen.de/id/PPN599415665_0027">Göttinger Digitalisierungszentrum</a>; see pp. 86–101.) </p>
<p>Hurwitz modified the Riemann zeta function in two ways. First, the constant \(a\) is added to each term, turning \(1/k^s\) into \(1/(a + k)^s\). Second, the summation begins with \(k = 0\) rather than \(k = 1\). By letting \(a\) take on any value in the range \(0 \lt a \le 1\) we gain access to a continuum of zeta functions. The elements of the series are no longer just reciprocals of integers but reciprocals of real numbers. Suppose \(a = \frac{1}{3}\). Then \(\zeta(s, a)\) becomes:</p>
<p>\[\frac{1}{\left(\frac{1}{3} + 0\right)^s} + \frac{1}{\left(\frac{1}{3} + 1\right)^s} + \frac{1}{\left(\frac{1}{3} + 2\right)^s} + \cdots\ = \left(\frac{3}{1}\right)^s + \left(\frac{3}{4}\right)^s + \left(\frac{3}{7}\right)^s + \cdots\]</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/hurwitz-s1.3-plot.svg" height="" width="300" alt="Hurwitz s=1 3 plot" border="0" class="alignright" />The Riemann zeta function and the Hurwitz zeta function differ substantially only for small values of \(k\) or large values of \(a\). When \(k\) is large, adding a small \(a\) to it makes little difference in the value of the function. Thus as \(k\) grows toward infinity, the two functions are asymptotically equal, as suggested in the graph at right. When the Hurwitz function is put to work packing disks into a square, a rule with \(a &gt; 1\) causes the first several disks to be smaller than they would be with the Riemann rule. A value of \(a\) between \(0\) and \(1\) enlarges the early disks. In either case, the later disks in the sequence are hardly affected at all.</p>
<p class="indent">If \(a\) is a positive integer, the interpretation of \(\zeta(s, a)\) is even simpler. The case \(a = 1\) corresponds to the Riemann zeta sum. When \(a\) is a larger integer, the effect is to omit the first \(a - 1\) entries, leaving only the tail of the series. For example,</p>
<p>\[\zeta(s, 5) = \frac{1}{5^s} + \frac{1}{6^s} + \frac{1}{7^s} + \cdots.\] </p>
<p class="indent">In his fractal artworks, Shier chooses various values of \(a\) as a way of controlling the size distribution of the placed objects, and thereby fine-tuning the appearance of the patterns. Having this adjustment knob available is very convenient, but in the interests of simplicity, I am going to revert to the Riemann function in the rest of this essay.</p>
<p>Before going on, however, I also have to confess that I don’t really understand the place of the Hurwitz zeta function in modern mathematical research, or what Hurwitz himself had in mind when he formulated it. Zeta functions have been an indispensable tool in the long struggle to understand how the prime numbers are sprinkled among the integers. The connection between these two realms was made by Euler, with his remarkable equation linking a sum of powers of integers with a product of powers of primes:</p>
<p>Euler’s other famous equation, \(e^{i\pi} + 1 = 0\), has a bigger fan club, but this is the one that revs <em>my</em> motor.\[\sum_{k = 1}^{\infty} \frac{1}{k^s} = \prod_{p \text{ prime}} \frac{1}{1 - \frac{1}{p^s}}.\]</p>
<p>Riemann went further, showing that everything we might want to know about the distribution of primes is encoded in the undulations of the zeta function over the complex plane. Indeed, if we could simply pin down all the complex values of \(s\) for which \(\zeta(s) = 0\), we would have a master key to the primes. Hurwitz, in his 1882 paper, was clearly hoping to make some progress toward this goal, but I have not been able to figure out how his work fits into the larger story. The Hurwitz zeta function gets almost no attention in standard histories and reference works (in contrast to the Riemann version, which is everywhere). <a href="https://en.wikipedia.org/wiki/Hurwitz_zeta_function">Wikipedia</a> notes: “At rational arguments the Hurwitz zeta function may be expressed as a linear combination of Dirichlet <em>L</em>-functions and vice versa”—which sounds interesting, but I don’t know if it’s useful or important. A recent <a href="https://arxiv.org/abs/1506.00856">article by Nicola Oswald and Jörn Steuding</a> puts Hurwitz’s work in historical context, but it does not answer these questions—at least not in a way I’m able to understand.</p>
<p>But again I digress. Back to dots in boxes.</p>
<hr />
<p>If a set of circular disks and a square container have the same total area, can you always arrange the disks so that they completely fill the square without overflowing? Certainly not! Suppose the set consists of a single disk with area equal to that of the square; the disk’s diameter is greater than the side length of the square, so it will bulge through the sides while leaving the corners unfilled. A set of two disks won’t work either, no matter how you apportion the area between them. Indeed, when you are putting round pegs in a square hole, no finite set of disks can ever fill all the crevices.</p>
<p>Only an infinite set—a set with no smallest disk—can possibly fill the square completely. But even with an endless supply of ever-smaller disks, it seems like quite a delicate task to find just the right arrangement, so that every gap is filled and every disk has a place to call home. It’s all the more remarkable, then, that simply plunking down the disks at random locations seems to produce exactly the desired result. This behavior is what intrigued and troubled me when I first saw Shier’s pictures and read about his method for generating them. If a <em>random</em> arrangement works, it’s only a small step to the proposition that <em>any</em> arrangement works. Could that possibly be true?</p>
<p>Computational experiments offer strong hints on this point, but they can never be conclusive. What we need is a proof. Ennis’s proof was published in <em><a href="https://www.tandfonline.com/doi/abs/10.4169/mathhorizons.23.3.8">Math Horizons</a></em>, a publication of the Mathe­matical Association of America, which keeps it behind a paywall. If you have no library access and won’t pay the $50 ransom, I can recommend a <a href="https://wp.stolaf.edu/mscs/2014-2015-colloquium-series/">video</a> of Ennis explaining his proof in a talk at St. Olaf College.And so I turn to the work of Christopher Ennis, a mathematician at Normandale Community College, who met Shier when they were both teaching there. </p>
<p>As a warm-up exercise, Ennis proves a one-dimensional version of the area-filling conjecture, where the geometry is simpler and some of the constraints are easier to satisfy. In one dimension a disk is merely a line segment; its area is its length, and its radius is half that length. As in the two-dimensional model, disks are placed in descending order of size at random positions, with the usual proviso that no disk can overlap another disk or extend beyond the end points of the containing interval. In Program 7 you can play with this scheme.</p>
<h2 class="zf">Program 7: One-Dimensional Disks</h2>
<div class="canvas-container canvas-failure" id="one-dim">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="undent">I have given the line segment some vertical thickness to make it visible. The resulting pattern of stripes may look like a supermarket barcode or an atomic spectrum, but please imagine it as one-dimensional.</p>
<p>If you adjust the slider in this program, you’ll notice a difference from the two-dimensional system. In 2D, the algorithm is fulfilling only if the exponent \(s\) is less than a critical value, somewhere in the neighborhood of 1.4. In one dimension, the process continues without impediment for all values of \(s\) throughout the range \(1 \lt s \lt 2\). Try as you might, you won’t find a setting that produces a jammed state. (In practice, the program halts after placing no more than 10,000 disks, but the reason is exhaustion rather than jamming.)</p>
<p>Ennis titles his <em>Math Horizons</em> article “(Always) room for one more.” He proves this assertion by keeping track of the set of points where the center of a new disk can legally be placed, and showing the set is never empty. Suppose \(n - 1\) disks have already been randomly scattered in the container. The next disk to be placed, disk \(n\), will have an area (or length) of \(A_n = 1 / n^s\). Since the geometry is one-dimensional, the corresponding disk radius is simply \(r_n = A_n / 2\). The center of this new disk cannot lie any closer than \(r_n\) to the perimeter of another disk. It must also be at a distance of at least \(r_n\) from the boundary of the containing segment. We can visualize these constraints by adding bumpers, or buffers, of thickness \(r_n\) to the outside of each existing disk and to the inner edges of the containing segment. A few stages of the process are illustrated below.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/08/1d-buffer-diagram.svg" height="" width="680" alt="1d buffer diagram" border="0" class="alignleft" /></p>
<p>Placed disks are blue, the excluded buffer areas are orange, and open areas—the set of all points where the center of the next disk could be placed—are black. In the top line, before any disks have been placed, the entire containing segment is open except for the two buffers at the ends. Each of these buffers has a length equal to \(r_1\),  the radius of the first disk to be placed; the center of that disk cannot lie in the orange regions because the disk would then overhang the end of the containing segment. After the first disk has been placed <em>(second line)</em>, the extent of the open area is reduced by the area of the disk itself and its appended buffers. On the other hand, all of the buffers have also shrunk; each buffer is now equal to the radius of disk \(2\), which is smaller than disk \(1\). The pattern continues as subsequent disks are added. Note that although the blue disks cannot overlap, the orange buffers can.</p>
<p>For another view of how this process evolves, click on the <em>Next</em> button in Program 8. Each click inserts one more disk into the array and adjusts the buffer and open areas accordingly.</p>
<h2 class="zf">Program 8: One-Dimensional with Buffers</h2>
<div class="canvas-container canvas-failure" id="one-dim-bumpers">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">Because the blue disks are never allowed to overlap, the total blue area must increase monotonically as disks are added. It follows that the orange and black areas, taken together, must steadily decrease. But there’s nothing steady about the process when you keep an eye on the separate area measures for the orange and black regions. Changes in the amount of buffer overlap cause erratic, seesawing tradeoffs between the two subtotals. If you keep clicking the <em>Next</em> button (especially with \(s\) set to a high value), you may see the black area falling below \(1\) percent. Can we be sure it will never vanish entirely, leaving no opening at all for the next disk?</p>
<p>Ennis answers this question through worst-case analysis. He considers only configurations in which no buffers overlap, thereby squeezing the black area to its smallest possible extent. If the black area is always positive under these conditions, it cannot be smaller when buffer overlaps are allowed.</p>
<p>The basic idea of the proofI have altered some of the notation and certain details of presentation to conform with choices I made elsewhere in this exposition. is to start with \(n - 1\) buffered disks already in place, arranged so that none of the orange buffer areas intersect. The total area of the blue disks is \(\sum_{k=1}^{k=n-1} 1/k^s\). Each buffer zone has a width equal to \(r_{n}\), the radius of the next disk to be added to the tableau; since each disk has two buffers, the total area of the orange buffers is \(2(n-1)r_{n}\). The black area is whatever’s left over. In other words,</p>
<p>\[A_{\square} = \zeta(s), \quad A_{\color{blue}{\mathrm{blue}}} = \sum_{k=1}^{k = n - 1} \frac{1}{k^s}, \quad A_{\color{orange}{\mathrm{orange}}} = 2(n-1)r_{n}.\]</p>
<p class="undent">Then we need to prove that</p>
<p> \[A_{\square} - (A_{\color{blue}{\mathrm{blue}}} + A_{\color{orange}{\mathrm{orange}}}) \gt 0.\]</p>
<p>A direct proof of this statement would require an exact, closed-form expression for \(\zeta(s)\), which we already know is problematic. Ennis evades this difficulty by turning to calculus. He needs to evaluate the remaining tail of the zeta series, \(\sum_{k = n}^\infty 1/k^s\), but this discrete sum is intractable. On the other hand, by shifting from a sum to an integral, the problem becomes an exercise in undergraduate calculus. Exchanging the discrete variable \(k\) for a continuous variable \(x\), we want to find the area under the curve \(1/x^s\) in the interval from \(n\) to infinity; this will provide a lower bound on the corresponding discrete sum. Evaluating the integral yields:</p>
<p>\[\int_{x = n}^{\infty} \frac{1}{x^{s}} d x = \frac{1}{(s-1) n^{s-1}}.\]</p>
<p class="undent">Some further manipulation reveals that the area of the black regions is never smaller than</p>
<p>\[\frac{2 - s}{(s - 1)n^{s - 1}}.\]</p>
<p class="undent">If \(s\) lies strictly between \(1\) and \(2\), this expression must be greater than zero, since both the numerator and the denominator will be positive. Thus for all \(n\) there is at least one black point where the center of a new disk can be placed.</p>
<p>Ennis’s proof is a stronger one than I expected. When I first learned there was a proof, I guessed that it would take a probabilistic approach, showing that although a jammed configuration may exist, it has probability zero of turning up in a random placement of the disks. Instead, Ennis shows that no such arrangement exists at all. Even if you replaced the randomized algorithm with an adversarial one that tries its best to block every disk, the process would still run to fulfillment.</p>
<hr />
<p>The proof for a two-dimensional system follows the same basic line of argument, but it gets more complicated for geometric reasons. In one dimension, as the successive disk areas get smaller, the disk radii diminish in simple proportion: \(r_k = A_k / 2\). In two dimensions, disk radius falls off only as the square root of the disk area: \(r_k = \sqrt{A_k / \pi}\). As a result, the buffer zone surrounding a disk excludes neighbors at a greater distance in two dimensions than it would in one dimension. There is still a range of \(s\) values where the process is provably unstoppable, but it does not extend across the full interval from \(s \gt 1\) to \(s \lt 2\).</p>
<p>Program 9, running in the panel below, is one I find very helpful in gaining intuition into the behavior of Shier’s algorithm. As in the one-dimensional model of Program 8, each press of the <em>Next</em> button adds a single disk to the containing square, and shows the forbidden buffer zones surrounding the disks.</p>
<h2 class="zf">Program 9: Two-Dimensional Disks with Buffers</h2>
<div class="canvas-container canvas-failure" id="bumpers2d">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p>Move the \(s\) slider to a position somewhere near 1.40. In the control panel for this program, the registers showing fill percentages for blue, orange, and black areas are not entirely trustworthy. In the one-dimensional case, it’s easy to calculate the areas to high precision. That’s much harder in two dimensions, where the overlapping regions of buffers can have complex shapes. I have resorted to counting pixels, a procedure that has limited resolution and is subject to errors caused by fuzzy boundaries.At this setting, there’s a fair chance (maybe 10 or 20 percent) that the very first disk and its orange buffer zone will entirely cover the open black region, creating a jammed state. On other runs you might get as far as two or three or 10 disks before you get stuck. If you make it beyond that point, however, you are likely to continue unimpeded for as long as you have the patience to keep pressing <em>Next</em>. Shier describes this phenomenon as “infant mortality”: If the placement process survives the high-risk early period, it is all but immortal.</p>
<p>There’s a certain whack-a-mole dynamic to the behavior of this system. Maybe the first disk covers all but one small corner of the black zone. It looks like the next disk will completely obliterate that open area. And so it does—but at the same time the shrinking of the orange buffer rings opens up another wedge of black elsewhere. The third disk blots out that spot, but again the narrowing of the buffers allows a black patch to peek out from still another corner. Later on, when there are dozens of disks, there are also dozens of tiny black spots where there’s room for another disk. You can often guess which of the openings will be filled next, because the random search process is likely to land in the largest of them. Again, however, as these biggest targets are buried, many smaller ones are born.</p>
<p>Ennis’s two-dimensional proof addresses the case of circular disks inside a circular boundary, rather than a square one. (The higher symmetry and the absence of corners streamlines certain calculations.) The proof strategy, again, is to show that after \(n - 1\) disks have been placed, there is still room for the \(n\)th disk, for any value of \(n \ge 1\). The argument follows the same logic as in one dimension, relying on an integral to provide a lower bound for the sum of a zeta series. But because of the \(\pi r^2\) area relation, the calculation now includes quadratic as well as linear terms. As a result, the proof covers only a part of the range of \(s\) values. The black area is provably nonempty if \(s\) is greater than \(1\) but less than roughly \(1.1\); outside that interval, the proof has nothing to say.</p>
<p>As mentioned above, Ennis’s proof applies only to circular disks in a circular enclosure. Nevertheless, in what follows I am going to assume the same ideas carry over to disks in a square frame, although the location of the boundary will doubtless be somewhat different. I have recently learned that Ennis has written a further paper on the subject, expected to be published in the <em>American Mathematical Monthly</em>. Perhaps he addresses this question there.</p>
<p>With Program 9, we can explore the entire spectrum of behavior for packing disks into a square. The possibilities are summarized in the candybar graph below.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/spectrum-of-behaviors.svg" height="" width="650" alt="Spectrum of behaviors" border="0" class="centered" /></p>
<ul>
<li>The leftmost band, in darker green, is the interval for which Ennis’s proof might hold. The question mark at the upper boundary line signifies that we don’t really know where it lies.</li>
<li>In the lighter green region no proof is known, but in Shier’s extensive experiments the system never jams there.</li>
<li>The transition zone sees the probability of jamming rise from \(0\) to \(1\) as \(s\) goes from about \(1.3\) to about \(1.5\).</li>
<li>Beyond \(s \approx 1.5\), experiments suggest that the system <em>always</em> halts in a jammed configuration.</li>
<li>At \(s \approx 1.6\) we enter a regime where the buffer zone surrounding the first disk invariably blocks the entire black region, leaving nowhere to place a second disk. Thus we have a simple proof that the system always jams.</li>
<li>Still another barrier arises at \(s \approx 2.7\). Beyond this point, not even one disk will fit. The diameter of a disk with area \(1\) is greater than the side length of the enclosing square.</li>
</ul>
<hr />
<p>Can we pin down the exact locations of the various threshold points in the diagram above? This problem is tractable in those situations where the placement of the very first disk determines the outcome. <img src="http://bit-player.org/wp-content/uploads/2019/09/centered-disk-128-160pts.png" height="160" width="160" alt="Centered disk 128 160pts" border="0" class="alignright" />At high values of \(s\) (and thus low values of \(\zeta(s)\), the first disk can obliterate the black zone and thereby preclude placement of a second disk. What is the lowest value of \(s\) for which this can happen? As in the image at right, the disk must lie at the center of the square box, and the orange buffer zone surrounding it must extend just far enough out to cover the corners of the inner black square, which defines the locus of all points that could accommodate the center of the second disk. Finding the value of \(s\) that satisfies this condition is a messy but straightforward bit of geometry and algebra. With the help of SageMath I get the answer \(s = 1.282915\). This value—let’s call it \(\overline{s}\)—is an upper bound on the “never jammed” region. Above this limit there is always a nonzero probability that the filling process will end after placing a single disk.</p>
<p>The value of \(\overline{s}\) lies quite close to the experimentally observed boundary between the never-jammed range and the transition zone, where jamming first appears. Is it possible that \(\overline{s}\) actually marks the edge of the transition zone—that below this value of \(s\) the program can never fail? To prove that conjecture, you would have to show that when the first disk is successfully placed, the process never stalls on a subsequent disk. That’s certainly not true in higher ranges of \(s\). Yet the empirical evidence near the threshold is suggestive. In my experiments I have yet to see a jammed outcome at \(s \lt \overline{s}\), not even in a million trials just below the threshold, at \(s = 0.999 \overline{s}\). In contrast, at \(s = 1.001 \overline{s}\), a million trials produced 53 jammed results—all of them occuring immediately after the first disk was placed.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/corner-disk-159-160pts.png" height="160" width="160" alt="Corner disk 159 160pts" border="0" class="alignright" />The same kind of analysis leads to a lower bound on the region where <em>every</em> run ends after the first disk <em>(medium pink in the diagram above)</em>. In this case the critical situation puts the first disk as close as possible to a corner of the square frame, rather than in the middle. If the disk and its orange penumbra are large enough to block the second disk in this extreme configuration, then they will also block it in any other position. Putting a number on this bound again requires some fiddly equation wrangling; the answer I get is \(\underline{s} = 1.593782\). No process with higher \(s\) can possibly live forever, since it will die with the second disk. In analogy with the lower-bound conjecture, one might propose that the probability of being jammed remains below \(1\) until \(s\) reaches \(\underline{s}\). If both conjectures were true, the transition region would extend from \(\overline{s}\) to \(\underline{s}\).</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/centered-disk-270-160pts.png" height="160" width="160" alt="Centered disk 270 160pts" border="0" class="alignright" />The final landmark, way out at \(s \approx 2.7\), marks the point where the first disk threatens to burst the bounds of the enclosing square. In this case the game is over before it begins. In program 9, if you push the slider far to the right, you’ll find that the black square in the middle of the orange field shrinks away and eventually winks out of existence. This extinction event comes when the diameter of the disk equals the side length of the square. Given a disk of area \(1\), and thus radius \(1/\sqrt{\pi}\), we want to find the value of \(s\) that satisfies the equation</p>
<p>\[\frac{2}{\sqrt{\pi}} = \sqrt{\zeta(s)}.\]</p>
<p class="undent">Experiments with Program 9 show that the value is just a tad more than 2.7. That’s an interesting numerical neighborhood, no? A famous number lives nearby. Do you suppose?</p>
<hr />
<p>Another intriguing set of questions concerns the phenomenon that Shier calls infant mortality. If you scroll back up to Program 5 and set the slider to \(s = 1.45\), you’ll find that roughly half the trials jam. The vast majority of these failures come early in the process, after no more than a dozen disks have been placed. At \(s = 1.50\) death at an early age is even more common; three-fourths of all the trials end with the very first disk. On the other hand, if a sequence of disks does manage to dodge all the hazards of early childhood, it may well live on for a very long time—perhaps forever.</p>
<p>Should we be surprised by this behavior? I am. As Shier points out, the patterns formed by our graduated disks are fractals, and one of their characteristic properties is self-similarity, or scale invariance. If you had a fully populated square—one filled with infinitely many disks—you could zoom in on any region to any magnification, and the arrangement of disks would look the same as it does in the full-size square. By “look the same” I don’t mean the disks would be in the same positions, but they would have the same size distribution and the same average number of neighbors at the same distances. This is a statistical concept of identity. And since the pattern looks the same and has the same statistics, you would think that the challenge of finding a place for a new disk would also be the same at any scale. Slipping in a tiny disk late in the filling operation would be no different from plopping down a large disk early on. The probability of jamming ought to be constant from start to finish.</p>
<p>But there’s a rejoinder to this argument: Scale invariance is broken by the presence of the enclosing square. The largest disks are strongly constrained by the boundaries, whereas most of the smaller disks are nowhere near the edges and are little influenced by them. The experimental data offer some support for this view. The graph below summarizes the outcomes of \(20{,}000\) trials at \(s = 1.50\). The red bars show the absolute numbers of trials ending after placing \(n\) disks, for each \(n\) from \(0\) through \(35\). The blue lollipops indicate the proportion of trials reaching disk \(n\) that halted after placing disk \(n\). This ratio can be interpreted (if you’re a frequentist!) as the probability of stopping at \(n\).</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/stymied-bars-and-pops-150-35-20000.svg" height="" width="" alt="Stymied bars and pops 150 35 20000" border="0" class="aligncenter" /></p>
<p class="indent">It certainly looks like there’s something odd happening on the left side of this graph. More than three fourths of the trials end after a single disk, but none at all jam at the second or third disks, and very few (a total of \(23\)) at disks \(4\) and \(5\). Then, suddenly, \(1{,}400\) more fall by the wayside at disk \(6\), and serious attrition continues through disk \(11\).</p>
<p>Geometry can explain some of this weirdness. It has to do with the squareness of the container; other shapes would produce different results.</p>
<p>At \(s = 1.50\) we are between \(\overline{s}\) and \(\underline{s}\), in a regime where the first disk is large enough to block off the entire black zone but not so large that it <em>must</em> do so. This is enough to explain the tall red bar at \(n = 1\): When you place the first disk randomly, roughly \(75\) percent of the time it will block the entire black region, ending the parade of disks. If the first disk <em>doesn’t</em> foreclose all further action, it must be tucked into one of the four corners of the square, leaving enough room for a second disk in the diagonally opposite corner. The sequence of images below (made with Program 9) tells the rest of the story.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/s150-stymied-at-6-sequence-1280px.png" height="105" width="636" alt="S=150 stymied at 6 sequence 1280px" border="0" class="alignright" /></p>
<p>The placement of the second disk blocks off the open area in that corner, but the narrowing of the orange buffers also creates two tiny openings in the cross-diagonal corners. The third and fourth disks occupy these positions, and simultaneously allow the black background to peek through in two other spots. Finally the fifth and sixth disks close off the last black pixels, and the system jams.</p>
<p>This stereotyped sequence of disk placements accounts for the near absence of mortality at ages \(n = 2\) through \(n = 5\), and the sudden upsurge at age \(6\). The elevated levels at \(n = 7\) through \(11\) are part of the same pattern; depending on the exact positioning of the disks, it may take a few more to expunge the last remnants of black background. </p>
<p>At still higher values of \(n\)—for the small subset of trials that get there—the system seems to shift to a different mode of behavior. Although numerical noise makes it hard to draw firm conclusions, it doesn’t appear that any of the \(n\) values beyond \(n = 12\) are more likely jamming points than others. Indeed, the data are consistent with the idea that the probability of jamming remains constant as each additional disk is added to the array, just as scale invariance would suggest.</p>
<p>A much larger data set would be needed to test this conjecture, and collecting such data is painfully slow. Furthermore, when it comes to rare events, I don’t have much trust in the empirical data. During one series of experiments, I noticed a program run that stalled after \(290\) disks—unusually late. The 290-disk configuration, produced at \(s = 1.47\), is shown at left below.</p>
<p>This pattern, compared with those produced at lower values of \(s\), has a strongly Apollonian texture. Many of the disks are nestled tightly among their neighbors, and they form the recursive triangular motifs characteristic of Apollonian circles.<img src="http://bit-player.org/wp-content/uploads/2019/09/stymied-at-290-and-then-314-s148-maxAttmp1e10.png" height="320" width="645" alt="Stymied at 290 and then 314 s=148 maxAttmp=1e10" border="0" class="centered" /></p>
<p>I wondered if it was <em>truly</em> jammed. My program gives up on finding a place for a disk after \(10^7\) random attempts. Perhaps if I had simply persisted, it would have gone on. So I reset the limit on random attempts to \(10^9\), and sat back to wait. After some minutes the program discovered a place where disk \(291\) would fit, and then another for disk \(292\), and kept going as far as 300 disks. The program had an afterlife! Could I revive it again? Upping the limit to \(10^{10}\) allowed another \(14\) disks to squueze in. The final configuration is shown at right above (with the original \(290\) disks faded, in order to make the \(24\) posthumous additions more conspicuous).</p>
<p>Is it really finished now, or is there still room for one more? I have no reliable way to answer that question. Checking \(10\) billion random locations sounds like a lot, but it is still a very sparse sampling of the space inside the square box. Using 64-bit floating-point numbers to define the coordinate system allows for more than \(10^{30}\) distinguishable points. And to settle the question mathematically, we would need unbounded precision.</p>
<p>We know from Ennis’s proof that at values of \(s\) not too far above \(1.0\), the filling process can always go on forever. And we know that beyond \(s \approx 1.6\), every attempt to fill the square is doomed. There must be some kind of transition between these two conditions, but the details are murky. The experimental evidence gathered so far suggests a smooth transition along a sigmoid curve, with the probability of jamming gradually increasing from \(0\) to \(1\). As far as I can tell, however, nothing we know for certain rules out a single hard threshold, below which all disk sequences are immortal and above which all of them die. Thus the phase diagram would be reduced to this simple form:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/two-state-spectrum-of-behaviors.svg" height="" width="640" alt="Two state spectrum of behaviors" border="0" class="centered" /></p>
<p class="undent">The softer transition observed in computational experiments would be an artifact of our inability to perform infinite random searches or place infinite sequences of disks.</p>
<hr />
<p>Here’s a different approach to understanding the random dots-in-a-box phenomenon. It calls for a mental reversal of figure and ground. Instead of placing disks on a square surface, we drill holes in a square metal plate. And the focus of attention is not the array of disks or holes but rather the spaces between them. Shier has a name for the perforated plate: the gasket. </p>
<p>Program 10 allows you to observe a gasket as it evolves from a solid black square to a delicate lace doily with less than 1 percent of its original substance.</p>
<h2 class="zf">Program 10: The Gasket</h2>
<div class="canvas-container canvas-failure" id="gasket">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">The gasket is quite a remarkable object. When the number of holes becomes infinite, the gasket must disappear entirely; its area falls to zero. Up until that very moment, however, it retains its structural integrity. This statement about the con­nectedness of the gasket requires a more careful consideration of what it means for two disks or holes to overlap. Are they allowed to touch, or in other words to be tangent, sharing a single point? The answer makes no difference to the calculation of areas, but it does matter for connectivity. Allowing tangency (as in Apollonian circles) would shatter the gasket, leaving tiny shards. To preserve connectivity, a computer program must test for overlaps with “\(\gt\)” rather than “\(\ge\)”.Although it may be reduced to a fine filigree, the perforated square never tears apart into multiple pieces; it remains a single, connected component, a network with multiple paths linking any two points you might choose.</p>
<p>As the gasket is etched away, can we measure the average thickness of the surviving wisps and tendrils? I can think of several methods that involve elaborate sampling schemes. Shier has a much simpler and more ingenious proposal: To find the average thickness of the gasket, divide its area by its perimeter. It was not immediately obvious to me why this number would serve as an appropriate measure of the width, but at least the units come out right: We are dividing a length squared by a length and so we get a length. And the operation does make basic sense: The area of the gasket represents the amount of substance in it, and the perimeter is the distance over which it is stretched. (The widths calculated in Program 10 differ slightly from those reported by Shier. The reason, I think, is that I include the outer boundary of the square in the perimeter, and he does not.)</p>
<p>Calculating the area and perimeter of a complicated shape such as a many-holed gasket looks like a formidable task, but it’s easy if we just keep track of these quantities as we go along. Initially (before any holes are drilled), the gasket area \(A_0^g\) is the area of the full square, \(A_\square\). The initial gasket perimeter \(P_0^g\) is four times the side length of the square, which is \(\sqrt{A_\square}\). Thereafter, as each hole is drilled, we subtract the new hole’s area from \(A^g\) and add its perimeter to \(P^g\). The quotient of these quantities is our measure of the average gasket width after drilling hole \(k\): \(\widehat{W}_k^g\). Since the gasket area is shrinking while the perimeter is growing, \(\widehat{W}_k^g\) must dwindle away as \(k\) increases.</p>
<p>The importance of \(\widehat{W}_k^g\) is that it provides a clue to how large a vacant space we’re likely to find for the next disk or hole. If we take the idea of “average” seriously, there must always be at least one spot in the gasket with a width equal to or greater than \(\widehat{W}_k^g\). From this observation Shier makes the leap to a whole new space-filling algorithm. Instead of choosing disk diameters according to a power law and then measuring the resulting average gasket width, he determines the radius of the next disk from the observed \(\widehat{W}_k^g\):</p>
<p>\[r_{k+1} = \gamma \widehat{W}_k^g = \gamma \frac{A_k^g}{P_k^g}.\]</p>
<p class="undent">Here \(\gamma\) is a fixed constant of proportionality that determines how tightly the new disks or holes fit into the available openings.</p>
<p>The area-perimeter algorithm has a recursive structure, in which each disk’s radius depends on the state produced by the previous disks. This raises the question of how to get started: What is the size of the first disk? Shier has found that it doesn’t matter very much. Initial disks in a fairly wide range of sizes yield jam-proof and aesthetically pleasing results.</p>
<p>Graphics produced by the original power-law algorithm and by the new recursive one look very similar. One way to understand why is to rearrange the equation of the recursion:</p>
<p>Perhaps this equation would be a little easier to interpret if the average width were defined in terms of hole diameters rather than hole perimeters. Then the denominator of the right hand side would be the sum of the first \(k\) diameters scaled by the \((k+1)\)st diameter.\[\frac{1}{2 \gamma} = \frac{A_k^g}{2 r_{k+1} P_k^g}.\]</p>
<p>On the right side of this equation we are dividing the average gasket width by the diameter of the next disk to be placed. The result is a dimensionless number—dividing a length by a length cancels the units. More important, the quotient is a constant, unchanging for all \(k\). If we calculate this same dimensionless gasket width when using the power-law algorithm, it also turns out to be nearly constant in the limit of karge \(k\), showing that the two methods yield sequences with similar statistics.</p>
<hr />
<p>Setting aside Shier’s recursive algorithm, all of the patterns we’ve been looking at are generated by a power law (or zeta function), with the crucial requirement that the series must converge to a finite sum. The world of mathematics offers many other convergent series in addition to power laws. Could some of them also create fulfilling patterns? The question is one that Ennis discusses briefly in his talk at St. Olaf and that Shier also mentions. </p>
<p>Among the obvious candidates are geometric series such as \(\frac{1}{1}, \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \dots\) A geometric series is a close cousin of a power law, defined in a similar way but exchanging the roles of \(s\) and \(k\). That is, a geometric series is the sum:</p>
<p>\[\sum_{k=0}^{\infty} \frac{1}{s^k} = \frac{1}{s^0} + \frac{1}{s^1} + \frac{1}{s^2} + \frac{1}{s^3} + \cdots\]</p>
<p class="undent">For any \(s &gt; 1\), the infinite geiometric series has a finite sum, namely \(\frac{s}{s - 1}\). Thus our task is to construct an infinite set of disks with individual areas \(1/s^k\) that we can pack into a square of area \(\frac{s}{s - 1}\). Can we find a range of \(s\) for which the series is fulfilling? As it happens, this is where Shier began his adventures; his first attempts were not with power laws but with geometric series. They didn’t turn out well. You are welcome to try your own hand in Program 11.</p>
<h2 class="zf">Program 11: Disk Sizes from Geometric Series</h2>
<div class="canvas-container canvas-failure" id="geometric">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p>There’s a curious pattern to the failures you’ll see in this program. No matter what value you assign to \(s\) (within the available range \(1 \lt s \le 2\)), the system jams when the number of disks reaches the neighborhood of \(A_\square = \frac{s}{s-1}\). <img src="http://bit-player.org/wp-content/uploads/2019/09/log-log-powerlaw-and-geom.svg" height="" width="" alt="Log log powerlaw and geom" border="0" class="alignright" />For example, at \(s = 1.01\), \(\frac{s}{s - 1}\) is 101 and the program typically gets stuck somewhere between \(k = 95\) and \(k = 100\). At \(s = 1.001\), \(\frac{s}{s - 1}\) is \(1{,}001\) and there’s seldom progress beyond about \(k = 1,000\). </p>
<p>For a clue to what’s going wrong here, consider the graph at right, plotting the values of \(1 / k^s\) <em>(red)</em> and \(1 / s^k\) <em>(blue)</em> for \(s = 1.01\). These two series converge on nearly the same sum (roughly \(100\)), but they take very different trajectories in getting there. On this log-log plot, the power-law series \(1 / s^k\) is a straight line. The geometric series \(1 / s^k\) falls off much more slowly at first, but there’s a knee in the curve at about \(k = 100\) <em>(dashed mauve line)</em>, where it steepens dramatically. If only we could get beyond this turning point, it looks like the rest of the filling process would be smooth sledding, but in fact we never get there. Whereas the first \(100\) disks of the power-law series fill up only about \(5\) percent of the available area, they occuy 63 percent in the geometric case. This is where the filling process stalls.</p>
<p>Even in one dimension, the geometric series quickly succumbs. (This is in sharp contrast to the one-dimensional power-law model, where any \(s\) between \(1\) and \(2\) yields a provably infinite progression of disks.)</p>
<h2 class="zf">Program 12: Disk Sizes from Geometric Series in One Dimension</h2>
<div class="canvas-container canvas-failure" id="geometric-1d">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">And just in case you think I’m pulling a fast one here, let me demonstrate that those same one-dimensional disks will indeed fit in the available space, if packed efficiently. In Program 13 they are placed in order of size from left to right.</p>
<h2 class="zf">Program 13: Deterministic One-Dimensional Geometric Series</h2>
<div class="canvas-container canvas-failure" id="deterministic-geom-1d">
<p>Sorry, the program will not run in this browser.</p>
</div>
<p class="indent">I have made casual attempts to find fulfillment with a few other convergent series, such as the reciprocals of the Fibonacci numbers (which converge to about \(3.36\)) and the reciprocals of the factorials (whose sum is \(e \approx 2.718\)). Both series jam after the first disk. There are plenty of other convergent series one might try, but I doubt this is a fruitful line of inquiry.</p>
<hr />
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/Shier-randomly-oriented-squares-600px.png" height="300" width="300" alt="Shier randomly oriented squares 600px" border="0" class="alignleft" />All the variations discussed above leave one important factor unchanged: The objects being fitted together are all circular. Exploring the wider universe of shapes has been a major theme of Shier’s work. He asks: What properties of a shape make it suitable for forming a statistical fractal pattern? And what shapes (if any) refuse to cooperate with this treatment? (The images in this section were created by John Shier and are reproduced here with his per­mission.)</p>
<p>Shier’s first experiments were with circular disks and axis-parallel squares; the filling algorithm worked splendidly in both cases. He also succeeded with axis-parallel rectangles of various aspect ratios, even when he mixed vertical and horizontal orientations in the same tableau. In collaboration with Paul Bourke he tried randomizing the orientation of squares as well as their positions. Again the outcome was positive, as the illustration above left shows.</p>
<p>Equilateral triangles were less cooperative, and at first Shier believed the algorithm would consistently fail with this shape. The triangles tended to form orderly arrays with the sharp point of one triangle pressed close against the broad side of another, leaving little “wiggle room.” <img src="http://bit-player.org/wp-content/uploads/2019/09/one_and_nines-transparent-300px.png" style="float: right;" height="600" width="300" alt="One and nines transparent 300px" border="0" class="alignright" />Further efforts showed that the algorithm was not truly getting stuck but merely slowing down. With an appropriate choice of parameters in the Hurwitz zeta function, and with enough patience, the triangles did come together in boundlessly extendable space-filling patterns.  </p>
<p>The casual exploration of diverse shapes eventually became a deliberate quest to stake out the limits of the space-filling process. Surely there must be <em>some</em> geometric forms that the algorithm would balk at, failing to pack an infinite number of objects into a finite area. Perhaps nonconvex shapes such as stars and snowflakes and flowers would expose a limitation—but no, the algorithm worked just fine with these figures, fitting smaller stars into the crevices between the points of larger stars. The next obvious test was “hollow” objects, such as annular rings, where an internal void is not part of the object and is therefore available to be filled with smaller copies. The image at right is my favorite example of this phenomenon. The bowls of the larger nines have smaller nines within them. It’s nines all the way down. When we let the process continue indefinitely, we have a whimsical visual proof of the proposition that \(.999\dots = 1\).</p>
<p>These successes with nonconvex forms and objects with holes led to an <em>Aha</em> moment, as Shier describes it. The search for a shape that would break the algorithm gave way to a suspicion that no such shape would be found, and then the suspicion gradually evolved into a conviction that any “reasonably compact” object is suitable for the <em>Fractalize That!</em> treatment. The phrase “reasonably compact” would presumably exclude shapes that are in fact dispersed sets of points, such as <a href="http://www.2dcurves.com/fractal/fractald.html">Cantor dust</a>. But Shier has shown that shapes formed of disconnected pieces, such as the words in the pair of images below, present no special difficulty.</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/09/MATH-and-ART-1280px.png" height="308" width="638" alt="MATH and ART 1280px" border="0" class="centered" /></p>
<p><em>Fractalize That!</em> is not all geometry and number theory. Shier is eager to explain the mathematics behind these curious patterns, but he also presents the algorithm as a tool for self-expression. MATH and ART both have their place.</p>
<hr />
<p>Finally, I offer some notes on what’s needed to turn these algorithms into computer programs. Shier’s book includes a chapter for do-it-yourselfers that explains his strategy and provides some crucial snippets of code (written in C). My own source code (in JavaScript) is available on <a href="https://github.com/bit-player/dotster">GitHub</a>. And if you’d like to play with the programs without all the surrounding verbiage, try the <a href="https://bit-player.github.io/dotster/">GitHub Pages version</a>.</p>
<p>The inner loop of a typical program looks something like this:</p>
<pre><code class="language-js">let attempt = 1;
while (attempt &lt;= maxAttempts) {
    disk.x = randomCoord();
    disk.y = randomCoord();
    if (isNotOverlapping(disk)) {
        return disk;
    }
    attempt++;
}
return false;</code></pre>
<p>We generate a pair of random \(x\) and \(y\) coordinates, which mark the center point of the new disk, and check for overlaps with other disks already in place. If no overlaps are discovered, the disk stays put and the program moves on. Otherwise the disk is discarded and we jump back to the top of the loop to try a new \(xy\) pair. </p>
<p>The main computational challenge lies in testing for overlaps. For any two specific disks, the test is easy enough: They overlap if the sum of their radii is greater than the distance between their centers. The problem is that the test might have to be repeated many millions of times. My program makes \(10\) million attempts to place a disk before giving up. If it has to test for overlap with \(100{,}000\) other disks on each attempt, that’s a trillion tests. A trillion is too many for an interactive program where someone is staring at the screen waiting for things to happen. To speed things up a little I divide the square into a \(32 \times 32\) grid of smaller squares. The largest disks—those whose diameter is greater than the width of a grid cell—are set aside in a special list, and all new candidate disks are checked for overlap with them. Below this size threshold, each disk is allocated to the grid cell in which its center lies. A new candidate is checked against the disks in its own cell and in that cell’s eight neighbors. The net result is an improvement by two orders of magnitude—lowering the worst-case total from \(10^{12}\) overlap tests to about \(10{10}\).</p>
<p>All of this works smoothly with circular disks. Devising overlap tests for the variety of shapes that Shier has been working with is much harder.</p>
<p>From a theoretical point of view, the whole rigmarole of overlap testing is hideously wasteful and unnecessary. If the box is already 90 percent full, then we know that 90 percent of the random probes will fail. A smarter strategy would be to generate random points only in the “black zone” where new disks can legally be placed. If you could do that, you would never need to generate more than one point per disk, and there’d be no need to check for overlaps. But keeping track of the points that comprise the black zone—scattered throughout multiple, oddly shaped, transient regions—would be a serious exercise in computational geometry.</p>
<p>For the actual drawing of the disks, Shier relies on the technology known as SVG, or scalable vector graphics. As the name suggests, these drawings retain full resolution at any size, and they are definitely the right choice if you want to create works of art. They are less suitable for the interactive programs embedded in this document, mainly because they consume too much memory. The images you see here rely on the HTML <em>canvas</em> element, which is simply a fixed-size pixel array.</p>
<p>Another point of possible interest is the evaluation of the zeta function. If we want to scale the disk sizes to match the box size (or vice versa), we need to compute a good approximation of the Riemann function \(\zeta(s)\) or the Hurwitz function \(\zeta(s, a)\). I didn’t know how to do that, and most of the methods I read about seemed overwhelming. Before I could get to zeta, I’d have to hack my way through thickets of polygamma functions and Stieltjes constants. For the Riemann zeta function I found a somewhat simpler algorithm published by Peter Borwein in 1995. It’s based on a polynomial approximation that yields ample precision and runs in less than a millisecond. For the Hurwitz zeta function I stayed with a straightforward translation of Shier’s code, which takes more of a brute-force approach. (There are alternatives for Hurwitz too, but I couldn’t understand them well enough to make them work.)</p>
<p>The JavaScript file in the <a href="https://github.com/bit-player/dotster">GitHub repository</a> has more discussion of implementation details.</p>
<hr />
<h2>Bibliography: Works by John Shier and colleagues</h2>
<p class="biblio">Shier, John. 2018. <em>Fractalize That! A Visual Essay on Statistical Geometry</em>. Singapore: World Scientific. <a href="https://www.worldscientific.com/worldscibooks/10.1142/11126">Publisher’s website</a>.</p>
<p class="biblio">Shier, John. Website: <a href="http://www.john-art.com/">http://www.john-art.com/</a></p>
<p class="biblio">Shier, John. 2011. The dimensionless gasket width \(b(c,n)\) in statistical geometry. <a href="http://www.john-art.com/gasket_width.pdf">http://www.john-art.com/gasket_width.pdf</a></p>
<p class="biblio">Shier, John. 2012. Random fractal filling of a line segment. <a href="http://www.john-art.com/gasket_width.pdf">http://www.john-art.com/gasket_width.pdf</a></p>
<p class="biblio">Dunham, Douglas, and John Shier. 2014. The art of random fractals. In <em>Proceedings of Bridges 2014: Mathematics, Music, Art, Architecture, Culture</em> pp. 79–86. <a href="http://archive.bridgesmathart.org/2014/bridges2014-79.pdf">PDF</a>.</p>
<p class="biblio">Shier, John. 2015. A new recursion for space-filling geometric fractals. <a href="http://www.john-art.com/gasket_width.pdf">http://www.john-art.com/gasket_width.pdf</a></p>
<p class="biblio">Dunham, Douglas, and John Shier. 2015. An algorithm for creating aesthetic random fractal patterns. Talk delivered at the Joint Mathematics Meetings January 2015, San Antonio, Texas. </p>
<p class="biblio">Dunham, Douglas, and John Shier. 2018. A property of area and perimeter. In <em>ICGG 2018: Proceedings of the 18th International Conference on Geometry and Graphics</em>, Milano, August 2018, pp. 228–237.</p>
<p class="biblio">Dunham, Douglas, and John Shier. 2017. New kinds of fractal patterns. In <em>Proceedings of Bridges 2017: Mathematics, Art, Music, Architecture, Education, Culture</em>,<br />
pp. 111–116. <a href="https://www.d.umn.edu/~ddunham/bridges17.pdf">Preprint</a>.</p>
<p class="biblio">Shier, John, and Paul Bourke. 2013. An algorithm for random fractal filling of space. <em>Computer Graphics Forum</em> 32(8):89–97. <a href="http://archive.bridgesmathart.org/2017/bridges2017-111.pdf">PDF</a>. <a href="http://www.john-art.com/Shier_Bourke_paper.pdf">Preprint</a>.</p>
<h2>Proof</h2>
<p class="biblio">Ennis, Christopher. 2016. (Always) room for one more. <em>Math Horizons</em> 23(3):8–12. <a href="https://www.tandfonline.com/doi/abs/10.4169/mathhorizons.23.3.8">PDF</a> (paywalled).</p>
<h2>Apollonian circles</h2>
<p class="biblio">Dodds, Peter Sheridan, and Joshua S. Weitz. 2002. Packing-limited growth. Physical Review E 65: 056108.</p>
<p class="biblio">Lagarias, Jeffrey C., Colin L. Mallows, and Allan R. Wilks. 2001. Beyond the Descartes circle theorem. <a href="https://arxiv.org/abs/math/0101066">https://arxiv.org/abs/math/0101066</a>. (Also published in <em>American Mathematical Monthly</em>, 2002, 109:338–361.)</p>
<p class="biblio">Mackenzie, Dana. 2010. A tisket, a tasket, an Apollonian gasket. <em>American Scientist</em> 98:10–14. <a href="https://www.americanscientist.org/article/a-tisket-a-tasket-an-apollonian-gasket">https://www.americanscientist.org/article/a-tisket-a-tasket-an-apollonian-gasket</a>.</p>
<p class="biblio">Manna, S. S. 1992. Space filling tiling by random packing of discs. <em>Physica A</em> 187:373–377.</p>
<h2>Zeta functions</h2>
<p class="biblio">Bailey, David H., and Jonathan M. Borwein. 2015. Crandall’s computation of the incomplete Gamma function and the Hurwitz zeta function, with applications to Dirichlet L-series. <em>Applied Mathematics and Computation</em>, 268, 462–477.</p>
<p class="biblio">Borwein, Peter. 1995. An efficient algorithm for the Riemann zeta function. http://www.cecm.sfu.ca/personal/pborwein/PAPERS/P155.pdf</p>
<p class="biblio">Coffey, Mark W. 2009. An efficient algorithm for the Hurwitz zeta and related functions. <em>Journal of Computational and Applied Mathematics</em> 225:338–346.</p>
<p class="biblio">Hurwitz, Adolf. 1882. Einige Eigenschaften der Dirichletschen Funktionen \(F(s) = \sum \left(\frac{D}{n} \frac{1}{n^s}\right)\), die bei der Bestimmung der Klassenzahlen binärer quadratischer Formen auftreten. <em>Zeitschrift für Mathematik und Physik</em> 27:86–101. <a href="https://gdz.sub.uni-goettingen.de/id/PPN599415665_0027">https://gdz.sub.uni-goettingen.de/id/PPN599415665_0027</a>.</p>
<p class="biblio">Oswald, Nicola, and Jörn Steuding. 2015. Aspects of zeta-function theory in the mathematical works of Adolf Hurwitz. <a href="https://arxiv.org/abs/1506.00856">https://arxiv.org/abs/1506.00856</a>.</p>
<p class="biblio">Xu, Andy. 2018. Approximating the Hurwitz zeta function. <a href="https://math.mit.edu/research/highschool/primes/materials/2018/Xu.pdf">PDF</a>.</p>
<p><br />
</p></div>







<p class="date">
by Brian Hayes <a href="http://bit-player.org/2019/my-god-its-full-of-dots"><span class="datestr">at September 27, 2019 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
