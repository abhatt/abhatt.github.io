<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" class="message" title="internal server error">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;redirect=false" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 27, 2022 05:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=629">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/07/26/tcs-we-want-your-feedback/">TCS+: We want YOU(R feedback)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>For the new season of TCS+, the organizers’ team has been discussing a range of suggestions to improve our general format, and adapt it to the all-Zoom, COVID/post-COVID world. We would love your feedback on some of these ideas — if you could take a few minutes to answer the following survey (4-5 questions), this would be greatly appreciated!</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4cb.png" style="height: 1em;" class="wp-smiley" alt="📋" /> <a href="https://docs.google.com/forms/d/e/1FAIpQLSeNhfAeGsbCWHfZpoH3b2A3_pb1iD2h_H4YTgsiomhKNldmZA/viewform">Link to the survey</a> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/07/26/tcs-we-want-your-feedback/"><span class="datestr">at July 27, 2022 02:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/26/two-2-year-postdocs-at-university-of-oxford-apply-by-september-12-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/26/two-2-year-postdocs-at-university-of-oxford-apply-by-september-12-2022/">Two 2-year postdocs at University of Oxford (apply by September 12, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two postdoc positions are available at Oxford as part of the UKRI Consolidator Grant “New Approaches to Approximability of Satisfiable Problems” led by Standa Zivny. Strong candidates with any background in maths or theoretical computer science will be considered. Background in approximation algorithms, universal algebra, category theory, combinatorics, or topology would be particularly useful.</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/2075-full.html">http://www.cs.ox.ac.uk/news/2075-full.html</a><br />
Email: standa.zivny@cs.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/26/two-2-year-postdocs-at-university-of-oxford-apply-by-september-12-2022/"><span class="datestr">at July 26, 2022 01:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/26/assistant-professor-3-posts-at-heriot-watt-university-uk-apply-by-august-15-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/26/assistant-professor-3-posts-at-heriot-watt-university-uk-apply-by-august-15-2022/">Assistant Professor (3 posts) at Heriot-Watt University, UK (apply by August 15, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for three assistant professors (T&amp;R) in Computer Science. The vacancy is not restricted to applicants working in any specific area, but the research of the successful candidate will fall into at least one of three themes the department organises itself around: interactive systems, intelligent systems, and rigorous systems.</p>
<p>Website: <a href="https://enzj.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/2282/?utm_medium=jobshare">https://enzj.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/2282/?utm_medium=jobshare</a><br />
Email: j.hage@hw.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/26/assistant-professor-3-posts-at-heriot-watt-university-uk-apply-by-august-15-2022/"><span class="datestr">at July 26, 2022 08:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11832">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11832">New Additive Spanner Lower Bounds by an Unlayered Obstacle Product</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodwin:Greg.html">Greg Bodwin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoppenworth:Gary.html">Gary Hoppenworth</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11832">PDF</a><br /><b>Abstract: </b>For an input graph $G$, an additive spanner is a sparse subgraph $H$ whose
shortest paths match those of $G$ up to small additive error. We prove two new
lower bounds in the area of additive spanners:
</p>
<p>1) We construct $n$-node graphs $G$ for which any spanner on $O(n)$ edges
must increase a pairwise distance by $+\Omega(n^{1/7})$. This improves on a
recent lower bound of $+\Omega(n^{1/10.5})$ by Lu, Wein, Vassilevska Williams,
and Xu [SODA '22].
</p>
<p>2) A classic result by Coppersmith and Elkin [SODA '05] proves that for any
$n$-node graph $G$ and set of $p = O(n^{1/2})$ demand pairs, one can exactly
preserve all pairwise distances among demand pairs using a spanner on $O(n)$
edges. They also provided a lower bound construction, establishing that that
this range $p = O(n^{1/2})$ cannot be improved. We strengthen this lower bound
by proving that, for any constant $k$, this range of $p$ is still unimprovable
even if the spanner is allowed $+k$ additive error among the demand pairs. This
negatively resolves an open question asked by Coppersmith and Elkin [SODA '05]
and again by Cygan, Grandoni, and Kavitha [STACS '13] and Abboud and Bodwin
[SODA '16].
</p>
<p>At a technical level, our lower bounds are obtained by an improvement to the
entire obstacle product framework used to compose ``inner'' and ``outer''
graphs into lower bound instances. In particular, we develop a new strategy for
analysis that allows certain non-layered graphs to be used in the product, and
we use this freedom to design better inner and outer graphs that lead to our
new lower bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11832"><span class="datestr">at July 26, 2022 10:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11824">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11824">Contention Resolution for Coded Radio Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bender:Michael_A=.html">Michael A. Bender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gilbert:Seth.html">Seth Gilbert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:John.html">John Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=eacute=dard:Muriel.html">Muriel Médard</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11824">PDF</a><br /><b>Abstract: </b>Randomized backoff protocols, such as exponential backoff, are a powerful
tool for managing access to a shared resource, often a wireless communication
channel (e.g., [1]). For a wireless device to transmit successfully, it uses a
backoff protocol to ensure exclusive access to the channel. Modern radios,
however, do not need exclusive access to the channel to communicate; in
particular, they have the ability to receive useful information even when more
than one device transmits at the same time. These capabilities have now been
exploited for many years by systems that rely on interference cancellation,
physical layer network coding and analog network coding to improve efficiency.
For example, Zigzag decoding [56] demonstrated how a base station can decode
messages sent by multiple devices simultaneously.
</p>
<p>In this paper, we address the following question: Can we design a backoff
protocol that is better than exponential backoff when exclusive channel access
is not required. We define the Coded Radio Network Model, which generalizes
traditional radio network models (e.g., [30]). We then introduce the Decodable
Backoff Algorithm, a randomized backoff protocol that achieves an optimal
throughput of $1-o(1)$. (Throughput $1$ is optimal, as simultaneous reception
does not increase the channel capacity.) The algorithm breaks the constant
throughput lower bound for traditional radio networks [47-49], showing the
power of these new hardware capabilities.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11824"><span class="datestr">at July 26, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11818">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11818">Towards An Optimal Solution to Place Bistatic Radars for Belt Barrier Coverage with Minimum Cost</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Tu N. Nguyen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Bing=Hong.html">Bing-Hong Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thai:My_T=.html">My T. Thai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Djordjevic:Ivan.html">Ivan Djordjevic</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11818">PDF</a><br /><b>Abstract: </b>With the rapid growth of threats, sophistication and diversity in the manner
of intrusion, traditional belt barrier systems are now faced with a major
challenge of providing high and concrete coverage quality to expand the
guarding service market. Recent efforts aim at constructing a belt barrier by
deploying bistatic radar(s) on a specific line regardless of the limitation on
deployment locations, to keep the width of the barrier from going below a
specific threshold and the total bistatic radar placement cost is minimized,
referred to as the Minimum Cost Linear Placement (MCLP) problem. The existing
solutions are heuristic, and their validity is tightly bound by the barrier
width parameter that these solutions only work for a fixed barrier width value.
In this work, we propose an optimal solution, referred to as the Opt_MCLP, for
the "open MCLP problem" that works for full range of the barrier width. Through
rigorous theoretical analysis and experimentation, we demonstrate that the
proposed algorithms perform well in terms of placement cost reduction and
barrier coverage guarantee.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11818"><span class="datestr">at July 26, 2022 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11591">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11591">The discriminating power of the generalized rank invariant</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Nate Clause, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Woojin.html">Woojin Kim</a>, Facundo Memoli <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11591">PDF</a><br /><b>Abstract: </b>It is a well-known fact that there is no complete and discrete invariant on
the collection of all multiparameter persistence modules. Nonetheless, many
invariants have been proposed in the literature to study multiparameter
persistence modules, though each invariant will lose some amount of
information. One such invariant is the generalized rank invariant. This
invariant is known to be complete on the class of interval decomposable
persistence modules in general, under mild assumptions on the indexing poset
$P$.
</p>
<p>There is often a trade-off, where the stronger an invariant is, the more
expensive it is to compute in practice. The generalized rank invariant on its
own is difficult to compute, whereas the standard rank invariant is readily
computable through software implementations such as RIVET. We can interpolate
between these two to induce new invariants via restricting the domain of the
generalized rank invariant, and this family exhibits the aforementioned
trade-off. This work studies the tension which exists between computational
efficiency and retaining strength when restricting the domain of the
generalized rank invariant. We provide a characterization result on where such
restrictions are complete invariants in the setting where $P$ is finite, and
furthermore show that such restricted generalized rank invariants are stable.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11591"><span class="datestr">at July 26, 2022 11:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11489">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11489">Average-Case to (shifted) Worst-Case Reduction for the Trace Reconstruction Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinstein:Ittai.html">Ittai Rubinstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11489">PDF</a><br /><b>Abstract: </b>The {\em insertion-deletion channel} takes as input a binary string $x
\in\{0, 1\}^n$, and outputs a string $\widetilde{x}$ where some of the bits
have been deleted and others inserted independently at random. In the {\em
trace reconstruction problem}, one is given many outputs (called {\em traces})
of the insertion-deletion channel on the same input message $x$, and asked to
recover the input message.
</p>
<p>Nazarov and Peres (STOC 2017), and De, Odonnell and Servido (STOC 2017)
showed that any string $x$ can be reconstructed from $\exp(O(n^{1/3}))$ traces.
Holden, Pemantle, Peres and Zhai (COLT 2018) adapt the techniques used to prove
this upper bound, to an algorithm for the average-case trace reconstruction
with a sample complexity of $\exp(O(\log^{1/3} n))$. However, it is not clear
how to apply their techniques more generally and in particular for the recent
worst-case upper bound of $\exp(\widetilde{O}(n^{1/5}))$ shown by Chase (STOC
2021) for the deletion-channel.
</p>
<p>We prove a general reduction from the average-case to smaller instances of a
problem similar to worst-case. Using this reduction and a generalization of
Chase's bound, we construct an improved average-case algorithm with a sample
complexity of $\exp(\widetilde{O}(\log^{1/5} n))$. Additionally, we show that
Chase's upper-bound holds for the insertion-deletion channel as well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11489"><span class="datestr">at July 26, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11479">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11479">3D Labeling Tool</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rachwan:John.html">John Rachwan</a>, Charbel Zalaket <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11479">PDF</a><br /><b>Abstract: </b>Training and testing supervised object detection models require a large
collection of images with ground truth labels. Labels define object classes in
the image, as well as their locations, shape, and possibly other information
such as pose. The labeling process has proven extremely time consuming, even
with the presence of manpower. We introduce a novel labeling tool for 2D images
as well as 3D triangular meshes: 3D Labeling Tool (3DLT). This is a standalone,
feature-heavy and cross-platform software that does not require installation
and can run on Windows, macOS and Linux-based distributions. Instead of
labeling the same object on every image separately like current tools, we use
depth information to reconstruct a triangular mesh from said images and label
the object only once on the aforementioned mesh. We use registration to
simplify 3D labeling, outlier detection to improve 2D bounding box calculation
and surface reconstruction to expand labeling possibility to large point
clouds. Our tool is tested against state of the art methods and it greatly
surpasses them in terms of speed while preserving accuracy and ease of use.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11479"><span class="datestr">at July 26, 2022 11:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11451">Optimization of the Shape of a Hydrokinetic Turbine's Draft Tube and Hub Assembly Using Design-by-Morphing with Bayesian Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sheikh:Haris_Moazam.html">Haris Moazam Sheikh</a>, Tess A. Callan, Kealan J. Hennessy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marcus:Philip_S=.html">Philip S. Marcus</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11451">PDF</a><br /><b>Abstract: </b>Finding the optimal design of a hydrodynamic or aerodynamic surface is often
impossible due to the expense of evaluating the cost functions (say, with
computational fluid dynamics) needed to determine the performances of the flows
that the surface controls. In addition, inherent limitations of the design
space itself due to imposed geometric constraints, conventional
parameterization methods, and user bias can restrict {\it all} of the designs
within a chosen design space regardless of whether traditional optimization
methods or newer, data-driven design algorithms with machine learning are used
to search the design space. We present a 2-pronged attack to address these
difficulties: we propose (1) a methodology to create the design space using
morphing that we call {\it Design-by-Morphing} (DbM); and (2) an optimization
algorithm to search that space that uses a novel Bayesian Optimization (BO)
strategy that we call {\it Mixed variable, Multi-Objective Bayesian
Optimization} (MixMOBO). We apply this shape optimization strategy to maximize
the power output of a hydrokinetic turbine. Applying these two strategies in
tandem, we demonstrate that we can create a novel, geometrically-unconstrained,
design space of a draft tube and hub shape and then optimize them
simultaneously with a {\it minimum} number of cost function calls. Our
framework is versatile and can be applied to the shape optimization of a
variety of fluid problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11451"><span class="datestr">at July 26, 2022 11:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11337">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11337">Fair Range k-center</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Huy Lê Nguyen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Thy.html">Thy Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Matthew.html">Matthew Jones</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11337">PDF</a><br /><b>Abstract: </b>We study the problem of fairness in k-centers clustering on data with
disjoint demographic groups. Specifically, this work proposes a variant of
fairness which restricts each group's number of centers with both a lower bound
(minority-protection) and an upper bound (restricted-domination), and provides
both an offline and one-pass streaming algorithm for the problem. In the
special case where the lower bound and the upper bound is the same, our offline
algorithm preserves the same time complexity and approximation factor with the
previous state-of-the-art. Furthermore, our one-pass streaming algorithm
improves on approximation factor, running time and space complexity in this
special case compared to previous works. Specifically, the approximation factor
of our algorithm is 13 compared to the previous 17-approximation algorithm, and
the previous algorithms' time complexities have dependence on the metric
space's aspect ratio, which can be arbitrarily large, whereas our algorithm's
running time does not depend on the aspect ratio.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11337"><span class="datestr">at July 26, 2022 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.11299">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.11299">Rank-constrained Hyperbolic Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Zhen.html">Zhen Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lim:Lek=Heng.html">Lek-Heng Lim</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.11299">PDF</a><br /><b>Abstract: </b>We extend rank-constrained optimization to general hyperbolic programs (HP)
using the notion of matroid rank. For LP and SDP respectively, this reduces to
sparsity-constrained LP and rank-constrained SDP that are already well-studied.
But for QCQP and SOCP, we obtain new interesting optimization problems. For
example, rank-constrained SOCP includes weighted Max-Cut and nonconvex QP as
special cases, and dropping the rank constraints yield the standard
SOCP-relaxations of these problems. We will show (i) how to do rank reduction
for SOCP and QCQP, (ii) that rank-constrained SOCP and rank-constrained QCQP
are NP-hard, and (iii) an improved result for rank-constrained SDP showing that
if the number of constraints is $m$ and the rank constraint is less than
$2^{1/2-\epsilon} \sqrt{m}$ for some $\epsilon&gt;0$, then the problem is NP-hard.
We will also study sparsity-constrained HP and extend results on LP
sparsification to SOCP and QCQP. In particular, we show that there always exist
(a) a solution to SOCP of cardinality at most twice the number of constraints
and (b) a solution to QCQP of cardinality at most the sum of the number of
linear constraints and the sum of the rank of the matrices in the quadratic
constraints; and both (a) and (b) can be found efficiently.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.11299"><span class="datestr">at July 26, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6741">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/rethinking-sgd-noise/">Rethinking SGD’s noise</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="has-text-align-justify">It seemed a bit unfair to devote a blog to machine learning (ML) without talking about its current core algorithm: stochastic gradient descent (SGD). Indeed, SGD has become, year after year, the basic foundation of many algorithms used for large-scale ML problems.  However, the history of stochastic approximation is much older than that of ML: its first study by Robbins and Monro [<a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full" target="_blank" rel="noreferrer noopener">1</a>] dates back to 1951. Their aim was to find the zeros of a function that can only be accessed through noisy measurements; and this set-up was applied (and studied!) later on for many problems [<a href="https://link.springer.com/book/9783540606994" target="_blank" rel="noreferrer noopener">2</a>]. Stochastic gradient descent, in its most general definition, is simply the application of the Robbins and Monro’s procedure to find the zeros of the <em>gradient</em> of a function \(f\).</p>



<p class="has-text-align-justify">In this post, I will try to show that the instance of SGD used to solve modern ML problems carries rich particularities. In particular, we will put emphasis on the difference between the under and overparametrised regimes. To provide prototypical representations of these, we will try to cast the typical dynamics of each regime as a known stochastic process.</p>



<h2 class="has-text-align-left" id="fun-and-useful-facts-and-common-mistakes">General set-up</h2>



<p class="has-text-align-justify justify-text"><strong>Stochastic approximation.</strong> At each time \(t \in \mathbb{N}\) of the procedure, let us suppose that we only have access to an unbiased estimate of the gradient, \(\nabla f_t\), of a function \(f\) we want to minimise. More formally this means that given the information of the first \(t-1\) steps, denoted by \(\mathcal{F}_{t-1}\), the estimate we get at time \(t\) is centred around the true gradient: \(\displaystyle \mathbb{E}\left[ \nabla f_t (\theta_{t-1}) | \mathcal{F}_{t-1} \right] = \nabla f (\theta_{t-1})\). Then, the SGD iterates with step-sizes \((\gamma_t)_{t \in \mathbb{N}}\), and initialised at \(\theta_{t=0} = \theta_0\), write $$\theta_t = \theta_{t-1} – \gamma_t \nabla f_t (\theta_{t-1}).$$ To put the emphasis on the noise induced by the noisy estimates of the true gradient, we prefer sometimes to rephrase the recursion in term of the conditional zero-mean noise sequence \(\varepsilon_t = \nabla f – \nabla f_t\). $$\theta_t = \theta_{t-1} – \gamma_t \nabla f (\theta_{t-1}) + \gamma_t\varepsilon_t(\theta_{t-1}).$$ As we will see later, to analyse this discrete time stochastic dynamics, we crucially need to understand the behaviour of \((\varepsilon_t)_{t \in \mathbb{N}}\).</p>



<p class="has-text-align-justify"><strong>Supervised learning reformulation</strong>. This general recursion can be applied to many settings and fits particularly well to the supervised learning framework. In this case, the function to minimise is the empirical risk $$\mathcal{R}_n(\theta)=\mathbb{E}_{(X,Y)\sim\widehat{\rho}_n}\left[\ell((X,Y), \theta)\right] = \frac{1}{n}\sum_{i=1}^n \ell((x_i,y_i), \theta),$$ where \(\widehat{\rho}_n:= \frac{1}{n}\sum_{i=1}^n \delta_{(x_i,y_i)}\) is the empirical measure associated to the samples \((x_i, y_i)_{1 \leqslant i \leqslant n}\). We can derive an unbiased estimate \(\nabla_\theta \ell((x_{i_t},y_{i_t}), \theta)\) of its gradient where \((i_t)_t\) is the sequence of uniformly sampled indices over \(\{1,…,n\}\). The recursion reads: $$\theta_t = \theta_{t-1} – \gamma_t \nabla_\theta \ell((x_{i_t},y_{i_t}), \theta_{t-1}).$$ However, one of the real power of SGD is that it can be seen as a direct <em>stochastic</em> gradient method to optimise the true risk [<a href="https://proceedings.neurips.cc/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html" target="_blank" rel="noreferrer noopener">3</a>]. Indeed, recall that the true risk is \(\mathcal{R}(\theta) = \mathbb{E}_\rho\left[\ell((X,Y), \theta)\right]\) and consider an input/output sample pair \((x_i,y_i)\) drawn from \(\rho\). Now, \(\ell((x_i,y_i), \theta)\) is an unbiased estimate of the true risk \(\mathcal{R}(\theta)\) such that \(\nabla_\theta \ell((x_i,y_i), \theta)\) is an unbiased estimate of its  gradient. Hence, if we denote \(\mathcal{F}_t = \sigma((x_i,y_i), \ i\leqslant t)\), then the stochastic gradient descent optimises \(\mathcal{R}\) as long as new points \((x,y)\) are added in the data set  $$\theta_t = \theta_{t-1} – \gamma_t \nabla_\theta \ell((x_t,y_t), \theta_{t-1}).$$ This reveals the real strength of SGD against other types of gradient descent algorithm: beyond its low computational cost, as long as we use <em>unseen data</em>, SGD optimises <em>directly</em> <em>the true risk</em> although it is an <em>a priori</em> unknown function. As a consequence, the SGD algorithm, when using only fresh samples, cannot overfit the dataset and does not need any regularisation.</p>



<h2 id="matrix-functions">The noise of SGD in practical ML setting</h2>



<p class="has-text-align-justify">The story outlined in the previous paragraph has been very popular to explain the success of SGD. However, nowadays, the number of samples is usually way smaller than the number of iterations performed, i.e. \(t \gg n\): several <em>epochs</em> are made over the dataset and the fact that SGD is a stochastic approximation algorithm that minimises directly the true risk doesn’t hold any longer. Hence, from now on, let us set the <strong>data to \(n\) input/output pairs</strong> \((x_i,y_i)_{1 \leqslant i \leqslant n}\). </p>



<p class="has-text-align-justify">First, let us briefly present certain crucial properties of SGD in this setting (see also the nice paper [<a href="https://arxiv.org/pdf/2105.01650.pdf" target="_blank" rel="noreferrer noopener">8</a>] for more details). We take a parameterised family of predictors \(\{x \mapsto h(\theta,x), \ \text{for } \theta \in \Theta\}\), and assume that \(\ell\) is the square-loss. The empirical risk classically writes \(\mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n (h(\theta,x_i)\, – y_i)^2\) and the SGD recursion can be rewritten as $$ \theta_t = \theta_{t-1} – \gamma \nabla \mathcal{R}_n (\theta_{t-1}) + \gamma \varepsilon_t(\theta_{t-1}), $$ where \(\varepsilon_t(\theta)\) is the noise term at time \(t\), which writes $$ \varepsilon_t(\theta):= \frac{1}{n} \sum_{j = 1}^n r_j(\theta) \nabla_\theta h(\theta, x_j)\, – r_{i_t}(\theta) \nabla_\theta h(\theta, x_{i_t}), $$ where for all \(i \in \{1,…,n \}\), we define the \(i\)-th residual \(r_i(\theta) = h(\theta,x_i)\, – y_i\). Note that while the expression written requires that the loss is the square-loss, the conclusions of this section extend to different losses (such as the logistic).</p>



<p class="has-text-align-justify"><strong>Multiplicative noise and additive noise.</strong> Let us call \(\theta^*\) any vector belonging to the set of critical points of the loss \(\mathcal{R}_n\), that is \(\nabla \mathcal{R}_n(\theta^*) = 0\) (note that, in the convex case, it is exactly the set of global minima of the loss). We say that we are in the <strong>“overparametrised” regime</strong> if there exists at least one interpolator which fits the data set, i.e. if there exists \(\theta^*\) such that \(R_n( \theta^*)=0\). Otherwise, we say that we are in the <strong>“underparametrised” regime</strong>. We now decompose the noise as the sum of two different noises, that will contribute differently to the dynamics: $$\varepsilon_t(\theta) = \underbrace{\varepsilon_t(\theta)\, – \varepsilon_t(\theta^*) }_{\text{Multiplicative noise}} +  \underbrace{\varepsilon_t(\theta^*)}_{\text{Additive noise}}. $$ If \(\varepsilon_t\) is \(L\)-Lipshitz continuous, which can be verified for instance if we use the squared loss, the term \(\varepsilon_t(\theta)\, – \varepsilon_t(\theta^*) \) converges to \(0\) when \(\theta\) approaches \(\theta^*\). This motivates the “multiplicative” denomination of this term. On the contrary, remark that the additive noise does not depend on the state of the dynamics, i.e. it is independent of \(\theta_t\). Notably, in the overparametrised regime, taking \(\theta^*\) as an interpolator, we see that \(\varepsilon_t(\theta^*)=0\): there is no additive noise in this case! This is part of the explanation of the very different optimisation dynamics observed in the underparametrised and overparametrised regimes [<a href="https://proceedings.mlr.press/v89/vaswani19a.html" target="_blank" rel="noreferrer noopener">14</a>, <a href="https://proceedings.mlr.press/v80/ma18a.html" target="_blank" rel="noreferrer noopener">15</a>].</p>



<p class="has-text-align-justify">As an illustration of these two distinct regimes, consider a least-squares problem: $$ \mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n \big(\langle \theta,x_i\rangle\, – y_i\big)^2.$$ In the overparametrised setting, which typically requires that \(d \geqslant n\), where \(d\) is the dimension of the inputs, it is possible to interpolate the dataset and the noise vanishes at optimum. On the opposite, in any underparametrised setting, the global minimum of the loss function is strictly positive, and the noise’s variance is always lower-bounded: the presence of additive noise totally changes the nature of the dynamics as shown by the following pictures.</p>



<div class="wp-container-3 wp-block-columns">
<div class="wp-container-1 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_first-1.gif" class="wp-image-7442" height="480" /></figure></div></div>



<div class="wp-container-2 wp-block-column is-vertically-aligned-center"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_noise_scale-1.gif" class="wp-image-7441" height="480" /></figure></div></div>
</div>



<div class="wp-container-5 is-vertical wp-block-group">
<div class="wp-container-4 is-vertical wp-block-group">
<p class="has-text-align-justify"><span style="text-decoration: underline;">Left:</span> Iterates of SGD for underparametrised (blue) and overparametrised (orange) regimes. The ellipses represent different level sets of \(\mathcal{R}_n\). Inputs \((x_i)_{i\leqslant n}\) are the same in the two models: centred Gaussian with anisotropic covariance (\(n=100,\ d = 2\)). In the underparametrised setting, the outputs have been generated randomly, and there is no interpolator of the dataset, whereas in the overparametrised problem, we generated the outputs according to \(Y = X \theta^* \), where \(\theta^*\) is the unique global minimum of the first setup and hereby of the second as well.</p>



<p class="has-text-align-justify"><span style="text-decoration: underline;">Right:</span> The \(\ell_2\)-norm of the SGD noise in \(\mathrm{log}\)-scale along iterations in the underparametrised (blue) and overparametrised (orange) regimes. In the underparametrised regime, the noise stays constant after \(\sim 75\) iterations, whereas the intensity of the noise goes to \(0\) linearly over the iterates.</p>
</div>



<p></p>



<p></p>
</div>



<p class="has-text-align-justify"><strong>Noise geometry.</strong> Another important fact to stress concerning the noise \(\varepsilon_t(\theta)\) is that, at state \(\theta\), it belongs to a specific linear space: \(\text{span}\{\nabla_\theta h(\theta, x_1), …, \nabla_\theta h(\theta, x_n) \}\). To provide an intuition on how restrictive it can be, consider once again our favourite linear model: in this case, \(\nabla_\theta h(\theta, x_i)=x_i\) and \(\varepsilon_t(\theta)\) belongs to \(\text{span}\{x_1, …, x_n \}\), which is a space of dimension at most \(n\). In the overparametrised case, this is a strict subspace of \(\mathbb{R}^d\): noise is degenerate in \(d-n\) directions! On the contrary, in the underparametrised setting, as \(d \leqslant n\) there is no specificity at this level.</p>



<div class="wp-container-12 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-11 wp-block-columns">
<div class="wp-container-10 wp-block-column">
<div class="wp-container-9 wp-block-columns">
<div class="wp-container-8 wp-block-column">
<div class="wp-container-7 wp-block-columns are-vertically-aligned-top">
<div class="wp-container-6 wp-block-column is-vertically-aligned-top"><div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img width="442" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/Geometric_bias-1024x691.png" class="wp-image-7090" height="297" />3D picture showing the geometric bias of SGD: in the least-squares setting, the iterates (red lines) stay in the affine space \(\theta_0 + \text{span}(x_i)\). The contours represent different levels set of the loss highlighting the translational invariance along the null space \(\text{Ker}(X)\).</figure></div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>



<h2 id="positivity-of-lowner-matrices">SGD as a Markov chain</h2>



<p class="has-text-align-justify">In this section, we develop a different point of view on the general SGD algorithm, that also provides helpful intuition: we try to understand its dynamics through a Markov chain interpretation.</p>



<p class="has-text-align-justify">The first key to understand the behaviour of SGD is that, with constant step-sizes \(\gamma_t = \gamma\), the iterates define an homogeneous <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noreferrer noopener">Markov chain</a> [<a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" target="_blank" rel="noreferrer noopener">4</a>]. Let us recall the SGD recursion to keep it next to us.$$ \theta_t = \theta_{t-1} – \gamma\nabla \mathcal{R}_n (\theta_{t-1}) + \gamma\varepsilon_t(\theta_{t-1}). $$ This point of view induces a natural question: <em>does the distribution of the iterates converge to some limit we can characterise?</em> This will again depend on the regime: under or overparametrised. Roughly, for the former, with a constant step size \(\gamma\), the distribution will converge to a stationary distribution with a strictly positive variance (e.g. a Gaussian centred around \(\theta^*\) at scale \(\gamma^{1/2}\) [<a href="https://epubs.siam.org/doi/10.1137/0324039" target="_blank" rel="noreferrer noopener">5</a>]). Less understood is the fact that, for the latter, it will converge to a Dirac distribution \(\delta_{ \theta^*}\), with \(\theta^*\) being one <strong>specific interpolator</strong>, that will depend on the precise structure of the algorithm  (initialisation, set-size, architecture of the model… more details on this will be discussed in a future blog post)!</p>



<p class="has-text-align-justify"><strong>The underparametrised case.</strong> Recall that in this case, the additive noise is nondegenerate. Hence, the gradient and noise parts of the dynamics cannot cancel simultaneously and under some mild assumption on \(\mathcal{R}_n\), the dynamics equilibrates after a certain time: formally this means that the distribution of the iterates \((\theta_t)_{t \geqslant 0}\) converges to an invariant distribution \(\pi^\gamma\). Hence the question: <em>how far is \(\pi^\gamma\) from \(\delta_{\theta^*}\)</em>? Giving a general and precise answer to this question is not trivial, and more importantly, it depends heavily on the multiplicative part of the noise. For now, let us stick with some common modelling to study the recursion: we make the assumption that the noise does not depend on the current iterate \(\theta\) (multiplicative part is \(0\)). In this case, it has been shown that the iterates of SGD have Gaussian fluctuations around \(\theta^*\) at scale \(\gamma^{1/2}\) [<a href="https://epubs.siam.org/doi/10.1137/0324039" target="_blank" rel="noreferrer noopener">5</a>]. From this point of view, the smaller \(\gamma\), the closer to \(\theta^*\) we get and this justifies a known variance reduction technique: decaying step-sizes. Another technique to reduce the variance is to resort to averaging. Once again, to explain this, ergodic theorems for Markov chains [<a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" target="_blank" rel="noreferrer noopener">4</a>] give us an important insight: the time-mean \(\bar{\theta}_t = \frac{1}{t+1} \sum_{i = 0}^t \theta_i\) converges to \(\bar{\theta}_\gamma = \mathbb{E}_{\pi^\gamma} [\theta]\) almost surely. Furthermore, some magic happens with the quadratic case, as we can show that \(\bar{\theta}_\gamma = \theta^*\), which implies,  as a direct application of the strong law of large numbers, that \(\bar{\theta}_t\) converges almost surely to \(\theta^*\), and proves that averaging the iterates is relevant [<a href="https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html" target="_blank" rel="noreferrer noopener">6</a>].</p>



<div class="wp-container-15 wp-block-columns">
<div class="wp-container-13 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/Large_gamma_comparison-1.gif" class="wp-image-7446" height="480" /></figure></div></div>



<div class="wp-container-14 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/Medium_gamma_comparison-1.gif" class="wp-image-7448" height="480" /></figure></div></div>
</div>



<div class="wp-container-18 wp-block-columns">
<div class="wp-container-16 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/Small_gamma_comparison-4.gif" class="wp-image-7447" height="480" /></figure></div></div>



<div class="wp-container-17 wp-block-column">
<p class="has-text-align-justify">Here is a series of plots corresponding to different step-sizes \(\gamma\) showing the dynamics of plain SGD (blue), averaged SGD (red) and step-sizes decay at \(1/\sqrt{t}\) rate (orange)  in an underparametrised setting (\(n=100\), \(d=2\)). Plain SGD is always faster that the other two methods but the larger the \(\gamma\) the larger the variance at optimum is.</p>
</div>
</div>



<p class="has-text-align-justify">Note also that the same analysis can be done in the convex, yet nonquadratic case, indeed, in <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-3/Bridging-the-gap-between-constant-step-size-stochastic-gradient-descent/10.1214/19-AOS1850.full" target="_blank" rel="noreferrer noopener">[7]</a> the authors showed that the order of magnitude of the distance between \(\bar{\theta}_\gamma\) and \({\theta}^*\) is \(\gamma^2\).</p>



<p class="has-text-align-justify"><strong>The overparametrised case.</strong> We have seen in the previous section that both the scale of noise and the geometry are affected in this case. The effect of the geometry is a little bit difficult to handle at a general level and we will detail its role in specific examples (see below and in the next blog post to come). Concerning the scale, we have seen that the noise vanishes at a global optimum and that the closer to a global optimum, the smaller the intensity of the noise is. Hence, global optima are fixed points of the dynamics, and under technical assumptions (e.g. local attractiveness), it can be shown the dynamics eventually converge to one of them [<a href="https://arxiv.org/pdf/2105.01650.pdf" target="_blank" rel="noreferrer noopener">8</a>, <a href="https://proceedings.neurips.cc/paper/2021/hash/b4a0e0fbaa9f16d8947c49f4e610b549-Abstract.html" target="_blank" rel="noreferrer noopener">9</a>]. In this case, the nature of the dynamics is very different from the previous one: the distribution of the iterates \((\theta_t)_{t \geqslant 0}\) converges to \(\delta_{\theta^*}\) where \(\theta^*\) is a random variable in the set of interpolators! Remarkably, no variance reduction technique are required for convergence towards a global minimum!</p>



<h2 id="characterizing-all-matrix-monotone-functions">The stochastic gradient flow and the least-squares example</h2>



<p class="has-text-align-justify justify-text"><strong>Continuous time model of SGD.</strong> Continuous time counterparts of (discrete) numerical optimisation methods and Markov chains are well-worn subjects in applied mathematics and have found applications in machine learning in particular. Indeed, in recent years, gradient descent has been actively studied through <a href="https://francisbach.com/gradient-flows/" target="_blank" rel="noreferrer noopener">gradient flows</a>, which have led to convergence results for neural networks discussed in previous <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">blog</a> <a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/" target="_blank" rel="noreferrer noopener">posts</a>. However, due to its stochasticity, SGD cannot be properly modelled as a deterministic flow. An interesting and natural approach is to consider stochastic differential equations (SDEs) $$ d \theta_t = b(t,\theta_t) d t + \sigma(t,\theta_t)d B_t, $$ where \((B_t)_{t \geqslant 0}\) is a standard Brownian motion [<a href="https://link.springer.com/book/10.1007/978-3-642-14394-6" target="_blank" rel="noreferrer noopener">10</a>]. In order to accurately represent the SGD dynamics, the drift term \(b\) and the noise \(\sigma\) should meet the following requirements:</p>



<div class="wp-container-19 is-vertical wp-block-group">
<p class="has-text-align-justify justify-text">(i) The drift term \(b\) should match the negative full gradient: \(b=-\nabla \mathcal{R}_n\).</p>



<p>(ii) The noise covariance \(\sigma\sigma^\top(t,\theta)\) should match \(\gamma\mathbb{E}[ \varepsilon_t(\theta_t) \varepsilon_t(\theta_t)^\top | \theta_t = \theta ]\).</p>



<p>(iii) The noise at state \(\theta\) should belong to \(\text{span}\{\nabla_\theta h(\theta, x_1), …, \nabla_\theta h(\theta, x_n) \}\).</p>



<p>\({\color{white} f} \)</p>
</div>



<p class="has-text-align-justify">Points (i) and (ii) imply that the SGD recursion corresponds in fact to the Euler-Maruyama discretisation with step-size \(\gamma\) of the SDE [<a href="https://jmlr.org/papers/v20/17-526.html" target="_blank" rel="noreferrer noopener">11</a>]. Hence, the SDE and the discrete models match perfectly for infinitesimal step-sizes (up to first order terms in \(\gamma\)) and the model is said to be <em>consistent</em>. Point (iii) states a geometrical property of the SGD noise and is of crucial importance for a proper modelisation.</p>



<p class="has-text-align-justify"><strong>The least-squares model.</strong> Consider once again our favourite least-squares model $$\mathcal{R}_n(\theta)= \frac{1}{2n}\sum_{i=1}^n (\langle \theta,x_i\rangle\, – y_i)^2,$$ and let \(X:= [x_1, …, x_n]^\top \in \mathbb{R}^{n \times d}\) be the data matrix and \(y = [y_1, …, y_n]^\top \in \mathbb{R}^n\) the vector of outputs. Let us explicitly write the SDE model in this case: the drift corresponds to the gradient \(b(\theta)=- \frac{1}{n}X^\top (X\theta – y) \) and it is simple linear algebra to calculate the covariance of the noise (see also [<a href="https://proceedings.mlr.press/v119/ali20a.html" target="_blank" rel="noreferrer noopener">19</a>, Remark 4])$$ \sigma \sigma^\top(\theta) =  \frac{\gamma}{n} X^\top \left(\textrm{diag}\left[ \big(\langle \theta, x_i\rangle\, – y_i\big)^2 \right]_{i=1}^n – \frac{1}{n}  \left[ \big(\langle \theta, x_i\rangle\, – y_i\big)\big(\langle \theta, x_j\rangle\, – y_j\big) \right]_{i,j=1}^n  \right) X. $$ To simplify a bit this analysis let us neglect the residual term of order \(1/n^2\) and assume that the residuals \(\langle \theta, x_i\rangle – y_i\), are approximately equal across all the samples of the data set: \(\textrm{diag}\left[ \left(\langle \theta, x_i\rangle\, – y_i\right)^2 \right]_{i=1}^n \simeq \frac{1}{n}\|X \theta\, – y\|^2 I_n \). Then the SDE model writes: $$d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \frac{\sqrt{\gamma}}{n} \| X\theta_t -y \| X^\top d B_t. $$ As stressed before, notice that there is an important difference between the under and overparametrised regimes: in the former, the noise spans all \(\mathbb{R}^d\) and as there exists \(\sigma &gt; 0\) such that \(\| X\theta_t -y \| \geqslant \sigma \), the noise is non degenerate in every direction. Instead, in the overparametrised regime, the noise vanishes at a global optimum \(\theta^*\) and is degenerate in the directions of \([\mathrm{range} (X^\top)]^\perp = \mathrm{Ker} (X)\). As also noticed in [<a href="https://proceedings.mlr.press/v119/ali20a.html" target="_blank" rel="noreferrer noopener">19</a>], this will result in important differences in term of the processes’ behaviour! In the rest of the post, let us denote \(\theta^{\text{LS}}:= X^\dagger y\), where \(X^\dagger\) is the pseudo inverse of \(X\). It corresponds to the ordinary least-square estimator in the underparametrised regime and the projection of \(0\) into \(\mathrm{Ker} (X)\) in the overparametrised setting.</p>



<p class="has-text-align-justify"><strong>The underparametrised regime and the multivariate <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" target="_blank" rel="noreferrer noopener">Ornstein–Uhlenbeck process</a>.</strong> As written before, let us consider a model where the residuals scale like the best possible fit, i.e.  \(\frac{1}{\sqrt{n}}\| X\theta_t -y \| \simeq \sigma \). The dynamics reads: $$ d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \sqrt{\frac{\gamma}{n}} \sigma  X^\top d B_t. $$ This can be viewed as a <em>multivariate <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" target="_blank" rel="noreferrer noopener">Ornstein–Uhlenbeck process</a></em> for which we can show the following convergence result: the law \(\pi_t\) of \(\theta_t\) converges to \(\pi^\gamma\), a Gaussian distribution of mean \(\theta^{\text{LS}}\) and covariance \(\frac{\gamma \sigma^2}{2} I_d\): $$\lim_{t \to \infty} \pi_t =\mathcal{N} \left( \theta^{\text{LS}}, \frac{\gamma \sigma^2}{2} I_d \right).$$ From this, as for the Markov chain interpretation, we can nicely interpret the need to decrease step-sizes to \(0\) in order to concentrate to \(\theta^{\text{LS}}\). Similarly, note that the ergodic theorem gives you automatically that the time average of \(\theta_t\) goes to \(\theta^{\text{LS}}\) as $$\qquad \qquad \qquad \lim_{t \to \infty} \frac{1}{t}\int_0^t \theta_s ds = \mathbb{E}[\pi^\gamma]=\theta^{\text{LS}}, \qquad \text{almost surely}.$$ Besides its nice interpretation as a Ornstein-Uhlenbeck process, we illustrate below the predictive power of the SDE model to understand the SGD dynamics.</p>



<div class="wp-container-22 wp-block-columns">
<div class="wp-container-20 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_SDE_under-1.gif" class="wp-image-7449" height="480" /></figure></div></div>



<div class="wp-container-21 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/hist-2.gif" class="wp-image-7444" height="480" /></figure></div></div>
</div>



<div class="wp-container-25 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-24 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-23 is-vertical wp-block-group">
<p class="has-text-align-justify"><span style="text-decoration: underline;">Left</span>: \(10\) realisations of SGD in blue-like colours (corresponding to \(10\) different random sampling sequences), next to \(10\) realisations of the SDE model in red-like colours  (corresponding to \(10\) different realisations of the Brownian motion). </p>



<p class="has-text-align-justify"><span style="text-decoration: underline;">Right</span>: A histogram of snap-shots of the SGD dynamics (marginal along the first coordinate) at increasing time intervals that show the convergence (by ergodicity) to the predicted Gaussian \(\mathcal{N}(\theta^{\text{LS}}, \gamma \sigma^2/2)\) ! </p>
</div>
</div></div>
</div></div>



<p></p>



<p class="has-text-align-justify justify-text"><strong>The overparametrised regime and the <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" target="_blank" rel="noreferrer noopener">geometric Brownian motion</a></strong>. We take back our SDE model, initialise it at \(\theta_0 = 0\), but now we are extra careful that the noise cancels at any interpolator \(\theta^*\). We will show that despite being a random process, the dynamics with constant \(\gamma &gt; 0\) converges almost surely to \(\theta^{\text{LS}}\). To do so let us consider the evolution of the  squared difference \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\). Thanks to <a href="https://en.wikipedia.org/wiki/It%C3%B4_calculus" target="_blank" rel="noreferrer noopener">Itô calculus</a> (chain rule for stochastic processes), $$\begin{aligned} d \eta_t &amp;= 2 \langle d \theta_t, \theta_t – \theta^{\text{LS}} \rangle + \frac{\gamma}{n^2} \mathrm{tr} (X^\top X) \| X (\theta_t – \theta^{\text{LS}}) \|^2   dt \\ &amp;=  2\left( \gamma \mathrm{tr} \left(n^{-1}X^\top X\right) – 2 \right) \mathcal{R}_n(\theta_t) dt + 4\sqrt{\gamma}\mathcal{R}_n(\theta_t) dW_t, \end{aligned} $$ where \((W_t)_{t \geqslant 0}\) is a one dimensional Brownian motion (see details of this calculation at the end of the post). To simplify the calculation set the step-size such that \(\gamma \mathrm{tr} (n^{-1}X^\top X) = 1\) (note that we retrieve here the usual step-size for least-squares SGD [<a href="https://proceedings.mlr.press/v38/defossez15.html" target="_blank" rel="noreferrer noopener">16</a>]). Noting \(\mu_t = \frac{2\mathcal{R}_n(\theta_t)}{\eta_t}\) and \(\nu_t = \frac{4\sqrt{\gamma}\mathcal{R}_n(\theta_t)}{\eta_t}\), the dynamics of \((\eta_t)_{t \geqslant 0}\) follows $$d  \eta_t =  -\mu_t  \eta_t dt + \nu_t \eta_t dW_t, $$ Furthermore, as \(\theta_t – \theta^{\text{LS}} \in \mathrm{span} (X^\top)\) , we have \(\mu\cdot \eta_t\leqslant 2 \mathcal{R}_n(\theta_t) \leqslant L\cdot \eta_t\), where \(\mu\) and \(L\) stand respectively for the minimum and maximum eigenvalues of \(\frac{1}{n}XX^\top\). Hence, the behaviour of \((\eta_t)_{t \geqslant 0}\) is approximately the same as that of the solution to the following SDE: $$d  S_t =  -\mu  S_t dt + \nu S_t dW_t, $$ for \(\nu \simeq 4 L \sqrt{\gamma}\). That is, the dynamics of \((\|\theta_t – \theta^{\text{LS}}\|^2)_{t \geqslant 0}\) is approximately the one of a <a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" target="_blank" rel="noreferrer noopener">Geometric Brownian Motion</a> (GBO), a.k.a. the process underlying the <a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model" target="_blank" rel="noreferrer noopener">Black-Scholes model</a>! Hence, to understand SGD in the overparametrised regime, one “simply” need to study the properties of the GBO. It is known that in fact there is an explicit solution of this SDE $$ S_t =  S_0 \exp\left( -\left(\mu + \frac{\nu^2}{2}\right) t  + \nu W_t\right). $$ Hence, for all \(t&gt;0\), \(S_t\) is distributed according to a log-normal random variable with mean \(\mathbb{E}(S_t) = S_0 e^{- \mu t}\) and variance \(\text{var}(S_t) = S_0^2 e^{- 2\mu t} (e^{\nu^2 t} – 1)\). Furthermore, it is a nice application to the <a href="https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm" target="_blank" rel="noreferrer noopener">law of the iterated logarithm</a> for the Brownian motion to show that  $$ \qquad \qquad \qquad \lim_{t \to \infty} S_t = 0, \qquad \text{almost surely!} $$ The almost sure convergence of \((\theta_t)_{t \geqslant 0}\) to \(\theta^{\text{LS}}\) is in stark contrast with the limiting behaviour we have seen in the underparametrised setting. Remarkably, the process does not require any variance reduction to converge to a specific point. Note also that among all the possible interpolators the process could have converged to, the process in fact goes almost surely to \(\theta^{\text{LS}}\). This selection of the minimum norm interpolator [<a href="https://openreview.net/forum?id=Sy8gdB9xx" target="_blank" rel="noreferrer noopener">12</a>], known as the implicit bias of the process  will be the core of a next blog post (for more complex architectures).</p>



<p>As in the underparametrised case, below are two plots showing the typical behaviour of the SGD dynamics as a GBO.</p>



<div class="wp-container-28 wp-block-columns">
<div class="wp-container-26 wp-block-column is-vertically-aligned-top"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/SGD_SDE_over-1.gif" class="wp-image-7445" height="480" /></figure></div></div>



<div class="wp-container-27 wp-block-column"><div class="wp-block-image justify-text">
<figure class="aligncenter size-full"><img width="720" alt="" src="https://francisbach.com/wp-content/uploads/2022/07/Geometric_brownian-1.gif" class="wp-image-7443" height="480" /></figure></div></div>
</div>



<div class="wp-container-29 is-vertical wp-block-group">
<p><span style="text-decoration: underline;">Left</span>: \(10\) realisations of SGD in blue-like colours (corresponding to \(10\) different random sampling sequences), next to \(10\) realisations of the SDE model in red-like colours (corresponding to \(10\) different realisations of the Brownian motion). </p>



<p><span style="text-decoration: underline;">Right</span>: The exponential convergence of  \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\) to \(0\) for the \(10\) realisations of SGD and the SDE (in log-scale).</p>



<p></p>



<p></p>
</div>



<h2 id="conclusion">Conclusion</h2>



<p class="has-text-align-justify">In this blog post, I have tried to show that despite the old legacy inherited from the stochastic approximation literature, the particularities of the noise of the stochastic gradient descent in machine learning contexts necessitate a fresh look.  This specificity strikes the most in modern overparametrised models as the noise can be largely degenerate, both (i) in direction (which I called <em>noise geometry</em>), and (ii) in scale, as its variance is dominated by the loss and hence vanishes at a global optimum. In fact I have tried to show that SDE models can be a relevant framework to shed light on typical behaviours of stochastic learning dynamics: for overparametrised architectures, they differentiate from the <a href="https://en.wikipedia.org/wiki/Langevin_dynamics" target="_blank" rel="noreferrer noopener">overdamped Langevin diffusion model</a>, and act as multiplicative noise dynamics like the geometric brownian motion.</p>



<p class="has-text-align-justify">Besides, as it is believed that stochasticity plays a role in the generalisation performances of optimisation algorithms [<a href="https://arxiv.org/pdf/1609.04836.pdf" target="_blank" rel="noreferrer noopener">13</a>], there is a need to consider its action precisely. Yet, grasping its overall nature appears to be particularly hard as it mixes properties from the model’s architecture, the data’s distribution and the loss. In a following post, I will try to analyse some simple non-convex models where we can show that the SGD-noise drives the dynamics to possibly “good for generalisation” regions [<a href="https://proceedings.neurips.cc/paper/2021/hash/f4661398cb1a3abd3ffe58600bf11322-Abstract.html" target="_blank" rel="noreferrer noopener">17</a>,<a href="https://proceedings.mlr.press/v178/vivien22a.html" target="_blank" rel="noreferrer noopener">18</a>].<br /><br /><strong>Acknowledgements</strong>. I would like to thank Scott Pesme and Etienne Boursier for fruitful discussions, and especially <a href="https://open.spotify.com/track/1r6EJWSoh9Kzn6L38BhYS6?si=b352952e9bac4420" target="_blank" rel="noreferrer noopener">Nicolas Le Hir</a> for proofreading this blog post and making good clarifying suggestions.</p>



<h2 id="references">References</h2>



<div class="wp-container-40 is-vertical wp-block-group">
<div class="wp-container-39 is-vertical wp-block-group">
<div class="wp-container-38 is-vertical wp-block-group">
<div class="wp-container-37 wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-container-36 is-vertical wp-block-group">
<div class="wp-container-35 is-vertical wp-block-group">
<div class="wp-container-34 is-vertical wp-block-group">
<div class="wp-container-33 is-vertical wp-block-group">
<div class="wp-container-32 is-vertical wp-block-group">
<div class="wp-container-31 is-vertical wp-block-group">
<div class="wp-container-30 is-vertical wp-block-group">
<p class="justify-text">[1] Herbert Robbins and Sutton Monro. <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full" target="_blank" rel="noreferrer noopener">A Stochastic Approximation Method</a>. The Annals of Mathematical Statistics, 22 (3): 400 – 407, September, 1951.<br />[2] Marie Duflo. <a href="https://link.springer.com/book/9783540606994" target="_blank" rel="noreferrer noopener">Algorithmes stochastiques</a>. Volume 23 of Mathématiques &amp; Applications (Berlin) [Mathematics &amp; Applications]. Springer-Verlag, Berlin, 1996<br />[3] Léon Bottou and Olivier Bousquet. <a href="https://proceedings.neurips.cc/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html" target="_blank" rel="noreferrer noopener">The Tradeoffs of Large Scale Learning</a>, In Advances in Neural Information Processing Systems, vol.20, 161-168, 2008.<br />[4] Sean P.Meyn and Richard L. Tweedie. <a href="https://link.springer.com/book/10.1007/978-1-4471-3267-7" target="_blank" rel="noreferrer noopener">Markov Chains and Stochastic Stability.</a> Springer, London, 1993.<br />[5] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/10.1137/0324039" target="_blank" rel="noreferrer noopener">Stochastic minimization with constant step-size: asymptotic laws</a>. SIAM Journal on Control and Optimization, 24(4):655–666, 1986.<br />[6] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html" target="_blank" rel="noreferrer noopener">Non-asymptotic analysis of stochastic approximation algorithms for machine learning.</a> In Advances in Neural Information Processing Systems (NIPS), 2011.<br />[7] Aymeric Dieuleveut, Alain Durmus and Francis Bach. <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-3/Bridging-the-gap-between-constant-step-size-stochastic-gradient-descent/10.1214/19-AOS1850.full" target="_blank" rel="noreferrer noopener">Bridging the gap between constant step size stochastic gradient descent and Markov chains.</a> The Annals of Statistics, 48 (3): 1348 – 1382, June 2020.<br />[8] Stephan Wojtowytsch. <a href="https://arxiv.org/pdf/2105.01650.pdf" target="_blank" rel="noreferrer noopener">Stochastic gradient descent with noise of machine learning type.</a> Part I: Discrete time analysis Technical Report, arXiv-2105.01650, 2021.<br />[9] Aditya Varre, Loucas Pillaud-Vivien and Nicolas Flammarion. <a href="https://proceedings.neurips.cc/paper/2021/hash/b4a0e0fbaa9f16d8947c49f4e610b549-Abstract.html" target="_blank" rel="noreferrer noopener">Last iterate convergence of SGD for Least-Squares in the Interpolation regime.</a> Advances in Neural Information Processing Systems 34, 21581-21591, 2021.<br />[10] Bernt Øksendal. <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-14394-6_5.pdf" target="_blank" rel="noreferrer noopener">Stochastic Differential Equations: An Introduction with Applications</a>, 6th edition. Springer, New York, 2003.<br />[11] Qianxiao Li, Cheng Tai, and Weinan E. <a href="https://jmlr.org/papers/v20/17-526.html" target="_blank" rel="noreferrer noopener">Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations.</a> Journal of Machine Learning Research, 20(40):1–47, 2019.<br />[12] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. <a href="https://openreview.net/forum?id=Sy8gdB9xx" target="_blank" rel="noreferrer noopener">Understanding deep learning requires rethinking generalization</a>. In International Conference on Learning Representations, 2017.<br />[13] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. <a href="https://arxiv.org/pdf/1609.04836.pdf" target="_blank" rel="noreferrer noopener">On large-batch training for deep learning: Generalization gap and sharp minima.</a> In International Conference on Learning Representations, 2017.[8] Stephan Wojtowytsch. <a href="https://arxiv.org/pdf/2105.01650.pdf" target="_blank" rel="noreferrer noopener">Stochastic gradient descent with noise of machine learning type.</a> Part I: Discrete time analysis Technical Report, arXiv-2105.01650, 2021.<br />[14] Sharan Vaswani, Francis Bach and Mark Schmidt. <a href="https://proceedings.mlr.press/v89/vaswani19a.html" target="_blank" rel="noreferrer noopener">Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron</a>. Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, PMLR 89:1195-1204, 2019.<br />[15] Siyuan Ma, Raef Bassily and Mikhail Belkin. <a href="https://proceedings.mlr.press/v80/ma18a.html" target="_blank" rel="noreferrer noopener">The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning</a>. Proceedings of the 35th International Conference on Machine Learning, PMLR 80:3325-3334, 2018.<br />[16] Alexandre Defossez and Francis Bach. <a href="https://proceedings.mlr.press/v38/defossez15.html" target="_blank" rel="noreferrer noopener">Averaged Least-Mean-Squares: Bias-Variance Trade-offs and Optimal Sampling Distributions.</a> Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, PMLR 38:205-213, 2015.<br />[17] Scott Pesme, Loucas Pillaud-Vivien and Nicolas Flammarion. <a href="https://proceedings.neurips.cc/paper/2021/hash/f4661398cb1a3abd3ffe58600bf11322-Abstract.html" target="_blank" rel="noreferrer noopener">Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity.</a> In Advances in Neural Information Processing Systems 34, 29218-29230, 2021.<br />[18] Loucas Pillaud Vivien, Julien Reygner and Nicolas Flammarion. <a href="https://proceedings.mlr.press/v178/vivien22a.html" target="_blank" rel="noreferrer noopener">Label noise (stochastic) gradient descent implicitly solves the Lasso for quadratic parametrisation.</a> Proceedings of Thirty Fifth Conference on Learning Theory, PMLR 178:2127-2159, 2022.<br />[19] Alnur Ali, Edgar Dobriban and Ryan Tibshirani. <a href="https://proceedings.mlr.press/v119/ali20a.html" target="_blank" rel="noreferrer noopener">The Implicit Regularization of Stochastic Gradient Flow for Least Squares</a>. Proceedings of the 37th International Conference on Machine Learning, PMLR 119:233-244, 2020.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>



<p class="justify-text"></p>



<h2>Details on the evolution equation of \(\eta_t = \|\theta_t – \theta^{\text{LS}}\|^2\).</h2>



<p></p>



<p class="has-text-align-justify">Recall that the evolution of the iterates is given by the SDE: $$ d \theta_t =\, – \frac{1}{n}X^\top (X\theta_t\, – y) d t + \frac{\sqrt{\gamma}}{n} \| X\theta_t -y \| X^\top d B_t. $$ Then by <a href="https://en.wikipedia.org/wiki/It%C3%B4_calculus" target="_blank" rel="noreferrer noopener">Itô calculus</a>, we have $$\begin{aligned} d \eta_t &amp;= 2 \langle d \theta_t, \theta_t – \theta^{\text{LS}} \rangle + \frac{\gamma}{n^2} \mathrm{tr} (X^\top X) \| X (\theta_t – \theta^{\text{LS}}) \|^2   dt \\ &amp;= -\frac{2}{n} \langle X^\top (X\theta_t\, – y), \theta_t – \theta^{\text{LS}} \rangle dt + 2 \sqrt{\frac{2\gamma}{n}} \sqrt{\mathcal{R}_n(\theta_t)} \langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle \\ &amp; \hspace{6.5cm}+ 2\gamma \mathrm{tr} (n^{-1}X^\top X) \mathcal{R}_n(\theta_t)   dt. \end{aligned}$$ Since,  \(\theta^{\text{LS}}\) is an interpolator, i.e \(X \theta^{\text{LS}} = y\), the first term corresponds to $$ -\frac{2}{n}\langle X^\top (X\theta_t\, – y), \theta_t – \theta^{\text{LS}} \rangle = -\frac{2}{n}\langle X(\theta_t – \theta^{\text{LS}}), X(\theta_t – \theta^{\text{LS}}) \rangle = -4 \mathcal{R}_n(\theta_t).$$ For the second term, note that \(\langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle = \langle d B_t, X(\theta_t – \theta^{\text{LS}}) \rangle\), and by <a href="https://almostsuremath.com/2010/04/13/levys-characterization-of-brownian-motion/" target="_blank" rel="noreferrer noopener">Lévy’s Characterization of Brownian Motion</a>, the local martingale $$ W_t = \int_0^t \frac{\langle d B_s, X(\theta_s – \theta^{\text{LS}})\rangle}{\|X(\theta_s – \theta^{\text{LS}})\|} $$ is in fact a one dimensional Brownian motion. Hence, $$ \langle X^\top d B_t, \theta_t – \theta^{\text{LS}} \rangle = \|X(\theta_s – \theta^{\text{LS}})\| dW_t = \sqrt{2 n \mathcal{R}_n(\theta_t)}dW_t,$$ and finally, $$ d \eta_t = -4 \mathcal{R}_n(\theta_t) dt + 4 \sqrt{\gamma} \mathcal{R}_n(\theta_t) d W_t + 2 \gamma \mathrm{tr} (n^{-1}X^\top X) \mathcal{R}_n(\theta_t)   dt,$$ which gives the claimed result of the main text.</p></div>







<p class="date">
by Loucas Pillaud-Vivien <a href="https://francisbach.com/rethinking-sgd-noise/"><span class="datestr">at July 25, 2022 01:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6792824547495697148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/100-best-number-theory-books-of-all.html">100 Best Number Theory books of all Time---except many are not on Number Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I was recently emailed this link:<div><br /></div><div><a href="https://bookauthority.org/books/best-number-theory-books">100 Best Number Theory books of all Time</a><br /></div><div><br /></div><div>That sounds like a good list to have!  But then I looked at it. </div><div><br /></div><div>The issue IS NOT that the books on it are not good. I suspect they all are.</div><div><br />The issue IS that many of the books on the list are not on Number Theory.</div><div><br /></div><div>DEFINITLY NOT:</div><div><br /></div><div>A Mathematicians Apology by Hardy</div><div><br /></div><div>The Universe Speaks in Numbers by Farmelo (looks like Physics)</div><div><br /></div><div>Category theory in Context by Riehl</div><div><br /></div><div>A First Course in Mathematical logic and set theory by O'Leary</div><div><br />Astronomical Algorithms by Meeus (Algorithms for Astronomy)</div><div><br /></div><div>Pocket Book of Integrals and Math Formulas by Tallardia</div><div><br /></div><div>Entropy and Diversity by Leinster</div><div><br /></div><div>BORDERLINE:</div><div><br />Too many to name, so I will name categories (Not the type Riehl talks about)</div><div><br /></div><div>Logic books. Here <i>Number Theory </i> seems to mean <i>Peano Arithmetic</i> and they are looking at what you can and can't prove in it. </div><div><br /></div><div>Combinatorics books:  Indeed, sometimes it is hard to draw the line between Combinatorics and Number Theory, but I still would not put a book on <i>Analytic Combinatorics o</i>n a list of top books in Number Theory. </div><div><br /></div><div>Discrete Math textbooks: Yes, most discrete math textbooks have some elementary number theory in them, but that does not make them number theory books.</div><div><br /></div><div>Abstract Algebra, Discrete Harmonic Analysis, other hard math books: These have theorems in Number Theory as an Application.  But they are not books on number theory. </div><div><br />WHAT OF ALL THIS? </div><div><br /></div><div>Lists like this often have several problems</div><div><br /></div><div>1) The actual object of study is not well defined.</div><div><br /></div><div>2) The criteria for being good is not well defined.</div><div><br /></div><div>3) The list is just one person's opinion. If I think the best math-novelty song of all time is William-Rowan-Hamilton (see  <a href="https://www.youtube.com/watch?v=SZXHoWwBcDc">here</a>) and the worse one is the Bolzano--Weierstrass rap (see <a href="https://www.youtube.com/watch?v=dfO18klwKHg&amp;t=8s">here</a>) that's just my opinion. Even if I was the leading authority on Math Novelty songs and had the largest collection in the world, its still just my opinion. (Another contender for worst math song of all time is <a href="https://www.youtube.com/watch?v=4xQFlsK7jKg&amp;t=10s">here</a>.)</div><div><br /></div><div>4) Who is the audience for such lists? For the Number Theory Books is the audience ugrad math majors? grad math majors? Number Theorists? This needs to be well defined.</div><div><br /></div><div>5) The list may tell more about the people making the list then the intrinsic qualify of the objects. This is more common in, say, the ranking of presidents. My favorite is Jimmy Carter since he is the only one with the guts to be sworn in by his nickname Jimmy, unlike  Bill Clinton (sworn in as William Jefferson Clinton- a name only used by his mother when she was mad at him) or Joe Biden (sworn in as Joseph Robinette Biden which I doubt even his mother ever used). My opinion may seem silly, but it reflects my bias towards nicknames, just as the people who rank presidents use their bias. </div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/100-best-number-theory-books-of-all.html"><span class="datestr">at July 24, 2022 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/108">TR22-108 |  Hardness Self-Amplification from Feasible Hard-Core Sets | 

	Shuichi Hirahara, 

	Nobutaka Shimizu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the question of hardness self-amplification: Given a Boolean function $f$ that is hard to compute on a $o(1)$-fraction of inputs drawn from some distribution, can we prove that $f$ is hard to compute on a $(\frac{1}{2} - o(1))$-fraction of inputs drawn from the same distribution? We prove hardness self-amplification results for natural distributional problems studied in fine-grained average-case complexity, such as the problem of counting the number of the triangles modulo 2 in a random tripartite graph and the online vector-matrix-vector multiplication problem over $\mathbb{F}_2$. More generally, we show that any problem that can be decomposed into ``computationally disjoint'' subsets of inputs admits hardness self-amplification. This is proved by generalizing the security proof of the Nisan--Wigderson pseudorandom generator, in which case nearly disjoint subsets of inputs are considered.
    
At the core of our proof techniques is a new notion of feasible hard-core set, which generalizes Impagliazzo's hard-core set [Impagliazzo, FOCS'95]. We show that any weak average-case hard function $f$ has a feasible hard-core set $H$: any small $H$-oracle circuit (that is allowed to make queries $q$ to $H$ if $f(q)$ can be computed without the oracle) fails to compute $f$ on a $(\frac{1}{2} - o(1))$-fraction of inputs in $H$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/108"><span class="datestr">at July 24, 2022 03:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=2216">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/cacm-formalizing-fairness/">CACM – Formalizing Fairness</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>There is a <a href="https://cacm.acm.org/magazines/2022/8/262911-formalizing-fairness/fulltext">new article in CACM</a>, by <a href="https://marinakrakovsky.com/">Mariana Krakovsky</a>, who wrote a really nice introduction to the work within TOC on Algorithmic Fairness. It does not (and cannot) get too deep into anything but I think it is quite well crafted.</p></div>







<p class="date">
by Omer Reingold <a href="https://toc4fairness.org/cacm-formalizing-fairness/"><span class="datestr">at July 23, 2022 02:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6593">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6593">More AI debate between me and Steven Pinker!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Several people have complained that <em>Shtetl-Optimized</em> has become too focused on the niche topic of “people being mean to Scott Aaronson on the Internet.”  In one sense, this criticism is deeply unfair—did I <em>decide</em> that a shockingly motivated and sophisticated troll should attack me all week, in many cases impersonating fellow academics to do so?  Has such a thing happened to <em>you</em>?  Did I <em>choose</em> a personality that forces me to respond when it happens?</p>



<p>In another sense, the criticism is of course completely, 100% justified.  That’s why I’m happy and grateful to have formed the <a href="https://scottaaronson.blog/?p=6576">SOCG (Shtetl-Optimized Committee of Guardians)</a>, whose purpose is to prevent a recurrence, thereby letting me get back to your regularly scheduled programming.</p>



<p>On that note, I hope the complainers will be satisfied with more exclusive-to-<em>Shtetl-Optimized</em> content from one of the world’s greatest living public intellectuals: the Johnstone Family Professor of Psychology at Harvard University, Steven Pinker.</p>



<p>Last month, you’ll recall, Steve and I <a href="https://scottaaronson.blog/?p=6524">debated the implications of scaling AI models</a> such as GPT-3 and DALL-E.  A main crux of disagreement turned out to be whether there’s any coherent concept of “superintelligence.”  I gave a qualified “yes” (I can’t provide <em>necessary and sufficient conditions</em> for it, nor do I know <em>when</em> AI will achieve it if ever, but there are certainly things an AI could do that would cause me to say it <em>was</em> achieved).  Steve, by contrast, gave a strong “no.”</p>



<p>My friend (and previous <em>Shtetl-Optimized</em> guest blogger) Sarah Constantin then wrote a <a href="https://sarahconstantin.substack.com/p/is-human-intelligence-simple-part-75c">thoughtful response to Steve</a>, taking a different tack than I had.  Sarah emphasized that Steve himself is on record defending the statistical validity of <a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)">Spearman’s <em>g</em></a>: the “general factor of human intelligence,” which accounts for a large fraction of the variation in humans’ performance across nearly every intelligence test ever devised, and which is <em>also</em> found to correlate with cortical thickness and other physiological traits.  Is it so unreasonable, then, to suppose that <em>g</em> is measuring <em>something</em> of abstract significance, such that it would continue to make sense when extrapolated, not to godlike infinity, but at any rate, well beyond the maximum that happens to have been seen in humans?</p>



<p>I relayed Sarah’s question to Steve.  (As it happens, the same question was also discussed at length in, e.g., <a href="https://www.vetta.org/documents/Machine_Super_Intelligence.pdf">Shane Legg’s 2008 PhD thesis</a>; Legg then went on to cofound <a href="https://en.wikipedia.org/wiki/DeepMind">DeepMind</a>.)  Steve was then gracious enough to write the following answer, and to give me permission to post it here.  I’ll also share my reply to him.  There’s some further back-and-forth between me and Steve that I’ll save for the comments section to kick things off there.  Everyone is warmly welcomed to join: just remember to stay on topic, be respectful, and <strong>click the link in your verification email!</strong></p>



<p>Without further ado:</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<h2>Comments on General, Artificial, and Super-Intelligence</h2>



<p><em>by Steven Pinker</em></p>



<p>While I defend the existence and utility of IQ and its principal component, general intelligence or <em>g</em>,  in the study of individual differences, I think it’s completely irrelevant to AI, AI scaling, and AI safety. It’s a measure of differences among humans within the restricted range they occupy, developed more than a century ago. It’s a statistical construct with no theoretical foundation, and it has tenuous connections to any mechanistic understanding of cognition other than as an omnibus measure of processing efficiency (speed of neural transmission, amount of neural tissue, and so on). It exists as a coherent variable only because performance scores on subtests like vocabulary, digit string memorization, and factual knowledge intercorrelate, yielding a statistical principal component, probably a global measure of neural fitness.</p>



<p>In that regard, it’s like a <em>Consumer Reports </em>global rating of cars, or overall score in the pentathlon. It would not be surprising that a car with a more powerful engine also had a better suspension and sound system, or that better swimmers are also, on average, better fencers and shooters. But this tells us precisely nothing about how engines or human bodies work. And imagining an extrapolation to a supervehicle or a superathlete is an exercise in fantasy but not a means to develop new technologies.</p>



<p>Indeed, if “superintelligence” consists of sky-high IQ scores, it’s been here since the 1970s! A few lines of code could recall digit strings or match digits to symbols orders of magnitude better than any human, and old-fashioned AI programs could also trounce us in multiple-choice vocabulary tests, geometric shape extrapolation (“progressive matrices”), analogies, and other IQ test components. None of this will help drive autonomous vehicles, discover cures for cancer, and so on.</p>



<p>As for recent breakthroughs in AI which may or may not surpass humans (the original prompt for this exchange); What is the IQ of GPT-3, or DALL-E, or AlphaGo? The question makes no sense!</p>



<p>So, to answer your question: yes, general intelligence in the psychometrician’s sense is not something that can be usefully extrapolated. And it’s “one-dimensional” only in the sense that a single statistical principal component can always be extracted from a set of intercorrelated variables.</p>



<p>One more point relevant to the general drift of the comments. My statement that “superintelligence” is incoherent is not a semantic quibble that the word is meaningless, and it’s not a pre-emptive strategy of Moving the True Scottish Goalposts. Sure, you could <em>define</em> “superintelligence,” just as you can define “miracle” or “perpetual motion machine” or “square circle.” And you could even recognize it if you ever saw it. But that does not make it coherent in the sense of being physically realizable.</p>



<p>If you’ll forgive me one more analogy, I think “superintelligence” is like “superpower.” Anyone can define “superpower” as “flight, superhuman strength, X-ray vision, heat vision, cold breath, super-speed, enhance hearing, and nigh-invulnerability.” Anyone could imagine it, and recognize it when he or she sees it. But that does not mean that there exists a highly advanced physiology called “superpower” that is possessed by refugees from Krypton!  It does not mean that anabolic steroids, because they increase speed and strength, can be “scaled” to yield superpowers. And a skeptic who makes these points is not quibbling over the meaning of the word <em>superpower, </em>nor would he or she balk at applying the word upon meeting a real-life Superman. Their point is that we almost certainly will never, in fact, meet a real-life Superman. That’s because he’s defined by human imagination, not by an understanding of how things work. We will, of course, encounter machines that are faster than humans, and that see X-rays, that fly, and so on, each exploiting the relevant technology, but “superpower” would be an utterly useless way of understanding them.</p>



<p>To bring it back to productive discussions of AI: there’s plenty of room to analyze the capabilities and limitations of particular intelligent algorithms and data structures—search, pattern-matching, error back-propagation, scripts, multilayer perceptrons, structure-mapping, hidden Markov models, and so on. But melting all these mechanisms into a global variable called “intelligence,” understanding it via turn-of-the-20<sup>th</sup>-century school tests, and mentally extrapolating it with a comic-book prefix, is, in my view, not a productive way of dealing with the challenges of AI.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<h2>Scott’s Response</h2>



<p>I wanted to drill down on the following passage:</p>



<blockquote class="wp-block-quote"><p>Sure, you could define “superintelligence,” just as you can define “miracle” or “perpetual motion machine” or “square circle.” And you could even recognize it if you ever saw it.  But that does not make it coherent in the sense of being physically realizable.</p></blockquote>



<p>The way I use the word “coherent,” it basically <em>means</em> “we could recognize it if we saw it.”  Clearly, then, there’s a sharp difference between this and “physically realizable,” although any physically-realizable empirical behavior must be coherent.  Thus, “miracle” and “perpetual motion machine” are both coherent but presumably not physically realizable.  “Square circle,” by contrast, is not even coherent.</p>



<p>You now seem to be saying that “superintelligence,” like “miracle” or “perpetuum mobile,” is coherent (in the “we could recognize it if we saw it” sense) but not physically realizable.  If so, then that’s a big departure from what I understood you to be saying before!  I thought you were saying that we couldn’t even recognize it.</p>



<p>If you do agree that there’s a quality that we could recognize as “superintelligence” if we saw it—and I don’t mean mere memory or calculation speed, but, let’s say, “the quality of being to John von Neumann in understanding and insight as von Neumann was to an average person”—and if the debate is merely over the physical realizability of that, then the arena shifts back to human evolution.  As you know far better than me, the human brain was limited in scale by the width of the birth canal, the need to be mobile, and severe limitations on energy.  And it wasn’t optimized for understanding algebraic number theory or anything else with no survival value in the ancestral environment.  So why should we think it’s gotten anywhere near the limits of what’s physically realizable in our world?</p>



<p>Not only does the concept of “superpowers” seem coherent to me, but from the perspective of someone a few centuries ago, we arguably <em>have</em> superpowers—the ability to summon any of several billion people onto a handheld video screen at a moment’s notice, etc. etc.  You’d probably reply that AI should be thought of the same way: just more tools that will enhance our capabilities, like airplanes or smartphones, not some terrifying science-fiction fantasy.</p>



<p>What I keep saying is this: we have the luxury of regarding airplanes and smartphones as “mere tools” <em>only</em> because there remain so many clear examples of tasks we can do that our devices can’t.  What happens when the devices can do <em>everything</em> important that we can do, much better than we can?  Provided we’re physicalists, I don’t see how we reject such a scenario as “not physically realizable.”  So then, are you making an empirical prediction that this scenario, although both coherent and physically realizable, won’t come to pass for thousands of years?  Are you saying that it might come to pass much sooner, like maybe this century, but even if so we shouldn’t worry, since a tool that can do everything important better than we can do it is still just a tool?</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6593"><span class="datestr">at July 22, 2022 01:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/07/21/flipping-until-lost">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/07/21/flipping-until-lost.html">Flipping until you are lost</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Start with any triangulation of a convex polygon, and then repeatedly choose a random diagonal and flip it, replacing the two triangles it borders with two different triangles. Eventually, these random flips will cause your triangulation to be nearly equally likely to be any of the possible triangulations of the polygon. But how long is “eventually”? My student Daniel Frishberg has a new answer, in our preprint “Improved mixing for the convex polygon triangulation flip walk” (<a href="https://arxiv.org/abs/2207.09972">arXiv:2207.09972</a>).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2006/fg6.png" alt="Flip graph of a hexagon" /></p>

<p>The triangulations of an \(n\)-gon, and the flips between them, form the vertices and edges of an \((n-3)\)-dimensional convex polytope called the <a href="https://en.wikipedia.org/wiki/Associahedron">associahedron</a>; the example above, showing the flips of a hexagon, is from <a href="https://11011110.github.io/blog/2006/10/13/another-gratuitously-nonplanar-drawing.html">an older post</a>. It’s difficult to convey all the symmetries of the four-dimensional associahedron (representing the triangulations of a heptagon) in a planar drawing, but here it is as a graph, at least:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2022/heptagon-flips.svg" alt="Flip graph of a heptagon" /></p>

<p>The number of steps until a random walk converges to near its <a href="https://en.wikipedia.org/wiki/Stable_distribution">stable distribution</a> is called its <a href="https://en.wikipedia.org/wiki/Markov_chain_mixing_time">mixing time</a>. So another way of stating the results in the preprint is that we provide a new bound on the mixing time of associahedra. <a href="https://tetali.math.gatech.edu/PUBLIS/mt.pdf">A 1997 paper of McShine and Tetali</a> shows that this mixing time is \(O(n^5\log n)\), and we improve the exponent slightly, to \(O(n^{4.75})\). Which doesn’t sound like much, but it’s the first improvement in 25 years!</p>

<p>It’s part of a line of research Daniel has been following on mixing times, <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>,  and <a href="https://en.wikipedia.org/wiki/Expander_graph">expansion</a> of large state spaces, including <a href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html">Hanoi graphs</a> and <a href="https://11011110.github.io/blog/2021/11/14/random-independent-sets.html">independent sets in bounded-treewidth graphs</a>. Like those other papers, it exploits known connections between mixing time, treewidth, expansion, and <a href="https://en.wikipedia.org/wiki/Multi-commodity_flow_problem">multi-commodity flow</a> to formulate the problem as one of finding a system of paths between every pair of vertices in the associahedron in such a way that each edge of the associahedron is used only for a small fraction of these paths. The construction of these paths exploits a recursive decomposition of the associahedra into products of smaller associahedra, obtained by cutting out a central triangle of any triangulated polygon and using associahedra to describe the triangulations of the remaining pieces.</p>

<p>The same paper also includes a weaker bound on the <a href="https://11011110.github.io/blog/2006/10/08/happy-endings-for.html">triangulations of rectangular grids of points</a>. The convex polygons have a <a href="https://en.wikipedia.org/wiki/Catalan_number">Catalan number</a> of triangulations, but for grids, we can’t say as precisely how many triangulations there are. Whatever this number \(N\) is, we show that these state spaces have treewidth \(N^{1-o(1)}\), by using a fixed choice of diagonals to partition the grid into smaller grids, triangulating each smaller grid independently, and showing that there are many of these partitioned triangulations and that their product structure gives them high expansion. But this result is still somewhat unsatisfactory because it doesn’t consider all triangulations and because the \(N^{o(1)}\) part of the bound is not polynomial in the grid size. Do triangulations of grids have polynomial mixing time?</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108689366093473745">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/07/21/flipping-until-lost.html"><span class="datestr">at July 21, 2022 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/107">TR22-107 |  Fractional certificates for bounded functions | 

	Shachar Lovett, 

	Jiapeng Zhang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A folklore conjecture in quantum computing is that the acceptance probability of a quantum query algorithm can be approximated by a classical decision tree, with only a polynomial increase in the number of queries. Motivated by this conjecture, Aaronson and Ambainis (Theory of Computing, 2014) conjectured that this should hold more generally for any bounded function computed by a low degree polynomial.

In this work we prove two new results towards establishing this conjecture: first, that any such polynomial has a small fractional certificate complexity; and second, that many inputs have a small sensitive block. We also give two new conjectures that, if true, would imply the Aaronson and Ambainis conjecture given our results. 

On the technical side, many classical techniques used in the analysis of Boolean functions seem to fail when applied to bounded functions. Here, we develop a new technique, based on a mix of combinatorics, analysis and geometry, and which in part extends a recent technique of Knop et al. (STOC 2021) to bounded functions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/107"><span class="datestr">at July 21, 2022 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/106">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/106">TR22-106 |  On Hardness of Testing Equivalence to Sparse Polynomials Under Shifts | 

	Suryajith Chillara, 

	Coral Grichener, 

	Amir Shpilka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We say that two given polynomials $f, g \in R[x_1, \ldots, x_n]$, over a ring $R$, are equivalent under shifts if there exists a vector $(a_1, \ldots, a_n)\in R^n$ such that $f(x_1+a_1, \ldots, x_n+a_n) = g(x_1, \ldots, x_n)$. This is a special variant of the polynomial projection problem in Algebraic Complexity Theory.  Grigoriev and Karpinski (FOCS 1990), Lakshman and Saunders (SIAM J. Computing, 1995), and Grigoriev and Lakshman (ISSAC 1995) studied the problem of testing polynomial equivalence of a given polynomial to "any" $t$-sparse polynomial, over the rational numbers, and gave exponential time algorithms. In this paper, we provide  hardness results for this problem.
  
Formally, for a ring $R$, let $\mathrm{SparseShift}_R$ be the following decision problem -- Given a polynomial $P(X)$, is there a vector $a$ such that $P(X+a)$ contains fewer monomials than $P(X)$. We show that $\mathrm{SparseShift}_R$ is at least as hard as checking if a given system of polynomial equations over $R[x_1,\ldots, x_n]$ has a solution (Hilbert's Nullstellensatz).  

As a consequence of this reduction, we get the following results.
1. $\mathrm{SparseShift}_\mathbb{Z}$ is undecidable.
2. For any ring $R$ (which is not a field) such that $\mathrm{HN}_R$ is $\mathrm{NP}_R$-complete over the Blum-Shub-Smale model of computation, $\mathrm{SparseShift}_{R}$ is also $\mathrm{NP}_{R}$-complete.  In particular, $\mathrm{SparseShift}_{\mathbb{Z}}$ is also $\mathrm{NP}_{\mathbb{Z}}$-complete.


We also study the gap version of the $\mathrm{SparseShift}_R$ and show the following.  
1. For every function $\beta:\mathbb{N}\to\mathbb{R}_+$ such that $\beta\in o(1)$, $N^\beta$-gap-$\mathrm{SparseShift}_\mathbb{Z}$ is also undecidable (where $N$ is the input length).
2. For $R=\mathbb{F}_p, \mathbb{Q}, \mathbb{R}$ or $\mathbb{Z}_q$ and for every $\beta&gt;1$  the $\beta$-gap-$\mathrm{SparseShift}_R$ problem is $\mathrm{NP}$-hard. Furthermore, there exists a constant $\alpha&gt;1$ such that for every $d = O(1)$ in the sparse representation model, and for every $d\leq n^{O(1)}$ in the arithmetic circuit model, the $\alpha^d$-gap-$\mathrm{SparseShift}_R$  problem is  $\mathrm{NP}$-hard when given polynomials of degree at most $d$, in $O(nd)$ many variables, as input.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/106"><span class="datestr">at July 21, 2022 07:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-798583255902437127">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/what-is-known-about-that-sequence.html">What is known about that sequence</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In my last post I wrote:</p><p><br /></p><p>---------------------------</p><div>Consider the recurrence</div><div><br /></div><div><br /></div><div>a_1=1</div><div><br /></div><div>for all n\ge 2, a_n = a_{n-1} + a_{n/2}.</div><div><br /></div><div>For which M does this recurrence have infinitely many n such that a_n \equiv  0 mod M?</div><div><br /></div><div><br /></div><div>I have written an open problems column on this for SIGACT News which also says</div><div>what is known (or at least what I know is known).  It will appear in the next issue.</div><div><br /></div><div>I will post that open problems column here on my next post.</div><div><br />Until then  I would like you to work on it, untainted by what I know. </div><div>----------------------------------------</div><div><br />I will now say what is known and point to the open problems column, co-authored with Emily Kaplitz and Erik Metz. </div><div><br /></div><div>If  M=2 or M=3 or M=5 or M=7 then there are infinitely many n such that a_n \equiv 0 mod M</div><div><br />If M\equiv 0 mod 4 then there are no n such that a_n \equiv 0 mod M</div><div><br /></div><div>Empirical evidence suggests that</div><div><br /></div><div>If M \not\equiv 0 mod 4 then there are infinitely many n such that a_n\equiv 0 mod M</div><div><br /></div><div>That is our conjecture. Any progress would be good- for example proving it for M=9. M=11 might be easier since 11 is prime. </div><div><br /></div><div>The article that I submitted is <a href="https://www.cs.umd.edu/~gasarch/open/cseq.pdf">HERE</a></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/what-is-known-about-that-sequence.html"><span class="datestr">at July 21, 2022 02:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6576">A low-tech solution</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Thanks so much to everyone who offered help and support as this blog’s comment section endured the weirdest, most motivated and sophisticated troll attack in its 17-year history.  For a week, a parade of self-assured commenters showed up to demand that I explain and defend my personal hygiene, private thoughts, sexual preferences, and behavior around female students (and, absurdly, to cajole me into taking my family on a specific Disney cruise ship).  In many cases, the troll or trolls <em>appropriated the names and email addresses of real academics</em>, imitating them so convincingly that those academics’ closest colleagues told me they were confident it was really them.  And when some trolls finally “outed” themselves, I had no way to know whether that was just another chapter in the trolling campaign.  It was enough to precipitate an epistemic crisis, where one actively doubts the authenticity of just about <em>every</em> piece of text.</p>



<p>The irony isn’t lost on me that I’ve endured this just as I’m <a href="https://scottaaronson.blog/?p=6484">starting my year-long gig at OpenAI</a>, to think, among other things, about the potential avenues for misuse of Large Language Models like GPT-3, and what theoretical computer science could contribute to mitigating them.  To say this episode has given me a more vivid understanding of the risks would be an understatement.</p>



<p><strong><em>But why didn’t I just block and ignore the trolls immediately?  Why did I bother engaging?</em></strong>  </p>



<p>At least a hundred people asked some variant of this question, and the answer is this.  For most of my professional life, this blog has been my forum, where anyone in the world could show up to raise any issue they wanted, as if we were tunic-wearing philosophers in the Athenian agora.  I prided myself on my refusal to take the coward’s way out and ignore anything—even, <em>especially</em>, severe personal criticism.  I’d witnessed how Jon Stewart, let’s say, would night after night completely eviscerate George W. Bush, his policies and worldview and way of speaking and justifications and lies, and then Bush would just continue the next day, totally oblivious, never deigning to rebut any of it.  And it became a core part of my identity that I’d never be like that.  If anyone on earth had a narrative of me where I was an arrogant bigot, a clueless idiot, etc., I’d confront that narrative head-on and refute it—or if I couldn’t, I’d reinvent my whole life.  What I’d <em>never</em> do is suffer anyone’s monstrous caricature of me to strut around the Internet unchallenged, as if conceding that only my academic prestige or tenure or power, rather than a reasoned rebuttal, could protect me from the harsh truths that the caricature revealed.</p>



<p>Over the years, of course, I carved out some exceptions: P=NP provers and quantum mechanics deniers enraged that I’d dismissed their world-changing insights.  Raving antisemites.  <em>Their</em> caricatures of me had no legs in any community I cared about.  But if an attack carried the implied backing of the whole modern social-justice movement, of thousands of angry grad students on Twitter, of <em>Slate</em> and <em>Salon</em> and <em>New York Times</em> writers and Wikipedia editors and university DEI offices, then the coward’s way out was closed.  The monstrous caricature then loomed directly over me; I could either parry his attacks or die.</p>



<p>With this stance, you might say, the astounding part is not that this blog’s “agora” model eventually broke down, but rather that it survived for so long!  I started blogging in October 2005.  It took until July 2022 for me to endure a full-scale “social/emotional denial of service attack” (not counting the comment-171 affair).  Now that I have, though, it’s obvious even to me that the old way is no longer tenable.</p>



<p>So what’s the solution?  Some of you liked the idea of requiring registration with real email addresses—but alas, when I tried to implement that, I found that WordPress’s registration system is a mess and I couldn’t see how to make it work.  Others liked the idea of moving to Substack, but others actively hated it, and in any case, even if I moved, I’d <em>still</em> have to figure out a comment policy!  Still others liked the idea of an army of volunteer moderators.  At least ten people volunteered themselves.</p>



<p>On reflection, the following strikes me as most directly addressing the actual problem.  I’m hereby establishing the <strong>Shtetl-Optimized Committee of Guardians</strong>, or SOCG (same acronym as the <a href="https://cse.buffalo.edu/socg21/socg.html">computational geometry conference</a> <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> ).  If you’re interested in joining, shoot me an email, or leave a comment on this post with your (real!) email address.  I’ll accept members only if I know them in real life, personally or by reputation, or if they have an honorable history on this blog.</p>



<p>For now, the SOCG’s only job is this: whenever I get a comment that gives me a feeling of unease—because, e.g., it seems trollish or nasty or insincere, it asks a too-personal question, or it challenges me to rebut a hostile caricature of myself—I’ll email the comment to the SOCG and ask what to do.  I commit to respecting the verdict of those SOCG members who respond, whenever a clear verdict exists.  The verdict could be, e.g., “this seems fine,” “if you won’t be able to resist responding then don’t let this appear,” or “email the commenter first to confirm their identity.”  And if I simply need reassurance that the commenter’s view of me is false, I’ll seek it from the SOCG before I seek it from the whole world.</p>



<p>Here’s what SOCG members can expect in return: I continue pouring my heart into this subscription-free, ad-free blog, and I credit you for making it possible—publicly if you’re comfortable with your name being listed, privately if not.  I buy you a fancy lunch or dinner if we’re ever in the same town.</p>



<p>Eventually, we might move to a model where the SOCG members can log in to WordPress and directly moderate comments themselves.  But let’s try it this way first and see if it works.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6576"><span class="datestr">at July 19, 2022 06:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4813311592001744524">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html">An open question about a sequence mod M.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post n/2 means floor{n/2}<div><br /></div><div>Consider the recurrence</div><div><br /></div><div><br /></div><div>a_1=1</div><div><br /></div><div>for all n\ge 2, a_n = a_{n-1} + a_{n/2}.</div><div><br /></div><div>For which M does this recurrence have infinitely many n such that a_n \equiv  0 mod M?</div><div><br /></div><div><br /></div><div>I have written an open problems column on this for SIGACT News which also says</div><div>what is known (or at least what I know is known).  It will appear in the next issue.</div><div><br /></div><div>I will post that open problems column here on my next post.</div><div><br />Until then  I would like you to work on it, untainted by what I know. </div><div><br /></div><div>ADDED LATER: I have now posted the sequel which includes a pointer to the open problems column. To save you time, I post it <a href="https://www.cs.umd.edu/~gasarch/open/cseq.pdf">here</a> as well.</div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html"><span class="datestr">at July 19, 2022 02:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/105">TR22-105 |  On vanishing sums of roots of unity in polynomial calculus and sum-of-squares | 

	Ilario Bonacina, 

	Nicola Galesi, 

	Massimo Lauria</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Vanishing sums of roots of unity can be seen as a natural generalization of knapsack from Boolean variables to variables taking values over the roots of unity. We show that these sums are hard to prove for polynomial calculus and for sum-of-squares, both in terms of degree and size.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/105"><span class="datestr">at July 18, 2022 01:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/104">TR22-104 |  On One-Sided Testing Affine Subspaces | 

	Nader Bshouty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the query complexity of one-sided $\epsilon$-testing the class of Boolean functions $f:F^n\to \{0,1\}$ that describe affine subspaces and Boolean functions that describe axis-parallel affine subspaces, where $F$ is any finite field. We give a polynomial-time $\epsilon$-testers that ask $\tilde O(1/\epsilon)$ queries. This improves the query complexity $\tilde O(|F|/\epsilon)$ in~[16]. 

We then show that any one-sided $\epsilon$-tester with proximity parameter $\epsilon&lt;1/|F|^d$ for the class of Boolean functions that describe $(n-d)$-dimensional affine subspaces and Boolean functions that describe axis-parallel $(n-d)$-dimensional affine subspaces must make at least
$\Omega(1/\epsilon+|F|^{d-1}\log n)$ and $\Omega(1/\epsilon+|F|^{d-1}n)$ queries, respectively.
This improves the lower bound $\Omega(\log n/\log\log n)$  that is proved in~[16] for $F=GF(2)$. We also give  testers for those classes with query complexity that almost match the lower bounds.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/104"><span class="datestr">at July 18, 2022 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20239">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/">Complexity 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Weaving patterns of proof and the accepted papers for this week’s conference</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/karendonde/" rel="attachment wp-att-20241"><img width="151" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/KarenDonde.jpg?resize=151%2C159&amp;ssl=1" class="alignright wp-image-20241" height="159" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://karendondehandwovens.com/page/1-Statement-Bio.html">her bio page</a></font></td>
</tr>
</tbody>
</table>
<p>
Karen Donde is the Chair of <a href="https://complexityexhibition.org/">Complexity 2022</a>, which is being held this month in Knoxville, Tennessee. This is not the same as the <a href="https://computationalcomplexity.org/Archive/2022/fullsite/">Computational Complexity 2022</a> (CCC22) conference, which is being held <b>in-person</b> at the University of Pennsylvania this <b>Wednesday, July 20</b>, through <b>Saturday, July 23</b>. The Knoxville event is not about computer science, nor dynamical nor biological complexity. It is about the art of weaving complex patterns in textiles by hand.</p>
<p>
Today we collect pointers to the papers at CCC22 after saying something separate about weaving and proofs.</p>
<p>
Unlike CCC22, the Knoxville exhibition is also <a href="https://complexityexhibition.org/all-works/">open online</a>. Here is a detail from the Complex Weaver first prize <a href="https://complexityexhibition.org/melanie-olde-morphology-i/">winner</a>, an example of three-dimensional weaving:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/complexweavingprize/" rel="attachment wp-att-20242"><img width="250" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/ComplexWeavingPrize.jpg?resize=250%2C270&amp;ssl=1" class="aligncenter wp-image-20242" height="270" /></a></p>
<p>
Like CCC22, the Knoxville event has a program committee. It consists of <a href="http://www.juliehedges.co.uk/">Julie Hedges</a>, <a href="https://www.spadystudios.com/">Robyn Spady</a>, and <a href="https://www.bettyvera.com/">Betty Vera</a>. Also like CCC22, it has a steering committee. Besides Donde, the committee consists of <a href="https://www.etsy.com/shop/MargepyeTextiles?ref=profile_header">Margaret Dugger</a>, <a href="https://www.pinterest.com/dyen2weave/">Diane Smith</a>, <a href="https://wovenful.com/an-interview-with-sarah-fortin/">Sarah Fortin</a>, <a href="https://pikespeakweavers.org/member-galleries/nggallery/member-galleries/susan-bowman">Susan Bowman</a>, <a href="https://newworldtextiles.com/about/">Eileen Hallman</a>, <a href="https://www.woodlandsgallerync.com/artists/pat-brown">Pat Brown</a>, <a href="https://www.facebook.com/katiedoanhandweaver/">Katie Doan</a>, <a href="https://www.weavingschool.com/Geri.html">Geri Forkner</a>, <a href="https://www.linkedin.com/in/cathy-mccarthy-42347828/">Cathy McCarthy</a>, <a href="http://www.spinningforth.com/perso/perso.html">Ruth MacGregor</a>, and <a href="https://weavingspace.co.uk/#About-Weaving-Space">Cally Booker</a>. The gender imbalance is more extreme than for CCC22 or what we <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">noted</a> last fall for POPL22 (also see end of <a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/">this</a>). Oh well.</p>
<p>
</p><p></p><h2> Weaving Into Theory </h2><p></p>
<p></p><p>
Karen Donde also writes a <a href="https://karen63615.wixsite.com/karendondeblog">blog</a>, <em>Speaking of Weaving</em>. The blog has numerous technical how-to articles. In some places they verge on mathematical theory. </p>
<p>
There are more express connections between mathematics and weaving. The mathematics teacher <a href="http://www.patrickhonner.com/about.html">Patrick Honner</a> has a <a href="https://mrhonner.com/weaving">page</a> of posts on weaving. He was featured in an <a href="https://naturalmath.com/2012/07/weaving-mathematics/">article</a> “Weaving your way through mathematics” by the mathematics educator Maria Droujkova. See also a <a href="https://www.youtube.com/watch?v=Breul3cnW9s">video</a> on weaving and the mathematics of <a href="https://en.wikipedia.org/wiki/Spirograph">Spirograph</a> patterns.</p>
<p>
On the computing theory side, connections to cellular automata are shown in a 2017 <a href="https://www.semanticscholar.org/paper/The-Complexity-of-Braids,-Cables,-and-Weaves-with-Holden/5b83e108623548c8b073a1e96b00d027eefb197e">paper</a> by Joshua Holden. There is also a recent <a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445750">paper</a> from CMU’s Human-Computer Interaction Institute on “Enabling Personal Computational Handweaving with a Low-Cost Jacquard Loom.” </p>
<p>
</p><p></p><h2> Weaving Into Proofs </h2><p></p>
<p></p><p>
Our association to weaving was really motivated, however, by an <a href="https://www.quantamagazine.org/how-do-mathematicians-know-their-proofs-are-correct-20220713/">article</a> last Wednesday by Steven Strogatz, a Cornell mathematician and extraordinary popularizer whom I (Ken) have known since we were undergraduates together at Princeton. The article interviews the Harvard mathematician Melanie Matchett Wood and is titled, “How Do Mathematicians Know Their Proofs Are Correct?”</p>
<p>
We have written about proofs and the social issue of verification <a href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/">several</a> <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">times</a> <a href="https://rjlipton.wpcomstaging.com/2020/12/10/the-future-of-mathematics/">recently</a>. But this article goes into a more particular topic that we tried to get at in a <a href="https://rjlipton.wpcomstaging.com/2014/09/09/a-challenge-from-dyson/">series</a> of <a href="https://rjlipton.wpcomstaging.com/2014/06/06/is-this-a-proof-2/">posts</a> in <a href="https://rjlipton.wpcomstaging.com/2014/10/11/more-on-testing-dysons-conjecture/">2014</a>. This is about whether probabilistic modeling—not the <a href="https://en.wikipedia.org/wiki/Probabilistic_method">Probabilistic Method</a> which is airtight—can give confidence in conjectures that is tantamount to proof.</p>
<p>
Strogatz’s interview leads off with a reference to a 2019 <a href="https://www.quantamagazine.org/where-proof-evidence-and-imagination-intersect-in-math-20190314/">article</a> for <em>Quanta</em> titled, “Where Proof, Evidence, and Imagination Intersect.” That article is by the same Patrick Honner whom we just mentioned for weaving, and gives caveats of how bias and unrecognized implicit constraints can creep into models, so as to invalidate them. </p>
<p>
Wood begins with basic “coinflip” random models of primes—such as mentioned in the above-listed posts—and fixes on a bias-revealing model that we also covered <a href="https://rjlipton.wpcomstaging.com/2016/03/26/bias-in-the-primes/">here</a>. She then describes how they incrementally build rules for adjusting coin weights to compensate for biases introduced by small-number cases: </p>
<blockquote><p><b> </b> <em> “So the model is something that starts with this coin-flipping model, but then it’s modified by all these other rules, and all the other things that we know about the primes. And once you put all of those things that we do know into the model, you then ask [it] well, do you see, infinitely often, coins coming up prime just 2 apart? And the model tells you, oh, yes, we do see that. In fact, we see it at this very particular rate we can give you a formula for. And then … you see that the model gives you a very accurate prediction for the number of pairs of twin primes you’ll find as you go along. And so then you think, you know, maybe this model knows what it’s talking about.” </em>
</p></blockquote>
<p></p><p>
In response to Strogatz noting that the accuracy must be judged by long computer runs, Wood is quick to emphasize that the rules given to the model are determined manually:</p>
<blockquote><p><b> </b> <em> “But for building this model and coming up with the formula that the model gives. You know, that’s done by hand, essentially, by mathematicians thinking about the model and figuring out with it. … [A]t some point, the computer stops. You know, there’s only so much computing power. But that formula that you would get, that the model would give you, that you could prove is true, again, about this model coin-flipping situation, that formula will keep going. You can put bigger and bigger numbers into that formula, much bigger than your computer could ever, ever compute with.” </em>
</p></blockquote>
<p>
</p><p></p><h2> Proof of the Loom? </h2><p></p>
<p></p><p>
Wood goes on to describe <em>universality</em> in probability theory as signifying “that there are certain kinds of machines that if you put in a lot of random inputs, you can predict the output.” She gives as a bellwether example how the Central Limit Theorem creates such a universal machine for the bell curve. Regardless of an unknown distribution <em>D</em>, if you take means of samples from <em>D</em>, then the bell curve gives progressively—and provably—more accurate predictions of your outputs. Strogatz catches the warp and asks whether “somehow we’re getting the idea of universality to show up in number theory? Or am I dreaming?” Her peroration is:</p>
<blockquote><p><b> </b> <em> “[W]hat my collaborators and I work on is trying to make that kind of dream a reality so that, that some of these puzzling questions about numbers that we don’t know the answer to, maybe we could understand that there are patterns that come out, like a bell curve, like a normal distribution, that we can prove came out of the machine even if we don’t know what mysteries were put in.” </em>
</p></blockquote>
<p></p><p>
So she is building a machine that takes rules as input—like Jacquard cards for a loom—and produces patterns that are analyzable. We use the computational success of the machine to judge how well universality has taken hold—as theoretically it must—and generate proofs from formulas based on the <em>a-priori</em> predicted outputs using the rules input thus far. </p>
<p>
This is like building a loom for weaving proofs—where, however, there is still the question of confidence in how well the patterns obtained match reality. Such doubt notwithstanding, the process may also augment non-linear ways of evaluating claimed proofs of the kind we discussed <a href="https://rjlipton.wpcomstaging.com/2020/06/13/proof-checking-not-line-by-line/">here</a> and recently debated <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">here</a>.</p>
<p>
</p><p></p><h2> The Papers </h2><p></p>
<p></p><p>
The proofs in the accepted papers were, to be sure, evaluated by the standard expert social process. Here they are, lifted from the conference’s own program <a href="https://computationalcomplexity.org/Archive/2022/program.php">page</a>. Clicking on the time of the talk gives a pointer to the paper. </p>
<p>
<b>Wednesday, July 20</b></p>
<p>
<a href="https://arxiv.org/pdf/2205.10749.pdf">9:00</a> “Vanishing Spaces of Random Sets and Applications to Reed-Muller Codes.”<br />
Siddharth Bhandari, Prahladh Harsha, Ramprasad Saptharishi, Srikanth Srinivasan</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16572/pdf/LIPIcs-CCC-2022-10.pdf">9:30</a> “New Near-Linear Time Decodable Codes Closer to the GV Bound.”<br />
Guy Blanc and Dean Doron</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16576/pdf/LIPIcs-CCC-2022-14.pdf">10:00</a> “The plane test is a local tester for Multiplicity Codes.”<br />
Dan Karliner, Roie Salama and Amnon Ta-Shma</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/025/">13:30</a> “Efficient Low-Space Simulations From the Failure of the Weak Pigeonhole Principle” (co-winner Best student paper).<br />
Oliver Korten</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/023/">14:00</a> “Nisan-Wigderson generators in Proof Complexity: New lower bounds.”<br />
Erfan Khaniki</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16577/pdf/LIPIcs-CCC-2022-15.pdf">14:30</a> “Pseudorandom Generators, Resolution and Heavy Width.”<br />
Dmitry Sokolov</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16565/pdf/LIPIcs-CCC-2022-3.pdf">15:30</a> “Hitting Sets for Regular Branching Programs.”<br />
Andrej Bogdanov, William Hoza, Gautam Prakriya and Edward Pyne</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/021/">16:00</a> “Improved Pseudorandom Generators for <img src="https://s0.wp.com/latex.php?latex=%7BAC%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{AC^0}" class="latex" /> Circuits” (co-winner Best student paper).<br />
Xin Lyu</p>
<p>
<a href="https://arxiv.org/abs/2103.14134">16:40</a> “Random restrictions and PRGs for PTFs in Gaussian Space.”<br />
Zander Kelley and Raghu Meka</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/024/">17:00</a> “Pseudorandomness of Expander Random Walks for Symmetric Functions and Permutation Branching Programs.”<br />
Louis Golowich and Salil Vadhan</p>
<p>
<b>Thursday, July 21</b></p>
<p>
<a href="https://arxiv.org/abs/2111.02999">9:00</a> “Quantum search-to-decision reductions and the state synthesis problem.”<br />
Sandy Irani, Anand Natarajan, Chinmay Nirkhe, Sujit Rao and Henry Yuen</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16590/pdf/LIPIcs-CCC-2022-28.pdf">9:30</a> “Influence in Completely Bounded Block-multilinear Forms and Classical Simulation of Quantum Algorithms.”<br />
Nikhil Bansal, Makrand Sinha and Ronald de Wolf</p>
<p>
<a href="https://arxiv.org/abs/2111.10409">10:00</a> “The Acrobatics of BQP” (winner – Best paper award).<br />
Scott Aaronson, Devon Ingram and William Kretschmer</p>
<p>
<a href="https://eprint.iacr.org/2021/513">13:30</a> “On One-way Functions from NP-Complete Problems.”<br />
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://nanashima.github.io">14:00</a> “Finding Errorless Pessiland in Error-Prone Heuristica.”<br />
Shuichi Hirahara and Mikito Nanashima</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/084/">14:30</a> “Characterizing Derandomization Through Fine-Grained Hardness of Levin-Kolmogorov Complexity.”<br />
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://www.researchgate.net/publication/356891307_Almost_Polynomial_Factor_Inapproximability_for_Parameterized_k-Clique">15:30</a> “Almost Polynomial Factor Inapproximability for Parameterized k-Clique.”<br />
Karthik C. S. and Subhash Khot</p>
<p>
<a href="https://arxiv.org/abs/2106.12710">16:00</a> “Certifying solution geometry in random CSPs: counts, clusters and balance.”<br />
Jun-Ting Hsieh, Sidhanth Mohanty and Jeff Xu</p>
<p>
<a href="https://arxiv.org/abs/2203.03705">16:40</a> “High-Dimensional Expanders from Chevalley Groups.”<br />
Ryan O’Donnell and Kevin Pratt</p>
<p>
<a href="https://arxiv.org/abs/2205.02374">17:10</a> “The composition complexity of majority.”<br />
Victor Lecomte, Prasanna Ramakrishnan and Li-Yang Tan</p>
<p>
<b>Friday, July 22</b></p>
<p>
<a href="https://arxiv.org/abs/2004.14318">9:00</a> “The Approximate Degree of Bipartite Perfect Matching.”<br />
Gal Beniamini</p>
<p>
<a href="https://arxiv.org/abs/2205.06249">9:30</a> “Optimal-Degree Polynomial Approximations for Exponentials and Gaussian Kernel Density Estimation.”<br />
Amol Aggarwal and Josh Alman</p>
<p>
<a href="https://arxiv.org/abs/2108.13578">10:00</a> “<img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_p}" class="latex" />-Spread and Restricted Isometry Properties of Sparse Random Matrices.”<br />
Venkatesan Guruswami, Peter Manohar and Jonathan Mosheiff</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/074/">13:30</a> “On Randomized Reductions to the Random Strings.”<br />
Michael Saks and Rahul Santhanam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/086/">14:00</a> “Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs.”<br />
Lijie Chen, Jiatu Li and Tianqi Yang</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16575/pdf/LIPIcs-CCC-2022-13.pdf">14:30</a> “A better-than-3log(n) depth lower bound for De Morgan formulas with restrictions on top gates.”<br />
Ivan Mihajlin and Anastasia Sofronova</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/072/">15:30</a> “Probabilistic Kolmogorov Complexity with Applications to Average-Case Complexity.”<br />
Halley Goldberg, Valentine Kabanets, Zhenjian Lu and Igor C. Oliveira</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16588/pdf/LIPIcs-CCC-2022-26.pdf">16:00</a> “Symmetry of Information from Meta-Complexity.”<br />
Shuichi Hirahara</p>
<p>
<a href="https://arxiv.org/abs/2201.08895">16:30</a> “On the Satisfaction Probability of k-CNF Formulas.”<br />
Till Tantau</p>
<p>
<b>Saturday, July 23</b></p>
<p>
<a href="https://arxiv.org/abs/2202.09883">9:00</a> “On Efficient Noncommutative Polynomial Factorization via Higman Linearization.”<br />
Vikraman Arvind and Pushkar Joglekar</p>
<p>
<a href="https://www.researchgate.net/publication/360332836_Improved_Low-Depth_Set-Multilinear_Circuit_Lower_Bounds">9:30</a> “Improved Low-Depth Set-Multilinear Circuit Lower Bounds.”<br />
Deepanshu Kush and Shubhangi Saraf</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16594/pdf/LIPIcs-CCC-2022-32.pdf">10:15</a> “On the Partial Derivative Method Applied to Lopsided Set-Multilinear Polynomials.”<br />
Nutan Limaye, Srikanth Srinivasan and Sebastien Tavenas</p>
<p>
<a href="https://arxiv.org/abs/2205.15168">10:45</a> “Subrank and Optimal Reduction of Scalar Multiplications to Generic Tensors.”<br />
Harm Derksen, Visu Makam and Jeroen Zuiddam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/026/">11:00</a> “Trading Time and Space in Catalytic Branching Programs.”<br />
Ian Mertz and James Cook</p>
<p>
<a href="https://arxiv.org/abs/2201.10997">12:30</a> “Linear Branching Programs and Directional Affine Extractors.”<br />
Svyatoslav Gryaznov, Pavel Pudlak and Navid Talebanfard</p>
<p>
<a href="https://www.cs.mcgill.ca/~robere/research.html">14:00</a> “Further collapses in TFNP.”<br />
Mika Goos, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere and Ran Tao</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16592/pdf/LIPIcs-CCC-2022-30.pdf">14:30</a> “Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes.”<br />
Sarah Bordage, Mathieu Lhotel, Jade Nardi and Hugues Randriam</p>
<p>
<a href="https://eprint.iacr.org/2022/168">15:00</a> “Hardness of Approximation for Stochastic Problems via Interactive Oracle Proofs.”<br />
Gal Arnon, Alessandro Chiesa and Eylon Yogev</p>
<p>
We congratulate all the authors of the accepted papers.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can we really build mathematical looms for helping us generate proofs at high level?</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/"><span class="datestr">at July 18, 2022 05:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/">Full Professorships and Tenure-Track Professorships at Ruhr-University Bochum, Germany (apply by July 29, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite outstanding applicants from all areas of computer science, including the Foundations of Computer Science. Salary and working conditions are internationally very competitive and come with civil servant status. Full professorships are chair positions with administrative staff. We offer dual-career &amp; relocation support and a family-friendly environment. Knowledge of German is not required.</p>
<p>Website: <a href="https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/">https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/</a><br />
Email: career@casa.rub.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/"><span class="datestr">at July 17, 2022 10:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/103">TR22-103 |  Almost Chor--Goldreich Sources and Adversarial Random Walks | 

	Dean Doron, 

	Dana Moshkovitz, 

	Justin Oh, 

	David Zuckerman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A Chor--Goldreich (CG) source [CG88] is a sequence of random variables  $X = X_1 \circ \ldots \circ X_t$, each $X_i \sim \{0,1 \{^d$, such that each $X_i$ has $\delta d$ min-entropy for some constant $\delta &gt; 0$, even conditioned on any fixing of $X_1 \circ \ldots \circ X_{i-1}$. We typically think of $d$ as constant. We extend this notion in several ways, and most notably allow each $X_i$ to be only $\gamma$-close to having $\delta d$ min entropy.

Studying such almost CG sources allows us to achieve pseudorandomness results which were not known to hold even for standard CG sources, and even for the weaker model of Santha--Vazirani sources [SV86]. We construct a deterministic condenser that on input $X$, outputs a distribution which is close to having constant entropy gap, namely a distribution $Z \sim \{0,1 \}^m$ for $m \approx \delta dt$ with min-entropy $m-O(1)$. 

Our new primitive readily implies fast simulation results:

*	We can simulate $\mathbf{BPP}$ using almost CG sources with constant multiplicative slowdown.
*	When the randomized algorithm has small failure probability, we can simulate it using almost CG sources with no multiplicative slowdown. This result extends to randomized protocols as well, and any setting in which we cannot simply cycle over all seeds, and a ``one-shot'' simulation is needed.

Moreover, our framework is flexible enough to work even when the $X_i$-s only have Shannon entropy rather than min-entropy, and in some cases, even when a few $X_i$-s are completely damaged.

Our main technical contribution is a novel analysis of random walks which may be of independent interest. We analyze walks with adversarially correlated steps, each step being entropy-deficient, on good enough lossless expanders. We prove that such walks (or certain interleaved walks on two expanders), starting from a fixed vertex and walking according to $X_1\circ \ldots \circ X_t$, accumulate most of the entropy in $X$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/103"><span class="datestr">at July 15, 2022 07:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/07/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/07/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://doi.org/10.1080/17513472.2022.2069417">Creating weaving patterns from subdivision schemes</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@msmathcomputer/108561058326179122">\(\mathbb{M}\)</a>),</span> new paper by Lipschütz, Reitebuch, Skrodzi, and Polthier, and explanatory thread by Skrodzi.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@peterrowlett/108578769727211199">How would you make a sphere from three circles?</a>, asks Peter Rowlett after his son said he could do it.</p>
  </li>
  <li>
    <p><a href="http://blog.computationalcomplexity.org/2022/06/a-gadget-for-3-colorings.html">Counting 3-colorings</a> and <a href="https://blog.computationalcomplexity.org/2022/06/counting-number-of-3-colorings-of-graph">follow-up post</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108584298969286802">\(\mathbb{M}\)</a>).</span> Like all “natural” \(\mathsf{NP}\)-complete problems (and many easier problems), the 3-coloring problem should have a \(\#\mathsf{P}\)-complete counting version, but the gadgets needed to prove it are a little subtle and tracking down the history of proof of this result took some effort.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/tH6vLXMaCwQ">Polyhedra in which all but one edge have a right angle</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@henryseg/108584342158901821">\(\mathbb{M}\)</a>),</span> 3d-printed based on a construction used by Sydler to study Dehn invariants. Achieving this property leads to surprisingly complicated polyhedra.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/tag/2022-fields-and-abacus-medals/">The 2022 Fields medals</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mathcination/108595654085200673">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://11011110.github.io/blog/2020/07/31/linkage.html">Two years ago</a> I linked to <a href="https://cp4space.wordpress.com/2020/07/25/rational-dodecahedron-inscribed-in-unit-sphere/">a post by Adam Goucher</a>, solving <a href="https://mathoverflow.net/q/234212/440">an old MathOverflow question</a> by showing that it is possible to find a dodecahedron, combinatorially equivalent to a regular one, with rational coordinates, inscribed in a unit sphere. But <a href="https://cp4space.hatsya.com/2022/06/20/infinitely-many-rational-dodecahedra/">now there are infinitely many</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108602499475969576">\(\mathbb{M}\)</a>)!</span> Some messy algebra, and then some work with elliptic curve group operations, eventually simplifies down to a parametric family with a dodecahedron for each integer right triangle.</p>
  </li>
  <li>
    <p>For integer \(A\), a grid of  \(n\) points has roughly \(n^2\sigma(A)/A\) area-\(A\) triangles, where \(\sigma\) is the sum of divisors <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108606089910227326">\(\mathbb{M}\)</a>);</span> see <a href="https://users.renyi.hu/~p_erdos/1971-20.pdf">Erdős &amp; Purdy 1971</a> who used non-square grids and factorial \(A\) to find points with \(\Omega(n\log\log n)\) unit-area triangles. So how big is \(\sigma(A)/A\)? <a href="https://en.wikipedia.org/wiki/Divisor_function#Robin's_theorem">It depends on the Riemann hypothesis!</a> If RH is true, at most \(e^\gamma\log\log A\) for \(A&gt;5040\). If not, slightly larger infinitely often.</p>
  </li>
  <li>
    <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-ICGT-22.pdf">Slides from my talk on “Graphs in Nature”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108611451435162895">\(\mathbb{M}\)</a>)</span> at the International Colloquium on Graph Theory and Combinatorics in Montpellier, France.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Prince_Rupert%27s_cube">Prince Rupert’s cube</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108623867716491741">\(\mathbb{M}\)</a>):</span> a cube can fit through a square hole drilled through another cube its size, or even slightly smaller. Now a Good Article on Wikipedia. I’ve been wondering: is it possible to make Prince Rupert’s Borromean rings, by drilling square holes into three unit cubes, each simultaneously passing through the hole in the next one?</p>
  </li>
  <li>
    <p>On the CSTheory stackexchange, Alexey Milovanov asks for updates on the (as far as I know still unknown) complexity of an old problem, <a href="https://cstheory.stackexchange.com/q/51680/95">finding shortest addition chains</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108627574398080196">\(\mathbb{M}\)</a>).</span> This highlights something I love about editing Wikipedia: if you take the effort to track down a repeated error in the literature, and <a href="https://en.wikipedia.org/wiki/Special:Diff/206806087">document it properly in the right Wikipedia article</a>, then maybe 14 years later the correction rather than the error can be common knowledge.</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/press/maa-reviews/pop-up-geometry">The MAA reviews Joe O’Rourke’s new book, <em>Pop-Up Geometry: The Mathematics Behind Pop-Up Cards</em></a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108632717277132574">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2207.04923">Killing a vortex</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108637362908341033">\(\mathbb{M}\)</a>),</span> by Thilikos and Wiederrecht. Robertson and Seymour’s structural decomposition of minor-closed graph families glues together surface-embedded graphs, a few arbitrarily-connected “apex” vertices, and “vortices”, bounded-pathwidth graphs attached to faces. For graph matching, vortices are problematic. This new preprint describes the families that don’t need them and shows that they are exactly the ones whose matchings can be counted quickly.</p>
  </li>
  <li>
    <p>Scott Aaronson, quantum complexity theorist and debunker of quantum hype on <a href="https://scottaaronson.blog/">his blog</a>, is also a <a href="https://scottaaronson.blog/?p=6552">target of trolls who have pushed him to back down from his free-speech principles and restrict comments</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108641124975905102">\(\mathbb{M}\)</a>).</span> <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">Boaz Barak gives some support</a>. Via Boaz I re-found Scott’s 2005 “<a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a>” debunking soap bubble and rubber band solvers for hard optimization problems. Worth a re-read!</p>
  </li>
  <li>
    <p>I tend to pick technologically better solutions over popular ones, despite popularity’s importance for long-term viability <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108649887665081093">\(\mathbb{M}\)</a>).</span> Which is why I just switched from a gas range to induction. It has all the responsiveness of gas (vs the glacial response of conventional electric), is more efficient, has a smaller carbon footprint, fewer noxious emissions, etc. These are still uncommon in Southern California, but new laws require electric appliances for new construction, and I hope that with familiarity they will become better liked as well.</p>
  </li>
  <li>
    <p><a href="https://www.surgehq.ai//blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled">Seriously bad data in Google’s GoEmotions dataset</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108653832818847808">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32090389">via</a>), some 58K reddit comments categorized by affect. Opinions in the post and comments vary on why the categorization was so inaccurate, including lack of context, farming it out to poorly-paid workers in countries less likely to be familiar with the specific idioms used in the comments, or maybe just that it’s a hard problem.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/07/15/linkage.html"><span class="datestr">at July 15, 2022 04:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/">Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The ability of large neural networks to generalize is commonly believed to stem from an implicit regularization — a tendency of gradient-based optimization towards predictors of low complexity.
A lot of effort has gone into theoretically formalizing this intuition.
Tackling modern neural networks head-on can be quite difficult, so existing analyses often focus on simplified models as stepping stones.
Among these, matrix and tensor factorizations have attracted significant attention due to their correspondence to linear neural networks and certain shallow non-linear convolutional networks, respectively. 
Specifically, they were shown to exhibit an implicit tendency towards low matrix and tensor ranks, respectively.</p>

<p>This post overviews a recent <a href="https://arxiv.org/abs/2201.11729">ICML 2022 paper</a> with <a href="https://asafmaman101.github.io/">Asaf Maman</a> and <a href="http://www.cohennadav.com/">Nadav Cohen</a>, in which we draw closer to practical deep learning by analyzing <em>hierarchical tensor factorization</em>, a model equivalent to certain <em>deep non-linear</em> convolutional networks. 
We find that, analogously to matrix and tensor factorizations, the implicit regularization in hierarchical tensor factorization strives to lower a notion of rank (called hierarchical tensor rank).
This turns out to have surprising implications on the origin of locality in convolutional networks, inspiring a practical method (explicit regularization scheme) for improving their performance on tasks with long-range dependencies.</p>

<h2 id="background-matrix-and-tensor-factorizations">Background: Matrix and Tensor Factorizations</h2>

<p>To put our work into context, let us briefly go over existing dynamical characterizations of implicit regularization in matrix and tensor factorizations.
In both cases they suggest an incremental learning process that leads to low rank solutions (for respective notions of rank). We will then see how these characterizations transfer to the considerably richer hierarchical tensor factorization.</p>

<h3 id="matrix-factorization-incremental-matrix-rank-learning">Matrix factorization: Incremental matrix rank learning</h3>
<p><em>Matrix factorization</em> is arguably the most extensively studied model in the context of implicit regularization. 
Indeed, it was already discussed in four previous posts (<a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">1</a>, <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">2</a>, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">3</a>, <a href="http://www.offconvex.org/2019/06/03/trajectories/">4</a>), but for completeness we will present it once more. 
Consider the task of minimizing a loss $\mathcal{L}_M : \mathbb{R}^{D, D’} \to \mathbb{R}$ over matrices, e.g. $\mathcal{L}_M$ can be a matrix completion loss — mean squared error over observed entries from some ground truth matrix. 
Matrix factorization refers to parameterizing the solution $W_M \in \mathbb{R}^{D, D’}$ as a product of $L$ matrices, and minimizing the resulting objective using <em>gradient descent (GD)</em>:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{W^{(1)}, \ldots, W^{(L)}} \mathcal{L}_M \big ( W_M \big ) := \mathcal{L}_M \big ( W^{(1)} \cdots W^{(L)} \big ) ~.
\]
</div>
<p>Essentially, matrix factorization amounts to applying a linear neural network (fully connected neural network with no non-linearity) for minimizing $\mathcal{L}_M$. 
We can explicitly constrain the matrix rank of $W_M$ by limiting the shared dimensions of the weight matrices $\{ W^{(l)} \}_l$. However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>Although it was initially conjectured that GD (with small initialization and step size) over matrix factorization minimizes a norm (see the seminal work of <a href="https://arxiv.org/abs/1705.09280">Gunasekar et al. 2017</a>), recent evidence points towards an implicit matrix rank minimization (see <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>; <a href="https://arxiv.org/abs/1904.13262">Gidel et al. 2019</a>; <a href="https://arxiv.org/abs/2005.06398">Razin &amp; Cohen 2020</a>; <a href="https://arxiv.org/abs/2011.13772">Chou et al. 2020</a>; <a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).
In particular, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> characterized the dynamics of $W_M$’s singular values throughout optimization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>):</strong>
Gradient flow (GD with infinitesimal step size) over matrix factorization initialized near zero leads the $r$’th singular value of $W_M$, denoted $\sigma_M^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_M^{(r)} (t) \propto \sigma_M^{(r)} (t)^{2 - 2/L}} ~.
]</p>
</blockquote>

<p>As can be seen from the theorem above, singular values evolve at a rate proportional to their size exponentiated by $2 - 2 / L$. This means that they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
When initializing near the origin (as commonly done in practice), we therefore expect singular values to progress slowly at first, and then, upon reaching a certain threshold, to quickly rise until convergence. 
<strong>These dynamics create an incremental learning process that promotes solutions with few large singular values and many small ones, i.e. low matrix rank solutions</strong>.
In their paper, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> support this qualitative explanation through theoretical illustrations and empirical evaluations. 
For example, the following plot reproduces one of their experiments:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/mf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 1:</b> 
Dynamics of singular values during GD over matrix factorization <br /> — incremental learning leads to low matrix rank.
</i>
</div>
<p><br />
We note that the incremental matrix rank learning phenomenon was later on used to prove exact matrix rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).</p>

<h3 id="tensor-factorization-incremental-tensor-rank-learning">Tensor factorization: Incremental tensor rank learning</h3>

<p>Despite the significant interest in matrix factorization, as a theoretical surrogate for deep learning its practical relevance is rather limited. 
It corresponds to linear neural networks, and thus misses non-linearity — a crucial aspect of modern neural networks.
As was mentioned in a <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">previous post</a>, by moving from matrix (two-dimensional array) to tensor (multi-dimensional array) factorizations it is possible to address this limitation.</p>

<p>A classical scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for more details on this scheme, see <a href="http://www.kolda.net/publication/TensorReview.pdf">this excellent survey</a>).
Given a loss $\mathcal{L}_T : \mathbb{R}^{D_1, \ldots, D_N} \to \mathbb{R}$ over $N$-dimensional tensors, e.g. $\mathcal{L}_T$ can be a tensor completion loss, we simply refer by <em>tensor factorization</em> to parameterizing the solution $\mathcal{W}_T \in \mathbb{R}^{D_1, \ldots, D_N}$ as a CP factorization, and minimizing the resulting objective via GD:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \mathcal{L}_T \big ( \mathcal{W}_T \big ) := \mathcal{L}_T \big (  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big) ~.
\]
</div>
<p>Each term $\mathbf{w}_r^{(1)} \otimes \cdots \otimes \mathbf{w}_r^{(N)}$ in the sum is called a <em>component</em>, and $\otimes$ stands for outer product.
The concept of rank naturally extends from matrices to tensors.
For a given tensor $\mathcal{W}$, its <em>tensor rank</em> is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that we can explicitly constrain the tensor rank of $\mathcal{W}_T$ by limiting the number of components $R$.
But, since our interest lies in implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>Similarly to how matrix factorization captures linear neural networks, tensor factorization is equivalent to certain <em>shallow non-linear</em> convolutional networks (with multiplicative non-linearity).
This equivalence was discussed in a couple of previous posts (<a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">1</a>, <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">2</a>), for the exact details behind it feel free to check out the preliminaries section of <a href="https://arxiv.org/abs/2201.11729">our paper</a> and references therein.
The bottom line is that tensor factorization takes us one step closer to practical neural networks.</p>

<p>Motivated by the incremental learning dynamics in matrix factorization, in a <a href="https://arxiv.org/abs/2102.09972">previous paper</a> (see accompanying <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">blog post</a>) we analyzed the behavior of component norms during optimization of tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>):</strong>
Gradient flow over tensor factorization initialized near zero leads the $r$’th component norm, $\sigma_T^{(r)} (t) := || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_T^{(r)} (t) \propto \sigma_T^{(r)} (t)^{2 - 2/N}} ~.
]</p>
</blockquote>

<p>The dynamics of component norms in tensor factorization are structurally identical to those of singular values in matrix factorization.
Accordingly, we get a momentum-like effect that attenuates the movement of small component norms and accelerates that of large ones.
This suggests that, <strong>in analogy with matrix factorization, when initializing near zero components tend to be learned incrementally, resulting in a bias towards low tensor rank</strong>.
The following plot empirically demonstrates this phenomenon:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 2:</b> 
Dynamics of component norms during GD over tensor factorization <br /> — incremental learning leads to low tensor rank.
</i>
</div>
<p><br />
Continuing with the analogy to matrix factorization, the incremental tensor rank learning phenomenon formed the basis for proving exact tensor rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>).</p>

<h2 id="hierarchical-tensor-factorization">Hierarchical Tensor Factorization</h2>

<p>Tensor factorization took us beyond linear predictors, yet it still lacks a critical feature of modern neural networks — depth (recall that it corresponds to <em>shallow</em> non-linear convolutional networks).
A natural extension that accounts for both non-linearity and depth is <em>hierarchical tensor factorization</em> — our protagonist — which corresponds to certain <em>deep</em> non-linear convolutional networks (with multiplicative non-linearity).
This equivalence is actually not new, and has facilitated numerous analyses of expressive power in deep learning (see <a href="https://arxiv.org/abs/1705.02302">this survey</a> for a high-level overview).</p>

<p>As opposed to tensor factorization, which is a simple construct dating back to at least the early 20’th century (<a href="https://onlinelibrary.wiley.com/doi/10.1002/sapm192761164">Hitchcock 1927</a>), hierarchical tensor factorization was formally introduced only recently (<a href="https://link.springer.com/article/10.1007/s00041-009-9094-9">Hackbusch &amp; Kuhn 2009</a>), and is much more elaborate.
Its exact definition is rather technical (the interested reader can find it in <a href="https://arxiv.org/abs/2201.11729">our paper</a>).
For our current purpose it suffices to know that a hierarchical tensor factorization consists of multiple local tensor factorizations, whose components we call the <em>local components</em> of the hierarchical factorization.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_htf_cnn_blog.png" style="width: 900px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 3:</b> 
Tensor factorization, which is a sum of components (outer products), <br /> corresponds to a shallow non-linear convolutional neural network (CNN).
<br /> Hierarchical tensor factorization, which consists of multiple local tensor <br /> factorizations, corresponds to a deep non-linear CNN.
</i>
</div>
<p><br />
In contrast to matrices, which have a single standard definition for rank, tensors posses several different definitions for rank.
Hierarchical tensor factorizations induce their own such notion, known as <em>hierarchical tensor rank</em>.
Basically, if a tensor can be represented through hierarchical tensor factorization with few local components, then it has low hierarchical tensor rank.
This stands in direct analogy with tensor rank, which is low if the tensor can be represented through tensor factorization with few components.</p>

<p>Seeing that the implicit regularization in matrix and tensor factorizations leads to low matrix and tensor ranks, respectively, in <a href="https://arxiv.org/abs/2201.11729">our paper</a> we investigated whether the implicit regularization in hierarchical tensor factorization leads to low hierarchical tensor rank. 
That is, whether GD (with small initialization and step size) over hierarchical tensor factorization learns solutions that can be represented with few local components.
Turns out it does.</p>

<h2 id="dynamical-analysis-incremental-hierarchical-tensor-rank-learning">Dynamical Analysis: Incremental Hierarchical Tensor Rank Learning</h2>

<p>At the heart of our analysis is the following dynamical characterization for local component norms during optimization of hierarchical tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal):</strong>
Gradient flow over hierarchical tensor factorization initialized near zero leads the $r$’th local component norm in a local tensor factorization, denoted $\sigma_H^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_H^{(r)} (t) \propto \sigma_H^{(r)} (t)^{2 - 2/K}} ~,
]
where $K$ is the number of axes of the local tensor factorization.</p>
</blockquote>

<p>This should really feel like deja vu, as these <strong>dynamics are structurally identical to those of singular values in matrix factorization and component norms in tensor factorization!</strong>
Again, we have a momentum-like effect, by which local component norms move slower when small and faster when large.
As a result, <strong>when initializing near zero local components tend to be learned incrementally, yielding a bias towards low hierarchical tensor rank</strong>.
In <a href="https://arxiv.org/abs/2201.11729">the paper</a> we provide theoretical and empirical demonstrations of this phenomenon.
For example, the following plot shows the evolution of local component norms at some local tensor factorization under GD:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/htf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 4:</b> 
Dynamics of local component norms during GD over hierarchical <br /> tensor factorization — incremental learning leads to low hierarchical tensor rank.
</i>
</div>
<p><br /></p>

<h2 id="practical-implication-countering-locality-in-convolutional-networks-via-explicit-regularization">Practical Implication: Countering Locality in Convolutional Networks via Explicit Regularization</h2>

<p>We saw that in hierarchical tensor factorization GD leads to solutions of low hierarchical tensor rank.
But what does this even mean for the associated convolutional networks?</p>

<p>Hierarchical tensor rank is known (<a href="https://arxiv.org/abs/1605.06743">Cohen &amp; Shashua 2017</a>) to measure the strength of long-range dependencies modeled by a network.
In the context of image classification, e.g., it quantifies how well we take into account dependencies between distant patches of pixels.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/local_vs_non_local_dep.png" style="width: 430px; padding-bottom: 8px; padding-top: 8px;" />
<br />
<i><b>Figure 5:</b> 
Illustration of short-range (local) vs. long-range dependencies in image data.
</i>
</div>
<p><br />
<strong>The implicit regularization towards low hierarchical tensor rank in hierarchical tensor factorization therefore translates to an implicit regularization towards <em>locality</em> in the corresponding convolutional networks</strong>.
At first this may not seem surprising, since convolutional networks typically struggle or completely fail to learn tasks entailing long-range dependencies.
However, conventional wisdom attributes this failure to expressive properties (i.e. to an inability of convolutional networks to realize functions modeling long-range dependencies), suggesting that addressing the problem requires modifying the architecture.
Our analysis, on the other hand, reveals that implicit regularization also plays a role: it is not just a matter of expressive power, the optimization algorithm is implicitly pushing towards local solutions.
Inspired by this observation, we asked:</p>

<blockquote>
  <p><strong>Question:</strong>
Is it possible to improve the performance of modern convolutional networks on long-range tasks via explicit regularization (without modifying their architecture)?</p>
</blockquote>

<p>To explore this prospect, <strong>we designed explicit regularization that counteracts locality by promoting high hierarchical tensor rank (i.e. long-range dependencies)</strong>.
Then, through a series of controlled experiments, <strong>we confirmed that it can greatly improve the performance of modern convolutional networks (e.g. ResNets) on long-range tasks</strong>.</p>

<p>For example, the following plot displays test accuracies achieved by a ResNet on an image classification benchmark, in which it is possible to control the spatial range of dependencies required to model.
When increasing the range of dependencies, the test accuracy obtained by an unregularized network substantially deteriorates, reaching performance no better than random guessing.
As evident from the plot, our regularization closes the gap between short- and long-range tasks, significantly boosting generalization on the latter.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/pathfinder_resnet18_with_reg_blog.png" style="width: 430px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 6:</b> 
Specialized explicit regularization promoting high hierarchical tensor rank (i.e. long-range dependencies between image regions) can counter the locality of convolutional networks, significantly improving their performance on long-range tasks.
</i>
</div>
<p><br /></p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>
<p>Looking forward, there are two main takeaways from our work:</p>

<ol>
  <li>
    <p>Across three different neural network types (equivalent to matrix, tensor, and hierarchical tensor factorizations), we have an architecture-dependant notion of rank that is implicitly lowered. Moreover, the underlying mechanism for this implicit regularization is identical in all cases. This leads us to believe that implicit regularization towards low rank may be a general phenomenon. If true, finding notions of rank lowered for different architectures can facilitate an understanding of generalization in deep learning.</p>
  </li>
  <li>
    <p>Our findings imply that the tendency of modern convolutional networks towards locality may largely be due to implicit regularization, and not an inherent limitation of expressive power as often believed. More broadly, they showcase that deep learning architectures considered suboptimal for certain tasks can be greatly improved through a right choice of explicit regularization. 
Theoretical understanding of implicit regularization may be key to discovering such regularizers.</p>
  </li>
</ol>

<p><strong><em><a href="https://noamrazin.github.io/">Noam Razin</a></em></strong></p></div>







<p class="date">
<a href="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/"><span class="datestr">at July 15, 2022 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/102">TR22-102 |  Range Avoidance for Low-depth Circuits and Connections to Pseudorandomness | 

	Xiuhan Wang, 

	Venkatesan Guruswami, 

	Xin Lyu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In the range avoidance problem, the input is a multi-output Boolean circuit with more outputs than inputs, and the goal is to find a string outside its range (which is guaranteed to exist). We show that well-known explicit construction questions such as finding binary linear codes achieving the Gilbert-Varshamov bound or list-decoding capacity, and constructing rigid matrices, reduce to the range avoidance problem of log-depth circuits, and by a further recent reduction [Ren, Santhanam, and Wang, ECCC 2022] to $NC^0_4$ circuits where each output depends on at most $4$ input bits. 

On the algorithmic side, we show that range avoidance for $NC^0_2$ circuits can be solved in polynomial time. We identify a general condition relating to correlation with low-degree parities that implies that any almost pairwise independent set has some string that avoids the range of every circuit in the class. We apply this to $NC^0$ circuits, and to small width CNF/DNF and general De Morgan formulae (via a connection to approximate-degree), yielding non-trivial small hitting sets for range avoidance in these cases.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/102"><span class="datestr">at July 15, 2022 07:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/">Postdoc at University of Cologne (apply by August 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position in theoretical computer sciece, algorithms and data structures or algorithmic data analysis is available in the Algorithmic Data Analysis group led by Prof. Dr. Christian Sohler in the department of mathematics/computer science at University of Cologne.</p>
<p>Website: <a href="https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer">https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer</a><br />
Email: sohler@cs.uni-koeln.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/"><span class="datestr">at July 15, 2022 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/101">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/101">TR22-101 |  A Near-Cubic Lower Bound for 3-Query Locally Decodable Codes from Semirandom CSP Refutation | 

	Omar Alrabiah, 

	Pravesh Kothari, 

	Venkatesan Guruswami, 

	Peter Manohar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A code $C \colon \{0,1\}^k \to \{0,1\}^n$ is a $q$-locally decodable code ($q$-LDC) if one can recover any chosen bit $b_i$ of the message $b \in \{0,1\}^k$ with good confidence by randomly querying the encoding $x = C(b)$ on at most $q$ coordinates. Existing constructions of $2$-LDCs achieve $n = \exp(O(k))$, and lower bounds show that this is in fact tight. However, when $q = 3$, far less is known: the best constructions achieve $n = \exp(k^{o(1)})$, while the best known results only show a quadratic lower bound $n \geq \widetilde{\Omega}(k^2)$ on the blocklength.

In this paper, we prove a near-cubic lower bound of $n \geq \widetilde{\Omega}(k^3)$ on the blocklength of $3$-query LDCs. This improves on the best known prior works by a polynomial factor in $k$. Our proof relies on a new connection between LDCs and refuting constraint satisfaction problems with limited randomness. Our quantitative improvement builds on the new techniques for refuting semirandom instances of CSPs developed in [GKM22] and, in particular, relies on bounding the $(\infty \to 1)$-norm of appropriate Kikuchi matrices.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/101"><span class="datestr">at July 14, 2022 09:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/100">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/100">TR22-100 |  Streaming complexity of CSPs with randomly ordered constraints | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Raghuvansh Saxena, 

	Madhu Sudan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate a study of the streaming complexity of constraint satisfaction problems (CSPs) when the constraints arrive in a random order. We show that there exists a CSP, namely Max-DICUT, for which random ordering makes a provable difference. Whereas a $4/9 \approx 0.445$ approximation of DICUT requires $\Omega(\sqrt{n})$ space with adversarial ordering, we show that with random ordering of constraints there exists a $0.48$-approximation algorithm that only needs $O(\log n)$ space. We also give new algorithms for Max-DICUT in variants of the adversarial ordering setting. Specifically, we give a two-pass  $O(\log n)$ space $0.48$-approximation algorithm for general graphs and a single-pass $\tilde{O}(\sqrt{n})$ space $0.48$-approximation algorithm for bounded degree graphs.
    
    On the negative side, we prove that CSPs where the satisfying assignments of the constraints support a one-wise independent distribution require $\Omega(\sqrt{n})$-space for any non-trivial approximation, even when the constraints are randomly ordered. This was previously known only for adversarially ordered constraints. Extending the results to randomly ordered constraints requires switching the hard instances from a union of random matchings to simple Erdos-Renyi random (hyper)graphs and extending tools that can perform Fourier analysis on such instances. 
    
    The only CSP to have been considered previously with random ordering is Max-CUT where the ordering is not known to change the approximability. Specifically, it is known to be as hard to approximate with random ordering as with adversarial ordering, for $o(\sqrt{n})$ space algorithms. Our results show a richer variety of possibilities and motivate further study of CSPs with randomly ordered constraints.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/100"><span class="datestr">at July 14, 2022 06:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/099">TR22-099 |  Equivalence Test for Read-Once Arithmetic Formulas | 

	Nikhil Gupta, 

	Chandan Saha, 

	Bhargav Thankey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the polynomial equivalence problem for orbits of read-once arithmetic formulas (ROFs). Read-once formulas have received considerable attention in both algebraic and Boolean complexity and have served as a testbed for developing effective tools and techniques for analyzing circuits. Two $n$-variate polynomials $f, g \in \mathbb{F}[\mathbf{x}]$ are equivalent, denoted as $f \sim g$, if there is an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$. The orbit of $f$ is the set of all polynomials equivalent to $f$. We investigate the complexity of the following two natural problems on ROFs:

1. Equivalence test for ROFs: Given black-box access to $f$, check if it is in the orbit of an ROF. If yes, output an ROF $C$ and an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = C(A\mathbf{x})$.  
2. Polynomial equivalence for orbits of ROFs: Given black-box access to $f$ and $g$ in the orbits of two unknown ROFs, check if $f \sim g$. If yes, output an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$.

These problems are significant generalizations of two well-studied problems in algebraic complexity, namely reconstruction of ROFs and quadratic form equivalence. In this work, we give the first randomized polynomial-time algorithms (with oracle access to quadratic form equivalence) to solve the two problems. The equivalence test works for general ROFs; it also implies an efficient learning algorithm for random arithmetic formulas of unbounded depth and fan-in (in the high number of variables setting). The algorithm for the second problem, which invokes the equivalence test, works for mildly restricted ROFs, namely additive-constant-free ROFs.  	
	
The equivalence test is based on a novel interplay between the factors and the essential variables of the Hessian determinant of an ROF, the essential variables of the ROF, and certain special structures in the ROF that we call "skewed paths". To our knowledge, the Hessian of a general ROF (or even a depth-4 ROF) has not been analyzed before. Analyzing the Hessian and combining the knowledge gained from it with the skewed paths to recursively discover formulas in the orbits of sub-ROFs of lower depth (without incurring an exponential blow-up due to unbounded depth) constitute the main technical contributions of this work.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/099"><span class="datestr">at July 14, 2022 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">My friend, Scott Aaronson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><a href="https://windowsontheory.files.wordpress.com/2022/07/image.png"><img width="471" alt="" src="https://windowsontheory.files.wordpress.com/2022/07/image.png?w=750" class="wp-image-8446" height="643" /></a></figure></div>


<p>This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a> on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and even philosophy. Scott has been one of the great popularizers of our field even before he started blogging in 2005. His surveys and blog posts provide some of the best introductions to our field. For example, when investigating  <a href="http://quant-ph/0502072">P vs NP and physical reality</a>, Scott actually <a href="https://scottaaronson.blog/?p=266">went out and verified</a> that nature indeed cannot solve an NP-complete problem via finding the globally minimal energy configuration of soap bubbles.  Through his blog, popular writing, and research, Scott has done more than anyone else to introduce new people of all backgrounds to theoretical computer science. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img width="543" alt="" src="https://lh6.googleusercontent.com/JRN6bxodVkY1JiHSWOEx4QXtvmH3twDk-O5jGgsio1IM92aUUxYZ87i7T0qU8bfrCnVbBWIt1OV4soNgnhoMpTHvnJBIVk8tAPO_oK2_WLzHbrGfWNpRDQyazey-CG9V0YLDDZ2r45pwOKsSF4PZaQ" height="408" /></figure></div>


<p>One of Scott’s endearing qualities is his openness to all people. While many of us would ignore a random email or anonymous blog comment, Scott would patiently explain for the millionth time why quantum computers can’t solve NP-hard problems by “trying all solutions in parallel” or why Bell’s Inequality does indeed rule out hidden-variable theories of nature. Alas, the same openness also results in him sometimes giving too much attention and caring far too much about the opinions of Internet “trolls” that are not worthy of his time. </p>



<p>While Scott has always attracted some vitriol, recently this has taken to a <a href="https://scottaaronson.blog/?p=6546">new level</a>, with commenters attacking his integrity, his speech mannerisms, even his T-shirt choice/frequency, and worst of all, his family, with misogynistic attacks on his wife and xenophobic and ableist attacks on neurodivergent researchers.</p>



<p>None of these people have made a fraction of the contributions of Scott not just to science, but also to broadening the diversity of computer science, and other causes including <a href="https://scottaaronson.blog/?p=6411">assisting women dealing with Texas’ restrictive abortion laws</a>. (As full disclosure, one of the causes <a href="https://scottaaronson.blog/?p=6256">Scott helped raise money</a> for is <a href="https://www.addiscoder.com/">AddisCoder</a> and <a href="https://jamcoder.org.jm/">JamCoders</a> of which I am a board member. I just came back from a <a href="https://twitter.com/boazbaraktcs/status/1545765613167067136?s=20&amp;t=53HLUIjmCwBsl1oYhEQXhw">week teaching in Jamaica</a>, the students were amazing and are so thankful for the chance to participate in this program; they couldn’t care less how often Scott changes his shirt.)</p>



<p>I am grateful that Scott is a member of our scientific community and proud to call him my friend. Does this mean that I agree with all his positions? Absolutely not. I tend to be on his left on many issues  (though am probably more conservative when it comes to oracle-based complexity..). Are there people he’s friendly with whom I even more strongly disagree with, and whose views I might even find repugnant? Probably. But it doesn’t matter, all of us are connected via 6 degrees of separation. If we start to “recursively cancel” every one that is somehow connected to someone we find odious, then we would not be able to talk to anyone.</p>



<p>I hope that Scott is not disheartened by these attacks, and continues to contribute for many years to CS research and education, outreach, and humanity at large.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/"><span class="datestr">at July 13, 2022 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6552">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6552">Choosing a new comment policy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong>Update (July 13):</strong> I was honored to read <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">this post</a> by my friend Boaz Barak.</p>



<p><strong>Update (July 14):</strong> By now, comments on this post allegedly from four CS professors  — namely, Josh Alman, Aloni Cohen, Rana Hanocka, and Anna Farzindar — as well as from the graduate student “BA,” <em>have been unmasked as from impersonator(s)</em>.</p>



<p>I’ve been the target of a motivated attack-troll (or multiple trolls, but I now believe just one) who knows about the CS community. This might be the single weirdest thing that’s happened to me in 17 years of blogging, surpassing even the legendary <a href="https://scottaaronson.blog/?p=277">Ricoh printer episode</a> of 2007.  It obviously underscores the need for a new, stricter comment policy, which is what this whole post was about.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Yesterday and today, both my work and my enjoyment of the James Webb images were interrupted by an anonymous troll, who used the <em>Shtetl-Optimized</em> comment section to heap <a href="https://scottaaronson.blog/?p=6546#comment-1941041">libelous abuse</a> on me—derailing an anodyne quantum computing discussion to opine at length about how I’m a disgusting creep who surely, probably, maybe has lewd thoughts about his female students.  Unwisely or not, I allowed it all to appear, and replied to all of it.  I had a few reasons: I wanted to prove that I’m now strong enough to withstand bullying that might once have driven me to suicide.  I wanted, frankly, many readers to come to my defense (thanks to those who did!).  I at least wanted readers to <em>see</em> firsthand what I now regularly deal with: the emotional price of maintaining this blog.  Most of all, I wanted my feminist, social-justice-supporting readers to either explicitly endorse or (hopefully) explicitly repudiate the unambiguous harassment that was now being gleefully committed in their name.</p>



<p>Then, though, the same commenter upped the ante further, by heaping misogynistic abuse on my wife <a href="https://www.cs.utexas.edu/~danama/">Dana</a>—while <em>still</em>, ludicrously and incongruously, cloaking themselves in the rhetoric of social justice.  Yes: apparently the woke, feminist thing to do is now to rate female computer scientists on their looks.</p>



<p>Let me be blunt: I cannot continue to write <em>Shtetl-Optimized</em> while dealing with regular harassment of me and my family.  At the same time, I’m also determined not to “surrender to the terrorists.”  So, I’m weighing the following options:</p>



<ul><li>Close comments except to commenters who provide a real identity—e.g., a full real name, a matching email address, a website.</li><li>Move to Substack, and then allow only commenters who’ve signed up.</li><li>Hire someone to pre-screen comments for me, and delete ones that are abusive or harassing (to me or others) before I even see them.  (Any volunteers??)</li><li>Make the comment sections for readers only, eliminating any expectation that I’ll participate.</li></ul>



<p>One thing that’s clear is that the status quo will not continue.  I can’t “just delete” harassing or abusive comments, because the trolls have gotten too good at triggering me, and they will continue to weaponize my openness and my ethic of responding to all possible arguments against me.</p>



<p>So, regular readers: what do you prefer?</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6552"><span class="datestr">at July 12, 2022 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3528238240939023029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html">Review of The Engines of Cognition: Essays From the LessWrong Forum/Meta question about posts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> A while back I reviewed<i> A Map that Reflects the Territory</i> which is a collection of essays posted on the lesswrong forum. My review is <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong.pdf">here</a>. I posted it to both this blog and to the lesswrong forum. In both cases I posted a link to it. My post to lesswrong is <a href="https://www.lesswrong.com/posts/JXTEDFCC5r4dW2tta/review-of-a-map-that-reflects-the-territory">here</a></p><p>On the lesswrong post many of the comments, plus some private emails, told me NO BILL- don't post a link, post it directly as text. It was not clear how to do that, but I got it done with help.</p><p>On complexity blog nobody commented that this was a problem. Then again, nobody commented at all, so its not clear what to make of that. </p><p>So</p><p>Meta Question: Is posting a link worse than posting direct text? Note that the book review was 12 pages long and looked great in LaTeX. </p><p>Meta Question: Why did lesswrong care about the format but complexityblog did not (Probably answer: omplexityblog readers did not care at all, whereas Lesswrong cared about what I though about Lesswrong)</p><p>Another Question, not Meta. One of the comments was (I paraphrase)</p><p><i>When I open a pdf file I expected to see something in the style of an academic paper. This is written in very much chatty, free-flowing blog post style with jokes like calling neologisms ``newords'', so the whole think felt more off-kilter than was intended. The style of writing would prob work better as an HTML blog post (which could then be posted directly as a Lesswrong post here instead of hosted elsewhere and linked.)</i></p><p>I think its interesting that the format of an article telegraphs (in this case incorrectly) what type of article it will be. Is this a common problem?  I have had the experience of reading a real academic paper and being surprised that some joke or cultural-reference is in it, though I do not object to this. </p><p>Another comment and question</p><p><i>I was surprised the post only had 11 karma when I saw it (William had send me an advance copy and I'd really liked reading it) but when I saw that it was a link post, I understood why.</i></p><p>I find this hilarious- they have some way the posts are rated!  For one, Lance told me very early on to never worry about comments, and I don't. Second, it reminds me of the Black Mirror episode <a href="https://en.wikipedia.org/wiki/Nosedive_(Black_Mirror)">Nosedive</a>.</p><p>ANYWAY, I have reviewed another collection of essays for less wrong, this one called <i>The Engines of</i> <i>Cognition. </i>I am posting it here as a link: <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong2.pdf">here</a>  and I will post it on lesswrong as full text (with help) in a few days. </p><p>I am posting it so I can get comments before I submit it to the SIGACT News book review column. But this is odd since I think this blog has more readers than SIGACT news has subscribers, so perhaps THIS is its real debut, not that. And of course the lesswrong forum is a place where more will read it since its about them. </p><p>So- I appreciate comments to make it a better review!</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html"><span class="datestr">at July 12, 2022 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/data-transfer/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/data-transfer/">A Data-Based Perspective on Transfer Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2207.05739" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/data-transfer" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br />
<i>
In our latest paper, we present a framework for pinpointing the impact of the source datasets in transfer learning. Our framework enables us to improve transfer learning performance by removing source datapoints that are detrimental for a specific target task. It also unlocks various other capabilities, such as debugging transfer learning failures, automatically identifying granular subpopulations in the target dataset, and detecting data leakage between source and target datasets. 
</i></p>

<p>Transfer learning is a widely utilized technique for adapting a model trained on a source dataset to improve performance on a downstream target task. Used in applications ranging from <a href="https://arxiv.org/abs/2101.06871">radiology</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.pdf">autonomous driving</a>, to <a href="https://arxiv.org/abs/1510.00098">satellite imagery analysis</a>, the transfer learning paradigm also fuels the recent emergence of large vision and language models trained on enormous amounts of data, such as <a href="https://openai.com/blog/clip/">CLIP</a> or <a href="https://openai.com/blog/gpt-3-apps/">GPT-3</a>.</p>

<p>Why is transfer learning so effective though? And, in particular, what drives transfer learning performance? Definitely, much depends on the properties of the source model, i.e., the model trained on the source dataset. For example, recent works highlight the impact of the model’s <a href="https://arxiv.org/abs/2101.06871">architecture</a>, <a href="https://arxiv.org/abs/1805.08974">accuracy</a>, <a href="https://arxiv.org/abs/2007.08489">adversarial vulnerability</a>, and <a href="https://arxiv.org/abs/1905.05901">training procedure</a>.</p>

<p>But, in addition to the source model, it is hard to not expect the source dataset to play a major role as well. Indeed, several works have shown that increasing the size of the dataset usually <a href="https://arxiv.org/abs/1912.11370">boosts transfer learning performance</a>. <a href="https://arxiv.org/abs/1811.07056">Others</a> have found that limiting the source dataset to images that are relevant to the target task can help as well. So: what is the exact role of the source dataset in transfer learning?</p>

<p>In our most <a href="https://arxiv.org/abs/2207.05739">recent paper</a>, we present a framework for pinpointing the impact of the source dataset on the downstream predictions in transfer learning. This framework draws inspiration from techniques such as <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> and it enables us, in particular, to automatically identify source datapoints that—positively or negatively—impact transfer learning performance.</p>

<p>We’ll now walk through how we calculate the influence of the source dataset in transfer learning, and then demonstrate how our framework can be used to:</p>

<ul type="a">
  <li>Boost transfer learning performance by removing detrimental source datapoints;</li>
  <li>Automatically extract granular subpopulations in the target dataset by projecting the  class hierarchy of the source dataset onto it; and</li>
  <li>Surface pathologies such as source-target data leakage and misleading or mislabelled source datapoints.</li>
</ul>

<h2 id="computing-the-influence-of-the-source-dataset">Computing the Influence of the Source Dataset</h2>

<p>How to pinpoint the relationship between the source dataset’s composition and the model’s downstream predictions? We build here on the <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> methodology (check out our <a href="https://gradientscience.org/datamodels-1/">previous post</a> for a deeper dive into these) to study the counterfactual effect of removing source datapoints on the target model’s predictions. However, unlike in the standard supervised setting in which the focus is on individual datapoints, here we focus on removing entire classes. This is motivated by the fact that we expect the removal of entire classes to have a more measurable impact on the features learned by the source model (and thus the resulting model’s predictions).</p>

<p>So, at a high level, we first train a large number of models on random subsets of classes in the source dataset, and fine-tune them on the target task. Then we compute the influence of a source class on a target example by simply measuring the average difference in the model’s performance on a given target example when the class was included versus excluded from the source dataset. A positive influence value thus means that including the class improved the model’s performance on that example, while a negative value indicates that the class was detrimental to the correctness of the corresponding model’s prediction.</p>

<h2 id="identifying-the-most-influential-classes-of-the-source-dataset">Identifying the Most Influential Classes of the Source Dataset</h2>
<p>Now that we’ve calculated these influences, how can we use them to study transfer learning? First, let’s take a look at the ranking of the most positively and negatively influencing classes for a variety of target tasks. We first look at the influences from different ImageNet classes to the entire CIFAR-10 test set:</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/most_influencing_classes.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Most influential" />
</p>

<p>Note that in the most positive source classes tend to semantically overlap with the classes in the target dataset. For example, tailed frog and sorrel horse have the most positive influence for the CIFAR-10 dataset, which contains the classes frog and horse.</p>

<p>Also, the plot above suggests that there is a number of source classes, such as bookshop or jigsaw puzzle, whose inclusion actually hurts the overall transfer learning performance. So what happens if we indeed remove these classes from the source dataset? Well, as one might hope, they do boost the transfer learning performance on a variety of downstream image classification:</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/main_counterfactual_exp.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Counterfactual main" /></p>
<div class="footnote">
Target task accuracies after removing the most positively or negatively influential ImageNet classes from the source dataset.
</div>

<p>In fact, we can get an accuracy boost of nearly 2.5% on CIFAR-10 (as compared to what one gets from pre-training with the full ImageNet dataset).</p>

<h2 id="leveraging-influences-to-study-transfer-learning">Leveraging Influences to Study Transfer Learning</h2>

<p>Above, we used our framework to pinpoint the most influential—be it positively or negatively— classes for transfer learning. Here, we’ll discuss how our framework provides us with a broader set of tools for studying transfer learning, including: (1) debugging transfer learning failures, (2) automatically extracting granular subpopulations in the target dataset, and (3) detecting data leakage between source and target datasets.</p>

<h3 id="1--debugging-the-failures-of-the-transferred-model">(1)  Debugging the failures of the transferred model</h3>
<p>Suppose our transferred model wrongly predicts the dog image displayed in the figure below—it labels it as a horse. Can we use our framework to understand why our model is making this mistake? Yes! The influences we computed enable us to identify the source class sorrel horse as one having a strong (and the strongest) negative influence on our image of interest. This suggests that the features learned by the source model due to the presence of this class might be the culprit here. Indeed, once we remove the sorrel horse class from the source dataset, our model now makes the correct prediction on our dog image much more frequently (with respect to the randomness of the training procedure).</p>

<div style="overflow: auto; text-align: center;" id="debug_examples_widget"></div>
<div class="footnote">
Identifying highly negatively influencing source classes can explain why our transfer learning model made a mistake (middle). Once we remove the most negatively influencing class from the source dataset, the model predicts the correct label more frequently (right). Click through the thumbnails on the left to see more examples!
</div>



<h3 id="2-automatically-extracting-granular-subpopulations-in-the-target-dataset">(2) Automatically extracting granular subpopulations in the target dataset</h3>
<p>Imagine you want to find all the images of ostriches in the CIFAR-10 dataset. However, the problem is that CIFAR-10 does not contain any subpopulation annotations that could help with this task and having to manually look for ostriches among all the images in the bird class is not a very appealing alternative. Our framework allows us to do something much more scalable!</p>

<p>Indeed, as we already observed, the most positively influencing source classes usually semantically overlap with the images in the target dataset they influence the most. In fact, this goes further: the target images which are most influenced by a given source class tend to share relevant salient features too. So, to identify our ostrich subpopulation in CIFAR-10, we just need to look at all the images that are most influenced by the source class “ostrich”! Below we display some of the CIFAR-10 images identified in this way.</p>

<div style="overflow: auto; text-align: center;" id="subpop_examples_widget"></div>
<div class="footnote">
The CIFAR-10 images which are most positively influenced by a particular ImageNet class. Click through the thumbnails on the left to see more examples!
</div>

<h3 id="3-detecting-data-leakage-and-misleading-source-dataset-examples">(3) Detecting data-leakage and misleading source dataset examples</h3>
<p>Thus far, we have focused on the role of classes in the source dataset in transfer learning. But we can also compute the influences of specific source examples on the transferred model’s predictions. This turns out to enable us to isolate, in particular, instances of data leakage and misleading examples in the source dataset.</p>

<p>Indeed, below, we display ImageNet training examples that are highly influential on CIFAR-10 test examples. The source images that have a highly positive influence are often identical copies of images from the target task (just at a higher resolution)—a clear example of data leakage. On the other hand, images that have a high negative influence tend to be the ones that are misleading, mislabeled, or otherwise surprising.  For example, the presence of the (amusing) ImageNet image of a flying lawnmower (see below) hurts the performance on a CIFAR-10 image of a regular (but similarly shaped) airplane.</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/detect_leakage.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Detect leakage" />
</p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we described a new framework for pinpointing the impact of the source dataset in transfer learning. Our framework allows one to improve the transfer learning performance on a range of downstream tasks by identifying and removing source datapoints that are detrimental. Furthermore, by using our framework one can automatically extract granular subpopulations in the target dataset by leveraging the fine-grained class hierarchy of the source dataset, better understand how the errors of the model on the downstream task are rooted in the source dataset, and detect potential data leakage from the source to the downstream dataset. We believe our framework provides a new perspective on transfer learning by highlighting the role of the source dataset in the transfer learning pipeline and gives us a toolkit for performing a fine-grained analysis of it.</p></div>







<p class="date">
<a href="https://gradientscience.org/data-transfer/"><span class="datestr">at July 12, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/">PostDoc position in Computer Science Logic at University of Sheffield (apply by July 20, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply.</p>
<p>The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions">https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions</a><br />
Email: j.t.virtema@sheffield.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/"><span class="datestr">at July 11, 2022 03:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
