<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 28, 2021 07:38 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/11/25/tcs-talk-wednesday-december-1-william-kuszmaul-mit/">TCS+ talk: Wednesday, December 1 — William Kuszmaul, MIT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, December 1th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="https://sites.google.com/site/williamkuszmaul"><strong>William Kuszmaul</strong></a> from MIT will speak about “<em>Linear Probing Revisited: Tombstones Mark the Demise of Primary Clustering</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The linear-probing hash table is one of the oldest and most widely used data structures in computer science. However, linear probing also famously comes with a major drawback: as soon as the hash table reaches a high memory utilization, elements within the hash table begin to cluster together, causing insertions to become slow. This phenomenon, now known as “primary clustering”, was first captured by Donald Knuth in 1963; at a load factor of <img src="https://s0.wp.com/latex.php?latex=1+-+1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1 - 1/x" class="latex" />, the expected time per insertion becomes <img src="https://s0.wp.com/latex.php?latex=%5CTheta%28x%5E2%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\Theta(x^2)" class="latex" />, rather than the more desirable <img src="https://s0.wp.com/latex.php?latex=%5CTheta%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\Theta(x)" class="latex" />.</p>
<p>We show that there is more to the story than the classic analysis would seem to suggest. It turns out that small design decisions in how deletions are implemented have dramatic effects on the asymptotic performance of insertions. If these design decisions are made correctly, then even a hash table that is continuously at a load factor <img src="https://s0.wp.com/latex.php?latex=1+-+%5CTheta%281%2Fx%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1 - \Theta(1/x)" class="latex" /> can achieve average insertion time <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\tilde{O}(x)" class="latex" />. A key insight is that the tombstones left behind by deletions cause a surprisingly strong “anti-clustering” effect, and that when insertions and deletions are one-for-one, the anti-clustering effects of deletions actually overpower the clustering effects of insertions.</p>
<p>We also present a new variant of linear probing, which we call “graveyard hashing”, that completely eliminates primary clustering on any sequence of operations. If, when an operation is performed, the current load factor is <img src="https://s0.wp.com/latex.php?latex=1+-+1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1 - 1/x" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="x" class="latex" />, then the expected cost of the operation is <img src="https://s0.wp.com/latex.php?latex=O%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="O(x)" class="latex" />. One corollary is that, in the external-memory model with a data block size of <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="B" class="latex" />, graveyard hashing offers the following remarkable guarantee: at any load factor <img src="https://s0.wp.com/latex.php?latex=1-1%2Fx&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1-1/x" class="latex" /> satisfying <img src="https://s0.wp.com/latex.php?latex=x+%3D+o%28B%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="x = o(B)" class="latex" />, graveyard hashing achieves <img src="https://s0.wp.com/latex.php?latex=1+%2B+o%281%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1 + o(1)" class="latex" /> expected block transfers per operation. Past external-memory hash tables have only been able to offer a <img src="https://s0.wp.com/latex.php?latex=1%2Bo%281%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="1+o(1)" class="latex" /> guarantee when the block size <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="B" class="latex" /> is at least <img src="https://s0.wp.com/latex.php?latex=%5COmega%28x%5E2%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\Omega(x^2)" class="latex" />.</p>
<p>Based on joint work with Michael A. Bender and Bradley C. Kuszmaul (<a href="https://tcsplus.wordpress.com/feed/_wp_link_placeholder">arXiv:2107.01250</a>). To appear in FOCS 2021.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/11/25/tcs-talk-wednesday-december-1-william-kuszmaul-mit/"><span class="datestr">at November 25, 2021 10:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19356">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/">Best To Dean Mynatt</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Plus updates on gender disparity, equity, and POPL 2022</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/bm/" rel="attachment wp-att-19358"><img width="150" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/bm.png?resize=150%2C150&amp;ssl=1" class="alignright wp-image-19358" height="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">IPaT <a href="https://news.gatech.edu/expert/elizabeth-mynatt">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Beth Mynatt is heading north to become the new Dean of Computer Science at Northeastern University. Georgia Tech will miss her; she has been a key part of Tech for over twenty years. Northeastern is getting a great leader, a valued colleague, and an excellent PhD graduate of Georgia Tech.</p>
<p>
Today we hail her work on solving problems of <em>aging</em> and compare to what we do in theory. These musings wend toward solving problems of the kind raised in our previous <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">post</a>, on which we have an update from POPL General Chair Rajeev Alur that comes full circle to a Dean at Northeastern.</p>
<p>
One aspect of aging is having a long memory of a field. When Ken and I were young, it was all about what humans can do with computers. Now it is much more about what computers can do with humans. <em>With</em> humans, not just <em>for</em> humans—the aspect of collaboration is key. </p>
<p>
</p><p></p><h2> Aging… </h2><p></p>
<p></p><p>
Beth worked on various projects over her time at Tech. In 1999–2000, she was part of a team that launched the Aware Home Research Initiative (<a href="https://www.cc.gatech.edu/fce/house/house.html">AHRI</a>), “whose goal is to develop the requisite technologies to create a home environment that can both perceive and assist its occupants.” She has led the Institute of People and Technology (IPaT) since 2011, its inaugural year, where her team has helped to support numerous, impactful research programs for faculty across Georgia Tech. </p>
<p>
To show how leadership and persistent work pay dividends, she is a co-PI on a new multi-institution grant led by Tech’s Sonia Chernova to build intelligent systems that support aging. The five-year, $20 million grant from the National Science Foundation, sponsored also by Amazon and Google, will go to create the NSF AI Institute for Collaborative Assistance and Responsive Interaction for Networked Groups (<a href="http://ai-caring.org/">AI-CARING</a>). The <a href="https://www.cc.gatech.edu/news/649114/new-ai-institute-builds-tech-support-aging">announcement</a> says:</p>
<blockquote><p><b> </b> <em> The institute aims to develop new longitudinal, collaborative AI systems that work with aging adults including those diagnosed with mild cognitive impairment, and their caregivers. </em>
</p></blockquote>
<p></p><p>
The AI-CARING front page says that they </p>
<blockquote><p><b> </b> <em> “will develop a discipline focused on personalized, longitudinal, collaborative AI, enabling the development of AI systems that learn personalized models of user behavior, understand how people’s behavior changes over time, and integrate that knowledge to support people and AIs working together. These networked Human-AI teams will work with elderly adults and their caregivers in order to provide sustainable long-term care solutions.” </em>
</p></blockquote>
<p>
</p><p>
As someone who is not young, I can definitely see why this is important. </p>
<p>
</p><p></p><h2> Missed the … </h2><p></p>
<p></p><p>
I sometimes feel that we in theory have missed the boat. The issue is that our problems are weighty—P=NP anyone?—but they do not directly impact practical computing. The areas that Beth is interested in, such as <a href="https://en.wikipedia.org/wiki/Ubiquitous_computing">ubiquitous computing</a>, have by definition had a large impact on real computing. The snippet from MIT’s <a href="http://oxygen.csail.mit.edu/Overview.html">Project Oxygen</a> quoted by Wikipedia expresses the direction of impact:</p>
<blockquote><p><b> </b> <em> In the future, computation will be human centered. It will be freely available everywhere, like batteries and power sockets, or oxygen in the air we breathe. … [C]onfigurable generic devices, either handheld or embedded in the environment, will bring computation to us, whenever we need it and wherever we might be. … We won’t have to type, click, or learn new computer jargon. Instead, we’ll communicate naturally, using speech and gestures that describe our intent … and leave it to the computer to carry out our will. </em>
</p></blockquote>
<p></p><p>
This may conjure a “Star Trek” vision, but some people already live with substantial parts of this reality. Is there a market for P=NP? On the “equals” side, we can think of a <a href="https://en.wikipedia.org/wiki/Sneakers_(1992_film)">couple</a> of <a href="https://en.wikipedia.org/wiki/Travelling_Salesman_(2012_film)">movies</a> featuring interested parties. For the “not equal” side, not so much?</p>
<p>
If pressed to think of a theory result that “launched a thousand ships” of practical effort, Peter Shor’s theorem about factoring belonging to quantum polynomial time springs to mind. But the ship of universal quantum computing needed to get it under steam won’t come in for decades.  It is <em>quantum devices with rudimentary computational features</em> that we see ruling in the meantime, while quantum communication protocols backed by quantum information theory are <a href="https://thequantuminsider.com/2021/06/23/11-global-banks-probing-the-wonderful-world-of-quantum-technologies/">banked on</a> now. </p>
<p>
</p><p></p><h2> The Solutions Business </h2><p></p>
<p></p><p>
I, Ken writing from here, have often winced at the way the word “solutions” is used in business advertising. To a theorist, <em>solutions</em> are what make a conjecture become a theorem, what we grade in theory courses, what enjoyers of puzzles pursue for recreation. </p>
<p>
I described a <em>solution</em> of the more practical kind in my “Pandemic Lag” <a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/">post</a> last July. It solves the problem of estimating the true current strength of rapidly developing young players (such as I was once) whose official ratings have been largely frozen for over a year and a half by the lack of in-person chess during the pandemic. Online play is not officially rated. </p>
<p>
For example, if a preteen’s frozen rating is <b>1575</b>, my formula will currently add 25 Elo points times 19 months of the pandemic to make <b>2050</b>. Players with higher ratings and longer track records are adjusted less. In some recent instances, when such kids have defeated players with more-established ratings near 2200, people seeing the 1500s rating in the official tournament table have raised questions. My answer is based on the average performance of <em>many</em> children keen enough to compete in similar-level championship events. Thus my computer-intensive studies are safeguarding the welfare of minors.</p>
<p>
</p><p></p><h2> Gender Gap and Pipeline in Chess </h2><p></p>
<p></p><p>
My rating solution raises another problem of the kind addressed in our last <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">post</a>. For a fresh instance, on Monday I submitted my final report on the European Team Championship. This ten-day tournament finished Sunday with the <a href="https://www.chess.com/news/view/alireza-firouzja-youngest-chess-player-ever-to-break-2800">sensation</a> of Alireza Firouzja becoming the youngest player ever rated over 2800, six months younger than world champion Magnus Carlsen was in 2009. (Carlsen will <a href="https://www.fide.com/news/1445">defend</a> his title starting Friday in Dubai against the Russian Ian Nepomniachtchi.)</p>
<p>
My intrinsic rating performance projections for the men’s/open section were all accurate to within 10 Elo points, within two-sigma error bars about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pm 25}" class="latex" />. When restricted to the 32 juniors (out of 191 total players) whose ratings I adjusted, all projections were within 23 Elo points of the results and their average was within <b>2</b> Elo points—on adjustments averaging 80 Elo points per the 32 players. </p>
<p>
In the women’s section, my gender-neutral formula adjusted up 39 of the 153 female players by an average of 169 Elo. The prescribed amount was larger because their ratings (average 2168) were lower than the 32 junior males’ ratings (average 2419) to begin with. The formula overshot their performance by 113 Elo, and accounted for the bulk of a 36 Elo average shortfall of my projections for the women’s section overall. This continues a pattern of observing not only a lower starting point, but also a lower first derivative when compared to males of the <em>same</em> rating, in tournaments with high junior participation where my adjustments are involved.</p>
<p>
The <em>Queen’s Gambit</em> miniseries magnified awareness of the disparity—witness the <a href="https://womensagenda.com.au/latest/whats-behind-the-gender-imbalance-in-top-level-chess/">article</a> a year ago by the Australian economist and grandmaster David Smerdon, “What’s behind the gender imbalance in top-level chess?” My work offers a new way to pinpoint when and where the male and female pipelines diverge. I do not see how it could <em>solve</em> the disparity, however. Even converting my big spreadsheets of test results into publications is a tall ask—for one, they are considered sensitive data. </p>
<p>
I have discoursed about the theory of my predictive model on this blog, and could say more about the thrill of empirical success in ways much of theory doesn’t reach. But this is still short of solving human problems in the manner of Dick’s intro, let alone solving the gender gap. On that, we’re grateful to have a communication from Rajeev Alur, General Chair of POPL, on how the statistics we noted came about and what the conference is doing.</p>
<p>
</p><p></p><h2> More About POPL 2022 </h2><p></p>
<p></p><p>
Rajeev began by observing that POPL PCs in previous years have been closer to the percentages we quoted for STOC and FOCS: </p>
<ul>
<li>
2021: 15.4% (8/52) <p></p>
</li><li>
2020: 14.8% (8/54) <p></p>
</li><li>
2019: 11.5% (6/52) <p></p>
</li><li>
2018: 23.1% (12/52) <p></p>
</li><li>
2017: 17.2% (5/29) <p></p>
</li><li>
2016: 21.4% (6/28) <p></p>
</li><li>
2015: 20.0% (6/30)
</li></ul>
<p>
A more private figure that he gave us permission to divulge is that the proportion of PC <em>invitations</em> this year was just under 20% for women. Had they accepted at the same rate as the men, there would have been 12 women on the committee this year, as there was in the first year the PC size was expanded in 2018. </p>
<p>
He went on to note an issue also raised in comments to our last post, namely that among the smaller population of women the same people are asked multiple times, leading to more declines. We can put the scaling problem another way: nearly doubling the committee size in 2018 also doubled the ratio of women being invited to their total number. Not all effects of scaling-up keep equal proportion. The root cause of course is not only the smaller population but its smaller first derivative: as Rajeev noted, the latest figures from the Computing Research Association in 2019 show that out of 417 PhDs in POPL-core areas, only 42 (10%) were female.</p>
<p>
The main thing that they are doing is to take the kind of steps pointed up in the quotation from Valerie King in our post. As shown in the POPL 2022 Overview schedule on their <a href="https://popl22.sigplan.org/">front page</a>, they have special lunch and breakfast events for women and LGTBQ attendees (the latter held since <a href="https://popl20.sigplan.org/track/POPL-2020-lgbtq-lunch">2020</a>) and mentoring workshops for graduate and undergraduate level students. POPL 2022 is the first to appoint a special Chair for organizing these events, Jennifer Paykin, who we note gave a special quantum-for-POPL <a href="https://www.youtube.com/watch?v=nVMm0PrF-j8">presentation</a> in 2020. </p>
<p>
Next year’s POPL PC will be chaired by <a href="https://www.khoury.northeastern.edu/people/amal-ahmed/">Amal Ahmed</a>. Coincidentally, she is Associate Dean for Graduate Programs at Northeastern’s Khoury College where Beth Mynatt will be Dean. <a href="https://alexandrasilva.org/#/main.html">Alexandra Silva</a>, who was on the POPL 2020 PC and on the POPL 2021 Organizing Committee as Accessibility Chair, is one of three Keynote Speakers this year. Hope for the pipeline picking up was raised by the 2021 SIGPLAN Robin Milner Award for Junior Researchers going to <a href="https://homes.cs.washington.edu/~emina/">Emina Torlak</a> of UW. </p>
<p>
Of course, it will take a long time to translate the promotion of opportunity into closer parity. The low dip this year is what we noticed, but the long time effect is what our post highlighted at the end. Again we thank Rajeev for bringing both the time range and effort by SIGPLAN and POPL into greater context.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We’ve tried to “zoom out” where theory people are used to zooming in. How can that change perspectives on our field?</p>
<p>
We are also thankful for how our field has branched out and wish everyone a happy Thanksgiving.</p>
<p></p><p><br />
[fixed name in POPL section]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/"><span class="datestr">at November 25, 2021 04:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6494812234459505156">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/11/inria-innovation-prize-2021-to-mateescu.html">Inria Innovation Prize 2021 to Mateescu, Garavel, Lang and Serwe</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I just learnt that <a href="https://convecs.inria.fr/people/Radu.Mateescu/">Radu Mateescu</a>, <a href="http://convecs.inria.fr/people/Hubert.Garavel/">Hubert Garavel</a>, <a href="http://convecs.inria.fr/people/Frederic.Lang/">Frédéric Lang</a> and <a href="http://convecs.inria.fr/people/Wendelin.Serwe/">Wendelin Serwe</a> from the <a href="http://convecs.inria.fr/">Construction of Verified Concurrent Systems</a> (Convecs) project team at INRIA Grenoble – Rhône-Alpes Centre have been recently awarded the <a href="https://www.inria.fr/en/convecs-team-safety-modeling-distributed-parallel-systems" target="_blank">Inria Innovation Prize – Académie des sciences – Dassault Systèmes</a>. Their work contributes to the development of the <a href="https://cadp.inria.fr/" target="_blank">CADP toolbox</a> for modelling and verifying parallel and distributed systems. The aim of that project is to automatically detect design flaws in highly complex systems. </p><p>Readers of this blog will be as delighted as I am by this news. <a href="https://en.wikipedia.org/wiki/Construction_and_Analysis_of_Distributed_Processes" target="_blank">CADP</a> is one of the tools from the concurrency community that has the longest history, dating back to its early releases in 1989. It has been used to good effect in a variety of applications, is still under continuous development and makes excellent use  in practice of classic tools from concurrency theory. By way of example, let me mention the recent successes by the Convecs team in dealing with difficult challenges posed by <a href="http://ls5-www.cs.tu-dortmund.de/cms/de/mitarbeiter/prof/Bernhard_Steffen.html" target="_blank">Bernhard Steffen</a> and his team on the evaluation of CTL and LTL formulae on large products of automata. (See, for instance, the news items <a href="http://cadp.inria.fr/news12.html" target="_blank">here</a> and <a href="http://cadp.inria.fr/news13.html#section-3" target="_blank">here</a>.) Traditional model checkers fail on those challenges because the state space of the product automata is too large for them. However, a wise use of bisimulations and congruence results allows CADP to solve many of those challenges. Interested readers might also wish to peruse the slides at </p><ul style="text-align: left;"><li><a href="http://cadp.inria.fr/ftp/presentations/Mazzanti-RERS-18.pdf">http://cadp.inria.fr/ftp/presentations/Mazzanti-RERS-18.pdf</a> </li><li><a href="http://cadp.inria.fr/ftp/presentations/Lang-RERS-19.pdf">http://cadp.inria.fr/ftp/presentations/Lang-RERS-19.pdf</a> and</li><li><a href="http://cadp.inria.fr/ftp/presentations/Lang-RERS-20.pdf">http://cadp.inria.fr/ftp/presentations/Lang-RERS-20.pdf</a>. </li></ul>Congratulations to the whole CADP team and to the concurrency community for this award!</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/11/inria-innovation-prize-2021-to-mateescu.html"><span class="datestr">at November 24, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/169">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/169">TR21-169 |  Hypercontractivity on High Dimensional Expanders: a Local-to-Global Approach for Higher Moments | 

	Max Hopkins, 

	Mitali Bafna, 

	Tali Kaufman, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Hypercontractivity is one of the most powerful tools in Boolean function analysis. Originally studied over the discrete hypercube, recent years have seen increasing interest in extensions to settings like the $p$-biased cube, slice, or Grassmannian, where variants of hypercontractivity have found a number of breakthrough applications including the resolution of Khot’s 2-2 Games Conjecture (Khot, Minzer, Safra FOCS 2018). In this work, we develop a new theory of hypercontractivity on high dimensional expanders (HDX), an important class of expanding complexes that has recently seen similarly impressive applications in both coding theory and approximate sampling. Our results lead to a new understanding of the structure of Boolean functions on HDX, including a tight analog of the KKL Theorem and a new characterization of non-expanding sets. 

Unlike previous settings satisfying hypercontractivity, HDX can be asymmetric, sparse, and very far from products, which makes the application of traditional proof techniques challenging. We handle these barriers with the introduction of two new tools of independent interest: a new explicit combinatorial Fourier basis for HDX that behaves well under restriction, and a new local-to-global method for analyzing higher moments. Interestingly, unlike analogous second moment methods that apply equally across all types of expanding complexes, our tools rely inherently on simplicial structure. This suggests a new distinction among high dimensional expanders based upon their behavior beyond the second moment.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/169"><span class="datestr">at November 24, 2021 08:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/168">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/168">TR21-168 |  Hypercontractivity on High Dimensional Expanders: Approximate Efron-Stein Decompositions for $\epsilon$-Product Spaces | 

	Tom Gur, 

	Noam Lifshitz, 

	Siqi Liu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove hypercontractive inequalities on high dimensional expanders. As in the settings of the p-biased hypercube, the symmetric group, and the Grassmann scheme, our inequalities are effective for global functions, which are functions that are not significantly affected by a restriction of a small set of coordinates. As applications, we obtain Fourier concentration, small-set expansion, and Kruskal-Katona theorems for high dimensional expanders. Our techniques rely on a new approximate Efron-Stein decomposition for high dimensional link expanders.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/168"><span class="datestr">at November 24, 2021 08:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/167">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/167">TR21-167 |  Post-Quantum Zero Knowledge, Revisited (or: How to Do Quantum Rewinding Undetectably) | 

	Alex Lombardi, 

	Fermi Ma, 

	Nicholas Spooner</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A major difficulty in quantum rewinding is the fact that measurement is destructive: extracting information from a quantum state irreversibly changes it. This is especially problematic in the context of zero-knowledge simulation, where preserving the adversary's state is essential.
    
    In this work, we develop new techniques for quantum rewinding in the context of extraction and zero-knowledge simulation:
    
1. We show how to extract information from a quantum adversary by rewinding it without disturbing its internal state. We use this technique to prove that important interactive protocols, such as the Goldreich-Micali-Wigderson protocol for graph non-isomorphism and the Feige-Shamir protocol for NP, are zero-knowledge against quantum adversaries. 

2. We prove that the Goldreich-Kahan protocol for NP is post-quantum zero knowledge using a simulator that can be seen as a natural quantum extension of the classical simulator. 

Our results achieve (constant-round) black-box zero-knowledge with negligible simulation error, appearing to contradict a recent impossibility result due to Chia-Chung-Liu-Yamakawa (FOCS 2021). This brings us to our final contribution:

3. We introduce coherent-runtime expected quantum polynomial time, a computational model that (a) captures all of our zero-knowledge simulators, (b) cannot break any polynomial hardness assumptions, and (c) is not subject to the CCLY impossibility. In light of our positive results and the CCLY negative results, we propose coherent-runtime simulation to be the right quantum analogue of classical expected polynomial-time simulation.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/167"><span class="datestr">at November 23, 2021 11:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/11/23/mask-rct-revisited/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/11/23/mask-rct-revisited/">Revisiting the Bangladesh Mask RCT.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In an earlier post, <a href="https://www.argmin.net/2021/09/13/effect-size/">I raised a few issues</a> with a <a href="https://www.poverty-action.org/sites/default/files/publications/Mask_Second_Stage_Paper_20211108.pdf.pdf">large-scale RCT run in Bangladesh aimed at estimating the effectiveness of masks on reducing the spread of the coronavirus</a>. In particular, I was a bit dismayed that the authors did not post the raw number of seropositive cases in their study, preventing me from computing standard statistical analyses of their results. I also objected to the number of statistical regressions run to pull signals out of a very complex intervention.</p>

<p>Recently, the authors were kind enough to release their <a href="https://gitlab.com/emily-crawford/bd-mask-rct">code and data</a>. I send nothing but kudos to them in this regard. Releasing code and data can help disambiguate questions that are not always answerable from papers alone. In fact, I was immediately able to answer my question by querying their data. In this post, I will walk through a simple analysis to estimate the efficacy of their proposed intervention.</p>

<p>In the Bangladesh Mask RCT, there were $n_C=$163,861 individuals from 300 villages in the control group. There were $n_T=$178,322 individuals from 300 villages in the intervention group. The main end point of the study was whether their intervention reduced the number of individuals who both reported covid-like symptoms and tested seropositive at some point during the trial. The number of such individuals appears nowhere in their paper, and one has to compute this from the data they kindly provided: There were $i_C=$1,106 symptomatic individuals confirmed seropositive in the control group and $i_T=$1,086 such individuals in the treatment group. The difference between the two groups was small: only <em>20 cases</em> out of over 340,000 individuals over a span of 8 weeks.</p>

<p>I have a hard time going from these numbers to the assured conclusions that “masks work” that was <a href="https://www.theatlantic.com/ideas/archive/2021/09/masks-were-working-all-along/619989/">promulgated</a> <a href="https://www.nature.com/articles/d41586-021-02457-y">by</a> <a href="https://www.nbcnews.com/science/science-news/largest-study-masks-yet-details-importance-fighting-covid-19-rcna1858">the</a> <a href="https://www.washingtonpost.com/world/2021/09/01/masks-study-covid-bangladesh/">media</a> or <a href="https://www.nytimes.com/2021/09/26/opinion/do-masks-work-for-covid-prevention.html">the authors</a> after this preprint appeared. This study was not blinded, as it’s impossible to blind a study on masks. The intervention was highly complex and included a mask promotion campaign and education about other mitigation measures including social distancing. Moreover, individuals were only added to the study if they consented to allow the researchers to visit and survey their household. There was a large differential between the control and treatment groups here, with 95% consenting in the treatment group but only 92% consenting in control. <em>This differential alone could wash away the difference in observed cases.</em> Finally, symptomatic seropositivity is a crude measure of covid as the individuals could have been infected before the trial began.</p>

<p>Given the numerous caveats and confounders, the study still only found a tiny effect size. My takeaway is that a complex intervention including an educational program, free masks, encouraged mask wearing, and surveillance in a poor country with low population immunity and no vaccination showed at best modest reduction in infection. I think this summary is fair to the study authors. And this is valuable information to have! It reaffirms my priors that non-pharmaceutical interventions are challenging to implement and have only modest benefits in the presence of a highly contagious respiratory infection. But your mileage may vary.</p>

<p>As I mentioned, of course, this was not the message that the majority of the media took away from this study. Instead we were told that this trial finally confirmed that masks worked. I think one of the key confusing points was <a href="http://www.argmin.net/2021/08/13/relative-risk/">using “efficacy” instead of relative risk</a> as a measure of intervention power.</p>

<p>One of the dark tricks of biostatistics is moving away from absolute case counts to  measures of risk such as relative risk reduction, efficacy, or the odds ratio. All of these measures are relative, and they tend to exaggerate effects. The relative risk reduction is the ratio of the rate of infection in the treatment group to the rate of infection in the control group</p>

\[{\small
    RR = \frac{i_T/n_T}{i_C/n_C}\,.
}\]

<p>A small $RR$ corresponds to a large reduction in risk. For the mask study, $RR=$0.9. That’s not a lot of risk reduction: in this study, community masking improved an individual’s risk of infection by a factor of only 1.1x. As a convenient comparator, the $RR$ in the MRNA vaccine trials was 0.05. In this case, vaccines reduce the risk of symptomatic infection by a factor of 20x.</p>

<p>The academic vaccine community unfortunately uses “efficacy” or “effectiveness” to describe relative risk reduction. <a href="http://www.argmin.net/xxx">Efficacy is a confusing, commonly misinterpreted metric</a>. Efficacy in a trial is one minus the relative risk reduction:</p>

\[{\small
EFF = 1-RR\,,
}\]

<p>reported as a percentage. So if the $RR=$0.9, then $EFF=$10%.</p>

<p>The important thing to realize about efficacy is that the range from 0% to 20% is barely better than nothing. Here, even a 20% efficacy corresponds to a reduction of risk by a factor of 1.25x. 1.25x is not literally nothing, but it’s also not enough to halt a highly contagious respiratory infection. For what it’s worth, a vaccine with 20% efficacy would not be approved. Another major flaw of using efficacy as a metric is that it is highly nonlinear. The difference between 10% and 20% efficacy is very small whereas the difference between 85% and 95% is huge, corresponding to a 7-fold and 20-fold risk reduction respectively. Efficacy is a nonlinear metric, but these percentages are bandied around as if they are linear effects, and this adds confusion to the public dialogue.</p>

<p class="center"><img width="65%" alt="The relationship between effectiveness and risk reduction is highly nonlinear" src="http://www.argmin.net/assets/eff_v_rr.png" /></p>

<p>To further dive into the absurdity of efficacy, let’s examine the claim that “cloth masks” worked less well than “surgical masks.” This is too strong an observation to be gleaned from the data. The preprint provides two stratified calculations to estimate the efficacy of types of masks. In the first case, the authors analyzed villages randomized to only be given surgical masks and their matched control villages. In this case there were 190 pairs of villages consisting of $n_C=$103,247 individuals in the control group and $n_T=$113,082 individuals in the treatment group. They observed $i_C=$774 symptomatic and seropositive individuals in the control group and $i_T=$756 symptomatic and seropositive individuals in the treatment group. <em>This is a difference of 18 individuals.</em> The corresponding efficacy is 11%, still woefully low.</p>

<p>We can do a similar analysis for the villages only given cloth masks. There were 96 pairs of villages consisting of $n_C=$53,691 individuals in the control group and $n_T=$57,415 individuals in the treatment group. They observed $i_C=$332 symptomatic and seropositive individuals in the control group and $i_T=$330 symptomatic and seropositive individuals in the treatment group. <em>This is a difference of only 2 individuals.</em> Certainly, no one would put much faith in an intervention where we see a difference of 2 cases in a study with over one hundred thousand people. However, to further demonstrate the absurdity of the notion of efficacy, the observed efficacy for cloth masks in this study is 7%. I think in many people’s minds, the difference between 7% and 11% is small. And 7% should be considered “no effect” as should 11%. <del>As a final absurd comparison, the study data shows cloth masks are more efficacious than purple surgical masks where the estimated efficacy is 0% ($n_C=$27,918, $n_T=$29,541, $i_C=$177, $i_T=$187)!</del> (<em>Ed note: turns out the purple masks were cloth. So the cloth purple masks did nothing, but the red masks “work.” Indeed, red masks were more effective than surgical masks!)</em> Certainly, comparing a bunch of such small effects is not telling us much.</p>

<p>Anyone who spends too much time around statisticians will note that I never once tried to compute a p-value for any of these results. As I’ve belabored, obsession with statistical significance distracts us from discussing effect sizes. We should be able to just look at the effect size and conclude the study did not find a significant impact of masks on coronavirus spread. We don’t need a p-value to tell us 10% efficacy is not helpful in this context. But it’s also important to note that you can’t just run a standard binomial test on this data because it is cluster-randomized and the subjects are anything but independent. In the next blog, just for the sake of academic navel gazing, I’ll discuss the lack of statistical significance of this study and show why cluster randomized trials are inherently more challenging to interpret than standard RCTs.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/11/23/mask-rct-revisited/"><span class="datestr">at November 23, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4583902398250806221">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/finding-element-with-nonadaptive.html">Finding an element with nonadaptive questions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Suppose you have a non-empty subset S of {1,...N} and want to find an element of S. You can ask arbitrary questions of the form "Does S contain an element in A?" for some A a subset of {1,...N}. How many questions do you need?</p><p>Of course you can use binary search, using questions of the form "is there number greater than <i>m</i> in S?". This takes log N questions and it's easy to show that's tight.</p><p>What if you have to ask all the questions ahead of time before you get any of the answers? Now binary search won't work. If |S|=1 you can ask "is there a number in S whose <i>i</i>th bit is one?" That also takes log N questions.</p><p>For arbitrary S the situation is trickier. With randomness you still don't need too many questions. <a href="https://doi.org/10.1007/BF02579206">Mulmuley, Vazirani and Vazirani</a>'s isolating lemma works as follows: For each i &lt;= log N, pick a random weight w<sub>i</sub> between 1 and 2 log N. For each element m in S, let the weight of m be the sum of the weights of the bits of m that are 1. With probability at least 1/2 there will be an m with an unique minimum weight. There's a <a href="https://blog.computationalcomplexity.org/2015/07/new-proof-of-isolation-lemma.html">cool proof</a> of an isolating lemma by Noam Ta-Shma.</p><p>Once you have this lemma, you can ask questions of the form "Given a list of w<sub>i</sub>'s and a value v, is there an m in S of weight v whose jth bit is 1?" Choosing w<sub>i</sub> and v at random you have a 1/O(log N) chance of a single m whose weight is v, and trying all j will give you a witness. </p><p>Randomness is required. The X-search problem described by <a href="https://doi.org/10.1016/0022-0000(88)90027-X">Karp, Upfal and Wigderson</a> shows that any deterministic procedure requires essentially N queries. </p><p>This all came up because Bill had some colleagues looking a similar problems testing machines for errors. </p><p>I've been interested in the related question of finding satisfying assignments using non-adaptive NP queries. The results are similar to the above. In particular, you can randomly find a satisfying assignment with high probability using a polynomial number of non-adaptive NP queries. It follows from the techniques above, and even earlier papers, but I haven't been able to track down a reference for the first paper to do so.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/finding-element-with-nonadaptive.html"><span class="datestr">at November 22, 2021 08:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=964">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/11/22/phd-in-complexity-theory-with-me/">PhD in complexity theory with me</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This is around the time when people start applying for PhD programs, at least judging from my inbox.  If you are applying, consider that we have an <a href="https://www2.ccs.neu.edu/theory/">amazing theory group </a>, are <a href="http://csrankings.org/#/index?all&amp;us">highly ranked</a>, have <a href="https://philanthropynewsdigest.org/news/northeastern-receives-50-million-for-college-of-computer-sciences">tons </a>of <a href="https://mainestartupsinsider.com/roux-institute-receives-another-100m-gift-to-support-its-high-tech-education-initiative-in-portland/">resources</a>, and I am looking for students.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/11/22/phd-in-complexity-theory-with-me/"><span class="datestr">at November 22, 2021 07:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/22/assistant-professors-theory-positions-at-uestc-chengdu-china-apply-by-february-28-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/22/assistant-professors-theory-positions-at-uestc-chengdu-china-apply-by-february-28-2022/">Assistant Professors, theory positions at UESTC, Chengdu, China (apply by February 28, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The theory group at the cs school invites applications for Assistant Professor positions. The school is a top school in China and competitive at the world stage. We thrive to become among the best theoretical computer science groups in China. By joining us, you work with experienced, young, exciting, curiosity driven researchers and talented students. Life is exciting &amp; remuneration is generous.</p>
<p>Website: <a href="https://tcs.uestc.edu.cn/">https://tcs.uestc.edu.cn/</a><br />
Email: bmk@uestc.edu.cn</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/22/assistant-professors-theory-positions-at-uestc-chengdu-china-apply-by-february-28-2022/"><span class="datestr">at November 22, 2021 12:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/21/research-fellow-at-university-of-oxford-apply-by-november-29-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/21/research-fellow-at-university-of-oxford-apply-by-november-29-2021/">Research Fellow at University of Oxford (apply by November 29, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science is pleased to invite applications for the Glasstone Fellowship in Computer Science—a three-year<br />
postdoctoral fellowship supported by the Glasstone Bequest. Candidates should be completing or have recently (i.e. normally within the past 3 years) completed a doctorate in<br />
Computer Science or a closely related discipline.</p>
<p>Website: <a href="https://www.cs.ox.ac.uk/news/1984-full.html">https://www.cs.ox.ac.uk/news/1984-full.html</a><br />
Email: james.worrell@cs.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/21/research-fellow-at-university-of-oxford-apply-by-november-29-2021/"><span class="datestr">at November 21, 2021 06:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/166">TR21-166 |  Average-case Hardness of NP and PH from Worst-case Fine-grained Assumptions | 

	Lijie Chen, 

	Shuichi Hirahara, 

	Neekon Vafa</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
What is a minimal worst-case complexity assumption that implies non-trivial average-case hardness of NP or PH? This question is well motivated by the theory of fine-grained average-case complexity and fine-grained cryptography. In this paper, we show that several standard worst-case complexity assumptions are sufficient to imply non-trivial average-case hardness of NP or PH:
    
    1. NTIME[$n$] cannot be solved in quasi-linear time on average if UP is not in DTIME[$2^{\widetilde{O}\left(\sqrt{n}\right)}$].
    
    2. $\Sigma_2$TIME[$n$] cannot be solved in quasi-linear time on average if $\Sigma_k$SAT cannot be solved in time $2^{\widetilde{O}\left(\sqrt{n}\right)}$ for some constant $k$. Previously, it was not known if even average-case hardness of $\Sigma_3$SAT implies the average-case hardness of $\Sigma_2$TIME[$n$].
    
    3. Under the Exponential-Time Hypothesis (ETH), there is no average-case $n^{1+\varepsilon}$-time algorithm for NTIME[$n$] whose running time can be estimated in time $n^{1+\varepsilon}$ for some constant $\varepsilon &gt; 0$.
    
    Our results are given by generalizing the non-black-box worst-case-to-average-case connections presented by Hirahara (STOC 2021) to the settings of fine-grained complexity. To do so, we construct quite efficient complexity-theoretic pseudorandom generators under the assumption that the nondeterministic linear time is easy on average, which may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/166"><span class="datestr">at November 21, 2021 01:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/165">TR21-165 |  Improved Merlin-Arthur Protocols for Central Problems in Fine-Grained Complexity | 

	Shyan Akmal, 

	Lijie Chen, 

	Ce Jin, 

	Malvika Raj, 

	Ryan Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In a Merlin-Arthur proof system, the proof verifier (Arthur) accepts valid proofs (from Merlin) with probability $1$, and rejects invalid proofs with probability arbitrarily close to $1$. The running time of such a system is defined to be the length of Merlin's proof plus the running time of Arthur.  We provide new Merlin-Arthur proof systems for some key problems in fine-grained complexity. In several cases our proof systems have optimal running time. Our main results include:

$\bullet$ Certifying that a list of $n$ integers has no 3-SUM solution can be done in Merlin-Arthur time $\tilde{O}(n)$. Previously, Carmosino et al. [ITCS 2016] showed that the problem has a nondeterministic algorithm running in $\tilde{O}(n^{1.5})$  time (that is, there is a proof system with proofs of length $\tilde{O}(n^{1.5})$ and a deterministic verifier running in $\tilde{O}(n^{1.5})$ time).

$\bullet$ Counting the number of $k$-cliques with total edge weight equal to zero in an $n$-node graph can be done in Merlin-Arthur time $\tilde O(n^{\lceil k/2\rceil })$ (where $k\ge 3$). For odd $k$, this bound can be further improved for sparse graphs: for example, counting the number of zero-weight triangles in an $m$-edge graph can be done in Merlin-Arthur time $\tilde O(m)$. Previous Merlin-Arthur protocols by Williams [CCC'16] and Bj\"orklund and Kaski [PODC'16] could only count $k$-cliques in unweighted graphs, and had worse running times for small $k$.

$\bullet$ Computing the All-Pairs Shortest Distances matrix for an $n$-node graph can be done in Merlin-Arthur time $\tilde{O}(n^2)$. Note this is optimal, as the matrix can have $\Omega(n^2)$ nonzero entries in general. Previously, Carmosino et al. [ITCS 2016] showed that this problem has an $\tilde{O}(n^{2.94})$ nondeterministic time algorithm.

$\bullet$ Certifying that an $n$-variable $k$-CNF is unsatisfiable can be done in Merlin-Arthur time $2^{n/2 - n/O(k)}$. We also observe an algebrization barrier for the previous $2^{n/2}\cdot \mathrm{poly}(n)$-time Merlin-Arthur protocol of R. Williams [CCC'16] for $\#$SAT: in particular, his protocol algebrizes, and we observe there is no algebrizing protocol for $k$-UNSAT running in $2^{n/2}/n^{\omega(1)}$ time. Therefore we have to exploit non-algebrizing properties to obtain our new protocol.


$\bullet$ Certifying a Quantified Boolean Formula is true can be done in Merlin-Arthur time $2^{4n/5}\cdot \mathrm{poly}(n)$. Previously, the only nontrivial result known along these lines was an Arthur-Merlin-Arthur protocol (where Merlin's proof depends on some of Arthur's coins) running in $2^{2n/3}\cdot\mathrm{poly}(n)$ time. 

Due to the centrality of these problems in fine-grained complexity, our results have consequences for many other problems of interest. For example, our work implies that certifying there is no Subset Sum solution to $n$ integers can be done in Merlin-Arthur time $2^{n/3}\cdot\mathrm{poly}(n)$, improving on the previous best protocol by Nederlof [IPL 2017] which took $2^{0.49991n}\cdot\mathrm{poly}(n)$ time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/165"><span class="datestr">at November 21, 2021 12:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/20/assistant-associate-professors-at-aalto-university-apply-by-january-12-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/20/assistant-associate-professors-at-aalto-university-apply-by-january-12-2022/">Assistant &amp; Associate Professors at Aalto University (apply by January 12, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for tenure-track positions at the Assistant Professor level, and tenured positions at the Associate Professor level. We are a diverse community welcoming applications in ALL AREAS of Computer Science. Our CS Theory group (<a href="https://research.cs.aalto.fi/theory/">https://research.cs.aalto.fi/theory/</a>) has e.g. received the best paper awards in FOCS 2019 and ICALP 2017, as well as ERC starting grants in 2014 and 2017.</p>
<p>Website: <a href="https://bit.ly/aalto-csprof">https://bit.ly/aalto-csprof</a><br />
Email: laura.kuusisto-noponen@aalto.fi</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/20/assistant-associate-professors-at-aalto-university-apply-by-january-12-2022/"><span class="datestr">at November 20, 2021 07:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/19/simons-berkeley-fellowships-for-fall-2022-and-spring-2023-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/19/simons-berkeley-fellowships-for-fall-2022-and-spring-2023-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2021/">Simons-Berkeley Fellowships for Fall 2022 and Spring 2023 at Simons Institute for the Theory of Computing (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships for the Fall 2022 and Spring 2023 semesters. The Institute will host programs on “Data-Driven Decision Processes” and “Graph Limits and Processes on Networks: From Epidemics to Misinformation” in Fall 2022 and “Meta-Complexity in Spring 2023.</p>
<p>Website: <a href="https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications">https://simons.berkeley.edu/simons-berkeley-research-fellowship-call-applications</a><br />
Email: simonsvisitorservices@berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/19/simons-berkeley-fellowships-for-fall-2022-and-spring-2023-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2021/"><span class="datestr">at November 19, 2021 10:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6129">The Acrobatics of BQP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Just in case anyone is depressed this afternoon and needs something to cheer them up, students <a href="https://www.cs.utexas.edu/~kretsch/">William Kretschmer</a>, <a href="https://www.quantitativebiology.northwestern.edu/2021/02/23/three-students-awarded-prizes-in-the-great-math-challenge-in-biology-contest/">DeVon Ingram</a>, and I have finally put out a new paper:</p>



<blockquote class="wp-block-quote"><p><strong><a href="https://eccc.weizmann.ac.il/report/2021/164/">The Acrobatics of BQP</a></strong></p><p><strong>Abstract:</strong> We show that, in the black-box setting, the behavior of quantum polynomial-time (BQP) can be remarkably decoupled from that of classical complexity classes like NP.  Specifically:</p><p>– There exists an oracle relative to which NP<sup>BQP</sup>⊄BQP<sup>PH</sup>, resolving a 2005 problem of Fortnow. Interpreted another way, we show that AC<sup>0</sup> circuits cannot perform useful homomorphic encryption on instances of the Forrelation problem. As a corollary, there exists an oracle relative to which P=NP but BQP≠QCMA.</p><p>– Conversely, there exists an oracle relative to which BQP<sup>NP</sup>⊄PH<sup>BQP</sup>.</p><p>– Relative to a random oracle, PP=PostBQP is not contained in the “QMA hierarchy” QMA<sup>QMA^QMA^…</sup>, and more generally PP⊄(MIP*)<sup>(MIP*)^(MIP*)^…</sup> (!), despite the fact that MIP*=RE in the unrelativized world. This result shows that there is no black-box quantum analogue of Stockmeyer’s approximate counting algorithm.</p><p>– Relative to a random oracle, Σ<sub>k+1</sub>⊄BQP<sup>Σ_k</sup> for every k.</p><p>– There exists an oracle relative to which BQP=P<sup>#P</sup> and yet PH is infinite. (By contrast, if NP⊆BPP, then PH collapses relative to all oracles.)</p><p>– There exists an oracle relative to which P=NP≠BQP=P<sup>#P</sup>.</p><p>To achieve these results, we build on the 2018 achievement by Raz and Tal of an oracle relative to which BQP⊄PH, and associated results about the Forrelation problem. We also introduce new tools that might be of independent interest. These include a “quantum-aware” version of the random restriction method, a concentration theorem for the block sensitivity of AC<sup>0</sup> circuits, and a (provable) analogue of the Aaronson-Ambainis Conjecture for sparse oracles.</p></blockquote>



<p>Incidentally, particularly when I’ve worked on a project with students, I’m often tremendously excited and want to shout about it from the rooftops for the students’ sake … but then I also don’t want to use this blog to privilege my own papers “unfairly.”  Can anyone suggest a principle that I should follow going forward?</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6129"><span class="datestr">at November 19, 2021 09:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/19/postdoc-at-georgia-institute-of-technology-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/19/postdoc-at-georgia-institute-of-technology-apply-by-december-15-2021/">PostDoc at Georgia Institute of Technology (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Algorithms and Randomness Center (ARC) at Georgia Tech is seeking postdoctoral fellows starting Fall 2022. ARC has faculty associated with many departments including CS, Math, ISyE. The selected candidate may work on any aspect of algorithms, optimization, broadly interpreted. Qualified applicants must possess a PhD in CS, Math, OR or a related field. Apply by December 15, 2021.</p>
<p>Website: <a href="http://arc.gatech.edu/node/384">http://arc.gatech.edu/node/384</a><br />
Email: ftonge3@cc.gatech.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/19/postdoc-at-georgia-institute-of-technology-apply-by-december-15-2021/"><span class="datestr">at November 19, 2021 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/our-2022-postdoc-program-is-up/">Our 2022 Postdoc Program is up</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We are excited to announce <a href="https://toc4fairness.org/postdoc-opportunities/">our new postdoc program</a>. We are seeking strong candidates from a diverse set of academic backgrounds and personal experiences who want to work with one or more of the PIs on algorithmic fairness and responsible computing more broadly. We expect to be extending multiple offers.  </p></div>







<p class="date">
by Omer Reingold <a href="https://toc4fairness.org/our-2022-postdoc-program-is-up/"><span class="datestr">at November 19, 2021 01:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/164">TR21-164 |  The Acrobatics of BQP | 

	Scott Aaronson, 

	DeVon Ingram, 

	William Kretschmer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that, in the black-box setting, the behavior of quantum polynomial-time (${BQP}$) can be remarkably decoupled from that of classical complexity classes like ${NP}$. Specifically:

-There exists an oracle relative to which ${NP}^{{BQP}}\not \subset {BQP}^{{PH}}$, resolving a 2005 problem of Fortnow. Interpreted another way, we show that ${AC^0}$ circuits cannot perform useful homomorphic encryption on instances of the Forrelation problem. As a corollary, there exists an oracle relative to which ${P} = {NP}$ but ${BQP} \neq {QCMA}$.
-Conversely, there exists an oracle relative to which ${BQP}^{{NP}}\not \subset {PH}^{{BQP}}$.
-Relative to a random oracle, ${PP} = {PostBQP}$ is not contained in the "${QMA}$ hierarchy" ${QMA}^{{QMA}^{{QMA}^{\cdots}}}$, and more generally ${PP} \not\subset ({MIP}^*)^{({MIP}^*)^{({MIP}^*)^{\cdots}}}$ (!), despite the fact that ${MIP}^{\ast}={RE}$ in the unrelativized world. This result shows that there is no black-box quantum analogue of Stockmeyer's approximate counting algorithm.
-Relative to a random oracle, ${\Sigma}_{k+1}^{P} \not\subset {BQP}^{{\Sigma}_{k}^{P}}$ for every $k$.
-There exists an oracle relative to which ${BQP} = {P^{\# P}}$ and yet ${PH}$ is infinite. (By contrast, if ${NP}\subseteq{BPP}$, then ${PH}$ collapses relative to all oracles.)
-There exists an oracle relative to which ${P}={NP} \neq {BQP}={P}^{{\#P}}$.

To achieve these results, we build on the 2018 achievement by Raz and Tal of an oracle relative to which ${BQP}\not \subset {PH}$, and associated results about the Forrelation problem. We also introduce new tools that might be of independent interest. These include a "quantum-aware" version of the random restriction method, a concentration theorem for the block sensitivity of ${AC^0}$ circuits, and a (provable) analogue of the Aaronson-Ambainis Conjecture for sparse oracles.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/164"><span class="datestr">at November 19, 2021 01:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/163">TR21-163 |  Algorithmizing the Multiplicity Schwartz-Zippel Lemma | 

	Siddharth Bhandari, 

	Prahladh Harsha, 

	Mrinal Kumar, 

	A. Shankar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The multiplicity Schwartz-Zippel lemma asserts that over a field, a low-degree polynomial cannot vanish with high multiplicity very often on a sufficiently large product set. Since its discovery in a work of Dvir, Kopparty, Saraf and Sudan [DKSS13], the lemma has found nu- merous applications in both math and computer science; in particular, in the definition and properties of multiplicity codes by Kopparty, Saraf and Yekhanin [KSY14].

In this work, we show how to algorithmize the multiplicity Schwartz-Zippel lemma for ar- bitrary product sets over any field. In other words, we give an efficient algorithm for unique decoding of multivariate multiplicity codes from half their minimum distance on arbitrary product sets over all fields. Previously, such an algorithm was known either when the un- derlying product set had a nice algebraic structure (for instance, was a subfield) [Kop15] or when the underlying field had large (or zero) characteristic, the multiplicity parameter was sufficiently large and the multiplicity code had distance bounded away from 1 [BHKS21]. In particular, even unique decoding of bivariate multiplicity codes with multiplicity two from half their minimum distance was not known over arbitrary product sets over any field.

Our algorithm builds upon a result of Kim &amp; Kopparty [KK17] who gave an algorithmic version of the Schwartz-Zippel lemma (without multiplicities) or equivalently, an efficient al- gorithm for unique decoding of Reed-Muller codes over arbitrary product sets. We introduce a refined notion of distance based on the multiplicity Schwartz-Zippel lemma and design a unique decoding algorithm for this distance measure. On the way, we give an alternate proof of Forney’s classical generalized minimum distance decoder that might be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/163"><span class="datestr">at November 18, 2021 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/18/postdocs-at-universite-de-lyon-ens-lyon-apply-by-january-4-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/18/postdocs-at-universite-de-lyon-ens-lyon-apply-by-january-4-2022/">Postdocs at Université de Lyon / ENS Lyon (apply by January 4, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two postdoctoral positions in mathematics, computer science and their interactions are open for the period 2022-2024.</p>
<p>Website: <a href="https://milyon.universite-lyon.fr/postdoctoral-positions-2022-2024-130160.kjsp?RH=1571748911317">https://milyon.universite-lyon.fr/postdoctoral-positions-2022-2024-130160.kjsp?RH=1571748911317</a><br />
Email: [guillaume.hanrot,nicolas.trotignon]@ens-lyon.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/18/postdocs-at-universite-de-lyon-ens-lyon-apply-by-january-4-2022/"><span class="datestr">at November 18, 2021 11:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/162">TR21-162 |  Fast, Algebraic Multivariate Multipoint Evaluation in Small Characteristic and Applications | 

	Vishwas Bhargava, 

	Sumanta Ghosh, 

	Mrinal Kumar, 

	Chandra Kanta Mohapatra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Multipoint evaluation is the computational task of evaluating a polynomial given as a list of coefficients at a given set of inputs. Besides being a natural and fundamental question in computer algebra on its own, fast algorithms for this problem is also closely related to fast algorithms for other natural algebraic questions like polynomial factorization and modular composition. And while \emph{nearly linear time} algorithms have been known for the univariate instance of multipoint evaluation  for close to five decades due to a work of Borodin and Moenck \cite{BM74}, fast algorithms for the multivariate version have been much harder to come by. In a significant improvement to the state of art for this problem, Umans \cite{Umans08} and Kedlaya \&amp; Umans \cite{Kedlaya11} gave nearly linear time algorithms for this problem over field of small characteristic and over all finite fields respectively, provided that the number of variables $n$ is at most $d^{o(1)}$ where the degree of the input polynomial in every variable is less than $d$. They also stated the question of designing fast algorithms for the large variable case (i.e. $n \notin d^{o(1)}$) as an open problem. 

In this work, we show that there is a deterministic algorithm for multivariate multipoint evaluation over a field $\F_{q}$ of characteristic $p$ which evaluates an $n$-variate polynomial of degree less than $d$ in each variable on $N$ inputs in time $$\left((N + d^n)^{1 + o(1)}\poly(\log q, d, p, n)\right) \, ,$$ provided that $p$ is at most $d^{o(1)}$, and $q$ is at most $(\exp(\exp(\exp(\cdots (\exp(d)))))$, where the height of this tower of exponentials is fixed. When the number of variables is large (e.g. $n \notin d^{o(1)}$), this is the first {nearly linear} time algorithm for this problem over any (large enough) field.

Our algorithm is based on elementary algebraic ideas and this algebraic structure naturally leads to the following two independently interesting applications.

\begin{itemize}
\item We  show that there is an \emph{algebraic} data structure for univariate polynomial evaluation with nearly linear space complexity and sublinear time complexity over finite fields of small characteristic and quasipolynomially bounded size. This  provides a counterexample to a conjecture of Milterson \cite{M95} who conjectured that over small finite fields, any algebraic data structure for polynomial evaluation using  polynomial space must have linear query complexity. 

 \item We also show that over finite fields of small characteristic and quasipolynomially bounded size,  Vandermonde matrices are not rigid enough to  yield size-depth tradeoffs for linear circuits via the current quantitative bounds in Valiant's program \cite{Valiant1977}. More precisely, for every fixed prime $p$, we show that for every constant $\epsilon &gt; 0$, and large enough $n$, the rank of any $n \times n$ Vandermonde matrix $V$ over the field $\F_{p^a}$ can be reduced to $
   \left(n/\exp(\Omega(\poly(\epsilon)\sqrt{\log n}))\right) $ by changing at most $n^{\Theta(\epsilon)}$ entries in every row of $V$, provided $a \leq \poly(\log n)$. Prior to this work, similar upper bounds on rigidity were known only for special Vandermonde matrices. For instance, the Discrete Fourier Transform matrices and Vandermonde matrices with generators in a geometric progression \cite{DL20}. 
   
\end{itemize}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/162"><span class="datestr">at November 17, 2021 09:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1208458514334119738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/cs-slow-to-change.html">CS Slow to Change?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Back in March of 2019 <a href="https://blog.computationalcomplexity.org/2019/03/scooped.html">I wrote</a></p><blockquote><p>I was also going to post about Yann LeCun's Facebook rant about stodgy CS departments but then Yann goes ahead and wins a Turing award with Geoffrey Hinton and Yoshua Bengio for their work on machine learning. I knew Yann from when we worked together at NEC Research in the early 2000's and let's just congratulate him and the others and let them bask in glory for truly transforming how we think of computing today. I'll get back to his post soon enough.</p></blockquote><p>So not that soon. Yann's <a href="https://www.facebook.com/story.php?story_fbid=10152719972317143&amp;id=722677142">post</a> was from 2015 where he went after "stodgy" CS departments naming Yale, Harvard, Princeton and Chicago.</p><blockquote><p>CS is a quickly evolving field.  Because of excess conservatism, these departments have repeatedly missed important trends in CS and related field, such as Data Science. They seem to view CS as meaning strictly theory, crypto, systems and programming  languages, what some have called "core CS", paying lip service to graphics, vision, machine learning, AI, HCI, robotics, etc. But these areas are the ones that have been expanding the fastest in the last decades, particularly machine learning and computer vision in the last decade....It is quite common, and somewhat natural, that newer areas (eg ML) be looked down upon by members of older, more established areas (eg Theory and Systems). After all, scientists are professional skeptics. But in a fast evolving disciplines like CS and now Data Science, an excessive aversion to risk and change is a recipe for failure.</p></blockquote><p>We've seen some changes since. Yale's Statistics Department is now <a href="https://statistics.yale.edu/">Statistics and Data Science</a>. The University of Chicago has a new Data Science <a href="https://news.uchicago.edu/story/new-college-data-science-major-foundations-insight-impact">undergrad major</a> and <a href="https://cdac.uchicago.edu/insights/introducing-the-uchicago-data-science-institute/">institute</a>.</p><p>I wonder if that's the future. CS doesn't really change that much, at least not quickly. Data science, and perhaps cybersecurity, evolve as separate fields which only have limited intersection with traditional CS. The CS degree itself just focuses on those interested in how the machines work and the theory behind them. We're busy trying to figure this out at Illinois Tech as are most other schools. And what about augmented/virtual reality and the metaverse, quantum computing, fintech, social networks, human and social factors and so on? How do you choose which bets to make? </p><p>Most of all, universities, traditionally slowly moving machines, need to far more agile even in fields outside computing since the digital transformation is affecting everything. How do you plan degrees when the computing landscape when students graduate is different from when they start? </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/cs-slow-to-change.html"><span class="datestr">at November 17, 2021 06:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6111">Scott Aaronson, when reached for comment, said…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong>About IBM’s new <a href="https://research.ibm.com/blog/127-qubit-quantum-processor-eagle">127-qubit superconducting chip</a>:</strong> As I <a href="https://www.newscientist.com/article/2297583-ibm-creates-largest-ever-superconducting-quantum-computer/">told <em>New Scientist</em></a>, I look forward to seeing the actual details!  As far as I could see, the marketing materials that IBM released yesterday take a lot of words to say absolutely nothing about what, to experts, is the single most important piece of information: namely, <em>what are the gate fidelities?</em>  How deep of a quantum circuit can they apply?  How have they benchmarked the chip?  Right now, all I have to go on is a <a href="https://quantum-computing.ibm.com/services?services=systems&amp;order=qubits%20DESC&amp;view=table&amp;system=ibm_washington">stats page</a> for the new chip, <s>which reports its average CNOT error as 0.9388—in other words, close to 1, or terrible! (But see also a <a href="https://twitter.com/decodoku/status/1460622008916627456?s=21">tweet by James Wootton</a>, which explains that such numbers are often highly misleading when a new chip is first rolled out.)  Does anyone here have more information?</s>  <strong><span class="has-inline-color has-vivid-red-color">Update (11/17):</span></strong> As of this morning, the average CNOT error has been updated to 2%.  Thanks to multiple commenters for letting me know!</p>



<p><strong>About the <a href="https://arxiv.org/abs/2110.14502">new simulation</a> of Google’s 53-qubit Sycamore chip in 5 minutes on a Sunway supercomputer (see also <a href="https://arxiv.org/abs/2111.01066">here</a>):</strong> This is an exciting step forward on the classical validation of quantum supremacy experiments, and—ironically, what currently amounts to almost the same thing—on the classical <em>spoofing</em> of those experiments.  Congratulations to the team in China that achieved this!  But there are two crucial things to understand.  First, “5 minutes” refers to the time needed to calculate a <em>single</em> amplitude (or perhaps, several correlated amplitudes) using tensor network contraction.  It doesn’t refer to the time needed to generate millions of <em>independent</em> noisy samples, which is what Google’s Sycamore chip does in 3 minutes.  For the latter task, more like a week still seems to be needed on the supercomputer.  (I’m grateful to Chu Guo, a coauthor of the new work who spoke in UT Austin’s weekly quantum Zoom meeting, for clarifying this point.)  Second, the Sunway supercomputer has parallel processing power equivalent to approximately ten million of your laptop.  Thus, even if we agreed that Google no longer had quantum supremacy as measured by time, it would still have quantum supremacy as measured by carbon footprint!  (And this despite the fact that the quantum computer itself requires a noisy, closet-sized dilution fridge.)  Even so, for me the new work underscores the point that quantum supremacy is not yet a done deal.  Over the next few years, I hope that Google and USTC, as well as any new entrants to this race (IBM? IonQ? Harvard? Rigetti?), will push forward with more qubits and, even more importantly, better gate fidelities leading to higher Linear Cross-Entropy scores.  Meanwhile, we theorists should try to do our part by inventing new and better protocols with which to demonstrate near-term quantum supremacy—<em>especially</em> protocols for which the classical verification is easier.</p>



<p><strong>About the new anti-woke <a href="https://bariweiss.substack.com/p/we-cant-wait-for-universities-to">University of Austin</a> (UATX):</strong> In general, I’m extremely happy for people to experiment with new and different institutions, and of course I’m happy for more intellectual activity in my adopted city of Austin.  And, as <em>Shtetl-Optimized</em> readers will know, I’m probably more sympathetic than most to the reality of the problem that UATX is trying to solve—living, as we do, in an era when one academic after another has been cancelled for ideas that a mere decade ago would’ve been considered unexceptional, moderate, center-left.  Having said all that, I wish I could feel more optimistic about UATX’s prospects.  I found its <a href="https://www.uaustin.org/">website</a> heavy on free-speech rhetoric but frustratingly light on what the new university is actually going to <em>do</em>: what courses it will offer, who will teach them, where the campus will be, etc. etc.  Arguably this is all excusable for a university still in ramp-up mode, but had I been in their shoes, I might have held off on the public launch until I had at least some sample content to offer.  Certainly, the fact that Steven Pinker has <a href="https://www.uaustin.org/news/uatx-statement-about-robert-zimmer-and-steven-pinker">quit UATX’s advisory board</a> is a discouraging sign.  If UATX asks me to get involved—to lecture there, to give them advice about their CS program, etc.—I’ll consider it as I would any other request.  So far, though, they haven’t.</p>



<p><strong>About the Association for Mathematical Research:</strong> Last month, some colleagues invited me to join a brand-new society called the <a href="https://amathr.org/">Association for Mathematical Research</a>.  Many of the other founders (Joel Hass, Abigail Thompson, Colin Adams, Richard Borcherds, Jeff Cheeger, Pavel Etingof, Tom Hales, Jeff Lagarias, Mark Lackenby, Cliff Taubes, …) were brilliant mathematicians who I admired, they seemed like they could use a bit of theoretical computer science representation, there was no time commitment, maybe they’d eventually do something good, so I figured why not?  Alas, to say that AMR has proved unpopular on Twitter would be an understatement: it’s received the same contemptuous reception that UATX has.  The argument seems to be: starting a new mathematical society, even an avowedly diverse and apolitical one, is really just an implicit claim that the existing societies, like the <a href="https://www.maa.org/">Mathematical Association of America (MAA)</a>  and the <a href="https://www.ams.org/home/page">American Mathematical Society (AMS)</a>, have been co-opted by woke true-believers.  But that’s paranoid and insane!  I mean, it’s not as if an <a href="https://blogs.ams.org/inclusionexclusion/">AMS blog</a> has called for the <a href="https://blogs.ams.org/inclusionexclusion/2017/05/11/get-out-the-way/#more-772">mass resignation</a> of white male mathematicians to make room for the marginalized, or the boycott of Israeli universities, or the abolition of the criminal justice system <font size="-3"><em>(what to do about Kyle Rittenhouse though?)</em></font>.  Still, even though claims of that sort of co-option are obviously far-out, rabid fantasies, yeah, I did decide to give a new organization the benefit of the doubt.  AMR might well fail or languish in obscurity, just like UATX might.  On the other hand, the barriers to making a positive difference for the intellectual world, the world I love, the world under constant threat from the self-certain ideologues of every side, do strike me as orders of magnitude smaller for a new professional society than they do for a new university.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6111"><span class="datestr">at November 16, 2021 11:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=105">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/11/16/thursday-nov-18th-nicole-immorlica-from-msr/">Thursday Nov 18th — Nicole Immorlica  from Microsoft Research</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, Nov 18</strong>th at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 19:00 Central European Time, 18:00 UTC). <strong><a href="https://immorlica.com/" target="_blank" rel="noreferrer noopener">Nicole Immorlica</a></strong> from<strong> Microsoft Research</strong> will speak about “Communicating with Anecdotes”.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: Classic models of communication in economics typically assume agents can communicate any message.  However, many important communications, such as those in newspapers or politicians’ speeches, use data to convey information.  In this talk, we explore how the reliance on data impacts communication.  In our model, there are two Bayesian agents (a sender and a receiver) who wish to communicate. The receiver must take an action whose payoff depends on their personal preferences and an unknown state of the world. The sender has access to a collection of data points correlated with the state of the world and can send exactly one of these to the receiver in order to influence her choice of action. Importantly, the sender’s personal preferences may differ from the receiver’s, which affects the sender’s strategic choice of what to send. We show that in a Nash equilibrium even a small difference in preferences can lead to a significant bias in the communicated datum. This can significantly reduce informativeness of the communication, leading to substantial utility loss for both sides. One implication is informational homophily: a receiver can rationally prefer to obtain data from a poorly-informed sender with aligned preferences, rather than a knowledgeable expert whose preferences may differ from her own.  </p>



<p>Joint work with Nika Haghtalab, Brendan Lucier, Markus Mobius and Divya Mohan.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/11/16/thursday-nov-18th-nicole-immorlica-from-msr/"><span class="datestr">at November 16, 2021 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/161">TR21-161 |  On Worst-Case Learning in Relativized Heuristica | 

	Mikito Nanashima, 

	Shuichi Hirahara</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A PAC learning model involves two worst-case requirements: a learner must learn all functions in a class on all example distributions. However, basing the hardness of learning on NP-hardness has remained a key challenge for decades. In fact, recent progress in computational complexity suggests the possibility that a weaker assumption might be sufficient for worst-case learning than the feasibility of worst-case algorithms for NP problems.

In this study, we investigate whether these worst-case requirements for learning are satisfied on the basis of only average-case assumptions in order to understand the nature of learning. First, we construct a strong worst-case learner based on the assumption that DistNP $\subseteq$ AvgP, i.e., in Heuristica. Our learner agnostically learns all polynomial-size circuits on all unknown P/poly-samplable distributions in polynomial time, where the complexity of learning depends on the complexity of sampling examples. Second, we study the limitation of relativizing constructions of learners based on average-case heuristic algorithms. Specifically, we construct a powerful oracle such that DistPH $\subseteq$ AvgP, i.e., every problem in PH is easy on average, whereas UP $\cap$ coUP and PAC learning on almost-uniform distributions are hard even for $2^{n/\omega(\log n)}$-time algorithms in the relativized world, which improves the oracle separation presented by Impagliazzo (CCC 2011). The core concept of our improvements is the consideration of a switching lemma on a large alphabet, which may be of independent interest. The lower bound on the time complexity is nearly optimal because Hirahara (STOC 2021) showed that DistPH $\subseteq$ AvgP implies that PH can be solved in time $2^{O(n / \log n)}$ under any relativized world.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/161"><span class="datestr">at November 16, 2021 03:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/16/assistant-professor-tenure-track-in-quantum-computing-quantum-information-theory-at-qici-department-of-computer-science-the-university-of-hong-kong-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/16/assistant-professor-tenure-track-in-quantum-computing-quantum-information-theory-at-qici-department-of-computer-science-the-university-of-hong-kong-apply-by-december-15-2021/">Assistant Professor (Tenure Track) in Quantum Computing/Quantum Information Theory at QICI, Department of Computer Science, The University of Hong Kong (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Eligible candidates will have a strong track record in quantum computing and/or quantum information theory, and will hold a PhD in Computer Science, Physics, or Mathematics.</p>
<p>The official deadline for applications is March 31 2022. To receive full consideration, candidates are recommended to submit their applications before December 15 2021.</p>
<p>Website: <a href="https://qici.weebly.com/">https://qici.weebly.com/</a><br />
Email: giulio@cs.hku.hk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/16/assistant-professor-tenure-track-in-quantum-computing-quantum-information-theory-at-qici-department-of-computer-science-the-university-of-hong-kong-apply-by-december-15-2021/"><span class="datestr">at November 16, 2021 06:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/15/postdoc-at-the-university-of-texas-at-austin-apply-by-january-15-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/15/postdoc-at-the-university-of-texas-at-austin-apply-by-january-15-2022/">postdoc at The University of Texas at Austin (apply by January 15, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The NSF AI Institute for Foundations of Machine Learning (<a href="https://ifml.institute/">https://ifml.institute/</a>), and the NSF TRIPODS program at UT-Austin seek candidates for UT Machine Learning Postdoctoral Fellowships. Appointments will begin Summer or Fall 2022. Fellows can collaborate with researchers involved in IFML partner institutions: UT-Austin, UW, MSR Redmond, and WSU.</p>
<p>Website: <a href="http://apply.interfolio.com/98753">http://apply.interfolio.com/98753</a><br />
Email: ifml@austin.utexas.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/15/postdoc-at-the-university-of-texas-at-austin-apply-by-january-15-2022/"><span class="datestr">at November 15, 2021 11:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/160">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/160">TR21-160 |  Tight Bounds for General Computation in Noisy Broadcast Networks | 

	Klim Efremenko, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Let $\Pi$ be a protocol over the $n$-party broadcast channel, where in each round, a pre-specified party broadcasts a symbol to all other parties. We wish to design a scheme that takes such a protocol $\Pi$ as input and outputs a noise resilient protocol $\Pi'$ that simulates $\Pi$ over the noisy broadcast channel, where each received symbol is flipped with a fixed constant probability, independently. What is the minimum overhead in the number of rounds that is incurred by any such simulation scheme?

A classical result by Gallager from the 80's shows that non-interactive $T$-round protocols, where the bit communicated in every round is independent of the communication history, can be converted to noise resilient ones with only an $\mathcal{O}(\log \log T)$ multiplicative overhead in the number of rounds. Can the same be proved for any protocol? Or, are there protocols whose simulation requires an $\Omega(\log T)$ overhead (which always suffices)? 

We answer both the above questions in the negative: We give a simulation scheme with an $\tilde{O}(\sqrt{\log T})$ overhead for every protocol and channel alphabet. We also prove an (almost) matching lower bound of $\Omega(\sqrt{\log T})$ on the overhead required to simulate the pointer chasing protocol with $T=n$ and polynomial alphabet.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/160"><span class="datestr">at November 15, 2021 09:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/11/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/11/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.youtube.com/watch?v=4gsp3CZUtV0">How to tell someone how to get to your house without knowing where they’re starting</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107206208305724024">\(\mathbb{M}\)</a>),</span> how to tell someone how to get to your house without knowing where they’re starting, how to translate a Penn &amp; Teller trick into Spanish, and other applications of the <a href="https://en.wikipedia.org/wiki/Synchronizing_word">Černý conjecture on synchronizing words</a>. This video by Nóra Szakács is one of an enormous number of recent mathematics explanation videos in the <a href="https://www.youtube.com/watch?v=F3Qixy-r_rQ">“Some1” summer of math exposition video project</a>.</p>
  </li>
  <li>
    <p>One of our instructor’s strategies for dealing with sites like Chegg that cater to homework answer-copiers <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107217545955814360">\(\mathbb{M}\)</a>):</span> seek out and assign homework questions that already have particularly bad answers on Chegg. That way the copiers are more easily caught and the students who actually do the work have a better chance to shine over their copying peers. And if doing this ends up causing Chegg to get more of a reputation for inaccuracy, that’s not a bad thing either.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-021-02906-8">Facebook isn’t the only company changing its name in an unsuccessful attempt to separate itself from its poor reputation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107223269542696792">\(\mathbb{M}\)</a>).</span> Around when the FTC fined OMICS for predatory practices, it began rebranding its journals and past papers with other publisher names (Hilaris, Longdom, iMedPub, and Research &amp; Reviews), rewriting the editorship history of its journals, and even lifting other publishers’ papers to fabricate a legitimate-looking backlog for its journals.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Constructible_number">Constructible number</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107233293561788882">\(\mathbb{M}\)</a>),</span> now a Good Article on Wikipedia. This concerns the correspondence between compass-and-straightedge constructions and formulas using square roots, and the use of that correspondence to prove the impossibility of classical geometric construction problems. Relatedly, see <a href="http://jdh.hamkins.org/the-hierarchy-of-geometric-constructibility-can-we-go-back/">Joel Hamkins’ new blog post</a> on how constructibility of one set of points from another produces an equivalence relation on pairs of points.</p>
  </li>
  <li>
    <p><a href="https://www.smh.com.au/national/nsw/quantum-computers-to-run-sydney-s-transport-network-20211107-p596r5.html">Quantum computers to run Sydney’s transport network</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107236800356354250">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=29139633">via</a>). In other news, Sydney politicians are unable to distinguish hype from actual technology advancements, or maybe more cynically unwilling to do so when the hype would let them dole out lucrative contracts.</p>
  </li>
  <li>
    <p><a href="https://terrytao.wordpress.com/2021/11/07/venn-and-euler-type-diagrams-for-vector-spaces-and-abelian-groups/">Terry Tao attempts to visualize vector spaces, linear maps, and exact sequences using arrows from arrows to arrows</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107240160987527319">\(\mathbb{M}\)</a>).</span> Ok, some of the arrows are really supposed to be half-open intervals, but drawn with a barred closed end and a pointy open end they look a lot like <span style="white-space: nowrap;">\(\mapsto\).</span></p>
  </li>
  <li>
    <p><a href="https://www.inputmag.com/culture/pinterest-sucks-google-image-photo-search-ruining-internet">How Pinterest utterly ruined photo search on the internet</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107246007841558595">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=29151055">via</a>).</span> The title is a bit of an exaggeration given how easy it is to add <code class="language-plaintext highlighter-rouge">-pinterest</code> to every single image search you do (with quick feedback for why you need to do it when you forget). But they’re not wrong about it being a useless annoyance.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Farthest-first_traversal">Farthest-first traversal / greedy permutation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107251720248944263">\(\mathbb{M}\)</a>):</span> generate a sequence of points, choosing each one to be as far as possible from already chosen points. Its first \(k\) points form good cluster centers (approximating min-max-diameter or min-max-radius clustering) and are well separated (approximating max-min distance). Its applications include halftoning, color quantization, sensor network distribution, and underwater robot task planning. Now another Wikipedia Good Article.</p>
  </li>
  <li>
    <p>Today in amusingly-named concepts: the <a href="https://en.wikipedia.org/wiki/Lorentz%E2%80%93Lorenz_equation">Lorentz–Lorenz equation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107257423214935119">\(\mathbb{M}\)</a>)</span> relating refractive index to polarizability. Even more amusingly, it was not a deliberate collaboration: Lorentz and Lorenz discovered the equation independently! Via a comment over on that other site, in <a href="https://twitter.com/hollykrieger/status/1456321630733574144">a thread making fun of the new Meta logo for its resemblance to a strange attractor</a>. Now if only we could somehow connect it to <a href="https://read.somethingorotherwhatever.com/entry/ItislikeeggPaulLorenzenandthecollapseofproofsofconsistency">Paul Lorenzen</a>…</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/107258956124702057">How to search for number facts sites without searching for number facts sites</a> and <a href="https://mathstodon.xyz/@jsiehler/107259309683467429">how to search for the mathematics of poker without getting a bunch of sketchy gambling sites</a>: search for the numbers, not the words.</p>
  </li>
  <li>
    <p><a href="https://eli.thegreenplace.net/2021/rust-data-structures-with-circular-references/">How memory-safe programming in Rust can complicate the design of data structures</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107271384674149840">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=29207397">via</a>, <a href="https://lobste.rs/s/ydstwh/rust_data_structures_with_circular">via2</a>). Rust data structures cannot have circular references, so if you need them you can either fall back to non-memory-safe techniques (reference counting or unsafe blocks) or make your top-level structure implement its own memory allocator and refer to everything by indexes into its vector of objects instead of by proper references.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">Too few women at POPL 2022</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107272383946893132">\(\mathbb{M}\)</a>).</span> The low 11% representation on the program committee is especially striking in contrast to the significant historical contributions of women to programming languages (Kathleen Booth, Cicely Popplewell, Grace Hopper, Jean Sammet, Mary Hawes, Gertrude Tierney, and Barbara Liskov among them). According to the same post, the major theoretical computer science conferences are only slightly better.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Handshaking_lemma">Handshaking lemma</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107284266077522291">\(\mathbb{M}\)</a>),</span> now another Good Article on Wikipedia. This is really about two different but closely-related results, both proved by Euler in his 1736 paper that kicked off the field of graph theory: in any finite undirected graph, the sum of vertex degrees equals twice the number of edges, and the number of odd-degree vertices is even.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/11/15/linkage.html"><span class="datestr">at November 15, 2021 06:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-378272747883656707">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/when-did-computer-science-theory-get-so.html">When did Computer Science Theory Get so Hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I posted on <a href="https://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html">When did Math get so hard?</a> a commenter pointed out that one can also ask </p><p><br /></p><p><i>When did Computer Science Theory Get so Hard?</i></p><p>For the Math-question I could only speculate. For CS- I WAS THERE! When I was in Grad School one could learn all of Complexity theory in a year-long course (a hard one, but still!). The main tools were logic and combinatorics. No Fourier Transforms over finite fields. I am NOT going to say</p><p><i>Those were the good old days.</i></p><p>I will say that it was easier to make a contribution without knowing much. Oddly enough, it is MORE common for ugrads and grad students to publish NOW then it was THEN, so that may be a pair of ducks.</p><p><b>Random Thoughts on This Question</b></p><p>1) The Graph Minor Theorem was when P lost its innocence. Before the GMT most (though not all)  problems in P had easy-to-understand  algorithms using algorithmic paradigms (e.g., Dynamic  Programming) and maybe some combinatorics. Computational Number Theory used.... Number Theory (duh), but I don't think it was hard number theory. One exception was Miller's Primality test which needed to assume the Extended Riemann Hypothesis- but you didn't have to understand ERH to use it. </p><p>1.5) GMT again. This did not only give hard-deep-math algorithms to get problems in P. It  also pointed to  how hard proving P NE NP would be--- to rule out something like a GMT-type result to get SAT in P seems rather hard. </p><p>2) Oracle Constructions were fairly easy diagonalizations. It was bummed out that I never had to use an infinite injury priority argument. That is, I knew some complicated recursion theory, but it was never used. </p><p>2.5) Oracles again. Dana Angluin had a paper which used some complicated combinatorics to construct an oracle, see <a href="https://www.sciencedirect.com/science/article/pii/0304397580900274?via%3Dihub">here</a>. Later Andy Yao showed that there is an oracle A such that  PH^A NE  PSPACE^A. You might know that result better as</p><p><i>Constant depth circuits for parity must have exponential size. </i></p><p>I think we now care about circuits more than oracles, see my post <a href="https://blog.computationalcomplexity.org/2015/04/the-new-oracle-result-new-circuit.html">here</a> about that issue. Anyway, oracle results since then have used hard combinatorial and other math arguments. </p><p>3) The PCP result was a leap forward for difficulty. I don't know which paper to pick as THE Leap since there were several. And papers after that were also rather difficult.  </p><p>4) I had a blog post <a href="https://blog.computationalcomplexity.org/2021/04/do-any-np-reductions-use-deep.html#comment-form">here</a> where I asked if REDUCTIONS ever use hard math. Some of the comments are relevant here:</p><p>Stella Biderman: The deepest part of the original PCP theorem is the invention of the VC paradigm in the 1990's.</p><p>Eldar: Fourier Theory was introduced to CS with Hastad's Optimal Approximation results. Today it might not be considered deep, but I recall when it was.</p><p>Also there are Algebraic Geometry codes which use downright arcane mathematics...</p><p>Hermann Gruber refers to Comp Topology and Comp Geometry and points to the result that 3-manifold knot genus is NP-complete, see <a href="https://dl.acm.org/doi/10.1145/509907.510016">here</a>.</p><p>Anonymous (they leave many comments) points to the deep math reductions in arithmetic versions of P/NP classes, and Mulmuley's work (Geometric Complexity Theory).</p><p>Timothy Chow points out that `deep' could mean several things and points to a math overflow post on the issue of depth, <a href="https://mathoverflow.net/questions/139607/what-are-some-deep-theorems-and-why-are-they-considered-deep">here</a>.</p><p>Marzio De Biasi points out that even back in 1978 there was a poly reduction that required a good amount of number theory: the NPC of the Diophantine binary quad equation</p><p>ax^2 + by + c = 0 </p><p>by Manders and Adelman, see <a href="https://www.sciencedirect.com/science/article/pii/0022000078900442?via%3Dihub">here</a>.</p><p>(Bill Comment) I tend to think this is an outlier- for the most part, CS theory back in the 1970's did not hard math. </p><p>4) Private Info Retrieval (PIR). k databases each have the same n-bit string and cannot talk to each other. a server wants the ith bit and (in the info-theoretic case) wants the DBs to know NOTHING about the question i. </p><p>Easy results (to understand) 2-server, n^{1/3}. <a href="http://www.cs.umd.edu/~gasarch/TOPICS/pir/first.pdf">here</a>.</p><p>Hard results: 2-server n^{O(\sqrt{loglogn/log n)},  <a href="https://www.cs.princeton.edu/~zdvir/papers/DvirGopi14.pdf">here</a>.</p><p>(I have a website on PIR, not maintained,  <a href="http://www.cs.umd.edu/~gasarch/TOPICS/pir/pir.html">here</a>.)</p><p>5) Babai's algorithm for GI in quasi-poly time used hard math. </p><p>6) If I knew more CS theory I am sure I would have more papers listed.</p><p>But now its your turn: </p><p>When did you realize Gee, <b>CS theory is harder than (a) you thought, (b) it used to be.</b></p><p><b><br /></b></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/when-did-computer-science-theory-get-so.html"><span class="datestr">at November 15, 2021 03:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/15/faculty-at-northwestern-university-apply-by-december-10-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/15/faculty-at-northwestern-university-apply-by-december-10-2021/">Faculty at Northwestern University (apply by December 10, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite candidates to apply for new positions as Assistant, Associate and Full Professor of Computer Science. We are interested in applications from outstanding candidates in all areas of Computer Science. See the website for cross-cutting areas of interest.</p>
<p>Website: <a href="https://www.mccormick.northwestern.edu/computer-science/careers/">https://www.mccormick.northwestern.edu/computer-science/careers/</a><br />
Email: nu.cstheory@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/15/faculty-at-northwestern-university-apply-by-december-10-2021/"><span class="datestr">at November 15, 2021 01:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/14/postdoc-at-university-of-waterloo-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/14/postdoc-at-university-of-waterloo-apply-by-december-31-2021/">Postdoc at University of Waterloo (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms &amp; Complexity group at the University of Waterloo is offering postdoctoral positions starting in the Fall of 2022. We seek candidates from all areas of theoretical computer science.</p>
<p>Interested applicants should have their CV, research statement, and three recommendation letters emailed to theory.waterloo@gmail.com. Applications are due December 31st.</p>
<p>Website: <a href="https://algcomp.uwaterloo.ca/positions/">https://algcomp.uwaterloo.ca/positions/</a><br />
Email: theory.waterloo@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/14/postdoc-at-university-of-waterloo-apply-by-december-31-2021/"><span class="datestr">at November 14, 2021 08:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/11/14/random-independent-sets">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/11/14/random-independent-sets.html">Random independent sets in bounded-treewidth graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This week my student Daniel Frishberg posted our new preprint “Rapid mixing of the hardcore Glauber dynamics and other Markov chains in bounded-treewidth graphs”, <a href="https://arxiv.org/abs/2111.03898">arXiv:2111.03898</a>. Despite the somewhat-technical title and content, it’s on an easy-to-explain problem, of generating random combinatorial objects (like independent sets in graphs) using random walks. Start with an undirected graph and an empty subset of its vertices, and then repeatedly choose a random vertex, flipping whether it is in or out of the subset whenever the resulting subset remains independent. How many of these steps does it take until the resulting random distribution on independent sets is nearly uniform?</p>

<p>In general, this can take exponentially many steps, but our paper proves that in graphs of bounded <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a> it is only polynomial. Our proof’s exponent depends on the treewidth, but maybe a better proof can get the dependence on treewidth out of the exponent and into the constant factor of the polynomial instead. However, if all you want to do is generate random independent sets, this is not the right way to do it. The point of the paper is less about fast algorithms and more about the connectivity of the space of independent sets. So if you do want a fast algorithm for random generation, what should you do?</p>

<p>This is, unfortunately, an area where some care is required and much of the literature does not take the required care. Problematic issues include:</p>

<ul>
  <li>
    <p>What is the model of computation for arithmetic operations? Much of the literature for the design and analysis of graph algorithms assumes without much attention that all arithmetic operations take unit time. This is a reasonable assumption for small numbers (of at most polynomial magnitude), as occur in many algorithms including the random walk above. In those cases it is a close match to the kind of operation that can be done in a single instruction on a modern CPU. It is not a reasonable assumption for counting algorithms dealing with numbers of exponential magnitude. Counting is an important subproblem for exact random generation, so we need large numbers. Typically, the number of independent sets is linear in the input size, the number of bits required to store this number is linear, and the amount of time needed to perform a single operation on a number this large should again be assumed to involve a bignum-package subroutine. The slow steps are multiplications, which can by <a href="https://www.jstor.org/stable/10.4007/annals.2021.193.2.4">recent results of Harvey and van der Hoeven</a> be assumed to take \(O(n\log n)\) time.</p>
  </li>
  <li>
    <p>What is the model for generating random numbers? If random numbers are generated as bits, this only allows the direct generation of random choices with probabilities that are <a href="https://en.wikipedia.org/wiki/Dyadic_rational">dyadic rational</a>. For independent sets, we need other probabilities. <a href="https://en.wikipedia.org/wiki/Rejection_sampling">Rejection sampling</a> leads to an algorithm whose running time is itself a random variable, so we need to use expected time or high-probability bounds instead of worst-case time analysis.</p>
  </li>
  <li>
    <p>How is the input presented? For the random-walk method, it is just a graph, but for other algorithms taking advantage of low treewidth we need a good <a href="https://en.wikipedia.org/wiki/Tree_decomposition">tree decomposition</a>, and finding one is nontrivial. We should either state explicitly that a tree decomposition is given and that its width rather than the graph width controls the algorithm’s runtime, or factor in the time to find a decomposition and the width of decomposition that we find.</p>
  </li>
</ul>

<p>With that all said, how might we go about computing a random independent set for a given \(n\)-vertex graph of treewidth \(w\), achieving the best theoretical performance?</p>

<p>First, we should find an approximate tree decomposition. There are many known algorithms for this problem, but I think the right choice is the one from the recent paper “An improvement of Reed’s treewidth approximation”, Belbasi and Fürer, WALCOM 2021, <a href="https://arxiv.org/abs/2010.03105">arXiv:2010.03105</a>. It finds a tree decomposition of width at most \(5w\), in time \(O(2^{8.8w}n\log n)\). There are other algorithms with a linear dependence on \(n\), but much worse dependence on \(w\), and the \(n\log n\) part of the bound will be dominated by later steps. We can also bootstrap this decomposition, using dynamic programming on it to find a better decomposition, but I think this is too expensive for the savings it produces.</p>

<p>Second, we should count independent sets. Root the tree decomposition, so that we can talk about the subtree descending from any bag and the subgraph of the given graph that it corresponds to. Then for each bag of the decomposition (in bottom-up order), and each independent subset of the bag, we want to count the number of independent sets in the corresponding subgraph that intersect the bag in the given subset. These independent sets are formed by combining choices of independent sets in child nodes. Therefore, we need to find the numbers of independent sets in each child node that are consistent with the choice of subset at the parent, and multiply them together. Each subset of a bag contributes to exactly one such computation at its parent, so the total number of arithmetic operations for this computation is the product of the number of bags and the number of subsets per bag. With bignum arithmetic, the total time for computing all of these counts is \(O(2^{5w}n^2\log n)\). Our new preprint cites a paper for this subproblem that isn’t sufficiently careful about bignum arithmetic, but still somehow ends up with a slower cubic total time bound; I think they’re just being sloppy and overcounting somehow.</p>

<p>Third, we can then reverse the counting process and step downward through the tree decomposition choosing how the random independent set intersects each bag. When we do this for a bag, we already know the intersection of the independent set with the parent bag. Therefore, all we have to do is choose among the subsets of the child bag that are consistent with that already-made choice, using probabilities that are proportional to the numbers of independent sets that can be formed for each consistent subset. We know these numbers because we already computed them in the second stage. So it’s just a single random choice per bag, which (because it involves bignum probabilities that are not dyadic rational) can reasonably be assumed to take a random amount of time whose expectation is linear. For the overall algorithm, the total expected time is \(O(n^2)\). Also, this stage of the algorithm can be repeated over and over, generating new random independent sets, without having to generate new tree decompositions or new counts.</p>

<p>Putting it all together, it appears that the total time for randomly generating independent sets, using dynamic programming on the tree decomposition to count these sets, and assuming fast bignum arithmetic, is \(O(2^{O(w)}n^2\log n)\) for the preprocessing stages of the algorithm, and then \(O(n^2)\) for each successive generated set.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107278583702646630">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/11/14/random-independent-sets.html"><span class="datestr">at November 14, 2021 04:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=956">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/11/14/i-doubt-that-there-will-be-eight-i-highly-doubt-there-will-be-eight/">“I doubt that there will be eight, I highly doubt there will be eight.”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://www.nbcboston.com/news/local/newton-voters-reject-measures-to-ban-or-limit-marijuana-shops/104470/">Says campaign strategist at 1:13 on this clip from almost exactly 3 years ago.</a></p>



<p>For background readers can look <a href="https://emanueleviola.wordpress.com/tag/marijuana/">at this tag</a>.</p>



<p>Where are we today?  (Official communication.)</p>



<ul><li>Union Twist, 1158 Beacon Street in Newton Centre/Four Corners, has been unanimously approved by the City Council for its Special Permit. Union Twist plans to demolish the current building on the site consisting of a dry cleaner and restaurant and build a new 2,300-square foot, one story retail building with 22 on-site parking spaces. For at least the first six months of operation, there will be an appointment only requirement for customers at the store. As their Special Permit has been approved, they can move forward with their state Cannabis Control Commission (CCC) licensing approval process.</li></ul>



<ul><li>Garden Remedies, Newton’s first adult-use retail marijuana store, opened at 697 Washington Street in Newtonville more than two years ago in May 2019. Since opening, Garden Remedies has sold to customers by appointment only. The request from Garden Remedies to amend its current Special Permit to remove the appointment only condition for their retail shop was approved by the City Council Land Use Committee. (The vote was 5-0, with three abstentions.) The full City Council will vote on lifting the appointment only condition at their meeting this Monday, Nov. 15.</li></ul>



<ul><li>Redi, Newton’s second adult-use retail marijuana establishment (formerly known as Cypress Tree) opened in July 2021 at 24 Eliot Street at the intersection with Boylston Street/Route 9 in Upper Falls. (It was (formerly known as Cypress Tree.) The Special Permit approved by the City Council requires that Redi’s retail customers must have an appointment to shop or pick-up products for at least the first six months of operation. </li></ul>



<ul><li>Ascend, 1089 Washington Street/58 Cross Street just outside West Newton Square, has a signed provisional HCA and an approved Special Permit. Construction is nearing completion and they are awaiting licensing approval by the CCC to open.</li></ul>



<ul><li>MedMen, 232 Boylston Street/ Rte. 9 in Chestnut Hill (at the former Shreve, Crump &amp; Low location), has a signed provisional Host Community Agreement (HCA) and an approved Special Permit. They are both awaiting licensing approval by the CCC to open as well as a building permit from the City of Newton to begin work on the building.</li></ul>



<ul><li>Green Lady, 740 Beacon Street in Newton Centre, a woman and minority owned business, has a signed provisional HCA and the City Council Land Use Committee is currently hearing its Special Permit application. If their Special Permit is approved, they can then move forward with their CCC licensing approval process </li></ul>



<ul><li>Verilife, 131 Rumford Avenue in Auburndale, has a signed provisional HCA and the City Council Land Use Committee is currently hearing its Special Permit application. If their Special Permit is approved, they can then move forward with their CCC licensing approval process.</li></ul>



<ul><li>Nuestra, 1185 Chestnut Street in Newton Upper Falls, has a signed provisional HCA. Nuestra is a Cannabis Control Commission certified Economic Empowerment Applicant. They have not yet started the City Council Special Permit approval process which they need to do before moving forward in the state licensing approval process.</li></ul>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/11/14/i-doubt-that-there-will-be-eight-i-highly-doubt-there-will-be-eight/"><span class="datestr">at November 14, 2021 10:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19328">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">POPL 2022—Et Tu, Brute?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Some things just cannot wait. Men must stand up now for women’s equality— Rick Goings</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/boothpopplewellhoppersammet/" rel="attachment wp-att-19330"><img width="185" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/BoothPopplewellHopperSammet.png?resize=185%2C231&amp;ssl=1" class="alignright size-full wp-image-19330" height="231" /></a></p>
<p>
Kathleen Booth, Cicely Popplewell, Grace Hopper, and Jean Sammet were four pioneers of the field of Programming Languages. They are credited for (co-)creating the first assembly language, programming manual, compiler, and widest-use language (COBOL), the last alongside Mary Hawes and Gertrude Tierney. Popplewell’s name heads the proceedings of the 1962 <a href="https://hal.inria.fr/IFIP">IFIP</a> conference, while Sammet wrote a defining textbook of the field, <em>Programming Languages: History and Fundamentals</em>, in 1969. </p>
<p>
Today Ken and I wonder why the 56-person strong programming committee for the 2022 Principles of Programming Languages conference has only six women.<br />
<span id="more-19328"></span></p>
<p>
If you were asked to name <b>a</b> <em>principle</em> of programming languages, there’s a good chance you’d think of the <a href="https://en.wikipedia.org/wiki/Liskov_substitution_principle">Liskov substitution principle</a>, which is named for Barbara Liskov. Of all fields of computing, we would hope that this would be the leader toward gender balance. It is true that across the board in computer science, we are struggling to get over 20% female representation. But we should be doing better than <img src="https://s0.wp.com/latex.php?latex=%7B6%2F56+%3D+10.714...%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6/56 = 10.714...\%}" class="latex" /> now.	</p>
<p>
</p><p></p><h2> The Full Committee </h2><p></p>
<p></p><p>
Here is the full POPL committee. We have starred the six women—whom we salute.</p>
<ol>
<p></p><li>
<a href="https://thakur.cs.ucdavis.edu">Aditya V. Thakur</a> <p></p>
</li><li>
<a href="https://www.lambdabetaeta.eu">Alex Kavvos</a> <p></p>
</li><li>
<a href="https://people.eecs.berkeley.edu/~akcheung/">Alvin Cheung</a>	 <p></p>
</li><li>
<b>*</b><a href="https://www.ccs.neu.edu/home/amal/">Amal Ahmed	</a> <p></p>
</li><li>
<a href="https://www.linkedin.com/in/andrew-kennedy-3324287/?originalSubdomain=uk">Andrew Kennedy</a>	 <p></p>
</li><li>
<a href="https://www.cs.ox.ac.uk/people/anthony.lin/">Anthony W. Lin</a>	 <p></p>
</li><li>
<a href="https://www.fos.kuis.kyoto-u.ac.jp/~igarashi/index.html.en">Atsushi Igarashi</a>	 <p></p>
</li><li>
<a href="https://cs.au.dk/~spitters/">Bas Spitters</a> <p></p>
</li><li>
<a href="https://scholar.google.de/citations?user=KEt1bfkAAAAJ&amp;hl=en">Benjamin Lucien Kaminski</a>	 <p></p>
</li><li>
<a href="https://www.cs.ox.ac.uk/people/luke.ong/">C.-H. Luke Ong</a>	 <p></p>
</li><li>
<b>*</b><a href="https://caterinaurban.github.io">Caterina Urban</a> <p></p>
</li><li>
<a href="https://users.cs.northwestern.edu/~chrdimo/">Christos Dimoulas</a> <p></p>
</li><li>
<a href="https://homes.luddy.indiana.edu/ccshan/">Chung-chieh Shan</a> <p></p>
</li><li>
<a href="https://www.kth.se/profile/dbro">David Broman</a>	 <p></p>
</li><li>
<a href="https://dimitriv.github.io">Dimitrios Vytiniotis</a>	 <p></p>
</li><li>
<a href="https://www.cs.toronto.edu/~fanl/">Fan Long</a>	 <p></p>
</li><li>
<a href="https://orcid.org/0000-0001-5011-3458">Filip Sieczkowski</a>	 <p></p>
</li><li>
<a href="http://pauillac.inria.fr/~fpottier/">Francois Pottier</a>	 <p></p>
</li><li>
<a href="https://cs.uiowa.edu/people/garrett-morris">Garrett Morris</a>	 <p></p>
</li><li>
<b>*</b><a href="https://cs.nju.edu.cn/hongjin/">Hongjin Liang</a>	 <p></p>
</li><li>
<a href="https://www.cs.ox.ac.uk/people/hongseok.yang/Public/Home.html">Hongseok Yang</a>	 <p></p>
</li><li>
<a href="https://jamesrwilcox.com">James Wilcox</a>	 <p></p>
</li><li>
<a href="https://www.cs.cmu.edu/~janh/">Jan Hoffmann</a>	 <p></p>
</li><li>
<a href="https://johnwickerson.github.io">John Wickerson</a>	 <p></p>
</li><li>
<a href="https://justinh.su">Justin Hsu</a>	 <p></p>
</li><li>
<a href="https://www.cs.utexas.edu/people/faculty-researchers/ken-mcmillan">Ken McMillan</a>	 <p></p>
</li><li>
<a href="https://scholar.google.co.kr/citations?user=bWY6MmgAAAAJ&amp;hl=ko">Kihong Heo</a> <p></p>
</li><li>
<a href="https://www.fos.kuis.kyoto-u.ac.jp/~ksuenaga/">Kohei Suenaga</a>	 <p></p>
</li><li>
<b>*</b><a href="https://www.kurims.kyoto-u.ac.jp/~kmuroya/">Koko Muroya</a>	 <p></p>
</li><li>
<a href="https://lemonidas.github.io">Leonidas Lampropoulos</a>	 <p></p>
</li><li>
<a href="https://www.cs.rhul.ac.uk/home/uxac009/">Matthew Hague</a>	 <p></p>
</li><li>
<a href="https://www.microsoft.com/en-us/research/people/mattpark/">Matthew Parkinson</a>	 <p></p>
</li><li>
<a href="https://people.csail.mit.edu/mcarbin/">Michael Carbin</a>	 <p></p>
</li><li>
<a href="https://www.yale-nus.edu.sg/about/faculty/michael-d-adams/">Michael D. Adams</a>	 <p></p>
</li><li>
<a href="https://michael-emmi.github.io">Michael Emmi</a> <p></p>
</li><li>
<a href="https://www.cl.cam.ac.uk/~nk480/">Neel Krishnaswami</a> <p></p>
</li><li>
<a href="https://www.imperial.ac.uk/people/n.wu">Nicolas Wu</a> <p></p>
</li><li>
<b>*</b><a href="https://nikivazou.github.io">Niki Vazou</a>	 <p></p>
</li><li>
<a href="https://www.cs.tau.ac.il/~orilahav/">Ori Lahav</a>	 <p></p>
</li><li>
<a href="https://cs.illinois.edu/about/people/faculty/madhu">P. Madhusudan</a>	 <p></p>
</li><li>
<a href="https://pavpanchekha.com">Pavel Panchekha</a>	 <p></p>
</li><li>
<a href="https://katalog.uu.se/profile/?id=N11-35">Philipp Ruemmer</a>	 <p></p>
</li><li>
<a href="https://pageperso.lis-lab.fr/pierre.clairambault/">Pierre Clairambault</a>	 <p></p>
</li><li>
<a href="https://helloqirun.github.io">Qirun Zhang</a> <p></p>
</li><li>
<a href="https://www.linkedin.com/in/rgrig/?originalSubdomain=uk">Radu Grigore</a>	 <p></p>
</li><li>
<a href="https://www.cs.ox.ac.uk/people/ralf.hinze/">Ralf Hinze</a>	 <p></p>
</li><li>
<a href="https://ranjitjhala.github.io">Ranjit Jhala</a>	 <p></p>
</li><li>
<a href="https://bentnib.org">Robert Atkey</a>	 <p></p>
</li><li>
<a href="https://scholar.harvard.edu/napadow/people/ronald-garcia-md-phd">Ronald Garcia</a>	 <p></p>
</li><li>
<a href="https://www.cs.columbia.edu/~rgu/">Ronghui Gu</a> <p></p>
</li><li>
<a href="http://www.cs.ox.ac.uk/people/samuel.staton/main.html">Sam Staton</a>	 <p></p>
</li><li>
<b>*</b><a href="https://people.irisa.fr/Sandrine.Blazy/">Sandrine Blazy</a>	 <p></p>
</li><li>
<a href="https://group-mmm.org/~s-katsumata/index-e.html">Shin-ya Katsumata</a> <p></p>
</li><li>
<a href="https://waseda.pure.elsevier.com/en/persons/tachio-terauchi">Tachio Terauchi</a>	 <p></p>
</li><li>
<a href="https://www.comp.nus.edu.sg/cs/bio/chinwn/">Wei-Ngan Chin</a>	 <p></p>
</li><li>
<a href="https://www.linkedin.com/in/woosuk-lee-59609035/">Woosuk Lee</a><p></p>
</li></ol>
<p>
</p><p></p><h2> I Cannot Believe It </h2><p></p>
<p></p><p>
I have always enjoyed the POPL conference. I recall being there ages ago—I was there in 1980 on a <a href="https://www.researchgate.net/publication/262331842_Theoretical_and_empirical_studies_on_using_program_mutation_to_test_the_functional_correctness_of_programs">paper</a>, “Theoretical and Empirical Studies on Using Program Mutation to Test the Functional Correctness of Programs,” with Timothy Budd, Richard DeMillo, and Frederick Sayward.</p>
<p>
I had planned to do a long post on POPL 2022. This POPL is planned to be a real in-person conference for this coming 2022—not virtual. But I was upset to say the least by the above ratio. </p>
<p>
Ken and I were similarly stopped the week before Halloween. We had intended a post saluting those who aggregate theory of computing blogs and including a list of over <b>50</b> active blogs. We stopped it because, as noted at the end of the <a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/">post</a> that Ken wrote instead, one has to stretch to reach even <b>4</b> mathematics/theory blogs by women. </p>
<p>
Counts of the most recent FOCS and STOC program committees give slightly better percentages. Slightly. For STOC 2022 we count 7 women out of 46, giving This is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B7%7D%7B46%7D+%3D+0.152...+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{7}{46} = 0.152... " class="latex" /></p>
<p>This is just a notch better. FOCS 2021 (meeting in February 2022) manages to improve both the numerator and the denominator: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B8%7D%7B35%7D+%3D+0.229...+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{8}{35} = 0.229... " class="latex" /></p>
<p>That’s double POPL, and over 20%. But it is still single-digits on a fairly large committee. </p>
<p>
</p><p></p><h2> Planting a Field </h2><p></p>
<p></p><p>
We’ve led off this post with pioneers of generations back. What about 20–30 years back? That seems to be the time frame alluded to in this statement by Valerie King—whom both of us have known since the late 1980s—in the first-year <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1834336&amp;HistoricalAwards=false">report</a> to NSF of the inaugural <a href="https://sigact.org/tcswomen/">TCS Women meeting</a> in 2018:</p>
<blockquote><p><b> </b> <em> “I chaired the STOC program committee last year [<a href="http://acm-stoc.org/stoc2017/">2017</a>], and while there were plenty of women on the committee, what a disappointment it was [in 2017] to see rooms full of men with only a sprinkling of woman in sight. I remember a time when there were more women at FOCS and STOC and It’s hard not to wonder what’s happened, and to want to fix this problem. I think it can lead to a downward spiral, the fewer the women, the more women who do come fill uncomfortable and the fewer that come next time. </em></p><em>
</em><p><em>
Thanks to efforts of Barna [Saha], Sofya [Raskhodnikova] and Virginia [Vassilevska Williams], STOC 2018 seemed different. As I looked around the conference rooms, I actually saw women and it felt good.” </em>
</p></blockquote>
<p></p><p>
So we wondered if we could take a snapshot of POPL 20 years ago. I (Ken writing this part) counted out the <a href="https://www.cs.cmu.edu/~mleone/language-people.html">“Language People”</a> list that is linked from Wikipedia’s own shorter <a href="https://en.wikipedia.org/wiki/List_of_programming_language_researchers">list</a> of programming language researchers. The longer, former list was compiled circa 2000 by Mark Leone, whom I knew as a student in a theory course I taught once in Cornell’s summer session, and who now works at NVIDIA. </p>
<p>
I count <b>319</b> names. I noticed Susan Horwitz, for whom we wrote a <a href="https://rjlipton.wpcomstaging.com/2014/12/02/susan-horwitz-1955-2014/">memorial</a> in 2014. I did not check whether others are still living, as I viewed this as a generation-ago snapshot. The list is not perfect—it <a href="https://en.wikipedia.org/wiki/CLU_(programming_language)">CLU</a>-lessly omits Liskov—but it serves the purpose. I included Horwitz in my quick count of women: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++17.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  17. " class="latex" /></p>
<p>That makes a ratio of <img src="https://s0.wp.com/latex.php?latex=%7B17%2F319+%3D+0.05329...%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{17/319 = 0.05329...}" class="latex" /> Just barely over 5%. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Even with Ken’s little experiment, I cannot understand how the low ratio could be possible with POPL. Here are the <a href="https://sigplan.org/Resources/Policies/Diversity/">goals</a> for POPL. Gender equity is in the list, but is seventh. Hmmm. </p>
<p></p><p><br />
[Fixed count of POPL committee; “wide-use”-&gt;”widest-use” in intro, FOCS 2022 8/36 –&gt; FOCS 2021 8/35]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/"><span class="datestr">at November 13, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2727537320196855723">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html">20 Years of Algorithmic Game Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Twenty years ago DIMACS hosted a <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/program.html">Workshop on Computational Issues in Game Theory and Mechanism Design</a>. This wasn't the very beginning of algorithmic game theory, but it was quite the coming out party. From the <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/announcement.html">announcement</a></p><p></p><blockquote><p>The research agenda of computer science is undergoing significant changes due to the influence of the Internet. Together with the emergence of a host of new computational issues in mathematical economics, as well as electronic commerce, a new research agenda appears to be emerging. This area of research is collectively labeled under various titles, such as "Foundations of Electronic Commerce", Computational Economics", or "Economic Mechanisms in Computation" and deals with various issues involving the interplay between computation, game-theory and economics.</p><p>This workshop is intended to not only summarize progress in this area and attempt to define future directions for it, but also to help the interested but uninitiated, of which there seem many, understand the language, the basis principles and the major issues.</p><p></p></blockquote><p>Working at the nearby NEC Research Institute at the time I attended as one of those "interested but unititated."</p><p>The workshop had talks from the current and rising stars in the field in both the theoretical computer science, AI and economics communities. The presentations included some classic early results including <a href="https://doi.org/10.1016/S0304-3975(03)00391-8">Competitive Analysis of Incentive Compatible Online Auctions</a>, <a href="https://doi.org/10.1145/506147.506153">How Bad is Selfish Routing?</a> and the seminal work on <a href="https://doi.org/10.1016/j.geb.2006.02.003">Competitive Auctions</a>. </p><p>Beyond the talks, just having the powerhouse of people at the meeting, established players, like Noam Nisan, Vijay Vazirani, Eva Tardos and Christos Papadimitriou, with several newcomers who are now the established players including Tim Roughgarden and Jason Hartline just to mention a few from theoretical computer science. </p><p>The highlight was a panel discussion on how to overcome the methodological differences between computer scientists and economic game theorists. The panelists were an all-star collection of  John Nash, Andrew Odlyzko, Christos Papadimitriou, Mark Satterthwaite, Scott Shenker and Michael Wellman. The discussion focused on things like competitive analysis though to me, in hindsight, the real difference is between the focus on models (game theory) vs theorems (CS). </p><div>Interest in these connections exploded after the workshop and a new field blossomed.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html"><span class="datestr">at November 11, 2021 12:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/">Online Optimization Post 7: Matrix Multiplicative Weights Update</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first two posts were about the technique of <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Multiplicative Weights Updates</a> and its application to <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">“derandomizing” probabilistic arguments</a> based on combining a Chernoff bound and a union bound. The third and fourth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Follow-the-Regularized-Leader</a> framework, which unifies multiplicative weights and gradient descent, and a <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">“gradient descent view” of the Frieze-Kannan Weak Regularity Lemma</a>. The fifth and sixth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">constrained version of the Follow-the-Regularized-Leader</a> framework, and the <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Impagliazzo Hard-Core Set Lemma</a>. Today we shall see the technique of Matrix Multiplicative Weights Updates.</p>
<p><b>1. Matrix Multiplicative Weights Update </b></p>
<p>In this post we consider the following generalization, introduced and studied by <a href="https://dl.acm.org/doi/10.1145/1250790.1250823">Arora and Kale</a>, of the “learning from expert advice” setting and the multiplicative weights update method. In the “experts” model, we have a repeated game in which, at each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, we have the option of following the advice of one of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> experts; if we follow the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, we incur a loss of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_t (i)}" class="latex" />, which is unknown to us (although, at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> we know the loss functions <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%28%5Ccdot%29%2C%5Cldots%2C%5Cell_%7Bt-1%7D%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_1(\cdot),\ldots,\ell_{t-1}(\cdot)}" class="latex" />). We are allowed to choose a probabilistic strategy, whereby we follow the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_t(i)}" class="latex" />, so that our expected loss at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_{i=1}^n x_t(i) \ell_t(i)}" class="latex" />.</p>
<p>In the matrix version, instead of choosing an expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> we are allowed to choose a unit <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-dimensional vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t}" class="latex" />, and the loss incurred in choosing the vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bv_t+%5ET+L_t+v_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t ^T L_t v_t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> is an unknown symmetric <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n\times n}" class="latex" /> matrix. We are also allowed to choose a probabilistic strategy, so that with probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_t(j)}" class="latex" /> we choose the unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t^{(j)}}" class="latex" />, and we incur the expected loss</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_j+x_t+%28j%29+%5Ccdot+%28v_t%5E%7B%28j%29%7D%29%5ET+L_t+v_t%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_j x_t (j) \cdot (v_t^{(j)})^T L_t v_t^{(j)} " class="latex" /></p>
<p><span id="more-4588"></span></p>
<p>The above expression can also be written as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_t+%5Cbullet+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_t \bullet L_t " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7BX_t+%3D+%5Csum_j+x_t%28j%29+v_t%5E%7B%28j%29%7D%28v_t%5E%7B%28j%29%7D%29%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t = \sum_j x_t(j) v_t^{(j)}(v_t^{(j)})^T}" class="latex" /> and we used the Frobenius inner product among square matrices defined as <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A \bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" />. The matrices <img src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t}" class="latex" /> that can be obtained as convex combinations of rank-1 matrices of the form <img src="https://s0.wp.com/latex.php?latex=%7Bvv%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{vv^T}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> is a unit vector are called <em>density matrices</em> and can be characterized as the set of positive semidefinite matrices whose trace is 1.</p>
<p>It is possible to see the above game as the “quantum version” of the experts settings. A choice of a unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t^{(j)}}" class="latex" /> is a <em>pure quantum state</em>, a probability distribution of pure quantum states, described by a density matrix, is a <em>mixed quantum state</em>. If <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is a density matrix describing a mixed quantum state, <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> is a symmetric matrix, and <img src="https://s0.wp.com/latex.php?latex=%7BL+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L = \sum_i \lambda_i w_i w_i^T}" class="latex" /> is the spectral decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> in terms of its eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> and orthonormal eigenvectors <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Cbullet+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \bullet L}" class="latex" /> is the expected outcome of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> in the basis <img src="https://s0.wp.com/latex.php?latex=%7Bw_1%2C%5Cldots%2Cw_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_1,\ldots,w_n}" class="latex" />, and such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> is the value of the measurement if the outcome is <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" />.</p>
<p>If you have no idea what the above paragraph means, that is perfectly ok because this view will not be particularly helpful in motivating the algorithm and analysis that we will describe. (Here I am reminded of the joke about the way people from Naples give directions: “How do I get to the post office?”, “Well, you see that road over there? After the a couple of blocks there is a pharmacy, where my uncle used to work, though now he is retired.” “Ok?” “Now, if you turn left after the pharmacy, after a while you get to a square with a big fountain and the church of St. Anthony where my niece got married. It was a beautiful ceremony, but the food at the reception was not great.” “Yes, I know that square”, “Good, don’t go there, the post office is not that way. Now, if you instead take that other road over there …”)</p>
<p>The main point of the above game, and of the Matrix Multiplicative Weights Update (MMWU) algorithm that plays it with bounded regret, is that it provides useful generalizations of the standard “experts” game and of the Multiplicative Weights Update (MWU) algorithm. For example, as we have already seen, MWU can provide a “derandomization” of the Chernoff bound; we will see that MMWU provides a derandomization of the <em>matrix</em> Chernoff bound. MWU can be used to approximate certain Linear Programming problems; MMWU can be used to approximate certain <em>Semidefinite Programming</em> problems.</p>
<p>To define and analyze the MMWU algorithm, we need to introduce certain operations on matrices. We will always work with real-valued symmetric matrices, but everything generalizes to complex-valued Hermitian matrices. If <img src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M = \sum_i \lambda_i w_i w_i^T}" class="latex" /> is a symmetric matrix, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> are the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" /> are corresponding orthonormal eigenvectors, then we will define a number of operations and functions on <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> that operate on the eigenvalues while leaving the eigenvectors unchanged.</p>
<p>The first operation is <em>matrix exponentiation</em>: we define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3A%3D+%5Csum_i+e%5E%7B%5Clambda_i%7D+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X := \sum_i e^{\lambda_i} w_i w_i^T " class="latex" /></p>
<p> The operation always defines a positive definite matrix, and the resulting matrix satisfies a “Taylor expansion”</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3D+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cfrac1+%7Bk%21%7D+X%5Ek+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X = \sum_{k=0}^\infty \frac1 {k!} X^k " class="latex" /></p>
<p> Indeed, it is more common to use the above expansion as the definition of the matrix exponential, and then derive the expression in terms of eigenvalues.</p>
<p>We also have the useful bounds</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Csucceq+I+%2B+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X \succeq I + X" class="latex" /></p>
<p> which is true for every <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Cpreceq+I+%2B+X+%2BX%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X \preceq I + X +X^2 " class="latex" /></p>
<p> which is true for all <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \preceq I}" class="latex" />.</p>
<p>Analogously, if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is positive definite, we can define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+X+%3A%3D+%5Csum_i+%28%5Clog+%5Clambda_i%29+%5Ccdot+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \log X := \sum_i (\log \lambda_i) \cdot w_i w_i^T " class="latex" /></p>
<p>and we have a number of identities like <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+e%5EX+%3D+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log e^X = X}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+X%5Ek+%3D+k+%5Clog+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log X^k = k \log X}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bk+X%7D+%3D+e%5Ek+%5Ccdot+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^{k X} = e^k \cdot X}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> is a scalar. We should be careful, however, not to take the analogy with real numbers too far: for example, if <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> are two symmetric matrices, in general it is not trues that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7BA%2BB%7D+%3D+e%5EA+%5Ccdot+e%5EB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^{A+B} = e^A \cdot e^B}" class="latex" />, in fact the above expression is actually always false except when <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> commute, in which case it is trivially true. We have, however, the following extremely useful fact.</p>
<blockquote><p><b>Theorem 1 (Golden-Thompson Inequality)</b> <em> </em></p>
<p><em></em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7BA%2BB%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5EA+%5Ccdot+e%5EB%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  {\rm tr}(e^{A+B}) \leq {\rm tr}(e^A \cdot e^B) " class="latex" /></p>
</em><p><em></em><em> </em></p></blockquote>
<p>The Golden-Thompson inequality will be all we need to generalize to this matrix setting everything we have proved about multiplicative weights. See <a href="https://terrytao.wordpress.com/2010/07/15/the-golden-thompson-inequality/">this post by Terry Tao</a> for a proof.</p>
<p>The <em>Von Neumann entropy</em> of a density matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Ccdots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_1,\cdots,\lambda_n}" class="latex" /> is defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X%29+%3D+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+1+%7B%5Clambda_i%7D+%3D+-+%7B%5Crm+tr%7D%28X%5Clog+X%29+%3D+-+X+%5Cbullet+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  S(X) = \sum_i \lambda_i \log \frac 1 {\lambda_i} = - {\rm tr}(X\log X) = - X \bullet \log X " class="latex" /></p>
<p> that is, if we view <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" /> as the mixed quantum state in which the pure state <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_i}" class="latex" /> has probability <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S(X)}" class="latex" /> is the entropy of the distribution over the pure states. Again, this is not a particularly helpful point of view, and in fact we will be interested in defining <img src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S(X)}" class="latex" /> not just for density matrices <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> but for arbitrary positive definite matrices, and even positive semidefinite (with the convention that <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Clog+0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 \log 0 = 0}" class="latex" />, which is used also in the standard definition of entropy of a distribution).</p>
<p>We will be interested in using Von Neumann entropy as a regularizer, and hence we will want to know what is its Bregman divergence. Some calculations show that the Bregman divergence of the Von Neumann entropy, which is called the quantum relative entropy, is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X_1%7C%7C+X_2%29+%3D+%7B%5Crm+tr%7D+%28X_1+%5Ccdot+%28+%5Clog+X_1+-+%5Clog+X_2%29%29+%2B+%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  S(X_1|| X_2) = {\rm tr} (X_1 \cdot ( \log X_1 - \log X_2)) + {\rm tr}(X_2) - {\rm tr}(X_1) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+X_1+%5Cbullet%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+I+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = X_1 \bullet(\log X_1 - \log X_2) + I \bullet (X_2 - X_1) " class="latex" /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7BX_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_2}" class="latex" /> are density matrices, the terms <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\rm tr}(X_2) - {\rm tr}(X_1)}" class="latex" /> cancel out; the above definition is valid for arbitrary positive definite matrices.</p>
<p>We will have to study the minima of various functions that take a matrix as an input, so it is good to understand how to compute the gradient of such functions. For example what is the gradient of the function <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\rm tr}(X)}" class="latex" />? Working through the definition we see that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+%7B%5Crm+tr%7D%28X%29+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\nabla {\rm tr}(X) = I}" class="latex" />, and indeed we always have that the gradient of the function <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+A%5Cbullet+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \rightarrow A\bullet X}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> everywhere. Somewhat less obvious is the calculation of the gradient of the Von Neumann entropy, which is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%28X+%5Cbullet+%5Clog+X%29+%3D+I+%2B+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \nabla (X \bullet \log X) = I + \log X " class="latex" /></p>
<p><b>2. Analysis in the Constrained FTRL Framework </b></p>
<p>Suppose that we play that we described above using agile mirror descent and using negative Von Neumann entropy (appropriately scaled) as a regularizer. That is, for some <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> that we will choose later, we use the regularizer</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28X%29+%3D+c+X+%5Cbullet+%5Clog+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  R(X) = c X \bullet \log X" class="latex" /></p>
<p> which has the Bregman divergence</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_1%2CX_2%29+%3D+c+S%28X_1+%7C%7C+X_2%29+%3D+c+X_1+%5Cbullet+%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+cI+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_1,X_2) = c S(X_1 || X_2) = c X_1 \bullet (\log X_1 - \log X_2) + cI \bullet (X_2 - X_1) " class="latex" /></p>
<p> and our feasible set is the set of density matrices</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CDelta+%3A%3D+%5C%7B+X%5Cin+%7B%5Cmathbb+R%7D%5E%7Bn%5Ctimes+n%7D+%3A+X+%5Csucceq+%7B%5Cbf+0%7D+%5Cwedge+%7B%5Crm+tr%7D%28X%29+%3D+1+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Delta := \{ X\in {\mathbb R}^{n\times n} : X \succeq {\bf 0} \wedge {\rm tr}(X) = 1 \} " class="latex" /></p>
<p> To bound the regret, we just have to plug the above definitions into the machinery that we developed <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">in our fifth post</a>.</p>
<p>At time 1, we play the identity matrix scaled by n, which is a density matrix of maximum Von Neumann entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log n}" class="latex" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_1+%3A%3D+%5Carg%5Cmin_%7BX+%5Cin+%5CDelta%7D+R%28X%29+%3D+%5Cfrac+1n+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_1 := \arg\min_{X \in \Delta} R(X) = \frac 1n I " class="latex" /></p>
<p> At time <img src="https://s0.wp.com/latex.php?latex=%7Bt%2B1%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t+1\geq 2}" class="latex" />, we play the matrix <img src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t}" class="latex" /> obtained as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%7D+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \hat X_{t+1} = \arg\min_{X} D(X,X_{t}) + X\bullet L_{t} " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%5Cin+%5CDelta%7D+D%28X%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_{t+1} = \arg\min_{X\in \Delta} D(X,\hat X_{t+1}) " class="latex" /></p>
<p> and recall that we proved that, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> steps,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T%28X%29+%5Cleq+D%28X%2CX_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28X_t%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T(X) \leq D(X,X_1) + \sum_{t=1}^T D(X_t,\hat X_{t+1}) " class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is a density matrix with eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Cldots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_1,\ldots,\lambda_n}" class="latex" />, then the first term is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D+%5Cleft%28+X%2C+%5Cfrac+1n+I+%5Cright%29+%3D+c+X+%5Cbullet+%28%5Clog+X+-+%5Clog+n%5E%7B-1%7D+I%29+%3D+c+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+n%7B%5Clambda_i%7D+%3D+c+%5Clog+n+-+c+%5Csum_i+%5Clambda_i+%5Cfrac+1+%7B%5Clambda_i%7D+%5Cleq+c%5Clog+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D \left( X, \frac 1n I \right) = c X \bullet (\log X - \log n^{-1} I) = c \sum_i \lambda_i \log \frac n{\lambda_i} = c \log n - c \sum_i \lambda_i \frac 1 {\lambda_i} \leq c\log n " class="latex" /></p>
<p> To complete the analysis we have to understand <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\hat X_{t+1}}" class="latex" />. We need to compute the gradient <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \rightarrow D(X,X_{t}) + X\bullet L_{t} }" class="latex" /> and set it to zero. The gradient of <img src="https://s0.wp.com/latex.php?latex=%7BX%5Cbullet+L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X\bullet L_t}" class="latex" /> is just <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" />. The gradient of <img src="https://s0.wp.com/latex.php?latex=%7BD%28X%2CX_%7Bt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D(X,X_{t})}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+D%28X%2CX_t%29+%3D+c+%5Cnabla+X+%5Cbullet+%5Clog+X+-+c+%5Cnabla+X+%5Cbullet+%5Clog+X_t+-+%5Cnabla+c+X+%5Cbullet+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \nabla D(X,X_t) = c \nabla X \bullet \log X - c \nabla X \bullet \log X_t - \nabla c X \bullet I " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+cI+%2B+c+%5Clog+X+-+c+%5Clog+X_t+-+cI+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = cI + c \log X - c \log X_t - cI " class="latex" /></p>
<p> Meaning that we want to solve for</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%5Clog+X+-+c%5Clog+X_t+%2B+L_t+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  c \log X - c\log X_t + L_t = 0 " class="latex" /></p>
<p> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\hat X_{t+1}}" class="latex" /> satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D+%3D+%5Cfrac+1c+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \log X_t - \log \hat X_{t+1} = \frac 1c L_t " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \hat X_{t+1} = e^{\log X_t - \frac 1c L_t } " class="latex" /></p>
<p> and we can write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%3D+c+%5Ccdot+%5Cleft%28+X_t+%5Cbullet+%28%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D%29%5Cright%29+%2B+c+%7B%5Crm+tr%7D%28+%5Chat+X_%7Bt%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_t,\hat X_{t+1} ) = c \cdot \left( X_t \bullet (\log X_t - \log \hat X_{t+1})\right) + c {\rm tr}( \hat X_{t+1} )" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%5Ccdot+X_t+%5Cbullet+%5Cfrac+1c+L_t+%2B+c+%5Ccdot+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%2B+c+%7B%5Crm+tr%7D+%5Chat+X_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = c \cdot X_t \bullet \frac 1c L_t + c \cdot {\rm tr}(e^{\log X_t - \frac 1c L_t}) + c {\rm tr} \hat X_{t+1} " class="latex" /></p>
<p> Then we can use Golden-Thompson and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E-%5Cfrac+1c+L_t+%5Cpreceq+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^-\frac 1c L_t \preceq I - \frac 1c L_t + \frac 1{c^2} L^2_t}" class="latex" />, which holds if <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq cI}" class="latex" />, to write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t%7D+%5Ccdot+e%5E%7B-%5Cfrac+1c+L_t%7D+%29+%3D+X_t+%5Cbullet+e%5E%7B-%5Cfrac+1c+L_t%7D+%5Cleq+X_t+%5Cbullet+%5Cleft%28+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  {\rm tr}(e^{\log X_t - \frac 1c L_t}) \leq {\rm tr}(e^{\log X_t} \cdot e^{-\frac 1c L_t} ) = X_t \bullet e^{-\frac 1c L_t} \leq X_t \bullet \left( I - \frac 1c L_t + \frac 1{c^2} L^2_t \right) " class="latex" /></p>
<p> Combining everything together we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%5Cleq+%5Cfrac+1c+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_t,\hat X_{t+1} ) \leq \frac 1c X_t \bullet L_t^2 " class="latex" /></p>
<p> and so, provided <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_%7B%5Cmax%7D+%28L_t%29+%5Cleq+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_{\max} (L_t) \leq c}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> This is the best bound we can hope for, and it matches Theorem 1 in <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">our first post</a> about the Xultiplicative Weights Update algorithm.</p>
<p>If we have <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq I}" class="latex" />, we can simplify it to</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+T+c+%3D+2+%5Csqrt%7BT+%5Clog+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T \leq c \log n + \frac T c = 2 \sqrt{T \log n} " class="latex" /></p>
<p> where the last step comes from optimizing <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>We can also write, under the condition <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+c+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq c I}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%28X%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+%28X_t+%5Cbullet+%7CL_t%7C+%29%7C%7CL_t%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T (X) \leq c \log n + \frac 1c \sum_{t=1}^T (X_t \bullet |L_t| )||L_t|| " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7B%7CL_t%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|L_t|}" class="latex" /> is the “absolute value” of the matrix <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> defined in the following way: if <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" /> is a symmetric matrix, then its absolute value is <img src="https://s0.wp.com/latex.php?latex=%7B%7CX%7C+%3D+%5Csum_i+%7C%5Clambda_i%7C+%5Ccdot+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|X| = \sum_i |\lambda_i| \cdot v_i v_i^T}" class="latex" />. Allen-Zhu, Liao and Orecchia state the analysis in this way in their <a href="https://arxiv.org/abs/1506.04838">on generalizations of Matrix Multiplicative Weights</a>.</p>
<p>Our next post will discuss applications at length, but for now let us gain a bit of intuition about the usefulness of these regret bounds. Recall that, for every symmetric matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_%7B%5Cmin%7D+%28M%29+%3D+%5Cmin_%7BX+%5Crm%5C+density%5C+matrix%7D+%5C+%5C+X+%5Cbullet+M+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_{\min} (M) = \min_{X \rm\ density\ matrix} \ \ X \bullet M " class="latex" /></p>
<p> and so the regret bound can be reintepreted in the following way: if we let <img src="https://s0.wp.com/latex.php?latex=%7BL_1%2C%5Cldots%2CL_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_1,\ldots,L_T}" class="latex" /> be the loss functions used in a game played against a MMWU algorithm, and the algorithm selects density matrices <img src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cldots%2CX_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_1,\ldots,X_T}" class="latex" />, then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Cmin_%7BX+%5Crm+%5C+density+%5C+matrix%7D+%5C+%5C+X+%5Cbullet+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \min_{X \rm \ density \ matrix} \ \ X \bullet \sum_{t=1}^T L_t \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> that is,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Clambda_%7B%5Cmin%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \lambda_{\min} \left( \sum_{t=1}^T L_t \right) \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> provided that <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq cI}" class="latex" />. For example, switching <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7B-L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-L_t}" class="latex" />, we have <a name="main"></a></p>
<p><a name="main"></a></p><a name="main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_%7B%5Cmax%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+%2B+c%5Clog+n+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_{\max} \left( \sum_{t=1}^T L_t \right) \leq \sum_{t=1}^T X_t \bullet L_t + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 + c\log n \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="main"></a><a name="main"></a> provided that <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Csucceq+-cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \succeq -cI}" class="latex" />, which means that if we can choose a sequence of loss matrices that make the MMWU have small loss at each step, then we are guaranteed that the sum of such matrices cannot have any large eigenvalue.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/"><span class="datestr">at November 10, 2021 12:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/neurips2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/neurips2021/">Conference Digest - NeurIPS 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The accepted papers for <a href="https://neurips.cc/Conferences/2020">NeurIPS 2021</a> were recently announced, and there’s a huge amount of differential privacy content. 
We found one relevant workshop and 48 papers.
This is up from 31 papers last year, an over 50% increase!
It looks like there’s huge growth in interest on differentially private machine learning.
Impressively, at the time of this writing, all but five papers are already posted on arXiv!
For the full list of accepted papers, see <a href="https://neurips.cc/Conferences/2021/AcceptedPapersInitial">here</a>.
Please let us know if we missed relevant papers on differential privacy!</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li><a href="https://priml2021.github.io/">Privacy in Machine Learning (PriML) 2021</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2103.08721">A Central Limit Theorem for Differentially Private Query Answering</a><br />
Jinshuo Dong, Weijie Su, Linjun Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02391">Adapting to function difficulty and growth conditions in private optimization</a><br />
Hilal Asi, Daniel Levy, John Duchi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.04378">Adaptive Machine Unlearning</a><br />
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, Chris Waites</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.13239">An Uncertainty Principle is a Price of Privacy-Preserving Microdata</a><br />
John Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Daniel Kifer, Philip Leclerc, William Sexton, Ashley Simpson, Christine Task, Pavel Zhuravlev</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03408">Antipodes of Label Differential Privacy: PATE and ALIBI</a><br />
Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, Florian Tramer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13329">Covariance-Aware Private Mean Estimation Without Private Covariance Estimation</a><br />
Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06062">Deep Learning with Label Differential Privacy</a><br />
Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.05855">Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent</a><br />
Rishav Chourasia, Jiayuan Ye, Reza Shokri</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02674">Differentially Private Empirical Risk Minimization under the Fairness Lens</a><br />
Cuong Tran, My Dinh, Ferdinando Fioretto</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.14153">Differentially Private Federated Bayesian Optimization with Distributed Exploration</a><br />
Zhongxiang Dai, Bryan Kian Hsiang Low, Patrick Jaillet</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.03871">Differentially Private Learning with Adaptive Clipping</a><br />
Galen Andrew, Om Thakkar, Swaroop Ramaswamy, Brendan McMahan</p>
  </li>
  <li>
    <p>Differentially Private Model Personalization<br />
Prateek Jain, John Rush, Adam Smith, Shuang Song, Abhradeep Guha Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02900">Differentially Private Multi-Armed Bandits in the Shuffle Model</a><br />
Jay Tenenbaum, Haim Kaplan, Yishay Mansour, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02831">Differentially Private n-gram Extraction</a><br />
Kunho Kim, Sivakanth Gopi, Janardhan Kulkarni, Sergey Yekhanin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02516">Differential Privacy Over Riemannian Manifolds</a><br />
Matthew Reimherr, Karthik Bharath, Carlos Soto</p>
  </li>
  <li>
    <p>Differentially Private Sampling from Distributions<br />
Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith, Marika Swanberg</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.05585">Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings</a><br />
Raef Bassily, Cristóbal Guzmán, Michael Menart</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.01177">Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence</a><br />
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09063">Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization</a><br />
Pranav Subramani, Nicholas Vadivelu, Gautam Kamath</p>
  </li>
  <li>
    <p>Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms<br />
Jeremy Seeman, Matthew Reimherr, Aleksandra Slavković</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.03013">Fast and Memory Efficient Differentially Private-SGD via JL Projections</a><br />
Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Hanwen Shen, Uthaipon Tantipongpipat</p>
  </li>
  <li>
    <p>G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators<br />
Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, Bo Li</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03365">Generalized Linear Bandits with Local Differential Privacy</a><br />
Yuxuan Han, Zhipeng Liang, Yang Wang, Jiheng Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11193">Individual Privacy Accounting via a Rényi Filter</a><br />
Vitaly Feldman, Tijana Zrnic</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.00979">Information-constrained optimization: can adaptive processing of gradients help?</a><br />
Jayadev Acharya, Clement Canonne, Prathamesh Mayekar, Himanshu Tyagi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00463">Instance-optimal Mean Estimation Under Differential Privacy</a><br />
Ziyue Huang, Yuting Liang, Ke Yi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.07153">Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods</a><br />
Terrance Liu, Giuseppe Vietri, Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11845">Learning with User-Level Privacy</a><br />
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13513">Littlestone Classes are Privately Online Learnable</a><br />
Noah Golowich, Roi Livni</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.07778">Local Differential Privacy for Regret Minimization in Reinforcement Learning</a><br />
Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, Matteo Pirotta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.03940">Locally differentially private estimation of functionals of discrete distributions</a><br />
Cristina Butucea, Yann Issartel</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.10675">Locally private online change point detection</a><br />
Tom Berrett, Yi Yu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.10870">Multiclass versus Binary Differentially Private PAC Learning</a><br />
Satchit Sivakumar, Mark Bun, Marco Gaboardi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02848">Numerical Composition of Differential Privacy</a><br />
Sivakanth Gopi, Yin Tat Lee, Lukas Wutschitz</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11526">On the Sample Complexity of Privately Learning Axis-Aligned Rectangles</a><br />
Menachem Sadigurschi, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03645">Photonic Differential Privacy with Direct Feedback Alignment</a><br />
Ruben Ohana, Hamlet Medina, Julien Launay, Alessandro Cappelli, Iacopo Poli, Liva Ralaivola, Alain Rakotomamonjy</p>
  </li>
  <li>
    <p>Private and Non-private Uniformity Testing for Ranking Data<br />
Róbert Busa-Fekete, Dimitris Fotakis, Emmanouil Zampetakis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.07171">Private learning implies quantum stability</a><br />
Yihui Quek, Srinivasan Arunachalam, John A Smolin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.15352">Private Non-smooth ERM and SCO in Subquadratic Steps</a><br />
Janardhan Kulkarni, Yin Tat Lee, Daogao Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02162">Privately Learning Mixtures of Axis-Aligned Gaussians</a><br />
Ishaq Aden-Ali, Hassan Ashtiani, Christopher Liaw</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00001">Privately Learning Subspaces</a><br />
Vikrant Singhal, Thomas Steinke</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02281">Privately Publishable Per-instance Privacy</a><br />
Rachel Redberg, Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2109.06153">Relaxed Marginal Consistency for Differentially Private Query Answering</a><br />
Ryan McKenna, Siddhant Pradhan, Daniel Sheldon, Gerome Miklau</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.03279">Remember What You Want to Forget: Algorithms for Machine Unlearning</a><br />
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.08763">Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning</a><br />
Antonious Girgis, Deepesh Data, Suhas Diggavi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.09159">Robust and differentially private mean estimation</a><br />
Xiyang Liu, Weihao Kong, Sham Kakade, Sewoong Oh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.04995">The Skellam Mechanism for Differentially Private Federated Learning</a><br />
Naman Agarwal, Peter Kairouz, Ken Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.11208">User-Level Differentially Private Learning via Correlated Sampling</a><br />
Badih Ghazi, Ravi Kumar, Pasin Manurangsi</p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/neurips2021/"><span class="datestr">at November 09, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/">The Artificial Intelligence Historian</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>The past actually happened but history is only what someone wrote down—Whitney Brown</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/mccorduck-obit-700x700-min/" rel="attachment wp-att-19317"><img width="125" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/mccorduck-obit-700x700-min.jpeg?resize=125%2C156&amp;ssl=1" class="alignright wp-image-19317" height="156" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">CMU <a href="https://www.cmu.edu/news/stories/archives/2021/october/mccorduck-obit.html">tribute</a></font></td>
</tr>
</tbody>
</table>
<p>
Pamela McCorduck passed away last month. The New York Times <a href="https://www.nytimes.com/2021/11/04/technology/pamela-mccorduck-dead.html">obituary</a> notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. </p>
<p>
Today we remember her and talk about AI’s near future.<br />
<span id="more-19306"></span></p>
<p>
I knew McCorduck through her late husband, Joe Traub, who we <a href="https://rjlipton.wpcomstaging.com/2015/08/31/how-joe-traub-beat-the-street/">memorialized</a> in 2015. He became the head of the CS department at Carnegie Mellon University in 1971. She also moved to CMU where she became an English teacher. Per the above quote by Brown, she helped make AI real by writing a number of books on its history. </p>
<p>
The NYT obit quotes something McCorduck wrote in her 2019 <a href="https://press.etc.cmu.edu/index.php/product/this-could-be-important/">memoir</a>, <em>This Could Be Important: My Life and Times With the Artificial Intelligentsia.</em> </p>
<blockquote><p><b> </b> <em> “For 60 years, I’ve lived in AI’s exponential. I’ve watched computers evolve from plodding sorcerer’s apprentices to machines that can best any humans at checkers, then chess, then the guessing game Jeopardy!, and now the deeply complex game of Go.” </em>
</p></blockquote>
<p></p><p>
It is hard to project the future of an exponential, however. The best way I can try is to align it with my own field.</p>
<p>
</p><p></p><h2> AI Movers and Shakers </h2><p></p>
<p></p><p>
At CMU McCorduck got to know the AI pioneers like Turing Award recipients Herbert Simon and Allen Newell and Raj Reddy. She already knew Edward Feigenbaum who said:</p>
<blockquote><p><b> </b> <em> She was dumped into this saturated milieu of the great and greatest in AI at Carnegie Mellon—some of the same people whose papers she’d helped us assemble—and decided to write a history of the field. </em>
</p></blockquote>
<p></p><p>
The <a href="https://www.routledge.com/Machines-Who-Think-A-Personal-Inquiry-into-the-History-and-Prospects-of/McCorduck/p/book/9781568812052">book</a> was <em>Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.</em> Said Simon: </p>
<blockquote><p><b> </b> <em> She was interacting with all the movers and shakers of AI. She was in the middle of it, an eyewitness to history. </em>
</p></blockquote>
<p></p><p>
I wish I knew more of her thoughts on the movers and shakers in Theory. She was well-versed in complexity of the dynamical-systems kind, and her husband’s work bridged to “our kind” of complexity. The title of her third <a href="https://www.amazon.com/Bounded-Rationality-Novel-Pamela-McCorduck/dp/0865348839">novel</a>, <em>Bounded Rationality</em>, speaks to both kinds of complexity from its setting at the Santa Fe Institute. This has led me to musing on the difference between AI and Theory.</p>
<p>
</p><p></p><h2> AI Beats Theory </h2><p></p>
<p></p><p>
I never have worked on AI problems of any kind. The closest I ever came is I took a class at CMU as graduate student from Newell. He was a fun lecturer and the class was interesting. But I always worked on Theory. I must reflect a bit on why AI is so successful and Theory is less so. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/see/" rel="attachment wp-att-19311"><img width="258" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/see.png?resize=258%2C195&amp;ssl=1" class="aligncenter size-full wp-image-19311" height="195" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pinterest <a href="https://www.pinterest.com/pin/55872851604309287/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Let’s start by saying that a field of research is determined not by who works in the field. Not by the tools that the field uses. It is determined by the problems that the field works on. AI is different from Theory because of the problems that it studies. This is the fundamental difference:</p>
<blockquote><p><b> </b> <em> AI looks at whole problems; Theory looks at sub-problems. </em>
</p></blockquote>
<p></p><p>
What do I mean? AI studies problems that are concrete, that are big, that are as close to real problems as possible. For example, how to play Go or how to recognize images of faces. AI looks at problems that humans actually wish to solve: </p>
<blockquote><p><b> </b> <em> What move to make in this Go position? Or is this an image of X or Y? </em>
</p></blockquote>
<p></p><p>
Theory looks at sub-problems. We look at a real problem and then identify some part of the problem that is hard to solve. We then try to invoke clever methods that show that this sub-problem can be done more efficiently that was previous known. This is hard in general. Is fun to work on in general. And leads to a beautiful field of study. One that is deep and rewarding. </p>
<p>
But Theory loses to AI. The issue is that no one really may wish to solve the sub-problem. This is real demand to solve the whole problem, but not the sub-problem. This is the fundamental advantage that AI holds over Theory.</p>
<p>
</p><p></p><h2> AI Future </h2><p></p>
<p></p><p>
Public figures such as Stephen Hawking and Elon Musk have expressed concern that full artificial intelligence (AI) could result in human extinction. The consequences of the technological <a href="https://en.wikipedia.org/wiki/Technological_singularity">singularity</a> and its potential benefit or harm to the human race have been intensely debated.</p>
<p>
For our own part, we have <a href="https://rjlipton.wpcomstaging.com/2011/02/17/are-mathematicians-in-jeopardy/">wondered</a> whether an AI can take over in theory research. This puts a second light on possible meanings of “problems” in another quotation by McCorduck from her memoir, as related <a href="https://computerhistory.org/blog/the-future-humans-and-ai/">here</a>:</p>
<blockquote><p><b> </b> <em> “We can’t now say what living beside other, in some ways superior, intelligences will mean to us. Will it widen and raise our own individual and collective intelligence? In significant ways, it already has. Find solutions to problems we could never solve? Probably. Find solutions to problems we lack the wit even to propose? Maybe. Cause problems? Surely. AI has already shattered some of our fondest myths about ourselves and has shone unwelcome light on others. This will continue.</em></p><em>
<p>
…</p>
</em><p><em>
When people ask me my greatest worry about AI, I say: what we aren’t smart enough even to imagine.” </em>
</p></blockquote>
<p></p><p>
Well, we can only talk about things we can imagine now. We can discuss facets of life that already outsource decisions to technology, such as high-speed stock trading and <a href="https://en.wikipedia.org/wiki/2010_flash_crash">several</a> <a href="https://www.technologyreview.com/2016/10/07/244656/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">flash</a>–<a href="https://www.motherjones.com/politics/2013/02/high-frequency-trading-danger-risk-wall-street/">crashes</a> it has caused. </p>
<p>
But looking ahead, what is one near-term application area as a litmus test for the impact of AI? We think many will agree with our looking to <em>self-driving cars</em>. In taking over driving decisions, the AI expressly aims to reduce the evils of impaired or aggressive drivers. There have been <a href="https://www.washingtonpost.com/technology/2021/11/08/tesla-regulation-elon-musk/">mishaps</a> during development, sure, and the algorithms have not yet demonstrated robustness against possible deceptions. That is to say:</p>
<ul>
<li>
We can already see teething problems with this tech and imagine more along the same lines. <p></p>
</li><li>
But can we project structural problems with the driverless paradigm whose concrete forms we have not imagined?
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your thoughts on the near future of AI? </p>
<p>
Our condolences go out to Pamela’s family and associates.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/"><span class="datestr">at November 08, 2021 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
