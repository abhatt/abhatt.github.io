<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 08, 2022 07:40 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02919">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02919">Branch Decomposition-Independent Edit Distances for Merge Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wetzels:Florian.html">Florian Wetzels</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leitte:Heike.html">Heike Leitte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garth:Christoph.html">Christoph Garth</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02919">PDF</a><br /><b>Abstract: </b>Edit distances between merge trees of scalar fields have many applications in
scientific visualization, such as ensemble analysis, feature tracking or
symmetry detection. In this paper, we propose branch mappings, a novel approach
to the construction of edit mappings for merge trees. Classic edit mappings
match nodes or edges of two trees onto each other, and therefore have to either
rely on branch decompositions of both trees or have to use auxiliary node
properties to determine a matching. In contrast, branch mappings employ branch
properties instead of node similarity information, and are independent of
predetermined branch decompositions. Especially for topological features, which
are typically based on branch properties, this allows a more intuitive distance
measure which is also less susceptible to instabilities from small-scale
perturbations. We describe a quartic runtime algorithm for computing optimal
branch mappings, which is faster than the only other branch
decomposition-independent method in the literature by more than a linear
factor. Furthermore, we compare the results of our method on synthetic and
real-world examples to demonstrate its practicality and utility.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02919"><span class="datestr">at April 07, 2022 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02902">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02902">Efficient Bayesian Network Structure Learning via Parameterized Local Search on Topological Orderings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=uuml=ttemeier:Niels.html">Niels Grüttemeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morawietz:Nils.html">Nils Morawietz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02902">PDF</a><br /><b>Abstract: </b>In Bayesian Network Structure Learning (BNSL), one is given a variable set
and parent scores for each variable and aims to compute a DAG, called Bayesian
network, that maximizes the sum of parent scores, possibly under some
structural constraints. Even very restricted special cases of BNSL are
computationally hard, and, thus, in practice heuristics such as local search
are used. A natural approach for a local search algorithm is a hill climbing
strategy, where one replaces a given BNSL solution by a better solution within
some pre-defined neighborhood as long as this is possible. We study
ordering-based local search, where a solution is described via a topological
ordering of the variables. We show that given such a topological ordering, one
can compute an optimal DAG whose ordering is within inversion distance $r$ in
subexponential FPT time; the parameter $r$ allows to balance between solution
quality and running time of the local search algorithm. This running time bound
can be achieved for BNSL without structural constraints and for all structural
constraints that can be expressed via a sum of weights that are associated with
each parent set. We also introduce a related distance called `window inversions
distance' and show that the corresponding local search problem can also be
solved in subexponential FPT time for the parameter $r$. For two further
natural modification operations on the variable orderings, we show that
algorithms with an FPT time for $r$ are unlikely. We also outline the limits of
ordering-based local search by showing that it cannot be used for common
structural constraints on the moralized graph of the network.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02902"><span class="datestr">at April 07, 2022 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02758">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02758">Computing expected multiplicities for bag-TIDBs with bounded multiplicities</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Su.html">Su Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Glavic:Boris.html">Boris Glavic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huber:Aaron.html">Aaron Huber</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kennedy:Oliver.html">Oliver Kennedy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rudra:Atri.html">Atri Rudra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02758">PDF</a><br /><b>Abstract: </b>In this work, we study the problem of computing a tuple's expected
multiplicity over probabilistic databases with bag semantics (where each tuple
is associated with a multiplicity) exactly and approximately. We consider
bag-TIDBs where we have a bound $c$ on the maximum multiplicity of each tuple
and tuples are independent probabilistic events (we refer to such databases as
c-TIDBs. We are specifically interested in the fine-grained complexity of
computing expected multiplicities and how it compares to the complexity of
deterministic query evaluation algorithms -- if these complexities are
comparable, it opens the door to practical deployment of probabilistic
databases. Unfortunately, our results imply that computing expected
multiplicities for c-TIDBs based on the results produced by such query
evaluation algorithms introduces super-linear overhead (under parameterized
complexity hardness assumptions/conjectures). We proceed to study approximation
of expected result tuple multiplicities for positive relational algebra queries
($RA^+$) over c-TIDBs and for a non-trivial subclass of block-independent
databases (BIDBs). We develop a sampling algorithm that computes a
1$\pm\epsilon$ approximation of the expected multiplicity of an output tuple in
time linear in the runtime of the corresponding deterministic query for any
$RA^+$ query.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02758"><span class="datestr">at April 07, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02668">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02668">Disentangling the Computational Complexity of Network Untangling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Froese:Vincent.html">Vincent Froese</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kunz:Pascal.html">Pascal Kunz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02668">PDF</a><br /><b>Abstract: </b>We study the network untangling problem introduced by Rozenshtein, Tatti, and
Gionis [DMKD 2021], which is a variant of Vertex Cover on temporal graphs --
graphs whose edge set changes over discrete time steps. They introduce two
problem variants. The goal is to select at most $k$ time intervals for each
vertex such that all time-edges are covered and (depending on the problem
variant) either the maximum interval length or the total sum of interval
lengths is minimized. This problem has data mining applications in finding
activity timelines that explain the interactions of entities in complex
networks.
</p>
<p>Both variants of the problem are NP-hard. In this paper, we initiate a
multivariate complexity analysis involving the following parameters: number of
vertices, lifetime of the temporal graph, number of intervals per vertex, and
the interval length bound. For both problem versions, we (almost) completely
settle the parameterized complexity for all combinations of those four
parameters, thereby delineating the border of fixed-parameter tractability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02668"><span class="datestr">at April 07, 2022 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02651">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02651">Scheduling Coflows for Minimizing the Total Weighted Completion Time in Identical Parallel Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chi=Yeh.html">Chi-Yeh Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02651">PDF</a><br /><b>Abstract: </b>Coflow is a recently proposed network abstraction to capture communication
patterns in data centers. The coflow scheduling problem in large data centers
is one of the most important $NP$-hard problems. Previous research on coflow
scheduling focused mainly on the single-switch model. However, with recent
technological developments, this single-core model is no longer sufficient.
This paper considers the coflow scheduling problem in identical parallel
networks. The identical parallel network is an architecture based on multiple
network cores running in parallel. Coflow can be considered as divisible or
indivisible. Different flows in a divisible coflow can be transmitted through
different network cores. Considering the divisible coflow scheduling problem,
we propose a $(6-\frac{2}{m})$-approximation algorithm with arbitrary release
times, and a $(5-\frac{2}{m})$-approximation without release time, where $m$ is
the number of network cores. On the other hand, when coflow is indivisible, we
propose a $(7-\frac{2}{m})$-approximation algorithm with arbitrary release
times, and a $(6-\frac{2}{m})$-approximation without release time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02651"><span class="datestr">at April 07, 2022 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02642">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02642">Unconstrained Proximal Operator: the Optimal Parameter for the Douglas-Rachford Type Primal-Dual Methods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ran:Yifan.html">Yifan Ran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Wei.html">Wei Dai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02642">PDF</a><br /><b>Abstract: </b>In this work, we propose an alternative parametrized form of the proximal
operator, of which the parameter no longer needs to be positive. That is, the
parameter can be a non-zero scalar, a full-rank square matrix, or, more
generally, a bijective bounded linear operator. We demonstrate that the
positivity requirement is essentially due to a quadratic form. We prove several
key characterizations for the new form in a generic way (with an operator
parameter). We establish the optimal choice of parameter for the
Douglas-Rachford type methods by solving a simple unconstrained optimization
problem. The optimality is in the sense that a non-ergodic worst-case
convergence rate bound is minimized. We provide closed-form optimal choices for
scalar and orthogonal matrix parameters under zero initialization.
Additionally, a simple self-contained proof of a sharp linear convergence rate
for a $ (1/L) $-cocoercive fixed-point sequence with $ L\in(0,1) $ is provided
(as a preliminary result).
</p>
<p>To our knowledge, an operator parameter is new. To show its practical use, we
design a dedicated parameter for the 2-by-2 block-structured semidefinite
program (SDP). Such a structured SDP is strongly related to the quadratically
constrained quadratic program (QCQP), and we therefore expect the proposed
parameter to be of great potential use. At last, two well-known applications
are investigated. Numerical results show that the theoretical optimal
parameters are close to the practical optimums, except they are not a priori
knowledge. We then demonstrate that, by exploiting problem model structures,
the theoretical optimums can be well approximated. Such approximations turn out
to work very well, and in some cases almost reach the underlying limits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02642"><span class="datestr">at April 07, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02570">Optimal Sublinear Sampling of Spanning Trees and Determinantal Point Processes via Average-Case Entropic Independence</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anari:Nima.html">Nima Anari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang_P=.html">Yang P. Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vuong:Thuy=Duong.html">Thuy-Duong Vuong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02570">PDF</a><br /><b>Abstract: </b>We design fast algorithms for repeatedly sampling from strongly Rayleigh
distributions, which include random spanning tree distributions and
determinantal point processes. For a graph $G=(V, E)$, we show how to
approximately sample uniformly random spanning trees from $G$ in
$\widetilde{O}(\lvert V\rvert)$ time per sample after an initial
$\widetilde{O}(\lvert E\rvert)$ time preprocessing. For a determinantal point
process on subsets of size $k$ of a ground set of $n$ elements, we show how to
approximately sample in $\widetilde{O}(k^\omega)$ time after an initial
$\widetilde{O}(nk^{\omega-1})$ time preprocessing, where $\omega&lt;2.372864$ is
the matrix multiplication exponent. We even improve the state of the art for
obtaining a single sample from determinantal point processes, from the prior
runtime of $\widetilde{O}(\min\{nk^2, n^\omega\})$ to
$\widetilde{O}(nk^{\omega-1})$.
</p>
<p>In our main technical result, we achieve the optimal limit on domain
sparsification for strongly Rayleigh distributions. In domain sparsification,
sampling from a distribution $\mu$ on $\binom{[n]}{k}$ is reduced to sampling
from related distributions on $\binom{[t]}{k}$ for $t\ll n$. We show that for
strongly Rayleigh distributions, we can can achieve the optimal
$t=\widetilde{O}(k)$. Our reduction involves sampling from $\widetilde{O}(1)$
domain-sparsified distributions, all of which can be produced efficiently
assuming convenient access to approximate overestimates for marginals of $\mu$.
Having access to marginals is analogous to having access to the mean and
covariance of a continuous distribution, or knowing "isotropy" for the
distribution, the key assumption behind the Kannan-Lov\'asz-Simonovits (KLS)
conjecture and optimal samplers based on it. We view our result as a moral
analog of the KLS conjecture and its consequences for sampling, for discrete
strongly Rayleigh measures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02570"><span class="datestr">at April 07, 2022 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02552">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02552">Quantum Approximate Counting for Markov Chains and Application to Collision Counting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gall:Fran=ccedil=ois_Le.html">François Le Gall</a>, Iu-Iong Ng <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02552">PDF</a><br /><b>Abstract: </b>In this paper we show how to generalize the quantum approximate counting
technique developed by Brassard, H{\o}yer and Tapp [ICALP 1998] to a more
general setting: estimating the number of marked states of a Markov chain (a
Markov chain can be seen as a random walk over a graph with weighted edges).
This makes it possible to construct quantum approximate counting algorithms
from quantum search algorithms based on the powerful "quantum walk based
search" framework established by Magniez, Nayak, Roland and Santha [SIAM
Journal on Computing 2011]. As an application, we apply this approach to the
quantum element distinctness algorithm by Ambainis [SIAM Journal on Computing
2007]: for two injective functions over a set of $N$ elements, we obtain a
quantum algorithm that estimates the number $m$ of collisions of the two
functions within relative error $\epsilon$ by making
$\tilde{O}\left(\frac{1}{\epsilon^{25/24}}\big(\frac{N}{\sqrt{m}}\big)^{2/3}\right)$
queries, which gives an improvement over the
$\Theta\big(\frac{1}{\epsilon}\frac{N}{\sqrt{m}}\big)$-query classical
algorithm based on random sampling when $m\ll N$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02552"><span class="datestr">at April 07, 2022 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02537">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02537">Nearly Tight Spectral Sparsification of Directed Hypergraphs by a Simple Iterative Sampling Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Kazusato Oko, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakaue:Shinsaku.html">Shinsaku Sakaue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tanigawa:Shin=ichi.html">Shin-ichi Tanigawa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02537">PDF</a><br /><b>Abstract: </b>Spectral hypergraph sparsification, which is an attempt to extend well-known
spectral graph sparsification to hypergraphs, has been extensively studied over
the past few years. For undirected hypergraphs, Kapralov, Krauthgamer, Tardos,
and Yoshida (2022) have recently obtained an algorithm for constructing an
$\varepsilon$-spectral sparsifier of optimal $O^*(n)$ size, where $O^*$
suppresses the $\varepsilon^{-1}$ and $\log n$ factors, while the optimal
sparsifier size has not been known for directed hypergraphs. In this paper, we
present the first algorithm for constructing an $\varepsilon$-spectral
sparsifier for a directed hypergraph with $O^*(n^2)$ hyperarcs. This improves
the previous bound by Kapralov, Krauthgamer, Tardos, and Yoshida (2021), and it
is optimal up to the $\varepsilon^{-1}$ and $\log n$ factors since there is a
lower bound of $\Omega(n^2)$ even for directed graphs. For general directed
hypergraphs, we show the first non-trivial lower bound of
$\Omega(n^2/\varepsilon)$.
</p>
<p>Our algorithm can be regarded as an extension of the spanner-based graph
sparsification by Koutis and Xu (2016). To exhibit the power of the
spanner-based approach, we also examine a natural extension of Koutis and Xu's
algorithm to undirected hypergraphs. We show that it outputs an
$\varepsilon$-spectral sparsifier of an undirected hypergraph with $O^*(nr^3)$
hyperedges, where $r$ is the maximum size of a hyperedge. Our analysis of the
undirected case is based on that of Bansal, Svensson, and Trevisan (2019), and
the bound matches that of the hypergraph sparsification algorithm by Bansal et
al. We further show that our algorithm inherits advantages of the spanner-based
sparsification in that it is fast, can be implemented in parallel, and can be
converted to be fault-tolerant.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02537"><span class="datestr">at April 07, 2022 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.02519">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.02519">Maintaining Expander Decompositions via Sparse Cuts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hua:Yiding.html">Yiding Hua</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kyng:Rasmus.html">Rasmus Kyng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gutenberg:Maximilian_Probst.html">Maximilian Probst Gutenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Zihang.html">Zihang Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.02519">PDF</a><br /><b>Abstract: </b>In this article, we show that the algorithm of maintaining expander
decompositions in graphs undergoing edge deletions directly by removing sparse
cuts repeatedly can be made efficient.
</p>
<p>Formally, for an $m$-edge undirected graph $G$, we say a cut $(S, \bar{S})$
is $\phi$-sparse if $|E_G(S, \bar{S})| &lt; \phi \cdot \min\{vol_G(S),
vol_G(\bar{S})\}$. A $\phi$-expander decomposition of $G$ is a partition of $V$
into sets $X_1, X_2, \ldots, X_k$ such that each cluster $G[X_i]$ contains no
$\phi$-sparse cut (meaning it is a $\phi$-expander) with $\tilde{O}(\phi m)$
edges crossing between clusters. A natural way to compute a $\phi$-expander
decomposition is to decompose clusters by $\phi$-sparse cuts until no such cut
is contained in any cluster. We show that even in graphs undergoing edge
deletions, a slight relaxation of this meta-algorithm can be implemented
efficiently with amortized update time $m^{o(1)}/\phi^2$.
</p>
<p>Our approach naturally extends to maintaining directed $\phi$-expander
decompositions and $\phi$-expander hierarchies and thus gives a unifying
framework while having simpler proofs than previous state-of-the-art work. In
all settings, our algorithm matches the run-times of previous algorithms up to
subpolynomial factors. Moreover, our algorithm provides stronger guarantees for
$\phi$-expander decompositions, for example, for graphs undergoing edge
deletions, our approach achieves the first sublinear $\phi m^{o(1)}$ recourse
bounds on the number of edges to become crossing between clusters.
</p>
<p>Our techniques also give by far the simplest, deterministic algorithms for
maintaining Strongly-Connected Components (SCCs) in directed graphs undergoing
edge deletions, and for maintaining connectivity in undirected fully-dynamic
graphs, both matching the current state-of-the-art run-times up to
subpolynomial factors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.02519"><span class="datestr">at April 07, 2022 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">Summer visiting researcher position at Boston College, Computer Science, Bento’s Lab (apply by May 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for multiple visiting scholar positions in any of the following areas:</p>
<p>* parallel and distributed algorithms/optimization, convex and non-convex optimization</p>
<p>* machine learning, graphical models, information theory</p>
<p>* network theory and graph matching</p>
<p>* deep learning, robustness in machine learning</p>
<p>Start and end dates flexible. Duration up to 4 months.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/21519">https://academicjobsonline.org/ajo/jobs/21519</a><br />
Email: visiting@jbento.info</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/"><span class="datestr">at April 06, 2022 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/049">TR22-049 |  Non-Adaptive Universal One-Way Hash Functions from Arbitrary One-Way Functions | 

	Noam Mazor, 

	Jiapeng Zhang, 

	Xinyu Mao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Two of the most useful cryptographic primitives that can be constructed from one-way functions are pseudorandom generators (PRGs) and universal one-way hash functions (UOWHFs). The three major efficiency measures of these primitives are: seed length, number of calls to the one-way function, and adaptivity of these calls. Although a long and successful line of research studied these primitives, their optimal efficiency is not yet fully understood: there are gaps between the known upper bound and the known lower bound for black-box constructions.

Interstingly, the first construction of PRGs by H ?astad, Impagliazzo, Levin, and Luby [SICOMP ’99], and the UOWHFs construction by Rompel [STOC ’90] shared a similar structure. Since then, there was an improvement in the efficiency of both constructions: The state of the art construction of PRGs by Haitner, Reingold, and Vadhan [STOC ’10] uses $O(n^4)$ bits of random seed and $O(n^3)$ non-adaptive calls to the one-way function, or alternatively, seed of size $O(n^3)$ with $O(n^3)$ adaptive calls (Vadhan and Zheng [STOC ’12]). Constructing a UOWHF with similar parameters is still an open question. Currently, the best UOWHF construction by Haitner, Holenstein, Reingold, Vadhan, and Wee [Eurocrypt ’10] uses $O(n^{13})$ adaptive calls with a key of size $O(n^5)$.

In this work we give the first non-adaptive construction of UOWHFs from arbitrary one-way functions. Our construction uses $O(n^9)$ calls to the one-way function, and key of length $O(n^{10})$. By the result of Applebaum, Ishai, and Kushilevitz [FOCS ’04], the above implies the existence of UOWHFs in NC0, given the existence of one-way functions in NC1. We also show that the PRG construction of Haitner et al., with small modifications, yields a relaxed notion of UOWHFs. In order to analyze this construction, we introduce the notion of next-bit unreachable entropy, which replaces the next-bit pseudoentropy notion, used in the PRG construction above.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/049"><span class="datestr">at April 06, 2022 06:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1641">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2022/04/news-for-march-2022/">News for March 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This was a relatively sleepy month with only two property testing papers. Do let us know if we missed any. Let us dig in. (<strong><em>EDIT</em></strong>: Two updates.) </p>



<ol><li>I missed two papers. One on the estimation of quantum entropies and the other on algorithms and lower bounds for estimating MST and TSP costs. </li><li>Finally, I forgot to welcome our new editor. <em>Welcome onboard, <a href="https://www.cmi.ac.in/~nithinvarma/">Nithin Varma</a></em>!!</li></ol>



<p></p>



<p><strong>Private High-Dimensional Hypothesis Testing</strong> by  Shyam Narayanan (<a href="https://arxiv.org/abs/2203.01537">arXiv</a>) This paper continues the novel study of distribution testing under the constraints brought forth by differential privacy extending the work of Canonne-Kamath-McMillan-Ullman-Zakynthinou (henceforth CKMUZ, covered in our <a href="https://ptreview.sublinear.info/2019/06/news-for-may-2019/">May 2019 post</a>). In particular, the paper presents algorithms with optimal sample complexity for private identity testing of \(d\)-dimensional Gaussians. In more detail, the paper shows that can be done with a mere \(\widetilde{O}\left(  \frac{d^{1/2}}{\alpha^2} + \frac{  d^{1/3} }{ \alpha^{4/3} \cdot \varepsilon^{2/3}} + \frac{1}{\alpha \cdot \varepsilon} \right)\). Here \(\alpha\) is the proximity parameter and \(\varepsilon\) is the privacy parameter. Combined with a previous result of Acharya-Sun-Zhang, the paper proves that private identity testing of \(d\)-dimensional Gaussians is doable with a sample complexity smaller than that of private identity testing of discrete distributions over a domain of size \(d\) thereby refuting a conjecture of CKMUZ.</p>



<p></p>



<p><strong>Differentially Private All-Pairs Shortest Path Distances: Improved Algorithms and Lower Bounds</strong> by Badih Ghazi, Ravi Kumar, Pasin Manurangsi and Jelani Nelson (<a href="https://arxiv.org/abs/2203.16476">arXiv</a>) Adam Sealfon considered the classic All Pairs Shortest Path Problem (the APSP problem) with privacy considerations in 2016. In the \((\varepsilon, \delta)\)-DP framework, Sealfon presented an algorithm which on input an edge-weighted graph \(G=(V,E,w)\) adds Laplace noise to all edge weights and computes the shortest paths on this noisy graph. The output of the algorithm satisfies that the estimated distance between every pair is within an additive \(\pm O(n \log n/\varepsilon)\) of the actual distance (the absolute value of this parameter is called the <em>accuracy of the algorithm</em>). Moreover, this error is tight up to a logarithmic factor if the algorithm is required to <em>release</em> the shortest paths. The current paper shows you can privately release all the pairwise distances while suffering only a sublinear accuracy if you additionally release the edge weights (in place of releasing the shortest paths). In particular, this paper presents an \(\varepsilon\)-DP algorithm with sublinear \(\widetilde{O}(n^{2/3})\) accuracy.</p>



<p></p>



<p><strong>Quantum algorithms for estimating quantum entropies</strong> by Youle Wang, Benchi Zhao, Xin Wang (<a href="https://arxiv.org/abs/2203.02386">arXiv</a>) So, remember <a href="https://ptreview.sublinear.info/2021/12/news-for-november-2021/">our post from December</a> on sublinear quantum algorithms for estimation of quantum (von Neumann) entropy?  The current paper begins by noting that the research so far (along the lines of the work above) assumes access to a quantum query model for the input state which we do not yet know how to construct efficiently. This paper addresses this issue and gives quantum algorithms to estimate the von Neumann entropy of a  \(n\)-qubit quantum state \(\rho\) by using independent copies of the input state.</p>



<p></p>



<p><strong>Sublinear Algorithms and Lower Bounds for Estimating MST and TSP Cost in General Metrics</strong> by Yu Chen, Sanjeev Khanna, Zihan Tan (<a href="https://arxiv.org/abs/2203.14798">arXiv</a>) As mentioned in the title, this paper studies sublinear algorithms for the metric MST and the metric TSP problem. The paper obtains a wide assortment of results and shows that both these problems admit an \(\alpha\)-approximation algorithm which uses \(O(n/\alpha)\) space. This algorithm assumes that the input is given as a stream of \(n \choose 2\) metric entries. Under this model, the paper also presents an \(\Omega(n/\alpha^2)\) space lower bound. Let me highlight one more result from the paper. In a <a href="https://ptreview.sublinear.info/2020/07/news-for-june-2020/">previous news</a> (from June 2020), we covered a result detailing a better than \(2\)-approximation for the <em>graphic</em> TSP and \((1,2)\) TSP which runs in sublinear time. This paper extends this result and obtains better than \(2\)-approximation for TSP on a relaively richer class of metrics.</p></div>







<p class="date">
by Akash <a href="https://ptreview.sublinear.info/2022/04/news-for-march-2022/"><span class="datestr">at April 05, 2022 08:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19873">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/">Blogs that are Current</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>Just because we can’t find a solution, it doesn’t mean there isn’t one—Andrew Wiles.</i></p>
<p>
Harry Lewis is the Gordon McKay Professor of Computer Science in Harvard’s School of Engineering and Applied Sciences, where he has taught since 1974. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/lewis-harry_200x300_0/" rel="attachment wp-att-19883"><img width="200" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/Lewis.Harry_200x300_0.jpg?resize=200%2C300&amp;ssl=1" class="aligncenter size-full wp-image-19883" height="300" /></a></p>
<p>
</p><p><b> A Current Blog </b></p>
<p></p><p>
A blog is <b>current</b> provided its last post refers to stuff that happen in late 2021 or 2022. Lewis has a current blog and this one is also current. I like a current one—some blogs are very incurrent indeed. </p>
<p>
</p><p><b> List of Current Blogs </b></p>
<p></p><p>
The first group below are ones we have listed before and the second group are those that were found elsewhere. All below are current. Note the symbol <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /> marks the next alphabetic grouping.</p>
<p>
<a href="https://agtb.wordpress.com">Turing Hand</a><br />
 <a href="https://johncarlosbaez.wordpress.com">Baez</a><br />
 <a href="http://harry-lewis.blogspot.com">Harry Lewis</a><br />
 <a href="https://gilkalai.wordpress.com">Gil Kalai</a><br />
 <a href="https://gentzen.wordpress.com">Gentzen</a><br />
 <a href="http://jdh.hamkins.org">Hawkins</a><br />
 <a href="https://inquiryintoinquiry.com">inquiry</a><br />
 <a href="https://lucatrevisan.wordpress.com">Trevisan</a><br />
 <a href="https://cameroncounts.wordpress.com">Peter Cameron’s Blog</a><br />
 <a href="https://priorprobability.com">Prior probability</a><br />
 <a href="https://thehighergeometer.wordpress.com">Geometer</a><br />
 <a href="https://windowsontheory.org">Windows On Theory</a></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://11011110.github.io/blog/">11011110</a><br />
 <a href="https://blogs.ams.org/inclusionexclusion/">AMS blogs</a><br />
 <a href="https://blogs.ams.org/mathgradblog/">AMS Graduate Student Blog</a><br />
 <a href="https://www.siam.org/conferences/cm/conference/pd22">Analysis &amp; PDE Conferences</a><br />
 <a href="http://avzel.blogspot.com">Avzel’s journal</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://gilkalai.wordpress.com">Combinatorics and more</a><br />
 <a href="http://dsp.rice.edu">Compressed sensing resources</a><br />
 <a href="https://blog.computationalcomplexity.org">Computational Complexity</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://sites.math.rutgers.edu/~zeilberg/OPINIONS.html">Dr. Zeilberger</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://equatorialmaths.wordpress.com">Equatorial Mathematics</a><br />
 <a href="https://ramismovassagh.wordpress.com/2022/03/24/ergodic-bridging-math-and-physics/">Ergodic</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://burttotaro.wordpress.com">Geometry Bulletin Board</a><br />
 <a href="https://gshakan.wordpress.com/about-the-blogger/">George Shakan</a><br />
 <a href="https://gerardbesson.wordpress.com">Gerard Besson’s Blog</a><br />
 <a href="http://www.girlsangle.org">Girl’s Angle</a><br />
 <a href="https://gottwurfelt.com">God Plays Dice</a><br />
 <a href="https://rjlipton.wpcomstaging.com">Godel’s Lost Letter and P=NP</a><br />
 <a href="https://dms.umontreal.ca/~andrew/">Andrew Granville</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://robwilson1.wordpress.com">Hidden Assumptions</a><br />
 <a href="https://alanrendall.wordpress.com">Hydrobates</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://igorpak.wordpress.com">Igor Pak’s blog</a><br />
 <a href="https://images.math.cnrs.fr/?lang=fr">Images des mathematiques</a><br />
 <a href="https://lucatrevisan.wordpress.com">In theory</a><br />
 <a href="http://combinatoricsinstitute.blogspot.com">Institute of Combinatorics</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://jbuzzi.wordpress.com">Jerome Buzzi’s Mathematical Ramblings</a><br />
 <a href="http://jdh.hamkins.org">Joel David Hamkins</a><br />
 <a href="https://www.ams.org/publications/journals/journalsframework/jams">Journal of the American Mathematical Society</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://kourovka-notebook.org">Kouroka</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://yetaspblog.wordpress.com">Le Petit Chercheur Illustre</a><br />
 <a href="https://djalil.chafai.net/blog/">Libres pensees d’un mathematicien ordinaire</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.math3ma.com">Math3ma</a><br />
 <a href="https://mathbabe.org">Mathbabe</a><br />
 <a href="https://mathblogging.org">Mathblogging</a><br />
 <a href="http://notable.math.ucdavis.edu/wiki/Mathematics_Jobs_Wiki">Mathematics Jobs Wiki</a><br />
 <a href="https://mattbaker.blog">Matt Baker’s Math Blog</a><br />
 <a href="http://muchadoaboutnothinggg.blogspot.com">Much ado about nothing</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/HomePage">nLab</a><br />
 <a href="http://www.math.columbia.edu/~woit/wordpress/">Not Even Wrong</a><br />
 <a href="http://www.numbertheory.org/ntw/number_theory.html">Number theory web</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://sites.google.com/view/o-a-r-s">Online Analysis Research Seminar</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="http://www.quantumfrontiers.com">Pengfei Zhang’s blog</a><br />
 <a href="https://www.galoisrepresentations.com">Persiflage</a><br />
 <a href="https://cameroncounts.wordpress.com">Peter Cameron’s Blog</a><br />
 <a href="https://philippelefloch.org">Phillipe LeFloch’s blog</a><br />
 <a href="http://processalgebra.blogspot.com">Process Algebra</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://reasonabledeviations.com">Reasonable Deviations</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.gregegan.net/SCIENCE/Science.html">Science Notes by Greg Egan</a><br />
 <a href="https://gowers.wordpress.com/2013/06/16/the-selected-papers-network/">Selected Papers Network</a><br />
 <a href="https://scottaaronson.blog">Shtetl-Optimized</a><br />
 <a href="https://georgerrmartin.com/notablog/">Since it is not…</a><br />
 <a href="https://sketchesoftopology.wordpress.com">Sketches of topology</a><br />
 <a href="https://nttoan81.wordpress.com">Snapshots in Mathematics!</a><br />
 <a href="https://www.math.columbia.edu/~dejong/wordpress/">Stacks Project Blog</a><br />
 <a href="https://symomega.wordpress.com">SymOmega</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://blog.tanyakhovanova.com">Tanya Khovanova’s Math Blog</a><br />
 <a href="https://terrytao.wordpress.com">Terry Tao</a><br />
 <a href="https://www.flickr.com/photos/gforsythe/6800089347">The Cost of Knowledge</a><br />
 <a href="http://matroidunion.org">The Matroid Union</a><br />
 <a href="https://golem.ph.utexas.edu/category/">The n-Category Cafe</a><br />
 <a href="https://golem.ph.utexas.edu/category/">The n-geometry cafe</a></p>
<p> <a href="https://thuses.com">Thuses</a><br />
 <a href="https://gowers.wordpress.com/about/">Tim Gowers’ blog</a><br />
 <a href="https://gowers.wordpress.com">Tim Gowers’ mathematical discussions</a><br />
 <a href="https://www.theoremoftheday.org">Theorem of Day</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.math.uh.edu/~climenha/math.html">Vaughn Climenhaga</a></p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Can we add more posts that are current? Do we also include some that are not current?</p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/"><span class="datestr">at April 05, 2022 08:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8659181144938614989">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/masks.html">Masks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Illinois Tech removed the last of their mandatory masking restrictions yesterday. Chicago had zero Covid deaths. Yet I still get messages like this in my twitter feed.</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I don't care who you are. If I see you without a mask indoors when infections are rampant and there are many still at risk, I will see you as ignorant of science, narcissistic, and uncaring of the vulnerable around you. I will truly see you unmasked for what you are.</p>— Bill Comeau 🇨🇦🇺🇦 (@Billius27) <a href="https://twitter.com/Billius27/status/1510714029072912390?ref_src=twsrc%5Etfw">April 3, 2022</a></blockquote> 
<p></p><p>The science is unequivocal for vaccines, which do a good job preventing infection and a strong job saving lives. I just got my second booster on Sunday.</p><p>Masks give you some protection but nothing like the vaccines. It's impossible to completely remove the risk of Covid so people need to make their own choices and tradeoffs. If you are vaccinated your chance of serious illness is tiny, whether or not your wear a mask. And mask wearing is not cost-free.</p><p>I just don't like wearing masks. Wearing a mask bends my ears and is mildly painful. People can't always understand me when I talk through a mask, and they can't read my facial expressions. People and computers don't recognize me in a mask. Masks fog up my glasses. I can't exercise with a mask, it gets wet with sweat and hard to breath. You can't eat or drink wearing a mask.</p><p>Now everyone has their own tolerance and I respect that. I'll wear a mask if someone asks nicely or if it is required, like on public transit and many theaters. If I have a meeting with someone wearing a mask, I'll ask if they would like me to put mine on. In most cases they remove theirs.</p><p>On the other hand, the Chicago Symphony concert I planned to attend tonight was cancelled because the conductor, Riccardo Muti, <a href="https://chicago.suntimes.com/2022/4/4/23010828/riccardo-muti-tests-positive-for-covid-chicago-symphony-orchestra-concert-canceled">tested positive for Covid</a> (with minor symptoms). For my own selfish reasons, I wish he had worn a mask.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg"><img width="320" src="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg" border="0" height="213" /></a></div><br /><p><br /></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/masks.html"><span class="datestr">at April 05, 2022 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/">Asynchronous Agreement Part 5: Binary Byzantine Agreement from a strong common coin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post we show how to use Binding Crusader Agreement from the previous post, along with a strong common coin to get a simple and efficient Binary Byzantine Agreement with only an expected $O(n^2)$ message complexity. In the four previous posts we: (1) defined the problem and discussed the...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/"><span class="datestr">at April 05, 2022 12:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/">Asynchronous Agreement Part 4: Crusader Agreement and Binding Crusader Agreement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post we introduce a key building block in the Byzantine Model called Binding Crusader Agreement. We show how to use it in the next post. In the three previous posts we (1) defined the problem and discussed the FLP theorem; (2) presented Ben-Or’s protocol for crash failures; and...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/"><span class="datestr">at April 05, 2022 10:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=806">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2022/04/05/lichess/">Lichess</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>After my <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">previous post, </a>a reader (who amazingly has a nearly identical playing routine) pointed me to it.  It’s great!  Much better than chess.com.</p>



<p>“<em>No ads, no subscriptions; but open-source and passion.</em>“</p>



<p>“<em>Are some features reserved to Patrons? No, because Lichess is entirely free, forever, and for everyone. That’s a promise.</em>“</p>



<p>It doesn’t get better than that, in this world; and that’s why I love computers and the community surrounding them.</p>



<p>Another good news is that I got tired of 5 | 5!  The games are just too slow, and my chess isn’t even worth this much of my time.  I am now playing 3 | 2.  It’s fun the adrenaline kick I get with every match.  Also I got much stricter with my routine, and I don’t lose more than once a day.</p>



<p>Here’s my <a href="https://lichess.org/@/EAmaiLeNuvole">profile</a>.  It’s an anagram of my unpronounceable name which can be loosely translated as <em>…and I loved the clouds.</em></p>



<p><strong>IMPORTANT</strong>: Excessive activity and blunders merely reflect my generous sharing of credentials.</p>



<p>And then I thought, why don’t we automatically analyze existing games to find interesting situations, and present them to the user as a puzzle?  Naturally, the beauty — and dismay — of living in the AI (after-internet) era is that… <a href="https://chesstempo.com/chess-tactics/">it’s already done</a>.</p>



<p>OK, time for my daily fix.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2022/04/05/lichess/"><span class="datestr">at April 05, 2022 10:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/048">TR22-048 |  On the Range Avoidance Problem for Circuits | 

	Hanlin Ren, 

	Rahul Santhanam, 

	Zhikun Wang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the range avoidance problem (called Avoid): given the description of a circuit $C:\{0, 1\}^n \to \{0, 1\}^\ell$ (where $\ell &gt; n$), find a string $y\in\{0, 1\}^\ell$ that is not in the range of $C$. This problem is complete for the class APEPP that corresponds to explicit constructions of objects whose existence follows from the probabilistic method (Korten, FOCS 2021). 
    
Motivated by applications in explicit constructions and complexity theory, we initiate the study of the range avoidance problem for weak circuit classes, and obtain the following results:
    
    
1. Generalising Williams's connections between circuit-analysis algorithms and circuit lower bounds (J. ACM 2014), we present a framework for solving C-Avoid in $FP^{NP}$ using circuit-analysis data structures for C, for "typical" multi-output circuit classes C. As an application, we present a non-trivial $FP^{NP}$ range avoidance algorithm for De Morgan formulas.
An important technical ingredient is a construction of rectangular PCPs of proximity, building on the rectangular PCPs by Bhangale, Harsha, Paradise, and Tal (FOCS 2020).
        
2. Using the above framework, we show that circuit lower bounds for $E^{NP}$ are equivalent to circuit-analysis algorithms with $E^{NP}$ preprocessing. This is the first equivalence result regarding circuit lower bounds for $E^{NP}$. Our equivalences have the additional advantages that they work in both infinitely-often and almost-everywhere settings, and that they also hold for larger (e.g., subexponential) size bounds.

3. Complementing the above results, we show that in some settings, solving C-Avoid would imply breakthrough lower bounds, even for very weak circuit classes C. In particular, an algorithm for $AC^0$-Avoid with polynomial stretch (i.e., $\ell = poly(n)$) implies lower bounds against $NC^1$, and an algorithm for $NC^0_4$-Avoid with very small stretch (i.e., $\ell = n + n^{o(1)}$) implies lower bounds against $NC^1$ and branching programs.

4. We show that Avoid is in FNP if and only if there is a propositional proof system that breaks every non-uniform proof complexity generator. This result connects the study of range avoidance with fundamental questions in proof complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/048"><span class="datestr">at April 04, 2022 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/047">TR22-047 |  Linear Hashing with $\ell_\infty$ guarantees and two-sided Kakeya bounds | 

	Manik Dhar, 

	Zeev Dvir</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that a randomly chosen linear map over a finite field gives a good hash function in the $\ell_\infty$ sense.  More concretely, consider a set $S \subset \mathbb{F}_q^n$ and a randomly chosen linear map $L :  \mathbb{F}_q^n \to  \mathbb{F}_q^t$ with $q^t$ taken to be sufficiently smaller than $|S|$. Let $U_S$ denote a random variable distributed uniformly on $S$. Our main theorem shows that, with high probability over the choice of $L$, the random variable  $L(U_S)$ is close to uniform in the $\ell_\infty$ norm.  In other words, every element in the range $\mathbb{F}_q^t$ has about the same number of elements in $S$ mapped to it. This complements the widely-used Leftover Hash Lemma (LHL) which proves the analog statement under the statistical, or $\ell_1$, distance  (for a richer class of functions) as well as prior work on the expected largest 'bucket size' in linear hash functions  [ADMPT99].  Our proof leverages a connection between linear hashing and the finite field Kakeya problem and extends some of the tools developed in this area, in particular the polynomial method.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/047"><span class="datestr">at April 04, 2022 07:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/046">TR22-046 |  Automating OBDD proofs is NP-hard | 

	Artur Riazanov, 

	Dmitry Itsykson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that the proof system OBDD(and, weakening) is not automatable unless P = NP. The proof is based upon the celebrated result of Atserias and Muller [FOCS 2019] about the hardness of automatability for resolution. The heart of the proof is lifting with a multi-output indexing gadget from resolution block-width to dag-like multiparty number-in-hand communication protocol size with $o(n)$ parties, where $n$ is the number of variables in the non-lifted formula. A similar lifting theorem for protocols with $n + 1$ participants was proved by Göös et. el. [STOC 2020] to establish the hardness of automatability result for Cutting Planes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/046"><span class="datestr">at April 04, 2022 03:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/045">TR22-045 |  Relaxed Locally Decodable and Correctable Codes: Beyond Tensoring | 

	Gil Cohen, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In their highly influential paper, Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004) introduced the notion of a relaxed locally decodable code (RLDC). Similarly to a locally decodable code (Katz-Trevisan; STOC 2000), the former admits access to any desired message symbol with only a few queries to a possibly corrupted codeword. An RLDC, however, is allowed to abort when identifying corruption. The natural analog to locally correctable codes, dubbed relaxed locally correctable codes (RLCC), was introduced by Gur, Ramnarayan and Rothblum (ITCS 2018) who constructed asymptotically-good length-$n$ RLCC and RLDC with $(\log{n})^{O(\log\log{n})}$ queries.

In this work we construct asymptotically-good RLDC and RLCC with an improved query complexity of $(\log{n})^{O(\log\log\log{n})}$. To achieve this, we devise a mechanism--an alternative to the tensor product--that squares the length of a given code. Compared to the tensor product that was used by Gur et al. and by many other constructions, our mechanism is significantly more efficient in terms of rate deterioration, allowing us to obtain our improved construction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/045"><span class="datestr">at April 04, 2022 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6837">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/information-theory-with-kernel-methods/">Information theory with kernel methods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In <a href="https://francisbach.com/von-neumann-entropy/">last month blog post</a>, I presented the von Neumann entropy. It is defined as a spectral function on positive semi-definite (PSD) matrices, and leads to a Bregman divergence called the <a href="https://en.wikipedia.org/wiki/Quantum_relative_entropy">von Neumann relative entropy</a> (or matrix Kullback Leibler divergence), with interesting convexity properties and applications in optimization (mirror descent, or smoothing) and probability (concentration inequalities for matrices). </p>



<p class="justify-text">This month, I will show how the relative entropy relates to usual notions of information theory when applied to specific covariance matrices or covariance operators, with potential applications to all areas where entropy plays a role, and with a simple use of quantum information theory. This post is based on a recent preprint: see all details in [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>], as well as the conclusion section below for several avenues for future work.</p>



<h2>Von Neumann relative entropy between covariance matrices</h2>



<p class="justify-text">We consider some set \(\mathcal{X}\), with a probability distribution \(p\). We assume given a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\), from \(\mathcal{X}\) to a \(d\)-dimensional vector space. This naturally defines a (non-centered) <em>covariance matri</em>x $$ \Sigma_{p} = \int_{\mathcal{X}} \varphi(x) \varphi(x)^\top dp(x), $$ which is a \(d \times d\) symmetric PSD matrix. We will assume throughout this post that the set \(\mathcal{X}\) is compact and that the Euclidean norm of \(\varphi\) is bounded on \(\mathcal{X}\).</p>



<p class="justify-text">Given two distributions \(p\) and \(q\), and their associated covariance matrices \(\Sigma_p\) and \(\Sigma_q\), we can compute their von Neumann relative entropy as: $$ D( \Sigma_p \| \Sigma_q) = {\rm tr} \big[ \Sigma_p ( \log \Sigma_p \, – \log \Sigma_q) \big] – {\rm tr}\big[  \Sigma_p \big] +  {\rm tr} \big[ \Sigma_q \big] .$$</p>



<p class="justify-text">We immediately get that \(D(\Sigma_p \| \Sigma_q)\) is always non-negative, and zero if and only if \(\Sigma_p = \Sigma_q\). This property, that makes it a potentially interesting measure of dissimilarity, is shared among all Bregman divergences on covariance matrices, and has been extensively used in data science (see, e.g., [<a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">2</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">3</a>]). What makes the von Neumann entropy special?</p>



<p class="justify-text">One important property is that it is jointly convex in \(p\) and \(q\), but this is also shared by other divergences [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Appendix A.4]. What really makes the von Neumann entropy special is its explicit link with with information theory.</p>



<h2>Finite sets with orthonormal embeddings</h2>



<p class="justify-text">Let’s start with the simplest situation. If \(\mathcal{X}\) is finite with \(d\) elements, and we have an orthonormal embedding, that is, for all \(x,y \in \mathcal{X}\), \(\varphi(x)^\top \varphi(y) = 1_{x = y}\), then, after a transformation by a rotation matrix, we can assume that each \(\varphi(x)\) is one element of the canonical basis, and all covariance matrices are diagonal (without the rotation, we would have that all covariance matrices are jointly diagonalizable), and for probability distributions \(p\) and \(q\), using the same notation for their probability mass functions, we have: $$D( \Sigma_p \| \Sigma_q)  = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = D(p \|q ), $$ which is exactly the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leilbler</a> (KL) divergence between the two distributions \(p\) and \(q\) [4].</p>



<p class="justify-text">Can we push this further beyond finite sets and finite-dimensional feature maps, and obtain a link with classical notions from information theory?</p>



<p class="justify-text">A first remark is the invariance by rotation within the feature space (that is, if we replace \(\varphi(x)\) by \(R \varphi(x)\), where \(R\) is an \(d \times d\) orthogonal matrix, the von Neumann entropy does not change), so that it depends only on the dot-product \(k(x,y) = \varphi(x)^\top \varphi(y)\), which is the traditional kernel function. As will see, this opens us the entire area of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel machines</a>.</p>



<p class="justify-text">A second remark is that we can go infinite-dimensional, and replace \(\mathbb{R}^d\) by any Hilbert space. This complicates a bit the exposition, and I refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for more formal definitions. In this post, I will shamelessly go infinite-dimensional by using matrices with infinite dimensions when needed.</p>



<h2>Lower-bound on Shannon relative entropy</h2>



<p class="justify-text">We first upper-bound \(D(\Sigma_p \| \Sigma_q)\) by joint convexity of the von Neumann relative entropy (see <a href="https://francisbach.com/von-neumann-entropy/">last post</a>) and Jensen’s inequality: $$ D(\Sigma_p \| \Sigma_q) =  D \Big( \int_{\mathcal{X}}\varphi(x)  \varphi(x)^\top dp(x) \Big\| \int_{\mathcal{X}} \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top dp(x) \Big)$$ $$\ \ \,  \leqslant  \int_{\mathcal{X}} D \Big( \varphi(x)  \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \Big) dp(x).$$ The two matrices  \( \varphi(x)  \varphi(x)^\top\) and \(\frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \) are proportional to each other, and have \(\varphi(x)\) as the only eigenvector with a potentially non zero eigenvalue, so that we have $$ D \Big( \varphi(x)  \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \Big) =  \| \varphi(x) \|^2  \bigg[ \log \Big( \frac{dp}{dq}(x) \Big)  \ – 1 +  \frac{dq}{dp}(x) \bigg].$$ Integrating with respect to \(p\) leads to cancellations: $$  D(\Sigma_p \| \Sigma_q) \leqslant \int_{\mathcal{X}}\| \varphi(x) \|^2  \log \Big( \frac{dq}{dp}(x) \Big) dp(x) \tag{1}.$$ </p>



<p class="justify-text">If we assume a unit norm bound on all features, that is, \(\| \varphi(x)\| \leqslant 1\) for all \(x \in \mathcal{X}\), we get  $$  D(\Sigma_p \| \Sigma_q) \leqslant  \int_{\mathcal{X}} \log \Big( \frac{ dp}{dq}(x) \Big) dp(x) = D(p\|q), $$ and thus we have found a lower bound on the regular Shannon relative entropy. </p>



<p class="justify-text"><strong>From relative entropy to entropy. </strong>We can also define a new notion of entropy by considering the relative entropy with respect to the uniform distribution on \(\mathcal{X}\), which we denote \(\tau\). That is, defining \(\Sigma\) the covariance matrix for the uniform distribution, we consider $$ H(\Sigma_p) = \ – D(\Sigma_p \| \Sigma),$$ which is thus greater than (with a unit norm bound on all features) $$- \int_{\mathcal{X}}   \log \big( \frac{d p}{d \tau}(x)\big) dp(x),$$ which is essentially the <a href="https://en.wikipedia.org/wiki/Differential_entropy">differential entropy</a>.</p>



<p class="justify-text">We now look at a few simple examples, and present further properties later (such as a well-defined bound in the other direction). We consider only one-dimensional examples, but the theory extends to all dimensions and all kernels. In particular, finite large sets such as \(\{-1,1\}^d\) can be considered (see [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>]).</p>



<h2>Polynomials and moments</h2>



<p class="justify-text">We consider \(\mathcal{X} = [-1,1]\) and \(\varphi(x) \in \mathbb{R}^{r+1}\) spanning all degree \(r\) polynomials, with \(k(x,y) = \varphi(x)^\top \varphi(y)\). For these examples, \(\Sigma_p\) is composed of all classical moments of order up to \(2r\). </p>



<p class="justify-text"><strong>Regular polynomial kernel. </strong>Following traditional kernel methods, we can first consider \(\displaystyle k(x,y) = \frac{1}{2^r} ( 1 + xy)^r\), which corresponds to $$\varphi(x)_j =  \frac{1}{2^{r/2}} {{ r \choose j}\!\!}^{1/2} x^j,$$ for \(j \in \{0,\dots,r\}\). We also have \(\sup_{ x\in \mathcal{X} } k(x,x) = 1\), which allows for the Shannon relative entropy upper-bound. Let us consider two distributions \(p\) and \(q\) on \([-1,1]\), and understand if letting \(r\) go to infinity leads to a good estimation of the Shannon relative entropy. </p>



<p class="justify-text">For example, we consider the distribution \(p\) with density \(\frac{2}{\pi} \sqrt{1-x^2}\) (this is a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>  affinely rescaled to \([-1,1]\) instead of \([0,1]\)), plotted below, and \(q\) the uniform distribution, with density \(1/2\). </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="290" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/density.png" class="wp-image-6964" height="181" /></figure></div>



<p class="justify-text">All moments are available in closed form (see end of post), and the Shannon relative entropy is equal to \(D(p\|q) = \int_{-1}^1 \! \frac{2}{\pi} \sqrt{1-x^2} \log \frac{4}{\pi} \sqrt{1-x^2} dx \approx 0.0484\). We can then compute for various values of \(r\), \(D(\Sigma_p \| \Sigma_q)\), which is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="289" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/reg_pol_kernel-2.png" class="wp-image-6937" height="269" /></figure></div>



<p class="justify-text">We should hope that the approximation improves as we use more monomials, but this is not the case. As common in kernel methods, the standard polynomial kernel does not have a good behavior. This is partly due to the fact that \(k(x,x) = \big( \frac{1+x^2}{2} \big)^r\) is far from uniform on \([-1,1]\).</p>



<p>Let’s now consider a better polynomial kernel.</p>



<p class="justify-text"><strong>Christofell-Darboux kernels. </strong>Other polynomial bases could be used as well beyond \(1, X, \dots, X^r\), in particular any basis of polynomials with increasing degrees \(P_0, \dots, P_r\) can be used, in particular polynomials that are <a href="https://en.wikipedia.org/wiki/Orthogonal_polynomials">orthogonal</a> with respect to a probability distribution on \([-1,1]\), with $$ \varphi(x)_j = \frac{1}{\sqrt{\alpha_r}} P_j(x) \mbox{ and } k(x,y) = \frac{1}{\alpha_r} \sum_{j=0}^r P_j(x) P_j(y),$$ where \(\alpha_r\) is here to ensure that \(k(x,x) \leqslant 1\) on \([-1,1]\). The kernel function \(k(x,y)\) has then itself a simple expression through the <a href="https://en.wikipedia.org/wiki/Christoffel%E2%80%93Darboux_formula">Christoffel–Darboux formula</a>, as $$ k(x,y) \propto   \frac{P_r(x)P_{r+1}(y)-P_r(y)P_{r+1}(x)}{x-y},$$ with an explicit proportionality constant. For more details on these kernels and their application to data analysis, see [<a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">5</a>].</p>



<p class="justify-text">For example, for <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev polynomials</a>, which are orthonormal with respect to the probability measure with density \(\big(\pi \sqrt{1-x^2}\big)^{-1}\) on \([-1,1]\), we have \(P_0 = 1\), \(P_1 =  \sqrt{2} X\), and \(P_2 = \sqrt{2}( 2X^2 – 1) \), and more generally, for \(k&gt;0\), \(P_k(\cos \theta) = \sqrt{2}\cos k\theta\). One can then check that for \(x = \cos \theta\) and \(y = \cos \eta\), we have $$k(x,y) = \frac{1}{2} \Big( \frac{ \sin ( 2r+1)( \theta-\eta)/2}{\sin ( \theta-\eta)/2} + \frac{ \sin ( 2r+1)( \theta+\eta)/2}{\sin ( \theta+\eta)/2}  \Big).$$ Moreover, we can check that $$ \sup_{x \in \, [-1,1]} k(x,x) = 2r+1, $$ leading to \(\alpha_r = 2r+1\).</p>



<p class="justify-text"> </p>



<p class="justify-text">As shown below, after computing (still in closed form, see end of the post) \(\Sigma_p\) and \(\Sigma_q\), we can plot the relative entropy, and compare it to the true one. This leads to an improved estimation.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="293" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/cheb_pol_kernel-2.png" class="wp-image-6936" height="273" /></figure></div>



<p class="justify-text">This is better, but still not quite there, and we get in fact convergence to exactly one half of the true relative entropy. The motivated reader will check that the limit of \(k(x,x)\) when \(r \) tends to infinity is \(1/2\) for all \(x \in (-1,1)\), and \(1\), for \(x \in \{-1,1\}\) (that is, the upper-bound of \(1\) is valid, but it is almost surely equal to \(1/2\)). This is where we lose the factor of \(1/2\) in Eq. (1) above.</p>



<p class="justify-text">One key overall insight is that the span of the feature space is not the only important aspect in the approximation, and that some rescalings are better than others (more on this below when optimize with respect to the chosen basis). Given that we end up estimating entropies from moments, it would be clearly interesting to compare with other ways of estimating entropies from moments, such as using asymptotic expansions based on Edgeworth expansions [8].</p>



<h2>Fourier series and translation-invariant kernels</h2>



<p class="justify-text">We still consider \(\mathcal{X} = [-1,1]\). For a \(2\)-periodic function \(q: \mathbb{R} \to \mathbb{R}\), such that \(q(0) = 1\), with Fourier series \(\hat{q}: \mathbb{Z} \to \mathbb{C}\), we can consider $$k(x,y) = q(x-y) = \sum_{\omega \in \mathcal{Z}} \hat{q}(\omega) e^{i\pi \omega(x-y)}.$$ We can for example take $$q(x) = \frac{\sin ( 2r+1) \pi x/2}{(2r+1)\sin \pi x/2} $$ which corresponds to \(\hat{q}(\omega) = \frac{1}{2r+1} 1_{|\omega| \leqslant r}\), and having $$ \varphi(x)_\omega = \frac{1}{\sqrt{2r+1}} e^{ i\pi \omega x } $$ for \(\omega \in \{-r-1,\dots,r+1\}\) (we consider complex-valued features for simplicity). We refer to this kernel as the <a href="https://en.wikipedia.org/wiki/Dirichlet_kernel">Dirichlet kernel</a>.</p>



<p class="justify-text"><strong>Link with eigenvalues of Toeplitz matrices.</strong> For the specific choice of the Dirichlet kernel, the element of the covariance matrix indexed by \(\omega\) and \(\omega’\) has value \(\int_{-1}^1 e^{i\pi (\omega-\omega’)} dp(x)\), which is the <a href="https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)">characteristic function</a> of \(p\) evaluated at \(\pi(\omega-\omega’)\). One consequence is that \(\Sigma_p\) is a <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrix</a> with constant values across all diagonals. It is then well-known that as \(m\) goes to infinity, the eigenvalues of \(\Sigma_p\) are tending in distribution to \(p\) (see [<a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">10</a>]), which provides another proof of good estimation of the entropy in this very particular case.</p>



<p class="justify-text">Note that we could also consider an infinite-dimensional kernel \(q(x) =\Big( 1+ \frac{\sin^2 \pi x/2}{\sinh^2 \sigma}\Big)^{-1}\) which corresponds to \( \hat{q}(\omega) \propto e^{-2\sigma |\omega|}\), where the link with Toeplitz matrices does not apply. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for details.</p>



<p class="justify-text"><strong>Increasing number of frequencies. </strong>We can then plot the estimation of the same relative entropy as for polynomial kernels, as the number of frequency grows (we can still get the Fourier moments in closed form, see end of post).</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="305" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/dirichlet_kernel-1.png" class="wp-image-6944" height="283" /></figure></div>



<p class="justify-text">We now see that as the dimension of the feature space grows, we get a perfect estimation of the relative entropy. </p>



<p class="justify-text">This estimation is provably from below, but how far is it? How big \(r\) needs to be? </p>



<h2>Upper-bound trough quantum information theory</h2>



<p class="justify-text">In order to find a lower-bound \(D(\Sigma_p \| \Sigma_q)\), we cannot rely on Jensen’s inequality, as it is in the wrong direction (this happens a lot!). But here, <a href="https://en.wikipedia.org/wiki/Quantum_information">quantum information theory</a> can come to the rescue. </p>



<p class="justify-text">Using the covariance matrix \(\Sigma\) associated to the uniform distribution \(\tau\) on \(\mathcal{X}\), we consider the PSD matrices \(D(y)\), indexed by \(y \in \mathcal{X}\), equal to $$D(y) = \Sigma^{-1/2} \varphi(y) \varphi(y)^\top \Sigma^{-1/2},$$ for which we have $$\int_{\mathcal{X}} D(y) d\tau(y) =\Sigma^{-1/2} \Big( \int_{\mathcal{X}}  \varphi(y) \varphi(y)^\top d\tau(y)\Big)\Sigma^{-1/2} =   I. $$ We can now consider, in quantum information theory terms, that each \(D(y)\) corresponds to a measurement that can be applied to a density operator, and that we look at measures obtained from \(p\), as \(\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p)\), and from \(q\) as \(\tilde{q}(y) = {\rm tr } ( D(y) \Sigma_q)\). This is a proper <a href="https://en.wikipedia.org/wiki/Quantum_operation">quantum operation</a> because the matrices \(D(y)\) are PSD and sum to identity.</p>



<p class="justify-text">The monotonicity of the von Neumann entropy with respect to quantum operations [9] applies, and we immediately get $$D(\Sigma_p \| \Sigma_q) \geqslant \int_{\mathcal{X}} \tilde{p}(y) \log \frac{ \tilde{p}(y)}{\tilde{q}(y)} d\tau(y).$$ The lower-bound can be seen as the Shannon relative entropy \(D( \tilde{p} \| \tilde{q} )\) between the distributions with density \(\tilde{p}\) and \(\tilde{q}\) with respect to \(\tau\).</p>



<p class="justify-text">These densities have direct relationships with \(p\) and \(q\). Indeed,  we have $$\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p) = \int_{\mathcal{X}} \big( \varphi(x)^\top \Sigma^{-1/2} \varphi(y) \big)^2 dp(x).$$ Denoting $$h(x,y) = \big( \varphi(x)^\top \Sigma_\tau^{-1/2} \varphi(y) \big)^2,$$ we have \(\int_{\mathcal{X}} h(x,y) d\tau(x) = \| \varphi(y)\|^2\) for all \(y \in \mathcal{X}\), and thus, when the features have unit norms, we can consider \(h\) as a smoothing kernel, and thus \(\tilde{p}\) and \(\tilde{q}\) as smoothed versions of \(p\) and \(q\). Thus, overall we have: $$D(\tilde{p} \| \tilde{q}) \leqslant D(\Sigma_p \| \Sigma_q)  \leqslant D(p\|q),$$ and we have sandwitched the kernel relative entropy by Shannon relative entropies.</p>



<p class="justify-text">We can then characterize the error made in estimating the relative entropy by measuring how the smoothing kernel \(h(x,y)\) differs from a Dirac. In  [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Section 4.2], I show how, for metric spaces, the difference between \(D(\tilde{p} \| \tilde{q})\) and \(D({p} \|{q}) \), and thus between \(D(\Sigma_p \| \Sigma_q)\) and \(D(\tilde{p} \| \tilde{q})\), is upperbounded by an explicit function of \(p\) and \(q\), times $$ \sup_{x \in \mathcal{X}} \int_{\mathcal{X}} h(x,y) d(x,y)^2 d\tau(y),$$ which is indeed small when \(h(x,y)\) is closed to a Dirac.</p>



<p class="justify-text">For the Dirichlet kernel presented above, we can show that $$h(x,y) = \frac{1}{4r+2} \bigg( \frac{\sin ( 2r+1) \pi (x-y)/2}{\sin \pi (x-y)/2}\bigg)^2,$$ and that the quantity above characterizing convergence scales as \(1 / (2r+1)\), thus providing an explicit convergence rate. As an illustration, we plot the smoothing kernel at \(y=0\), that is, \(h(x,0)\) in regular scale (left) and log-scale (right), for the Dirichlet kernel above (for which we have unit norm features) for various values of \(r\). We see how it converges to a Dirac as \(r\) grows.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="503" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/smoothh_both-2.gif" class="wp-image-6999" height="223" /></figure></div>



<h2>Estimation</h2>



<p class="justify-text">Above, we have relied on finite-dimensional feature maps and explicit expectations under \(p\) and \(q\). This is clearly not scalable to generic situations and potentially infinite-dimensional feature spaces. But we can rely here on 30 years of progress in kernel methods.</p>



<p class="justify-text">For simplicity here, we will only estimate the von Neumann entropy \({\rm tr}\big[ \Sigma_p \log \Sigma_p \big]\) from \(n\) independent and identically distributed samples. The natural estimator is \({\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big]\), where \(\hat{\Sigma}_p\) and \(\hat{\Sigma}_q\) are empirical covariance matrices (the averages of \(\varphi(x_i) \varphi(x_i)^\top\) over all data points \(x_1,\dots,x_n\)).</p>



<p class="justify-text">In order to compute it efficiently, we can use the usual kernel trick, exactly as used within <a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel principal component analysis</a>, and if \(K\) is the \(n \times n\) kernel matrix associated with the \(n\) data points, then the eigenvalues of \(\frac{1}{n} K\) and \(\hat{\Sigma}_p\) are the same, and thus $$  {\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big] =  {\rm tr}\big[ \big(\frac{1}{n}K  \big)\log \big( \frac{1}{n} K \big)  \big].$$ </p>



<p class="justify-text">In terms of statistical analysis, we refer to [1] where it is shown that the difference between the estimate and the true value is of order \(C / \sqrt{n}\), with an explicit constant \(C\), for a wide range of kernels. Surprisingly, there is no need to add any form of regularization.</p>



<p class="justify-text"><strong>Entropy estimation. </strong>Together with the approximation result presented above, by letting \(r\) go to infinity for the Dirichlet kernel, we get an estimator of the regular Shannon entropy which is not that bad (see [<a href="https://arxiv.org/pdf/1711.02141">7</a>] for an optimal one).</p>



<h2>Learning representations</h2>



<p class="justify-text">As mentioned above for polynomials, a key question is how to choose a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\). Given that we have a lower-bound on the relative entropy, it is tempting to try to <em>maximize</em> this lower bound, subject to \(\forall x \in \mathcal{X}\), \(\| \varphi(x)^\top \varphi(y)\|^2 \leqslant 1\). The magic is that this is a tractable optimization problem (concave maximization with convex constraints) in the kernel function \((x,y) \mapsto \varphi(x)^\top \varphi(y)\). This is not an obvious result (in fact, I first tried to prove that the von Neumann relative entropy was convex in the kernel, while it is in fact concave). See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for the concavity proofs and simple optimization algorithms.</p>



<h2>Duality and log-partition functions</h2>



<p class="justify-text">Given that our kernel notions of relative entropies are lower bounds on the regular notions, by the traditional convex duality results between maximum entropy and log-partition functions (see, e.g., [<a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">11</a>]), we should obtain upper-bounds on log-partition functions. This is exactly what we can get, we simply provide the gist here and refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for further details. </p>



<p class="justify-text">Given a probability distribution \(q\) on \(\mathcal{X}\) and a function \(f: \mathcal{X} \to \mathbb{R}\), the log-partition function is defined as $$a(f) = \log \bigg( \int_{\mathcal{X}} e^{f(x)} dq(x)\bigg) \tag{2}.$$ It plays a crucial role in probabilistic and statistical inference, and obtaining upper-bounds is instrumental in deriving approximate inference algorithms [11]. A classical convex duality result shows a link with the Shannon relative entropy, as we can express \(a(f)\) as $$a(f) = \sup_{ p\  {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x) \ – D( p \| q).$$</p>



<p class="justify-text">Assuming that all features have unit norm, we have \(D(p \|q) \geqslant D(\Sigma_p \| \Sigma_q)\), and thus $$ a(f) \leqslant \sup_{ p \ {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  – D(\Sigma_p \| \Sigma_q),$$ which is itself smaller than the supremum over all signed measures with unit mass (thus removing the non-negativity constraint). Therefore, $$ a(f) \leqslant \sup_{ p \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  – D(\Sigma_p \| \Sigma_q) \tag{3},$$ which can be made computationally tractable in a number of cases.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I tried to exhibit a new link between kernels and information theory, which mostly relies on the convexity properties of von Neumann entropy and a bit of quantum analysis. Clearly, the practical relevance of this link remains to be established, but it opens tractable information theory tools to all types of data on which positive kernels can be defined (essentially all of them), with many potential avenues for future work.</p>



<p class="justify-text"><strong>Multivariate modelling.</strong> Regular entropies and relative entropies are used extensively in data science, such as for variational inference, but also to characterize various forms of statistical dependences between random variable. It turns out that the new kernel notions allow for this as well, connecting with a classical line of work within kernel methods [<a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">12</a>, <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">13</a>].</p>



<p class="justify-text"><strong>Other divergences.</strong> I have focused primarily on the link with Shannon relative entropies, but most of the results extend to all <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a>. For example, we can get kernel-based lower bounds on the square <a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a>, or the \(\chi^2\)-divergences.</p>



<p class="justify-text"><strong>Kernel sum-of-squares with a probabilistic twist.</strong> As a final note, there is a link between the new notions of kernel entropies with global optimization with <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">kernel sum-of-squares</a> [<a href="http://arxiv.org/abs/2012.11978">14</a>]. Indeed, in Eq. (2), if we add a temperature parameter \(\varepsilon\) and define $$a_\varepsilon(f) =\varepsilon  \log \bigg( \int_{\mathcal{X}} e^{f(x) / \varepsilon} dq(x) \bigg), $$ then when \(\varepsilon\) tends to zero, \(a_\varepsilon(f) \) tends to \(\sup_{x \in \mathcal{X}} f(x)\). The lower bound from Eq. (3) then becomes $$\sup_{ p \ {\rm measure}}  \int_{\mathcal{X}} f(x) dp(x) \ – \varepsilon D(\Sigma_p \| \Sigma_q),$$ which exactly tends to the sum-of-squares relaxation for the optimization problem. Given that for global optimization, smoothness can be leveraged to circumvent the curse of dimensionality, we can imagine something similar for log-partition functions and entropies.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Francis Bach. <a href="http://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical report, arXiv:2202.08545, 2022. <br />[2] Brian Kulis, Mátyás A. Sustik, Inderjit S. Dhillon. <a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">Low-Rank Kernel Learning with Bregman Matrix Divergences</a>. <em>Journal of Machine Learning Research</em> 10(2):341-376, 2009.<br />[3] Inderjit S. Dhillon, Joel A. Tropp. <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">Matrix nearness problems with Bregman divergences</a>. <em>SIAM Journal on Matrix Analysis and Applications</em> 29(4):1120-1146, 2008.<br />[4] Thomas M. Cover and Joy A. Thomas. <em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br />[5] Edouard Pauwels, Mihai Putinar, Jean-Bernard Lasserre. <a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">Data analysis from empirical moments and the Christoffel function</a>. <em>Foundations of Computational Mathematics</em> 21(1):243-273, 2021.<br />[6] Ronald De Wolf. <a href="https://theoryofcomputing.org/articles/gs001/gs001.pdf">A brief introduction to Fourier analysis on the Boolean cube</a>. <em>Theory of Computin</em>g, 1:1-20, 2008.<br />[7] Yanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. <a href="https://arxiv.org/pdf/1711.02141">Optimal rates of entropy estimation<br />over Lipschitz balls</a>. <em>The Annals of Statistics</em>, 48(6):3228–3250, 2020.<br />[8] Marc M. Van Hulle. <a href="https://direct.mit.edu/neco/article-abstract/17/9/1903/7008/Edgeworth-Approximation-of-Multivariate">Edgeworth approximation of multivariate differential entropy</a>. <em>Neural computation</em>, 17(9):1903-1910, 2005.<br />[9] Mark M. Wilde. <em>Quantum Information Theory</em>. Cambridge University Press, 2013.<br />[10] Robert M. Gray. <a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">Toeplitz and Circulant Matrices: A Review</a>. <em>Foundations and Trends in Communications and Information Theory</em>, 2(3):155-239, 2006.<br />[11] Martin J. Wainwright and Michael I. Jordan. <a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">Graphical Models, Exponential Families, and Variational<br />Inference</a>. <em>Foundations and Trends in Machine Learning,</em> 1(1–2), 1-305, 2008.<br />[12] Francis Bach and Michael I. Jordan. <a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">Kernel independent component analysis</a>. Journal of Machine<br />Learning Research, 3(Jul):1–48, 2002<br />[13] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R. G.<br />Lanckriet. <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">Hilbert space embeddings and metrics on probability measures</a>. <em>Journal of Machine Learning Research</em>, 11:1517–1561, 2010.<br />[14] Alessandro Rudi, Ulysse Marteau-Ferey, and Francis Bach. <a href="https://arxiv.org/abs/2012.11978">Finding global minima via kernel approximations</a>. Technical Report 2012.11978, arXiv, 2020</p>



<h2>Integrals in closed form</h2>



<p class="justify-text">In order to compute in closed form \(\Sigma_p\) for the distribution \(p\) with density on \([-1,1]\) equal to \( \frac{2}{\pi} \sqrt{1-x^2}\), we need to compute some integrals</p>



<ul class="justify-text"><li><strong>Polynomial kernel</strong>: we have $$\int_{-1}^1   \frac{2}{\pi} \sqrt{1-x^2}x^{2k} dx = 2^{1-2k} \frac{ ( 2k-1)!}{(k-1)! (k+1)!}.$$</li><li><strong>Chebyshev kernel</strong>: we need to compute the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos^2 k\theta \, d\theta,$$ which is equal to \(1\) for \(k=0\), to \(1/4\) for \(k=1\), and to \(1/2\) for all larger integers. We also need the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos k\theta \cos(k+2)\theta \, d\theta,$$ which is equal to \(-1/2\) for \(k=0\), and to \(-1/4\) for all larger integers. </li><li><strong>Dirichlet kernel</strong>: we have $$ \int_{-1}^1 \frac{2}{\pi} \sqrt{1-x^2} \cos k \pi x dx = \frac{2}{k \pi} J_1(k\pi),$$ where \(J_1\) is the<a href="https://en.wikipedia.org/wiki/Bessel_function#Bessel_functions_of_the_first_kind"> Bessel function of the first kind</a>.</li></ul>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/information-theory-with-kernel-methods/"><span class="datestr">at April 04, 2022 04:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/044">TR22-044 |  An Optimal Algorithm for Certifying Monotone Functions | 

	Meghal Gupta, 

	Naren Manoj</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given query access to a monotone function $f\colon\{0,1\}^n\to\{0,1\}$ with certificate complexity $C(f)$ and an input $x^{\star}$, we design an algorithm that outputs a size-$C(f)$ subset of $x^{\star}$ certifying the value of $f(x^{\star})$. Our algorithm makes $O(C(f) \cdot \log n)$ queries to $f$, which matches the information-theoretic lower bound for this problem and resolves the concrete open question posed in the STOC '22 paper of Blanc, Koch, Lange, and Tan [BKLT22].

We extend this result to an algorithm that finds a size-$2C(f)$ certificate for a real-valued monotone function with $O(C(f) \cdot \log n)$ queries. We also complement our algorithms with a hardness result, in which we show that finding the shortest possible certificate in $x^{\star}$ may require $\Omega\left(\binom{n}{C(f)}\right)$ queries in the worst case.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/044"><span class="datestr">at April 04, 2022 04:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html">Dissection into rectangles and tensor rank</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>It’s easy to cut a Greek cross into pieces that can be reassembled into a rectangle:</p>

<p style="text-align: center;"><img width="60%" alt="Dissection of a Grek cross to a 4x5 rectangle" src="https://11011110.github.io/blog/assets/2022/cross2rect.svg" /></p>

<p>Here’s another example. The three yellow rectangles below have dimensions <span style="white-space: nowrap;">\(2^{2/3}\times 1\),</span> <span style="white-space: nowrap;">\(2^{1/3}\times 2^{1/3}\),</span> <span style="white-space: nowrap;">and \(1\times 2^{2/3}\).</span> (The order of multiplication matters here!) Put them together, and you get the blue stealth aircraft shape. Can you cut this shape into pieces and reassemble them into a single rectangle?</p>

<p style="text-align: center;"><img width="80%" alt="Union of three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect.svg" /></p>

<p>The answer is yes; any polygon can be cut and reassembled into a rectangle, or into any other polygon of the same area. This is the <a href="https://en.wikipedia.org/wiki/Wallace%E2%80%93Bolyai%E2%80%93Gerwien_theorem">Wallace–Bolyai–Gerwien theorem</a>. But the Greek cross dissection above uses only axis-parallel cuts and translation of the pieces. To get a single rectangle from the stealth shape, you’re going to need a more general class of operations that cut it at odd angles and rotate the pieces. It’s impossible to make a rectangle from this shape using only axis-parallel cuts and translation. And even for the Greek cross, although you can make a rectangle as shown, it’s impossible to make a square using only axis-parallel cuts and translation.</p>

<p>To see why, we need to understand the <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a>. As usually defined, this is a value that can be determined for any three-dimensional polyhedron. It’s called an invariant because it stays unchanged when you cut up the polyhedron into polyhedral parts and reassemble the pieces into something else, allowing odd-angled cuts and rotations. In order for one polyhedron to have a dissection into another, they must both have the same volume and the same Dehn invariant. This can be used to show, for instance, that you cannot dissect a regular tetrahedron into a cube. But as several authors have described, a version of the same method also applies to axis-parallel dissections of orthogonal <span style="white-space: nowrap;">polygons.<sup id="fnref:s"><a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fn:s" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:b"><a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fn:b" class="footnote" rel="footnote">2</a></sup></span></p>

<p>To evaluate the Dehn invariant of an orthogonal polygon, the first step is to represent the polygon in Cartesian coordinates, and to find a <em>rational basis</em> for its coordinates, a system of numbers such that every coordinate has a unique representation as a linear combination of basis elements with rational-number coefficients. This is just a basis, in the usual meaning of the word from linear algebra, when we interpret the real numbers as being a vector space over the rational numbers. In the example of the stealth shape, we can use the same basis for both the <span style="white-space: nowrap;">\(x\)- and</span> <span style="white-space: nowrap;">\(y\)-coordinates,</span> the set of numbers <span style="white-space: nowrap;">\(\{1,2^{1/3},2^{2/3}\}\).</span> The choice of basis is arbitrary (as long as it can represent all coordinates, without any redundancy) and will affect the rest of the calculation but not the conclusions we draw from it. If we’re comparing two shapes, we can use a common basis that can represent the coordinates from both; it won’t hurt if some of the basis elements are only used for one of the two shapes. So for instance, if the Greek cross is made out of unit squares, the square we’d like to dissect it into has side length \(\sqrt5\) and we can use the basis \(\{1,\sqrt 5\}\) to compare the cross to the square.</p>

<p>Next, we decompose the given shape into rectangles through its vertices (like the yellow rectangles that we’ve already used to decompose the stealth shape). Express the width and height of each rectangle as a rational combination of basis elements; in our example, each rectangle is just a product of two basis elements, but sometimes more complex combinations might be required. If we multiply the expression for the width of a rectangle by the expression for the height of a rectangle, we get an expression for the area of the whole rectangle as a sum of terms \(a\cdot b_x\cdot b_y\) where \(a\) is a rational number and \(b_x\) and \(b_y\) are <span style="white-space: nowrap;">\(x\)-basis</span> and <span style="white-space: nowrap;">\(y\)-basis</span> elements, respectively. Adding these expressions for every rectangle in the decomposition gives a similar expression for the area of the whole shape. Since we’re separating out the terms for each product of basis elements, it’s convenient to write it out as a matrix. For the Greek cross and same-area square, with the basis \(\{1,\sqrt 5\}\) for both <span style="white-space: nowrap;">\(x\) and \(y\),</span> we get the matrices</p>

\[\begin{pmatrix} 5 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \qquad\text{and}\qquad
\begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix},\]

<p>respectively. For the stealth shape, with the basis \(\{1,2^{1/3},2^{2/3}\}\) for both <span style="white-space: nowrap;">\(x\) and \(y\),</span> we get the matrix</p>

\[\begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \end{pmatrix}.\]

<p>That’s the Dehn invariant! Because the Greek cross and square have different invariants, they cannot be dissected into each other (for this restricted axis-parallel translation-only form of dissection). But what about the dissection of the stealth shape into a single rectangle? How can we apply this analysis when there are infinitely many possible rectangles to try?</p>

<p>Here it’s convenient to move from the notion of bases and matrices to a more <a href="https://en.wikipedia.org/wiki/Coordinate-free">coordinate-free</a> formulation of the same thing using <a href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)">abstract tensors</a>. When we’re expressing the area of an \(x\times y\) rectangle as a product of rational linear combinations of basis elements, what we’re really doing is constructing the element \(x\otimes y\) of the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a> <span style="white-space: nowrap;">\(\mathbb{R}\otimes_{\mathbb{Q}}\mathbb{R}\).</span> Then when we add the expressions arising from the decomposition into rectangles, to form a matrix describing the whole shape, this is just addition of these elements within the tensor product. The subscript \(\mathbb{Q}\) in the tensor notation corresponds to the fact that we’re interpreting real numbers as forming a vector space over the field \(\mathbb{Q}\) of rational numbers. (Some sources use a subscript \(\mathbb{Z}\) but this is not an important difference here.)</p>

<p>One thing we know about matrices is that they have a <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank</a>, the dimension of the vector space generated by their rows or columns. But tensors also have a notion of rank. The rank of a tensor is the minimum number of terms needed to express it as a sum of products</p>

\[x_1\otimes y_1+x_2\otimes y_2+\cdots,\]

<p>where the \(x_i\) and \(y_i\) can be any elements of the spaces that are being tensored together (they are not required to be basis elements). For tensor products of three or more spaces, the rank is still defined in the same way, but it can get very tricky to calculate. This is a big part about why the time bounds for fast matrix multiplication are still so mysterious: they involve tensors of order three. But for the order-two tensor space \(\mathbb{R}\otimes_{\mathbb{Q}}\mathbb{R}\), we’re saved by linear algebra: the rank of a tensor of order two is just the rank of a matrix representing it, for any basis over the appropriate field <span style="white-space: nowrap;">(here \(\mathbb{Q}\)).</span></p>

<p>Suppose that we could dissect a <span style="white-space: nowrap;">polygon \(P\),</span> using axis-parallel cuts and translations, into a collection of \(k\) rectangles. Then the sum of the tensor expressions for those rectangles would be an expression of the Dehn invariant <span style="white-space: nowrap;">of \(P\),</span> as a sum of \(k\) products. Therefore, the rank of the Dehn invariant of \(P\) would have to be at <span style="white-space: nowrap;">most \(k\).</span> But for our stealth shape, we know the rank: we have expressed its Dehn invariant as a matrix of rank three. Therefore, to dissect it into rectangles, we need at least three rectangles.</p>

<p>Another use of the polyhedral Dehn invariant, besides dissection of one shape into another, involves tiling. Any polyhedron that tiles space must have Dehn invariant zero, and any polyhedron with Dehn invariant zero can be dissected into a different polyhedron that tiles space. For the axis-parallel polygonal Dehn invariant we’re looking at here, things don’t work out quite so neatly. The Greek cross can tile, but has nonzero Dehn invariant. More, any axis-parallel polygon can be cut into multiple rectangles, and these can tile space aperiodically by grouping them into rows of the same type of rectangle.</p>

<p style="text-align: center;"><img width="65%" alt="Aperiodic tiling by three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect-row-tiling.svg" /></p>

<p>So the Dehn invariant cannot be used to prove that such a thing is impossible, because it is always possible. If we could rotate pieces, we could also form these three rectangles into a single-piece axis-parallel hexagon that can tile the plane:</p>

<p style="text-align: center;"><img width="80%" alt="Periodic tiling by a hexagon formed from three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect-flip-tiling.svg" /></p>

<p>But this is not possible without rotation. Every periodic tiling of the plane has a <a href="https://en.wikipedia.org/wiki/Fundamental_domain">fundamental region</a> in the shape of an axis-parallel hexagon, like the hexagons in this periodic tiling.  If one or more copies of a polygon could be cut up by axis-parallel cuts and reassembled by translations to form a single polygon that can tile the plane periodically, it would also be possible to dissect copies of the polygon (possibly a larger number of copies of it) into an axis-parallel hexagon. But these hexagons have a Dehn invariant with rank at most two (they can be cut into two rectangles), while any number of copies of our stealth shape combine to have rank three (making more copies just multiplies the matrix by a scalar). Because the Dehn invariants have different ranks, no dissection is possible.</p>

<p>This same rank analysis can also be applied to the more familiar polyhedral form of the Dehn invariant. For that form, a polyhedron whose Dehn invariant has high rank cannot be dissected into another polyhedron with fewer edges than the rank.</p>

<div class="footnotes">
  <ol>
    <li id="fn:s">
      <p>John Stillwell (1998), <em><a href="https://doi.org/10.1007/978-1-4612-0687-3">Numbers and Geometry</a></em>, Springer, p. 164. <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fnref:s" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:b">
      <p>David Benko (2007), “<a href="https://www.jstor.org/stable/27642302">A new approach to Hilbert’s third problem</a>”, <em>Amer. Math. Monthly</em> 114 (8): 665–676. <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fnref:b" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/108071296947021436">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html"><span class="datestr">at April 03, 2022 06:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=22548">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2022/04/02/amazing-jinyoung-park-and-huy-tuan-pham-settled-the-expectation-threshold-conjecture/">Amazing: Jinyoung Park and Huy Tuan Pham settled the expectation threshold conjecture!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p></p>


<p><img width="1024" alt="kahn-kalai" src="https://gilkalai.files.wordpress.com/2022/04/kahn-kalai.webp" class="alignnone size-full wp-image-22549" height="762" /></p>
<p>A brief summary: <a href="https://arxiv.org/abs/2203.17207">In the paper, A proof of the Kahn-Kalai conjecture,</a> <a href="https://sites.google.com/view/jinyoungpark">Jinyoung Park</a> and <a href="https://web.stanford.edu/~huypham/">Huy Tuan Pham</a> proved the 2006 expectation threshold conjecture posed by Jeff Kahn and me. The proof is wonderful. Congratulations Jinyoung and Huy Tuan!</p>
<p>The 2006 <a href="https://arxiv.org/abs/math/0603218">expectation threshold conjecture</a> gives a justification for a naive way to estimate the threshold probability of a random graph property. Suppose that you are asked about the critical probability for a random graph in G(n,p) for having a perfect matching (or a Hamiltonian cycle). You compute the expected number of perfect matchings and realize that when p is C/n this expected number equals 1/2. (For Hamiltonian cycles it will be C’/n.) Of course, if the expectation is one half, the probability for a perfect matching can still be very low; indeed, in this case, an isolated vertex is quite likely but when there is no isolated vertices the expected number of perfect matchings is rather large. Our 2006 conjecture boldly asserts that the gap between the value given by such a naive computation and the true threshold value is at most logarithmic in the number of vertices. Jeff and I tried hard to find a counterexample but instead we managed to find more general and stronger forms of the conjecture that we could not disprove.</p>
<p>Two years ago Keith Frankston, Jeff Kahn, Bhargav Narayanan, and Jinyoung Park proved a weak form of the conjecture which was proposed in a 2010 paper by Michel Talagrand. (See <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">this post</a>.) Indeed, the expectation threshold conjecture had some connections with a 1995 paper of Michel Talagrand entitled <a href="https://link.springer.com/chapter/10.1007%2F978-3-0348-9090-8_25">Are all sets of positive measure essentially convex?</a> In a 2010 STOC paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Are Many Small Sets Explicitly Small?</a> Michel formulated a whole array of interesting conjectures and commented that he feels that these conjectures are related to the expectation threshold conjecture to which he offered a weaker fractional version. This weak version suffices for various applications of the original conjecture. Keith, Jeff, Bhargav, and Jinyoung’s work built on the breakthrough work of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture. </p>
<p>Proving the full expectation threshold conjecture looked like a  difficult task. The only path that people saw was to try to relate Talagrand’s fractional expectation threshold with our expectation threshold. (Indeed Talagrand also conjectured that they only differ by a multiplicative constant factor. This looked if true very difficult to prove.)  However this is not the path taken by Jinyoung Park and Huy Tuan Pham and they found a direct simple argument! Jinyoung and Huy Tuan also used their method to settle one of the central conjectures (Conj 5.7) from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Talagrand’s paper</a> and this will be presented in a forthcoming paper.</p>
<p>The <em>logn</em> gap in our conjecture looked rather narrow but now that it was proved we can ask for conditions that will guarantee a smaller gap. For example, when is the gap only a constant?</p>
<p>Here is a nice IAS video on Jinyoung Park’s path to math.</p>
<p></p>
<p>Here is a lecture by Michel Talagrand.</p>
<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2022/04/02/amazing-jinyoung-park-and-huy-tuan-pham-settled-the-expectation-threshold-conjecture/"><span class="datestr">at April 02, 2022 04:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/043">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/043">TR22-043 |  Polynomial Bounds On Parallel Repetition For All 3-Player Games With Binary Inputs | 

	Kunal Mittal, 

	Wei Zhan, 

	Uma Girish, 

	Ran Raz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every 3-player (3-prover) game $\mathcal G$ with value less than one, whose query distribution has the support $\mathcal S = \{(1,0,0), (0,1,0), (0,0,1)\}$ of hamming weight one vectors, the value of the $n$-fold parallel repetition $\mathcal G^{\otimes n}$ decays polynomially fast to zero; that is, there is a constant $c = c(\mathcal  G)&gt;0$ such that the value of the game $\mathcal  G^{\otimes n}$ is at most $n^{-c}$.
	
Following the recent work of Girish, Holmgren, Mittal, Raz and Zhan (STOC 2022), our result is the missing piece that implies a similar bound for a much more general class of multiplayer games: For $\textbf{every}$ 3-player game $\mathcal G$ over $\textit{binary questions}$ and $\textit{arbitrary answer lengths}$, with value less than 1, there is a constant $c = c(\mathcal G)&gt;0$ such that the value of the game $\mathcal G^{\otimes n}$ is at most $n^{-c}$.
	
Our proof technique is new and requires many new ideas. For example, we make use of the Level-$k$ inequalities from Boolean Fourier Analysis, which, to the best of our knowledge, have not been explored in this context prior to our work.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/043"><span class="datestr">at April 02, 2022 01:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8669802159969006721">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/a-ramsey-theory-podcast-no-strangers-at.html">A Ramsey Theory Podcast: No Strangers at this Party</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> BILL: Lance, I am going to blog about the Ramsey Theory Podcast called </p><p>                            <i>No strangers at this party</i><br /></p><p>LANCE: Oh, so that will be your April Fools Day post? That is too unbelievable so it won't work as a joke.</p><p>BILL: Okay, you got me. But it will work if I get 14 Ramsey Theorists to do Podcasts on Ramsey Theory and pretend its coming from... where should it come from. </p><p>LANCE: A Hungarian Middle School. </p><p>BILL: That's  too realistic. How about Simon Fraser University in Canada?</p><p>LANCE: Why there?</p><p>BILL: Why not there?</p><p>LANCE: Knock yourself out.</p><p>---------------------------------------------------------------------------</p><p>At Simon Fraser University they have a <a href="https://www.sfu.ca/~vjungic/Ramsey.html">podcast on Ramsey Theory</a>. They had 14 episodes, each one was an interview with someone who is interested in Ramsey Theory. I don't like the term `Ramsey Theorist' since I doubt anyone does JUST Ramsey Theory (e.g., I do Muffins to!).</p><p>Here is the list of people they interviewed. You can find the podcasts at <a href="https://open.spotify.com/show/4UrTrYkJc9rFhiOQRoNbm3">Spotify</a>, <a href="https://anchor.fm/veselin-jungic">Anchor</a>, <a href="https://podcasts.apple.com/us/podcast/the-ramsey-theory-podcast-no-strangers-at-this-party/id1602576205">Apple
Podcasts</a>, and <a href="https://podcasts.google.com/search/Ramsey%20theory">Google Podcasts</a>.</p><p class="MsoNormal"></p><p>Julian Sahasarabudhe, </p><p>Jaroslav Nesetril</p><p>Joel Spencer </p><p>Donald Robertson</p><p>Fan Chung</p><p>Steve Butler</p><p>Tomas Kaiser</p><p>David Conlon</p><p>Bruce Landman </p><p>William Gasarch</p><p>Bryna Kra</p><p>Neil Hindman</p><p>Adriana Hansberg</p><p>Amanda Montejano</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/a-ramsey-theory-podcast-no-strangers-at.html"><span class="datestr">at April 02, 2022 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19850">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/04/01/an-ozone-hole-in-logic/">An Ozone Hole in Logic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Set Theory proved to be perpetually paradoxigenic…</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/04/01/an-ozone-hole-in-logic/powellliteraturerealpersonjan85/" rel="attachment wp-att-19865"><img width="185" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/PowellLiteratureRealPersonJan85.png?resize=185%2C132&amp;ssl=1" class="alignright wp-image-19865" height="132" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">“Literature and the Real Person” <a href="https://www.chroniclesmagazine.org/literature-and-the-real-person/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Mr. Apollinax might have made a good escort for Lofa Polir, had she lived a hundred years ago. The titular character of a <a href="https://interestingliterature.com/2017/09/a-short-analysis-of-t-s-eliots-mr-apollinax/">poem</a> written circa 1915 by Thomas Eliot was based on Bertrand Russell, whose 1901 <a href="https://en.wikipedia.org/wiki/Russell's_paradox">paradox</a> revealed a gaping inconsistency in set theory as it was then known. Russell spent much of the intervening years trying to repair the gap via <a href="https://en.wikipedia.org/wiki/Type_theory#History">type theory</a>.</p>
<p>
Today we present a recently-discovered inconsistency in the type theory of the <a href="https://en.wikipedia.org/wiki/Scala_(programming_language)">Scala</a> programming language, which forced a feature-chopping <a href="https://docs.scala-lang.org/scala3/reference/dropped-features/type-projection.html">revision</a> in its new version <a href="https://www.scala-lang.org/blog/2021/05/14/scala3-is-here.html">3.0</a>. Then, in a worldwide exclusive, we reveal a new theorem by Polir that explains why such inconsistencies are <em>inevitable</em>.</p>
<p>
Ernst Zermelo had already noticed the paradox in 1899 but brushed it off because his axiom system already evaded it. It is possible that Georg Cantor knew it in <a href="https://books.google.com/books?id=BtZM33PzV9UC&amp;pg=PA923&amp;lpg=PA923&amp;dq=Cantor+letter+to+Dedekind+and+Hilbert+about+inconsistency&amp;source=bl&amp;ots=hE28iI4IT4&amp;sig=ACfU3U2CMPDUFBheRaXGQcIMFngE-Iytdg&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj1j-7ej_T2AhXJY98KHd7WDuUQ6AF6BAgOEAM#v=onepage&amp;q=Cantor letter to Dedekind and Hilbert about inconsistency&amp;f=false">letters</a> in 1897 to Richard Dedekind and David Hilbert. I could call this a continuation of a theme we’ve often addressed about discoveries not being named for their first discoverers, but I will not, for this reason: The discover<b>ee</b> was Gottlob Frege, who immediately recognized that the treatise on set theory he was about to publish had been devastated by the paradox. And the person he heard that from, in June 1902, was Russell—not any of his own compatriots.</p>
<p>
The poem comes with an epigram from a Greek text by Lucian satirically praising the painter Zeuxis of Herakleia: “Ω τῆς καινότητος. Ἡράκλεις, τῆς παραδοξολογίας. εὐμήχανος ἄνθρωπος.”  The reference <a href="https://genius.com/annotations/3603099/standalone_embed">translation</a> Eliot had to hand goes: “Such novelty! Heavens, what paradoxes! How inventive he is!”  I believe, however, that Eliot intended a more-particulate reading: “Oh for his innovation! Hercules, his paradoxology! Sweet machine man!”  That goes better with the poem <a href="https://rpo.library.utoronto.ca/content/mr-apollinax">text</a> and with aspects of Russell that were visible to Eliot at the time.  We’ll only talk about the <i>paradoxology</i>.  Russell was well aware that other monsters besides his own were lurking, like <a href="https://en.wikipedia.org/wiki/Kleene-Rosser_paradox">ones</a> that emerged in the US.  What if patching paradoxes is just the way of progress?  Let’s see this play out in the Scala programming language.</p>
<p></p><h2> The Inconsistency </h2><p></p>
<p></p><p>
The root of the inconsistency is allowing <em>projection</em> of an <em>abstract</em> type. Let’s explain the former concept with concrete types first. A standard way to represent trees and other graphs is by having a nested node class, for instance:</p>
<p><code><br />
class BinaryTree[A &lt;: Comparable[A]] {</code></p>
<p>   class Node(var left: Node, var item: A, var right: Node)</p>
<p>   private var root: Node = null<br />
   //etc.<br />
}<br />
</p>
<p>
A key wrinkle of Scala is that if you have two different trees <font size="+1"><tt>T1</tt></font> and <font size="+1"><tt>T2</tt></font>, then <font size="+1"><tt>T1.Node</tt></font> and <font size="+1"><tt>T2.Node</tt></font> are separate types. This notion of <em>path-dependent types</em> enables the compiler to catch bugs where you might try to connect a node on one tree as a child of a node in the other tree. If there were just one <font size="+1"><tt>BinaryTree.Node</tt></font> type, as is standard in Java for instance, then you could easily make this kind of bug and the compiler would not bat an eye.</p>
<p>
If you really do want to combine nodes from two different trees, however, you can do so via code using the projection type <font size="+1"><tt>BinaryTree#Node</tt></font>. The hash sign # makes this defined to be the <i>supertype</i> of nodes of all possible binary trees. </p>
<p>
The <font size="+1"><tt>[A &lt;: Comparable[A]]</tt></font> part means that the tree can hold any type <font size="+1"><tt>A</tt></font> of data that extends a <font size="+1"><tt>trait</tt></font> called <font size="+1"><tt>Comparable</tt></font>, whose function is to mandate having a comparison function for <font size="+1"><tt>A</tt></font> so that the tree can be kept sorted. The symbol <font size="+1"><tt>&gt;:</tt></font> written the other way means supertype rather than subtype.</p>
<p>
We could try to make our binary tree class more versatile by making it more abstract. By itself it could be a <font size="+1"><tt>trait</tt></font> so as to allow deferred implementations. It could try to define a more general notion of <font size="+1"><tt>Node</tt></font> that might relate more directly to stored data, for instance strings or numbers. This is not to say such abstractions are necessarily well-motivated, but the point is this:</p>
<ul>
<li>
The Scala compiler defines what is workable. <p></p>
</li><li>
Its notion of workability is based on mathematical type theory. <p></p>
</li><li>
Lots of abstract constructions are made workable to encourage creativity.
</li></ul>
<p>
Workability is not supposed to lead to type errors, however. That is the unsoundness allowed in versions 2.x of Scala.</p>
<p>
</p><p></p><h2> The Code </h2><p></p>
<p></p><p>
If you have a lot of time—time to read the longest <a href="https://rjlipton.wpcomstaging.com/2013/11/15/the-graph-of-math/">post</a> ever on this blog—then you can draw some motivation for the unsound code. That post shows how to represent the <a href="https://en.wikipedia.org/wiki/Von_Neumann-Bernays-Godel_set_theory">NBG</a> version of set theory, due to John von Neuman, Paul Bernays, and Kurt Gödel, as an infinite graph. At the top of the graph are nodes representing the class of all sets and various other expansive types, which are symbolized by <font size="+1"><tt>V</tt></font> in the post. At the bottom is a node for the null set identified with <font size="+1"><tt>O</tt></font>. We can think of them as representing “zones” for possible supertypes and subtypes of some types of interest, such as string and number. </p>
<p>
Aside from some abstract manipulation of types, the following code does nothing except compose two versions of the identity function, applied to a <font size="+1"><tt>String</tt></font>. As you are welcome to try yourself, it compiles in Scala 2.x. But before you try to run it, you can ask yourself: exactly what number would the string “Hello World!” become?</p>
<p>
<code><br />
/** File "Unsound.scala", mod by KWR from the last example in<br />
    <a href="https://lptk.github.io/programming/2019/09/13/type-projection.html" rel="nofollow">https://lptk.github.io/programming/2019/09/13/type-projection.html</a><br />
 */<br />
object Unsound extends App {</code></p>
<p>   trait VZone { type Node &gt;: String }<br />
   trait OZone { type Node &lt;: Double }</p>
<p>   def liftup[A &lt;: VZone]: String =&gt; A#Node<br />
      = (x =&gt; x)<br />
   def godown[A &lt;: OZone]: A#Node =&gt; Double<br />
      = (x =&gt; x)</p>
<p>   def hole[A &lt;: VZone with OZone]: String =&gt; Double<br />
      = (x =&gt; godown[A](liftup[A](x)))</p>
<p>   println(hole("Hello World!"))<br />
}<br />
</p>
<p>
The point remains, however, that this program is accepted as a “theorem” in the logical theory underlying the Scala 2.x type system. That theory is thereby unsound.</p>
<p>
</p><p></p><h2> Lofa Polir’s Theorem </h2><p></p>
<p></p><p>
A point we have been at pains to make is that this kind of happening is far from rare. Systems are proven secure and then break all the time. Creating more ways to write correct programs has created new ways to have bugs, even as the frequency of old kinds of bugs is reduced. As with the Earth’s ozone hole, the situation is not worsening but the problem persists.</p>
<p>
Dr. Polir’s new theorem not only explains why but also argues that we should be neither surprised nor dismayed by this state of affairs. It can be given the following intuitive statement:</p>
<blockquote><p><b>Theorem 1</b> <em> There is a sound theory <b>A.F.</b> that can be represented as a complexity-bounded limit of inconsistent theories, but not as an effective limit of consistent theories. </em>
</p></blockquote>
<p></p><p>
The soundness is with respect to the standard model of arithmetic and includes every statement of arithmetic proved in set theory. The clever new idea in her proof is the manner of the proof itself: it formalizes and self-applies a criterion of <em>feasible correctness in the limit</em> for a sequence of incorrect proofs. </p>
<p>
The upshot is the following:</p>
<blockquote><p><b> </b> <em> The most productive and feasible way to do mathematics is to design inconsistent theories and fix them incrementally, rather than to demand global correctness from the get-go. </em>
</p></blockquote>
<p></p><p>
Before Dr. Polir types up her paper, she has a request for readers of this blog: What should the initials <b>A.F.</b> of her theory stand for? The name “Arithmetical Foundations” is both hackneyed and taken. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do you think Lofa’s <b>A.F.</b> stands for? </p>
<p></p><p><br />
[hand-fixing of content after posting while still April 1, added paragraph to intro, changed image at top]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/04/01/an-ozone-hole-in-logic/"><span class="datestr">at April 01, 2022 04:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19829">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/03/31/the-2021-turing-award/">The 2021 Turing Award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>If a machine is expected to be infallible, it cannot also be intelligent.—Alan Turing</i></p>
<p>
Jack Dongarra has just won the 2021 <a href="https://amturing.acm.org">Turing Award</a>. Congrats to him. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/31/the-2021-turing-award/jd/" rel="attachment wp-att-19835"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/jd.png?resize=300%2C168&amp;ssl=1" class="aligncenter size-full wp-image-19835" height="168" /></a></p>
<p></p><p><br />
Dongarra is a Distinguished Professor of Computer Science in the Electrical Engineering and Computer Science Department at the University of Tennessee. He has connections to the nearby Oak Ridge National Laboratory, to Rice University, and in England to the University of Manchester—Turing’s academic home.  The nub of ACM’s <a href="https://awards.acm.org/about/2021-turing">article</a> says the following, with <i>linear algebra</i> emphasized:</p>
<blockquote><p>
Dongarra’s major contribution was in creating open-source software libraries and standards which employ linear algebra as an intermediate language that can be used by a wide variety of applications. These libraries have been written for single processors, parallel computers, multicore nodes, and multiple GPUs per node. Dongarra’s libraries also introduced many important innovations including autotuning, mixed precision arithmetic, and batch computations.
</p></blockquote>
<p>Dongarra’s doctoral advisor was Cleve Moler decades ago in 1980.  I always think we should thank our advisors—having been one myself.  But they continued on parallel tracks in the 1980s.  Moler had invented <a href="https://en.wikipedia.org/wiki/MATLAB">MATLAB</a> in the 1970s, but with the advent of IBM PCs in the 1980s, realized their promise for bringing advanced numerical computations into wide use.  Moler and John Little and Steve Bangert co-founded <a href="https://en.wikipedia.org/wiki/MathWorks">MathWorks</a> in 1984 to market their rewrite of MATLAB in C.  Dongarra went on to co-write a host of mathematical tools: <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>, <a href="https://en.wikipedia.org/wiki/LAPACK">LAPACK</a> (which succeeded his work with Moler and others on <a href="https://en.wikipedia.org/wiki/EISPACK">EISPACK</a> and <a href="https://en.wikipedia.org/wiki/LINPACK">LINPACK</a>), <a href="https://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software">ATLAS</a>, <a href="https://en.wikipedia.org/wiki/HPCG_benchmark">HPCG</a>, and much more.</p>
<p>
</p><p></p><h2> Why The Turing Award? </h2><p></p>
<p></p><p>
Dongarra received the Turing for his software—code that has had significant impact in many areas of computational science from data analytics, healthcare, renewable energy, weather prediction, genomics, and economics, to name a few. His work was over four decades of writing code that can solve linear systems. Such systems occur in just about all of engineering and science. They challenge us because we constantly wish to solve bigger and more difficult systems. And we are able to solve bigger systems because computers continue to get faster—able to do more operations per second. </p>
<p>
However, the challenge was and is that while computers get faster every year, <em>how</em> they get faster remains involved. They cannot just compute faster but how they get faster continues to be complex. If a computer just got faster every year, then Dongarra would <b>not</b> have been able to win a Turing award. That how they get faster is more complex, more involved, that made his Turing award possible. </p>
<p>
</p><p></p><h2> Why So Hard? </h2><p></p>
<p></p><p>
What are some of the challenges that Dongarra had to fight, and continues to fight?  We can build off the following <a href="https://web.archive.org/web/20111021183914/http://www.csbruce.com/~csbruce/quotes/craig-cut.html">quip</a> by the Canadian software developer Craig Bruce:</p>
<blockquote><p>“It’s hardware that makes a machine fast. It’s software that makes a fast machine slow.”</p></blockquote>
<p>
And in 1980 we could add: what could make an IBM PC fast?</p>
<p>
The goal of Dongarra’s software was to run fast on the current hardware systems. The challenge over the decades is simple:  As hardware systems were improved in performance they did not simply get faster. They needed to change how they worked in order to get faster.  The systems changed from single processors, to parallel computers, multicore nodes, and multiple GPUs per node. These changes made the ability to exploit the potential performance harder and harder. </p>
<p>
Let’s assume, for example, that the hardware could not go 100X faster but could execute 100 operations at the same time. Then provided we can exploit this parallelism we can make the hardware seem to be 100 times faster. But if we cannot exploit this, we are in trouble. This is the key issue that Dongarra faced. This challenge is exactly what made his work so difficult and important.</p>
<p></p><h2>The Test of Time</h2><p></p>
<p>
The solutions found by Dongarra and his co-workers were definitive enough, and the core of linear algebra in LINPACK pure enough, that it could yield a salient benchmark of hardware power.  The <a href="https://en.wikipedia.org/wiki/LINPACK_benchmarks">LINPACK benchmark suite</a> won <a href="https://www.nytimes.com/1991/09/22/business/technology-measuring-how-fast-computers-really-are.html">agreement</a> by the early 1990s as giving the definitive <a href="https://en.wikipedia.org/wiki/TOP500">measure</a> of hardware power.  A decade ago, they began <a href="https://news.utk.edu/2013/07/10/professor-jack-dongarra-announces-supercomputer-benchmark/">updating</a> it to involve HPCG.</p>
<p>
Thus it is not enough to say that this work has stood the test of time: it <b>is</b> the test of time.  And this is carrying into the future.  A high LINPACK score in 2013 was an early <a href="https://newatlas.com/d-wave-quantum-computer-supercomputer-ranking/27476/">indicator</a> of the power achievable by D-Wave’s quantum adiabatic hardware.  An <a href="https://www.bcg.com/publications/2022/value-of-quantum-computing-benchmarks">article</a> last month titled “The Race to Quantum Advantage Depends on Benchmarking” includes LINPACK as a contender.  There is recent work toward a more-tailored <a href="https://math.berkeley.edu/~linlin/presentations/202007_GoogleQuantum_LinLin_handout.pdf">quantum LINPACK</a> benchmark.</p>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Some interesting milestones about the Turing awards are:  The first recipient of the award, in 1966, was Alan Perlis, an American computer scientist who wrote the compiler for the ALGOL computer programming language. The first woman to win the prize was Frances Allen, in 2006, for her work in compiler optimization, which contributed to the development of parallel execution in multiprocessing. The youngest recipient was Donald Knuth, who was 36 when he received the award in 1974 for his work on algorithms and computer programming. The oldest recipient was Alfred Aho, who was 79 when he received the award in 2020 for his work on algorithms and the theory of programming language implementation.  I knew all the above winners pretty well. Rich DeMillo and I wrote a paper with Perlis in <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">1979</a>.  </p>
<p>
Turing himself was also a pioneer in AI, of course.  He thought about how computers could reason, how they could think.  What directions will the Turing Awards take in the future—and will they reflect this side of Turing’s vision more?</p></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/03/31/the-2021-turing-award/"><span class="datestr">at April 01, 2022 03:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/03/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/03/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="http://www.math.columbia.edu/~thaddeus/ranking/investigation.html">Columbia University mathematician Michael Thaddeus is dubious of the data Columbia used for its #2 ranking from US News &amp; World Report</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107970703826205607">\(\mathbb{M}\)</a>,</span> <a href="http://www.math.columbia.edu/~thaddeus/ranking/executivesummary.pdf">summary</a>, <a href="https://www.chronicle.com/article/columbia-is-ranked-no-2-by-u-s-news-a-professor-says-its-spot-is-based-on-false-data">via</a>). Conclusion: the whole ranking system is irredeemably flawed and casts a pernicious influence on priorities (e.g. Columbia would be discouraged from keeping a Nobelist who happens not to have a Ph.D., because the ranking counts faculty with Ph.D.s but not Nobelists).</p>
  </li>
  <li>
    <p><a href="https://www.theguardian.com/commentisfree/2022/mar/15/ukrainian-heritage-under-threat-truth-soviet-era-russia">Ukrainian archivists rush to digitize and digitally export the information in their documents</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107975640992420745">\(\mathbb{M}\)</a>),</span> both in case the Russian invaders act to destroy unflattering material and as a precaution against more indiscriminate destruction: 
Their physical artworks and architecture are harder to protect, though: “In the midst of the unfolding human tragedy is the appalling cultural loss the war may wreak.”</p>
  </li>
  <li>
    <p>I saw the scene below while returning from my final exam this morning <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107980104426629314">\(\mathbb{M}\)</a>),</span> the first in-person exam I’ve given in two years. Tomorrow we stop requiring masks indoors. Spring has sprung? On the other hand, I don’t have any hard data to support this, but my TAs think that the prevalence of suspected academic dishonesty is much higher than it was before we locked down, perhaps because the students have become used to doing homeworks and exams in an environment where it’s harder to detect.</p>

    <p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/2trees/2trees-m.jpg" style="border-style: solid; border-color: black;" alt="Two trees in Aldrich Park, UC Irvine" /></p>
  </li>
  <li>
    <p><a href="https://vimeo.com/286360639">Sudanese Möbius Band</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107987571486022171">\(\mathbb{M}\)</a>).</span> This is a minimal surface within a hypersphere, with a great circle as its boundary. It can then be stereographically projected into 3d Euclidean space, preserving the circular shape of the boundary. There was apparently a SIGGRAPH’84 video on it by Dan Asimov, one of its discoverers and namesakes (with Sue Goodman: Sue-Dan-ese), but it appears to exist only on old VHS tapes, so instead this more recent video by Charlie Gunn will have to make do.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2022/03/21/combinatorial-convexity-a-wonderful-new-book-by-imre-barany/">Gil Kalai provides a micro-review of Imre Bárány’s new book <em>Combinatorial Convexity</em></a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107995544907177318">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.theverge.com/2022/3/21/22988994/russia-science-publication-database-conferences">Russian government bans Russian-based scientists from participating in international conferences and discourages them from publishing in internationally-indexed journals</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107998691420226306">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Two recent deaths <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108002049810796984">\(\mathbb{M}\)</a>):</span> <a href="https://www.itsoc.org/news/alexander-vardy-distinguished-coding-theorist-passed-away">Alexander Vardy, UCSD coding theorist</a>, and <a href="https://www.ukrinform.net/rubric-society/3435402-ukrainian-mathematician-commits-suicide-in-russia-after-not-being-allowed-to-leave-country.html">Konstantin Olmezov, Ukrainian combinatorist, after being prevented from leaving Russia</a>.</p>
  </li>
  <li>
    <p><a href="https://lthmath.tumblr.com/post/189129742584/a-little-mobius-strip-art-by-adam-p%C4%99kalski-for">A little Möbius strip art by Adam Pękalski</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108009843844704577">\(\mathbb{M}\)</a>).</span></p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2022/pekalsky.png" alt="Möbius strip art by Adam Pękalski" /></p>
  </li>
  <li>
    <p><a href="http://www.ponticulus.hu/rovatok/hidverok/vierling-models-of-surfaces.html">Models of surfaces and abstract art in the early 20th century</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108016004847866791">\(\mathbb{M}\)</a>),</span> Angela Vierling-Claassen, <em>Ponticulus Hungaricus</em>. A brief survey how physical models of mathematical surfaces, made for teaching and research, came to be influential in modern art. This whole site looks like it would be interesting, if you read Hungarian. A Hungarian version of bridgesmathart.org, if you like (where the linked paper originally appeared). They have occasional other pieces in English, as well.</p>
  </li>
  <li>
    <p><a href="https://www.seas.harvard.edu/news/2022/02/bubbles-bubbles-everywhere">Simulating “tens of thousands of bubbles in foamy flows”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108021804418963231">\(\mathbb{M}\)</a>,</span> <a href="https://doi.org/10.1126/sciadv.abm0590">research article</a>,
<a href="https://technews.acm.org/archives.cfm?fo=2022-02-feb/feb-09-2022.html">via</a>). The issue is that preventing spurious bubble coalescence by tracking the shapes of individual bubbles is slow and expensive. As far as I can tell the new solution involves a grid-based heuristic for coloring the graph of bubbles that are dangerously near each other, and tracking the shape of an entire color class at once.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/siam-honors-womens-history-month">SIAM spotlights the accomplishments of four women mathematicians for Women’s History Month</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108025979413339894">\(\mathbb{M}\)</a>).</span> They are mathematical systems biologist and drug developer Carolyn Cho, image reconstruction researcher Carola-Bibiane Schönlieb, fluid mechanics expert and numerical analyst María González Taboada, and computational flow software developer Carolyn Woodward.</p>
  </li>
  <li>
    <p><a href="https://mathematicalstamps.eu/index">Mathematical postage stamps</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108032725203711460">\(\mathbb{M}\)</a>).</span> The entry page is rather bare and not obvious in its navigation: use the “stamps” menu at the top of the page to find the actual content.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-022-00793-1">The rise of citational justice: how scholars are making references fairer</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108038364210238562">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/03/26/weekend-reads-concussion-researcher-faces-more-scrutiny-mendel-the-fraud-seeking-redemption-after-misconduct-finding/">via</a>). The title of this <em>Nature</em> article is not really accurate: it’s mostly about how scholars are making more efforts to discover how biased against disadvantaged groups their citation patterns are, not so much what to do about it. But knowledge is power.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/in-music-and-math-lillian-pierce-builds-landscapes-20220330">Nice profile and interview of number theorist and musician Lillian Pierce</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108046674227966771">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Lillian_Pierce">see also</a>), in part on how those two interests tie together in unexpected ways.</p>
  </li>
  <li>
    <p><a href="https://www.acm.org/media-center/2022/march/turing-award-2021">Jack Dongarra has won the 2021 Turing Award</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108048639276194672">\(\mathbb{M}\)</a>,</span> <a href="https://www.nytimes.com/2022/03/30/technology/turing-award-jack-dongarra.html">see also</a>), for his “pioneering contributions to numerical algorithms and libraries that enabled high performance computational software to keep pace with exponential hardware improvements for over four decades”.</p>
  </li>
  <li>
    <p><a href="https://desystemize.substack.com/p/desystemize-9">How changing your knowledge representation can lead to new insights in sudoku solving</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108052563128328185">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30863073">via</a>). In particular, the post describes a notion of equivalent sets (pairs of sets of sudoku cells that must contain the same values of each other), how to recognize sets as equivalent, and how to use the idea in puzzle solving and puzzle construction, that was all new to me.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/03/31/linkage.html"><span class="datestr">at March 31, 2022 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/042">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/042">TR22-042 |  Matrix Polynomial Factorization via Higman Linearization | 

	Vikraman Arvind, 

	Pushkar Joglekar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In continuation to our recent work on noncommutative
 polynomial factorization, we consider the factorization problem for
 matrices of polynomials and show the following results.
\begin{itemize}  
\item Given as input a full rank $d\times d$ matrix $M$ whose entries
  $M_{ij}$ are polynomials in the free noncommutative ring
  $\mathbb{F}_q\langle x_1,x_2,\ldots,x_n \rangle$, where each $M_{ij}$ is given by a
  noncommutative arithmetic formula of size at most $s$, we give a
  randomized algorithm that runs in time polynomial in $d,s, n$ and
  $\log_2q$ that computes a factorization of $M$ as a matrix product
  $M=M_1M_2\cdots M_r$, where each $d\times d$ matrix factor $M_i$ is
  irreducible (in a well-defined sense) and the entries of each $M_i$
  are polynomials in $\mathbb{F}_q \langle x_1,x_2,\ldots,x_n \rangle$ that are output
  as algebraic branching programs. We also obtain a deterministic
  algorithm for the problem that runs in $poly(d,n,s,q)$.
\item A special case is the efficient factorization of matrices whose
  entries are univariate polynomials in $\mathbb{F}[x]$. When $\mathbb{F}$ is a finite
  field the above result applies. When $\mathbb{F}$ is the field of rationals
  we obtain a deterministic polynomial-time algorithm for the problem.
  \end{itemize}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/042"><span class="datestr">at March 31, 2022 12:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/">PhD Studentship at University of Liverpool (apply by April 18, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a 4-year PhD studentship on Game Theory and Computational Social Choice on Blockchain at the University of Liverpool, in collaboration with the blockchain technology company IOHK. The selected candidate will be advised by Aris Filos-Ratsikas and Rahul Savani from the University of Liverpool, and Philip Lazos from IOHK.</p>
<p>Website: <a href="https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348">https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348</a><br />
Email: Aris.Filos-Ratsikas@liverpool.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/"><span class="datestr">at March 31, 2022 12:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2022/03/31/external-evaluations/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2022/03/31/external-evaluations/">There’s more to data than distributions.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><em>This is first guest post by Deb. More to come!</em></p>

<p>In “<a href="https://www.nejm.org/doi/full/10.1056/NEJMc2104626">The clinician and dataset shift in artificial intelligence</a>,” published in the New England Journal of Medicine, a set of physician-scientists describe how a popular sepsis-prediction system developed by the company Epic needed to be deactivated. “Changes in patients’ demographic characteristics associated with the coronavirus disease 2019 pandemic” supposedly caused spurious alerting arising from the system, rendering it of little value to clinicians. For the authors, this is a clear illustration of distribution shift, a change in training and test data that, in this case, made it difficult to distinguish between fevers and bacterial sepsis. They go into detail about what this means: distribution shift is a fundamental challenge in machine learning, and whenever we attempt to deploy machine learning in the real world without considering the way in which that real world environment can change (whether its changes in technology (e.g., software vendors), changes in population and setting (e.g., new demographics), or changes in behavior (e.g., new reimbursement incentives), then we fail to properly consider the ways in which the data can change or shift between train and test environments. If not considered, the model will inevitably fail.</p>

<p>And why not? If the underlying test data diverges from the data used in the development of the model, we should expect disappointing results. But the distribution shift problem is so common that ML researchers and practitioners have started seeing it everywhere they look. In fact, in many cases, they will inappropriately characterize any failure of deployed ML models as a distribution shift. This both muddles our understanding of what exactly distribution shift means, and limits our vocabulary for the range of failures that can show up in deployment. In this blog post, I’ll use Epic’s sepsis-detector to illustrate some of the current confusion about distribution shift, and why the notion of “external validity”, a description of generalization problems used widely in other fields, is perhaps more relevant.</p>

<p>The terminology of distribution shift is both too specific and not specific enough. A “change in distribution” could be characterized as anything from a variation in source to a re-sampling. <a href="https://rtg.cis.upenn.edu/cis700-2019/papers/dataset-shift/dataset-shift-terminology.pdf">These changes could involve changes to the input features (ie. covariate shift), changes to the labels (ie. prior probability shift) or both (ie. concept drift).</a></p>

<p>The notion of “data distributions” themselves assumes data comes from an imagined data generating function. In that world of infinitely abundant independent data points samples from a bespoke probability distribution (ie. the “independent and identically distributed” i.i.d. assumption), describing data in terms of how it’s distributed makes a lot of sense. But as Breiman describes in <a href="http://www2.math.uu.se/~thulin/mm/breiman.pdf">“Statistical Modeling: The Two Cultures”</a>, that assumption doesn’t often hold for real world data. Very rarely does one actually know the data generating function, or even a reasonable proxy - real world data is disorganized, inconsistent, and unpredictable. As a result, the term “distribution” is vague enough to not address the additional specificity necessary to direct actions and interventions. When we talk about a hypothetical distribution shift, we talk about data changes but are not specific about which data changes happen and why they happen. We’re also constraining our discourse by just looking at changes in the data in the first place, when in fact, many other changes occur between development and deployment (such as changes in interactions with the model, changes in the interpretation of model results, etc.). Specifying the type of distribution shift is one solution, but more importantly, we need to understand specific distribution shifts as part of a broader phenomenon of external validity that we need to begin to articulate as a field.</p>

<p>The most significant consequence of the myopic obsession with distributions is how it constrains ML evaluations. The benchmarking paradigm that dominates ML at the moment is a by-product of its obsession with detecting shifts in data - the evaluation of models on static data test sets are tied to assumptions about failures being due to shifts in data distribution and not much else. A myopic view on distribution shift confuses the discourse on how to evaluate models for deployment. <a href="https://www.nature.com/articles/s41591-021-01312-x">Several</a> <a href="https://www.bmj.com/content/374/bmj.n1872">studies</a> on <a href="https://www.nature.com/articles/s41746-020-00324-0">regulatory approvals</a> of ML-based tools in healthcare already demonstrate how over-emphasis on data distribution shift failures has led ML practitioners and even regulators within the healthcare space to inappropriately prioritize the use of <em>retrospective studies</em> (ie. evaluations on static collections of past data) rather than <em>prospective studies</em> (ie. examinations of the system within its context of use). Things like multi-site assessment, median evaluation sample size, demographic subgroup performance and
“side-by-side comparison of clinicians’ performances with and without AI” are also exceedingly rare in the evaluation of ML-based healthcare tools, as they don’t fit our current narrow perception of what can go wrong when you throw an ML model into the real world. Of course distribution shift matters but the nature in which we focus on it to the exclusion of everything else is regrettable. For better regulation and evaluation methodology for machine learning deployments, we need to expand our thinking and align ourselves with the other fields attempting to understand performance gaps between the theory and practice of interventions.</p>

<p>This broader notion of validity characterizes the accuracy of the claims being made in a specific context. The related notion of reliability has to do with reproducibility and the consistency of results (think of measurement precision), but validity is concerned with some notion of truthfulness and how close claims get to the target of describing the real relationship between inputs and outputs. There are various notions of validity discussed in measurement theory, evaluation science, program evaluation and experiment design literature, but there are common core concepts. For example, internal validity is about assessing a consistent causal relationship between the inputs and outputs within the experiment and construct validity is related to the evaluation of how well experimental variables represent the real world phenomena being observed. When discussing generalization issues, we are most interested in external validity, which analyzes if  the causal relationship between inputs and outputs observed in experiments holds for inputs and outputs outside the experimental setting.</p>

<p>To understand how external validity differs from the current discourse on distribution shift, let’s go back to the sepsis monitoring example. <a href="https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2781307">“External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients,”</a>, published in JAMA, describes a retrospective study on the use of the sepsis tool between December 2018 and October 2019 (notably well before the pandemic began). They examined 27,697 patients undergoing 38,455 hospitalizations and found that the Epic Sepsis Model predicted the onset of sepsis with an area under the curve of 0.63, “which is substantially worse than the performance reported by its developer”. Furthermore, the tool “did not identify 1,709 patients with sepsis (67%) despite generating alerts… for 6,971 of all 38 455 hospitalized patients (18%), thus creating a large burden of alert fatigue.” These researchers rightfully describe these issues as “external validity” issues, and go into detail examining a range of problems far beyond the data-related “shifts” described in the “Clinician and Dataset Shift” oped. They don’t pretend that this doesn’t have to do with changes in the data - of course it does. Epic’s system evaluation was on data from 3 US health systems from 2013 to 2015, and that’s certainly a different dataset than University of Michigan’s 2018-2019 patient records. But they also comment on changes to the interactions doctors had with the model and how that modified outcomes, as well as other external validity factors that had very little to do with data at all, much less “data distribution shift.” Even when discussing substantive data changes, they are specific in characterizing what it is and breaking down the differences that occurred upon deployment at their hospital.</p>

<p>As this study shows, machine learning needs some clean guidelines for evaluating external validity.
To begin scaffolding such frameworks, we can learn from the social sciences. For example, Erin Hartman, a UC Berkeley colleague in political science, and her co-author Naoki Egami <a href="https://erinhartman.com/publication/elements/">propose a taxonomy that provides an interesting start to this discussion</a>. Their interest is in assessing external validity in scenarios where a population is given a policy treatment (eg. sending out voting reminders, updating the tax code, giving out free vaccines, etc.) and the impact of this treatment as measured within the experiment and also once implemented in the real world. If we consider the treatment to be an ML model’s integration into a broader system, we can begin to articulate what external validity could mean in the algorithmic context.  In my next blog post, I’ll try to work through Hartman and Egami’s framework and other specific proposals from other fields on how we could begin to taxonomize external validity issues, and see which of the external validity problems they describe are actually quite relevant to machine learning.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2022/03/31/external-evaluations/"><span class="datestr">at March 31, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=613">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/03/30/tcs-talk-wednesday-april-6-jessica-sorrell-ucsd/">TCS+ talk: Wednesday, April 6 — Jessica Sorrell, UCSD</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://cseweb.ucsd.edu/~jlsorrel/"><strong>Jessica Sorrell</strong></a> from UCSD will speak about “<em>Reproducibility in Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Reproducibility is vital to ensuring scientific conclusions are reliable, but failures of reproducibility have been a major issue in nearly all scientific areas of study in recent decades. A key issue underlying the reproducibility crisis is the explosion of methods for data generation, screening, testing, and analysis, where, crucially, only the combinations producing the most significant results are reported. Such practices (also known as p-hacking, data dredging, and researcher degrees of freedom) can lead to erroneous findings that appear to be significant, but that don’t hold up when other researchers attempt to replicate them.</p>
<p>In this talk, we introduce a new notion of reproducibility for randomized algorithms. This notion ensures that with high probability, an algorithm returns exactly the same output when run with two samples from the same distribution. Despite the exceedingly strong demand of reproducibility, there are efficient reproducible algorithms for several fundamental problems in statistics and learning. We show that any statistical query algorithm can be made reproducible with a modest increase in sample complexity, and we use this to construct reproducible algorithms for finding approximate heavy-hitters and medians. Using these ideas, we give the first reproducible algorithm for learning halfspaces via a reproducible weak learner and a reproducible boosting algorithm. We initiate the study of lower bounds and inherent tradeoffs for reproducible algorithms, giving nearly tight sample complexity upper and lower bounds for reproducible versus non-reproducible SQ algorithms. Finally, we discuss connections to other well-studied notions of algorithmic stability, such as differential privacy.</p>
<p>Joint work with Russell Impagliazzo (UCSD), Rex Lei (UCSD), and Toniann Pitassi (Columbia University), to appear in STOC 2022.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/03/30/tcs-talk-wednesday-april-6-jessica-sorrell-ucsd/"><span class="datestr">at March 30, 2022 10:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blog.simons.berkeley.edu/?p=636">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/simons.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/">Where Do Q-Functions Come From?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>by <a href="https://meyn.ece.ufl.edu/" target="_blank" rel="noreferrer noopener">Sean Meyn</a> (University of Florida) and <a href="http://cs.bme.hu/~gergo/" target="_blank" rel="noreferrer noopener">Gergely Neu</a> (Pompeu Fabra University)</p>



<p>One theoretical foundation of reinforcement learning is optimal control, usually rooted in the Markovian variety known as Markov decision processes (MDPs). The MDP model consists of a state process, an action (or input) process, and a one-step reward that is a function of state and action. The goal is to obtain a policy (function from states to actions) that is optimal in some predefined sense. Chris Watkins introduced the Q-function in the 1980s as part of a methodology for reinforcement learning. Given its importance for over three decades, it is not surprising that the question of the true meaning of <em>Q</em> was a hot topic for discussion during the Simons Institute’s Fall 2020 program on <a href="https://simons.berkeley.edu/programs/rl20" target="_blank" rel="noreferrer noopener">Theory of Reinforcement Learning</a>.</p>



<p>This short note focuses on interactions at the start of the program, and research directions inspired in part by these interactions. To start with, <em>who is Q</em>? Was this code for one of Watkins’ friends at Cambridge? The question was posed early on, which led to an online investigation. The mystery was shattered through a response from Chris: we now know that the letter <em>Q</em> stands for <em>quality</em>, not Quinlyn or Quincy. To discuss further questions and potential answers requires some technicalities.</p>



<p>The discounted-cost optimality criterion is a favorite metric for performance in computer science and operations research, and is the setting of the original Q-function formulation. The definition requires a state process <span class="math inline">\(\{X_k : k\ge 0\}\)</span> and an action (or input) process <span class="math inline">\(\{A_k : k\ge 0\}\)</span>, evolving on respective spaces (which are assumed discrete in this note). There is a controlled transition matrix <span class="math inline">\(P\)</span> that describes dynamics: <span class="math inline">\(X_{k+1}\)</span> is distributed according to <span class="math inline">\(P(\cdot|x,a)\)</span> when <span class="math inline">\(X_k=x\)</span> and <span class="math inline">\(A_k=a\)</span>, for any action sequence that is adapted to the state sequence.</p>



<p>With <span class="math inline">\(\gamma\)</span> denoting the discount factor, the Q-function is the solution to a nonlinear fixed-point equation <span class="math inline">\(T^*Q = Q\)</span> in which <span class="math inline">\(T^*\)</span> is the Bellman operator: <span class="math display">\[\left(T^*Q\right)(x,a) = r(x,a) + \gamma \mathbb{E}_{X’\sim P(\cdot|x,a)}\left[\max_{a’} Q(X’,a’)\right]\]</span> This must hold for each state-action pair <span class="math inline">\((x,a)\)</span>, with the maximum over all possible actions. This is a version of the dynamic programming (DP) equation that has been with us for about seven decades.</p>



<p>The magic of Q-learning, which is based on this DP equation, is that the maximum appears within an expectation. This makes possible the application of Monte Carlo methods to obtain an approximate solution based solely on observations of the actual system to be controlled, or through simulations.</p>



<p>One core idea of modern reinforcement learning (RL) is to find approximate solutions of the DP equation within a function class (e.g., neural networks, as popularized by the deep Q-learning approach of Mnih et al., 2015). While success stories are well-known, useful theory is scarce: we don’t know if a solution exists to an approximate DP equation except in very special settings, and we don’t know if a good approximation will lead to good performance for the resulting policy. We don’t even know if the recursive algorithms that define Q-learning will be stable — estimates may diverge to infinity.</p>



<p>There are many ways to read these negative results, and indeed many articles have been written around this subject. Our own reading is probably among the most radical: without understanding the issues around the existence of solutions to these DP equation approximations or their interpretation, we should search for alternative approximations of dynamic programming suitable for application in RL.</p>



<p>These concerns were raised at Sean Meyn’s <a href="https://www.youtube.com/watch?v=nUaFu3WzW7o" target="_blank" rel="noreferrer noopener">boot camp lecture</a>, where he called on listeners to revisit an alternate foundation of optimal control: the linear programming (LP) approach introduced by Manne (1960) and further developed by Denardo (1970) and d’Epenoux (1963). The message was greeted with enthusiasm from some attendees, including Gergely Neu, who responded, “You have blown my mind!” He had been working on his own formulation of this idea, which became logistic Q-learning (more on this below).</p>



<span id="more-636"></span>



<p>The least-squares approach that is central to most traditional RL algorithms (such as DQN) is replaced by an LP, so that we move from <span class="math inline">\(L_2\)</span> to <span class="math inline">\(L_\infty\)</span> approximation. There is ample motivation for this point of view:</p>



<ul><li>
		<p>Lyapunov theory and the theory of inverse dynamic programming provide firm motivation: a good approximation of the DP equation in an <span class="math inline">\(L_\infty\)</span> sense implies strict bounds on closed-loop performance of the resulting policy. You can learn more about this theory in standard RL texts, and Meyn’s new RL monograph to appear this year.</p>
	</li><li>
		<p>The LP approach reframes the DP approximation as a numerical optimization problem rather than a root-finding problem. Existence of a solution is guaranteed under minimal assumptions, even when working with a restricted set of candidate solutions.</p>
	</li></ul>



<p>Despite these advantages, LP-based approaches were left out of the mainstream RL tool kit until recently. One reason may be a glaring gap in the framework: the classical LPs are phrased in terms of state-value functions and not Q-functions! This means it was not obvious how to bring in Monte Carlo methods for algorithm design.</p>



<p>In 2020, we independently discovered the following LP that can be used to cast Q-learning as a constrained optimization problem — the so-called DPLP: <span class="math display">\[\begin{aligned} \mbox{minimize } \qquad &amp; \mathbb{E}_{X\sim\nu_0}\left[\max_{a} Q(X,a)\right] \\ \mbox{subject to } \qquad &amp; Q(x,a) \ge r(x,a) + \gamma \mathbb{E}_{X’\sim P(\cdot|x,a)}\left[\max_{a’} Q(X’,a’)\right]\end{aligned}\]</span> Without any constraints on the function class, the optimizer is the solution to the DP equation. The constraints amount to the DP inequality <span class="math inline">\(Q \ge T^* Q\)</span>, which tells us only <em>negative</em> deviations should be penalized! This is to be contrasted with the commonly used squared Bellman error criterion that penalizes deviations of both signs. In seeking an approximation within a function class <span class="math inline">\(\mathcal{F}\)</span>, it is not at all difficult to ensure a feasible solution exists, even when there is no theory to ensure that an approximate Bellman optimality operator has a fixed point.</p>



<p>There are a variety of possible ways of turning <span> this</span> insight into practical algorithms. One path (taken by Bas-Serrano et al., 2021) is to replace the hard constraint in the LP with a smooth upper bound on the expected constraint violations, resulting in the following unconstrained minimization problem: <span class="math display">\[%  \mbox{minimize } \mathbb{E}_{X\sim\nu_0}[\max_a Q(X,a)] + \frac 1\eta \log 
% \mathbb{E}_{(X,A)\sim\mu}[e^{\eta(r(X,A) + \gamma \mathbb{E}_{X’\sim P(\cdot|X,A)}[\max_{a’} Q(X’,a’)] – Q(X,A))}].
\mbox{minimize} \;\;\;\;\;\;\,\, \mathbb{E}_{X\sim\nu_0}\left[\max_a Q(X,a)\right] + \frac 1\eta \log 
\mathbb{E}_{(X,A)\sim\mu}\left[e^{\eta((T^*Q)(X,A) – Q(X,A))}\right].\]</span> Bas-Serrano et al. (2021) dubbed the above objective function the “logistic Bellman error” and have shown that it enjoys several favorable properties that make it an ideal objective for stochastic optimization. Interactions at the Simons Institute led to the development of another related algorithmic framework called “convex Q-learning,” in which the hard constraints in the LP are replaced with a different continuous approximation (Lu et al., 2021).</p>



<p>It is straightforward to derive practical algorithms from both approaches by replacing the expectations with sample averages. This approach led to good empirical performance, along with the beginnings of theory for convergence and performance bounds.</p>



<p>The LP approach had been simmering in the background over the prior decade: de Farias and Van Roy (2003) proposed an LP approach for approximate dynamic programming, and Mehta and Meyn (2009) contains foundations for convex Q-learning, including a variant of the DPLP for deterministic systems in continuous time.</p>



<p>The new techniques bring challenges that must be overcome before these LP-based approaches can take on the world. One is that these optimization problems can be very poorly conditioned. Another is that the approximation may greatly underestimate the true Q-function when using a poor choice of function class for approximation. We remain optimistic: just like the RL community has successfully developed numerical recipes for addressing the much more severe issue of nonexistence of fixed points to the approximate Bellman operators, we believe that similar know-how can be developed for LP-based methods if theorists and practitioners join forces. Now that we understand where Q comes from, let us decide together where it will go next!</p>



<p><em>Acknowledgment</em>. The authors owe a great debt to the Simons Institute for providing inspiration and interactions throughout the fall program. In particular, our joint work (Lu et al., 2021) was in large part an outcome of discussions at the meeting. The LP approach is one theme of the new textbook (Meyn, 2022), and the impact of the program is seen throughout the book.</p>



<h2 class="unnumbered" id="references">References</h2>



<ul><li> <p>A. S. Manne. Linear programming and sequential decisions. <em>Management Science</em> 6(3), pages 259–267, 1960.</p> </li><li> <p>E. V. Denardo. On linear programming in a Markov decision problem. <em>Management Science</em> 16(5), pages 281-288, 1970.</p> </li><li> <p>F. d’Epenoux. A probabilistic production and inventory problem. <em>Management Science</em> 10(1), pages 98-108, 1963.</p> </li><li> <p>D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. <em>Operations Research</em>, 51(6), pages 850–865, 2003.</p> </li><li> <p>J. Bas-Serrano, S. Curi, A. Krause, and G. Neu. Logistic Q-learning. In International Conference on Artificial Intelligence and Statistics, pages 3610-3618, 2021.</p> </li><li> <p>P. G. Mehta and S. P. Meyn. Q-learning and Pontryagin’s minimum principle. In Conference on Decision and Control, pages 3598–3605, Dec. 2009.</p> </li><li> <p>F. Lu, P. G. Mehta, S. P. Meyn, and G. Neu. Convex Q-learning. In American Control Conference, pages 4749–4756. IEEE, 2021.</p> </li><li> <p>S. Meyn. <em>Control Systems and Reinforcement Learning</em>. Cambridge University Press (to appear), Cambridge, 2022. Draft manuscript available at <a href="https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/" target="_blank" rel="noreferrer noopener">https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/</a></p> </li><li> <p>V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. <em>Nature</em> 518 (7540), pages 529-533, 2015.</p> </li></ul></div>







<p class="date">
by Simons Institute Editor <a href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/"><span class="datestr">at March 30, 2022 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-three-a-modern-version-of-ben-ors-protocol/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-three-a-modern-version-of-ben-ors-protocol/">Asynchronous Agreement Part Three: a Modern version of Ben-Or's protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this series of posts, we explore the marvelous world of consensus in the Asynchronous model. In this third post, we present a modern version of Ben-Or’s classic protocol that is part of our new work on Asynchronous Agreement. In the first post we defined the problem and in the...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-three-a-modern-version-of-ben-ors-protocol/"><span class="datestr">at March 30, 2022 11:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-two-ben-ors-protocol/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-two-ben-ors-protocol/">Asynchronous Agreement Part Two: Ben-Or's protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue to explore the marvelous world of consensus in the Asynchronous model. In this post, we present Ben-Or’s classic protocol from 1983. In the next post, we will present a more modern version. In the previous post we defined the problem of Asynchronous Agreement, so without further ado, here...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-two-ben-ors-protocol/"><span class="datestr">at March 30, 2022 11:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-one-defining-the-problem/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-one-defining-the-problem/">Asynchronous Agreement Part One: Defining the problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this series of posts, we explore the marvelous world of consensus in the Asynchronous model. In this post, we start by simply defining the problem. Recall the FLP theorem: FLP theorem 1985: Any protocol where no two non-faulty parties decide different values in the asynchronous model that is resilient...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-one-defining-the-problem/"><span class="datestr">at March 30, 2022 11:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19785">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/">Waiting For Self-Deriving Cars</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Once you trust a self-driving car with your life, you pretty much will trust Artificial Intelligence with anything—Dave Waters.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/kk/" rel="attachment wp-att-19787"><img width="110" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/kk.jpg?resize=110%2C110&amp;ssl=1" class="alignright wp-image-19787" height="110" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">ITProToday <a href="https://www.itprotoday.com/author/Keith-Kirkpatrick">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Keith Kirkpatrick is the author of an interesting CACM <a href="https://cacm.acm.org/magazines/2022/4/259392-still-waiting-for-self-driving-cars/fulltext">article</a> on self-driving cars. It is titled “Still Waiting For Self-Driving Cars” and appears in the news section of this month’s issue. </p>
<p>
Today we discuss why it has been so difficult to get self-driving cars started.</p>
<p>
Kirkpatrick’s article starts:</p>
<blockquote><p><b> </b> <em> <i>Over the past decade, technology and automotive pundits have predicted the “imminent” arrival of fully autonomous vehicles that can drive on public roads without any active monitoring or input from a human driver. Elon Musk has predicted his company Tesla would deliver fully autonomous vehicles by the end of 2021, but he made similar predictions in 2020, 2019, and 2017. Each prediction has fallen flat, largely due to real-world safety concerns, particularly related to how self-driving cars perform in adverse conditions or situations.</i> </em>
</p></blockquote>
<p>
</p><p></p><h2> An Issue </h2><p></p>
<p></p><p>
As printed in the current CACM issue, his article says the following on page 13:</p>
<blockquote><p><b> </b> <em> A potential intermediate solution currently being tested in Germany is to utilize remote drivers to control vehicles. Vay, a Berlin-based startup, has been testing a fleet of remote-controlled electric vehicles in that city and plans to roll out a mobility service in Europe and potentially the U.S. this year. The service will allow customers to order a remote-controlled car and have it drive them to their desired destination; on arrival, they get out if the vehicle and leave it to a human teledriver miles away to either park the vehicle or steer it to the next client. </em>
</p></blockquote>
<p></p><p>
I claim this shows the issue that we have with automatic driving systems. There is a typo in the above paragraph. A typo that shows what we are up against in the attempt to make automatic driving systems. </p>
<p>
<b>DO YOU SEE IT?</b></p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/cacmifoftypo/" rel="attachment wp-att-19792"><img width="253" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/CACMifoftypo.jpg?resize=253%2C36&amp;ssl=1" class="aligncenter wp-image-19792" height="36" /></a></p>
<p></p><p><br />
We claim that a typo like this in a CACM article is part of the reason it is so hard to make self-driving cars. Do you agree? Or is the typo not critical? While we’re at it, does the title of this post have a typo?  Read on…</p>
<p>
Ken contributes a more difficult example of this kind, one he used while addressing the same issue of AI advances in his department’s Freshman Seminar. It relates to a later section in Kirkpatrick’s article, where he discusses the issue of</p>
<blockquote><p><b> </b> <em> …testing to ensure vehicle navigation systems understand the complex social interactions that often occur between oncoming and adjacent drivers, or drivers and pedestrians. Generally, if a pedestrian is about to cross or is crossing a street, the driver and pedestrian will make eye contact, and will use nonverbal cues to indicate the direction and speed of their movement. </em>
</p></blockquote>
<p></p><p>
Over to Ken:</p>
<p>
</p><p></p><h2> Ken’s Example </h2><p></p>
<p></p><p>
Here is the relevant portion of the White House <a href="https://obamawhitehouse.archives.gov/the-press-office/2012/07/13/remarks-president-campaign-event-roanoke-virginia">transcript</a> of Barack Obama’s July 2012 campaign speech in Roanoke, Virginia—the one with the notorious phrase “you didn’t build that”:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/youdidntbuildthat/" rel="attachment wp-att-19798"><img width="363" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/YouDidntBuildThat.jpg?resize=363%2C229&amp;ssl=1" class="aligncenter wp-image-19798" height="229" /></a></p>
<p>
The question we could submit to Google—whose own automated-driving efforts have had <a href="https://www.nytimes.com/2015/09/02/technology/personaltech/google-says-its-not-the-driverless-cars-fault-its-other-drivers.html">difficulties</a> with interpreting rules—is:</p>
<blockquote><p><b> </b> <em> What does the word “that” in “you didn’t build that” refer to? </em>
</p></blockquote>
<p></p><p>
By rules of linguistic derivation, the answer seems clear:</p>
<ul>
<li>
The word “that” refers to the most recent noun, which is “business.” <p></p>
</li><li>
The dashes ‘- -‘ connect two parts that refer to each other. <p></p>
</li><li>
The word cannot anyway refer to “roads and bridges” because that is plural whereas “that” is singular, like “business.” <p></p>
</li><li>
(The preceding point is not a self-contradiction because in quotes, “roads and bridges” is a phrase—singular.)
</li></ul>
<p>
Now please take a minute-plus to listen to the actual delivery of this part of the speech (you may need to click back from 0:00:08 to 0:00:00 to get the start):</p>
<p></p><center><br />
<br />
</center><p></p>
<p></p><p><br />
First, the transcript is missing a word: Obama actually said “<em>that</em>—you didn’t build <em>that</em>.” Perhaps having a rhetorically emphasized <em>that</em> right after “business” makes the semantic designation even clearer? But then notice that the part “if you’ve got a business” was delivered in a quick and parenthetical manner inside the full phrase “Somebody invested in roads and bridges … you didn’t build that.” </p>
<p>
The interpretation that the “that” refers to <em>infrastructure</em> is supported by the speech’s immediate segue to the Internet, sandwiched around “somebody else made that happen.” Going out to sources, Obama was <a href="https://www.washingtonpost.com/blogs/fact-checker/post/an-unoriginal-obama-quote-taken-out-of-context/2012/07/20/gJQAdG7hyW_blog.html">channeling</a> a point already notoriously made by Elizabeth Warren in a 2011 <a href="https://www.nationalreview.com/the-agenda/elizabeth-warrens-quote-reihan-salam/">speech</a>. But without going out to sources—something we cannot expect an AI to do in the moment—there is Obama’s next sentence (also included in the speech clip):</p>
<blockquote><p><b> </b> <em> “The point is, is that when we succeed, we succeed because of our individual initiative, but also because we do things together.” </em>
</p></blockquote>
<p></p><p>
Whether Obama intended to say that people did not build their businesses can still be argued, but that’s not the real point. My first point is that AIs based on current art for ascribing meanings not only would make that ascription, but <em>should</em> do so for overall consistency. And the second, larger, point at the end of my <a href="https://cse.buffalo.edu/~regan/cse199/InternetAndData199np.pdf">slides</a>, where I get to hard AI problems, is this:</p>
<blockquote><p><b> </b> <em> We base our confidence in developing AI systems on Alan Turing’s principle that whatever the human mind can resolve, a computer can be programmed to apprehend and execute. But what if important data, on fine scales, concern matters that our human minds cannot resolve? </em>
</p></blockquote>
<p></p><p>
CACM puts in a bubble Kirkpatrick’s quoting another expert that “it will take years and massive compute power to train self-driving systems to understand the nonverbal cues that pass between drivers and pedestrians.”  We believe the task and need of getting cars to <em>self-derive</em> meanings is greater still.  </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do these issues say anything about the difficulty of getting code right? We think so, but what do you think?</p>
<p></p><p><br />
[some word changes; I seem unable to fix the endpoints of my clip on the C-SPAN website.]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/"><span class="datestr">at March 27, 2022 02:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
