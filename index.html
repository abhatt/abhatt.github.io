<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 24, 2020 10:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7654">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/19/focs-deadline-pushed-back-6-days/">FOCS deadline pushed back 6 days</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From Sandy Irani, FOCS 2020 PC chair:</p>



<p>In light of the very unusual developments in the world due to the spread of Covid-19 and the disruption it is causing to many people in our field, especially those with young children at home, the FOCS PC has decided to push back the final deadline for papers by six days. We would have liked to do more, but we still are trying to stick to our timeline for notification since that date is coordinated with other deadlines in the theory community. In addition, some members of the committee are also affected by this crisis and there is concern that we may not be able to do our job as a committee in a shorter time frame. Pre-registration (including titles and abstracts) is still required by the original deadline. Here are the new dates:</p>



<p><strong>Pre-registration (including titles and abstracts):</strong> Thursday <strong>April 9</strong>, 11:59 PM (EDT)</p>



<p><strong>Final Paper Submission:</strong> Wednesday <strong>April 15</strong>, 11:59 PM (EDT)</p>



<p>Conference url: <a href="https://focs2020.cs.duke.edu/" target="_blank" rel="noreferrer noopener">https://focs2020.cs.duke.edu/</a></p>



<p>We hope you all stay healthy!</p>



<p>–Sandy Irani, for the FOCS 2020 Committee</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/19/focs-deadline-pushed-back-6-days/"><span class="datestr">at March 19, 2020 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=39">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/03/19/friday-march-27-sujay-sanghavi-from-ut-austin/">Friday, March 27 — Sujay Sanghavi from UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The second Foundations of Data Science virtual talk will take place next Friday, March 27th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Sujay Sanghavi</strong> from University of Texas at Austin will speak about “<em>Towards Model Agnostic Robustness</em>”.</p>



<p><strong>Abstract</strong>: It is now common practice to try and solve machine learning problems by starting with a complex existing model or architecture, and fine-tuning/adapting it to the task at hand. However, outliers, errors or even just sloppiness in training data often lead to drastic drops in performance. </p>



<p>We investigate a simple generic approach to correct for this, motivated by a classic statistical idea: trimmed loss. This advocates jointly (a) selecting which training samples to ignore, and (b) fitting a model on the remaining samples. As such this is computationally infeasible even for linear regression. We propose and study the natural iterative variant that alternates between these two steps (a) and (b) – each of which individually can be easily accomplished in pretty much any statistical setting. We also study the batch-SGD variant of this idea. We demonstrate both theoretically (for generalized linear models) and empirically (for vision and NLP neural network models) that this effectively recovers accuracy in the presence of bad training data.</p>



<p>This work is joint with Yanyao Shen and Vatsal Shah and appears in NeurIPS 2019, ICML 2019 and AISTATS 2020.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/03/19/friday-march-27-sujay-sanghavi-from-ut-austin/"><span class="datestr">at March 19, 2020 02:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/03/18/private-and-secure-distributed-matrix-multiplication/">Private and Secure Distributed Matrix Multiplication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Machine learning on big data sets takes a significant amount of computational power, so it  is often necessary to offload some of the work to external distributed systems, such as an Amazon EC2 cluster. It is useful to be able to utilize external resources for computation tasks while keeping the actual data <em>private</em> and<em> secure</em>. In particular, matrix multiplication is an essential step in many machine learning processes, but the owner of the matrices may have reasons to keep the actual values protected.</p>



<p>In this post, we’ll discuss four works about secure distributed computation. First, we’ll talk about a method of using MDS (maximum distance separable) error correcting codes to add security and privacy to general data storage (“<a href="https://arxiv.org/pdf/1808.07457.pdf">Cross Subspace Alignment and the Asymptotic Capacity of X-Secure T-Private Information Retrieval”</a> by Jia, Sun, Jafar). </p>



<p>Then we’ll discuss method of adapting a coding strategy for straggler mitigation (<a href="http://papers.nips.cc/paper/7027-polynomial-codes-an-optimal-design-for-high-dimensional-coded-matrix-multiplication.pdf">“Polynomial codes: an optimal design for high-dimensional coded matrix multiplication”</a> by Yu, Qian, Maddah-Ali, Avestimehr) in matrix multiplication to instead add security or privacy (<a href="https://uweb.engr.arizona.edu/~wchang/Globecom-SecureMM-2018.pdf">“On the capacity of secure distributed matrix multiplication”</a> by Chang, Tandon and <a href="https://ieeexplore-ieee-org.stanford.idm.oclc.org/abstract/document/8832193">“Private Coded Matrix Multiplication”</a> by Kim, Yang, Lee)</p>



<p>Throughout this post we will use variations on the following communication model:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig1.png?w=1024" alt="" width="404" class="wp-image-1572" height="192" /></figure>



<p>The data in the grey box is only given to the master, so workers only have access to what they receive (via green arrows). Later on we will also suppose the workers have a shared library not available to the master. The workers do not communicate with each other as part of the computation, but we want to prevent them from figuring out anything about the data if they do talk to each other.</p>



<p> This model is related to <em>private computation</em> but not exactly the same. We assume the servers are “honest but curious”, meaning they won’t introduce malicious computations. We also only require the master to receive the final result, and don’t need to protect any data from the master. This is close to the BGW scheme ([Ben-Or, Goldwasser, Wigderson ’88]), but we do not allow workers to communicate with each other as part of the computation of the result.</p>



<p> We consider <em>unconditional</em> or <em>information-theoretic</em> security, meaning the data is protected even if the workers have unbounded computational power. Furthermore, we will consider having <em>perfect  secrecy</em>, in which the mutual information between the information revealed to the workers and the actual messages is zero.</p>



<h2>X-Secure T-Private Information Retrieval</h2>



<p> Before we get into matrix-matrix multiplication, consider the problem of storing information on the workers to be retrieved by the master, such that it is “protected.” What do we mean by that? [Jia, Sun, and Jafar ’19] define X-secure T-private information retrieval as follows: </p>



<blockquote class="wp-block-quote"><p>Let <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_{K}" class="latex" title="W_1,...,W_{K}" /> be a data set of messages, such that each <img src="https://s0.wp.com/latex.php?latex=W_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_i" class="latex" title="W_i" /> consists of <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> random bits. A storage scheme of <img src="https://s0.wp.com/latex.php?latex=W_%7B1%7D%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_{1},...,W_{K}" class="latex" title="W_{1},...,W_{K}" /> on <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> nodes is </p><p>1. <em>X-secure</em> if any set of up to <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> servers cannot determine anything about any <img src="https://s0.wp.com/latex.php?latex=W_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_i" class="latex" title="W_i" /> and</p><p>2. <em>T-private </em> if given a query from the user to retrieve some data element <img src="https://s0.wp.com/latex.php?latex=W_%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_{\theta}" class="latex" title="W_{\theta}" />, any set of up to <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="T" class="latex" title="T" /> users cannot determine the value of <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\theta" class="latex" title="\theta" />.</p><cite>[Jia, Sun, and Jafar ’19]</cite></blockquote>



<p>Letting <img src="https://s0.wp.com/latex.php?latex=Q_%7B1%7D%5E%7B%5B%5Ctheta%5D%7D%2C...%2CQ_%7BN%7D%5E%7B%5B%5Ctheta%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Q_{1}^{[\theta]},...,Q_{N}^{[\theta]}" class="latex" title="Q_{1}^{[\theta]},...,Q_{N}^{[\theta]}" /> be the set of queries sent to each node and <img src="https://s0.wp.com/latex.php?latex=S_1%2C...%2CS_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1,...,S_N" class="latex" title="S_1,...,S_N" /> be the information stored on each node  (all vectors of length L), we depict this as:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig2.png?w=1024" alt="" width="452" class="wp-image-1581" height="223" /></figure>



<p>The information theoretic requirements of this system to be correct can be summarized as follows (using notation <img src="https://s0.wp.com/latex.php?latex=S_%7B%5B1%3AN%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_{[1:N]}" class="latex" title="S_{[1:N]}" /> for set <img src="https://s0.wp.com/latex.php?latex=S_1%2C...%2CS_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1,...,S_N" class="latex" title="S_1,...,S_N" />):</p>



<figure class="wp-block-table"><table><tbody><tr><td><strong>Property</strong></td><td><strong>Information Theoretic Requirement</strong></td></tr><tr><td>Data messages are size <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> bits</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%29%3DH%28W_2%29%3D....%3DH%28W_K%29+%3D+L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1)=H(W_2)=....=H(W_K) = L" class="latex" title="H(W_1)=H(W_2)=....=H(W_K) = L" /></td></tr><tr><td>Data messages are independent</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%2C...%2CW_K%29+%3D+KL&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1,...,W_K) = KL" class="latex" title="H(W_1,...,W_K) = KL" /></td></tr><tr><td>Data can be determined from the stored information</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%2C....%2CW_K%29%7CS_%7B%5B1%3AN%5D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1,....,W_K)|S_{[1:N]}) = 0" class="latex" title="H(W_1,....,W_K)|S_{[1:N]}) = 0" /></td></tr><tr><td>User has no prior knowledge of server data</td><td><img src="https://s0.wp.com/latex.php?latex=I%28S_%7B%5B1%3AN%5D%7D%3BQ%5E%7B%5B%5Ctheta%5D%7D_%7B%5B1%3AN%5D%7D%2C%5Ctheta%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(S_{[1:N]};Q^{[\theta]}_{[1:N]},\theta) = 0" class="latex" title="I(S_{[1:N]};Q^{[\theta]}_{[1:N]},\theta) = 0" /></td></tr><tr><td>X-Security</td><td><img src="https://s0.wp.com/latex.php?latex=I%28S_%7B%5Cmathcal%7BX%7D%7D%3BW_1%2C...%2CW_K%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(S_{\mathcal{X}};W_1,...,W_K) = 0" class="latex" title="I(S_{\mathcal{X}};W_1,...,W_K) = 0" />, <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%5Cmathcal%7BX%7D%5Csubset+%5B1%3AN%5D%2C%7C%5Cmathcal%7BX%7D%7C%3DX&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\forall \mathcal{X}\subset [1:N],|\mathcal{X}|=X" class="latex" title="\forall \mathcal{X}\subset [1:N],|\mathcal{X}|=X" /></td></tr><tr><td> T-Privacy</td><td><img src="https://s0.wp.com/latex.php?latex=I%28Q_%7B%5Cmathcal%7BT%7D%7D%5E%7B%5B%5Ctheta%5D%7D%2CS_%7B%5Cmathcal%7BT%7D%7D%3B+%5Ctheta%29+%3D+0%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(Q_{\mathcal{T}}^{[\theta]},S_{\mathcal{T}}; \theta) = 0," class="latex" title="I(Q_{\mathcal{T}}^{[\theta]},S_{\mathcal{T}}; \theta) = 0," />  <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%5Cmathcal%7BT%7D+%5Csubset+%5B1%3AN%5D%2C+%7C%5Cmathcal%7BT%7D%7C%3DT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\forall \mathcal{T} \subset [1:N], |\mathcal{T}|=T" class="latex" title="\forall \mathcal{T} \subset [1:N], |\mathcal{T}|=T" /></td></tr><tr><td>Nodes answer only based on their data and received query </td><td><img src="https://s0.wp.com/latex.php?latex=H%28A_n%5E%7B%5B%5Ctheta%5D%7D%7C+Q_n%5E%7B%5B%5Ctheta%5D%7D%2CS_n%29+%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(A_n^{[\theta]}| Q_n^{[\theta]},S_n) =0" class="latex" title="H(A_n^{[\theta]}| Q_n^{[\theta]},S_n) =0" /></td></tr><tr><td>User can decode desired message from answers</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_%7B%5Ctheta%7D+%7C+A_%7B%5B1%3AN%5D%7D%5E%7B%5B%5Ctheta%5D%7D%2CQ_%7B%5B1%3AN%5D%7D%5E%7B%5B%5Ctheta%5D%7D+%2C%5Ctheta%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_{\theta} | A_{[1:N]}^{[\theta]},Q_{[1:N]}^{[\theta]} ,\theta) = 0" class="latex" title="H(W_{\theta} | A_{[1:N]}^{[\theta]},Q_{[1:N]}^{[\theta]} ,\theta) = 0" /></td></tr></tbody></table></figure>



<p>Given these constraints, Jia et al. give bounds on the capacity of the system. Capacity is the maximum rate achievable, where rate is defined as bits requested by the worker (<img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" />, the length of a single message) divided by the number of bits downloaded by the worker. The bounds are in terms of the capacity of T-Private Information Retrieval, (which is the same as the above definition, with only requirement 2).</p>



<blockquote class="wp-block-quote"><p>If <img src="https://s0.wp.com/latex.php?latex=N+%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N \leq X+T" class="latex" title="N \leq X+T" /> then for arbitrary <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" />, <img src="https://s0.wp.com/latex.php?latex=C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BN%7DC_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BNK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="C_{XSTPIR}(N,K,X,T) = \frac{N-X}{N}C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" class="latex" title="C_{XSTPIR}(N,K,X,T) = \frac{N-X}{N}C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" />.</p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%5Cleq+%5Cleft%28%5Cfrac%7BN-X%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle C_{XSTPIR}(N,K,X,T) \leq \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T)" class="latex" title="\displaystyle C_{XSTPIR}(N,K,X,T) \leq \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T)" /></p><p> When <img src="https://s0.wp.com/latex.php?latex=N%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N\leq X+T" class="latex" title="N\leq X+T" />: <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Cleft%28%5Cfrac%7BN-X%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BNK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle C_{XSTPIR}(N,K,X,T) = \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" class="latex" title="\displaystyle C_{XSTPIR}(N,K,X,T) = \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" /> </p><p> <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Clim_%7BK%5Cto+%5Cinfty%7D+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Clim_%7BK%5Cto+%5Cinfty%7D+%5Cleft%28%5Cfrac%7BN-K%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D%5Cbegin%7Bcases%7D%C2%A0+%C2%A01-%28%5Cfrac%7BX%2BT%7D%7BN%7D%29+%26+N%3EX%2BT++%5C%5C%C2%A0++0+%26+N%5Cleq+X%2BT+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle\lim_{K\to \infty} C_{XSTPIR}(N,K,X,T) = \lim_{K\to \infty} \left(\frac{N-K}{N}\right) C_{TPIR}(N-X,K,T) =\begin{cases}   1-(\frac{X+T}{N}) &amp; N&gt;X+T  \\   0 &amp; N\leq X+T \end{cases}" class="latex" title="\displaystyle\lim_{K\to \infty} C_{XSTPIR}(N,K,X,T) = \lim_{K\to \infty} \left(\frac{N-K}{N}\right) C_{TPIR}(N-X,K,T) =\begin{cases}   1-(\frac{X+T}{N}) &amp; N&gt;X+T  \\   0 &amp; N\leq X+T \end{cases}" /></p><cite>[Jia, Sun, and Jafar ’19]</cite></blockquote>



<p>Jia et al. give schemes that achieve these bounds while preserving the privacy and security constraints by introducing random noise vectors into how data is stored and queries are constructed. The general scheme for <img src="https://s0.wp.com/latex.php?latex=N%3EX%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N&gt;X+T" class="latex" title="N&gt;X+T" /> uses <em>cross subspace alignment</em>, which essentially chooses how to construct the stored information and the queries such that the added noise mostly “cancels out” when the master combines all the response from the servers. The scheme for <img src="https://s0.wp.com/latex.php?latex=N%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N\leq X+T" class="latex" title="N\leq X+T" /> is straightforward to explain, and demonstrates the idea of using error correcting codes that treat the random values as the message and the actual data as the “noise.”</p>



<p>For this scheme, the message length <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> is set to <img src="https://s0.wp.com/latex.php?latex=N-X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N-X" class="latex" title="N-X" /> (the number of nodes <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />, minus the maximum number of colluding servers <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" />). First, we generate <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" /> random bit vectors of length <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" />:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig3.png?w=856" alt="" width="370" class="wp-image-1590" height="157" /></figure>



<p>Next, apply an <img src="https://s0.wp.com/latex.php?latex=%28N%2CX%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(N,X)" class="latex" title="(N,X)" /> MDS code to <img src="https://s0.wp.com/latex.php?latex=Z_1%2C...%2CZ_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_1,...,Z_K" class="latex" title="Z_1,...,Z_K" /> to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_1%2C..%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_1,..,\bar{Z}_K" class="latex" title="\bar{Z}_1,..,\bar{Z}_K" />, which are <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" /> encoded vectors of length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig4.png?w=1024" alt="" class="wp-image-1591" /></figure>



<p>For our data <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_K" class="latex" title="W_1,...,W_K" />, we pad each vector with zeros to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BW%7D_1%2C..%2C%5Cbar%7BW%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{W}_1,..,\bar{W}_K" class="latex" title="\bar{W}_1,..,\bar{W}_K" /> of length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig5.png?w=1024" alt="" class="wp-image-1592" /></figure>



<p>Now that the dimensions line up, we can add the two together and store each column <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" /> at the <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bth%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n^{th}" class="latex" title="n^{th}" /> node:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig6.png?w=1024" alt="" class="wp-image-1593" /></figure>



<p>To access the data, the user downloads all <img src="https://s0.wp.com/latex.php?latex=NK&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="NK" class="latex" title="NK" /> bits. The length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> string downloaded from  row <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> can be used to decode <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_i" class="latex" title="\bar{Z}_i" />: <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BW%7D_%7BL%2B1%7D%2C%5Cbar%7BW%7D_%7BL%2B2%7D%2C...%2C%5Cbar%7BW%7D_%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{W}_{L+1},\bar{W}_{L+2},...,\bar{W}_{N}" class="latex" title="\bar{W}_{L+1},\bar{W}_{L+2},...,\bar{W}_{N}" /> are all zero, so columns <img src="https://s0.wp.com/latex.php?latex=L%2B1+%3D+N-X%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L+1 = N-X+1" class="latex" title="L+1 = N-X+1" /> through <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> have the values of <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_%7Bi%2CN-X%2B1%7D%2C...%2C%5Cbar%7BZ%7D_%7Bi%2CN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_{i,N-X+1},...,\bar{Z}_{i,N}" class="latex" title="\bar{Z}_{i,N-X+1},...,\bar{Z}_{i,N}" />. This gives the user <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> values from the MDS code used on each row, so they can decode and get <img src="https://s0.wp.com/latex.php?latex=Z_1%2C...%2CZ_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_1,...,Z_K" class="latex" title="Z_1,...,Z_K" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_%7B1%7D%2C...%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_{1},...,\bar{Z}_K" class="latex" title="\bar{Z}_{1},...,\bar{Z}_K" />. Then a subtraction from the downloaded data gives <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_{K}" class="latex" title="W_1,...,W_{K}" />. Because of the MDS property of the code used to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_1%2C...%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_1,...,\bar{Z}_K" class="latex" title="\bar{Z}_1,...,\bar{Z}_K" />, this scheme is X-secure and because the user downloads all bits, it is T-private.</p>



<h2>Matrix Multiplication with Polynomial Codes</h2>



<p>We now move on to the task of matrix-matrix multiplication. The methods for secure and private distributed matrix multiplication we will discuss shortly are based on <em>polynomial codes</em>, used by [Yu, Maddah-Ali, Avestimehr ’17] for doing distributed matrix multiplications robust to stragglers. Suppose the master has matrices <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BA%7D%7D+%5Cin+%5Cmathbb%7BF%7D_q%5E%7Bm%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{A}} \in \mathbb{F}_q^{m\times n}" class="latex" title="{\bf{A}} \in \mathbb{F}_q^{m\times n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BB%7D%7D+%5Cin+%5Cmathbb%7BF%7D_q%5E%7Bn+%5Ctimes+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{B}} \in \mathbb{F}_q^{n \times p}" class="latex" title="{\bf{B}} \in \mathbb{F}_q^{n \times p}" /> for some finite field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D_q&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{F}_q" class="latex" title="\mathbb{F}_q" />, and <img src="https://s0.wp.com/latex.php?latex=m%2Cn%2Cp+%5Cin+%5Cmathbb%7BZ%7D%5E%2B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="m,n,p \in \mathbb{Z}^+" class="latex" title="m,n,p \in \mathbb{Z}^+" />. Assume <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="m" class="latex" title="m" /> and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" /> are divisible by <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />, so we can represent the matrices divided into <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> submatrices: </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BA%7D%7D%3D+%5Cbegin%7Bbmatrix%7DA_1%5C%5C+A_2+%5C%5C+%5Cvdots+%5C%5C+A_m%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{A}}= \begin{bmatrix}A_1\\ A_2 \\ \vdots \\ A_m\end{bmatrix}" class="latex" title="{\bf{A}}= \begin{bmatrix}A_1\\ A_2 \\ \vdots \\ A_m\end{bmatrix}" />               and        <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BB%7D%7D%3D+%5Cbegin%7Bbmatrix%7DB_1%26+B_2+%26+%5Cdots+%26B_n%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{B}}= \begin{bmatrix}B_1&amp; B_2 &amp; \dots &amp;B_n\end{bmatrix}" class="latex" title="{\bf{B}}= \begin{bmatrix}B_1&amp; B_2 &amp; \dots &amp;B_n\end{bmatrix}" /> </p>



<p>So to recover <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{AB}" class="latex" title="\bf{AB}" />, the master needs each entry of: </p>



<p><img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BAB%7D%7D+%3D+%5Cbegin%7Bbmatrix%7DA_1B_1+%26+A_1B_2+%26+%5Cdots+%26+A_1B_n%5C%5CA_2B_1+%26+A_2B_2+%26+%5Cdots+%26+A_2+B_n%5C%5C%5Cvdots+%26+%5Cvdots+%26%5Cvdots+%26%5Cvdots%5C%5CA_mB_1+%26A_mB_2+%26+%5Cdots%C2%A0+%26+A_mB_n+%5Cend%7Bbmatrix%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{AB}} = \begin{bmatrix}A_1B_1 &amp; A_1B_2 &amp; \dots &amp; A_1B_n\\A_2B_1 &amp; A_2B_2 &amp; \dots &amp; A_2 B_n\\\vdots &amp; \vdots &amp;\vdots &amp;\vdots\\A_mB_1 &amp;A_mB_2 &amp; \dots  &amp; A_mB_n \end{bmatrix}." class="latex" title="{\bf{AB}} = \begin{bmatrix}A_1B_1 &amp; A_1B_2 &amp; \dots &amp; A_1B_n\\A_2B_1 &amp; A_2B_2 &amp; \dots &amp; A_2 B_n\\\vdots &amp; \vdots &amp;\vdots &amp;\vdots\\A_mB_1 &amp;A_mB_2 &amp; \dots  &amp; A_mB_n \end{bmatrix}." /></p>



<p>The key idea of polynomial codes is to encode <img src="https://s0.wp.com/latex.php?latex=A_1%2C...%2CA_m&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A_1,...,A_m" class="latex" title="A_1,...,A_m" /> and <img src="https://s0.wp.com/latex.php?latex=B_1%2C...%2CB_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="B_1,...,B_n" class="latex" title="B_1,...,B_n" /> in polynomials <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" /> to be sent to the <img src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i^{th}" class="latex" title="i^{th}" /> worker,  where they are multiplied and the result is returned. The goal of Yu et al. was to create robustness to stragglers, and so they add redundancy in this process so that not all workers need to return a result for the master to be able to determine <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{AB}" class="latex" title="\bf{AB}" />. In particular, only <img src="https://s0.wp.com/latex.php?latex=mn&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="mn" class="latex" title="mn" /> returned values are needed, so <img src="https://s0.wp.com/latex.php?latex=N-mn&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N-mn" class="latex" title="N-mn" /> servers can be slow or fail completely without hurting the computation. This method can be thought of as setting up the encodings of <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{A}" class="latex" title="\bf{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{B}" class="latex" title="\bf{B}" /> so that the resulting multiplications <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BC%7D_1%3D%5Ctilde%7BA%7D_1%5Ctilde%7BB%7D_1%2C...%2C%5Ctilde%7BC%7D_N%3D%5Ctilde%7BA%7D_N%5Ctilde%7BB%7D_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{C}_1=\tilde{A}_1\tilde{B}_1,...,\tilde{C}_N=\tilde{A}_N\tilde{B}_N" class="latex" title="\tilde{C}_1=\tilde{A}_1\tilde{B}_1,...,\tilde{C}_N=\tilde{A}_N\tilde{B}_N" /> are evaluations of a polynomial with coefficients  <img src="https://s0.wp.com/latex.php?latex=A_1B_1%2CA_1B_2%2C...%2CA_2B_1....%2CA_mB_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A_1B_1,A_1B_2,...,A_2B_1....,A_mB_n" class="latex" title="A_1B_1,A_1B_2,...,A_2B_1....,A_mB_n" /> at <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> different points — equivalent to a Reed-Solomon code. </p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig7.png?w=1024" alt="" class="wp-image-1603" /></figure>



<p>This idea is adapted by [Chang, Tandon ’18] to protect the data from colluding servers: noise is incorporated into the encodings such that the number of encoded matrices required to determine anything about the data is greater than the security threshold <img src="https://s0.wp.com/latex.php?latex=X+%3C+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X &lt; N" class="latex" title="X &lt; N" />. Since the master receives all <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> responses it is able to decode the result of <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{AB}" class="latex" title="\textbf{AB}" />, but no set of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> nodes can decode <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" />, or <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{AB}" class="latex" title="\textbf{AB}" />. Similarly, [Kim, Yang, Li ’19] adapts this idea to impose privacy on a matrix-matrix multiplication: workers are assumed to have a shared library <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D_i%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}_i\}_{i=1}^M" class="latex" title="\{{\textbf{B}}_i\}_{i=1}^M" />, and the user would like to multiply <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BA%7D%7D%7B%5Ctextbf%7BB%7D%7D_%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{A}}{\textbf{B}}_{D}" class="latex" title="{\textbf{A}}{\textbf{B}}_{D}" /> for some <img src="https://s0.wp.com/latex.php?latex=D+%5Cin%5B1%3AM%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="D \in[1:M]" class="latex" title="D \in[1:M]" /> without revealing the value of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="D" class="latex" title="D" /> to the workers. The workers encode the entire library such that when the  encoding is multiplied by an encoded input <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}" class="latex" title="\tilde{A}" /> from the master, the result is useful to the master in decoding <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BA%7D%7D%7B%5Ctextbf%7BB%7D%7D_D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{A}}{\textbf{B}}_D" class="latex" title="{\textbf{A}}{\textbf{B}}_D" />.</p>



<p>Chang and Tandon consider the following two privacy models, where up to <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\ell" class="latex" title="\ell" /> servers may collude. The master also has <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K^{(A)}" class="latex" title="K^{(A)}" /> (and in the second model, <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K^{(B)}" class="latex" title="K^{(B)}" />), which are matrices of random values with the same dimensions as <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> (and <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" />). These are used in creating the encodings <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> (and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" />).</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" /> is public, <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> is private:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig8.png?w=1024" alt="" class="wp-image-1604" /></figure>



<p>Both private:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig9.png?w=1024" alt="" class="wp-image-1605" /></figure>



<p>Kim, Yang, and Lee take a similar approach of applying the method of polynomial code to <em>private</em> matrix multiplication. As before, there are <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> workers, but now the master wants to multiply <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> with some <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BB%7D%7D_D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{B}}_D" class="latex" title="{\textbf{B}}_D" /> in shared library <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D_i%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}_i\}_{i=1}^M" class="latex" title="\{{\textbf{B}}_i\}_{i=1}^M" /> (all the workers have the shared library). </p>



<p>Since the master isn’t itself encoding <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}\}_{i=1}^M" class="latex" title="\{{\textbf{B}}\}_{i=1}^M" /> it has to tell the workers how to encode the library so that it can reconstruct the desired product. This is done by having the master tell the workers what values of <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\vec{y}" class="latex" title="\vec{y}" /> they should use to evaluate the polynomial that corresponds to encoding each library matrix. We denote the encoding of the library done by each worker <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> as the multivariate polynomial <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h" class="latex" title="h" /> which is evaluated at <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}\}_{i=1}^M" class="latex" title="\{{\textbf{B}}\}_{i=1}^M" /> and the node-specific vector <img src="https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D_%7B%5B1%3AM%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y^{(i)}_{[1:M]}" class="latex" title="y^{(i)}_{[1:M]}" /> to get the node’s encoding, <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" />. The worker multiplies this with the encoding of <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> it receives, <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> and returns the resulting value <img src="https://s0.wp.com/latex.php?latex=Z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_i" class="latex" title="Z_i" />. All together, we get the following communication model: </p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig10.png?w=1024" alt="" class="wp-image-1611" /></figure>



<h2>Conclusion</h2>



<p>As we’ve seen, coding techniques originally designed to add redundancy and protect against data loss can also be used to intentionally incorporate noise for data protection. In particular, this can be done when out-sourcing matrix multiplications, making it a useful technique in many data processing and machine learning applications.</p>



<p>References:</p>



<ul><li><a href="https://arxiv.org/pdf/1808.07457.pdf">Jia, Zhuqing, Hua Sun, and Syed Ali Jafar. “Cross Subspace Alignment and the Asymptotic Capacity of  X-Secure T-Private Information Retrieval.” <em>IEEE Transactions on Information Theory</em> 65.9 (2019): 5783-5798.</a></li><li><a href="http://papers.nips.cc/paper/7027-polynomial-codes-an-optimal-design-for-high-dimensional-coded-matrix-multiplication.pdf">Yu, Qian, Mohammad Maddah-Ali, and Salman Avestimehr. “Polynomial codes: an optimal design for high-dimensional coded matrix multiplication.” <em>Advances in Neural Information Processing Systems</em>. 2017.</a></li><li><a href="https://uweb.engr.arizona.edu/~wchang/Globecom-SecureMM-2018.pdf">Chang, Wei-Ting, and Ravi Tandon. “On the capacity of secure distributed matrix multiplication.” <em>2018 IEEE Global Communications Conference (GLOBECOM)</em>. IEEE, 2018.</a></li><li><a href="https://ieeexplore-ieee-org.stanford.idm.oclc.org/abstract/document/8832193">Kim, Minchul, Heecheol Yang, and Jungwoo Lee. “Private Coded Matrix Multiplication.” <em>IEEE Transactions on Information Forensics and Security</em> (2019).</a></li></ul>



<p></p></div>







<p class="date">
by Alex Porter <a href="https://theorydish.blog/2020/03/18/private-and-secure-distributed-matrix-multiplication/"><span class="datestr">at March 18, 2020 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=405">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/18/tcs-talk-wednesday-march-25-dana-moshkovitz-ut-austin/">TCS+ talk: Wednesday, March 25 — Dana Moshkovitz, UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 25th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Dana Moshkovitz</strong> from UT Austin will speak about “<em>Nearly Optimal Pseudorandomness From Hardness</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Existing proofs that deduce BPP=P from circuit lower bounds convert randomized algorithms into deterministic algorithms with a large polynomial slowdown. We convert randomized algorithms into deterministic ones with little slowdown. Specifically, assuming exponential lower bounds against randomized single-valued nondeterministic (SVN) circuits, we convert any randomized algorithm over inputs of length n running in time <img src="https://s0.wp.com/latex.php?latex=t%5Cgeq+n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="t\geq n" class="latex" title="t\geq n" /> to a deterministic one running in time <img src="https://s0.wp.com/latex.php?latex=t%5E%7B2%2B%5Calpha%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="t^{2+\alpha}" class="latex" title="t^{2+\alpha}" /> for an arbitrarily small constant <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+0&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\alpha &gt; 0" class="latex" title="\alpha &gt; 0" />. Such a slowdown is nearly optimal, as, under complexity-theoretic assumptions, there are problems with an inherent quadratic derandomization slowdown. We also convert any randomized algorithm that errs rarely into a deterministic algorithm having a similar running time (with pre-processing). The latter derandomization result holds under weaker assumptions, of exponential lower bounds against deterministic SVN circuits. Our results follow from a new, nearly optimal, explicit pseudorandom generator. The construction uses, among other ideas, a new connection between pseudoentropy generators and locally list recoverable codes.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/18/tcs-talk-wednesday-march-25-dana-moshkovitz-ut-austin/"><span class="datestr">at March 18, 2020 11:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=398">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/17/updated-spring-schedule-increased-talk-frequency-and-joining-as-individuals/">Updated Spring schedule: increased talk frequency, and joining as individuals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We hope you are all as safe and sound as possible these days, and will be for the weeks to come.</p>
<p>For those of you confined at home, it may be hard to remain connected with the TCS community or stay up-to-date with the current research, as in-person seminars and conferences are cancelled. In case this may help, we have decided to <strong>increase for now the frequency of our online seminars</strong>, to try and mitigate this aspect. This won’t restore normalcy, but every little thing counts.</p>
<p>Here is our current, now <strong>weekly</strong> schedule: we may add more talks to it in the days to come, so keep an eye on <a href="https://sites.google.com/site/plustcs/">our calendar</a> and don’t hesitate to <a href="https://sites.google.com/site/plustcs/suggest">suggest talks and results</a> you are curious about.</p>
<ul>
<li>03/25: <a href="https://www.cs.utexas.edu/~danama/">Dana Moshkovitz</a> (UT Austin) on <em>“Nearly Optimal Pseudorandomness From Hardness”</em></li>
<li>04/01: <a href="http://www.cs.cmu.edu/~venkatg/">Venkat Guruswami</a> (CMU) on <em>“Arıkan meets Shannon: Polar codes with near-optimal convergence to channel capacity”</em></li>
<li>04/08: Rahul Ilango (MIT) on <em>“NP-Hardness of Circuit Minimization for Multi-Output Functions”</em></li>
<li>04/15: <span class="JtukPc"><span id="rAECCd"><a href="https://web.math.princeton.edu/~rvan/">Ramon van Handel</a> (Princeton) </span></span>on <em>“Rademacher type and Enflo type coincide”</em></li>
<li>04/22: <a href="https://www.cs.princeton.edu/~hy2/">Huacheng Yu</a> (Princeton) on <em>“Nearly Optimal Static Las Vegas Succinct Dictionary”</em></li>
<li>04/29: <a href="https://www.mit.edu/~mahabadi/">Sepideh Mahabadi</a> (TTIC) on<em> “Non-Adaptive Adaptive Sampling on Turnstile Streams”</em></li>
</ul>
<p>We emphasize that <strong>you can</strong> (and <em>should</em>) <strong>join as individuals</strong>, from home: if you are interested in a talk, ask for a spot for yourself, no need to go to your institution. We have the live audience capacity for this to work, so don’t hesitate!</p>
<p>We will post individual announcements several days before each talk, including the abstracts and how to ask for a spot, as usual; and the talks will of course be available on <a href="https://www.youtube.com/user/TCSplusSeminars/">YouTube</a> and <a href="https://sites.google.com/site/plustcs/past-talks">our website</a> afterwards if you couldn’t make it to the live, interactive one. Crucially, <strong>we would like your feedback</strong>: not only talk suggestions as pointed out before, but also any idea or suggestion you may have of things we could do or implement, or of content you would like to see. You can send us feedback by <a href="https://sites.google.com/site/plustcs/credits">emailing any of our organizers</a>, or leaving a comment below.</p>
<p>Stay safe,</p>
<p>The TCS+ team</p>
<p><span class="css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0"><strong>Note:</strong> <em>you don’t need to sign up in advance</em> (the link will be made public on <a href="https://sites.google.com/site/plustcs/livetalk">our website</a></span><span class="css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0"> the day of the talk, and you can just join then). We only encourage you to do so in order for us to get a sense of the audience size, but it’s optional: don’t feel you have to plan ahead!</span></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/17/updated-spring-schedule-increased-talk-frequency-and-joining-as-individuals/"><span class="datestr">at March 18, 2020 12:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=741">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/03/12/1348-1665-2020/">1348, 1665, 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Besides unimaginable suffering and horror, the Black Death of the 1340’s also brought increased wages and better living standards.  It came back, among other times, in 1665.  Then like now, universities closed and students went home.  Among them was Newton, who spent his time alone in the countryside thus:</p>



<p><strong>In the beginning of the year 1665 I found the method of approximating series and the rule for reducing any dignity [power] of any binomial into such a series. The same year in May I found the method of tangents of Gregory and Slusius, and in November had the direct method of fluxions and the next year [1666] in January had the theory of colours and in May following I had entrance into the inverse method of fluxions. And the same year I began to think of gravity extending to the orb of the moon … All this was in the two plague years of 1665 and 1666, for in those days I was in the prime of my age for invention and minded Mathematics and Philosophy more than at any time since.</strong></p>



<p>Today’s Coronavirus pandemic is probably the first in history that’s been fought with telecommunication.  People are advised to work remotely, and many universities are switching to online courses.  Besides the suffering and horror, it is also an opportunity <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">to realize that many things can be done remotely just as well if not better, change our lifestyle, and stop polluting the environment.</a></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/03/12/1348-1665-2020/"><span class="datestr">at March 13, 2020 01:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7645">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/12/life-and-cs-theory-in-the-age-of-coronavirus/">Life and CS theory in the age of Coronavirus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Harvard University, as well most other places that I know of will be moving to remote lectures. I just gave the last in-person lecture in my <a href="https://cs127.boazbarak.org/schedule/">cryptography course</a>. I would appreciate technical suggestions on the best format for teaching remotely. At the moment I plan to use Zoom and log-in from both my laptop (for the video) and from my iPad pro (for the interactive whiteboard).  </p>



<p>The one silver lining is that more lectures will be available online. In particular, if you’re teaching algorithms, you might find <a href="https://gt-algorithms.com/">Eric Vigoda’s videos helpful</a>. (If you know of more sources, please let us know.)</p>



<p>I was hoping that reducing meetings and activities will be good for research, but currently find it hard to concentrate on anything except this train-wreck of a situation. The <a href="https://www.ft.com/content/ff3affea-63c7-11ea-b3f3-fe4680ea68b5">financial times</a> has a chart that summarizes the progress of the  disease in several countries:</p>



<figure class="wp-block-image size-large"><img src="https://windowsontheory.files.wordpress.com/2020/03/http___com.ft_.imagepublish.upp-prod-us.s3.amazonaws.png?w=700" alt="" class="wp-image-7646" /></figure>



<p>The number of confirmed cases grows by about 33% each day. This growth in confirmed cases is partially due to increased testing  as cases increase – there is <a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30260-9/fulltext">some evidence</a>  that the doubling time of the disease (time between <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> infected people to <img src="https://s0.wp.com/latex.php?latex=2n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2n" class="latex" title="2n" /> infected) is about 6 days (rather than the 2.4 days that this figure suggest). However, a doubling time of 6 days still means that the number of cases grows 10-fold in a month, and so if there are 10K actual cases in the U.S., today, there would be 100K by mid April and 1M by mid May.</p>



<p></p>



<p>Strong quarantine regimes, contact tracing, and drastically reducing activity and increasing “social distance” can very significantly reduce the base of this exponent. Reducing the base of the exponent is more than simply “delaying the inevitable”. The mortality statistics mask the fact that this can be a very serious illness even for the people who don’t die of it – about 5% of the cases need intensive care (see this <a href="https://www.economist.com/united-states/2020/03/12/covid-19-is-rapidly-spreading-in-america-the-country-does-not-look-ready">Economist article</a>). Spreading the infections over time will enable the healthcare system to handle the increased caseload, which will completely overwhelm it otherwise.</p>



<p>Such steps are clearly much easier to achieve before the number of cases is too large to be manageable, but despite having “advance warning” from other countries, this lesson does not seem at the moment to have sunk in, at least here in the U.S. At the moment no such initiatives are taken at the federal level, the states are doing more but still not enough, and it’s up to private companies and institutions to come up with their own policies. As faculty and citizens there is not much we can do about it except support such decisions even when they are <a href="https://www.thecrimson.com/article/2020/3/12/parents-petition-coronavirus-measure/">unpopular</a>, and just try to make the remote experience as good as possible for us, our colleagues, and our students.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/12/life-and-cs-theory-in-the-age-of-coronavirus/"><span class="datestr">at March 12, 2020 08:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/">Workshop in Quantum Information, Complexity &amp; Cryptography</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 11-12, 2020 University of York, UK https://sites.google.com/york.ac.uk/quicc/quicc We are organising a two-day event at the University of York in collaboration with York Interdisciplinary Centre for Cyber Security, which brings researchers in Quantum Information, Complexity and Cryptography together. The goal is to cover recent topics in these areas and facilitate interactions between them. The event … <a href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/" class="more-link">Continue reading <span class="screen-reader-text">Workshop in Quantum Information, Complexity &amp; Cryptography</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/"><span class="datestr">at March 10, 2020 05:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/03/10/cacm-through-the-lens-of-a-passionate-theoretician/">CACM – Through the Lens of a Passionate Theoretician</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Spending time in insulation? Nothing more to watch on Netflix? Looking for something to read instead? <a href="https://cacm.acm.org/magazines/2020/3/243025-through-the-lens-of-a-passionate-theoretician/fulltext">My CACM article</a>, about <a href="https://www.math.ias.edu/avi/">Avi Wigderson</a>‘s <a href="https://www.math.ias.edu/avi/book">excellent book</a> and about the reach of computation appeared recently. Enjoy!</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/03/10/cacm-through-the-lens-of-a-passionate-theoretician/"><span class="datestr">at March 10, 2020 04:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=395">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/04/tcs-talk-wednesday-march-11-thomas-steinke-ibm-research-almaden/">TCS+ talk: Wednesday, March 11 — Thomas Steinke, IBM Research Almaden</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Thomas Steinke</strong> from IBM Research Almaden will speak about “<em>Reasoning About Generalization via Conditional Mutual Information</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. In view of the recent travel restrictions and coronavirus precautions, in particular, do not hesitate to reserve a seat even for a group <i class="moz-txt-slash">of size one</i>: there should be enough room for everyone, so don’t be shy!</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.</p>
<p>Based on joint work with Lydia Zakynthinou.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/04/tcs-talk-wednesday-march-11-thomas-steinke-ibm-research-almaden/"><span class="datestr">at March 05, 2020 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/03/02/conferences-in-an-era-of-expensive-carbon/">Conferences in an Era of Expensive Carbon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>At least there’s that: I live in a world where some people care about it and publish their <a href="https://cacm.acm.org/magazines/2020/3/243024-conferences-in-an-era-of-expensive-carbon/abstract">viewpoint in the latest CACM</a>. Read it on your next flight.  Some interesting things that won’t shock anyone:</p>



<p>There’s a nice picture with different environmental costs based on the location of the conference.  It also shows that people like to go to nearby conferences, one of the reasons why “The impulse to ignore the issue is entirely understandable.”  For more perspective see some of our earlier posts for example <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">here</a> and <a href="https://emanueleviola.wordpress.com/2020/01/01/publish-and-perish/">here</a>.</p>



<p>The viewpoint also reports on a recent switch from in-person to online program committees for flagship conferences (POPL and ICFP), following a recent trend.  For starters we continue to suggest that <a href="https://emanueleviola.wordpress.com/2017/07/28/stocfocs-pc-meetings-does-nature-of-decisions-justify-cost/">STOC and FOCS do the same, because the nature of decisions does not justify the cost.</a> The latter post also includes hard numbers on the added value of a physical meeting (with respect to accept/reject decisions — of course one can value at infinity meeting in person luminaries in your field, but that can be done in other ways and should not be tied to PC meetings).</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/03/02/conferences-in-an-era-of-expensive-carbon/"><span class="datestr">at March 02, 2020 03:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/">Prague Summer School on Discrete Mathematics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 24-28, 2020 Prague, Czech Republic http://pssdm.math.cas.cz/ Registration deadline: March 22, 2020 The third edition of Prague Summer School on Discrete Mathematics will feature two lecture series: Subhash Khot (New York University): Hardness of Approximation: From the PCP Theorem to the 2-to-2 Games Theorem, and Shayan Oveis Gharan (University of Washington): Polynomial Paradigm in Algorithm … <a href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/" class="more-link">Continue reading <span class="screen-reader-text">Prague Summer School on Discrete Mathematics</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/"><span class="datestr">at March 02, 2020 02:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2386">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/richardson-extrapolation/">On the unreasonable effectiveness of Richardson extrapolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month, I will follow up on <a href="https://francisbach.com/acceleration-without-pain/">last month’s blog post</a>, and describe classical techniques from numerical analysis that aim at accelerating the convergence of a vector sequence to its limit, by only combining elements of the sequence, and without the detailed knowledge of the iterative process that has led to this sequence. </p>



<p class="justify-text">Last month, I focused on sequences that converge to their limit exponentially fast (which is referred to as <em>linear</em> convergence), and I described <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) method</a>, the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, Anderson acceleration and its <a href="https://arxiv.org/pdf/1606.04133">regularized version</a>. These methods are called “non-linear” acceleration techniques, because, although they combine linearly iterates as \(c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}\), the scalar weights in the linear combination depend non-linearly on \(x_k,\dots,x_{k+m}\).</p>



<p class="justify-text">In this post, I will focus on sequences that converge sublinearly, that is, with a difference to their limit that goes to zero as an inverse power of \(k\), typically in \(O(1/k)\). </p>



<h2>Richardson extrapolation</h2>



<p class="justify-text">We consider a sequence \((x_k)_{k \geq 0} \in \mathbb{R}^d\), with an asymptotic expansion of the form $$ x_k = x_\ast + \frac{1}{k}\Delta + O\Big(\frac{1}{k^2}\Big), $$ where \(x_\ast \in \mathbb{R}^d\) is the limit of \((x_k)_k\) and \(\Delta\) a vector in \(\mathbb{R}^d\).</p>



<p class="justify-text">The idea behind <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> [<a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">1</a>] is to combine linearly two iterates taken at two different values of \(k\) so that the zero-th order term \(x_\ast\) is left unchanged, but the first order term in \(1/k\) cancels out. For \(k\) even, we can consider $$  2 x_k – x_{k/2} =  2 \Big( x_\ast + \frac{1}{k} \Delta  +O\Big(\frac{1}{k^2}\Big)  \Big) \, – \Big( x_\ast +  \frac{2}{k} \Delta  + O\Big(\frac{1}{k^2}\Big) \Big)  =  x_\ast +O\Big(\frac{1}{k^2}\Big).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><a href="https://arxiv.org/pdf/1707.06386"><img src="https://francisbach.com/wp-content/uploads/2020/02/reg_k-1024x454.png" alt="" width="404" class="wp-image-2421" height="179" /></a>Illustration of Richardson extrapolation. Iterates (in black) with their first-order expansions (in red). The deviations (represented by circles) are of order \(O(1/k^2)\). Adapted from [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="https://arxiv.org/pdf/2002.02835">2</a>].  </figure></div>



<p class="justify-text">The key benefit of Richardson extrapolation is that we only need to know that the leading term in the asymptotic expansion is proportional to \(1/k\), <em>without the need to know the vector \(\Delta\)</em>. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/richardson_2d.gif" alt="" width="332" class="wp-image-2481" height="280" />Richardson extrapolation in two dimensions. The sequence is of the form \(x_k = \frac{1}{k} \Delta_1 + \frac{(-1)^k}{k^2} \Delta_2\). The extrapolated sequence \(2 x_k – x_{k/2}\) is only plotted for \(k\) even.</figure></div>



<p class="justify-text">In this post, following [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], I will explore situations where Richardson extrapolation can be useful within machine learning. I identified three situations where Richardson extrapolation can be useful (there are probably more):</p>



<ol class="justify-text"><li>Iterates of an optimization algorithms \((x_k)_{k \geq 0}\), and the extrapolation is \( 2x_k – x_{k/2}.\)</li><li>Extrapolation on the step-size of stochastic gradient descent, where we will combine iterates obtained from two different values of the step-size.</li><li>Extrapolation on a regularization parameter.</li></ol>



<p class="justify-text">As we will show, extrapolation techniques come with no significant loss in performance, but in several situations strong gains. It is thus “<a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences">unreasonably effective</a>“.</p>



<h2>Application to optimization algorithms</h2>



<p class="justify-text">We consider an iterate \(x_k\) of an iterative optimization algorithm which is minimizing a function \(f\), thus converging to a global minimizer \(x_\ast\) of \(f\). Then so is \(x_{k/2}\), and thus also $$  x_k^{(1)} = 2x_k – x_{k/2}.$$ Therefore, performance is never significantly deteriorated (the risk is essentially to lose half of the iterations). The potential gains depend on the way \(x_k\) converges to \(x_\ast\). The existence of a convergence rate of the form \(f(x_k) -f(x_\ast) = O(1/k)\) or \(O(1/k^2)\) is not enough, as Richardson extrapolation requires a specific direction of asymptotic convergence. As illustrated below, some algorithms are oscillating around their solutions, while some converge with a specific direction. Only the latter ones can be accelerated with Richardson extrapolation, while the former ones are good candidates for <a href="https://francisbach.com/acceleration-without-pain/">Anderson acceleration</a>.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/nonoscillating_oscillating-1024x350.png" alt="" width="476" class="wp-image-2395" height="162" /> Left: Oscillating convergence, where Richardson extrapolation does not lead to any gain. Right: non-oscillating  convergence, with a main direction \(\Delta\) (in red dotted), where Richardson extrapolation can be beneficial if the oscillations orthogonal to the direction \(\Delta\) are negligible compared to convergence along the direction \(\Delta\). </figure></div>



<p class="justify-text"><strong>Averaged gradient descent.</strong> We consider the usual gradient descent algorithm $$x_k = x_{k-1} – \gamma f'(x_{k-1}),$$ where \(\gamma &gt; 0 \) is a step-size, with Polyak-Ruppert averaging [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>]: $$ y_k = \frac{1}{k} \sum_{i=0}^{k-1} x_i.$$ Averaging is key to robustness to potential noise in the gradients [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>]. However it comes with the unintended consequence of losing the exponential forgetting of initial conditions for strongly-convex problems [<a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">6</a>].</p>



<p class="justify-text">A common way to restore exponential convergence (up to the noise level in the stochastic case) is to consider “tail-averaging”, that is, to replace \(y_k\) by the average of only the latest \(k/2\) iterates [<a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">7</a>]. As shown below for \(k\) even, this corresponds exactly to Richardson extrapolation on the sequence \((y_k)_k\): $$ \frac{2}{k} \sum_{i=k/2}^{k-1} x_i = \frac{2}{k} \sum_{i=0}^{k-1} x_i – \frac{2}{k} \sum_{i=0}^{k/2-1} x_i = 2 y_k – y_{k/2}. $$</p>



<p class="justify-text">With basic  assumptions on \(f\), it is shown in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>] that for locally strongly-convex problems: $$y_k = x_\ast + \frac{1}{k} \Delta + O(\rho^k), $$ where  \(\displaystyle \Delta = \sum_{i=0}^\infty (x_i – x_\ast)\) and \(\rho \in (0,1)\) depends on the condition number of \(f”(x_\ast)\). This is illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/averaged_gradient.png" alt="" width="342" class="wp-image-2507" height="250" />Averaged gradient descent on a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem in dimension \(d=400\), and with \(n=4000\) observations. For the regular averaged recursion, the line in the log-log plot has slope \(-2\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].</figure></div>



<p class="justify-text">We can make the following observations:</p>



<ul class="justify-text"><li>Before Richardson extrapolation, the asymptotic convergence rate after averaging is of order \(O(1/k^2)\), which is better than the usual \(O(1/k)\) upper-bound for the rate of gradient descent, but with a stronger assumption that in fact leads to exponential convergence before averaging.</li><li>While \(\Delta\) has a simple expression, it cannot be computed in practice (but Richardson extrapolation does not need to know it).</li><li>Richardson extrapolation leads to an exponentially convergent algorithm from an algorithm converging asymptotically in \(O(1/k^2)\).</li></ul>



<p class="justify-text"><strong>Accelerated gradient descent.</strong> Above, we considered averaged gradient descent, which is asymptotically converging as \(O(1/k^2)\), and on which Richardson extrapolation could be used with strong gains. Is it possible also for the accelerated gradient descent method [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">8</a>], which has a (non-asymptotic) convergence rate of \(O(1/k^2)\) for convex functions?</p>



<p class="justify-text">It turns out that the behavior of the iterates of accelerated gradient descent is exactly of the form depicted in the left plot of the figure above: that is, the iterates \(x_k\) oscillate around the optimum [<a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">9</a>, <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">10</a>], and Richardson extrapolation is of no help, but is not degrading performance too much. See below for an illustration. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/accelerated_gradient.png" alt="" width="332" class="wp-image-2509" height="243" />Accelerated gradient descent on a quadratic optimization problem in dimension \(d=1000\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].  </figure></div>



<p class="justify-text"><strong>Other algorithms.</strong> It is tempting to test it on other optimization algorithms. For example, as explained in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], Richardson extrapolation can be used to the <a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe</a> algorithm, where sometimes it helps, sometimes it doesn’t. Others could be tried.</p>



<h2>Extrapolation on the step-size of stochastic gradient descent</h2>



<p class="justify-text">While above we have focused on Richardson extrapolation applied to the number of iterations of an iterative algorithm, it is most often used in integration methods (for computing integrals or solving ordinary differential equations), and then often referred to as <a href="https://en.wikipedia.org/wiki/Romberg%27s_method">Romberg-Richardson extrapolation</a>. Within machine learning, in a similar spirit, this can be applied to the step-size of stochastic gradient descent [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">11</a>], which I now describe.</p>



<p class="justify-text">We consider the minimization of a function \(F(x)\) defined on \(\mathbb{R}^d\), which can be written as an expectation as $$F(x) = \mathbb{E}_{z} f(x,z).$$ We assume that we have access to \(n\) independent and identically distributed observations (i.i.d.) \(z_1,\dots,z_n\). This is a typical scenario in machine learning, where \(f(x,z)\) represents the loss for the predictor parameterized by \(x\) on the observation \(z\). </p>



<p class="justify-text">The stochastic gradient method is particularly well adapted, and we consider here a single pass, as $$x_i= x_{i-1} – \gamma f'(x_{i-1},z_i),$$ where the gradient is taken with respect to the first variable, for \(i = 1,\dots,n\). It is known that with a constant step-size, when \(n\) tends to infinity, \(x_n\) will <em>not</em> converge to the minimizer \(x_\ast\) of \(F\), as the algorithm always moves [<a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">16</a>], as illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/logistic_2d-1.gif" alt="" width="403" class="wp-image-2488" height="279" />Stochastic gradient descent on a logistic regression problem: (blue) without averaging, (red) with averaging.</figure></div>



<p class="justify-text">One way to damp the oscillations is to consider averaging, that is, $$ y_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i$$ (we consider uniform averaging for simplicity). For least-squares regression, this leads to a converging algorithm [<a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">12</a>] with attractive properties for ill-conditioned problems (see also <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/">January’s blog post</a>). However, for general loss functions, it is shown in [<a href="https://arxiv.org/pdf/1707.06386">3</a>] that \(y_n\) converges to some \(y^{(\gamma)} \neq x_\ast\). There is a bias due to a step-size \(\gamma\) that does not go to zero. In order to apply Richardson extrapolation, together with Aymeric Dieuleveut and Alain Durmus [<a href="https://arxiv.org/pdf/1707.06386">3</a>], we showed that $$ y^{(\gamma)} = x_\ast + \gamma \Delta + O(\gamma^2),$$ for some \(\Delta \in \mathbb{R}^d\) with some complex expression. Thus, we have $$2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2),$$ thus gaining one order. If we consider the iterate \(y_n^{(\gamma)}\) and \(y_n^{(2 \gamma)}\) associated to the two step-sizes \(\gamma\) and \(2 \gamma\), the linear combination $$2 y_n^{(\gamma)} – y_n^{(2\gamma)} $$ has an improved behavior as it tends to \(2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2)\): it remains not convergent, but get to way smaller values. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/SGD_logistic-1.png" alt="" width="359" class="wp-image-2525" height="274" />Averaged stochastic gradient descent on a logistic regression problem in dimension 20.</figure></div>



<p class="justify-text"><strong>Higher-order extrapolation.</strong> If we can accelerate a sequence by extrapolation, why not extrapolate the extrapolated sequence? This is possible if we have an higher-order expansion of the form $$ y^{(\gamma)} = \theta_\ast + \gamma \Delta_1 + \gamma^2 \Delta_2 + O(\gamma^3),$$ for some (typically unknown) vectors \(\Delta_1\) and \(\Delta_2\). Then, the sharp reader can check that $$3 y_n^{(\gamma)} – 3 y_n^{(2\gamma)} +  y_n^{(3\gamma)}, $$ will lead to cancellation of the first two orders \(\gamma\) and \(\gamma^2\). This is illustrated above for SGD.</p>



<p class="justify-text">Then, why not extrapolate the extrapolation of the extrapolated sequence? One can check that $$4 y_n^{(\gamma)} – 6 y_n^{(2\gamma)} + 4  y_n^{(3\gamma)}  -y_n^{(4\gamma)}, $$ will lead to cancellation of the first three orders of an expansion of \(y^{(\gamma)}\). The <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> aficionados have already noticed the pattern there, and checked that $$ \sum_{i=1}^{m+1} (-1)^{i-1} { m+1 \choose i} y_n^{(i\gamma)}$$ will lead to cancellations of the first \(m\) orders.</p>



<p class="justify-text">Then, why not go on forever? First because \(m+1\) recursions have to be run in parallel, and second, because the constant in front of the term in \(\gamma^{m+1}\) typically explodes, a phenomenon common to many expansion methods.</p>



<h2>Extrapolation on a regularization parameter</h2>



<p class="justify-text">We now explore the application of Richardson extrapolation to regularization methods. In a nutshell, regularization allows to make an estimation problem more stable (less subject to variations for statistical problems) or the algorithm faster (for optimization problems). However, regularization adds a bias that needs to be removed. In this section, we apply Richardson extrapolation to the regularization parameter to reduce this bias. I will only present an application to smoothing for non-smooth optimization (see an application to  ridge regression in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>]).</p>



<p class="justify-text"><strong>Non-smooth optimization problems</strong>. We consider the minimization of a convex function of the form \(f = h + g\), where \(h\) is smooth and \(g\) is non-smooth. These optimization problems are ubiquitous in machine learning and signal processing, where the lack of smoothness can come from (a) non-smooth losses such as max-margin losses used in support vector machines and more generally structured output classification [<a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">13</a>], and (b) sparsity-inducing regularizers (see, e.g., [<a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">14</a>] and references therein). While many algorithms can be used to deal with this non-smoothness, we consider a classical smoothing technique below.</p>



<p class="justify-text"><strong>Nesterov smoothing</strong>. In this section, we consider the smoothing approach of Nesterov [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>] where the non-smooth term is “smoothed” into \(g_\lambda\), where \(\lambda\) is a regularization parameter, and accelerated gradient descent is used to minimize \(h+g_\lambda\). </p>



<p class="justify-text">A typical way of smoothing the function \(g\) is to add \(\lambda\) times a strongly convex regularizer (such as the squared Euclidean norm) to the Fenchel conjugate of \(g\); this leads to a function \(g_\lambda\) which has a smoothness constant (defined as the maximum of the largest eigenvalues of all Hessians) proportional to \(1/\lambda\), with a uniform error of \(O(\lambda)\) between \(g\) and \(g_\lambda\). Given that accelerated gradient descent leads to an iterate with excess function values proportional to \(1/(\lambda k^2)\) after \(k\) iterations, with the choice of \(\lambda \propto 1/k\), this leads to an excess in function values proportional to \(1/k\), which improves on the subgradient method which converges in \(O(1/\sqrt{k})\). Note that the amount of regularization depends on the number of iterations, so that this smoothing method is not “anytime”.</p>



<p class="justify-text"><strong>Richardson extrapolation.</strong> If we denote by \(x_\lambda\) the minimizer of \(h+g_\lambda\) and \( x_\ast\) the global minimizer of \( f=h+g\), if we can show that \( x_\lambda = x_\ast + \lambda \Delta + O(\lambda^2)\), then \( x^{(1)}_\lambda = 2 x_\lambda – x_{2\lambda} = O(\lambda^2)\) and we can expand \( f(x_\lambda^{(1)})  = f(x_\ast)  + O(\lambda^2)\), which is better than the \(O(\lambda)\) approximation without extrapolation. </p>



<p class="justify-text">Then, given a number of iterations \(k\), with \( \lambda \propto k^{-2/3}\), to balance the two terms \( 1/(\lambda k^2)\) and \( \lambda^2\),  we get an overall convergence rate for the non-smooth problem of \( k^{-4/3}\). </p>



<p class="justify-text"><strong>\(m\)-step Richardson extrapolation</strong>. Like above for the step-size, we can also consider \(m\)-step Richardson extrapolation \(x_{\lambda}^{(m)}\), which leads to a bias proportional to \(\lambda^{m+1}\). Thus, if we consider \(\lambda \propto 1/k^{2/(m+2)}\), to balance the terms \(1/(\lambda k^2)\) and \(\lambda^{m+1}\), we get an error for the non-smooth problem of \(1/k^{2(m+1)/(m+2)}\), which can get arbitrarily close to \(1/k^2\) when \(m\) gets large. The downsides (like for the extrapolation on the step-size above) are that (a) the constants in front of the asymptotic equivalent may blow up (a classical problem in high-order expansions), and (b) \(m\)-step extrapolation requires running the algorithm \(m\) times (this can be down in parallel). In the experiment below, 3-step extrapolation already brings in most of the benefits.</p>



<p class="justify-text">In order to experimentally study the benefits of extrapolation, for the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> optimization problem, and for a series of regularization parameters equal to \(2^{i}\) for \(i\) between \(-18\) and \(1\) (sampled every \(1/5\)), we run accelerated gradient descent on \(h+g_\lambda\) and we plot the value of \(f(x)-f(x_\ast)\) for the various estimates, where for each number of iterations, we minimize over the regularization parameter. This is an oracle version of varying \(\lambda\) as a function of the number of iterations. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/smoothing.png" alt="" width="351" class="wp-image-2530" height="255" />Excess function values as a function of the number of iterations, <em>taking into account that \(m\)-step Richardson extrapolation requires \(m\)-times more iterations</em>. There is indeed a strong improvement approaching the rate \(1/k^2\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">These last two blog posts were dedicated to acceleration techniques coming from numerical analysis. They are cheap to implement, typically do not interfere with the underlying algorithm, and when used in the appropriate situation, can bring in significant speed-ups.</p>



<p class="justify-text">Next month, I will most probably host an invited post by my colleague <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a>, who will explain how machines can <s>replace</s> help researchers that prove bounds on optimization algorithms.</p>



<h2>References</h2>



<p class="justify-text">[1] Lewis Fry Richardson. <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam</a>. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, 210(459-470):307–357, 1911.<br />[2] Francis Bach. <a href="https://arxiv.org/pdf/2002.02835">On the Effectiveness of Richardson Extrapolation in Machine Learning</a>. Technical report, arXiv:2002.02835, 2020.<br />[3] Aymeric Dieuleveut, Alain Durmus, Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains</a>. To appear in <em>The Annals of Statistics</em>, 2019.<br />[4] Boris T. Polyak,  Anatoli B. Juditsky. <a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">Acceleration of stochastic approximation by averaging</a>. <em>SIAM journal on control and optimization</em> 30(4):838-855, 1992.<br />[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>. </em><a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br />[6] Francis Bach, Eric Moulines. <a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. <em>Advances in Neural Information Processing Systems</em>, 2011.<br />[7] Prateek Jain, Praneeth Netrapalli, Sham Kakade, Rahul Kidambi, Aaron Sidford. <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</a>. <em>The Journal of Machine Learning Research</em>, 18(1), 8258-8299, 2017.<br />[8] Yurii E. Nesterov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">A method of solving a convex programming problem with convergence rate \(O(1/k^2)\)</a>, <em>Doklady Akademii Nauk SSSR</em>, 269(3):543–547, 1983.<br />[9] Weijie Su, Stephen Boyd, and Emmanuel J. Candes. <a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1):5312-5354, 2016.<br />[10] Nicolas Flammarion, and Francis Bach. <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">From Averaging to Acceleration, There is Only a Step-size</a>. <em>Proceedings of the International Conference on Learning Theory (COLT)</em>, 2015. <br />[11] Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, and Gaël Richard. <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">Stochastic gradient Richardson-Romberg Markov chain Monte Carlo</a>. In <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2016.<br />[12] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)</a>. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2013.<br />[13] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. <a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">Learning structured prediction models: A large margin approach</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2005.<br />[14] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. <a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. Foundations and Trends in Machine Learning, 4(1):1–106, 2012<br />[15] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming , 103(1):127–152, 2005.<br />[16] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">Stochastic minimization with constant step-size: asymptotic laws</a>. <em>SIAM Journal on Control and Optimization</em>, (24)4:655-666, 1986.</p>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/richardson-extrapolation/"><span class="datestr">at March 01, 2020 12:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=26">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/">Friday, February 28 — Jon Kleinberg from Cornell University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The first Foundations of Data Science virtual talk will take place this coming Friday, February 28th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC). <strong>Jon Kleinberg</strong> from Cornell University will speak about “<em>Fairness and Bias in Algorithmic Decision-Making</em>”.</p>



<p><strong>Abstract</strong>: As data science has broadened its scope in recent years, a number of domains have applied computational methods for classification and prediction to evaluate individuals in high-stakes settings. These developments have led to an active line of recent discussion in the public sphere about the consequences of algorithmic prediction for notions of fairness and equity. In part, this discussion has involved a basic tension between competing notions of what it means for such classifications to be fair to different groups. We consider several of the key fairness conditions that lie at the heart of these debates, and in particular how these properties operate when the goal is to rank-order a set of applicants by some criterion of interest, and then to select the top-ranking applicants. The talk will be based on joint work with Sendhil Mullainathan and Manish Raghavan.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/"><span class="datestr">at February 27, 2020 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/20/tcs-talk-wednesday-february-26-henry-yuen-university-of-toronto/">TCS+ talk: Wednesday, February 26 — Henry Yuen, University of Toronto</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, February 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Henry Yuen</strong> from University of Toronto will speak about “<em>MIP* = RE</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: MIP* denotes the class of problems that admit interactive proofs with quantum entangled provers. It has been an outstanding question to characterize the complexity of MIP*. Most notably, there was no known computable upper bound on this class.<br />
We show that MIP* is equal to the class RE, the set of recursively enumerable languages. In particular, this shows that MIP* contains uncomputable problems. Through a series of known connections, this also yields a negative answer to Connes’ Embedding Problem from the theory of operator algebras. In this talk, I will explain the connection between Connes’ Embedding Problem, quantum information theory, and complexity theory. I will then give an overview of our approach, which involves reducing the Halting Problem to the problem of approximating the entangled value of nonlocal games.<br />
Joint work with Zhengfeng Ji, Anand Natarajan, Thomas Vidick, and John Wright.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/20/tcs-talk-wednesday-february-26-henry-yuen-university-of-toronto/"><span class="datestr">at February 20, 2020 08:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=717">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">Working remotely will be the most significant transformation since agriculture</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Its impact on civilization will be exactly opposite.  Rather than concentrating population, it will disperse it.  Commuting and the traffic crisis will disappear.  So will the housing crisis.  You will have a large lot of land with a robot-ready house built new with safe, eco-friendly material and free of hazardous substance.  You will live away from volcanoes, fault lines, tornadoes, wild fires and other hazards. You’ll be able to move to a location with ideal climate, which for historical reasons are now under-populated.  This will dramatically reduce housing costs, especially heating, and solve  or greatly mitigate the pollution problem. Huge amounts of space will be cleared up and given back to nature, or used for housing.</p>



<p>Doctors will visit patients remotely.  This will enable patients to be followed up more regularly and consistently throughout their lives regardless of where they are.  Doctors will have more time to give meaningful advice rather than having the patient wait 1 year for the appointment and then spend 1 hour to get to the doctor for a 10-minute visit of which 8 are spent looking at the screen and filling reports. Robo-tools will take measurements and send them to the doctor.  If a complicated procedure is required, the expert will connect with the patient and the doctor remotely first, and then the patient will schedule a trip for the procedure.</p>



<p>You’ll take gym classes remotely via a remote gym. The instructor will give you personalized advice and follow your progress anywhere, anytime. Demanding facilities like swimming pools will be next to your house.</p>



<p>Courts of law, and the entire judicial system will be taken off-line.</p>



<p>People will vote from home, elections will be more frequent and granular.  Constituents choosing not to vote will (maybe) have to specifically abstain.  This will finally realize the democratic ideal where the government represents the will of the people.</p>



<p>Constituents will be able to participate to discussions, instead of having to travel 1 hour for a 5-minute in-person discussion.  The level of engagement will be measured by the level of engagement as opposed to travel distance.</p>



<p>Wireless won’t be used on a large scale, since its noxious effects will be undeniable. Instead we will have network cables densely spread out over the earth — one of the few duties of the government will be to maintain these cables for the free, democratic, public use.</p>



<p>Banking will be done remotely, and physical money will disappear.</p>



<p>We will have immersive work-stations with wall-to-wall, solar-powered e-ink screens, holographic images, and audio indistinguishable from reality.  You will be able to attend meetings while exercising, like walking or biking on a machine or outside.  This will boost your health, lowering health care costs for all.</p>



<p>People with special needs will have the same opportunities and duties as everyone else and will be fully integrated.</p>



<p>All learning will be done remotely.  The instructor will be able to provide better, more personalized teaching, and connect with each student face-to-face.  Testing will be done remotely, each student monitored via cameras.  Critical examinations will be administered in special-purpose facilities which are next to your house (similar in spirit to say the way GRE is administered, but much more large scale and flexible, including for example synchronized examination).</p>



<p>You will have farms next to your house, growing organic food that you can eat fresh. Epidemics will be much rarer and more easily controlled, as population will be less concentrated and will travel less.</p>



<p>Fantasy? Actually, many of these things are already happening!</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/"><span class="datestr">at February 18, 2020 08:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1555">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/02/11/approx-random-2020/">APPROX-RANDOM 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CFPs for <a href="https://approxconference.wordpress.com/approx-2020/">APPROX 2020</a> and <a href="https://randomconference.com/random-2020-home/">RANDOM 2020</a> are out. The conference will be held August 17-19, 2020 at the University of Washington in Seattle. <strong>Submissions:</strong> April 24, 2020.</p>
<p> </p>
<p> </p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/02/11/approx-random-2020/"><span class="datestr">at February 12, 2020 06:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=40">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2020/02/08/icalp-and-lics-2020-relocation-and-extended-deadline/">ICALP (and LICS) 2020 – Relocation and Extended Deadline</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Due to the Wuhan coronavirus outbreak, the organizers of ICALP and LICS have made the difficult decision to relocate both (co-located) conferences from Beijing, China, to Saarbrücken, Germany. Speaking specifically about ICALP now (I do not have further information about LICS): As a result of previous uncertainty regarding the situation, the deadline has been extended by about six days, until Tuesday February 18, 2020, at 6 AM GMT. The dates of the conference remain (roughly) the same, July 8 – 11, 2020. <br />The following is a more official message from ICALP Track A Chair, Artur Czumaj.</p>



<hr class="wp-block-separator" />



<p>The ICALP and the LICS steering committee have agreed together with the conference chairs in Beijing to relocate the two conferences.<br />ICALP and LICS 2020 will take place in <strong>Saarbrücken</strong>, Germany, July 8 – 11 2020 (with satellite workshops on July 6 – 7 2020).<br />The deadline is extended, see below.</p>



<p><strong>Call for Papers – ICALP 2020</strong><br /><strong>July 8 – 11 2020, Saarbrücken, Germany</strong></p>



<p><strong>NEW Paper submission deadline: Tuesday February 18, 2020, 6am GMT</strong><br /><a href="https://easychair.org/conferences/?conf=icalp2020">https://easychair.org/conferences/?conf=icalp2020</a></p>



<p>ICALP (International Colloquium on Automata, Languages and Programming) is the main European conference in Theoretical Computer Science and annual meeting of the European Association for Theoretical Computer Science (EATCS). ICALP 2020 will be hosted on the Saarland Informatics Campus in Saarbrücken, in co-location with LICS 2020 (ACM/IEEE Symposium on Logic in Computer Science).</p>



<p><strong>Invited speakers:</strong><br />Track A: Virginia Vassilevska (MIT), Robert Krauthgamer (Weizmann)<br />Track B: Stefan Kiefer (Oxford)<br />Joint ICALP-LICS: Andrew Yao (Tsinghua), Jérôme Leroux (Bordeaux)</p>



<p><strong>Submission Guidelines:</strong> see <a href="https://easychair.org/conferences/?conf=icalp2020">https://easychair.org/conferences/?conf=icalp2020</a></p>



<p><strong>NEW Paper submission deadline: February 18</strong>, 2020, 6am GMT<br />notifications: April 15, 2020<br />camera ready: April 28, 2020</p>



<p>Topics: ICALP 2020 will have the two traditional tracks<br />A (Algorithms, Complexity and Games – including Algorithmic Game Theory, Distributed Algorithms and Parallel, Distributed and External Memory Computing) and<br />B (Automata, Logic, Semantics and Theory of Programming).<br /><strong><em>    (Notice that the old tracks A and C have been merged into a single track A.)</em></strong><br />Papers presenting original, unpublished research on all aspects of theoretical computer science are sought.</p>



<p>Typical, but not exclusive topics are:</p>



<p>Track A — Algorithmic Aspects of Networks and Networking, Algorithms for Computational Biology, Algorithmic Game Theory, Combinatorial Optimization, Combinatorics in Computer Science, Computational Complexity, Computational Geometry, Computational Learning Theory, Cryptography, Data Structures, Design and Analysis of Algorithms, Foundations of Machine Learning, Foundations of Privacy, Trust and Reputation in Network, Network Models for Distributed Computing, Network Economics and Incentive-Based Computing Related to Networks, Network Mining and Analysis, Parallel, Distributed and External Memory Computing, Quantum Computing, Randomness in Computation, Theory of Security in Networks</p>



<p>Track B — Algebraic and Categorical Models, Automata, Games, and Formal Languages, Emerging and Non-standard Models of Computation, Databases, Semi-Structured Data and Finite Model Theory, Formal and Logical Aspects of Learning, Logic in Computer Science, Theorem Proving and Model Checking, Models of Concurrent, Distributed, and Mobile Systems, Models of Reactive, Hybrid and Stochastic Systems, Principles and Semantics of Programming Languages, Program Analysis and Transformation, Specification, Verification and Synthesis, Type Systems and Theory, Typed Calculi</p>



<p><strong>PC Track A chair: Artur Czumaj</strong> (University  of Warwick)<br /><strong>PC Track B chair: Anuj Dawar</strong> (University of Cambridge)</p>



<p>Contact<br />All questions about submissions should be emailed to the PC Track chairs:<br />Artur Czumaj <a href="mailto:A.Czumaj@warwick.ac.uk">A.Czumaj@warwick.ac.uk&lt;mailto:A.Czumaj@warwick.ac.uk&gt;</a><br />Anuj Dawar <a href="mailto:Anuj.Dawar@cl.cam.ac.uk">Anuj.Dawar@cl.cam.ac.uk&lt;mailto:Anuj.Dawar@cl.cam.ac.uk&gt;</a></p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2020/02/08/icalp-and-lics-2020-relocation-and-extended-deadline/"><span class="datestr">at February 08, 2020 03:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=387">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/06/tcs-talk-wednesday-february-12-albert-atserias-universitat-politecnica-de-catalunya/">TCS+ talk: Wednesday, February 12 — Albert Atserias, Universitat Politecnica de Catalunya</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, February 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Albert Atserias</strong> from Universitat Politecnica de Catalunya will speak about “<em>Automating Resolution is NP-Hard</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We show that it is NP-hard to distinguish CNF formulas that have Resolution refutations of almost linear length from CNF formulas that do not even have weakly exponentially long ones. It follows from this that Resolution is not automatable in polynomial time unless P = NP, or in weakly exponential time unless ETH fails. The proof of this is simple enough that all its ideas can be explained in a talk. Along the way, I will try to explain the process of discovery that led us to the result. This is joint work with Moritz Müller.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/06/tcs-talk-wednesday-february-12-albert-atserias-universitat-politecnica-de-catalunya/"><span class="datestr">at February 06, 2020 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5488">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/02/04/an-algorithms-course-with-minimal-prerequisites/">An Algorithms Course with Minimal Prerequisites</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
There are amazing materials for teaching theoretical algorithms courses: excellent books, lecture notes, and online courses. But none of the resources I am familiar with fits the algorithms course I was supposed to prepare. I wanted to teach a course for students who hardly have any prerequisites. My students are non-CS majors (mostly math majors), […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/02/04/an-algorithms-course-with-minimal-prerequisites/"><span class="datestr">at February 04, 2020 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2109">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/acceleration-without-pain/">Acceleration without pain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">I don’t know of any user of iterative algorithms who has not complained one day about their convergence speed. Whether the data are too big, the processors not fast or numerous enough, waiting for an algorithm to converge unfortunately remains a core practical component of computer science and applied mathematics. This was already a concern long before computers were invented (and most of the techniques I will describe date back to the early 19th century): imagine you are doing all the operations (multiplications, additions, divisions) by hand, wouldn’t you want some cheap way to accelerate your algorithm (and here literally reduce your pain)?</p>



<p class="justify-text">Acceleration is a key concept in numerical analysis and can be carried through in two main ways. The first way is to modify some steps of the algorithm (such as Nesterov acceleration for gradient descent, or <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> / <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> acceleration for linear recursions). This requires a good knowledge of the inner structure of the underlying algorithm. A second way is to totally ignore the specifics of the algorithm, and see the acceleration problem as trying to find good “combinations” of the observed iterates that converge faster.</p>



<p class="justify-text">In this blog post, I thus consider a sequence of iterates \((x_k)_{k \geq 0}\) in \(\mathbb{R}^d\) obtained from an iterative algorithm \(x_{k+1} = T(x_k)\), which will typically be an optimization algorithm. The main question I will address is: Can we do better than outputting the last iterate?</p>



<p class="justify-text">This has a long history in numerical analysis, where many techniques have been developed for uni-dimensional sequences. Acceleration techniques vary according to the <a href="https://en.wikipedia.org/wiki/Rate_of_convergence">type of convergence</a> of the original sequence (quadratic, linear, sublinear), the amount of knowledge about the asymptotic behavior, and the possibility of extensions to high-dimensional and noisy problems.</p>



<p class="justify-text">Acceleration techniques are often based on an explicit or implicit modelling of the sequence \(x_k\), either through a model of the function \(T: \mathbb{R}^d \to \mathbb{R}^d\) (the iteration of the algorithm) or through an asymptotic expansion of \(x_k\). In this post, I will focus on linearly convergent sequences, that is, sequences \(x_k\) converging to some \(x_\ast\) at an exponential rate. As we will see, this will done through modelling \(x_k\) as an autoregressive process.</p>



<p class="justify-text">I will first start from the simplest scheme, the <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) process</a> from 1926 [1], then look at higher order generalizations still in one dimension, and finally to the general vector case. We will then apply all that to gradient descent.</p>



<h2>Aitken’s \(\Delta^2\) process</h2>



<p class="justify-text">This is the simplest of all techniques and the source of all others in this post. We try to model \(x_k\in \mathbb{R}\) as first-order auto-regressive sequence, that is, \(x_{k+1} = ax_{k}+b\), for \(a,b \in \mathbb{R}\). The method works by (a) estimating \(a\) and \(b\) from a sequence of few consecutive (here three) iterates, and (b) extrapolating by computing the limit \(x_{\rm acc}\) of the estimated model. Given that we fit the model to consecutive iterates \((x_k,x_{k+1},x_{k+2})\), the model \((a,b)\) will also depend on \(k\), as well as its limit \(x_{\rm acc}\). In order to avoid having too many \(k\)’s in my notations, I will drop the dependence in \(k\) of the model parameters.</p>



<p class="justify-text">In this situation, the model recursion has a limit when \(a \neq 1\), and the limit is \(x_{\rm acc} =  \frac{b}{1-a}\). In order to fit the two parameters, we need two equations, which can be obtained by considering two consecutive evaluations of the recursions (which require three iterates). That is, we consider the linear system in \((a,b)\): $$ \Big\{ \begin{array}{ll} ax_{k}+b  &amp; = x_{k+1} \\    ax_{k+1}+b &amp; = x_{k+2} \end{array}$$ which can be solved in a variety of ways. All of them are equivalent, but naturally lead to different extensions.</p>



<p class="justify-text"><strong>Solving by elimination.</strong> We can eliminate \(b\) by subtracting the two equations, leading to $$ x_{k+2}  – x_{k+1} = a ( x_{k+1} – x_{k}),$$  and thus $$a = \frac{ x_{k+2}  – x_{k+1}}{ x_{k+1} – x_{k}}.$$ We then get $$b = x_{k+1} – a x_{k} = x_{k+1} –  \frac{ x_{k+2}  – x_{k+1}}{ x_{k+1} – x_{k}} x_{k} =  \frac{  x_{k+1}^2 – x_{k} x_{k+2}}{ x_{k+1} – x_{k}}, $$ and the extrapolating sequence $$x_{\rm acc} = \frac{b}{1-a} = \frac{x_{k} x_{k+2} – x_{k+1}^2 }{-2 x_{k+1} + x_{k+2} + x_{k}},$$ which we denote \(x^{(1)}_k\), to highlight its dependence on \(k\). Note that to compute \(x^{(1)}_k\), we need access to the three iterates \((x_k,x_{k+1},x_{k+2})\), and thus, when comparing the original sequence to the extrapolated one, we will compare \(x_k\) and \(x^{(1)}_{k-2}\).</p>



<p class="justify-text"><strong>Asymptotic auto-regressive model</strong>. A key feature of the acceleration techniques that I describe in this post is that although they implicitly or explicitly model sequence with auto-regressive processes, the models do not need to be correct, that is, they also work if the autoregressive recursion is true only asymptotically, for example \(\displaystyle \frac{x_{k+1}-x_\ast}{x_k – x_\ast}\) converging to a constant \(a \in [-1,1)\). Then we also get some acceleration, which can be quantified (see the end of the post for details), and for which we present a classical example below.</p>



<p class="justify-text"><strong>Approximating \(\pi\).</strong> We consider the <a href="https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80">Leibniz formula</a>, which is one of many ways of <a href="https://en.wikipedia.org/wiki/Approximations_of_%CF%80">approximating \(\pi\)</a>: $$ \pi = \lim_{k \to +\infty} x_k  \mbox{ with } x_k = 4 \sum_{k=0}^{+\infty} \frac{(-1)^k}{2k+1}.$$ This formula can be proved by expanding the derivative \(x \mapsto \frac{1}{1+x^2}\) of \(x \mapsto \arctan x\) as a power series and then integrating it. We can check that $$\frac{x_{k+1}-x_\ast}{x_k – x_\ast} =  \ – 1 + \frac{1}{k} + o( \frac{1}{k}), $$ and as detailed at the end of the post, we should expect the error to go from \(1/k\) to \(1/k^3\).  Below, we show the first 10 iterates of the two sequences, with the correct significant digits in bold. $$ \begin{array}{|l|l|l|} \hline k &amp; x_k &amp;  x_k^{(1)} \\ \hline  1  &amp;   4.0000   &amp;  \times \\      2  &amp;  2.6667     &amp;    \times \\  3  &amp;  \mathbf{3}.4667   &amp; \mathbf{3.1}667 \\   4 &amp;   2.8952  &amp;  \mathbf{3.1}333 \\     5  &amp;  \mathbf{3}.3397  &amp;  \mathbf{3.14}52 \\  6 &amp;   2.9760 &amp;   \mathbf{3.1}397 \\   7  &amp;  \mathbf{3}.2837  &amp;  \mathbf{3.14}27 \\   8  &amp;  \mathbf{3}.0171  &amp;  \mathbf{3.14}09 \\  9  &amp;  \mathbf{3}.2524  &amp;  \mathbf{3.14}21 \\   10  &amp;  \mathbf{3}.0418 &amp;  \mathbf{3.141}3 \\ \hline \end{array}$$ We see that the extrapolated sequence converges much faster. This is confirmed in the convergence plot below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/01/deltasquared.png" alt="" width="335" class="wp-image-2148" height="283" />\(\Delta^2\) method on the Leibniz series.  Notice the improvement from \(O(1/k)\) to \(O(1/k^3)\).</figure></div>



<h2>Higher-order one-dimensional extensions</h2>



<p class="justify-text">The Aitken’s \(\Delta^2\) process relies on fitting a first-order auto-regressive model, or on assuming that \(x_{k+1} – x_\ast – a (x_k – x_\ast) \to 0\) asymptotically. This can be extended to \(m\)-th order constant recursions. This corresponds to modelling \(x_k\) as the sum of \(m\) exponentials.</p>



<p class="justify-text">We thus try to fit the model $$x_{k+m} = a_0 x_k + a_1 x_{k+1} + \cdots + a_{m-1} x_{k+m-1} + b = \sum_{i=0}^{m-1} a_i x_{k+i} + b, $$ which has \(m+1\) parameters. We thus need \(m + 1\) equations, that is we consider the recursion for \(k, k+1,\dots, k+m\), which requires the knowledge of the \(2m+1\) iterates \(x_k, x_{k+1},\dots,x_{k+2m}\). This leads to the \(m+1\) equations: $$x_{k+m+j} = a_0 x_{k+j} + a_1 x_{k+j+1} + \cdots + a_{m-1} x_{k+j+m-1} + b = \sum_{i=0}^{m-1} a_i x_{k+j+i} + b,$$ for \(j \in \{0,\dots,m\}\). This is a system with \(m+1\) unknowns and \(m+1\) equations, from which we could get all \(a_j\)’s and \(b\), and then the model limit as the extrapolated sequence \(x^{(m)}_k = \frac{b}{1 – a_0 – a_1 – \cdots – a_{m-1}}\). </p>



<p class="justify-text">This linear system can be solved in a variety of ways. At the end of the blog post, I show how it can be solved using determinants of Hankel-like matrices, often referred to as the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, which then leads to an iterative algorithm dating back from Wynn [2], which is called the <a href="https://fr.wikipedia.org/wiki/Epsilon_algorithme">\(\varepsilon\)-algorithm</a>. In order to smooth our way to the vector case extension, I will present it in a slightly non-standard way. See [3] for a detailed survey on acceleration and extrapolation.</p>



<p class="justify-text">Instead of learning the model parameters to estimate \(x_{k+m}\) from the past iterates, we focus directly on the prediction of the limit \(x_{\rm acc}\) by looking for real numbers \(c_0,\dots,c_m\) such that for all \(k\), $$\sum_{i=0}^m c_i ( x_{k+i} – x_{\rm acc} ) = 0,$$ with the arbitrary normalization \(\sum_{i=0}^m c_i = 1\). The \(c_i\)’s can be obtained from the \(a_i\)’s and \(b\) as \((c_0,c_1,\dots,c_{m-1},c_m)\propto (a_0,a_1,\dots,a_{m-1},-1)\). We then have $$x_{\rm acc} = \sum_{i=0}^m c_{i} x_{k+i}.$$ Again, the parameters \(c_i\)’s depend on \(k\), but we omit this dependence.</p>



<p class="justify-text">In order to estimate the \(m+1\) parameters \(c_0,\dots,c_m\), we subtract two versions of the equality for \(k\) and \(k+1\), leading to $$\sum_{i=0}^m c_i ( x_{k+1+i} – x_{k+i} ) = 0.$$ Defining the matrix \(U \in \mathbb{R}^{m \times (m+1)}\) by $$U_{ji} = x_{k+1+i+j} – x_{k+i+j},$$ for \(i \in \{0,\dots,m\}\) and \(j \in \{0,\dots,m-1\}\), we have $$ U c = 0. $$ Together with the constraint \(1_{m+1}^\top c = 1\), this leads to the correct number of equations to estimate \(c\), from \(2m+1\) iterates \(x_k,\dots,x_{k+2m}\). The extrapolated iterate \(x^{(m)}_k\) is then $$x^{(m)}_k = x_{\rm acc} =   \sum_{i=0}^m c_{i} x_{k+i}.$$ Note that the extrapolation is exact when the sequence is exactly following a \(m\)-th order recursion. See an example of application on the Leibniz formula below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/shanks_Uc.png" alt="" width="424" class="wp-image-2320" height="330" />Higher-order acceleration through the Shanks transformation for the Leibniz formula. Acceleration is possible only up to machine precision.</figure></div>



<h2>Extension to vectors</h2>



<p class="justify-text">We now consider accelerating vector sequences \(x_k \in \mathbb{R}^d\). There are multiple approaches to extend acceleration from real numbers to vectors, as presented in [4, 5]. The simplest way is to apply high-order extrapolation to all coordinates separately (which is often called the vector \(\varepsilon\)-algorithm [10]); this depends however a lot on the chosen basis, requires too many linear systems to solve, and performs worse (see examples below for gradient descent). We now present a vector extension which exists under many names: the Eddy-Mesina method [6,7], reduced rank extrapolation [4, 11, 12], or Anderson acceleration [8]. </p>



<p class="justify-text">We want to model the sequence \(x_k \in \mathbb{R}^d\) as $$x_{k+1} = A x_{k} + b,$$ where \(A \in \mathbb{R}^{d \times d}\) and \(b \in \mathbb{R}^d\). By a simple variable / equation counting arguments, there are \(d^2+d\) parameters, and we thus need \(d+1\) equations in \(\mathbb{R}^d\), and thus \(d+2\) consecutive iterates, to estimate \(A\) and \(b\). </p>



<p class="justify-text">In order to use only \(m+2\) iterates, with \(m \) much less than \(d\), we will focus directly on the extrapolation equation $$x_{\rm acc} = c_0 x_k + c_1 x_{k+1}+  \cdots +c_m x_{k+m}, $$ with the constraint that \(c_0+c_1+\cdots+c_m =1\). Therefore, we will not try to explicitly fit the model parameters \(A\) and \(b\).</p>



<p class="justify-text">A sufficient condition for good extrapolation weights is that the extrapolated version is close for two consecutive \(k\)’s, that is $$c_0 x_k + c_1 x_{k+1}+  \cdots +c_m x_{k+m} \approx c_0 x_{k+1} + c_1 x_{k+2}+  \cdots +c_m x_{k+m+1},$$ which can be rewritten as $$c_0 (x_{k}-x_{k+1}) + c_1 ( x_{k+1} -x_{k+2}) + \cdots + c_m (x_{k+m} – x_{k+m+1}) \approx 0.$$ A natural criterion is thus to minimize the \(\ell_2\)-norm $$ \big\| c_0 \Delta x_{k} + c_1  \Delta x_{k+1} + \cdots + c_m \Delta x_{k+m} \big\|_2 \mbox{ such that } c_0+c_1+\cdots+c_m = 1,$$ where \(\Delta x_{i} = x_{i} -x_{i+1}\). Denoting \(U \in \mathbb{R}^{d \times (m+1)}\) the matrix with columns \(\Delta x_{k}, \dots,  \Delta x_{k+m}\), we need to minimize \(\| U c \|_2\) such that \(c^\top 1_{m+1} = 1\), whose solution is $$ c \propto ( U^\top U)^{-1} 1_{m+1}, \mbox{ that is, } c = \frac{1}{1_{m+1}^\top (U^\top U)^{-1} 1_{m+1}}  ( U^\top U)^{-1} 1_{m+1}.$$ Note that while the weights \(c_0,\dots,c_m\) sum to one, they may be negative, that is, the extrapolated sequence is not always a convex combination (hence the name extrapolation). Moreover, note that unless \(m\) is large enough, the optimal \(U c\) is in general not equal to zero (it is when modelling real sequences, see below).</p>



<p class="justify-text">For \(m=1\), the solution is particularly simple, as we need to minimize $$ \|\Delta x_{k+1}  – c_0 ( \Delta x_{k+1} – \Delta x_k ) \|^2,$$ leading to $$c_0 = \frac{\Delta x_{k+1}^\top ( \Delta x_{k+1} – \Delta x_k )}{ \|  \Delta x_{k+1} – \Delta x_k \|^2} \mbox{ and } c_1 = \frac{\Delta x_{k}^\top ( \Delta x_{k} – \Delta x_{k+1} )}{ \|  \Delta x_{k+1} – \Delta x_k \|^2}.$$ The acute reader can check that when \(d=1\), we recover Aitken’s formula. See an example in two dimensions below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/anderson_2d.gif" alt="" width="380" class="wp-image-2276" height="319" />Anderson acceleration in two dimensions. The sequence is following an auto-regressive process with a symmetric \(A\) with eigenvalues in \((-1,1)\). Anderson acceleration cancels the oscillation due to the eigenvalue with largest magnitude.</figure></div>



<p class="justify-text"><strong>Recovering one-dimensional sequence acceleration.</strong> Given a real sequence \(y_k \in \mathbb{R}\), we can define the vector \(x_k\) in \(\mathbb{R}^m\) as $$x_k = \left( \begin{array}{c} y_k \\  y_{k+1} \\ \vdots \\ y_{k+m-1} \end{array} \right).$$ One can then check that the matrix \(U\) defined for this vector sequence is exactly the same as the matrix \(U\) defined earlier for the real valued sequence. The optimal \(\| Uc \|\) is then equal to zero (which is not the case in general).</p>



<p class="justify-text"><strong>When is it exact?</strong> The derivation I followed is only intuitive, and as for the other acceleration mechanisms, a natural question is: when is it exact? We will consider linear recursions.</p>



<p class="justify-text"><strong>Analysis for linear recursions.</strong> Assuming that \(x_{k+1} = A x_{k} + b\) is exact for all \(k \geq 0\), then \(x_k – x_\ast = A^{k} ( x_0 – x_\ast)\), and thus, following [13], $$\sum_{i=0}^m c_i (x_{k+i}-x_{k+i+1}) = \sum_{i=0}^m c_i A^{i} A^{k}(I – A )(x_0 -x_\ast) = P_m(A)(I – A) A^{k}(x_0 -x_\ast) ,$$ for \(P_m(\sigma)  =  \sum_{i=0}^m c_i \sigma^{i}\) a \(m\)-th order polynomial such that \(P_m(1) = 1\). We can write \(Uc\)  as $$ Uc = P_m(A) ( x_k – x_{k+1}) = ( I – A)  P_m(A)  (x_k – x_\ast) .$$ The error between the true limit and the extrapolation is equal to: $$  x_\ast – \sum_{i=0}^m c_i x_{k+i} = \sum_{i=0}^m c_i ( x_\ast – x_{k+i}  ) = P_m(A) (   x_\ast -x_{k}) = (I-A)^{-1} Uc.$$ Thus, we have $$ \Big\| x_\ast – \sum_{i=0}^m c_i x_{k+i} \Big\|_2 \leq \| U c \|_2 \times  \|(I – A)^{-1}\|_{\rm op} \leq \|(I – A)^{-1}\|_{\rm op}  \|I – A\|_{\rm op} \|P_m(A) ( x_k – x_\ast)\|.  $$</p>



<p class="justify-text">The method will be exact when one can find a degree \(m\) polynomial so that \(P_m(A) (x_{k} – x_\ast) = 0 \), and a sufficient condition is that \(P_m(A)=0\), which is only possible if \(A\) had only \(m\) distinct eigenvalues. This is exactly minimal polynomial extrapolation [9]. Another situation is when \(m = d\) (like for the special case of real sequences above). </p>



<p class="justify-text">Otherwise, the method will be inexact, but the method can find a good polynomial \(P_m\), and the error is less than the infimum of \( \|P_m(A) ( x_k – x_\ast)\|\) over all polynomial of degree \(m\) such that \(P_m(1)=1\). Assuming that the matrix \(A\) is symmetric and with all eigenvalues between \(-\rho\) and \(\rho\) (which will be the case for the gradient method below), then the error is less than the infimum of \(\sup_{\sigma \in [-\rho,\rho]} |P_m(\sigma)|\),  which is attained for the Chebyshev polynomial (see a <a href="https://francisbach.com/chebyshev-polynomials/">previous post</a>). The improvement in terms of convergence is similar to Chebyshev acceleration, but (a) without the need to know \(\rho\) in advance (the method is totally adaptive), and (b) with a provable robustness when the iterates deviate from following an autoregressive process (see [13] for details).</p>



<p class="justify-text"><strong>Going beyond linear recursions. </strong>As presented, Anderson acceleration does not lead to stable acceleration (see the experiment below for gradient descent). The main reason is that when iterates deviate from an autoregressive process, or when the recursion is naturally noisy, the estimation of the parameters \(c\) is unstable, in particular because the matrix \(U^\top U\) which has to be inverted is severely ill-conditioned [14]. In a joint work with Damien Scieur and Alexandre d’Aspremont, we considered regularizing  the estimation of \(c\) by penalizing its \(\ell_2\)-norm. We thus minimize  \( \| U c \|_2^2 + \lambda \| c\|_2^2\) such that \(c^\top 1_{m+1} = 1\), whose solution is $$ c \propto ( U^\top U + \lambda I)^{-1} 1_{m+1}, \mbox{ that is, } c = \frac{1}{1_{m+1}^\top (U^\top U + \lambda I)^{-1} 1_{m+1}}  ( U^\top U +  \lambda I)^{-1} 1_{m+1}.$$ This simple modification leads to theoretical guarantees for non-linear recursions, and I will refer to it as regularized non-linear acceleration (RNA, see [13] for details; the “non-linearity” comes from the non-linear dependence of \(c\) on the iterates).</p>



<h2>Application to gradient descent</h2>



<p class="justify-text">We can apply RNA to the recursion, $$x_{k+1}  = x_k – \gamma \nabla f(x_k),$$ where \(f: \mathbb{R}^d \to \mathbb{R}^d\) is a differentiable function, and \(\gamma\) a step-size. In the plot below, we consider accelerating gradient descent with \(m\)-th order RNA, with \(m=8\). We compare this acceleration with applying RNA to each variable separately (“RNA-univ.”), and to the unregularized version (“Anderson”). We can see the benefits of our simple extrapolation steps, and in particular the instability of unregularized acceleration.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/anderson_grad_nonest.png" alt="" width="419" class="wp-image-2331" height="326" />Gradient descent on a regularized <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem, with 1000 observations in dimension 100. We compare regular gradient descent, to plain Anderson acceleration (with no regularization), RNA [13] applied to each variable separately, and RNA. All accelerations are with order \(m =8\).</figure></div>



<p class="justify-text">In order to obtain stronger benefits from non-linear acceleration, several extensions are considered in [13]; in particular line search to find the good regularization parameter \(\lambda\) is quite useful. Another interesting extension is the <em>online</em> version of the algorithm [16, section 2.5], where the extrapolated sequence is used directly within the acceleration procedure, and not as a separate sequence with no interaction with the original gradient method: this corresponds to using RNA to accelerate iterates coming from RNA!</p>



<p class="justify-text">Moreover, while the simplest theoretical guarantees come for deterministic convex optimization problems and gradient descent, RNA can be extended to stochastic algorithms [15] and to non-convex optimization problems such as the ones encountered in deep learning [16].</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I described acceleration techniques that combine iterates of an existing algorithm, without the need to understand finely the inner structure of the original algorithm. They come at little extra-cost and can provide strong benefits.</p>



<p class="justify-text">This month’s post was dedicated to algorithms which converge linearly, that is, the iterates are asymptotically equivalent to sums of exponentials. Next month, I will consider situations where the convergence is sublinear, where <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> excels.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. This post is based on joint work with Damien Scieur and Alexandre d’Aspremont, and in particular on their presentation slides. I would also like to thank them for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Alexander Aitken, On Bernoulli’s numerical solution of algebraic equations, <em>Proceedings of the Royal Society of Edinburgh</em>, 46:289–305, 1926.<br />[2] Peter Wynn. On a device for computing the \(e_m(S_n)\) transformation. <em>Mathematical Tables and Other Aids to Computation</em>, 91-96, 1956.<br />[3] Claude Brezinski. <em>Accélération de la convergence en analyse numérique</em>. Lecture notes in mathematics, Springer (584), 1977.<br />[4] David A. Smith, William F. Ford, Avram Sidi. Extrapolation methods for vector sequences. <em>SIAM review</em>, 29(2):199-233, 1987<br />[5] Allan J. Macleod. Acceleration of vector sequences by multi‐dimensional \(\Delta^2\) methods. <em>Communications in Applied Numerical Methods</em>, 2(4):385-392, 1986.<br />[6] Marián Mešina. Convergence acceleration for the iterative solution of the equations X= AX+ f. <em>Computer Methods in Applied Mechanics and Engineering</em>, <em>10</em>(2), 1977.<br />[7] Robert P. Eddy. Extrapolating to the limit of a vector sequence. <em>Information linkage between applied mathematics and industry</em>, 387-396, 1979.<br />[8] Homer F. Walker, Peng Ni. Anderson acceleration for fixed-point iterations. <em>SIAM Journal on Numerical Analysis</em>, 49(4):1715-1735, 2011.<br />[9] Sidi, Avram, William F. Ford, and David A. Smith. Acceleration of convergence of vector sequences. <em>SIAM Journal on Numerical Analysis</em> 23(1):178-196, 1986.<br />[10] Peter Wynn. Acceleration techniques for iterated vector and matrix problems. <em>Mathematics of Computation</em>, 16(79), 301-322,  1962.<br />[11] Stan Cabay,  L. W. Jackson. A polynomial extrapolation method for finding limits and antilimits of vector sequences. <em>SIAM Journal on Numerical Analysis</em>, 13(5), 734-752, 1976.<br />[12] Stig Skelboe. Computation of the periodic steady-state response of nonlinear networks by extrapolation methods. <em>IEEE Transactions on Circuits and Systems</em>, 27(3), 161-175, 1980.<br />[13] Damien Scieur, Alexandre d’Aspremont, Francis Bach. Regularized Nonlinear Acceleration. <em>Mathematical Programming</em>, 2018.<br />[14] Evgenij E. Tyrtyshnikov. How bad are Hankel matrices? <em>Numerische Mathematik</em>, 67(2):261-269, 1994.<br />[15] Damien Scieur, Alexandre d’Aspremont, Francis Bach. Nonlinear Acceleration of Stochastic Algorithms. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2017.<br />[16] Damien Scieur, Edouard Oyallon, Alexandre d’Aspremont, Francis Bach. Nonlinear Acceleration of Deep Neural Networks. Technical report, arXiv-1805.09639, 2018.</p>



<h2>Asymptotic analysis for Aitken’s \(\Delta^2\) process</h2>



<p class="justify-text">In one dimension, we do not need the auto-regressive model to be exact, and an asymptotic analysis is possible. The asymptotic condition corresponds to \(\displaystyle \frac{x_{k+1}-x_\ast}{x_k – x_\ast}\) converging to a constant \(a \in [-1,1)\). More precisely if $$\frac{x_{k+1}-x_\ast}{x_k – x_\ast} = a + \varepsilon_{k+1},$$ with \(\varepsilon_k\) tending to zero, then we can estimate \(a\) through the <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitkens \(\Delta^2\) method</a> [1] as, $$ a_{k+1} = \frac{x_{k+1}-x_k}{x_{k}-x_{k-1}} = \frac{(x_{k+1}-x_\ast) – (x_k-x_\ast) }{(x_{k} – x_\ast) -( x_{k-1}-x_\ast) } = \frac{ a + \varepsilon_{k+1} – 1}{1 – 1/(a+\varepsilon_{k})} = a + \varepsilon_{k+1} + o( \varepsilon_{k+1}).$$ A closer Taylor expansion leads to $$ a + \varepsilon_{k} + \frac{1}{a-1}( \varepsilon_{k+1}- \varepsilon_{k}) + O(\varepsilon_{k}^2).$$ We can then provide a better estimate of \(x_\ast\) as $$ \frac{x_{k+1} – a_{k+1}x_k}{1- a_{k+1}} = x_\ast + \frac{x_{k+1} -x_\ast – a_{k+1}( x_k-x_\ast)}{1- a_{k+1}} = x_\ast + \frac{(a+\varepsilon_{k+1}-a_{k+1}) ( x_k – x_\ast)}{1-a_{k+1}},$$ whose difference with \(x_\ast\) is equivalent to $$ \frac{(a+\varepsilon_k-a_{k+1}) ( x_k – x_\ast)}{1-a }  \sim \frac{\varepsilon_{k+1}- \varepsilon_{k}}{(1-a)^2} ( x_k – x_\ast).$$ We have thus provided an acceleration of order \(\displaystyle \frac{\varepsilon_{k+1}- \varepsilon_{k}}{(1-a)^2} \).</p>



<h2>High-order Shanks transformation</h2>



<p>In order to relate our formulas to classical expressions, we first rewrite the recursion for \(m=1\) and then \(m=2\), and then to general \(m\).</p>



<p class="justify-text"><strong>First-order recursion (Aitken’s \(\Delta^2\)).</strong> We write the autorecursive recursion as $$ (x_{k+1} – x_\ast) = a ( x_{k} – x_\ast) \Leftrightarrow c_0(x_k – x_\ast) + c_1 (x_{k+1}-x_\ast) = 0 ,$$ with the constraint \(c_0 + c_1 = 1\), that is, \(c_0 = \frac{-a}{1-a}\) and \(c_1 = \frac{1}{1-a}\). We can then write \(x_\ast = c_0 x_k + c_1 x_{k+1}\), and we have the linear system in \((c_0,c_1,x_\ast)\): $$  \left( \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ x_k &amp; x_{k+1} &amp; – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using Cramer’s formula $$x_{\rm acc} = x_\ast = \frac{\left| \begin{array}{cc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2}  \\ x_{k} &amp; x_{k+1} \end{array}\right|}{\left| \begin{array}{cc}  x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2}  \\ 1 &amp; 1 \end{array}\right|},$$  which leads to the same formula for \(x^{(1)}_k\).</p>



<p class="justify-text"><strong>Second-order recursion.</strong> Here, we only consider the case \(m=2\) for simplicity. We consider the model, $$c_0 (x_k – x_\ast) + c_1 (x_{k+1} – x_\ast)+ c_{2} (x_{k+2} – x_\ast) = 0, $$ with the normalization \(c_0 + c_1 + c_2 = 1\). We can then extract \(x_\ast\) as $$ x_\ast = c_0 x_k + c_1 x_{k+1} +  c_2 x_{k+2}.$$ In order to provide the extra \(2\) equations that are necessary to estimate the three parameters, we take first order differences and get $$c_0 (x_k -x_{k+1}) + c_1 (x_{k+1}-x_{k+2}) +  c_2 ( x_{k+2} – x_{k+3} ) =0,$$ for \(k\) and \(k+1\). This leads to the linear system in \((c_0,c_1, c_2, x_\ast)\): $$  \left( \begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3} &amp; 0 \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4} &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; 0  \\ x_{k} &amp; x_{k+1} &amp; x_{k+2} &amp;  – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ c_2 \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c}  0  \\ 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using <a href="https://en.wikipedia.org/wiki/Cramer%27s_rule">Cramer’s rule</a> (and classical manipulations of determinants) as $$x_k^{(2)} = \frac{\left| \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3}   \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4}   \\ x_{k} &amp; x_{k+1} &amp; x_{k+2}    \end{array} \right|}{\left| \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3}     \\ x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4}   \\ 1 &amp; 1 &amp; 1 &amp;  \end{array} \right|}.$$  </p>



<p class="justify-text">The formula extends to order \(m\) (see below) and is often called the Shanks transformation; it is cumbersome and not easy to use. However, the coefficients can be computed recursively (which is to be expected for a Hankel matrix, but rather tricky to derive), through Wynn’s \(\varepsilon\)-algorithm.  See [3] for a survey on acceleration and extrapolation.</p>



<p class="justify-text"><strong>Higher-order recursion.</strong> We consider the model, for \(m \geq 1\), $$c_0 (x_k – x_\ast) + c_1 (x_{k+1} – x_\ast) + \cdots + c_{m} (x_{k+m} – x_\ast) = 0, $$ with the normalization \(c_0 + c_1 + \cdots + c_m = 1\). We can then extract \(x_\ast\) as $$ x_\ast = c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}.$$ In order to provide the extra \(m\) equations, we take first order differences and get $$c_0 (x_k -x_{k+1}) + c_1 (x_{k+1}-x_{k+2}) + \cdots + c_m ( x_{k+m} – x_{k+m+1} ) =0,$$ for \(k, k+1,\dots, k+m-1\). This leads to the linear system in \((c_0,c_1,\cdots, c_m, x_\ast)\): $$  \left( \begin{array}{ccccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} &amp; 0 \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2} &amp; 0 \\ \vdots &amp; \vdots &amp;  &amp; \vdots  &amp; \vdots \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} &amp; 0 \\ 1 &amp; 1 &amp; \dots &amp; 1 &amp; 0  \\ x_{k+m+1} &amp; x_{k+m+2} &amp; \cdots &amp; x_{k+2m+1} &amp;  – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ \vdots \\ c_m \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using Cramer’s formula $$x_\ast = \frac{\left|\begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2}   \\ \vdots &amp; \vdots &amp;  &amp; \vdots  \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} \\ x_{k+m+1} &amp; x_{k+m+2} &amp; \cdots &amp; x_{k+2m+1}  \end{array} \right|}{\left|\begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2}   \\ \vdots &amp; \vdots &amp;  &amp; \vdots  \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} \\ 1 &amp; 1 &amp; \cdots &amp; 1 \end{array}  \right|}.$$  </p>



<p class="justify-text">Like for \(m=1\), Wynn’s \(\varepsilon\)-algorithm can be used to compute the iterates recursively.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/acceleration-without-pain/"><span class="datestr">at February 04, 2020 07:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1302">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/02/02/upcoming-sigact-awards-deadlines/">Upcoming SIGACT Awards deadlines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From the SIGACT executive committee:</p>
<p>The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below.</p>
<ul>
<li><a href="https://sigact.org/prizes/g%C3%B6del/g%C3%B6del_call20.pdf">Gödel Prize</a>: deadline <strong>February 15</strong>, 2020.</li>
<li><a href="https://sigact.org/prizes/knuth.html">Knuth prize</a>: deadline <strong>April 12</strong>, 2020. Note that this deadline is a bit later than usual, because the award will be presented at FOCS this year. Next year the deadline will be moved back to February.</li>
<li><a href="https://sigact.org/prizes/service.html">SIGACT Distinguished Service Award</a>: deadline <strong>March 1</strong>, 2020.</li>
</ul></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/02/02/upcoming-sigact-awards-deadlines/"><span class="datestr">at February 02, 2020 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=703">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/02/02/the-will-of-the-framers/">The will of the framers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>What a great title for a legal thriller.  And for a history buff like me — something I never thought I would become and that must be a side effect of having learnt absolutely zero history in school — how arousing it is to hear what John Adams said in 1776, and details of the Great Debate, and the rhetoric!  It is  apt that I am following the discussion on an analog radio, my habit of the last 10 years or so, another thing I never thought I would do but that I actually find quite relaxing now.  It takes my mind off my own worry, and it soothes my eyes.  I recommend it at small doses to avoid sudden onset of nausea.  It is also apt that I follow it from <a href="https://www.redfin.com/MA/Chestnut-Hill/150-Woodland-Rd-02467/home/11457721">my house</a>, built two centuries ago though not as long ago as reported online, as I recently discovered sifting historical records. It comes to my mind that I now know what it means to renovate an old house. This is not something that I can recommend, but it is an experience that has had a profound and lasting impact on me.  I am aware of mortise locks, three-tab shingles, the terminological jungle of drywall et similia, caulking, baseboards, the difference between granite and quartz, between 4-inch and no backsplash, pvc, fixtures, the evolution of toilets and countless other things that I can’t list but that suddenly spring up in my mind when entering any house, including most recent additions such as the electrical system.</p>



<p>Once, while waiting for yet another late sub-contractor I wrote:</p>



<p>The revenge of the housekeepers</p>



<p>For centuries they slept in niches inside their masters’ houses, cooked meals in crammed kitchens, hand-washed laundry bent in basements. Now they are gone, but the houses still stand. Their niches are our offices where we can’t fit a table. We spend most of our family time in the crammed kitchen, the other rooms unused since nobody has the energy to shuttle the food, or clean. And faltering to hoist the laundry load from the basement we bump the head.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/02/02/the-will-of-the-framers/"><span class="datestr">at February 02, 2020 02:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=385">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/01/tcs-spring-approaches-talks-resume/">TCS+: Spring approaches, talks resume!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The winter hiatus is nearly over, and the new season of TCS+ is about to start! Our first two talks will take place on Feb 12th and 26th, respectively. We’re pretty excited about them…</p>
<ul>
<li>On February 12th, 1pm EST, <a href="https://www.cs.upc.edu/~atserias/">Albert Atserias</a> (Universitat Politecnica de Catalunya) will tell us all about how <em>Automating Resolution is NP-Hard</em>.</li>
<li>Then, on February 26th, <a href="http://www.henryyuen.net/">Henry Yuen</a> (University of Toronto) will speak about the recent proof that <em>MIP*=RE</em>.</li>
</ul>
<p>Stay tuned for the official talk announcements. And this is only the beginning of the semester…</p>
<p><em>In the meantime, if you have suggestions, <a href="https://sites.google.com/site/plustcs/suggest">here is the link</a>.</em></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/01/tcs-spring-approaches-talks-resume/"><span class="datestr">at February 01, 2020 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/">Krajíček’s Fest – Celebrating Jan Krajíček’s 60th Anniversary and his Contributions to Logic and Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 1, 2020 Tábor, Czech Republic https://www.dcs.warwick.ac.uk/~igorcarb/events/krajicek-fest/index.html We would like to invite you to participate in a workshop on the Logical Foundations of Complexity Theory to celebrate Prof. Jan Krajicek’s 60th anniversary. Preliminary list of speakers: Pavel Pudlák (Czech Adacemy of Sciences) Samuel Buss (University of California, San Diego) Neil Thapen (Czech Adacemy of Sciences) … <a href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/" class="more-link">Continue reading <span class="screen-reader-text">Krajíček’s Fest – Celebrating Jan Krajíček’s 60th Anniversary and his Contributions to Logic and Complexity</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/"><span class="datestr">at January 28, 2020 03:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7640">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/01/21/summer-school-on-statistical-physics-and-machine-learning/">Summer School on Statistical Physics and Machine Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Gerard Ben Arous, Surya Ganguli, Florent Krzakala and Lenka Zdeborova are organizing a <a href="http://leshouches2020.krzakala.org/">summer school on statistical physics of machine learning </a>on August 2-28, 2020 in Les Houches, France. If you don’t know Les Houches, it apparently  looks like this:</p>



<figure class="wp-block-image size-large"><img src="https://windowsontheory.files.wordpress.com/2020/01/chalet.jpg?w=414" alt="" class="wp-image-7641" /></figure>



<p>They are looking for applications from students, postdocs, and young researchers in physics &amp; math, as well computer scientists. While I am biased (I will be lecturing there too) I think the combination of lecturers, speakers, and audience members will yield a very unique opportunity for interaction across communities, and strongly encourage theoretical computer scientists to apply (which you can from the <a href="http://leshouches2020.krzakala.org/">website</a>). Let me also use this opportunity to remind people again of <a href="https://windowsontheory.org/2019/03/30/physics-computation-blog-post-round-up/">Tselil Schramm’s blog post</a> where she collected some of the lecture notes from the seminar we ran on physics &amp; computation.</p>



<p><strong>More information about the summer school:</strong></p>



<p>The “Les Houches school of physics”, situated close to Chamonix and the Mont Blanc in the French Alps, has a long history of forming generations of young researchers on the frontiers of their fields. Our school is aimed primarily at the growing audience of theoretical physicists and applied mathematicians interested in machine learning and high-dimensional data analysis, as well as to <strong>colleagues from other fields</strong> interested in this interface. <em>[my emphasis –Boaz]</em> We will cover basics and frontiers of high-dimensional statistics, machine learning, the theory of computing and learning, and probability theory. We will focus in particular on methods of statistical physics and their results in the context of current questions and theories related to machine learning and neural networks. The school will also cover examples of applications of machine learning methods in physics research, as well as other emerging applications of wide interest. Open questions and directions will be presented as well.</p>



<p>Students, postdocs and young researchers interested to participate in the event are invited to apply on the website <a href="http://leshouches2020.krzakala.org/" rel="nofollow">http://leshouches2020.krzakala.org/</a> before March 15, 2020. The capacity of the school is limited, and due to this constraint participants will be selected from the applicants and participants will be required to attend the whole event.</p>



<p><strong>Lecturers:</strong></p>



<ul><li>Boaz Barak (Harvard): Computational hardness perspectives</li><li>Giulio Biroli (ENS, Paris): High-dimensional dynamics</li><li>Michael Jordan (UC Berkeley): Optimization, diffusion &amp; economics</li><li>Marc Mézard (ENS, Paris): Message-Passing algorithms</li><li>Yann LeCun (Facebook AI, NYU). Challenges and directions in machine learning</li><li>Remi Monasson (ENS, Paris): Statistical physics or learning in neural networks</li><li>Andrea Montanari (Stanford): High-dimensional statistics &amp; neural networks</li><li>Maria Schuld (Univ. KwaZulu Natal &amp; Xanadu): Quantum machine learning</li><li>Haim Sompolinsky (Harvard &amp; Hebrew Univ.): Statistical mechanics of deep neural networks</li><li>Nathan Srebro (TTI-Chicago): Optimization and implicit regularisation</li><li>Miles Stoudenmire (Flatiron, NYC): Tensor network methods</li><li>Pierre Vandergheynst (EPFL, Lausanne): Graph signal processing &amp; neural networks</li></ul>



<p><strong>Invited Speakers</strong> (to be completed):</p>



<ul><li>Christian Borgs (UC Berkeley)</li><li>Jennifer Chayes (UC Berkeley)</li><li>Shirley Ho (Flatiron NYC)</li><li>Levent Sagun (Facebook AI)</li></ul></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/01/21/summer-school-on-statistical-physics-and-machine-learning/"><span class="datestr">at January 21, 2020 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1295">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/01/19/prize-nominations-relevant-to-tcs-due-on-april-1/">Prize nominations relevant to TCS: due on April 1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Breakthrough Prize committee is now accepting nominations for the <strong>Breakthrough Prize</strong> as well as <strong>New Horizons Prizes</strong> in Mathematics. The New Horizons Prizes are awarded to early-career researchers (Ph.D. within the last 10 years) who have already produced important work. In addition, for the first time, nominations will be taken for the <strong>Maryam Mirzakhani New Frontiers Prize</strong> – an annual $50,000 award that will be presented to early-career women mathematicians who have completed their PhDs within the previous two years.</p>
<p>Further information is available at <a href="https://breakthroughprize.org/Rules/3" rel="nofollow">https://breakthroughprize.org/Rules/3</a>. Nominations for 2021 are due on April 1, 2020.</p>
<p>Please consider nominating deserving candidates. The task of nominating someone for a high-profile award can be daunting, but the CATCS is available to help. Please contact us if you need help with preparing a nomination.</p>
<p> </p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/01/19/prize-nominations-relevant-to-tcs-due-on-april-1/"><span class="datestr">at January 20, 2020 03:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=697">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/01/17/what-is-wrong-with-this/">What is wrong with this?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://emanueleviola.files.wordpress.com/2020/01/xinfertilitygraph.png?w=350" alt="" class="wp-image-698" /></figure></div></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/01/17/what-is-wrong-with-this/"><span class="datestr">at January 17, 2020 06:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7627">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/01/15/intro-tcs-recap/">Intro TCS recap</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This semester I taught <a href="https://cs121.boazbarak.org/">another iteration</a> of my “Introduction to Theoretical Computer Science” course, based on my <a href="https://introtcs.org/">textbook in process</a>.  The book was also used in <a href="https://uvatoc.github.io/">University of Virgnia CS 3102</a> by David Evans and Nathan Brunelle.</p>



<p>The main differences I made in the text and course since its <a href="https://windowsontheory.org/2017/07/27/rethinking-the-intro-theory-course/">original version </a>were to make it less “idiosyncratic”: while I still think using programming language terminology is the conceptually “right” way to teach this material, there is a lot to be said for sticking with well-established models. So, I used <strong>Boolean circuits</strong> as the standard model for finite-input non-uniform computation, and <strong>Turing Machines</strong>, as the standard model for unbounded-input uniform computation. (I do talk about the equivalent programming languages view of both models, which can be a more useful perspective for some results, and is also easier to <a href="https://github.com/boazbk/tcscode">work with in code</a>.)</p>



<p>In any course on intro to theoretical CS, there are always beautiful topics that are left on the “cutting room floor”. To partially compensate for that, we had an entirely optional “advanced section” where guest speakers talked about topics such as error correcting codes, circuit lower bounds,  communication complexity, interactive proofs, and more. The TA in charge of this section – amazing sophomore named <a href="https://singerng.github.io/">Noah Singer</a> – wrote <a href="https://windowsontheory.files.wordpress.com/2020/01/cs_121_5_notes_2019-2.pdf">very detailed lecture notes</a>  for this section.</p>



<p>This semester, students in CS 121 could also do an optional project. Many chose to do a video about topics related to the course, here are some examples:</p>



<ul><li>Helen Huang and Phil Labrum: <a href="https://youtu.be/28lsaX7vZho">“Is your brain Turing complete?”</a> </li><li>Dylan Zhou:  <a href="https://youtu.be/bpSBCFDPoXw" target="_blank" rel="noreferrer noopener">Candy Crush</a> is NP complete and <a href="https://youtu.be/zjr-0HhHWHM" target="_blank" rel="noreferrer noopener">Mario Kart tour</a> is PSPACE hard.</li><li>Carolyn Ge, Kavya Kopparapu, and Eric Lin: a 4-part series on theory of machine learning including an interview with Les Valiant: <a href="https://youtu.be/fIbKT0s-Dyw" target="_blank" rel="noreferrer noopener">What is learnability?</a> <a href="https://youtu.be/9-CVk2b8mec" target="_blank" rel="noreferrer noopener">Cryptographic limitations</a>, <a href="https://youtu.be/F8s3QdfJs48" target="_blank" rel="noreferrer noopener">Cryptography and Machine Learning</a>, and <a href="https://youtu.be/LUyDekgRP_8" target="_blank" rel="noreferrer noopener">future possibilities</a>.</li><li>And finally, Christine Cai, April Chen, Zev Nicolai-Scanio, and Grace Tian produced a rap song <a href="https://docs.google.com/document/d/1aeOqdN8ItqSfXFepJXSCrTGaS-aBL1ZFXltw6LAVgZc/edit">“Proof Göds”</a> inspired by the course.</li></ul>



<p>There is much work to still do on both the text and the course. Though the text has improved a lot (we do have <a href="https://github.com/boazbk/tcs/issues">267 closed issues</a> after all) some students still justifiably complained about typos, which can throw off people that are just getting introduced to the topic. I also want to add significantly more solved exercises and examples, since students do find them extremely useful. I need to significantly beef up the NP completeness chapter with more examples of reductions, though I do have Python implementation of <a href="https://github.com/boazbk/tcscode/blob/master/Chap_13_reductions.ipynb">several reductions </a>and the <a href="https://github.com/boazbk/tcscode/blob/master/Lec_17_Cook_Levin.ipynb">Cook Levin theorem</a>.</p>



<p>This type of course is often known as a “great ideas” in computer science, and so in the book I also added a “Big Idea” environment to highlight those. Of course some of those ideas are bigger than others, but I think the list below reflects well the contents of the course:</p>



<ul><li>If we can represent objects of type T as strings, then we can represent tuples of objects of type T as strings as well.</li><li>A <em>function</em> is not the same as a <em>program</em>. A program <em>computes</em> a function.</li><li>Two models are <em>equivalent in power </em> if they can be used to compute  the same set of functions.</li><li><em>Every</em> finite function can be computed by a large enough Boolean  circuit.</li><li>A <em>program</em> is a piece of text, and so it can be fed as input to other  programs.</li><li>Some functions  <img src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En%C2%A0%5Crightarrow%C2%A0%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f:\{0,1\}^n \rightarrow \{0,1\}" class="latex" title="f:\{0,1\}^n \rightarrow \{0,1\}" />  <em>cannot</em> be computed by a Boolean circuit using fewer than exponential (in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />) number of gates.</li><li>We can precisely define what it means for a function to be computable by<em>  any possible algorithm</em>.</li><li>Using equivalence results such as those between Turing and RAM machines, we can <em>“have our cake and eat it too”</em>: We can use a simpler model such as Turing machines when we want to prove something <em>can’t </em> be done, and use a feature-rich model such as RAM machines when we want to prove something <em>can</em> be done.</li><li>There is a  <em>“universal” </em>algorithm that can evaluate arbitrary algorithms on arbitrary inputs.</li><li>There are some functions that <em>can not</em> be computed by <em>any</em> algorithm.</li><li>If a function <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> is uncomputable we can show that another function <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is uncomputable by giving a way to <em>reduce</em> the task of computing <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> to computing <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />.</li><li>We can use <em>restricted computational models</em> to bypass limitations such as uncomputability of the Halting problem and Rice’s Theorem. Such models can compute only a restricted subclass of functions, but allow to answer at least some <em>semantic questions</em> on programs.</li><li>A <em>proof</em> is just a string of text whose meaning is given by a <em>verification algorithm</em>.</li><li>The running time of an algorithm is not a <em>number</em>, it is a <em>function</em> of the length of the input.</li><li>For a function <img src="https://s0.wp.com/latex.php?latex=F%3A%7B0%2C1%7D%5E%2A+%5Crightarrow+%7B0%2C1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F:{0,1}^* \rightarrow {0,1}" class="latex" title="F:{0,1}^* \rightarrow {0,1}" /> and <img src="https://s0.wp.com/latex.php?latex=T%3A%5Cmathbb%7BN%7D+%5Crightarrow+%5Cmathbb%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T:\mathbb{N} \rightarrow \mathbb{N}" class="latex" title="T:\mathbb{N} \rightarrow \mathbb{N}" />, we can formally define what it means for <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> to be computable in time at most <img src="https://s0.wp.com/latex.php?latex=T%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T(n)" class="latex" title="T(n)" /> where <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> is the size of the input.</li><li>All “reasonable” computational models are equivalent if we only care about the distinction between  polynomial and exponential. (The book immediately notes quantum computers as a possible exception for this.)</li><li>If we have more time, we can compute more functions.</li><li>By “unrolling the loop” we can transform an algorithm that takes <img src="https://s0.wp.com/latex.php?latex=T%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T(n)" class="latex" title="T(n)" /> steps to compute <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> into a circuit that uses <img src="https://s0.wp.com/latex.php?latex=poly%28T%28n%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="poly(T(n))" class="latex" title="poly(T(n))" /> gates to compute the restriction of <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> to <img src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{0,1}^n" class="latex" title="{0,1}^n" />.</li><li>A <em>reduction</em> <img src="https://s0.wp.com/latex.php?latex=F+%5Cleq_p+G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F \leq_p G" class="latex" title="F \leq_p G" /> shows that <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" /> is “no harder than <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />” or equivalently that <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is “no easier than <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" />“.</li><li>If a <em>single</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BNP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{NP}" class="latex" title="\mathbf{NP}" />-complete has a polynomial-time algorithm, then there is such an algorithm for every decision problem that corresponds to the existence of an <em>efficiently-verifiable</em> solution.</li><li>If <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BP%7D%3D%5Cmathbf%7BNP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{P}=\mathbf{NP}" class="latex" title="\mathbf{P}=\mathbf{NP}" />, we can efficiently solve a fantastic number of decision, search, optimization, counting, and sampling problems from all areas of human endeavors.</li><li>A randomized algorithm outputs the correct value with good probability on <em>every possible input</em>.</li><li>We can <em>amplify</em> the success of randomized algorithms to a value that is arbitrarily close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />.</li><li>There is no <em>secrecy</em> without <em>randomness</em>.</li><li><em>Computational hardness</em> is <em>necessary and sufficient</em> for almost all cryptographic applications.</li><li>Just as we did with classical computation, we can define mathematical models for quantum computation, and represent quantum algorithms as binary strings.</li><li>Quantum computers are not a panacea and are unlikely to solve <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BNP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{NP}" class="latex" title="\mathbf{NP}" /> complete problems, but they can provide exponential speedups to certain <em>structured</em> problems.</li></ul>



<p>These are all ideas that I believe are important for Computer Science undergraduates to be exposed to, but covering all of these does make for a every challenging course, which gets literally mixed reviews from the students, with some loving it and some hating it. (I post all reviews on the course home page.) Running a 200-student class is definitely something that I’m still learning how to do. </p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/01/15/intro-tcs-recap/"><span class="datestr">at January 15, 2020 08:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7618">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/01/14/mipre-connes-embedding-conjecture-disproved/">MIP*=RE, disproving Connes embedding conjecture.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In an exciting <a href="https://arxiv.org/abs/2001.04383">manuscript just posted on the arxiv</a>, Zhengfeng Ji, Anand Natarajan, Thomas Vidick, John Wright, and Henry Yuen prove that there is a 2-prover quantum protocol (with shared entanglement) for the halting problem. As a consequence they resolve negatively a host of open problems in quantum information theory and operator algebra, including refuting the longstanding <a href="https://en.wikipedia.org/wiki/Connes_embedding_problem">Connes embedding conjecture</a>. See also <a href="https://www.scottaaronson.com/blog/?p=4512">Scott’s post</a> and <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">this blog post</a> of Thomas Vidick discussing his personal history with these questions, that started with his Masters project under Julia Kempe’s supervision 14 years ago.</p>



<p>I am not an expert in this area, and still have to look the paper beyond the first few pages, but find the result astounding. In particular, the common intuition is that since all physical quantities are “nice” function (continuous, differentiable, etc..), we could never distinguish between the case that the universe is infinite or discretized at a fine enough grid.  The new work (as far as I understand) provides a finite experiment that can potentially succeed with probability 1 if the two provers use an infinite amount of shared entangled state, but would succeed with probability at most 1/2 if they use only a finite amount. A priori you would expect that if there is a strategy that succeeds with probability 1 with an infinite entanglement, then you could succeed with probability at least <img src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1-\epsilon" class="latex" title="1-\epsilon" /> with a finite entangled state whose dimension depends only on <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />.</p>



<p>The result was preceded by Ito and Vidick’s 2012 result that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BNEXP%7D+%5Csubseteq+%5Cmathbf%7BMIP%5E%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{NEXP} \subseteq \mathbf{MIP^*}" class="latex" title="\mathbf{NEXP} \subseteq \mathbf{MIP^*}" /> and Natarajan and Wright’s result last year that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BNEEXP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{NEEXP}" class="latex" title="\mathbf{NEEXP}" /> (non deterministic <em>double exponential</em> time) is contained in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BMIP%5E%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{MIP^*}" class="latex" title="\mathbf{MIP^*}" />. This brings to mind Edmonds’ classic quote that:</p>



<blockquote class="wp-block-quote"><p><em>“For practical purposes the difference between algebraic and exponential order is often more crucial than the difference between finite and non-finite”</em></p></blockquote>



<p>sometimes, the difference between double-exponential and infinite turns out to be non-existent.. </p>



<p></p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/01/14/mipre-connes-embedding-conjecture-disproved/"><span class="datestr">at January 14, 2020 05:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1234">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">A Masters project</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In a <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">previous post</a> I reported on the beautiful <a href="https://arxiv.org/abs/1904.05870">recent result</a> by Natarajan and Wright showing the astounding power of multi-prover interactive proofs with quantum provers sharing entanglement: in letters, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BNEEXP%7D+%5Csubseteq+%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{NEEXP} \subseteq \text{MIP}^\star}" class="latex" title="{\text{NEEXP} \subseteq \text{MIP}^\star}" />. In this post I want to report on follow-up work with Ji, Natarajan, Wright, and Yuen, that we just posted to <a href="https://arxiv.org/abs/2001.04383">arXiv</a>. This time however I will tell the story from a personal point of view, with all the caveats that this implies: the “hard science” will be limited (but there could be a hint as to how “science”, to use a big word, “progresses”, to use an ill-defined one), the story is far too long, and it might be mostly of interest to me only. It’s a one-sided story, but that has to be. (In particular below I may at times attribute credit in the form “X had this idea”. This is my recollection only, and it is likely to be inaccurate. Certainly I am ignoring a lot of important threads.) I wrote this because I enjoyed recollecting some of the best moments in the story just as much as some the hardest; it is fun to look back and find meanings in ideas that initially appeared disconnected. Think of it as an example of how different lines of work can come together in unexpected ways; a case for open-ended research. It’s also an antidote against despair that I am preparing for myself: whenever I feel I’ve been stuck on a project for far too long, I’ll come back to this post and ask myself if it’s been 14 years yet — if not, then press on.</p>
<p>It likely comes as a surprise to me only that I am no longer fresh out of the cradle. My academic life started in earnest some 14 years ago, when in the Spring of 2006 I completed my Masters thesis in Computer Science under the supervision of Julia Kempe, at Orsay in France. I had met Julia the previous term: her class on quantum computing was, by far, the best-taught and most exciting course in the Masters program I was attending, and she had gotten me instantly hooked. Julia agreed to supervise my thesis, and suggested that I look into some interesting recent result by Stephanie Wehner that linked the study of entanglement and nonlocality in quantum mechanics to complexity-theoretic questions about interactive proof systems (specifically, this was Stephanie’s <a href="https://arxiv.org/abs/quant-ph/0508201">paper</a> showing that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BXOR-MIP%7D%5E%5Cstar+%5Csubseteq+%5Ctext%7BQIP%7D%282%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{XOR-MIP}^\star \subseteq \text{QIP}(2)}" class="latex" title="{\text{XOR-MIP}^\star \subseteq \text{QIP}(2)}" />).</p>
<p>At the time the topic was very new. It had been initiated the previous year with a beautiful <a href="https://arxiv.org/abs/quant-ph/0404076">paper</a> by Cleve et al. (that I have recommended to many a student since!). It was a perfect fit for me: the mathematical aspects of complexity theory and quantum computing connected to my undergraduate background, while the relative concreteness of quantum mechanics (it is a physical theory after all) spoke to my desire for real-world connection (not “impact” or even “application” — just “connection”). Once I got myself up to speed in the area (which consisted of three papers: the two I already mentioned, together with a <a href="https://arxiv.org/abs/cs/0102013">paper</a> by Kobayashi and Matsumoto where they studied interactive proofs with quantum messages), Julia suggested looking into the the “entangled-prover” class <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> introduced in the aforementioned paper by Cleve et al. Nothing was known about this class! Nothing besides the trivial inclusion of single-prover interactive proofs, IP, and the containment in…ALL, the trivial class that contains all languages.<br />
Yet the characterization MIP=NEXP of its classical counterpart by Babai et al. in the 1990s had led to one of the most productive lines of work in complexity of the past few decades, through the PCP theorem and its use from hardness of approximation to efficient cryptographic schemes. Surely, studying <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> had to be a productive direction? In spite of its well-established connection to classical complexity theory, via the formalism of interactive proofs, this was a real gamble. The study of entanglement from the complexity-theoretic perspective was entirely new, and bound to be fraught with difficulty; very few results were available and the existing lines of works, from the foundations of nonlocality to more recent endeavors in device-independent cryptography, provided little other starting point than strong evidence that even the simplest examples came with many unanswered questions. But my mentor was fearless, and far from a novice in terms of defraying new areas, having done pioneering work in areas ranging from quantum random walks to Hamiltonian complexity through adiabatic computation. Surely this would lead to something?</p>
<p>It certainly did. More sleepless nights than papers, clearly, but then the opposite would only indicate dullness. Julia’s question led to far more unexpected consequences than I, or I believe she, could have imagined at the time. I am writing this post to celebrate, in a personal way, the latest step in 15 years of research by dozens of researchers: today my co-authors and I uploaded to the quant-ph arXiv what we consider a complete characterization of the power of entangled-prover interactive proof systems by proving the equality <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar+%3D+%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star = \text{RE}}" class="latex" title="{\text{MIP}^\star = \text{RE}}" />, the class of all recursively enumerable languages (a complete problem for RE is the halting problem). Without goign too much into the result itself (if you’re interested, we have a long introduction waiting for you), and since this is a personal blog, I will continue on with some personal thoughts about the path that got us there.</p>
<p>When Julia &amp; I started working on the question, our main source of inspiration were the results by Cleve et al. showing that the nonlocal correlations of entanglement had interesting consequences when seen through the lens of interactive proof systems in complexity theory. Since the EPR paper a lot of work in understanding entanglement had already been accomplished in the Physics community, most notably by Mermin, Peres, Bell, and more recently the works in device-indepent quantum cryptography by Acin, Pironio, Scarani and many others stimulated by Ekert’s proposal for quantum key distribution and Mayers and Yao’s idea for “device-independent cryptography”. By then we certainly knew that “spooky action-at-a-distance” did not entail any faster-than-light communication, and indeed was not really “action-at-a-distance” in the first place but merely “correlation-at-a-distance”. What Cleve et al. recognized is that these “spooky correlations-at-a-distance” were sufficiently special so as to not only give numerically different values in “Bell inequalities”, the tool invented by Bell to evidence nonlocality in quantum mechanics, but also have some potentially profound consequences in complexity theory. In particular, examples such as the “Magic Square game” demonstrated that enough correlation could be gained from entanglement so as to defeat basic proof systems whose soundness relied only on the absence of communication between the provers, an assumption that until then had been wrongly equated with the assumption that any computation performed by the provers could be modeled entirely locally. I think that the fallacy of this implicit assumption came as a surprise to complexity theorists, who may still not have entirely internalized it. Yet the perfect quantum strategy for the Magic Square game provides a very concrete “counter-example” to the soundness of the “clause-vs-variable” game for 3SAT. Indeed this game, a reformulation by Aravind and Cleve-Mermin of a Bell Inequality discovered by Mermin and Peres in 1990, can be easily re-framed as a 3SAT system of equations that is <em>not</em> satisfiable and yet is such that the associated two-player clause-vs-variable game has a <em>perfect</em> quantum strategy. It is this observation, made in the paper by Cleve et al., that gave the first strong hint that the use of entanglement in interactive proof systems could make many classical results in the area go awry.</p>
<p>By importing the study of non-locality into complexity theory Cleve et al. immediately brought it into the realm of asymptotic analysis. Complexity theorists don’t study fixed objects, they study families of objects that tend to have a uniform underlying structure and whose interesting properties manifest themselves “in the limit”. As a result of this new perspective focus shifted from the study of single games or correlations to infinite families thereof. Some of the early successes of this translation include the “unbounded violations” that arose from translating asymptotic separations in communication complexity to the language of Bell inequalities and correlations (e.g. this <a href="https://arxiv.org/abs/1012.5043">paper</a>). These early successes attracted the attention of some physicists working in foundations as well as some mathematical physicists, leading to a productive exploration that combined tools from quantum information, functional analysis and complexity theory.</p>
<p>The initial observations made by Cleve et al. had pointed to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> as a possibly interesting complexity class to study. Rather amazingly, nothing was known about it! They had shown that under strong restrictions on the verifier’s predicate (it should be an XOR of two answer bits), a collapse took place: by the work of Hastad, XOR-MIP equals NEXP, but <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BXOR-MIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{XOR-MIP}^\star}" class="latex" title="{\text{XOR-MIP}^\star}" /> is included in EXP. This seemed very fortuitous (the inclusion is proved via a connection with semidefinite programming that seems tied to the structure of XOR-MIP protocols): could entanglement induce a collapse of the entire, unrestricted class? We thought (at this point mostly Julia thought, because I had no clue) that this ought not to be the case, and so we set ourselves to show that the equality <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%3D%5Ctext%7BNEXP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star=\text{NEXP}}" class="latex" title="{\text{MIP}^\star=\text{NEXP}}" />, that would directly parallel Babai et al.’s characterization MIP=NEXP, holds. We tried to show this by introducing techniques to “immunize” games against entanglement: modify an interactive proof system so that its structure makes it “resistant” to the kind of “nonlocal powers” that can be used to defeat the clause-vs-variable game (witness the Magic Square). This was partially successful, and led to one of the papers I am most proud of — I am proud of it because I think it introduced elementary techniques (such as the use of the Cauchy-Schwarz inequality — inside joke — more seriously, basic things such as “prover-switching”, “commutation tests”, etc.) that are now routine manipulations in the area. The paper was a hard sell! It’s good to remember the first rejections we received. They were not unjustified: the main point of criticism was that we were only able to establish a hardness result for exponentially small completeness-soundness gap. A result for such a small gap in the classical setting follows directly from a very elementary analysis based on the Cook-Levin theorem. So then why did we have to write so many pages (and so many applications of Cauchy-Schwarz!) to arrive at basically the same result (with a <img src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^\star}" class="latex" title="{^\star}" />)?</p>
<p>Eventually we got lucky and the paper was accepted to a conference. But the real problem, of establishing any non-trivial lower bound on the class <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> with constant (or, in the absence of any parallel repetition theorem, inverse-polynomial) completeness-soundness gap, remained. By that time I had transitioned from a Masters student in France to a graduate student in Berkeley, and the problem (pre-)occupied me during some of the most difficult years of my Ph.D. I fully remember spending my first year entirely thinking about this (oh and sure, that systems class I had to pass to satisfy the Berkeley requirements), and then my second year — yet, getting nowhere. (I checked the arXiv to make sure I’m not making this up: two full years, no posts.) I am forever grateful to my fellow student Anindya De for having taken me out of the cycle of torture by knocking on my door with one of the most interesting questions I have studied, that led me into quantum cryptography and quickly resulted in an enjoyable <a href="https://arxiv.org/abs/0911.4680">paper</a>. It was good to feel productive again! (Though the paper had fun reactions as well: after putting it on the arXiv we quickly heard from experts in the area that we had solved an irrelevant problem, and that we better learn about information theory — which we did, eventually leading to another <a href="https://arxiv.org/abs/0912.5514">paper</a>, etc.) The project had distracted me and I set interactive proofs aside; clearly, I was stuck.</p>
<p>About a year later I visited IQC in Waterloo. I don’t remember in what context the visit took place. What I do remember is a meeting in the office of Tsuyoshi Ito, at the time a postdoctoral scholar at IQC. Tsuyoshi asked me to explain our result with Julia. He then asked a very pointed question: the bedrock for the classical analysis of interactive proof systems is the “linearity test” of Blum-Luby-Rubinfeld (BLR). Is there any sense in which we could devise a quantum version of that test?</p>
<p>What a question! This was great. At first it seemed fruitless: in what sense could one argue that quantum provers apply a “linear function”? Sure, quantum mechanics is linear, but that is besides the point. The linearity is a property of the prover’s answers as a function of their question. So what to make of the quantum state, the inherent randomness, etc.?</p>
<p>It took us a few months to figure it out. Once we got there however, the answer was relatively simple — the prover should be making a question-independent measurement that returns a linear function that it applies to its question in order to obtain the answer returned to the verifier — and it opened the path to our subsequent <a href="https://arxiv.org/abs/1207.0550">paper</a> showing that the inclusion of NEXP in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> indeed holds. Tsuyoshi’s question about linearity testing had allowed us to make the connection with PCP techniques; from there to MIP=NEXP there was only one step to make, which is to analyze multi-linearity testing. That step was suggested by my Ph.D. advisor, Umesh Vazirani, who was well aware of the many pathways towards the classical PCP theorem (indeed a lot of the activity that led to the proof of the theorem took place in Berkeley, with many of Umesh’s current or former students making substantial contributions). It took a lot of technical work, yet conceptually a single question from my co-author had sufficed to take me out of a 3-year slumber.</p>
<p>This was in 2012, and I thought we were done. For some reason the converse inclusion, of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> in NEXP, seemed to resist our efforts, but surely it couldn’t resist much longer. Navascues et al. had introduced a hierarchy of semidefinite programs that seemed to give the right answer (technically they could only show convergence to a relaxation, the commuting value, but that seemed like a technicality; in particular, the values coincide when restricted to finite-dimensional strategies, which is all we computer scientists cared about). There were no convergence bounds on the hierarchy, yet at the same time commutative SDP hierarchies were being used to obtain very strong results in combinatorial optimization, and it seemed like it would only be a matter of time before someone came up with an analysis of the quantum case. (I had been trying to solve a related “dimension reduction problem” with Oded Regev for years, and we were making no progress; yet it seemed <em>someone</em> ought to!)</p>
<p>In Spring 2014 during an open questions session at a <a href="https://simons.berkeley.edu/workshops/qhc2014-1">workshop</a> at the Simons Institute in Berkeley Dorit Aharonov suggested that I ask the question of the possible inclusion of QMA-EXP, the exponential-sized-proofs analogue of QMA, in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" />. A stronger result than the inclusion of NEXP (under assumptions), wouldn’t it be a more natural “fully quantum” analogue of MIP=NEXP? Dorit’s suggestion was motivated by research on the “quantum PCP theorem”, that aims to establish similar hardness results in the realm of the local Hamiltonian problem; see e.g. <a href="https://mycqstate.wordpress.com/2014/10/31/quantum-pcp-conjectures/">this post</a> for the connection. I had no idea how to approach the question — I also didn’t really believe the answer could be positive — but what can you do, if Dorit asks you something… So I reluctantly went to the board and asked the question. Joe Fitzsimons was in the audience, and he immediately picked it up! Joe had the fantastic ideas of using quantum error-correction, or more specifically secret-sharing, to distribute a quantum proof among the provers. His enthusiasm overcame my skepticism, and we eventually <a href="https://arxiv.org/abs/1409.0260">showed</a> the desired inclusion. Maybe <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> <em>was</em> bigger than <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BNEXP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{NEXP}}" class="latex" title="{\text{NEXP}}" /> after all.</p>
<p>Our result, however, had a similar deficiency as the one with Julia, in that the completeness-soundness gap was exponentially small. Obtaining a result with a constant gap took 3 years of couple more years of work and the fantastic energy and insights of a Ph.D. student at MIT, Anand Natarajan. Anand is the first person I know of to have had the courage to dive in to the most technical aspects of the analysis of the aforementioned results, while also bringing in the insights of a “true quantum information theorist” that were supported by Anand’s background in Physics and upbringing in the group of Aram Harrow at MIT. (In contrast I think of myself more as a “raw” mathematician; I don’t really understand quantum states other than as psd matrices…not that I understand math either of course; I suppose I’m some kind of a half-baked mish-mash.) Anand had many ideas but one of the most beautiful ones led to what he poetically called the “Pauli braiding test”, a “truly quantum” analogue of the BLR linearity test that amounts to doing <em>two</em> linearity tests in conjugate bases and piecing the results together into a robust test for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit entanglement (I wrote about our work on this <a href="https://mycqstate.wordpress.com/2017/06/28/pauli-braiding/">here</a>).</p>
<p>At approximately the same time Zhengfeng Ji had another wonderful idea, that was in some sense orthogonal to our work. (My interpretation of) Zhengfeng’s idea is that one can see an interactive proof system as a computation (verifier-prover-verifier) and use Kitaev’s circuit-to-Hamiltonian construction to transform the entire computation into a “quantum CSP” (in the same sense that the local Hamiltonian problem is a quantum analogue of classical constraint satisfaction problems (CSP)) that could then itself be verified by a quantum multi-prover interactive proof system…with exponential gains in efficiency! Zhengfeng’s result implied an exponential improvement in complexity compared to the result by Julia and myself, showing inclusion of NEEXP, instead of NEXP, in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" />. However, Zhengfeng’s technique suffered from the same exponentially small completeness-soundness gap as we had, so that the best lower bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> per se remained NEXP.</p>
<p>Both works led to follow-ups. With Natarajan we promoted the Pauli braiding test into a “<a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a>” that allowed us to show the inclusion of QMA-EXP into <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" />, with constant gap, thereby finally answering the question posed by Aharonov 4 years after it was asked. (I should also say that by then all results on <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> started relying on a sequence of parallel repetition results shown by Bavarian, Yuen, and others; I am skipping this part.) In parallel, with Ji, Fitzsimons, and Yuen we showed that Ji’s compression technique could be “iterated” an arbitrary number of times. In fact, by going back to “first principles” and representing verifiers uniformly as Turing machines we realized that the compression technique could be used iteratively to (up to small caveats) give a new proof of the fact (first <a href="https://arxiv.org/abs/1703.08618">shown</a> by Slofstra using an embedding theorem for finitely presented group) that the zero-gap version of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> contains the halting problem. In particular, the entangled value is uncomputable! This was not the first time that uncomputability crops in to a natural problem in quantum computing (e.g. the <a href="https://arxiv.org/abs/1502.04573">spectral gap paper</a>), yet it still surprises when it shows up. Uncomputable! How can anything be uncomputable!</p>
<p>As we were wrapping up our paper Henry Yuen realized that our “iterated compression of interactive proof systems” was likely optimal, in the following sense. Even a mild improvement of the technique, in the form of a slower closing of the completeness-soundness gap through compression, would yield a much stronger result: undecidability of the constant-gap class <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" />. It was already known by work of Navascues et al., Fritz, and others, that such a result would have, if not surprising, certainly consequences that seemed like they would be taking us out of our depth. In particular, undecidability of any language in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> would imply a negative resolution to a series of equivalent conjectures in functional analysis, from Tsirelson’s problem to Connes’ Embedding Conjecture through Kirchberg’s QWEP conjecture. While we liked our result, I don’t think that we believed it could resolve any conjecture(s) in functional analysis.</p>
<p>So we moved on. At least I moved on, I did some cryptography for a change. But Anand Natarajan and his co-author John Wright did not stop there. They had the last major insight in this story, which underlies their recent STOC best paper described in the previous <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">post</a>. Briefly, they were able to combine the two lines of work, by Natarajan &amp; myself on low-degree testing and by Ji et al. on compression, to obtain a compression that is specially tailored to the existing <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> protocol for NEXP and compresses that protocol without reducing its completeness-soundness gap. This then let them show Ji’s result that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" /> contains NEEXP, but this time with constant gap! The result received well-deserved attention. In particular, it is the first in this line of works to not suffer from any caveats (such as a closing gap, or randomized reductions, or some kind of “unfair” tweak on the model that one could attribute the gain in power to), and it implies an unconditional separation between MIP and <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star}" class="latex" title="{\text{MIP}^\star}" />.</p>
<p>As they were putting the last touches on their result, suddenly something happened, which is that a path towards a much bigger result opened up. What Natarajan &amp; Wright had achieved is a one-step gapless compression. In our iterated compression paper we had observed that iterated gapless compression would lead to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar%3D%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star=\text{RE}}" class="latex" title="{\text{MIP}^\star=\text{RE}}" />, implying negative answers to the aforementioned conjectures. So then?</p>
<p>I suppose it took some more work, but in some way all the ideas had been laid out in the previous 15 years of work in the complexity of quantum interactive proof systems; we just had to put it together. And so a decade after the characterization <a href="https://arxiv.org/abs/0907.4737">QIP = PSPACE</a> of single-prover quantum interactive proof systems, we have arrived at a characterization of quantum multiprover interactive proof systems, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7BMIP%7D%5E%5Cstar+%3D+%5Ctext%7BRE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\text{MIP}^\star = \text{RE}}" class="latex" title="{\text{MIP}^\star = \text{RE}}" />. With one author in common between the two papers: congratulations Zhengfeng!</p>
<p>Even though we just posted a paper, in a sense there is much more left to do. I am hopeful that our complexity-theoretic result will attract enough interest from the mathematicians’ community, and especially operator algebraists, for whom CEP is a central problem, that some of them will be willing to devote time to understanding the result. I also recognize that much effort is needed on our own side to make it accessible in the first place! I don’t doubt that eventually complexity theory will not be needed to obtain the purely mathematical consequences; yet I am hopeful that some of the ideas may eventually find their way into the construction of interesting mathematical objects (such as, who knows, a non-hyperlinear group).</p>
<p>That was a good Masters project…thanks Julia!</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/"><span class="datestr">at January 14, 2020 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/01/14/socal-theory-day-2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/01/14/socal-theory-day-2020/">SoCal Theory Day 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 20, 2020 UC Riverside https://www.cs.ucr.edu/~silas/ An all day event to celebrate the TCS research community in Southern California</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/01/14/socal-theory-day-2020/"><span class="datestr">at January 14, 2020 12:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/01/09/women-in-theory-2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/01/09/women-in-theory-2020/">Women in Theory 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 16-19, 2020 Simons Institute, Berkeley, CA https://womenintheory.wordpress.com/ Submission deadline: February 7, 2020 Registration deadline: February 7, 2020 The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the … <a href="https://cstheory-events.org/2020/01/09/women-in-theory-2020/" class="more-link">Continue reading <span class="screen-reader-text">Women in Theory 2020</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/01/09/women-in-theory-2020/"><span class="datestr">at January 09, 2020 04:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1547">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/01/09/women-in-theory-2018-call-for-application-2/">Women in Theory 2020 Call for Application</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The wonderful Women in Theory (WIT) biennial series of workshops started in 2008 and the 7th meeting will take place at   <a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>, Jun 16 – 19, 2020. Please see below the call for application.</p>
<p>WIT is one of my favorite (if not <em>the</em> favorite) program in the theory community. Many in our community share my enthusiasm (and theory groups fight for the honor of hosting these meetings). The reactions from past participants leave no room for doubt – this is an important a great and experience. So if you fit the workshop’s qualifications – please do yourself a favor and apply!</p>
<hr />
<p> </p>
<p>The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the field, as well as social events and activities. The motivation for the workshop is twofold. The first goal is to deliver an invigorating educational program; the second is to bring together theory women students from different departments and foster a sense of kinship and camaraderie.</p>
<p>The 7th WIT workshop will take place at  <a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>, Jun 16 – 19, 2020.</p>
<p><strong>Confirmed Speakers</strong>: <a href="https://www.cs.tau.ac.il/~mfeldman/">Michal Feldman</a> (Tel-Aviv University), <a href="https://simons.berkeley.edu/people/shafi-goldwasser">Shafi Goldwasser</a> (Simons, UC Berkeley)<br />
<strong>Organizers</strong>: <a href="http://researcher.ibm.com/view.php?person=us-talr">Tal Rabin</a> (IBM), <a href="http://www.math.ias.edu/~shubhangi/">Shubhangi Saraf </a>(Rutgers) and <a href="mailto:lisa.zhang@nokia-bell-labs.com">Lisa Zhang</a> (Bell Labs).<br />
<strong>Local Host Institution:  </strong><a href="https://simons.berkeley.edu/">Simons Institute at Berkeley</a>.<br />
<b>Local Arrangements</b>:<br />
<strong>Special Guest:</strong>  <a href="https://omereingold.wordpress.com/">Omer Reingold</a> (Stanford).<br />
<strong>Contact us:</strong> <a href="mailto:womenintheory2016@gmail.com">womenintheory2020@gmail.com</a>.</p>
<p><strong>To apply</strong>: click <a href="https://womenintheory.wordpress.com/apply">here</a>.</p>
<p><strong>Important dates:</strong><br />
<strong>Application deadline: </strong>Feb 7, 2020<br />
<strong>Notification of acceptance: </strong>March 15, 2020<br />
<strong>Workshop: </strong>June 16-19, 2020.</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/01/09/women-in-theory-2018-call-for-application-2/"><span class="datestr">at January 09, 2020 04:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1291">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/01/06/tcs-job-market-profiles/">TCS job market profiles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Earlier this academic year, CATCS <a href="https://thmatters.wordpress.com/2019/10/02/a-solicitation-for-tcs-job-market-profiles/">started a project</a> to collect the profiles of TCS candidates on the job market this academic year. Those profiles can now be found at <a href="http://tcsjobcandidates.web.unc.edu/job-candidates/">this webpage</a>. The webpage offers a limited amount of search functionality, and we are continuing to improve the layout and add features.</p>
<p>If you are at an institution that is hiring in theory, please feel free to use this database and share it with your colleagues as appropriate.</p>
<p>If you are going on the job market and would like to fill out a profile, please use <a href="http://tcsjobcandidates.web.unc.edu/application/">this form</a>. We will periodically update the webpage adding newer profiles.</p>
<p>Many thanks to Nicole Immorlica, Jack Snoeyink, and Chris Umans for putting together the website, and to all those who contributed their profiles. Questions and comments may be emailed to Shuchi Chawla.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/01/06/tcs-job-market-profiles/"><span class="datestr">at January 06, 2020 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/01/06/complexity-theory-with-a-human-face/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/01/06/complexity-theory-with-a-human-face/">Complexity Theory with a Human Face</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 1-4, 2020 Czech Republic http://users.math.cas.cz/talebanfard/workshop2020/ The workshop consists of excellent speakers giving enlightening tutorials on delectable aspects of complexity theory which will take place in Tábor, Czech Republic. The event is co-organized with Krajíček’s Fest celebrating the 60th birthday of Jan Krajíček.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/01/06/complexity-theory-with-a-human-face/"><span class="datestr">at January 06, 2020 01:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=1734">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/">The sum of a geometric series is all you need!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">I sometimes joke with my students about one of the main tools I have been using in the last ten years: the explicit sum of a geometric series. <em>Why is this?</em></p>



<h2>From numbers to operators</h2>



<p class="justify-text">The simplest version of this basic result for real numbers is the following: $$ \forall r \neq 1, \ \forall n \geq 0, \   \sum_{k=0}^n r^k = \frac{1-r^{n+1}}{1-r},$$ and is typically proved by multiplying the two sides by \(1-r\) and forming a telescoping sum. When \(|r|&lt;1\), we can let \(n\) tend to infinity and get $$ \forall |r| &lt;  1, \  \sum_{k=0}^\infty r^k = \frac{1}{1-r}.$$</p>



<p class="justify-text"><strong>Proofs without words.</strong> There is a number of classical proofs of the last identities, many of them <a href="https://en.wikipedia.org/wiki/Proof_without_words">without words</a>, presented in the beautiful series of books by Roger Nelsen [1, 2, 3]. I particularly like the two below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2019/12/triangles-1-1024x447.png" alt="" width="478" class="wp-image-1916" height="209" />Proof of the infinite sum of a geometric series with \(r=\frac{1}{2}.\) The area of the right triangle which is the half of a square with side length equal to \(2\), is equal to \(2\) and to the sum of the areas of the smaller triangles, that is, \(2 = \frac{1}{1- \frac{1}{2}}= 1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots\). Adapted from [3, p. 155].</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2019/12/rectangles_r.png" alt="" width="430" class="wp-image-2005" height="390" />Proof of the finite sum of a geometric series, started at \(k=0\) up to \(k=n=5\). The area of the full square of unit side length is equal to the sum of the areas of all yellow rectangles plus the pink one, that is, \(1 = (1-r) \sum_{k=0}^n r^k + r^{n+1}\). Adapted from [1, p. 118].</figure></div>



<p class="justify-text"><strong>High school: from philosophy to trigonometry.</strong> Before looking at extensions beyond real numbers, I can’t resist mentioning some of <a href="https://en.wikipedia.org/wiki/Zeno%27s_paradoxes">Zeno’s paradoxes</a>, like Achilles and the tortoise, which are intimately linked with the sum of a geometric series (and which were a highlight of my high school philosophy “career”).</p>



<p class="justify-text">Speaking of high school, it is interesting to note that there, the core identity of this blog post, is often used as \(a^{n+1}-b^{n+1} = (a-b)(a^n+a^{n-1}b+\cdots+ab^{n-1}+b^{n})\) in order to factorize polynomials (and not in the other way around like done later in this post).</p>



<p class="justify-text">Another nice elementary use of geometric series comes up with complex numbers, in order to compute sum of cosines, such as: $$\! \sum_{k=0}^n \! \cos k\theta = {\rm Re} \Big(\!\sum_{k=0}^n e^{ i k \theta}\!\Big) = {\rm Re} \Big(\!\frac{e^{i(n+1)\theta}-1}{e^{i\theta}-1}\!\Big) =  {\rm Re} \Big(\! \frac{ \sin \frac{n+1}{2} \theta e^{i(n+1)\theta/2}}{ \sin \frac{\theta}{2} e^{i\theta/2} }\!\Big) = \frac{  \sin \frac{n+1}{2} \theta}{\sin \frac{\theta}{2}} \cos \frac{n\theta}{2}. $$</p>



<p class="justify-text"><strong>Square matrices and operators.</strong> Within applied mathematics, the matrix and operator versions are the most useful. For \(A\) a square matrix or any linear operator, we have $$ \forall A \mbox{ such that } I-A \mbox{ is invertible}, \  \sum_{k=0}^n A^k = (I-A)^{-1}(I-A^{n+1}),$$ with the classical proof: \(\displaystyle (I – A)\sum_{k=0}^n A^k = \sum_{k=0}^n A^k – \sum_{k=1}^{n+1} A^k = I – A^{n+1}.\)</p>



<p class="justify-text">When \(\| A\| &lt;1\) for any <a href="https://en.wikipedia.org/wiki/Matrix_norm">matrix norm</a> induced from a vector norm, we can let \(n\) go to infinity, to obtain the <a href="https://en.wikipedia.org/wiki/Neumann_series">Neumann series</a> \(\displaystyle \sum_{k=0}^{+\infty} A^k = (I-A)^{-1}\).</p>



<p class="justify-text">We are now ready to talk about machine learning and optimization!</p>



<h2>Stochastic gradient for quadratic functions</h2>



<p class="justify-text">Matrix geometric series come up naturally when analyzing iterative algorithms based on linear recursions. In this blog post, I will focus on stochastic gradient descent (SGD) techniques to solve the following problem: $$ \min_{\theta \in \mathbb{R}^d} F(\theta) = \frac{1}{2} \mathbb{E} \big[ y – \theta^\top \Phi(x) \big]^2,$$ where the expectation is taken with respect to some joint distribution on \((x,y)\). We denote by \(\theta_\ast \in \mathbb{R}^d\) the minimizer of the objective function above (which is assumed to exist). We assume the feature vector \(\Phi(x)\) is high-dimensional, so that the moment matrix \(H = \mathbb{E} \big[ \Phi(x)\Phi(x)^\top \big]\) cannot be assumed to be invertible.</p>



<p class="justify-text"><strong>Stochastic gradient descent recursion. </strong>Starting from some initial guess \(\theta_0 \in \mathbb{R}^d\), typically \(\theta_0 =0\), the SGD recursion is: $$  \theta_n = \theta_{n-1} – \gamma ( \theta_{n-1}^\top \Phi(x_n) \, – y_n) \Phi(x_n),$$ where \((x_n,y_n)\) is an independent sample from the distribution mentioned above, and \(\gamma &gt; 0\) is the step-size. This algorithm dates back to <a href="https://en.wikipedia.org/wiki/Stochastic_approximation">Robbins and Monro</a> in the 50’s and is particularly adapted to machine learning as it updates the parameter \(\theta\) after each observation \((x_n,y_n)\) (as opposed to waiting for a full pass over the data).</p>



<p class="justify-text">Denoting \(\varepsilon_n = y_n – \theta_\ast^\top \Phi(x_n)\) the residual between the observation \(y_n\) and the optimal linear prediction \(\theta_\ast^\top \Phi(x_n)\), we can rewrite the SGD recursion as $$\theta_n = \theta_{n-1} – \gamma ( \theta_{n-1}^\top \Phi(x_n) \, – \theta_{\ast}^\top \Phi(x_n) -\, \varepsilon_n) \Phi(x_n),$$ leading to $$\theta_n \, – \theta_\ast = \big[ I – \gamma \Phi(x_n) \Phi(x_n)^\top \big] ( \theta_{n-1}- \theta_\ast) + \gamma \varepsilon_n \Phi(x_n).$$</p>



<p class="justify-text">This is a stochastic linear recursion on the deviation to optimum \(\theta_n \, – \theta_\ast\). The expectation of the SGD recursion is: $$\mathbb{E} [ \theta_n] \, – \theta_\ast = \big[ I – \gamma H \big] ( \mathbb{E}[ \theta_{n-1}] – \theta_\ast), $$ where \(H = \mathbb{E} \big[ \Phi(x) \Phi(x)^\top \big]\), and where we have used that  \(\gamma \varepsilon_n \Phi(x_n)\) has zero expectation (as a consequence of optimality conditions for \(\theta_\ast\)). This is exactly the gradient descent recursion on the expected loss (which cannot be run in practice because we only have access to a finite amount of data).</p>



<p class="justify-text"><strong>Simplification. </strong>The main difficulty in analyzing the stochastic recursion is the presence of two sources of randomness when compared to the gradient descent recursion: (A) some additive noise \(\gamma \varepsilon_n \Phi(x_n)\) independent of the current iterate \(\theta_{n-1}\) and with zero expectation, and (B) some multiplicative noise \(\gamma \big[  H – \Phi(x_n) \Phi(x_n)^\top \big] (\theta_{n-1} – \theta_\ast)\), which comes from the use of \(  I – \gamma \Phi(x_n) \Phi(x_n)^\top  \) instead of \(I – \gamma H\), and depends on the current iterate \(\theta_{n-1}\).</p>



<p class="justify-text">In this blog post, for simplicity, I will ignore the multiplicative noise and only focus on the additive noise, and thus consider the recursion $$\theta_n\, – \theta_\ast = ( I – \gamma H )  ( \theta_{n-1}- \theta_\ast) + \gamma \varepsilon_n \Phi(x_n).$$ Detailed studies with multiplicative noise can be found in [<a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">4</a>, <a href="http://proceedings.mlr.press/v38/defossez15.pdf">5</a>, <a href="http://jmlr.org/papers/volume18/16-335/16-335.pdf">6</a>, <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">7</a>], and lead to similar results. Moreover, I will assume (again for simplicity) that \(\varepsilon_n\) and \(\Phi(x_n)\) are independent, and that \(\mathbb{E} [ \varepsilon_n^2 ] = \sigma^2\) (which corresponds to uniform noise of variance \(\sigma^2\) on top of the optimal linear predictions); again, results directly extends without this assumption.</p>



<p class="justify-text"><strong>Bias / variance decomposition. </strong>Having a constant multiplicative term \(I – \gamma H\) in the recursion leads to an explicit formula: $$\theta_n – \theta_\ast = ( I – \gamma H ) ^n ( \theta_{0}- \theta_\ast) + \sum_{k=1}^n \gamma  ( I – \gamma H ) ^{n-k} \varepsilon_k \Phi(x_k),$$ which is now easy to analyze. It is the sum of a deterministic term depending on initial conditions,  and a zero mean term which is the sum of independent zero-mean terms due to the noise in the gradients. Thus, we can compute the expectation of the excess risk \(F(\theta_n)\, – F(\theta_\ast) = \frac{1}{2} ( \theta_n – \theta_\ast)^\top H ( \theta_n – \theta_\ast)\), as follows: $$\! \mathbb{E} \big[ F(\theta_n) \,- F(\theta_\ast)\big] = {\rm Bias} + { \rm Variance}, $$ where the bias term characterizes the forgetting of initial conditions: $$ {\rm Bias} = \frac{1}{2}  ( \theta_0 – \theta_\ast)^\top ( I – \gamma H ) ^{2n} H ( \theta_0 – \theta_\ast),  $$ and the variance term characterizes the effect of the noise: $${\rm Variance} =  \frac{\gamma^2 }{2}\! \sum_{k=1}^n \! \mathbb{E} \big[ \varepsilon_k^2 \Phi(x_k)^\top ( I – \gamma H ) ^{2n-2k}H  \Phi(x_k) \big] = \frac{\gamma^2  \sigma^2}{2}\! \sum_{k=1}^n \! {\rm tr} \big[  ( I – \gamma H ) ^{2n-2k}H^2   \big] .$$</p>



<p class="justify-text">The bias term is exactly the convergence rate of gradient descent on the expected risk, and can be controlled by upper-bounding the eigenvalues of the matrix  \(( I – \gamma H ) ^{2n} H\). As shown at the end of the post, when \(\gamma \leq \frac{1}{L}\), where \(L\) is the largest eigenvalue of \(H\), then this matrix has all eigenvalues less than \(1/(4n\gamma)\), leading to a bound on the bias term of $$ \frac{1}{8 \gamma n} \|\theta_0 – \theta_\ast\|^2.$$ We recover the traditional \(O(1/n)\) convergence rate of gradient descent. For the variance term we use the sum of a geometric series, to obtain the bound $$\frac{\gamma^2  \sigma^2}{2}   {\rm tr} \big[ H^2( I – (I – \gamma H)^2 )^{-1}  \big] = \frac{\gamma^2  \sigma^2}{2}  {\rm tr} \big[ H^2( 2\gamma H – \gamma^2 H^2 )^{-1}  \big] \leq \frac{\gamma  \sigma^2}{2}  {\rm tr}(H) .$$ While the bias term that characterizes the forgetting of initial conditions goes to zero as \(n\) goes to infinity, this is not the case for the variance term. This is the traditional lack of convergence of SGD with a constant step-size. In the left plot of the figure below, we compare deterministic gradient descent to stochastic gradient with a constant step-size. For convergence rates in higher dimension, see further below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img src="https://francisbach.com/wp-content/uploads/2019/12/paths_video-1.gif" alt="" width="564" class="wp-image-2039" height="236" />Gradient descent algorithms run from the same starting point. Left plot: plain gradient descent (GD), plain (non-averaged) gradient descent (SGD). Right plot: averaged SGD with uniform weights (ASGD-1) and with weights proportional to the iteration index (ASGD-k).</figure></div>



<p class="justify-text">Convergence can be obtained by using a decreasing step-size, typically of the order \(1 / \sqrt{n},\) leading to an overall convergence rate proportional to \(1 / \sqrt{n}.\) This can be improved through averaging, which I now present.</p>



<h2>Impact of averaging</h2>



<p class="justify-text">We consider the averaged iterate \(\bar{\theta}_n = \frac{1}{n+1} \sum_{k=0}^n \theta_k\), an off-line (no interaction with the stochastic recursion) averaging often referred to as Polyak–Ruppert averaging, after [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">8</a>, 9]. The averaged iterate can also be expressed as a linear function of initial conditions and noise variables as $$\bar{\theta}_n -\theta_\ast = \frac{1}{n+1} \sum_{k=0}^n  ( I – \gamma H ) ^k ( \theta_{0}- \theta_\ast) + \frac{\gamma}{n+1}   \sum_{k=1}^n \sum_{j=k}^n   ( I – \gamma H ) ^{j-k} \varepsilon_k \Phi(x_k).$$ Geometric series come in again! We can get a closed form for \(\bar{\theta}_n -\theta_\ast\) as: $$\frac{1}{n+1} (\gamma H)^{-1} \big[ I – ( I – \gamma H ) ^{n+1} \big] ( \theta_{0}- \theta_\ast) + \frac{\gamma}{n+1} (\gamma H)^{-1} \sum_{k=1}^n \big[ I – ( I – \gamma H ) ^{n-k+1} \big] \varepsilon_k \Phi(x_k).$$</p>



<p class="justify-text">The bound on \(\mathbb{E} \big[ F(\bar{\theta}_n) \,- F(\theta_\ast)\big] \), will here also be composed of a bias term and a variance term. </p>



<p class="justify-text"><strong>Bias.</strong> The bias term is equal to $$\frac{1}{2(n+1)^2}( \theta_{0}- \theta_\ast) ^\top   (\gamma H)^{-2} H \big[ I – ( I – \gamma H ) ^{n+1} \big]^2 ( \theta_{0}- \theta_\ast).$$ Using the fact that the eigenvalues of the matrix \( (\gamma H)^{-2} H \big[ I – ( I – \gamma H ) ^{n+1} \big]^2\) are all less than \((n+1)/\gamma\) (see proof at the end of the post), we obtain the upper bound $$ \frac{1}{2 (n+1)\gamma} \| \theta_0 – \theta_\ast\|^2,$$ which is essentially the same than with averaging (but, see an important difference in the discussion below, regarding the behavior for large \(n\)).</p>



<p class="justify-text"><strong>Variance.</strong> The variance term is equal to $$ \frac{\gamma^2}{2 (n+1)^2}  \sum_{k=1}^n \sigma^2 {\rm tr} \Big( \big[ I – ( I – \gamma H ) ^{n-k+1} \big]^2 (\gamma H)^{-2} H^2 \Big).$$ Using the positivity of the matrix \(( I – \gamma H )\), we finally obtain the following bound for variance term: $$  \frac{ \sigma^2 n {\rm tr}(I)}{2(n+1)^2}\leq \frac{\sigma^2 d}{2n}.$$ We now have a convergent algorithm, and we recover traditional quantities from the statistical analysis of <a href="https://en.wikipedia.org/wiki/Least_squares">least-squares regression</a>.</p>



<p class="justify-text"><strong>Experiments.</strong> Now, both bias and variance are converging at rate \(1/n\). See an illustration in two dimensions in the right plot of the figure above, as well as a convergence rates below. In these two figures, what differs is the decay of eigenvalues of \(H\) (fast in the first figure, slower in the second).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2019/12/rates_d3-1-1024x453.png" alt="" width="533" class="wp-image-2029" height="235" />Bias (left) and variance (right) terms for plain SGD, averaged SGD with uniform averaging (ASGD-1) and non-uniform averaging (ASGD-k). The matrix \(H\) is of dimension \(100 \times 100\) and has eigenvalues \(1/k^3\), \(k \in \{1,\dots,100\}\). Averaged over 100 replications.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2019/12/rates_d-1-1024x453.png" alt="" width="549" class="wp-image-2030" height="242" />Bias (left) and variance (right) terms for plain SGD, averaged SGD with uniform averaging (ASGD-1) and non-uniform averaging (ASGD-k). The matrix \(H\) is of dimension \(100 \times 100\) and has eigenvalues \(1/k\), \(k \in \{1,\dots,100\}\). Averaged over 100 replications.</figure></div>



<p>We can make the following observations:</p>



<ul class="justify-text"><li>Depending on the amount of noise in the gradients, the sum of the variance and the bias terms will either be dominated by one of the two; typically the bias term in early iterations, and then the variance term (see more  details in [<a href="http://proceedings.mlr.press/v38/defossez15.pdf">5</a>]).</li><li>The variance term of non-averaged SGD is converging to a constant (while the bias term is exactly the one of regular gradient descent).</li><li>With averaging, the variance terms decay as a line in a log-log plot. The reader with good eyes can check that the slope is indeed -1, thus illustrating the convergence rate in \(1/n\) (here the bound is tight). For the variance terms (right plots), there is no significant difference between the two eigenvalue decays.</li><li>The bias terms have different behaviors for the two decays. For the fast decays (top plot), the bounds in \(1/n\) are reasonably tight (lines of slope -1). For slower decay, we see some strong-convexity behavior entering the scene, as the slowest eigenvalue is 1/100 and the number of iterations is far larger than 100, and thus the optimization problem looks strongly convex, and then the bias term of plain SGD (which corresponds to deterministic gradient descent) converges exponentially fast, while the two averaging techniques decay as powers of \(n\) (again, the acute reader can spot slopes of -2 and -4, and the smart reader can explain why; more on this in a future post dedicated to SGD).</li></ul>



<p class="justify-text"><strong>Pros and cons of averaging.</strong> Overall, we can see that averaging may slow down the forgetting of initial conditions, while it makes the method robust to noise in the gradients. The trade-off depends on the amount of noise, but in most high-dimensional learning problems and for the accuracies practitioners are interested in (no need to have an excess risk of \(10^{-5}\) then the best risk is of order \(10^{-1}\)), the bias term is the one which is seen the most. Thus, a less-aggressive form of averaging that puts more weights on later iterates seems advantageous (pink curve above). More on this in a future blog post.</p>



<p class="justify-text"><strong>Beyond least-squares.</strong> In this blog post, to illustrate the use of sums of geometric series, I have focused on an idealized version (no multiplicative noise) of least-squares regression. For other losses, e.g., logistic loss, the analysis is more involved, and averaging does not lead to a converging algorithm, but transforms a term in \(\gamma\) into a term in \(\gamma^2\), which is still a significant improvement when the step-size \(\gamma\) is small (see [<a href="https://arxiv.org/pdf/1707.06386">10</a>] for more details).</p>



<h3>Extensions</h3>



<p class="justify-text">The sum of a geometric series can be extended in a variety of ways. For example, we can take the derivative with respect to \(r\), to get $$ \forall r \neq 1, \  \sum_{k=1}^n k r^{k-1} = \frac{1-r^{n+1}}{(1-r)^2} – \frac{ (n+1) r^n}{1-r} = \frac{1 + n r^{n+1} – (n+1) r^n }{(1-r)^2}.$$ This is useful for example to compute the performance of the weighted average \(\frac{2}{n(n+1)} \sum_{k=1}^n k \theta_k\). This can be extended further with the <a href="https://en.wikipedia.org/wiki/Binomial_series">binomial series</a> $$ (1-r)^{-1-\beta} = \sum_{k=0}^\infty { k + \beta \choose k} r^k. $$</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I have essentially used the sum of a geometric series as an excuse to talk about stochastic gradient descent, but there are other places where such series pop out, such as in the analysis of Markov chains and the associated ergodic theorems [11], which are ubiquitous in the analysis of simulation algorithms.</p>



<p class="justify-text">This year, I am still planning to post every first Monday of the month. Expect additional posts on acceleration, stochastic gradient, and orthogonal polynomials, but also new topics should be covered, always with connections with machine learning. </p>



<p class="justify-text">Happy new year!</p>



<h2>References</h2>



<p class="justify-text">[1] Roger B. Nelsen, <em>Proofs without Words: Exercises in Visual Thinking</em>, Mathematical Association of America, 1997.<br />[2] Roger B. Nelsen, <em>Proofs without Words II: More Exercises in Visual Thinking</em>, Mathematical Association of America, 2000.<br />[3] Roger B. Nelsen, <em>Proofs Without Words III: Further Exercises in Visual Thinking</em>, Mathematical Association of America, 2015.<br />[4] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)</a>. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2013.<br />[5] Alexandre Defossez, Francis Bach. <a href="http://proceedings.mlr.press/v38/defossez15.pdf">Averaged Least-Mean-Square: Bias-Variance Trade-offs and Optimal Sampling Distributions</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2015.<br />[6] Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. <a href="http://jmlr.org/papers/volume18/16-335/16-335.pdf">Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression</a>. <em>Journal of Machine Learning Research</em>, 18(101):1−51, 2017.<br />[7] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Aaron Sidford. <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification</a>. <em>Journal of Machine Learning Research</em>, 18(223):1−42, 2018.<br />[8] Boris T. Polyak, Anatoli B. Juditsky. <a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">Acceleration of Stochastic Approximation by Averaging</a>. <em>SIAM Journal on Control and Optimization</em>. 30(4):838-855, 1992.<br />[9] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical Report 781, Cornell University Operations Research and Industrial Engineering, 1988.<br />[10] Aymeric Dieuleveut, Alain Durmus, Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains</a>. To appear in <em>Annals of Statistics</em>, 2020.<br />[11] Sean Meyn and Richard L. Tweedie. <em><a href="https://www.cambridge.org/fr/academic/subjects/statistics-probability/applied-probability-and-stochastic-networks/markov-chains-and-stochastic-stability-2nd-edition?format=PB">Markov Chains and Stochastic Stability</a></em>. Cambridge University Press, 2009.</p>



<h2>Detailed computations</h2>



<p><strong>Bounds on eigenvalues – I.</strong> In order to show the first desired inequality on eigenvalues, we simply need to show that \((1-t)^{2n} t \leq 1/(4n)\) for any \(t \in [0,1]\), which is the consequence of $$(1-t)^{2n} t \leq (e^{-t})^{2n} t \leq \frac{1}{2n} \sup_{u \geq 0} e^{-u} u = \frac{1}{2e n} \leq \frac{1}{4n}.$$</p>



<p class="justify-text"><strong>Bounds on eigenvalues – II.</strong> In order to show the second desired inequality on eigenvalues, we simply need to show that \(( 1 – ( 1 – t)^{n+1}) \leq \sqrt{n+1} \sqrt{t}\), for any \(t \in [0,1]\). This is a simple consequence of the straightforward inequality \(( 1 – ( 1 – t)^{n+1}) \leq 1\) and the inequality \(( 1 – ( 1 – t)^{n+1}) \leq (n+1) t \), which itself can be obtained by integrating the inequality \((1-u)^n \leq 1\) between \(0\) and \(t\).</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/"><span class="datestr">at January 06, 2020 10:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=684">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/01/01/publish-and-perish/">Publish and perish</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://cacm.acm.org/magazines/2020/1/241717-publish-and-perish/fulltext">Moshe Vardi’s latest insight in the Communications of the ACM </a>(whose title we adopt for this post) agrees with our previous post “<a href="https://emanueleviola.wordpress.com/2019/08/04/because-of-pollution-conferences-should-be-virtual/">Because of pollution, conferences should be virtual.</a>” Vardi calls for “sweeping policy change […] requiring that authors of accepted papers that must fly to  participate in a conference may opt out from in-person involvement and  contribute instead by video.”  Vardi gives some indication of the environmental impact of the travel-based system, and further suspects that in-person conference participation is “much less valuable than we would like to believe.”</p>



<p>These issues are closely related to the decades-old discussion of journals vs. conferences. <a href="https://emanueleviola.wordpress.com/tag/utopia/">Several older posts on this blog </a>were devoted to that. Lance Fortnow, back in 2009, wrote that it’s <a href="https://cacm.acm.org/magazines/2009/8/34492-viewpoint-time-for-computer-science-to-grow-up/abstract">Time for computer science to grow up</a>.  The full text of this article is premium content, but you can read a pre-publication version <a href="http://www.cs.uchicago.edu/~fortnow/papers/growup.pdf">here</a>. Basically, he argues in favor of a journal-based publication system.</p>



<p>Apparently in response, judging from the title, Boaz Barak wrote a piece titled <a href="https://cacm.acm.org/magazines/2016/6/202644-computer-science-should-stay-young/abstract">Computer science should stay young.</a> I can’t quickly find a link to the whole thing, but his bottom line is online “I disagree with the conclusion that we should transition to a classical journal-based model similar to that of other fields. I believe conferences offer a number of unique advantages that have helped make computer science dynamic and successful, and can continue to do so in the future.”</p>



<p>I disagree that conferences are young. They belong to the BI (before internet) era, and so look rather anchored in the past to me.  Historically, I also suppose in-person discussion predates writing, though this is irrelevant.  What is young is the health impact of pollution (Fortnow and Barak’s pieces don’t touch on health issues). (By health impact I include climate change, but I prefer not to use that term for various reasons.)</p>



<p>And what is young and cool is arxiv overlay journals, TCS+ talks, videoconferences,  <a href="https://eccc.weizmann.ac.il/">ECCC</a>, etc.</p>



<p>Instead, we impose on our community most inconvenient transoceanic flights.  To end I’ll quote from <a href="http://www.wisdom.weizmann.ac.il/~oded/MC/269.html">Oded Goldreich’s my choices</a>:</p>



<p><b>Phoenix in June:</b> […] One must be out of their mind to hold a conference under such weather conditions. I guess humans can endure such weather conditions and even worse ones, but why choose to do so? Why call upon people from all over the world to travel to one of the least comfortable locations (per the timing)?</p>


<p><br /><br /></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/01/01/publish-and-perish/"><span class="datestr">at January 01, 2020 05:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7610">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/12/30/a-bet-for-the-new-decade/">A bet for the new decade</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am in <a href="https://sites.google.com/view/tau-theory-fest/home">Tel Aviv Theory Fest</a> this week – a fantastic collection of talks and workshops organized by Yuval Filmus , Gil Kalai, Ronen Eldan, and Muli Safra.</p>



<p>It was a good chance to catch up with many friends and colleagues. In particular I met Elchanan Mossel and Subhash Khot, who asked me to serve as a “witness” for their bet on the unique games conjecture. I am recording it here so we can remember it a decade from noe.</p>



<p>Specifically, Elchanan bets that the Unique Games conjecture will be proven in the next decade – sometime between January 1, 2020 and December 31, 2029 there will be a paper uploaded to the arxiv with a correct proof of the conjecture. Subhash bets that this won’t happen. They were not sure what to bet on, but eventually agreed to take my offer that the loser will have to collaborate on a problem chosen by the winner, so I think science will win in either case. (For what it’s worth, I think there is a good chance that Subhash will lose the bet because he himself will prove the UGC in this decade, though it’s always possible Subhash can both win the bet and prove the UGC if he manages to do it by tomorrow <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /> )</p>



<p></p>



<figure class="wp-block-image size-large"><img src="https://windowsontheory.files.wordpress.com/2019/12/img-1240.jpg?w=1024" alt="" class="wp-image-7612" /></figure>



<p>The conference itself is, as I mentioned, wonderful with an amazing collection of speakers.  Let me mention just a couple of talks from this morning. Shafi Goldwasser talked about “Law and Algorithms”. There is a recent area of research studying how to regulate algorithms, but Shafi’s talk focused mostly on the other direction: how algorithms and cryptography can help achieve legal objectives such as the “right to be forgotten” or the ability to monitor secret proceedings such as wiretap requests. </p>



<p>Christos Papadimitriou  talked about “Language, Brain, and Computation”.  Christos is obviously excited about understanding the language mechanisms in the brain. He said that studying the brain gives him the same feeling that you get when you sit in a coffee shop in Cambridge and hear intellectual discussions all around you: you don’t understand why everyone is not dropping everything they are doing and come here. (Well, his actual words were “sunsets over the Berkeley hills” but I think the Cambridge coffee shops are a better metaphor <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /> )</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/12/30/a-bet-for-the-new-decade/"><span class="datestr">at December 30, 2019 09:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/12/28/quantum-school-lecture-notes-videos-and-more-guest-post-by-dorit-aharonov/">A crash course on the math of quantum computing (guest post by Dorit Aharonov)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[The post below is by Dorit Aharonov who co-organized the wonderful school on quantum computing last week which I attended and greatly enjoyed. –Boaz] </em></p>



<p><strong>TL;DR: </strong> Last week we had a wonderful one-week intro course into the math of quantum computing at Hebrew U;  It included a one day crash course on the basics, and 7 mini-courses on math-oriented research topics (quantum delegation, Hamiltonian complexity, algorithms and more) by top-notch speakers. Most importantly – it is all online, and could be very useful if you want to take a week or two to enter the area and don’t know <a href="https://iias.huji.ac.il/SchoolCSE4" target="_blank" rel="noreferrer noopener">where to start</a>.   </p>



<p><br /><br /><br /></p>



<p>Hi Theory people!  </p>



<p>I want to tell you about a 5-days winter school called “<a href="https://iias.huji.ac.il/SchoolCSE4" target="_blank" rel="noreferrer noopener">The Mathematics of Quantum Computation</a>“, which we (me, Zvika Brakerski, Or Sattath and Amnon Ta-Shma) organized last week at the Institute for advanced studies (IIAS) at the Hebrew university in Jerusalem. </p>



<p>There were two reasons I happily agreed to Boaz’s suggestion to write a guest blogpost about this school. <br /><br />a) The school was really great fun. We enjoyed it so much, that I think you might find it interesting to hear about it even if you were not there, or are not even into quantum computation. <br /><br />And b), it might actually be useful for you or your quantum-curious friends. We put all material online, with the goal in mind that after the school, this collection of talks+written material will constitute all that is needed for an almost self-contained <strong>very-intensive-one-week-course of introduction into the mathematical side of quantum computation</strong>; I think this might be of real use for any theoretical computer scientist or mathematician interested in entering this fascinating but hard-to-penetrate area, and not knowing where to start.<br />Before telling you a little more about what we actually learned in this school, let’s start with some names and numbers. We had:  </p>



<ul><li>160 participants (students and faculty) from all over the world. </li><li>7 terrific speakers: Adam Bouland (UC Berkeley), Sergey Bravyi (IBM), Matthias Christandl (Coppenhagen), András Gilyén (Caltech), Sandy Irani (UC Irvine), Avishay Tal (Berkeley), and Thomas Vidick (Caltech); </li><li>2 great TAs:  András Gilyén (Caltech) and Chinmay Nirkhe (UC Berkeley)</li><li>4 busy organizers: myself (Hebrew U), Zvika Brakerski (Weizmann), Or Sattath (Ben Gurion U), and Amnon Ta-Shma (Tel Aviv U)</li><li>1 exciting and very intensive <a href="https://docs.google.com/document/d/1OcFzE1PHxBN4l87sOQB033FZLTgY934GHxnpVKi3NaM/edit" target="_blank" rel="noreferrer noopener">program</a></li><li>5 challenging and fascinating days of <a href="https://www.youtube.com/playlist?list=PLTn74Qx5mPsS2dqTpEq_2zxq8kWBmPUfb" target="_blank" rel="noreferrer noopener"> talks</a>, <a href="https://drive.google.com/drive/u/0/folders/1n6W3Yjq2TYsyRwrocbN2bWK-xpyzkqSM" target="_blank" rel="noreferrer noopener">problem sessions</a> and really <a href="https://shvil.im/" target="_blank" rel="noreferrer noopener">nice food</a>.   </li><li>1 great <a href="https://huji.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=599bc3e9-855e-42c1-ab1d-ab1f0062c481" target="_blank" rel="noreferrer noopener">Rabin’s lecture</a> by Boaz Barak (Harvard)</li><li>1 beautiful <a href="https://www.youtube.com/watch?v=DxrxXneg3lU&amp;list=PLTn74Qx5mPsS2dqTpEq_2zxq8kWBmPUfb&amp;index=21&amp;t=0s" target="_blank" rel="noreferrer noopener">Quantum perspective lecture</a> by Sergey Bravyi (IBM)</li><li>8 panelists in the <a href="https://www.youtube.com/watch?v=H4t2G2gay7Q&amp;feature=youtu.be" target="_blank" rel="noreferrer noopener">supremacy panel</a> we had on the fifth day: Sandy Irani (UC Irvine), our wise moderator, and 7 panelists on stage and online: myself, Scott Aaronson (Austin, online), Boaz Barak, Adam Bouland, Sergio Boixo (Google, online), Gil Kalai (Hebrew U), and Umesh Vazirani (UC Berkeley, online) </li><li>8 brave speakers in the gong show, our very last session, each talking for 3 minutes;    </li><li>1 group-tour to 1 UNESCO site (<a href="https://everything-everywhere.com/caves-of-maresha-and-bet-guvrin-in-the-judean-lowlands-as-a-microcosm-of-the-land-of-the-caves/" target="_blank" rel="noreferrer noopener">Tel Maresha</a>) and <a href="https://www.srigim-beer.co.il/" target="_blank" rel="noreferrer noopener">6 beers tasted</a> by ~80 tour participants</li><li>3 problem sets with 43 <a href="https://drive.google.com/drive/u/0/folders/1n6W3Yjq2TYsyRwrocbN2bWK-xpyzkqSM" target="_blank" rel="noreferrer noopener">problems</a> and (!) their <a href="https://drive.google.com/drive/u/0/folders/1n6W3Yjq2TYsyRwrocbN2bWK-xpyzkqSM" target="_blank" rel="noreferrer noopener">solution</a>s.   </li></ul>



<p>So why did we decide to organize this particular quantum school, given the many quantum schools around? Well, the area of quantum computation is just bursting now with excitement and new mathematical challenges; But there seems to be no easy way for theoreticians to learn about all these things unless you are already in the loop… The (admittedly) very ambitious goal of the school was to assume zero background in quantum computation, and quickly bring people up to speed on six or seven of the most interesting mathematical research forefronts in the area. </p>



<p>The first day of the school was intended to put everyone essentially on the same page: it included four talks about the very basics (qubits by Or Sattath, circuits, by myself, algorithms by Adam Bouland, and error correction by Sergey Bravyi). By the end of this first day everyone was at least supposed to be familiar with the basic concepts, and capable of listening to the mini-courses to follow. The rest of the school was devoted mainly to those mini-courses, whose topics included what I think are some of the most exciting topics on the more theoretical and mathematical side of quantum computation.</p>



<p>Yes, it was extremely challenging… the good thing was that we had two great TAs, András and Chinmay, who helped prepare problem sets, which people actually seriously tried to solve (!) during the daily one+ hour TA problem-solving sessions (with the help of the team strolling around ready to answer questions…). It seems that this indeed helped people follow, despite the fact that we did get into some hard stuff in those mini-courses… The many questions that were asked throughout the school proved that many people were following and interested till the bitter end. </p>



<p>So here is a summary of the mini-courses, by order of appearance. <br />I added some buzz words of interesting related mathematical notions so that you know where these topics might lead you if you take the paths they suggest.   <br /></p>



<ul><li> Thomas Vidick gave a three-lecture wonderfully clear mini-course providing an intro to the recently very active and exciting area of <strong>quantum verification and delegation</strong>, connecting cryptography and quantum computational complexity. [Thomas didn’t have time to talk about it, but down the road this eventually connects to  approximate representation theory, as well as to Connes embedding conjecture, and more.]</li><li> Sandy Irani gave a beautiful exposition (again, in a a three lecture mini-course) on <strong>quantum Hamiltonian complexity</strong>. Sandy started with Kitaev’s quantum version of the Cook Levin theorem, showing that the local Hamiltonian problem is quantum NP complete; she then explained how this can be extended to more physically relevant questions such as translationally invariant 1D systems, questions about the thermodynamical limit, and more. [This topic is related to open questions such as quantum PCP, which was not mentioned in the school, as well as to beautiful recent results about undecidability of the spectral gap problem, and more.]    </li><li> Matthias Christandl gave an exciting two-lecture mini-course on the fascinating connection between <strong>tensor ranks and matrix product multiplication</strong>. Starting from what seemed to be childish games with small pictures in his first talk, he cleverly used those as his building blocks in his second talk, to enable him to talk about Strassen’s universal spectral points program for approaching the complexity of matrix multiplication, asymptotic ranks, border ranks and more. That included also very beautiful pictures of polytopes! Matthias explained the connection that underlines this entire direction, between entanglement properties of three body systems, with these old combinatorial problems.  </li><li> Avishay Tal gave a really nice two-lecture exposition on his recent breakthrough result with Ran Raz, proving that <strong>quantum polynomial time computation is not contained in the polynomial Hierarchy, in the oracle model</strong>. This included talking about AC0, a problem called forrelation, Fourier expansion, Gaussians and much more.</li><li>  András Gilyén gave a wonderful talk about a recent development: the evolution of the <strong>singular value approach to quantum algorithms</strong>. He left us all in awe showing that essentially almost any quantum algorithm you can think of falls into this beautiful framework… Among other things, he mentioned Chebychev’s polynomials, quantum walks, Hamiltonian simulations, and more. What else can be done with this framework remains to be seen.</li><li>Sergey Bravyi gave two talks (on top of his intro to quantum error correction). The first was as part of a monthly series at Hebrew university, called  “quantum perspectives”; in this talk, Sergey gave a really nice exposition of his breakthrough result (with Gosset and Konig) demonstrating an <strong><em>information theoretical</em> separation between quantum and classical constant depth circuits</strong>; this uses in a clever way the well known quantum magic square game enabling quantum correlations to win with probability one, while classical correlations are always bounded away from one;  somehow this result manages to cleverly turn this game into a computational advantage. In Sergey’s last talk, he gave the basics of the beautiful topic of <strong>stoqaustic Hamiltonians </strong>–  a model in between quantum Hamiltonians and classical constrained satisfaction problems, which poses many fundamental and interesting open questions (and is tightly related to classical Markov chains, and Markov chain Monte Carlo). </li><li>Finally, Adam Bouland gave two superb talks on <strong>quantum supremacy</strong>, explaining the beautiful challenges in this area – including his recent average case to worst case hardness results about sampling using quantum circuits, which is related to Google’s supremacy experiment.  </li><li>Ah, I also gave a talk – it was about three of the many different equivalent models of quantum computation – <strong>adiabatic computation</strong>, <strong>quantum walks</strong>, and the <strong>Jones polynomial </strong>(I also briefly mentioned a differential geometry model). The talk came out way too disordered in my mind (never give a talk when you are an organizer!), but hopefully it gave some picture about the immense variety of ways to think about quantum computation and quantum algorithms.</li></ul>



<p>In addition to the main lectures, we also had some special events intertwined: </p>



<ul><li>Boaz Barak gave the distinguished annual Rabin lecture, joint with the CS colloquium; His talk, which was given the intriguing title  <strong>“Quantum computing and classical algorithms: The best of frenemies”</strong>, focused on the fickle relationships between quantum and classical algorithms. The main players in this beautiful talk were SDPs and sums of squares, and it left us with many open questions.     </li><li>Last but not least, we had an <strong>international panel about the meaning of Google’s recent experiment claiming supremacy</strong>, joined by Sergio Boixo from Google explaining the experiment, as well as Scott Aaronson and Umesh Vazirani who woke up very early in the US to join us. I feared we would have some friction and fist fights, but this actually became a deep and interesting discussion! We went with quite some depth into the most important question in my mind about the supremacy experiment, which is the issue of noise; Unbelievably, it all went well even from the technological aspect! I really recommend watching this <a href="https://www.youtube.com/watch?v=H4t2G2gay7Q&amp;feature=youtu.be" target="_blank" rel="noreferrer noopener">discussion</a>. </li></ul>



<p>So, we had a great time…. and as I said, one of the best things is that it is all recorded and saved. You are welcome to follow the <a href="https://docs.google.com/document/d/1OcFzE1PHxBN4l87sOQB033FZLTgY934GHxnpVKi3NaM/edit" target="_blank" rel="noreferrer noopener">program</a>, watch the recorded <a href="https://www.youtube.com/playlist?list=PLTn74Qx5mPsS2dqTpEq_2zxq8kWBmPUfb" target="_blank" rel="noreferrer noopener">talks</a>, consult the <a href="https://drive.google.com/drive/u/0/folders/1M9F0zha6LY8DX58eKxjkiI1Dxq9uvptE" target="_blank" rel="noreferrer noopener">slides</a><a href="https://drive.google.com/drive/u/0/folders/1y_sHH1NsKzHeZwQ_xa2aVR_HKgOczytm" target="_blank" rel="noreferrer noopener">, lecture notes</a><a href="https://drive.google.com/drive/u/0/folders/1n6W3Yjq2TYsyRwrocbN2bWK-xpyzkqSM" target="_blank" rel="noreferrer noopener">, exercises and solutions</a> and also read the <a href="https://docs.google.com/document/d/1s-DLl_M-Dpqh9RdSqn6f9l01e09AaLndMlbxDEmlPrw/edit?ts=5dc7c2b3" target="_blank" rel="noreferrer noopener">reading material</a> if you want to extend your knowledge beyond what is covered in the school. In case you know of any math or TCS-oriented person who wants to enter the field and start working on some problem at the forefront of research, just send him or her this post, or the link of the <a href="https://iias.huji.ac.il/SchoolCSE4" target="_blank" rel="noreferrer noopener">school’s website</a>;  It will take a very intensive week (well, maybe two) of following lectures and doing the exercises, but by the end of that time, one is guaranteed to be no longer a complete amateur to the area, as the set of topics covered gives a pretty good picture of what is going on in the field.   <br />  </p>



<p>Last but not least, I would like to thank the Israeli quantum initiative, Vatat, and the IIAS, for their generous funding which enabled this school and the funding of students; the IIAS team for their immense help in organization;   and of course, thanks a lot to all participants who attended the school!<br /></p>



<p>Wishing everyone a very happy year of 2020,  </p>



<p>Dorit</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/12/28/quantum-school-lecture-notes-videos-and-more-guest-post-by-dorit-aharonov/"><span class="datestr">at December 28, 2019 10:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
