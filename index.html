<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 20, 2019 12:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07504">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07504">Minimum-Width Double-Strip and Parallelogram Annulus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bae:Sang_Won.html">Sang Won Bae</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07504">PDF</a><br /><b>Abstract: </b>In this paper, we study the problem of computing a minimum-width double-strip
or parallelogram annulus that encloses a given set of $n$ points in the plane.
A double-strip is a closed region in the plane whose boundary consists of four
parallel lines and a parallelogram annulus is a closed region between two
edge-parallel parallelograms. We present several first algorithms for these
problems. Among them are $O(n^2)$ and $O(n^3 \log n)$-time algorithms that
compute a minimum-width double-strip and parallelogram annulus, respectively,
when their orientations can be freely chosen.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07504"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07352">Robust Algorithms for the Secretary Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bradac:Domagoj.html">Domagoj Bradac</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07352">PDF</a><br /><b>Abstract: </b>In classical secretary problems, a sequence of $n$ elements arrive in a
uniformly random order, and we want to choose a single item, or a set of size
$K$. The random order model allows us to escape from the strong lower bounds
for the adversarial order setting, and excellent algorithms are known in this
setting. However, one worrying aspect of these results is that the algorithms
overfit to the model: they are not very robust. Indeed, if a few "outlier"
arrivals are adversarially placed in the arrival sequence, the algorithms
perform poorly. E.g., Dynkin's popular $1/e$-secretary algorithm fails with
even a single adversarial arrival.
</p>
<p>We investigate a robust version of the secretary problem. In the Byzantine
Secretary model, we have two kinds of elements: green (good) and red (rogue).
The values of all elements are chosen by the adversary. The green elements
arrive at times uniformly randomly drawn from $[0,1]$. The red elements,
however, arrive at adversarially chosen times. Naturally, the algorithm does
not see these colors: how well can it solve secretary problems?
</p>
<p>We give algorithms which get value comparable to the value of the optimal
green set minus the largest green item. Specifically, we give an algorithm to
pick $K$ elements that gets within $(1-\varepsilon)$ factor of the above
benchmark, as long as $K \geq \mathrm{poly}(\varepsilon^{-1} \log n)$. We
extend this to the knapsack secretary problem, for large knapsack size $K$.
</p>
<p>For the single-item case, an analogous benchmark is the value of the
second-largest green item. For value-maximization, we give a $\mathrm{poly}
\log^* n$-competitive algorithm, using a multi-layered bucketing scheme that
adaptively refines our estimates of second-max over time. For
probability-maximization, we show the existence of a good randomized algorithm,
using the minimax principle.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07352"><span class="datestr">at November 20, 2019 12:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07287">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07287">A Crossing Lemma for Families of Jordan Curves with a Bounded Intersection Number</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Maya Bechler-Speicher <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07287">PDF</a><br /><b>Abstract: </b>A family of closed simple (i.e., Jordan) curves is {\it $m$-intersecting} if
any pair of its curves have at most $m$ points of common intersection. We say
that a pair of such curves {\it touch} if they intersect at a single point of
common tangency. In this work we show that any $m$-intersecting family of $n$
Jordan curves in general position in the plane contains
$O\left(n^{2-\frac{1}{3m+15}}\right)$ touching pairs.\footnote{A family of
Jordan curves is in general position if no three of its curves pass through the
same point, and no two of them overlap. The constant of proportionality with
the $O(\cdot)$-notation may depend on $m$.}
</p>
<p>Furthermore, we use the string separator theorem of Fox and Pach \cite{FP10}
in order to establish the following Crossing Lemma for contact graphs of Jordan
curves: Let $\Gamma$ be an $m$-intersecting family of closed Jordan curves in
general position in the plane with exactly $T=\Omega(n)$ touching pairs of
curves, then the curves of $\Gamma$ determine
$\Omega\left(T\cdot\left(\frac{T}{n}\right)^{\frac{1}{9m+45}}\right)$
intersection points.
</p>
<p>This extends the similar bounds that were previously established by Salazar
for the special case of pairwise intersecting (and $m$-intersecting) curves.
Specializing to the case at hand, this substantially improves the bounds that
were recently derived by Pach, Rubin and Tardos for arbitrary families of
Jordan curves.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07287"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07255">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07255">Deep geometric matrix completion: Are we doing it right?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyarski:Amit.html">Amit Boyarski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vedula:Sanketh.html">Sanketh Vedula</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bronstein:Alex.html">Alex Bronstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07255">PDF</a><br /><b>Abstract: </b>We address the problem of reconstructing a matrix from a subset of its
entries. Current methods, branded as geometric matrix completion, augment
classical rank regularization techniques by incorporating geometric information
into the solution. This information is usually provided as graphs encoding
relations between rows/columns. In this work we propose a simple spectral
approach for solving the matrix completion problem, via the framework of
functional maps. We introduce the zoomout loss, a multiresolution spectral
geometric loss inspired by recent advances in shape correspondence, whose
minimization leads to state-of-the-art results on various recommender systems
datasets. Surprisingly, for some datasets we were able to achieve comparable
results even without incorporating geometric information. This puts into
question both the quality of such information and current methods' ability to
use it in a meaningful and efficient way.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07255"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07166">On the existence of four or more curved foldings with common creases and crease patterns</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Honda:Atsufumi.html">Atsufumi Honda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naokawa:Kosuke.html">Kosuke Naokawa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saji:Kentaro.html">Kentaro Saji</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umehara:Masaaki.html">Masaaki Umehara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamada:Kotaro.html">Kotaro Yamada</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07166">PDF</a><br /><b>Abstract: </b>Consider a curve $\Gamma$ in a domain $D$ in the plane $\boldsymbol{R}^2$.
Thinking of $D$ as a piece of paper, one can make a curved folding in the
Euclidean space $\boldsymbol{R}^3$. This can be expressed as the image of an
`origami map' $\varphi:D\to \boldsymbol{R}^3$ such that $\Gamma$ is the
singular set of $\varphi$, the word `origami' coming from the Japanese term for
paper folding. We call the singular set image $C:=\varphi(\Gamma)$ the crease
of $\varphi$ and the singular set $\Gamma$ the crease pattern of $\varphi$. We
are interested in the number of origami maps whose creases and crease patterns
are $C$ and $\Gamma$ respectively. Two such possibilities have been known. In
the previous authors' work, two other new possibilities and an explicit example
with four such non-congruent distinct curved foldings were established. In this
paper, we show that the case of four mutually non-congruent curved foldings
with the same crease and crease pattern occurs if and only if $\Gamma$ and $C$
do not admit any symmetries. Moreover, when $C$ is a closed curve, we show that
there are infinitely many distinct possibilities for curved foldings with the
same crease and crease pattern, in general.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07166"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06958">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06958">Regularized Weighted Low Rank Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ban:Frank.html">Frank Ban</a>, David Woodruff, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qiuyi.html">Qiuyi Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06958">PDF</a><br /><b>Abstract: </b>The classical low rank approximation problem is to find a rank $k$ matrix
$UV$ (where $U$ has $k$ columns and $V$ has $k$ rows) that minimizes the
Frobenius norm of $A - UV$. Although this problem can be solved efficiently, we
study an NP-hard variant of this problem that involves weights and
regularization. A previous paper of [Razenshteyn et al. '16] derived a
polynomial time algorithm for weighted low rank approximation with constant
rank. We derive provably sharper guarantees for the regularized version by
obtaining parameterized complexity bounds in terms of the statistical dimension
rather than the rank, allowing for a rank-independent runtime that can be
significantly faster. Our improvement comes from applying sharper matrix
concentration bounds, using a novel conditioning technique, and proving
structural theorems for regularized low rank problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06958"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06924">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06924">Approximating the Distance to Monotonicity of Boolean Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pallavoor:Ramesh_Krishnan_S=.html">Ramesh Krishnan S. Pallavoor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raskhodnikova:Sofya.html">Sofya Raskhodnikova</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06924">PDF</a><br /><b>Abstract: </b>We design a nonadaptive algorithm that, given a Boolean function $f\colon
\{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n,
1/\alpha)$ queries and returns an estimate that, with high probability, is an
$\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity.
Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the
distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires
$2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n,
1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a
question of Seshadhri (Property Testing Review, 2014) for the case of
nonadaptive algorithms. Approximating the distance to a property is closely
related to tolerantly testing that property. Our lower bound stands in contrast
to standard (non-tolerant) testing of monotonicity that can be done
nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.
</p>
<p>We obtain our lower bound by proving an analogous bound for erasure-resilient
testers. An $\alpha$-erasure-resilient tester for a desired property gets
oracle access to a function that has at most an $\alpha$ fraction of values
erased. The tester has to accept (with probability at least 2/3) if the
erasures can be filled in to ensure that the resulting function has the
property and to reject (with probability at least 2/3) if every completion of
erasures results in a function that is $\varepsilon$-far from having the
property. Our method yields the same lower bounds for unateness and being a
$k$-junta. These lower bounds improve exponentially on the existing lower
bounds for these properties.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06924"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06907">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06907">Strategy-Stealing is Non-Constructive</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodwin:Greg.html">Greg Bodwin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grossman:Ofer.html">Ofer Grossman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06907">PDF</a><br /><b>Abstract: </b>In many combinatorial games, one can prove that the first player wins under
best play using a simple but non-constructive argument called
strategy-stealing. This work is about the complexity behind these proofs: how
hard is it to actually find a winning move in a game, when you know by
strategy-stealing that one exists? We prove that this problem is PSPACE-hard
already for Minimum Poset Games and Symmetric Maker-Maker Games, which are
simple classes of games that capture two of the main types of strategy-stealing
arguments in the current literature.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06907"><span class="datestr">at November 20, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/">A Market for TCS Papers??</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>By David Eppstein &amp; Vijay Vazirani</em></p>
<p>No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – a matching market – for matching papers to conferences.</p>
<p>First, a short preamble on how the idea emerged.</p>
<p><strong>Preamble</strong> (by Vijay):  Soon after my recent <a href="https://www.youtube.com/watch?v=MLr6Ud5qmt4&amp;t=74s"><u>Simons talk</u></a> on Matching Markets, I sent its url to Al Roth. Obviously, I wasn’t expecting a return email. However, the perfect gentleman and ultimate scholar that Al is, he did reply, and mentioned that he did not like my “definition” of matching markets and said, “I guess I would say matching markets are markets because they aggregate information that is held by the participants, which is what markets do (even if they don’t use prices to do it..).” This hit me like lightening from the sky – suddenly it crystallized the innate intuition about markets which I had formed through work on algorithmic aspects of markets! I thanked Al profusely and added, “This definitely helps in me get the right perspective on the notion!”</p>
<p>About a week ago, while updating my talk for a seminar at Columbia University, I included this beautiful insight in it and then a thought occurred: Each PC meeting involves aggregation of information from a large number of agents: PC members as well as external experts. Hence, isn’t a conference a matching market? Excitedly, I sent this question to Al. He replied, “… the conference process, matching papers to conferences, is a market and a particular conference might be a marketplace … ”</p>
<p>When I returned home, my esteemed colleague, David Eppstein, stunned me by declaring that he had thought of a market relevant to our field in which no money changes hands. I immediately knew he was thinking of the conference process. But he got to it out of the blue … and not the long process it took me!</p>
<p><strong>Back to the idea:  </strong>In the past, matching markets have brought immense efficiency and order in allocation problems in which use of money is considered repugnant, the prime examples being matching medical residents to hospitals, kidney exchange, and assignment of students of a large city to its schools.</p>
<p>At present we are faced with massive inefficiencies in the conference process – numerous researchers are trapped in unending cycles of submit … get reject … incorporate comments … resubmit — often to the next deadline which has been conveniently arranged a couple of days down the road so the unwitting participants are conditioned into mindlessly keep coming back for more, much like Pavlov’s dog.</p>
<p>We are proposing a matching market approach to finally obliterate this madness. We believe such a market is feasible using the following ideas. No doubt our scheme will have some drawbacks; however, as should be obvious, the advantages far outweigh them.</p>
<p>First, for co-located symposia within a larger umbrella conference, such as the<br />
conferences within ALGO or FCRC, the following process should be a no-brainer:</p>
<p>1). Ensure a common deadline for all symposia; denote the latter by <em>S.</em></p>
<p>2). Let <em>R</em> denote the set of researchers who wish to submit one paper to a symposium in this umbrella conference – assume that researchers submitting more than one paper will have multiple names, one for each submission. Each researcher will provide a strict preference order over the subset of symposia to which they wish to submit their paper. Let <em>G</em> denote the bipartite graph with vertex sets (<em>R, S</em>) and an edge (<em>r, s</em>) only if researcher <em>r</em> chose symposium <em>s.</em></p>
<p>3). The umbrella conference will have a large common PC with experts representing all of its symposia. The process of assigning papers to PC members will of course use <em>G</em> in a critical way.</p>
<p>Once papers are reviewed by PC members and external reviewers, each symposium will rank its submissions using its own criteria of acceptance. We believe the overhead of ranking each paper multiple times is minimal since that is just an issue of deciding how “on-topic” a paper is – an easy task once the reviews of the paper are available.</p>
<p>4). Finally, using all these preference lists, a researcher-proposing stable matching is computed using the Gale-Shapley algorithm. As is well-known, this mechanism will be dominant strategy incentive compatible for researchers.</p>
<p>With a little extra effort, a similar scheme can also be used for a group of conferences at diverse locations but similar times, such as some of the annual summer theory conferences, STOC, ICALP, ESA, STAC, WADS/SWAT, etc.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/"><span class="datestr">at November 19, 2019 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07536">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07536">Time-inconsistent Planning: Simple Motivation Is Hard to Find</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Str=oslash=mme:Torstein_J=_F=.html">Torstein J. F. Strømme</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07536">PDF</a><br /><b>Abstract: </b>With the introduction of the graph-theoretic time-inconsistent planning model
due to Kleinberg and Oren, it has been possible to investigate the
computational complexity of how a task designer best can support a
present-biased agent in completing the task. In this paper, we study the
complexity of finding a choice reduction for the agent; that is, how to remove
edges and vertices from the task graph such that a present-biased agent will
remain motivated to reach his target even for a limited reward. While this
problem is NP-complete in general, this is not necessarily true for instances
which occur in practice, or for solutions which are of interest to task
designers. For instance, a task designer may desire to find the best task graph
which is not too complicated.
</p>
<p>We therefore investigate the problem of finding simple motivating subgraphs.
These are structures where the agent will modify his plan at most $k$ times
along the way. We quantify this simplicity in the time-inconsistency model as a
structural parameter: The number of branching vertices (vertices with
out-degree at least $2$) in a minimal motivating subgraph.
</p>
<p>Our results are as follows: We give a linear algorithm for finding an optimal
motivating path, i.e. when $k=0$. On the negative side, we show that finding a
simple motivating subgraph is NP-complete even if we allow only a single
branching vertex --- revealing that simple motivating subgraphs are indeed hard
to find. However, we give a pseudo-polynomial algorithm for the case when $k$
is fixed and edge weights are rationals, which might be a reasonable assumption
in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07536"><span class="datestr">at November 19, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07465">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07465">Implicit Enumeration of Topological-Minor-Embeddings and Its Application to Planar Subgraph Enumeration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakahata:Yu.html">Yu Nakahata</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawahara:Jun.html">Jun Kawahara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horiyama:Takashi.html">Takashi Horiyama</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Minato:Shin=ichi.html">Shin-ichi Minato</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07465">PDF</a><br /><b>Abstract: </b>Given graphs $G$ and $H$, we propose a method to implicitly enumerate
topological-minor-embeddings of $H$ in $G$ using decision diagrams. We show a
useful application of our method to enumerating subgraphs characterized by
forbidden topological minors, that is, planar, outerplanar, series-parallel,
and cactus subgraphs. Computational experiments show that our method can find
all planar subgraphs in a given graph at most five orders of magnitude faster
than a naive backtracking-based method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07465"><span class="datestr">at November 19, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07417">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07417">Algorithmic Discrepancy Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Whitmeyer:Michael.html">Michael Whitmeyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Jonathan.html">Jonathan Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07417">PDF</a><br /><b>Abstract: </b>This report will be a literature review on a result in algorithmic
discrepancy theory. We will begin by providing a quick overview on discrepancy
theory and some major results in the field, and then focus on an important
result by Shachar Lovett and Raghu Meka. We restate the main algorithm and
ideas of the paper, and rewrite proofs for some of the major results in the
paper.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07417"><span class="datestr">at November 19, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07378">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07378">Finding Skewed Subcubes Under a Distribution</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopalan:Parikshit.html">Parikshit Gopalan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levin:Roie.html">Roie Levin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wieder:Udi.html">Udi Wieder</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07378">PDF</a><br /><b>Abstract: </b>Say that we are given samples from a distribution $\psi$ over an
$n$-dimensional space. We expect or desire $\psi$ to behave like a product
distribution (or a $k$-wise independent distribution over its marginals for
small $k$). We propose the problem of enumerating/list-decoding all large
subcubes where the distribution $\psi$ deviates markedly from what we expect;
we refer to such subcubes as skewed subcubes. Skewed subcubes are certificates
of dependencies between small subsets of variables in $\psi$. We motivate this
problem by showing that it arises naturally in the context of algorithmic
fairness and anomaly detection.
</p>
<p>In this work we focus on the special but important case where the space is
the Boolean hypercube, and the expected marginals are uniform. We show that the
obvious definition of skewed subcubes can lead to intractable list sizes, and
propose a better definition of a minimal skewed subcube, which are subcubes
whose skew cannot be attributed to a larger subcube that contains it. Our main
technical contribution is a list-size bound for this definition and an
algorithm to efficiently find all such subcubes. Both the bound and the
algorithm rely on Fourier-analytic techniques, especially the powerful
hypercontractive inequality.
</p>
<p>On the lower bounds side, we show that finding skewed subcubes is as hard as
the sparse noisy parity problem, and hence our algorithms cannot be improved on
substantially without a breakthrough on this problem which is believed to be
intractable. Motivated by this, we study alternate models allowing query access
to $\psi$ where finding skewed subcubes might be easier.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07378"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07375">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07375">Top-down induction of decision trees: rigorous guarantees and inherent limitations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, Jane Lange, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07375">PDF</a><br /><b>Abstract: </b>Consider the following heuristic for building a decision tree for a function
$f : \{0,1\}^n \to \{\pm 1\}$. Place the most influential variable $x_i$ of $f$
at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the
left and right subtrees respectively; terminate once the tree is an
$\varepsilon$-approximation of $f$. We analyze the quality of this heuristic,
obtaining near-matching upper and lower bounds:
</p>
<p>$\circ$ Upper bound: For every $f$ with decision tree size $s$ and every
$\varepsilon \in (0,\frac1{2})$, this heuristic builds a decision tree of size
at most $s^{O(\log(s/\varepsilon)\log(1/\varepsilon))}$.
</p>
<p>$\circ$ Lower bound: For every $\varepsilon \in (0,\frac1{2})$ and $s \le
2^{\tilde{O}(\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that
this heuristic builds a decision tree of size $s^{\tilde{\Omega}(\log s)}$.
</p>
<p>We also obtain upper and lower bounds for monotone functions:
$s^{O(\sqrt{\log s}/\varepsilon)}$ and $s^{\tilde{\Omega}(\sqrt[4]{\log s } )}$
respectively. The lower bound disproves conjectures of Fiat and Pechyony (2004)
and Lee (2009).
</p>
<p>Our upper bounds yield new algorithms for properly learning decision trees
under the uniform distribution. We show that these algorithms---which are
motivated by widely employed and empirically successful top-down decision tree
learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees
that compare favorably with those of the current fastest algorithm (Ehrenfeucht
and Haussler, 1989). Our lower bounds shed new light on the limitations of
these heuristics.
</p>
<p>Finally, we revisit the classic work of Ehrenfeucht and Haussler. We extend
it to give the first uniform-distribution proper learning algorithm that
achieves polynomial sample and memory complexity, while matching its
state-of-the-art quasipolynomial runtime.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07375"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07357">Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canonne:Cl=eacute=ment_L=.html">Clément L. Canonne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Amit.html">Amit Levi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07357">PDF</a><br /><b>Abstract: </b>We give a nearly-optimal algorithm for testing uniformity of distributions
supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$
queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty
(2018)). The key technical component is a natural notion of random restriction
for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a
restriction affects the mean vector of the distribution. Along the way, we
consider the problem of mean testing with independent samples and provide a
nearly-optimal algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07357"><span class="datestr">at November 19, 2019 11:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07324">Testing Properties of Multiple Distributions with Few Samples</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aliakbarpour:Maryam.html">Maryam Aliakbarpour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07324">PDF</a><br /><b>Abstract: </b>We propose a new setting for testing properties of distributions while
receiving samples from several distributions, but few samples per distribution.
Given samples from $s$ distributions, $p_1, p_2, \ldots, p_s$, we design
testers for the following problems: (1) Uniformity Testing: Testing whether all
the $p_i$'s are uniform or $\epsilon$-far from being uniform in
$\ell_1$-distance (2) Identity Testing: Testing whether all the $p_i$'s are
equal to an explicitly given distribution $q$ or $\epsilon$-far from $q$ in
$\ell_1$-distance, and (3) Closeness Testing: Testing whether all the $p_i$'s
are equal to a distribution $q$ which we have sample access to, or
$\epsilon$-far from $q$ in $\ell_1$-distance. By assuming an additional natural
condition about the source distributions, we provide sample optimal testers for
all of these problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07324"><span class="datestr">at November 19, 2019 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07306">Quantum Speedup for Graph Sparsification, Cut Approximation and Laplacian Solving</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Ronald_de.html">Ronald de Wolf</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07306">PDF</a><br /><b>Abstract: </b>Graph sparsification underlies a large number of algorithms, ranging from
approximation algorithms for cut problems to solvers for linear systems in the
graph Laplacian. In its strongest form, "spectral sparsification" reduces the
number of edges to near-linear in the number of nodes, while approximately
preserving the cut and spectral structure of the graph. The breakthrough work
by Bencz\'ur and Karger (STOC'96) and Spielman and Teng (STOC'04) showed that
sparsification can be done optimally in time near-linear in the number of edges
of the original graph.
</p>
<p>In this work we show that quantum algorithms allow to speed up spectral
sparsification, and thereby many of the derived algorithms. Given
adjacency-list access to a weighted graph with $n$ nodes and $m$ edges, our
algorithm outputs an $\epsilon$-spectral sparsifier in time
$\widetilde{O}(\sqrt{mn}/\epsilon)$. We prove that this is tight up to
polylog-factors. The algorithm builds on a string of existing results, most
notably sparsification algorithms by Spielman and Srivastava (STOC'08) and
Koutis and Xu (TOPC'16), a spanner construction by Thorup and Zwick (STOC'01),
a single-source shortest-paths quantum algorithm by D\"urr et al. (ICALP'04)
and an efficient $k$-wise independent hash construction by Christiani, Pagh and
Thorup (STOC'15). Combining our sparsification algorithm with existing
classical algorithms yields the first quantum speedup, roughly from
$\widetilde{O}(m)$ to $\widetilde{O}(\sqrt{mn})$, for approximating the max
cut, min cut, min $st$-cut, sparsest cut and balanced separator of a graph.
Combining our algorithm with a classical Laplacian solver, we demonstrate a
similar speedup for Laplacian solving, for approximating effective resistances,
cover times and eigenvalues of the Laplacian, and for spectral clustering.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07306"><span class="datestr">at November 19, 2019 11:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07234">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07234">Approximation of Steiner Forest via the Bidirected Cut Relaxation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ali Çivril <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07234">PDF</a><br /><b>Abstract: </b>The classical algorithm of Agrawal, Klein and Ravi [SIAM J. Comput., 24
(1995), pp. 440-456], stated in the setting of the primal-dual schema by
Goemans and Williamson [SIAM J. Comput., 24 (1995), pp. 296-317] uses the
undirected cut relaxation for the Steiner forest problem. Its approximation
ratio is $2-\frac{1}{k}$, where $k$ is the number of terminal pairs. A variant
of this algorithm more recently proposed by K\"onemann et al. [SIAM J. Comput.,
37 (2008), pp. 1319-1341] is based on the lifted cut relaxation. In this paper,
we continue this line of work and consider the bidirected cut relaxation for
the Steiner forest problem, which lends itself to a novel algorithmic idea
yielding the same approximation ratio as the classical algorithm. In doing so,
we introduce an extension of the primal-dual schema in which we run two
different phases to satisfy connectivity requirements in both directions. This
reveals more about the combinatorial structure of the problem. In particular,
there are examples on which the classical algorithm fails to give a good
approximation, but the new algorithm finds a near-optimal solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07234"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07232">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07232">5/4-Approximation of Minimum 2-Edge-Connected Spanning Subgraph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ali Çivril <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07232">PDF</a><br /><b>Abstract: </b>We provide a $5/4$-approximation algorithm for the 2-edge connected spanning
subgraph problem. This improves upon the previous best ratio of $4/3$. The
algorithm is based on applying local improvement steps on a starting solution
provided by a standard ear decomposition. Unlike previous approaches, we
consider modifications involving $5$-ears together with $3$-ears.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07232"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07154">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07154">Sparse Hopsets in Congested Clique</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nazari:Yasamin.html">Yasamin Nazari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07154">PDF</a><br /><b>Abstract: </b>We give the first Congested Clique algorithm that computes a sparse hopset
with polylogarithmic hopbound in polylogarithmic time. Given a graph $G=(V,E)$,
a $(\beta,\epsilon)$-hopset $H$ with "hopbound" $\beta$, is a set of edges
added to $G$ such that for any pair of nodes $u$ and $v$ in $G$ there is a path
with at most $\beta$ hops in $G \cup H$ with length within $(1+\epsilon)$ of
the shortest path between $u$ and $v$ in $G$.
</p>
<p>Our hopsets are significantly sparser than the recent construction of
Censor-Hillel et al. [6], that constructs a hopset of size
$\tilde{O}(n^{3/2})$, but with a smaller polylogarithmic hopbound. On the other
hand, the previously known constructions of sparse hopsets with polylogarithmic
hopbound in the Congested Clique model, proposed by Elkin and Neiman
[10],[11],[12], all require polynomial rounds.
</p>
<p>One tool that we use is an efficient algorithm that constructs an
$\ell$-limited neighborhood cover, that may be of independent interest.
</p>
<p>Finally, as a side result, we also give a hopset construction in a variant of
the low-memory Massively Parallel Computation model, with improved running time
over existing algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07154"><span class="datestr">at November 19, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07151">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07151">A one-phase tree-based algorithm for mining high-utility itemsets from a transaction database</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dawar:Siddharth.html">Siddharth Dawar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vikram.html">Vikram Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Debajyoti.html">Debajyoti Bera</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07151">PDF</a><br /><b>Abstract: </b>High-utility itemset mining finds itemsets from a transaction database with
utility no less than a fixed user-defined threshold. The utility of an itemset
is defined as the sum of the utilities of its item. Several algorithms were
proposed to mine high-utility itemsets. However, no state-of-the-art algorithm
performs consistently good across dense and sparse datasets. In this paper, we
propose a novel data structure called Utility-Tree, and a tree-based algorithm
called UT-Miner that mines high-utility itemsets in one-phase only without
generating any candidates and uses a lightweight construction method to reduce
the cost of creating projected databases during the search space exploration.
The transaction information is stored compactly with every node of the
Utility-Tree, and the information is computed efficiently during the recursive
invocation of the algorithm. Experimental results on several real-life dense
and sparse datasets reveal that UT-Miner is among the top-performing efficient
algorithms across different datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07151"><span class="datestr">at November 19, 2019 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07124">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07124">Faster Integer Multiplication Using Preprocessing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Groff:Matt.html">Matt Groff</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07124">PDF</a><br /><b>Abstract: </b>A New Number Theoretic Transform(NTT), which is a form of FFT, is introduced,
that is faster than FFTs. Also, a multiplication algorithm is introduced that
uses this to perform integer multiplication faster than O(n log n). It uses
preprocessing to achieve an upper bounds of (n log n/(log log n/ log log log
n).
</p>
<p>Also, we explore the possibility of O(n) time multiplication via NTTs that
require only O(n) operations, using preprocessing.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07124"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.07020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.07020">Counting solutions to random CNF formulas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galanis:Andreas.html">Andreas Galanis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldberg:Leslie_Ann.html">Leslie Ann Goldberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Heng.html">Heng Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Kuan.html">Kuan Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07020">PDF</a><br /><b>Abstract: </b>We give the first efficient algorithm to approximately count the number of
solutions in the random $k$-SAT model when the density of the formula scales
exponentially with $k$. The best previous counting algorithm was due to
Montanari and Shah and was based on the correlation decay method, which works
up to densities $(1+o_k(1))\frac{2\log k}{k}$, the Gibbs uniqueness threshold
for the model. Instead, our algorithm harnesses a recent technique by Moitra to
work for random formulas. The main challenge in our setting is to account for
the presence of high-degree variables whose marginal distributions are hard to
control and which cause significant correlations within the formula.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.07020"><span class="datestr">at November 19, 2019 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06985">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06985">Constructing the Bijective BWT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=auml=rkk=auml=inen:Juha.html">Juha Kärkkäinen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, Marcin Picatkowski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06985">PDF</a><br /><b>Abstract: </b>The Burrows-Wheeler transform (BWT) is a permutation whose applications are
prevalent in data compression and text indexing. The bijective BWT (BBWT) is a
bijective variant of it. Although it is known that the BWT can be constructed
in linear time for integer alphabets by using a linear time suffix array
construction algorithm, it was up to now only conjectured that the BBWT can
also be constructed in linear time. We confirm this conjecture by proposing a
construction algorithm that is based on SAIS, improving the best known result
of $O(n \lg n /\lg \lg n)$ time to linear.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06985"><span class="datestr">at November 19, 2019 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06951">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06951">Memory-Efficient Performance Monitoring on Programmable Switches with Lean Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Zaoxing.html">Zaoxing Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rottenstreich:Ori.html">Ori Rottenstreich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Vladimir.html">Vladimir Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rexford:Jennifer.html">Jennifer Rexford</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06951">PDF</a><br /><b>Abstract: </b>Network performance problems are notoriously difficult to diagnose. Prior
profiling systems collect performance statistics by keeping information about
each network flow, but maintaining per-flow state is not scalable on
resource-constrained NIC and switch hardware. Instead, we propose sketch-based
performance monitoring using memory that is sublinear in the number of flows.
Existing sketches estimate flow monitoring metrics based on flow sizes. In
contrast, performance monitoring typically requires combining information
across pairs of packets, such as matching a data packet with its acknowledgment
to compute a round-trip time. We define a new class of \emph{lean} algorithms
that use memory sublinear in both the size of input data and the number of
flows. We then introduce lean algorithms for a set of important statistics,
such as identifying flows with high latency, loss, out-of-order, or
retransmitted packets. We implement prototypes of our lean algorithms on a
commodity programmable switch using the P4 language. Our experiments show that
lean algorithms detect $\sim$82\% of top 100 problematic flows among real-world
packet traces using just 40KB memory.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06951"><span class="datestr">at November 19, 2019 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06895">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06895">Delta-stepping SSSP: from Vertices and Edges to GraphBLAS Implementations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sridhar:Upasana.html">Upasana Sridhar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanco:Mark.html">Mark Blanco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mayuranath:Rahul.html">Rahul Mayuranath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spampinato:Daniele_G=.html">Daniele G. Spampinato</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Low:Tze_Meng.html">Tze Meng Low</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McMillan:Scott.html">Scott McMillan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06895">PDF</a><br /><b>Abstract: </b>GraphBLAS is an interface for implementing graph algorithms. Algorithms
implemented using the GraphBLAS interface are cast in terms of linear
algebra-like operations. However, many graph algorithms are canonically
described in terms of operations on vertices and/or edges. Despite the known
duality between these two representations, the differences in the way
algorithms are described using the two approaches can pose considerable
difficulties in the adoption of the GraphBLAS as standard interface for
development. This paper investigates a systematic approach for translating a
graph algorithm described in the canonical vertex and edge representation into
an implementation that leverages the GraphBLAS interface. We present a two-step
approach to this problem. First, we express common vertex- and edge-centric
design patterns using a linear algebraic language. Second, we map this
intermediate representation to the GraphBLAS interface. We illustrate our
approach by translating the delta-stepping single source shortest path
algorithm from its canonical description to a GraphBLAS implementation, and
highlight lessons learned when implementing using GraphBLAS.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06895"><span class="datestr">at November 19, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06889">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06889">New Query Lower Bounds for Submodular Function MInimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Andrei Graur, Tristan Pollner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramaswamy:Vidhya.html">Vidhya Ramaswamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinberg:S=_Matthew.html">S. Matthew Weinberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06889">PDF</a><br /><b>Abstract: </b>We consider submodular function minimization in the oracle model: given
black-box access to a submodular set function $f:2^{[n]}\rightarrow
\mathbb{R}$, find an element of $\arg\min_S \{f(S)\}$ using as few queries to
$f(\cdot)$ as possible. State-of-the-art algorithms succeed with
$\tilde{O}(n^2)$ queries [LeeSW15], yet the best-known lower bound has never
been improved beyond $n$ [Harvey08].
</p>
<p>We provide a query lower bound of $2n$ for submodular function minimization,
a $3n/2-2$ query lower bound for the non-trivial minimizer of a symmetric
submodular function, and a $\binom{n}{2}$ query lower bound for the non-trivial
minimizer of an asymmetric submodular function.
</p>
<p>Our $3n/2-2$ lower bound results from a connection between SFM lower bounds
and a novel concept we term the cut dimension of a graph. Interestingly, this
yields a $3n/2-2$ cut-query lower bound for finding the global mincut in an
undirected, weighted graph, but we also prove it cannot yield a lower bound
better than $n+1$ for $s$-$t$ mincut, even in a directed, weighted graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06889"><span class="datestr">at November 19, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/18/faculty-at-boston-university-apply-by-december-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/18/faculty-at-boston-university-apply-by-december-1-2019/">Faculty at Boston University (apply by December 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Boston University Computer Science has openings for multiple tenure-track assistant professorships beginning July 1, 2020. Applications in all areas of computer science are invited. The university is also considering senior applicants for its Data Science Initiative.</p>
<p>The Department, currently at 31 faculty members, is in the midst of an extended period of sustained growth.</p>
<p>Website: <a href="https://www.bu.edu/cs/2019/11/18/bu-cs-searching-for-new-faculty-members/">https://www.bu.edu/cs/2019/11/18/bu-cs-searching-for-new-faculty-members/</a><br />
Email: ads22@bu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/18/faculty-at-boston-university-apply-by-december-1-2019/"><span class="datestr">at November 18, 2019 07:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/18/faculty-quantum-computation-at-uc-san-diego-apply-by-january-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/18/faculty-quantum-computation-at-uc-san-diego-apply-by-january-1-2020/">faculty – quantum computation at UC San Diego (apply by January 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science and the Mathematics departments at UC San Diego are looking for excellent candidates in Quantum Computation. This includes: computational complexity of quantum computation, quantum algorithms, ways to establish quantum supremacy, applications of quantum computation in the sciences, quantum error-correction, quantum communication complexity, or similar topics.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF02292">https://apol-recruit.ucsd.edu/JPF02292</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/18/faculty-quantum-computation-at-uc-san-diego-apply-by-january-1-2020/"><span class="datestr">at November 18, 2019 05:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3436178446517561376">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html">Fields used to be closer together than they are now. Good? Bad?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
There was a retired software Eng professor that I had heard two very non-controversial rumors about:<br />
<br />
1) He got his PhD in Numerical Analysis<br />
<br />
2) He got his PhD in Compiler Optimization.<br />
<br />
So I asked him which was true.<br />
<br />
The answer: Both! In those days you had to optimize your code to get your NA code to run fast enough.<br />
<br />
We cannot imagine that anymore. Or at least I cannot.<br />
<br />
Over time the fields of computer science advance more so its hard to be  master of more than one field.  But its not that simple: there has been work recently applying Machine Learning to... well<br />
everything really. Even so, I think the trend is more towards separation. Or perhaps it oscillates.<br />
<br />
I am NOT going to be the grumpy old man (Google once thought I was 70, see <a href="https://blog.computationalcomplexity.org/2018/10/google-added-years-to-my-life.html">here</a>) who says things were better in my day when the fields were closer together. But I will ask the question:<br />
<br />
1) Are people more specialized new? While I think yes since each field has gotten more complicated and harder to master. There are exceptions: Complexity theory uses much more sophisticated mathematics then when I was a grad student (1980-1985), and of course Quantum Computing has lead to more comp sci majors knowing physics.<br />
<br />
2) Is it good for the field that people are specialized? I am supposed to say that it is terrible and that great advances are made when people are interdiscplinary. But there are many more small advances that are made by someone who has a mastery of one (or two) fields.<br />
<br />
3) The PhD Process and the Tenure Process encourage specialization. This I think IS bad since there are different modes of research that should all be respected.'<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html"><span class="datestr">at November 18, 2019 04:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/165">TR19-165 |  Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning | 

	Clement Canonne, 

	Xi Chen, 

	Gautam Kamath, 

	Amit Levi, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/165"><span class="datestr">at November 18, 2019 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4414">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4414">The Aaronson-Ambainis Conjecture (2008-2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that Lance Fortnow suggested in one of his papers: is it possible to separate <a href="https://en.wikipedia.org/wiki/P_(complexity)">P</a> from <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> relative to a <a href="https://en.wikipedia.org/wiki/Random_oracle">random oracle</a>?  (That is, without first needing to separate P from PSPACE or whatever in the real world?)  Or to the contrary: suppose that a quantum algorithm Q makes T queries to a Boolean input string X.  Is there then a classical simulation algorithm that makes poly(T) queries to X, and that approximates Q’s acceptance probability for <em>most</em> values of X?  Such a classical simulation, were it possible, would still be consistent with the existence of quantum algorithms like <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon’s</a> and <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s</a>, which are able to achieve exponential (and even greater) speedups in the black-box setting.  It would simply demonstrate the importance, for Simon’s and Shor’s algorithms, of global structure that makes the string X extremely <em>non</em>-random: for example, encoding a periodic function (in the case of Shor’s algorithm), or encoding a function that hides a secret string s (in the case of Simon’s).  It would underscore that superpolynomial quantum speedups depend on structure.</p>



<p>I never managed to solve this problem.  Around 2008, though, I noticed that a solution would follow from a perhaps-not-obviously-related conjecture, about <em>influences</em> in low-degree polynomials.  Namely, let p:R<sup>n</sup>→R be a degree-d real polynomial in n variables, and suppose p(x)∈[0,1] for all x∈{0,1}<sup>n</sup>.  Define the <i>variance</i> of p to be<br />   Var(p):=E<sub>x,y</sub>[|p(x)-p(y)|],<br />and define the <i>influence</i> of the i<sup>th</sup> variable to be<br />   Inf<sub>i</sub>(p):=E<sub>x</sub>[|p(x)-p(x<sup>i</sup>)|].<br />Here the expectations are over strings in {0,1}<sup>n</sup>, and x<sup>i</sup> means x with its i<sup>th</sup> bit flipped between 0 and 1.  Then the conjecture is this: there must be some variable i such that Inf<sub>i</sub>(p) ≥ poly(Var(p)/d) (in other words, that “explains” a non-negligible fraction of the variance of the entire polynomial).</p>



<p>Why would this conjecture imply the statement about quantum algorithms?  Basically, because of the seminal result of <a href="https://arxiv.org/abs/quant-ph/9802049">Beals et al.</a> from 1998: that if a quantum algorithm makes T queries to a Boolean input X, then its acceptance probability can be written as a real polynomial over the bits of X, of degree at most 2T.  Given that result, if you wanted to classically simulate a quantum algorithm Q on most inputs—and if you only cared about query complexity, not computation time—you’d simply need to do the following:<br />(1) Find the polynomial p that represents Q’s acceptance probability.<br />(2) Find a variable i that explains at least a 1/poly(T) fraction of the total remaining variance in p, and query that i.<br />(3) Keep repeating step (2), until p has been restricted to a polynomial with not much variance left—i.e., to nearly a constant function p(X)=c.  Whenever that happens, halt and output the constant c.<br />The key is that by hypothesis, this algorithm will halt, with high probability over X, after only poly(T) steps.</p>



<p>Anyway, around the same time, Andris Ambainis had a major break on a different problem that I’d told him about: namely, whether randomized and quantum query complexities are polynomially related for all partial functions with permutation symmetry (like the collision and the element distinctness functions).  Andris and I decided to write up the two directions jointly.  The result was our 2011 paper entitled <a href="https://arxiv.org/abs/0911.0996">The Need for Structure in Quantum Speedups</a>.</p>



<p>Of the two contributions in the “Need for Structure” paper, the one about random oracles and influences in low-degree polynomials was clearly the weaker and less satisfying one.  As the reviewers pointed out, that part of the paper didn’t solve anything: it just reduced one unsolved problem to a new, slightly different problem that was <em>also</em> unsolved.  Nevertheless, that part of the paper acquired a life of its own over the ensuing decade, as the world’s experts in analysis of Boolean functions and polynomials began referring to the “Aaronson-Ambainis Conjecture.”  Ryan O’Donnell, Guy Kindler, and many others had a stab.  I even got Terry Tao to spend an hour or two on the problem when I visited UCLA.</p>



<p>Now, at long last, Nathan Keller and Ohad Klein say they’ve proven the Aaronson-Ambainis Conjecture, in a preprint whose title is a riff on ours: <a href="https://arxiv.org/abs/1911.03748">“Quantum Speedups Need Structure.”</a></p>



<p>Their paper hasn’t yet been peer-reviewed, and I haven’t yet carefully studied it, but I <em>could</em> and <em>should</em>: at 19 pages, it looks very approachable and clear, if not as radically short as (say) <a href="https://www.scottaaronson.com/blog/?p=4229">Huang’s proof of the Sensitivity Conjecture</a>.  Keller and Klein’s argument subsumes all the earlier results that I knew would need to be subsumed, and involves all the concepts (like a real analogue of block sensitivity) that I knew would need to be involved somehow.</p>



<p>My plan had been as follows:<br />(1) Read their paper in detail.  Understand every step of their proof.<br />(2) Write a blog post that reflects my detailed understanding.</p>



<p>Unfortunately, this plan did not sufficiently grapple with the fact that I now have two kids.  It got snagged for a week at step (1).  So I’m now executing an alternative plan, which is to jump immediately to the blog post.</p>



<p>Assuming Keller and Klein’s result holds up—as I expect it will—by combining it with the observations in my and Andris’s paper, one immediately gets an explanation for why no one has managed to separate P from BQP relative to a <em>random</em> oracle, but only relative to non-random oracles.  This complements the work of <a href="https://www.uncg.edu/mat/faculty/cdsmyth/thesis.pdf">Kahn, Saks, and Smyth</a>, who around 2000 gave a precisely analogous explanation for the difficulty of separating P from NP∩coNP relative to a random oracle.</p>



<p>Unfortunately, the polynomial blowup is quite enormous: from a quantum algorithm making T queries, Keller and Klein apparently get a classical algorithm making O(T<sup>18</sup>) queries.  But such things can almost always be massively improved.</p>



<p>Feel free to use the comments to ask any questions about this result or its broader context.  I’ll either do my best to answer from the limited amount I know, or else I’ll pass the questions along to Nathan and Ohad themselves.  Maybe, at some point, I’ll even be forced to understand the new proof.</p>



<p>Congratulations to Nathan and Ohad!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4414"><span class="datestr">at November 17, 2019 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/164">TR19-164 |  Improved bounds for perfect sampling of $k$-colorings in graphs | 

	Siddharth Bhandari, 

	Sayantan Chakraborty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present a randomized algorithm that takes as input an undirected $n$-vertex graph $G$ with maximum degree $\Delta$ and an integer $k &gt; 3\Delta$, and returns a random proper $k$-coloring of $G$. The 
 distribution of the coloring is perfectly uniform over the set of all proper $k$-colorings; the expected running time of the algorithm is $\mathrm{poly}(k,n)=\widetilde{O}(n\Delta^2\cdot \log(k))$. 
 This improves upon a result of Huber~(STOC 1998) who obtained polynomial time perfect sampling algorithm for $k&gt;\Delta^2+2\Delta$.
 Prior to our work, no algorithm with expected running time $\mathrm{poly}(k,n)$ was known to guarantee perfectly sampling for $\Delta = \omega(1)$ and for any $k \leq \Delta^2+2\Delta$. 
 
 Our algorithm (like several other perfect sampling algorithms including Huber's) is based on  the Coupling from the Past method. Inspired by the bounding chain approach pioneered independently by H\"aggstr\"om \&amp; Nelander~(Scand.{} J.{} Statist., 1999) and Huber~(STOC 1998), our algorithm is based on a novel bounding chain for the coloring problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/164"><span class="datestr">at November 17, 2019 10:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/">Postdoc at Princeton University (apply by January 10, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Princeton University is seeking exceptional recent Ph.D. recipients for research positions in theoretical computer science and theoretical machine learning.</p>
<p>Note that there are two positions, <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281</a> and at the link below.</p>
<p>Website: <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221</a><br />
Email: smweinberg@princeton.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/"><span class="datestr">at November 16, 2019 05:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/163">TR19-163 |  Approximating the Distance to Monotonicity of Boolean Functions | 

	Ramesh Krishnan S. Pallavoor, 

	Sofya Raskhodnikova, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We design a nonadaptive algorithm that, given a Boolean function $f\colon \{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n, 1/\alpha)$ queries and returns an estimate that, with high probability, is an $\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity. Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires $2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n, 1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a question of Seshadhri (Property Testing Review, 2014) for the case of nonadaptive algorithms. Approximating the distance to a property is closely related to tolerantly testing that property. Our lower bound stands in contrast to standard (non-tolerant) testing of monotonicity that can be done nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.

We obtain our lower bound by proving an analogous bound for erasure-resilient testers. An $\alpha$-erasure-resilient tester for a desired property gets oracle access to a function that has at most an $\alpha$ fraction of values erased. The tester has to accept (with probability at least 2/3) if the erasures can be filled in to ensure that the resulting function has the property and to reject (with probability at least 2/3) if every completion of erasures results in a function that is $\varepsilon$-far from having the property. Our method yields the same lower bounds for unateness and being a $k$-junta. These lower bounds improve exponentially on the existing lower bounds for these properties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/163"><span class="datestr">at November 16, 2019 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://scholarlyoa.com/omics-group-now-charging-for-article-withdrawals/">OMICS Group now charging for article withdrawals</a> (<a href="https://mathstodon.xyz/@11011110/103069657170279386"></a>): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</p>
  </li>
  <li>
    <p><a href="https://www.shapeoperator.com/2016/12/12/sunset-geometry/">Sunset geometry</a> (<a href="https://mathstodon.xyz/@11011110/103075417779881667"></a>, <a href="https://news.ycombinator.com/item?id=21413358">via</a>). How to tell the radius of the earth from a photo of a sunset over a still body of water (knowing the height of the camera over the water). Not explained: how still the water needs to be, how badly the results are affected by atmospheric refraction, how accurately the measurements need to be performed to get a meaningful estimate, or how likely you are to see the sun meet the horizon before low clouds get in the way.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/profession/ams-fellows/new-fellows">The new AMS fellows</a> (<a href="https://mathstodon.xyz/@11011110/103077029444407287"></a>) include graph theorists Daniel Kráľ and Bojan Mohar, and fellow Wikipedia editor Marie Vitulli. Their announcement also led me to create new Wikipedia articles on new fellows <a href="https://en.wikipedia.org/wiki/Chikako_Mese">Chikako Mese</a>, <a href="https://en.wikipedia.org/wiki/Julianna_Tymoczko">Julianna Tymoczko</a>, and <a href="https://en.wikipedia.org/wiki/Jang-Mei_Wu">Jang-Mei Wu</a>, and Vitulli to create one for <a href="https://en.wikipedia.org/wiki/Tara_E._Brendle">Tara Brendle</a>. Congratulations, all!</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/maa-awards/writing-awards/the-graph-menagerie-abstract-algebra-and-the-mad-veterinarian">The graph menagerie: abstract algebra and the mad veterinarian </a> (<a href="https://mathstodon.xyz/@11011110/103083916322783935"></a>). Or, how to solve puzzles like: “Suppose a mad veterinarian creates a transmogrifier that can convert one cat into two dogs and five mice, or one dog into three cats and three mice, or a mouse into a cat and a dog. It can also do each of these operations in reverse. Can it, through any sequence of operations, convert two cats into a pack of dogs? How about one cat?”</p>
  </li>
  <li>
    <p><a href="http://processalgebra.blogspot.com/2019/11/call-for-opinions-length-of-papes-in.html">LIPIcs series editor Luca Aceto polls the community on page limits</a> (<a href="https://mathstodon.xyz/@11011110/103086511516971922"></a>). It used to be that conferences in theoretical computer science had page limits because you couldn’t bind volumes with too many paper pages, now long irrelevant. So now that we <em>can</em> publish much longer conference papers, should we? Limits encourage authors to publish full details in a properly refereed journal version, but unlimited length recognizes the reality that many authors are too lazy to make journal versions.</p>
  </li>
  <li>
    <p><a href="https://understandinguncertainty.org/squaring-square-glass">Squaring the square, in stained glass</a> (<a href="https://mathstodon.xyz/@11011110/103095223291132423"></a>, <a href="https://scilogs.spektrum.de/hlf/perfect-squares/">via</a>). By David Spiegelhalter, 2013.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Edge_tessellation">Tiling the plane by edge reflection</a> (<a href="https://mathstodon.xyz/@11011110/103099409883184951"></a>). Here’s a proof sketch that there are eight ways to do this:
Each prototile vertex must have angle  for integer , and if  is odd, the subsequence of the remaining angles must be symmetric. For an -gon, considering the sum of interior angles shows that</p>

    

    <p>Searching for sequences of integers with these properties (choosing the smallest integers first to make the search bounded) finds that the only cyclic sequences of integers meeting these constraints are (3,12,12), (4,8,8), (4,6,12), (6,6,6), (3,4,6,4), (3,6,3,6), (4,4,4,4), and (3,3,3,3,3,3), the sequences of the 8 known tessellations.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/11/08/turkish-academics-sound-alarm-over-gender-segregation-plans">Turkish academics sound alarm over gender segregation plans</a> (<a href="https://mathstodon.xyz/@11011110/103103753533385831"></a>). When women’s universities are set up to provide alternatives in the face of persistent discrimination against women in the existing universities (as they were in the US and Korea), that’s one thing. When the women are already successful in the existing universities but women’s universities are set up as a pathway to push them out, that’s entirely different.</p>
  </li>
  <li>
    <p><a href="http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010">Engineers put Leonardo da Vinci’s bridge design to the test:
proposed bridge would have been the world’s longest at the time; new analysis shows it would have worked</a> (<a href="https://mathstodon.xyz/@11011110/103111919197001339"></a>, <a href="https://arstechnica.com/science/2019/10/testing-leonardo-da-vincis-bridge-his-design-was-stable-study-finds/">via</a>). I don’t think the link does justice to the scale of the thing. Da Vinci proposed a single stone arch across the Golden Horn in Istanbul, roughly 280m. That’s much longer than anything on the current <a href="https://en.wikipedia.org/wiki/List_of_longest_masonry_arch_bridge_spans">list of the world’s biggest stone arches</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Order_polytope">Order polytope</a> (<a href="https://mathstodon.xyz/@11011110/103115656245496084"></a>). New Wikipedia article on a convex polytope derived from any finite partial order as the points in a unit hypercube whose coordinate order is consistent with the partial order. Its vertices come from upper sets, its faces come from quotients, its facets come from covering pairs, and its volume comes from the number of linear extensions of the partial order. Coordinatewise min and max gives its points the structure of a continuous distributive lattice.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/socked-into-the-puppet-hole-on-wikipedia/">Socked into the puppet-hole on Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/103120959491226706"></a>). Journalist Noam Cohen’s Wikipedia biography is collateral damage in the war on slowking4, a prolific creator of Wikipedia articles whose problematic behavior (copying content from other sites, creating sockpuppet accounts to hide their identity, and reinstating articles from another user that were so riddled with errors that they were deleted en masse) has led to delete-on-sight actions.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Arithmetic_billiards">Arithmetic billiards</a> (<a href="https://mathstodon.xyz/@11011110/103129171790722530"></a>): using billiard ball trajectories to compute number-theoretic functions.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@olligobber/103066273568117018">olligober made a regex to match all multiples of 7</a>, but it was more than 10,000 characters long so grep couldn’t handle it. Applying <a href="https://en.wikipedia.org/wiki/Kleene%27s_algorithm">Kleene’s algorithm</a> to convert the natural DFA for this sort of problem into a regular expression blows its size up from polynomial in the modulus to exponential, but is this necessary? And if it is, what is the best possible base for the exponential?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/questions/338888/dividing-a-chocolate-bar-into-any-proportions">Dividing a chocolate bar into any proportions</a> (<a href="https://mathstodon.xyz/@11011110/103140599116798730"></a>). The bar has  squares, and you want to give each of  people an integer number of squares, but the integers are not known in advance. How to break the bar into few pieces so this will always be possible? Reid Hardison asked this months ago but Ilya Bogdanov answered with an efficient construction of the optimal partition much more recently.</p>
  </li>
  <li>
    <p><a href="http://www.personal.psu.edu/sot2/books/billiardsgeometry.pdf">Geometry and Billiards</a> (<a href="https://mathstodon.xyz/@11011110"></a>). An undergraduate-level textbook on the mathematics of reflection by Serge Tabachnikov.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/15/linkage.html"><span class="datestr">at November 15, 2019 10:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/162">TR19-162 |  The Random-Query Model and the Memory-Bounded Coupon Collector | 

	Ran Raz, 

	Wei Zhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study a new model of space-bounded computation, the {\it random-query} model. The model is based on a branching-program over input variables $x_1,\ldots,x_n$. In each time step, the branching program gets as an input a random index $i \in \{1,\ldots,n\}$, together with the input variable $x_i$ (rather than querying an input variable of its choice, as in the case of a standard (oblivious) branching program). We motivate the new model in various ways and study time-space tradeoff lower bounds in this model.

Our main technical result is a quadratic time-space lower bound for zero-error computations in the random-query model, for XOR, Majority and many other functions. More precisely, a zero-error computation is a computation that stops with high probability and such that conditioning on the event that the computation stopped, the output is correct with probability~1. We prove that for any Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$, with sensitivity $k$, any zero-error computation with time $T$ and space $S$, satisfies 
$T\cdot (S+\log n) \geq \Omega(n \cdot k)$. We note that the best time-space lower bounds for standard oblivious branching programs are only slightly super linear and improving these bounds is an important long-standing open problem.

To prove our results, we study a memory-bounded variant of the coupon-collector problem that seems to us of independent interest and to the best of our knowledge has not been studied before. We consider a zero-error version of the coupon-collector problem. In this problem, the coupon-collector could explicitly choose to stop when he/she is sure with zero-error that
all coupons have already been collected. We prove that any zero-error coupon-collector that stops with high probability in time $T$, and uses space $S$, satisfies $T\cdot (S+\log n) \geq \Omega(n^2)$, where $n$ is the number of different coupons.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/162"><span class="datestr">at November 15, 2019 06:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7574">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">Puzzles of modern machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-jetpack-markdown"><p>It is often said that "we don’t understand deep learning" but it is not as often clarified what is it exactly that we don’t understand. In this post I try to list some of the "puzzles" of modern machine learning, from a theoretical perspective. This list is neither comprehensive nor authoritative.  Indeed, I  only started looking at these issues last year, and am very much in the position of not yet fully understanding the questions, let alone potential answers.
On the other hand, at the rate ML research is going, a calendar year corresponds to about 10 "ML years"…</p>
<p>Machine learning  offers many opportunities for theorists; there are many more questions than answers, and it is clear that a better theoretical understanding of what makes certain training procedures work or fail is desperately needed. Moreover, recent advances in software frameworks made it much easier to test out intuitions and conjectures. While in the past running training procedures might have required a Ph.D in machine learning, recently the "barrier to entry" was reduced to first to undergraduates, then to high school students, and these days it’s so easy that even theoretical computer scientists can do it <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<p>To set the context for this discussion, I focus on the task of supervised learning. In this setting we are given a <em>training set</em> <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> examples of the form <img src="https://s0.wp.com/latex.php?latex=%28x_i%2Cy_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x_i,y_i)" class="latex" title="(x_i,y_i)" /> where <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i \in \mathbb{R}^d" class="latex" title="x_i \in \mathbb{R}^d" /> is some vector (think of it as the pixels of an image) and <img src="https://s0.wp.com/latex.php?latex=y_i+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i \in { \pm 1 }" class="latex" title="y_i \in { \pm 1 }" /> is some label (think of <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i" class="latex" title="y_i" /> as equaling <img src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+1" class="latex" title="+1" /> if <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> is the image of a dog and <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-1" class="latex" title="-1" /> if <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> is the image of a cat). The goal in supervised learning is to find a <em>classifier</em> <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> such that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x)=y" class="latex" title="f(x)=y" /> will hold for many future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)" class="latex" title="(x,y)" />.</p>
<p>The standard approach is to consider some parameterized family of classifiers, where for every vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" />  of parameters, we associate a classifier <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta+%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" class="latex" title="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" />. For example, we can fix a certain neural network architecture (depth, connections, activation functions, etc.) and let <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" />  be the vector of weights that characterizes every network in this architecture. People then run some optimizing algorithm such as stochastic gradient descent with the objective function set as finding the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" /> that minimizes a <em>loss function</em> <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" />. This loss function can be the fraction of labels that <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> gets wrong on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> or a more continuous loss that takes into account the confidence level or other parameters of <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> as well. By now this general approach has been successfully applied to a many classification tasks, in many cases achieving near-human to super-human performance.  In the rest of this post I want to discuss some of the questions that arise when trying to obtain a theoretical understanding of both the powers and the limitations of the above approach. I focus on deep learning, though there are still some open questions even for over-parameterized linear regression.</p>
<h2>The generalization puzzle</h2>
<p>The approach outlined above has been well known and  analyzed for many decades in the statistical learning literature. There are many cases where we can <em>prove</em> that a classifier obtained in this case has a small <em>generalization gap</em>, in the sense that if the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> was obtained by sampling <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> independent and identical samples from a distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, then the performance of a classifier <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> on new samples from <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> will be close to its performance on the training set.</p>
<p>Ultimately, these results all boil down to the Chernoff bound. Think of the random variables <img src="https://s0.wp.com/latex.php?latex=X_1%2C%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1,\ldots,X_n" class="latex" title="X_1,\ldots,X_n" /> where <img src="https://s0.wp.com/latex.php?latex=X_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_i=1" class="latex" title="X_i=1" /> if the classifier makes an error on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th training example. The Chernoff bound tells us that  probability that  that <img src="https://s0.wp.com/latex.php?latex=%5Csum+X_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum X_i" class="latex" title="\sum X_i" /> deviates by more than <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon n" class="latex" title="\epsilon n" /> from its expectation is something like <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cepsilon%5E2+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(-\epsilon^2 n)" class="latex" title="\exp(-\epsilon^2 n)" /> and so as long as the total number of classifiers is less than <img src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k" class="latex" title="2^k" /> for <img src="https://s0.wp.com/latex.php?latex=k+%3C+%5Cepsilon%5E2+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k &lt; \epsilon^2 n" class="latex" title="k &lt; \epsilon^2 n" />, we can use a union bound over all possible classifiers to argue that if we make a <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> fraction of errors on the training set, the probability we make an error on a new example is at  most <img src="https://s0.wp.com/latex.php?latex=p%2B%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p+\epsilon" class="latex" title="p+\epsilon" />. We can of course "bunch together" classifiers that behave similarly on our distribution, and so it is enough if there are at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{\epsilon^2 n}" class="latex" title="2^{\epsilon^2 n}" /> of these equivalence classes. Another approach is to add a "regularizing term" <img src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R(\theta)" class="latex" title="R(\theta)" /> to the objective function, which amounts to restricting attention to the set of all classifiers <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> such that <img src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29+%5Cleq+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R(\theta) \leq \mu" class="latex" title="R(\theta) \leq \mu" /> for some parameter <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" />. Again, as long as the number of equivalence classes in this set is less than <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{\epsilon^2 n}" class="latex" title="2^{\epsilon^2 n}" />, we can use this bound.</p>
<p>To a first approximation, the number of classifiers (even after "bunching together") is roughly exponential in the number <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> of parameters, and so these results tell us that as long as the number of <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> of parameters is smaller than the number of examples, we can expect to have a small <em>generalization gap</em> and can infer future performance (known as "test performance") from the performance on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> (known as "train performance").
Once the number of parameters <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> becomes close to or even bigger than the number of samples <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, we are in danger of "overfitting" where we could have excellent train performance but terrible test performance. Thus according to the classical statistical learning theory, the ideal number of parameters would be some number between <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> and the number of samples <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />, with the precise value governed by the so called "bias variance tradeoff".</p>
<p>This is a beautiful theory, but unfortunately the classical theorems yield vacous  results in the realm of modern machine learning, where we  often train networks with millions of parameters on a mere tens of thousands of examples. Moreover, <a href="https://arxiv.org/abs/1611.03530">Zhang et al</a> showed that this is not just a question of counting  parameters better. They showed that modern deep networks can in fact "overfit" and achieve 100% success on the training set even if you gave them random or arbitrary labels.</p>
<p>The results above in particular show that we can find classifiers that perform great on the training set but perform terribly on the future tests, as well as classifiers that perform terrible on the training set but pretty good on future test. Specifically, consider an architecture that has the capacity to fit <img src="https://s0.wp.com/latex.php?latex=20n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="20n" class="latex" title="20n" /> arbitrary labels, and suppose that we train it on a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> examples.  Then we can find a setting of parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that both fits the training set exactly (i.e.,  satisfies <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta(x)=y" class="latex" title="f_\theta(x)=y" /> for all <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)\in S" class="latex" title="(x,y)\in S" />) but also satisfies that the additional constraint that <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3D+-y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta(x)= -y" class="latex" title="f_\theta(x)= -y" /> (i.e., the negation of the label <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />) for every <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)" class="latex" title="(x,y)" /> in some additional set <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> of <img src="https://s0.wp.com/latex.php?latex=19m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="19m" class="latex" title="19m" /> pairs.
(The set <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is not part of the actual training set, but rather an "auxiliary set" that we simply use for the sake of constructing this counterexample; note that we can use <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> as means to generate the initial network which can then be fed into standard stochastic gradient descent on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" />.) The network <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> fits its training set perfectly, but since it effectively corresponds to training with 95% label noise, it will  perform  worse than even a coin toss.</p>
<p>In an analogous way, we can find parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that completely fail on the training set, but fit correctly the additional "auxiliary set" <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. This will correspond to the case of standard training with 5% label noise, which typically yields about 95% of the performance on the noiseless distribution.</p>
<p>The above insights  break the <strong>separation of concerns</strong> or separation of <strong>computational problems</strong> from <strong>algorithms</strong> which we theorists  like so much. Ideally, we would like to phrase the "machine learning problem" as a well defined optimization objective, such as finding, given a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" />, the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" /> that mimimizes <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" />. Once phrased in this way, we can try to find with an algorithm that achieves this goal as efficiently as possible.</p>
<p>Unfortunately, modern machine learning does not currently lend itself to such a clean partition. In particular, since not all optima are equally good, we  <em>don’t  actually want</em> to solve the task of minimizing the loss function in a "black box" way. In fact, many of the ideas that make optimization faster such as accelaration, lower learning rate, second order methods and others, yield <em>worse</em>  generalization performance. Thus, while the objective function is somewhat correlated with generalization performance, it is neither necessary nor sufficient for it. This is a clear sign that we don’t really understand what makes machine learning work, and there is still much left to discover. I don’t know what machine learning textbooks in the 2030’s will contain, but my guess is that they would <em>not</em> prescribe running stochastic gradient descent on one of these loss functions. (Moritz Hardt counters that what we teach in ML today is not that far from the <a href="https://www.amazon.com/Pattern-Classification-Scene-Analysis-Richard/dp/0471223611">1973 book of Duda and Hart</a>, and that by some measures ML moved <em>slower</em> than other areas of CS.)</p>
<p>The <em>generalization puzzle</em> of machine learning can be phrased as the question of understanding what properties of procedures that map a training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> into a classifier <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> lead to good generalization performance with respect to certain distributions. In particular we would like to understand what are the properties of natural  natural distributions and stochastic gradient descent that make the latter into such a map.</p>
<h2>The computational puzzle</h2>
<p>Yet another puzzle in modern machine learning arises from the fact that we are able to find the minimum of <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> in the first place. A priori this is surprising since, apart from very special cases (e.g., linear regression with a square loss), the function <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cmapsto+L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \mapsto L_S(\theta)" class="latex" title="\theta \mapsto L_S(\theta)" /> is in general <em>non convex</em>. Indeed, for almost any natural loss function, the problem of finding <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that minimizes <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> is NP hard.
However, if we look at the computational question in the context of the generalization puzzle above, it might not be as mysterious. As we have seen, the fact that the <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> we output is a global minimizer (or close to minimizer)  of <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\cdot)" class="latex" title="L_S(\cdot)" /> is in some sense accidental and by far not the  the most important property of <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" />. There are many minima of the loss function that  generalize badly, and many non minima that  generalize well.</p>
<p>So perhaps the right way to phrase the computational puzzle is as</p>
<blockquote>
<p><em>"How come that we are able to use stochastic gradient descent to find the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that is output by stochastic gradient descent."</em></p>
</blockquote>
<p>which when phrased like that, doesn’t seem like much of a puzzle after all.</p>
<h2>The off-distribution performance puzzle</h2>
<p>In the supervised learning problem, the training samples <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> are drawn from the same distribution as the final test sample. But in any applications of machine learning, classifiers are expected to perform on samples that arise from very different settings. The image that the camera of a self-driving car observes is not drawn from ImageNet, and yet it still needs to (and often can) detect whether not it is seeing a dog or a cat (at which point it will break or accelerate, depending on whether the programmer was a dog or cat lover). Another insight to this question comes from a recent work of <a href="https://arxiv.org/abs/1902.10811">Recht et al</a>. They generated a new set of images that is very similar to the original ImageNet test set, but not identical to it. One can think of it as generated from a distribution <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> that is close but not the same as the original distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> of ImageNet. They then checked how well do neural networks that were trained on the original ImageNet distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> perform on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" />. They saw that while these networks performed significantly worse on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> than they did on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, their performance on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> was <em>highly correlated</em> with their performance on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />. Hence doing better on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> did correspond to being better in a way that carried over to the (very closely related) <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" />. (However, the networks did perform worse on $D’$ so off-distribution performance is by no means a full success story.)</p>
<p>Coming up with a theory that can supply some predictions for learning in a way that is not as tied to the particular distribution is still very much open. I see it as somewhat akin to finding a theory for the performance of algorithms that is somewhere between average-case complexity (which is highly dependant on the distribution) and worst-case complexity (which does not depend on the distribution at all, but is not always achievable).</p>
<h2>The robustness puzzle</h2>
<p>If the previous puzzles were about understanding why deep networks are surprisingly good, the next one is about understanding why they are surprisingly bad. Images of physical objects have the property that if we modify them in some ways,  such as perturbing them in  a small number of pixels or by few shades or rotating by an angle, they still correspond to the same object. Deep neural networks do not seem to "pick up" on this property.
Indeed, there are many examples of how tiny perturbations can cause a neural net to think that one image is another, and people have even <a href="https://youtu.be/piYnd_wYlT8">printed a 3D turtle</a> that most modern systems recognize as a rifle. (See this <a href="https://adversarial-ml-tutorial.org/">excellent tutorial</a>, though note an "ML decade" has already passed since it was published). This "brittleness" of neural networks can be a significant concern when we deploy them in the wild. (Though perhaps mixing up turtles and rifles is not so bad: I can imagine  some people that would normally resist regulations to protect the environment but would support them if they confused turtles with guns..)
Perhaps one reason for this brittleness is that neural networks can be thought of as a way of embedding a set of examples in dimension <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> into dimension <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ell" class="latex" title="\ell" /> (where <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ell" class="latex" title="\ell" /> is the number of neurons in the penultimate layer) in a way that will make the positive examples be linearly separable from the negative examples. Amplifying small differences can help in achieving such a separation, even if it hurts robustness.</p>
<p>Recent works have attempted to rectify this, by using a variants of the loss function where <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> corresponds to the maximum error under all possible such perturbations of the data. A priori you would think that while robust training might come at a computational cost, statistically it would be a "win win" with the resulting classifiers not only being more robust but also overall better at classifying. After all, we are providing the training procedure with the additional information (i.e., "updating its prior") that the label should be unchanged by certain transformations, which should be equivalent to supplying it with more data. Surprisingly, the robust classifiers currently perform <em>worse</em> than standard trained classifiers on unperturbed data. <a href="https://arxiv.org/abs/1905.02175">Ilyas et al</a> argued that this may be because even if humans ignore information encoded in, for example, whether the intensity level of a pixel is odd or even, it does not mean that this information is not predictive of the label. Suppose that (with no basis whatsoever – just as an example) cat owners are wealthier than dog owners and hence cat pictures tend to be taken with higher quality lenses. One could imagine that a neural network would pick up on that, and use some of the fine grained information in the pixels to help in classification. When we force such a network to be robust it would perform worse. Distill journal published <a href="https://distill.pub/2019/advex-bugs-discussion/">six discussion pieces</a> on the Ilyas et al paper. I like the idea of such "paper discussions" very much and hope it catches on in machine learning and beyond.</p>
<h2>The interpretability puzzle</h2>
<p>Deep neural networks are inspired by our brain, and it is tempting to try to understand their internal structure just like we try to understand the brain and see if it has a <a href="https://en.wikipedia.org/wiki/Grandmother_cell">"grandmother neuron"</a>. For example, we could try to see if there is a certain neuron (i.e., gate) in a neural network that "fires" only when it is fed images with certain high level features (or more generally find vectors that have large correlation with the state at a certain layer only when the image has some features). This also of practical importance, as we increasingly use classifiers to make decisions such as whether to approve or deny bail, whether to prescribe to a patient treatment A or B, or whether a car should steer left or right, and would like to understand what is the basis for such decisions. There are beautiful visualizations of neural networks’ decisions and internal structures , but given the robustness puzzle above, it is unclear if these really capture the decision process. After all, if we could change the classification from a cat to a dog by perturbing a tiny number of pixels, in what sense can we explain <em>why</em> the network made this decision or the other.</p>
<h2>The natural distributions puzzle</h2>
<p>Yet another puzzle (pointed out to me by Ilya Sutskever) is to understand what is it about "natural" distributions such as images, texts, etc.. that makes them so amenable to learning via neural networks, even though such networks can have a very hard time with learning even simple concepts such as parities.
Perhaps this is related to the "noise robustness" of natural concepts which is related to being correlated with low degree polynomials.
Another suggestion could be that at least for text etc.., human languages are implicitly designed to fit neural network. Perhaps on some other planets there are languages where the meaning of a sentence completely changes depending on whether it has an odd or an even number of letters…</p>
<h2>Summary</h2>
<p>The above are just a few puzzles that modern machine learning offers us. Not all of those might have answers in the form of mathematical theorems, or even well stated conjectures, but it is clear that there is still much to be discovered, and plenty of research opportunities for theoretical computer scientists. In this blog I focused on supervised learning, where at least the problem is well defined, but there are other areas of machine learning, such as transfer learning and generative modeling, where we don’t even yet know how to phrase the computational task, let alone prove that any particular procedure solves it. In several ways, the state of machine learning today seems to me as similar to the state of cryptography in the late 1970’s. After the discovery of public key cryptography, researchers has highly promising techniques and great intuitions, but still did not really understand even what security means, let alone how to achieve it. In the decades since, cryptography has turned from an art to a science, and I hope and believe the same will happen to machine learning.</p>
<p><strong>Acknowledgements:</strong> Thanks to Preetum Nakkiran, Aleksander Mądry, Ilya Sutskever and Moritz Hardt for helpful comments. (In particular, I dropped an interpretability experiment suggested in an earlier version of this post since Moritz informed me that several similar experiments have been done.) Needless to say, none of them is responsible for any of the speculations and/or errors above.</p>
</div>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/"><span class="datestr">at November 15, 2019 03:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/">Aarhus University is looking for excellent and visionary Assistant and Associate Professors to push the frontiers of Computer Science at Department of Computer Science, Aarhus University (apply by January 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for 2-4 Assistant Professors (Tenure Track – see full job description) and Associate Professors. We in particular wish to build competencies and groups within Machine Learning/Artificial Intelligence, Software Engineering and Systems Security, as well as Computer Graphics, and Computer Vision. In general, we encourage candidates within all areas of Computer Science to apply.</p>
<p>Website: <a href="https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en">https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en</a><br />
Email: kgronbak@cs.au.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/"><span class="datestr">at November 15, 2019 12:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/">Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br />
Email: simonsvisitorservices@berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/"><span class="datestr">at November 14, 2019 11:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
