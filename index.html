<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 18, 2019 02:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3436178446517561376">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html">Fields used to be closer together than they are now. Good? Bad?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
There was a retired software Eng professor that I had heard two very non-controversial rumors about:<br />
<br />
1) He got his PhD in Numerical Analysis<br />
<br />
2) He got his PhD in Compiler Optimization.<br />
<br />
So I asked him which was true.<br />
<br />
The answer: Both! In those days you had to optimize your code to get your NA code to run fast enough.<br />
<br />
We cannot imagine that anymore. Or at least I cannot.<br />
<br />
Over time the fields of computer science advance more so its hard to be  master of more than one field.  But its not that simple: there has been work recently applying Machine Learning to... well<br />
everything really. Even so, I think the trend is more towards separation. Or perhaps it oscillates.<br />
<br />
I am NOT going to be the grumpy old man (Google once thought I was 70, see <a href="https://blog.computationalcomplexity.org/2018/10/google-added-years-to-my-life.html">here</a>) who says things were better in my day when the fields were closer together. But I will ask the question:<br />
<br />
1) Are people more specialized new? While I think yes since each field has gotten more complicated and harder to master. There are exceptions: Complexity theory uses much more sophisticated mathematics then when I was a grad student (1980-1985), and of course Quantum Computing has lead to more comp sci majors knowing physics.<br />
<br />
2) Is it good for the field that people are specialized? I am supposed to say that it is terrible and that great advances are made when people are interdiscplinary. But there are many more small advances that are made by someone who has a mastery of one (or two) fields.<br />
<br />
3) The PhD Process and the Tenure Process encourage specialization. This I think IS bad since there are different modes of research that should all be respected.'<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html"><span class="datestr">at November 18, 2019 04:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/165">TR19-165 |  Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning | 

	Clement Canonne, 

	Xi Chen, 

	Gautam Kamath, 

	Amit Levi, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/165"><span class="datestr">at November 18, 2019 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06793">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06793">Testing linear-invariant properties</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tidor:Jonathan.html">Jonathan Tidor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Yufei.html">Yufei Zhao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06793">PDF</a><br /><b>Abstract: </b>We prove that all linear-invariant, linear-subspace hereditary, locally
characterized properties are proximity-oblivious testable (with one-sided error
and constant query-complexity). In other words, we show that we can distinguish
functions $\mathbb F_p^n\to[R]$ satisfying a given property from those that are
$\epsilon$-far from satisfying the property as long as the property is
definable by restrictions to bounded dimension subspaces.
</p>
<p>This result can be equivalently stated as an induced arithmetic removal
lemma: given a set of colored patterns $\mathbb F_p^d \to [R]$, for every
$\epsilon &gt; 0$ there exists $\delta &gt; 0$ such that if a function $f\colon
\mathbb F_p^n\to[R]$ has density at most $\delta$ of each of the prescribed
patterns, then $f$ can be made free of these patterns by recoloring at most
$\epsilon p^n$ points.
</p>
<p>The proof of this result uses two main techniques. The first builds upon a
long line of work which applies regularity methods, including results from
higher order Fourier analysis, to prove results on property testing and removal
lemmas. The second, the main innovation of this work, is a novel recoloring
technique that allows us to handle an important obstacle encountered by
previous works in the arithmetic setting. Roughly speaking, this obstacle is
the inability of regularity methods to regularize functions in a neighborhood
of the origin.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06793"><span class="datestr">at November 18, 2019 02:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06790">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06790">Computationally Data-Independent Memory Hard Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ameri:Mohammad_Hassan.html">Mohammad Hassan Ameri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blocki:Jeremiah.html">Jeremiah Blocki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06790">PDF</a><br /><b>Abstract: </b>Memory hard functions (MHFs) are an important cryptographic primitive that
are used to design egalitarian proofs of work and in the construction of
moderately expensive key-derivation functions resistant to brute-force attacks.
Broadly speaking, MHFs can be divided into two categories: data-dependent
memory hard functions (dMHFs) and data-independent memory hard functions
(iMHFs). iMHFs are resistant to certain side-channel attacks as the memory
access pattern induced by the honest evaluation algorithm is independent of the
potentially sensitive input e.g., password. While dMHFs are potentially
vulnerable to side-channel attacks (the induced memory access pattern might
leak useful information to a brute-force attacker), they can achieve higher
cumulative memory complexity (CMC) in comparison than an iMHF. In this paper,
we introduce the notion of computationally data-independent memory hard
functions (ciMHFs). Intuitively, we require that memory access pattern induced
by the (randomized) ciMHF evaluation algorithm appears to be independent from
the standpoint of a computationally bounded eavesdropping attacker --- even if
the attacker selects the initial input. We then ask whether it is possible to
circumvent known upper bound for iMHFs and build a ciMHF with CMC
$\Omega(N^2)$. Surprisingly, we answer the question in the affirmative when the
ciMHF evaluation algorithm is executed on a two-tiered memory architecture
(RAM/Cache).
</p>
<p>See paper for the full abstract.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06790"><span class="datestr">at November 18, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06738">Semi-Algebraic Proofs, IPS Lower Bounds and the $\tau$-Conjecture: Can a Natural Number be Negative?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alekseev:Yaroslav.html">Yaroslav Alekseev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grigoriev:Dima.html">Dima Grigoriev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirsch:Edward_A=.html">Edward A. Hirsch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzameret:Iddo.html">Iddo Tzameret</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06738">PDF</a><br /><b>Abstract: </b>We introduce the binary value principle which is a simple subset-sum instance
expressing that a natural number written in binary cannot be negative, relating
it to central problems in proof and algebraic complexity. We prove conditional
superpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of
this instance, based on a well-known hypothesis by Shub and Smale about the
hardness of computing factorials, where IPS is the strong algebraic proof
system introduced by Grochow and Pitassi (2018). Conversely, we show that short
IPS refutations of this instance bridge the gap between sufficiently strong
algebraic and semi-algebraic proof systems. Our results extend to full-fledged
IPS the paradigm introduced in Forbes et al. (2016), whereby lower bounds
against subsystems of IPS were obtained using restricted algebraic circuit
lower bounds, and demonstrate that the binary value principle captures the
advantage of semi-algebraic over algebraic reasoning, for sufficiently strong
systems. Specifically, we show the following: (abstract continues in document.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06738"><span class="datestr">at November 18, 2019 02:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06664">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06664">Automated Derivation of Parametric Data Movement Lower Bounds for Affine Programs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Auguste Olivry, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langou:Julien.html">Julien Langou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pouchet:Louis=No=euml=l.html">Louis-Noël Pouchet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadayappan:P=.html">P. Sadayappan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rastello:Fabrice.html">Fabrice Rastello</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06664">PDF</a><br /><b>Abstract: </b>For most relevant computation, the energy and time needed for data movement
dominates that for performing arithmetic operations on all computing systems
today. Hence it is of critical importance to understand the minimal total data
movement achievable during the execution of an algorithm. The achieved total
data movement for different schedules of an algorithm can vary widely depending
on how efficiently the cache is used, e.g., untiled versus effectively tiled
matrix-matrix multiplication. A significant current challenge is that no
existing tool is able to meaningfully quantify the potential reduction to the
data movement of a computation that can be achieved by more effective use of
the cache through operation rescheduling. Asymptotic parametric expressions of
data movement lower bounds have previously been manually derived for a limited
number of algorithms, often without scaling constants. In this paper, we
present the first compile-time approach for deriving non-asymptotic parametric
expressions of data movement lower bounds for arbitrary affine computations.
The approach has been implemented in a fully automatic tool (IOLB) that can
generate these lower bounds for input affine programs.
</p>
<p>IOLB's use is demonstrated by exercising it on all the benchmarks of the
PolyBench suite. The advantages of IOLB are many: (1) IOLB enables us to derive
bounds for few dozens of algorithms for which these lower bounds have never
been derived. This reflects an increase of productivity by automation. (2)
Anyone is able to obtain these lower bounds through IOLB, no expertise is
required. (3) For some of the most well-studied algorithms, the lower bounds
obtained by \tool are higher than any previously reported manually derived
lower bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06664"><span class="datestr">at November 18, 2019 02:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06600">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06600">GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Anh=Duc.html">Anh-Duc Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choi:Seonghwa.html">Seonghwa Choi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Woojae.html">Woojae Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Sanghoon.html">Sanghoon Lee</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06600">PDF</a><br /><b>Abstract: </b>In this paper, we present a novel deep method to reconstruct a point cloud of
an object from a single still image. Prior arts in the field struggle to
reconstruct an accurate and scalable 3D model due to either the inefficient and
expensive 3D representations, the dependency between the output and number of
model parameters or the lack of a suitable computing operation. We propose to
overcome these by deforming a random point cloud to the object shape through
two steps: feature blending and deformation. In the first step, the global and
point-specific shape features extracted from a 2D object image are blended with
the encoded feature of a randomly generated point cloud, and then this mixture
is sent to the deformation step to produce the final representative point set
of the object. In the deformation process, we introduce a new layer termed as
GraphX that considers the inter-relationship between points like common graph
convolutions but operates on unordered sets. Moreover, with a simple trick, the
proposed model can generate an arbitrary-sized point cloud, which is the first
deep method to do so. Extensive experiments verify that we outperform existing
models and halve the state-of-the-art distance score in single image 3D
reconstruction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06600"><span class="datestr">at November 18, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06436">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06436">Weighted Triangle-free 2-matching Problem with Edge-disjoint Forbidden Triangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yusuke.html">Yusuke Kobayashi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06436">PDF</a><br /><b>Abstract: </b>The weighted $\mathcal{T}$-free $2$-matching problem is the following
problem: given an undirected graph $G$, a weight function on its edge set, and
a set $\mathcal{T}$ of triangles in $G$, find a maximum weight $2$-matching
containing no triangle in $\mathcal{T}$. When $\mathcal{T}$ is the set of all
triangles in $G$, this problem is known as the weighted triangle-free
$2$-matching problem, which is a long-standing open problem. A main
contribution of this paper is to give a first polynomial-time algorithm for the
weighted $\mathcal{T}$-free $2$-matching problem under the assumption that
$\mathcal{T}$ is a set of edge-disjoint triangles. In our algorithm, a key
ingredient is to give an extended formulation representing the solution set,
that is, we introduce new variables and represent the convex hull of the
feasible solutions as a projection of another polytope in a higher dimensional
space. Although our extended formulation has exponentially many inequalities,
we show that the separation problem can be solved in polynomial time, which
leads to a polynomial-time algorithm for the weighted $\mathcal{T}$-free
$2$-matching problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06436"><span class="datestr">at November 18, 2019 02:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06403">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06403">New Bounds on $k$-Planar Crossing Numbers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Alireza Shavali, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zarrabi=Zadeh:Hamid.html">Hamid Zarrabi-Zadeh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06403">PDF</a><br /><b>Abstract: </b>The crossing number $cr(G)$ of a graph $G$ is the minimum number of crossings
over all possible drawings of $G$ in the plane. Analogously, the $k$-planar
crossing number of $G$, denoted by $cr_{k}(G)$, is the minimum number of
crossings over all possible drawings of the edges of $G$ in $k$ disjoint
planes. We present new bounds on the $k$-planar crossing number of complete
graphs and complete bipartite graphs. In particular, for the case of $k=2$, we
improve the current best lower bounds on biplanar crossing numbers by a factor
of 1.37 for complete graphs, and by a factor of 1.34 for complete bipartite
graphs. We extend our results to the $k$-planar crossing number of complete
(bipartite) graphs, for any positive integer $k \geq 2$. To better understand
the relation between crossing numbers and biplanar crossing numbers, we pose a
new problem of finding the largest crossing number that implies biplanarity. In
particular, we prove that for every graph $G$, $cr(G) \leq 10$ implies
$cr_{2}(G)=0$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06403"><span class="datestr">at November 18, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06358">Hardness of Learning DNFs using Halfspaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghoshal:Suprovat.html">Suprovat Ghoshal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saket:Rishi.html">Rishi Saket</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06358">PDF</a><br /><b>Abstract: </b>The problem of learning $t$-term DNF formulas (for $t = O(1)$) has been
studied extensively in the PAC model since its introduction by Valiant (STOC
1984). A $t$-term DNF can be efficiently learnt using a $t$-term DNF only if $t
= 1$ i.e., when it is an AND, while even weakly learning a $2$-term DNF using a
constant term DNF was shown to be NP-hard by Khot and Saket (FOCS 2008). On the
other hand, Feldman et al. (FOCS 2009) showed the hardness of weakly learning a
noisy AND using a halfspace -- the latter being a generalization of an AND,
while Khot and Saket (STOC 2008) showed that an intersection of two halfspaces
is hard to weakly learn using any function of constantly many halfspaces. The
question of whether a $2$-term DNF is efficiently learnable using $2$ or
constantly many halfspaces remained open.
</p>
<p>In this work we answer this question in the negative by showing the hardness
of weakly learning a $2$-term DNF as well as a noisy AND using any function of
a constant number of halfspaces. In particular we prove the following. For any
constants $\nu, \zeta &gt; 0$ and $\ell \in \mathbb{N}$, given a distribution over
point-value pairs $\{0,1\}^n \times \{0,1\}$, it is NP-hard to decide whether,
</p>
<p>YES Case: There is a $2$-term DNF that classifies all the points of the
distribution, and an AND that classifies at least $1-\zeta$ fraction of the
points correctly.
</p>
<p>NO Case: Any boolean function depending on at most $\ell$ halfspaces
classifies at most $1/2 + \nu$ fraction of the points of the distribution
correctly.
</p>
<p>Our result generalizes and strengthens the previous best results mentioned
above on the hardness of learning a $2$-term DNF, learning an intersection of
two halfspaces, and learning a noisy AND.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06358"><span class="datestr">at November 18, 2019 02:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06347">In Search of the Fastest Concurrent Union-Find Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alistarh:Dan.html">Dan Alistarh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fedorov:Alexander.html">Alexander Fedorov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koval:Nikita.html">Nikita Koval</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06347">PDF</a><br /><b>Abstract: </b>Union-Find (or Disjoint-Set Union) is one of the fundamental problems in
computer science; it has been well-studied from both theoretical and practical
perspectives in the sequential case. Recently, there has been mounting interest
in analyzing this problem in the concurrent scenario, and several
asymptotically-efficient algorithms have been proposed. Yet, to date, there is
very little known about the practical performance of concurrent Union-Find.
</p>
<p>This work addresses this gap. We evaluate and analyze the performance of
several concurrent Union-Find algorithms and optimization strategies across a
wide range of platforms (Intel, AMD, and ARM) and workloads (social, random,
and road networks, as well as integrations into more complex algorithms). We
first observe that, due to the limited computational cost, the number of
induced cache misses is the critical determining factor for the performance of
existing algorithms. We introduce new techniques to reduce this cost by storing
node priorities implicitly and by using plain reads and writes in a way that
does not affect the correctness of the algorithms. Finally, we show that
Union-Find implementations are an interesting application for Transactional
Memory (TM): one of the fastest algorithm variants we discovered is a
sequential one that uses coarse-grained locking with the lock elision
optimization to reduce synchronization cost and increase scalability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06347"><span class="datestr">at November 18, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06264">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06264">The Isoperimetric Problem in a Lattice of $\mathbb{H}^3$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Guillermo Lobos, Alvaro Hancco, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Batista:Val=eacute=rio_Ramos.html">Valério Ramos Batista</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06264">PDF</a><br /><b>Abstract: </b>The isoperimetric problem is one of the oldest in geometry and it consists of
finding a surface of minimum area that encloses a given volume $V$. It is
particularly important in physics because of its strong relation with
stability, and this also involves the study of phenomena in non-Euclidean
spaces. Of course, such spaces cannot be customized for lab experiments but we
can resort to computational simulations, and one of the mostly used softwares
for this purpose is the Surface Evolver. In this paper we use it to study the
isoperimetric problem in a lattice of the three dimensional hyperbolic space.
More precisely: up to isometries, there exists a unique tesselation of
$\mathbb{H}^3$ by non-ideal cubes $\mathcal{C}$. Now let $\Omega$ be a
connected isoperimetric region inside the non-ideal hyperbolic cube
$\mathcal{C}$. Under weak assumptions on graph and symmetry we find all
numerical solutions $\Sigma=\partial\Omega$ of the isoperimetric problem in
$\mathcal{C}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06264"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4414">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4414">The Aaronson-Ambainis Conjecture (2008-2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that Lance Fortnow suggested in one of his papers: is it possible to separate <a href="https://en.wikipedia.org/wiki/P_(complexity)">P</a> from <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> relative to a <a href="https://en.wikipedia.org/wiki/Random_oracle">random oracle</a>?  (That is, without first needing to separate P from PSPACE or whatever in the real world?)  Or to the contrary: suppose that a quantum algorithm Q makes T queries to a Boolean input string X.  Is there then a classical simulation algorithm that makes poly(T) queries to X, and that approximates Q’s acceptance probability for <em>most</em> values of X?  Such a classical simulation, were it possible, would still be consistent with the existence of quantum algorithms like <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon’s</a> and <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s</a>, which are able to achieve exponential (and even greater) speedups in the black-box setting.  It would simply underscore the importance, for Simon’s and Shor’s algorithms, of global structure that makes the string X extremely <em>non</em>-random: for example, encoding a periodic function (in the case of Shor’s algorithm), or encoding a function that hides a secret string s (in the case of Simon’s).  It would underscore that superpolynomial quantum speedups depend on structure.</p>



<p>I never managed to solve this problem.  Around 2008, though, I noticed that a solution would follow from a perhaps-not-obviously-related conjecture, about <em>influences</em> in low-degree polynomials.  Namely, let p:R<sup>n</sup>→R be a degree-d real polynomial in n variables, and suppose p(x)∈[0,1] for all x∈{0,1}<sup>n</sup>.  Define the <i>variance</i> of p to be Var(p):=E<sub>x,y</sub>[|p(x)-p(y)|], and define the <i>influence</i> of the i<sup>th</sup> variable to be Inf<sub>i</sub>(p):=E<sub>x</sub>[|p(x)-p(x<sup>i</sup>)|].  Here the expectations are over strings in {0,1}<sup>n</sup>, and x<sup>i</sup> means x with its i<sup>th</sup> bit flipped between 0 and 1.  Then the conjecture is this: there must be some variable i such that Inf<sub>i</sub>(p) &gt; poly(Var(p)/d) (in other words, that “explains” a non-negligible fraction of the variance of the entire polynomial).</p>



<p>Why would this conjecture imply the statement about quantum algorithms?  Basically, because of the seminal result of <a href="https://arxiv.org/abs/quant-ph/9802049">Beals et al.</a> from 1998: that if a quantum algorithm makes T queries to a Boolean input X, then its acceptance probability can be written as a real polynomial over the bits of X, of degree at most 2T.  Given that result, if you wanted to classically simulate a quantum algorithm Q on most inputs—and if you only cared about query complexity, not computation time—you’d simply need to do the following:<br />(1) Find the polynomial p that represents Q’s acceptance probability.<br />(2) Find a variable i that explains at least a 1/poly(T) fraction of the total remaining variance in p, and query that i.<br />(3) Keep repeating step (2), until p has been restricted to a polynomial with not much variance left—i.e., to nearly a constant function p(X)=c.  Whenever that happens, halt and output the constant c.<br />The key is that by hypothesis, this algorithm will halt, with high probability over X, after only poly(T) steps.</p>



<p>Anyway, around the same time, Andris Ambainis had a major break on a different problem that I’d told him about: namely, whether randomized and quantum query complexities are polynomially related for all partial functions with permutation symmetry (like the collision and the element distinctness functions).  Andris and I decided to write up the two directions jointly.  The result was our 2011 paper entitled <a href="https://arxiv.org/abs/0911.0996">The Need for Structure in Quantum Speedups</a>.</p>



<p>Of the two contributions in the “Need for Structure” paper, the one about random oracles and influences in low-degree polynomials was clearly the weaker and less satisfying one.  As the reviewers pointed out, that part of the paper didn’t solve anything: it just reduced one unsolved problem to a new, slightly different problem that was <em>also</em> unsolved.  Nevertheless, that part of the paper acquired a life of its own over the last decade, as the world’s experts in analysis of Boolean functions and polynomials began referring to the “Aaronson-Ambainis Conjecture.”  Ryan O’Donnell, Guy Kindler, and many others had a stab.  I even got Terry Tao to spend an hour to two on the problem when I visited UCLA.</p>



<p>Now, at long last, Nathan Keller and Ohad Klein say they’ve solved the problem, in a preprint whose title is a riff on ours: <a href="https://arxiv.org/abs/1911.03748">“Quantum Speedups Need Structure.”</a></p>



<p>Their paper hasn’t yet been peer-reviewed, and I haven’t yet carefully studied it, but I <em>could</em> and <em>should</em>: at 19 pages, it looks very approachable and clear, if not as radically short as (say) <a href="https://www.scottaaronson.com/blog/?p=4229">Huang’s proof of the Sensitivity Conjecture</a>.  Keller and Klein’s argument subsumes all the earlier results that I knew would need to be subsumed, and involves all the concepts (like a real analogue of block sensitivity) that I knew would need to be involved.</p>



<p>My plan had been as follows:<br />(1) Read their paper in detail.  Understand every step of their proof.<br />(2) Write a blog post that reflects my detailed understanding.</p>



<p>Unfortunately, this plan did not sufficiently grapple with the fact that I now have two kids.  It got snagged for a week at step (1).  So I’m now executing an alternative plan, which is to jump immediately to the blog post.</p>



<p>Anyway, assuming Keller and Klein’s result holds up—as I expect it will—by combining it with the observations in my and Andris’s paper, one immediately gets an explanation for why no one has managed to separate P from BQP relative to a <em>random</em> oracle (but only relative to non-random oracles).  This complements the work of <a href="https://www.uncg.edu/mat/faculty/cdsmyth/thesis.pdf">Kahn, Saks, and Smyth</a>, who around 2000 gave a precisely analogous explanation for the difficulty of separating P from NP∩coNP relative to a random oracle.  Unfortunately, the polynomial blowup is quite enormous: from a quantum algorithm making T queries, Keller and Klein apparently get a classical algorithm making O(T<sup>18</sup>) queries.  But such things can almost always be massively improved.</p>



<p>Feel free to use the comments to ask any questions about this result or its broader context.  I’ll either do my best to answer from the limited amount I know, or else I’ll pass the questions along to Nathan and Ohad themselves.  Maybe, at some point, I’ll even be forced to understand the new proof.</p>



<p>Congratulations to Nathan and Ohad!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4414"><span class="datestr">at November 17, 2019 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/164">TR19-164 |  Improved bounds for perfect sampling of $k$-colorings in graphs | 

	Siddharth Bhandari, 

	Sayantan Chakraborty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present a randomized algorithm that takes as input an undirected $n$-vertex graph $G$ with maximum degree $\Delta$ and an integer $k &gt; 3\Delta$, and returns a random proper $k$-coloring of $G$. The 
 distribution of the coloring is perfectly uniform over the set of all proper $k$-colorings; the expected running time of the algorithm is $\mathrm{poly}(k,n)=\widetilde{O}(n\Delta^2\cdot \log(k))$. 
 This improves upon a result of Huber~(STOC 1998) who obtained polynomial time perfect sampling algorithm for $k&gt;\Delta^2+2\Delta$.
 Prior to our work, no algorithm with expected running time $\mathrm{poly}(k,n)$ was known to guarantee perfectly sampling for $\Delta = \omega(1)$ and for any $k \leq \Delta^2+2\Delta$. 
 
 Our algorithm (like several other perfect sampling algorithms including Huber's) is based on  the Coupling from the Past method. Inspired by the bounding chain approach pioneered independently by H\"aggstr\"om \&amp; Nelander~(Scand.{} J.{} Statist., 1999) and Huber~(STOC 1998), our algorithm is based on a novel bounding chain for the coloring problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/164"><span class="datestr">at November 17, 2019 10:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06144">A Penetration Metric for Deforming Tetrahedra using Object Norm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Jisu.html">Jisu Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Young_J=.html">Young J. Kim</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06144">PDF</a><br /><b>Abstract: </b>In this paper, we propose a novel penetration metric, called deformable
penetration depth PDd, to define a measure of inter-penetration between two
linearly deforming tetrahedra using the object norm. First of all, we show that
a distance metric for a tetrahedron deforming between two configurations can be
found in closed form based on object norm. Then, we show that the PDd between
an intersecting pair of static and deforming tetrahedra can be found by solving
a quadratic programming (QP) problem in terms of the distance metric with
non-penetration constraints. We also show that the PDd between two,
intersected, deforming tetrahedra can be found by solving a similar QP problem
under some assumption on penetrating directions, and it can be also accelerated
by an order of magnitude using pre-calculated penetration direction. We have
implemented our algorithm on a standard PC platform using an off-the-shelf QP
optimizer, and experimentally show that both the static/deformable and
deformable/deformable tetrahedra cases can be solvable in from a few to tens of
milliseconds. Finally, we demonstrate that our penetration metric is
three-times smaller (or tighter) than the classical, rigid penetration depth
metric in our experiments.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06144"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.06132">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.06132">$\{-1,0,1\}$-APSP and (min,max)-Product Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Hodaya Barr, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopelowitz:Tsvi.html">Tsvi Kopelowitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roditty:Liam.html">Liam Roditty</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06132">PDF</a><br /><b>Abstract: </b>In the $\{-1,0,1\}$-APSP problem the goal is to compute all-pairs shortest
paths (APSP) on a directed graph whose edge weights are all from $\{-1,0,1\}$.
In the (min,max)-product problem the input is two $n\times n$ matrices $A$ and
$B$, and the goal is to output the (min,max)-product of $A$ and $B$.
</p>
<p>This paper provides a new algorithm for the $\{-1,0,1\}$-APSP problem via a
simple reduction to the target-(min,max)-product problem where the input is
three $n\times n$ matrices $A,B$, and $T$, and the goal is to output a Boolean
$n\times n$ matrix $C$ such that the $(i,j)$ entry of $C$ is 1 if and only if
the $(i,j)$ entry of the (min,max)-product of $A$ and $B$ is exactly the
$(i,j)$ entry of the target matrix $T$. If (min,max)-product can be solved in
$T_{MM}(n) = \Omega(n^2)$ time then it is straightforward to solve
target-(min,max)-product in $O(T_{MM}(n))$ time. Thus, given the recent result
of Bringmann, K\"unnemann, and Wegrzycki [STOC 2019], the $\{-1,0,1\}$-APSP
problem can be solved in the same time needed for solving approximate APSP on
graphs with positive weights.
</p>
<p>Moreover, we design a simple algorithm for target-(min,max)-product when the
inputs are restricted to the family of inputs generated by our reduction. Using
fast rectangular matrix multiplication, the new algorithm is faster than the
current best known algorithm for (min,max)-product.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.06132"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.05991">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.05991">Graph Spanners in the Message-Passing Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernandez:Manuel.html">Manuel Fernandez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yasuda:Taisuke.html">Taisuke Yasuda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05991">PDF</a><br /><b>Abstract: </b>Graph spanners are sparse subgraphs which approximately preserve all pairwise
shortest-path distances in an input graph. The notion of approximation can be
additive, multiplicative, or both, and many variants of this problem have been
extensively studied. We study the problem of computing a graph spanner when the
edges of the input graph are distributed across two or more sites in an
arbitrary, possibly worst-case partition, and the goal is for the sites to
minimize the communication used to output a spanner. We assume the
message-passing model of communication, for which there is a point-to-point
link between all pairs of sites as well as a coordinator who is responsible for
producing the output. We stress that the subset of edges that each site has is
not related to the network topology, which is fixed to be point-to-point. While
this model has been extensively studied for related problems such as graph
connectivity, it has not been systematically studied for graph spanners. We
present the first tradeoffs for total communication versus the quality of the
spanners computed, for two or more sites, as well as for additive and
multiplicative notions of distortion. We show separations in the communication
complexity when edges are allowed to occur on multiple sites, versus when each
edge occurs on at most one site. We obtain nearly tight bounds (up to polylog
factors) for the communication of additive $2$-spanners in both the with and
without duplication models, multiplicative $(2k-1)$-spanners in the with
duplication model, and multiplicative $3$ and $5$-spanners in the without
duplication model. Our lower bound for multiplicative $3$-spanners employs
biregular bipartite graphs rather than the usual Erd\H{o}s girth conjecture
graphs and may be of wider interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.05991"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.05949">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.05949">Online Second Price Auction with Semi-bandit Feedback Under the Non-Stationary Setting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Haoyu.html">Haoyu Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wei.html">Wei Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05949">PDF</a><br /><b>Abstract: </b>In this paper, we study the non-stationary online second price auction
problem. We assume that the seller is selling the same type of items in $T$
rounds by the second price auction, and she can set the reserve price in each
round. In each round, the bidders draw their private values from a joint
distribution unknown to the seller. Then, the seller announced the reserve
price in this round. Next, bidders with private values higher than the
announced reserve price in that round will report their values to the seller as
their bids. The bidder with the highest bid larger than the reserved price
would win the item and she will pay to the seller the price equal to the
second-highest bid or the reserve price, whichever is larger. The seller wants
to maximize her total revenue during the time horizon $T$ while learning the
distribution of private values over time. The problem is more challenging than
the standard online learning scenario since the private value distribution is
non-stationary, meaning that the distribution of bidders' private values may
change over time, and we need to use the \emph{non-stationary regret} to
measure the performance of our algorithm. To our knowledge, this paper is the
first to study the repeated auction in the non-stationary setting
theoretically. Our algorithm achieves the non-stationary regret upper bound
$\tilde{\mathcal{O}}(\min\{\sqrt{\mathcal S T},
\bar{\mathcal{V}}^{\frac{1}{3}}T^{\frac{2}{3}}\})$, where $\mathcal S$ is the
number of switches in the distribution, and $\bar{\mathcal{V}}$ is the sum of
total variation, and $\mathcal S$ and $\bar{\mathcal{V}}$ are not needed to be
known by the algorithm. We also prove regret lower bounds
$\Omega(\sqrt{\mathcal S T})$ in the switching case and
$\Omega(\bar{\mathcal{V}}^{\frac{1}{3}}T^{\frac{2}{3}})$ in the dynamic case,
showing that our algorithm has nearly optimal \emph{non-stationary regret}.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.05949"><span class="datestr">at November 17, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.05911">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.05911">Recent Advances in Algorithmic High-Dimensional Robust Statistics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05911">PDF</a><br /><b>Abstract: </b>Learning in the presence of outliers is a fundamental problem in statistics.
Until recently, all known efficient unsupervised learning algorithms were very
sensitive to outliers in high dimensions. In particular, even for the task of
robust mean estimation under natural distributional assumptions, no efficient
algorithm was known. Recent work in theoretical computer science gave the first
efficient robust estimators for a number of fundamental statistical tasks,
including mean and covariance estimation. Since then, there has been a flurry
of research activity on algorithmic high-dimensional robust estimation in a
range of settings. In this survey article, we introduce the core ideas and
algorithmic techniques in the emerging area of algorithmic high-dimensional
robust statistics with a focus on robust mean estimation. We also provide an
overview of the approaches that have led to computationally efficient robust
estimators for a range of broader statistical tasks and discuss new directions
and opportunities for future work.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.05911"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.05896">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.05896">Linear Time Subgraph Counting, Graph Degeneracy, and the Chasm at Size Six</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, Noujan Pashanasangi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05896">PDF</a><br /><b>Abstract: </b>We consider the problem of counting all $k$-vertex subgraphs in an input
graph, for any constant $k$. This problem (denoted sub-cnt$_k$) has been
studied extensively in both theory and practice. In a classic result, Chiba and
Nishizeki (SICOMP 85) gave linear time algorithms for clique and 4-cycle
counting for bounded degeneracy graphs. This is a rich class of sparse graphs
that contains, for example, all minor-free families and preferential attachment
graphs. The techniques from this result have inspired a number of recent
practical algorithms for sub-cnt$_k$. Towards a better understanding of the
limits of these techniques, we ask: for what values of $k$ can sub-cnt$_k$ be
solved in linear time?
</p>
<p>We discover a chasm at $k=6$. Specifically, we prove that for $k &lt; 6$,
sub-cnt$_k$ can be solved in linear time. Assuming a standard conjecture in
fine-grained complexity, we prove that for all $k \geq 6$, sub-cnt$_k$ cannot
be solved even in near-linear time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.05896"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1911.05834">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1911.05834">The Complexity of Synthesizing nop-Equipped Boolean Nets from g-Bounded Inputs (Technical Report)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tredup:Ronny.html">Ronny Tredup</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05834">PDF</a><br /><b>Abstract: </b>Boolean Petri nets equipped with nop allow places and transitions to be
independent by being related by nop. We characterize for any fixed natural
number g the computational complexity of synthesizing nop-equipped Boolean
Petri nets from labeled directed graphs whose states have at most g incoming
and at most g outgoing arcs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1911.05834"><span class="datestr">at November 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/">Postdoc at Princeton University (apply by January 10, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Princeton University is seeking exceptional recent Ph.D. recipients for research positions in theoretical computer science and theoretical machine learning.</p>
<p>Note that there are two positions, <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281</a> and at the link below.</p>
<p>Website: <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221</a><br />
Email: smweinberg@princeton.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/"><span class="datestr">at November 16, 2019 05:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/163">TR19-163 |  Approximating the Distance to Monotonicity of Boolean Functions | 

	Ramesh Krishnan S. Pallavoor, 

	Sofya Raskhodnikova, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We design a nonadaptive algorithm that, given a Boolean function $f\colon \{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n, 1/\alpha)$ queries and returns an estimate that, with high probability, is an $\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity. Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires $2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n, 1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a question of Seshadhri (Property Testing Review, 2014) for the case of nonadaptive algorithms. Approximating the distance to a property is closely related to tolerantly testing that property. Our lower bound stands in contrast to standard (non-tolerant) testing of monotonicity that can be done nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.

We obtain our lower bound by proving an analogous bound for erasure-resilient testers. An $\alpha$-erasure-resilient tester for a desired property gets oracle access to a function that has at most an $\alpha$ fraction of values erased. The tester has to accept (with probability at least 2/3) if the erasures can be filled in to ensure that the resulting function has the property and to reject (with probability at least 2/3) if every completion of erasures results in a function that is $\varepsilon$-far from having the property. Our method yields the same lower bounds for unateness and being a $k$-junta. These lower bounds improve exponentially on the existing lower bounds for these properties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/163"><span class="datestr">at November 16, 2019 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://scholarlyoa.com/omics-group-now-charging-for-article-withdrawals/">OMICS Group now charging for article withdrawals</a> (<a href="https://mathstodon.xyz/@11011110/103069657170279386"></a>): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</p>
  </li>
  <li>
    <p><a href="https://www.shapeoperator.com/2016/12/12/sunset-geometry/">Sunset geometry</a> (<a href="https://mathstodon.xyz/@11011110/103075417779881667"></a>, <a href="https://news.ycombinator.com/item?id=21413358">via</a>). How to tell the radius of the earth from a photo of a sunset over a still body of water (knowing the height of the camera over the water). Not explained: how still the water needs to be, how badly the results are affected by atmospheric refraction, how accurately the measurements need to be performed to get a meaningful estimate, or how likely you are to see the sun meet the horizon before low clouds get in the way.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/profession/ams-fellows/new-fellows">The new AMS fellows</a> (<a href="https://mathstodon.xyz/@11011110/103077029444407287"></a>) include graph theorists Daniel Kráľ and Bojan Mohar, and fellow Wikipedia editor Marie Vitulli. Their announcement also led me to create new Wikipedia articles on new fellows <a href="https://en.wikipedia.org/wiki/Chikako_Mese">Chikako Mese</a>, <a href="https://en.wikipedia.org/wiki/Julianna_Tymoczko">Julianna Tymoczko</a>, and <a href="https://en.wikipedia.org/wiki/Jang-Mei_Wu">Jang-Mei Wu</a>, and Vitulli to create one for <a href="https://en.wikipedia.org/wiki/Tara_E._Brendle">Tara Brendle</a>. Congratulations, all!</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/maa-awards/writing-awards/the-graph-menagerie-abstract-algebra-and-the-mad-veterinarian">The graph menagerie: abstract algebra and the mad veterinarian </a> (<a href="https://mathstodon.xyz/@11011110/103083916322783935"></a>). Or, how to solve puzzles like: “Suppose a mad veterinarian creates a transmogrifier that can convert one cat into two dogs and five mice, or one dog into three cats and three mice, or a mouse into a cat and a dog. It can also do each of these operations in reverse. Can it, through any sequence of operations, convert two cats into a pack of dogs? How about one cat?”</p>
  </li>
  <li>
    <p><a href="http://processalgebra.blogspot.com/2019/11/call-for-opinions-length-of-papes-in.html">LIPIcs series editor Luca Aceto polls the community on page limits</a> (<a href="https://mathstodon.xyz/@11011110/103086511516971922"></a>). It used to be that conferences in theoretical computer science had page limits because you couldn’t bind volumes with too many paper pages, now long irrelevant. So now that we <em>can</em> publish much longer conference papers, should we? Limits encourage authors to publish full details in a properly refereed journal version, but unlimited length recognizes the reality that many authors are too lazy to make journal versions.</p>
  </li>
  <li>
    <p><a href="https://understandinguncertainty.org/squaring-square-glass">Squaring the square, in stained glass</a> (<a href="https://mathstodon.xyz/@11011110/103095223291132423"></a>, <a href="https://scilogs.spektrum.de/hlf/perfect-squares/">via</a>). By David Spiegelhalter, 2013.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Edge_tessellation">Tiling the plane by edge reflection</a> (<a href="https://mathstodon.xyz/@11011110/103099409883184951"></a>). Here’s a proof sketch that there are eight ways to do this:
Each prototile vertex must have angle  for integer , and if  is odd, the subsequence of the remaining angles must be symmetric. For an -gon, considering the sum of interior angles shows that</p>

    

    <p>Searching for sequences of integers with these properties (choosing the smallest integers first to make the search bounded) finds that the only cyclic sequences of integers meeting these constraints are (3,12,12), (4,8,8), (4,6,12), (6,6,6), (3,4,6,4), (3,6,3,6), (4,4,4,4), and (3,3,3,3,3,3), the sequences of the 8 known tessellations.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/11/08/turkish-academics-sound-alarm-over-gender-segregation-plans">Turkish academics sound alarm over gender segregation plans</a> (<a href="https://mathstodon.xyz/@11011110/103103753533385831"></a>). When women’s universities are set up to provide alternatives in the face of persistent discrimination against women in the existing universities (as they were in the US and Korea), that’s one thing. When the women are already successful in the existing universities but women’s universities are set up as a pathway to push them out, that’s entirely different.</p>
  </li>
  <li>
    <p><a href="http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010">Engineers put Leonardo da Vinci’s bridge design to the test:
proposed bridge would have been the world’s longest at the time; new analysis shows it would have worked</a> (<a href="https://mathstodon.xyz/@11011110/103111919197001339"></a>, <a href="https://arstechnica.com/science/2019/10/testing-leonardo-da-vincis-bridge-his-design-was-stable-study-finds/">via</a>). I don’t think the link does justice to the scale of the thing. Da Vinci proposed a single stone arch across the Golden Horn in Istanbul, roughly 280m. That’s much longer than anything on the current <a href="https://en.wikipedia.org/wiki/List_of_longest_masonry_arch_bridge_spans">list of the world’s biggest stone arches</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Order_polytope">Order polytope</a> (<a href="https://mathstodon.xyz/@11011110/103115656245496084"></a>). New Wikipedia article on a convex polytope derived from any finite partial order as the points in a unit hypercube whose coordinate order is consistent with the partial order. Its vertices come from upper sets, its faces come from quotients, its facets come from covering pairs, and its volume comes from the number of linear extensions of the partial order. Coordinatewise min and max gives its points the structure of a continuous distributive lattice.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/socked-into-the-puppet-hole-on-wikipedia/">Socked into the puppet-hole on Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/103120959491226706"></a>). Journalist Noam Cohen’s Wikipedia biography is collateral damage in the war on slowking4, a prolific creator of Wikipedia articles whose problematic behavior (copying content from other sites, creating sockpuppet accounts to hide their identity, and reinstating articles from another user that were so riddled with errors that they were deleted en masse) has led to delete-on-sight actions.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Arithmetic_billiards">Arithmetic billiards</a> (<a href="https://mathstodon.xyz/@11011110/103129171790722530"></a>): using billiard ball trajectories to compute number-theoretic functions.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@olligobber/103066273568117018">olligober made a regex to match all multiples of 7</a>, but it was more than 10,000 characters long so grep couldn’t handle it. Applying <a href="https://en.wikipedia.org/wiki/Kleene%27s_algorithm">Kleene’s algorithm</a> to convert the natural DFA for this sort of problem into a regular expression blows its size up from polynomial in the modulus to exponential, but is this necessary? And if it is, what is the best possible base for the exponential?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/questions/338888/dividing-a-chocolate-bar-into-any-proportions">Dividing a chocolate bar into any proportions</a> (<a href="https://mathstodon.xyz/@11011110/103140599116798730"></a>). The bar has  squares, and you want to give each of  people an integer number of squares, but the integers are not known in advance. How to break the bar into few pieces so this will always be possible? Reid Hardison asked this months ago but Ilya Bogdanov answered with an efficient construction of the optimal partition much more recently.</p>
  </li>
  <li>
    <p><a href="http://www.personal.psu.edu/sot2/books/billiardsgeometry.pdf">Geometry and Billiards</a> (<a href="https://mathstodon.xyz/@11011110"></a>). An undergraduate-level textbook on the mathematics of reflection by Serge Tabachnikov.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/15/linkage.html"><span class="datestr">at November 15, 2019 10:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/162">TR19-162 |  The Random-Query Model and the Memory-Bounded Coupon Collector | 

	Ran Raz, 

	Wei Zhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study a new model of space-bounded computation, the {\it random-query} model. The model is based on a branching-program over input variables $x_1,\ldots,x_n$. In each time step, the branching program gets as an input a random index $i \in \{1,\ldots,n\}$, together with the input variable $x_i$ (rather than querying an input variable of its choice, as in the case of a standard (oblivious) branching program). We motivate the new model in various ways and study time-space tradeoff lower bounds in this model.

Our main technical result is a quadratic time-space lower bound for zero-error computations in the random-query model, for XOR, Majority and many other functions. More precisely, a zero-error computation is a computation that stops with high probability and such that conditioning on the event that the computation stopped, the output is correct with probability~1. We prove that for any Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$, with sensitivity $k$, any zero-error computation with time $T$ and space $S$, satisfies 
$T\cdot (S+\log n) \geq \Omega(n \cdot k)$. We note that the best time-space lower bounds for standard oblivious branching programs are only slightly super linear and improving these bounds is an important long-standing open problem.

To prove our results, we study a memory-bounded variant of the coupon-collector problem that seems to us of independent interest and to the best of our knowledge has not been studied before. We consider a zero-error version of the coupon-collector problem. In this problem, the coupon-collector could explicitly choose to stop when he/she is sure with zero-error that
all coupons have already been collected. We prove that any zero-error coupon-collector that stops with high probability in time $T$, and uses space $S$, satisfies $T\cdot (S+\log n) \geq \Omega(n^2)$, where $n$ is the number of different coupons.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/162"><span class="datestr">at November 15, 2019 06:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7574">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">Puzzles of modern machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-jetpack-markdown"><p>It is often said that "we don’t understand deep learning" but it is not as often clarified what is it exactly that we don’t understand. In this post I try to list some of the "puzzles" of modern machine learning, from a theoretical perspective. This list is neither comprehensive nor authoritative.  Indeed, I  only started looking at these issues last year, and am very much in the position of not yet fully understanding the questions, let alone potential answers.
On the other hand, at the rate ML research is going, a calendar year corresponds to about 10 "ML years"…</p>
<p>Machine learning  offers many opportunities for theorists; there are many more questions than answers, and it is clear that a better theoretical understanding of what makes certain training procedures work or fail is desperately needed. Moreover, recent advances in software frameworks made it much easier to test out intuitions and conjectures. While in the past running training procedures might have required a Ph.D in machine learning, recently the "barrier to entry" was reduced to first to undergraduates, then to high school students, and these days it’s so easy that even theoretical computer scientists can do it <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<p>To set the context for this discussion, I focus on the task of supervised learning. In this setting we are given a <em>training set</em> <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> examples of the form <img src="https://s0.wp.com/latex.php?latex=%28x_i%2Cy_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x_i,y_i)" class="latex" title="(x_i,y_i)" /> where <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i \in \mathbb{R}^d" class="latex" title="x_i \in \mathbb{R}^d" /> is some vector (think of it as the pixels of an image) and <img src="https://s0.wp.com/latex.php?latex=y_i+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i \in { \pm 1 }" class="latex" title="y_i \in { \pm 1 }" /> is some label (think of <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i" class="latex" title="y_i" /> as equaling <img src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+1" class="latex" title="+1" /> if <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> is the image of a dog and <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-1" class="latex" title="-1" /> if <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> is the image of a cat). The goal in supervised learning is to find a <em>classifier</em> <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> such that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x)=y" class="latex" title="f(x)=y" /> will hold for many future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)" class="latex" title="(x,y)" />.</p>
<p>The standard approach is to consider some parameterized family of classifiers, where for every vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" />  of parameters, we associate a classifier <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta+%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" class="latex" title="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" />. For example, we can fix a certain neural network architecture (depth, connections, activation functions, etc.) and let <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" />  be the vector of weights that characterizes every network in this architecture. People then run some optimizing algorithm such as stochastic gradient descent with the objective function set as finding the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" /> that minimizes a <em>loss function</em> <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" />. This loss function can be the fraction of labels that <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> gets wrong on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> or a more continuous loss that takes into account the confidence level or other parameters of <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> as well. By now this general approach has been successfully applied to a many classification tasks, in many cases achieving near-human to super-human performance.  In the rest of this post I want to discuss some of the questions that arise when trying to obtain a theoretical understanding of both the powers and the limitations of the above approach. I focus on deep learning, though there are still some open questions even for over-parameterized linear regression.</p>
<h2>The generalization puzzle</h2>
<p>The approach outlined above has been well known and  analyzed for many decades in the statistical learning literature. There are many cases where we can <em>prove</em> that a classifier obtained in this case has a small <em>generalization gap</em>, in the sense that if the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> was obtained by sampling <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> independent and identical samples from a distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, then the performance of a classifier <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> on new samples from <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> will be close to its performance on the training set.</p>
<p>Ultimately, these results all boil down to the Chernoff bound. Think of the random variables <img src="https://s0.wp.com/latex.php?latex=X_1%2C%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1,\ldots,X_n" class="latex" title="X_1,\ldots,X_n" /> where <img src="https://s0.wp.com/latex.php?latex=X_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_i=1" class="latex" title="X_i=1" /> if the classifier makes an error on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th training example. The Chernoff bound tells us that  probability that  that <img src="https://s0.wp.com/latex.php?latex=%5Csum+X_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum X_i" class="latex" title="\sum X_i" /> deviates by more than <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon n" class="latex" title="\epsilon n" /> from its expectation is something like <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cepsilon%5E2+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(-\epsilon^2 n)" class="latex" title="\exp(-\epsilon^2 n)" /> and so as long as the total number of classifiers is less than <img src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k" class="latex" title="2^k" /> for <img src="https://s0.wp.com/latex.php?latex=k+%3C+%5Cepsilon%5E2+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k &lt; \epsilon^2 n" class="latex" title="k &lt; \epsilon^2 n" />, we can use a union bound over all possible classifiers to argue that if we make a <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> fraction of errors on the training set, the probability we make an error on a new example is at  most <img src="https://s0.wp.com/latex.php?latex=p%2B%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p+\epsilon" class="latex" title="p+\epsilon" />. We can of course "bunch together" classifiers that behave similarly on our distribution, and so it is enough if there are at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{\epsilon^2 n}" class="latex" title="2^{\epsilon^2 n}" /> of these equivalence classes. Another approach is to add a "regularizing term" <img src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R(\theta)" class="latex" title="R(\theta)" /> to the objective function, which amounts to restricting attention to the set of all classifiers <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> such that <img src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29+%5Cleq+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R(\theta) \leq \mu" class="latex" title="R(\theta) \leq \mu" /> for some parameter <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" />. Again, as long as the number of equivalence classes in this set is less than <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{\epsilon^2 n}" class="latex" title="2^{\epsilon^2 n}" />, we can use this bound.</p>
<p>To a first approximation, the number of classifiers (even after "bunching together") is roughly exponential in the number <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> of parameters, and so these results tell us that as long as the number of <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> of parameters is smaller than the number of examples, we can expect to have a small <em>generalization gap</em> and can infer future performance (known as "test performance") from the performance on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> (known as "train performance").
Once the number of parameters <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> becomes close to or even bigger than the number of samples <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, we are in danger of "overfitting" where we could have excellent train performance but terrible test performance. Thus according to the classical statistical learning theory, the ideal number of parameters would be some number between <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> and the number of samples <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />, with the precise value governed by the so called "bias variance tradeoff".</p>
<p>This is a beautiful theory, but unfortunately the classical theorems yield vacous  results in the realm of modern machine learning, where we  often train networks with millions of parameters on a mere tens of thousands of examples. Moreover, <a href="https://arxiv.org/abs/1611.03530">Zhang et al</a> showed that this is not just a question of counting  parameters better. They showed that modern deep networks can in fact "overfit" and achieve 100% success on the training set even if you gave them random or arbitrary labels.</p>
<p>The results above in particular show that we can find classifiers that perform great on the training set but perform terribly on the future tests, as well as classifiers that perform terrible on the training set but pretty good on future test. Specifically, consider an architecture that has the capacity to fit <img src="https://s0.wp.com/latex.php?latex=20n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="20n" class="latex" title="20n" /> arbitrary labels, and suppose that we train it on a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> examples.  Then we can find a setting of parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that both fits the training set exactly (i.e.,  satisfies <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta(x)=y" class="latex" title="f_\theta(x)=y" /> for all <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)\in S" class="latex" title="(x,y)\in S" />) but also satisfies that the additional constraint that <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3D+-y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta(x)= -y" class="latex" title="f_\theta(x)= -y" /> (i.e., the negation of the label <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />) for every <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)" class="latex" title="(x,y)" /> in some additional set <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> of <img src="https://s0.wp.com/latex.php?latex=19m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="19m" class="latex" title="19m" /> pairs.
(The set <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is not part of the actual training set, but rather an "auxiliary set" that we simply use for the sake of constructing this counterexample; note that we can use <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> as means to generate the initial network which can then be fed into standard stochastic gradient descent on the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" />.) The network <img src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_\theta" class="latex" title="f_\theta" /> fits its training set perfectly, but since it effectively corresponds to training with 95% label noise, it will  perform  worse than even a coin toss.</p>
<p>In an analogous way, we can find parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that completely fail on the training set, but fit correctly the additional "auxiliary set" <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. This will correspond to the case of standard training with 5% label noise, which typically yields about 95% of the performance on the noiseless distribution.</p>
<p>The above insights  break the <strong>separation of concerns</strong> or separation of <strong>computational problems</strong> from <strong>algorithms</strong> which we theorists  like so much. Ideally, we would like to phrase the "machine learning problem" as a well defined optimization objective, such as finding, given a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" />, the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \in \mathbb{R}^m" class="latex" title="\theta \in \mathbb{R}^m" /> that mimimizes <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" />. Once phrased in this way, we can try to find with an algorithm that achieves this goal as efficiently as possible.</p>
<p>Unfortunately, modern machine learning does not currently lend itself to such a clean partition. In particular, since not all optima are equally good, we  <em>don’t  actually want</em> to solve the task of minimizing the loss function in a "black box" way. In fact, many of the ideas that make optimization faster such as accelaration, lower learning rate, second order methods and others, yield <em>worse</em>  generalization performance. Thus, while the objective function is somewhat correlated with generalization performance, it is neither necessary nor sufficient for it. This is a clear sign that we don’t really understand what makes machine learning work, and there is still much left to discover. I don’t know what machine learning textbooks in the 2030’s will contain, but my guess is that they would <em>not</em> prescribe running stochastic gradient descent on one of these loss functions. (Moritz Hardt counters that what we teach in ML today is not that far from the <a href="https://www.amazon.com/Pattern-Classification-Scene-Analysis-Richard/dp/0471223611">1973 book of Duda and Hart</a>, and that by some measures ML moved <em>slower</em> than other areas of CS.)</p>
<p>The <em>generalization puzzle</em> of machine learning can be phrased as the question of understanding what properties of procedures that map a training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> into a classifier <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> lead to good generalization performance with respect to certain distributions. In particular we would like to understand what are the properties of natural  natural distributions and stochastic gradient descent that make the latter into such a map.</p>
<h2>The computational puzzle</h2>
<p>Yet another puzzle in modern machine learning arises from the fact that we are able to find the minimum of <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> in the first place. A priori this is surprising since, apart from very special cases (e.g., linear regression with a square loss), the function <img src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cmapsto+L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta \mapsto L_S(\theta)" class="latex" title="\theta \mapsto L_S(\theta)" /> is in general <em>non convex</em>. Indeed, for almost any natural loss function, the problem of finding <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that minimizes <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> is NP hard.
However, if we look at the computational question in the context of the generalization puzzle above, it might not be as mysterious. As we have seen, the fact that the <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> we output is a global minimizer (or close to minimizer)  of <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\cdot)" class="latex" title="L_S(\cdot)" /> is in some sense accidental and by far not the  the most important property of <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" />. There are many minima of the loss function that  generalize badly, and many non minima that  generalize well.</p>
<p>So perhaps the right way to phrase the computational puzzle is as</p>
<blockquote>
<p><em>"How come that we are able to use stochastic gradient descent to find the vector <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\theta" class="latex" title="\theta" /> that is output by stochastic gradient descent."</em></p>
</blockquote>
<p>which when phrased like that, doesn’t seem like much of a puzzle after all.</p>
<h2>The off-distribution performance puzzle</h2>
<p>In the supervised learning problem, the training samples <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> are drawn from the same distribution as the final test sample. But in any applications of machine learning, classifiers are expected to perform on samples that arise from very different settings. The image that the camera of a self-driving car observes is not drawn from ImageNet, and yet it still needs to (and often can) detect whether not it is seeing a dog or a cat (at which point it will break or accelerate, depending on whether the programmer was a dog or cat lover). Another insight to this question comes from a recent work of <a href="https://arxiv.org/abs/1902.10811">Recht et al</a>. They generated a new set of images that is very similar to the original ImageNet test set, but not identical to it. One can think of it as generated from a distribution <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> that is close but not the same as the original distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> of ImageNet. They then checked how well do neural networks that were trained on the original ImageNet distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> perform on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" />. They saw that while these networks performed significantly worse on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> than they did on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, their performance on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" /> was <em>highly correlated</em> with their performance on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />. Hence doing better on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> did correspond to being better in a way that carried over to the (very closely related) <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D'" class="latex" title="D'" />. (However, the networks did perform worse on $D’$ so off-distribution performance is by no means a full success story.)</p>
<p>Coming up with a theory that can supply some predictions for learning in a way that is not as tied to the particular distribution is still very much open. I see it as somewhat akin to finding a theory for the performance of algorithms that is somewhere between average-case complexity (which is highly dependant on the distribution) and worst-case complexity (which does not depend on the distribution at all, but is not always achievable).</p>
<h2>The robustness puzzle</h2>
<p>If the previous puzzles were about understanding why deep networks are surprisingly good, the next one is about understanding why they are surprisingly bad. Images of physical objects have the property that if we modify them in some ways,  such as perturbing them in  a small number of pixels or by few shades or rotating by an angle, they still correspond to the same object. Deep neural networks do not seem to "pick up" on this property.
Indeed, there are many examples of how tiny perturbations can cause a neural net to think that one image is another, and people have even <a href="https://youtu.be/piYnd_wYlT8">printed a 3D turtle</a> that most modern systems recognize as a rifle. (See this <a href="https://adversarial-ml-tutorial.org/">excellent tutorial</a>, though note an "ML decade" has already passed since it was published). This "brittleness" of neural networks can be a significant concern when we deploy them in the wild. (Though perhaps mixing up turtles and rifles is not so bad: I can imagine  some people that would normally resist regulations to protect the environment but would support them if they confused turtles with guns..)
Perhaps one reason for this brittleness is that neural networks can be thought of as a way of embedding a set of examples in dimension <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> into dimension <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ell" class="latex" title="\ell" /> (where <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ell" class="latex" title="\ell" /> is the number of neurons in the penultimate layer) in a way that will make the positive examples be linearly separable from the negative examples. Amplifying small differences can help in achieving such a separation, even if it hurts robustness.</p>
<p>Recent works have attempted to rectify this, by using a variants of the loss function where <img src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_S(\theta)" class="latex" title="L_S(\theta)" /> corresponds to the maximum error under all possible such perturbations of the data. A priori you would think that while robust training might come at a computational cost, statistically it would be a "win win" with the resulting classifiers not only being more robust but also overall better at classifying. After all, we are providing the training procedure with the additional information (i.e., "updating its prior") that the label should be unchanged by certain transformations, which should be equivalent to supplying it with more data. Surprisingly, the robust classifiers currently perform <em>worse</em> than standard trained classifiers on unperturbed data. <a href="https://arxiv.org/abs/1905.02175">Ilyas et al</a> argued that this may be because even if humans ignore information encoded in, for example, whether the intensity level of a pixel is odd or even, it does not mean that this information is not predictive of the label. Suppose that (with no basis whatsoever – just as an example) cat owners are wealthier than dog owners and hence cat pictures tend to be taken with higher quality lenses. One could imagine that a neural network would pick up on that, and use some of the fine grained information in the pixels to help in classification. When we force such a network to be robust it would perform worse. Distill journal published <a href="https://distill.pub/2019/advex-bugs-discussion/">six discussion pieces</a> on the Ilyas et al paper. I like the idea of such "paper discussions" very much and hope it catches on in machine learning and beyond.</p>
<h2>The interpretability puzzle</h2>
<p>Deep neural networks are inspired by our brain, and it is tempting to try to understand their internal structure just like we try to understand the brain and see if it has a <a href="https://en.wikipedia.org/wiki/Grandmother_cell">"grandmother neuron"</a>. For example, we could try to see if there is a certain neuron (i.e., gate) in a neural network that "fires" only when it is fed images with certain high level features (or more generally find vectors that have large correlation with the state at a certain layer only when the image has some features). This also of practical importance, as we increasingly use classifiers to make decisions such as whether to approve or deny bail, whether to prescribe to a patient treatment A or B, or whether a car should steer left or right, and would like to understand what is the basis for such decisions. There are beautiful visualizations of neural networks’ decisions and internal structures , but given the robustness puzzle above, it is unclear if these really capture the decision process. After all, if we could change the classification from a cat to a dog by perturbing a tiny number of pixels, in what sense can we explain <em>why</em> the network made this decision or the other.</p>
<h2>The natural distributions puzzle</h2>
<p>Yet another puzzle (pointed out to me by Ilya Sutskever) is to understand what is it about "natural" distributions such as images, texts, etc.. that makes them so amenable to learning via neural networks, even though such networks can have a very hard time with learning even simple concepts such as parities.
Perhaps this is related to the "noise robustness" of natural concepts which is related to being correlated with low degree polynomials.
Another suggestion could be that at least for text etc.., human languages are implicitly designed to fit neural network. Perhaps on some other planets there are languages where the meaning of a sentence completely changes depending on whether it has an odd or an even number of letters…</p>
<h2>Summary</h2>
<p>The above are just a few puzzles that modern machine learning offers us. Not all of those might have answers in the form of mathematical theorems, or even well stated conjectures, but it is clear that there is still much to be discovered, and plenty of research opportunities for theoretical computer scientists. In this blog I focused on supervised learning, where at least the problem is well defined, but there are other areas of machine learning, such as transfer learning and generative modeling, where we don’t even yet know how to phrase the computational task, let alone prove that any particular procedure solves it. In several ways, the state of machine learning today seems to me as similar to the state of cryptography in the late 1970’s. After the discovery of public key cryptography, researchers has highly promising techniques and great intuitions, but still did not really understand even what security means, let alone how to achieve it. In the decades since, cryptography has turned from an art to a science, and I hope and believe the same will happen to machine learning.</p>
<p><strong>Acknowledgements:</strong> Thanks to Preetum Nakkiran, Aleksander Mądry, Ilya Sutskever and Moritz Hardt for helpful comments. (In particular, I dropped an interpretability experiment suggested in an earlier version of this post since Moritz informed me that several similar experiments have been done.) Needless to say, none of them is responsible for any of the speculations and/or errors above.</p>
</div>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/"><span class="datestr">at November 15, 2019 03:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/">Aarhus University is looking for excellent and visionary Assistant and Associate Professors to push the frontiers of Computer Science at Department of Computer Science, Aarhus University (apply by January 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for 2-4 Assistant Professors (Tenure Track – see full job description) and Associate Professors. We in particular wish to build competencies and groups within Machine Learning/Artificial Intelligence, Software Engineering and Systems Security, as well as Computer Graphics, and Computer Vision. In general, we encourage candidates within all areas of Computer Science to apply.</p>
<p>Website: <a href="https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en">https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en</a><br />
Email: kgronbak@cs.au.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/"><span class="datestr">at November 15, 2019 12:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/">Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br />
Email: simonsvisitorservices@berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/"><span class="datestr">at November 14, 2019 11:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3438">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/14/postdoc-and-full-time-researcher-positions-in-econcs-in-msr-nyc/">Postdoc and full-time researcher positions in EconCS in MSR-NYC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="m_1667087327619533166xxparagraph"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">Microsoft Research New York City </span>is<span style="color: black;"> hiring in Economics and Computation. We are looking for a <b>postdoc</b> and a <b>full-time researcher </b>(at all levels), with a start date in July 2020. Please encourage strong candidates to apply by December 1.</span></span></p>
<p class="m_1667087327619533166xxparagraph"><u></u><u></u><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">Research in Economics and Computation group at Microsoft Research NYC, and a closely affiliated group at MSR New England, spans a wide variety of topics at the intersection of economics and computation. Topics include algorithmic game theory, design of mechanisms and markets, crowdsourcing &amp; human computation, exploration and incentives, information aggregation &amp; elicitation, including polling and prediction markets, machine learning in economics, fair decision making, and social network theory.</span></span><span class="m_1667087327619533166xxeop"> </span><u></u><u></u></p>
<p class="m_1667087327619533166xxparagraph"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">More information on the position and how to apply: </span></span><a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcareers.microsoft.com%2Fus%2Fen%2Fjob%2F730319%2FPostdoctoral-Researcher-Economics&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042738452&amp;sdata=wfJWdroTIrZ3SUDtWtrQA9v5RT7slmBtL49qANSVWE0%3D&amp;reserved=0" target="_blank" rel="noopener"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: #0563c1;">https://careers.microsoft.com/us/en/job/730319/Postdoctoral-Researcher-Economics</span></span></a><span class="m_1667087327619533166xxnormaltextrun">,</span><span class="m_1667087327619533166xxscxw39414290"> </span><br />
<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcareers.microsoft.com%2Fus%2Fen%2Fjob%2F730201%2FSenior-Researcher&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042748405&amp;sdata=WfDLNbPEAK2zMNKPzprvdeZqgkM8S3cN1Hyc4SiXEns%3D&amp;reserved=0" target="_blank" rel="noopener"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: #0563c1;">https://careers.microsoft.com/us/en/job/730201/Senior-Researcher</span></span></a><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black;">,</span></span><span class="m_1667087327619533166xxscxw39414290"> </span><br />
<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcareers.microsoft.com%2Fus%2Fen%2Fjob%2F730202%2FPrincipal-Researcher&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042748405&amp;sdata=XyeCMmtrQNvfTA4NKIi443%2Bf%2FLRZkUcmRTBk4jNs37I%3D&amp;reserved=0" target="_blank" rel="noopener"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: #0563c1;">https://careers.microsoft.com/us/en/job/730202/Principal-Researcher</span></span></a><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black;">.</span></span><span class="m_1667087327619533166xxeop"> </span><u></u><u></u></p>
<p class="m_1667087327619533166xxparagraph"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">Our </span>Economics and Computation<span style="color: black;"> group:</span></span><span class="m_1667087327619533166xxscxw39414290"> </span><br />
<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fgroup%2Feconomics-and-computer-science%2F&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042758367&amp;sdata=mnx7R1%2BZcRjK91N5q9D8ERIMxnrf9%2F7L0KhzYcfTQAo%3D&amp;reserved=0" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/group/economics-and-computer-science/</a><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black;">  </span></span></p>
<p class="m_1667087327619533166xxparagraph"><u></u><u></u><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">And everyone at MSR-NYC:</span></span><span class="m_1667087327619533166xxscxw39414290"> </span><br />
<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Flab%2Fmicrosoft-research-new-york%2Fpeople%2F%3F&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042758367&amp;sdata=RzjqieRyRzM%2ByutWrAxFkbVxW1jrXVIVWbjEC1LrIO4%3D&amp;reserved=0" target="_blank" rel="noopener"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: #0563c1;">https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/people/?#</span></span></a><span class="m_1667087327619533166xxnormaltextrun"> .</span><span class="m_1667087327619533166xxscxw39414290"> </span></p>
<p class="m_1667087327619533166xxparagraph"><span class="m_1667087327619533166xxnormaltextrun"><span style="color: black; font-family: Calibri; font-size: small;">Please note that MSR-NYC is hiring in other areas too. To learn more, go to: </span></span><br />
<span class="m_1667087327619533166xxnormaltextrun"><u><span style="color: #0563c1;"><a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Flab%2Fmicrosoft-research-new-york%2Fopportunities%2F%3F&amp;data=02%7C01%7Cslivkins%40microsoft.com%7C2c31aa0ca14c4797105c08d764968b08%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637088470042768327&amp;sdata=R75zoEWIaMMGKXJrcKqAF4SFvN7tNmsDva2KzvPB1fM%3D&amp;reserved=0" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/opportunities/?#</a></span></u> .</span><span class="m_1667087327619533166xxeop"><span style="font-family: Lucida Console; font-size: xx-small;"> </span></span></p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2019/11/14/postdoc-and-full-time-researcher-positions-in-econcs-in-msr-nyc/"><span class="datestr">at November 14, 2019 02:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=378">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/11/13/tcs-talk-wednesday-november-20-jason-li-cmu/">TCS+ talk: Wednesday, November 20 — Jason Li, CMU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Jason Li</strong> from CMU will speak about “<em>The Karger-Stein Algorithm is Optimal for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-cut</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-cut problem, we are given an edge-weighted graph and want to find the least-weight set of edges whose deletion breaks the graph into <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" /> connected components. Algorithms due to Karger-Stein and Thorup showed how to find such a minimum <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-cut in time approximately <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2k-2%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="O(n^{2k-2})" class="latex" title="O(n^{2k-2})" />. The best lower bounds come from conjectures about the solvability of the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-clique problem and a reduction from <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-clique to <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-cut, and show that solving <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" />-cut is likely to require time <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%5Ek%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\Omega(n^k)" class="latex" title="\Omega(n^k)" />. Our recent results have given special-purpose algorithms that solve the problem in time <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1.98k+%2B+O%281%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n^{1.98k + O(1)}" class="latex" title="n^{1.98k + O(1)}" />, and ones that have better performance for special classes of graphs (e.g., for small integer weights).</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/11/13/tcs-talk-wednesday-november-20-jason-li-cmu/"><span class="datestr">at November 13, 2019 06:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/161">TR19-161 |  Hardness of Learning DNFs using Halfspaces | 

	Suprovat Ghoshal, 

	Rishi Saket</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The problem of learning $t$-term DNF formulas (for $t = O(1)$) has been studied extensively in the PAC model since its introduction by Valiant (STOC 1984). A $t$-term DNF can be efficiently learnt using a $t$-term DNF only if $t = 1$ i.e., when it is an AND, while even weakly learning a $2$-term DNF using a constant term DNF was shown to be NP-hard by Khot and Saket (FOCS 2008). On the other hand, Feldman et al. (FOCS 2009) showed the hardness of weakly learning a noisy AND using a halfspace -- the latter being a generalization of an AND, while Khot and Saket (STOC 2008) showed that an intersection of two halfspaces is hard to weakly learn using any function of constantly many halfspaces. The question of whether a $2$-term DNF is efficiently learnable using $2$ or constantly many halfspaces remained open.
	In this work we answer this question in the negative by showing the hardness of weakly learning a $2$-term DNF as well as a noisy AND using any function of a constant number of halfspaces. In particular we prove the following. 
	For any constants $\nu, \zeta &gt; 0$ and $\ell \in \mathbb{N}$, given a distribution over point-value pairs $\{0,1\}^n \times \{0,1\}$, it is NP-hard to decide whether,
YES Case. There is a $2$-term DNF that classifies all the points of the distribution, and an AND that classifies at least $1-\zeta$ fraction of the points correctly.
NO Case. Any boolean function depending on at most $\ell$ halfspaces classifies at most $1/2 + \nu$ fraction of the points of the distribution correctly.
	Our result generalizes and strengthens the previous best results mentioned above on the hardness of learning a $2$-term DNF, learning an intersection of two halfspaces, and learning a noisy AND.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/161"><span class="datestr">at November 13, 2019 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/13/phd-fellow-in-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/13/phd-fellow-in-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/">PhD fellow in Algorithms at University of Copenhagen (apply by January 5, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC) which also involves the IT University of Copenhagen. The aim is to attract top talent from around the world to an ambitious, creative, collaborative, and fun environment.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642</a><br />
Email: pbl@science.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/13/phd-fellow-in-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/"><span class="datestr">at November 13, 2019 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/13/postdoctoral-fellowships-of-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/13/postdoctoral-fellowships-of-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/">Postdoctoral Fellowship(s) of Algorithms at University of Copenhagen (apply by January 5, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms and Complexity Section. The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC). The aim is to attract top talent from around the world to an ambitious, creative, collaborative, and fun environment.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642</a><br />
Email: pbl@science.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/13/postdoctoral-fellowships-of-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/"><span class="datestr">at November 13, 2019 12:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=18435">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/">Gil’s Collegial Quantum Supremacy Skepticism FAQ</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.files.wordpress.com/2019/11/s53.png"><img src="https://gilkalai.files.wordpress.com/2019/11/s53.png?w=300&amp;h=152" alt="" width="300" class="size-medium wp-image-18585 aligncenter" height="152" /></a><span style="color: #ff0000;">The first 15 samples of Google’s  53 qubit  flagship quantum supremacy experiment!  </span></p>
<p>After the sensationally successful <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Superiority FAQ</a> and <a href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/">Boaz’s inferior classical inferiority FAQ</a> let me add my contribution, explaining my current skeptical view. (I was actually asked many of the questions below.) I also recommend <a href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/">Lipton and Regan’s post on the Google paper.</a></p>
<p><span style="color: #0000ff;">While much of the post will be familiar let me mention right away a new critique of the Google supremacy claim: One of the central claims in the Google experiment – that the fidelity (quality) of a complex circuit is very close to the product of the fidelity of its basic components –  qubit and gates, seems very improbable and this may shed serious doubts on the validity of the experiment and on its conclusions.</span></p>
<p>Before we start, a few links: For the amazing news on the threshold of random discrete structures, see <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">this post</a>.  Here is my <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">first post</a> on Google matter. Let me recommend the paper <a href="https://www.ams.org/journals/notices/201910/rnoti-p1618.pdf">From Operator Algebras to Complexity Theory and Back</a> by Thomas Vidick. It is about a problem by Boris Tsirelson (related to various deep mathematics) and about connections to quantum computation. And just fresh on the arXiv, <a href="https://arxiv.org/abs/1911.03748">Quantum speedups need structure</a> by Nathan Keller, Ohad Klein, resolving the Aaronson-Ambainis Conjecture. (Update: here is <a href="https://www.scottaaronson.com/blog/?p=4414">a blog post</a> on the Shtetl-Optimized.) Congrats to Nathan and Ohad!</p>
<p><span style="color: #993300;">And now, lets start.</span></p>
<p><span style="color: #0000ff;">So what is quantum supremacy? And what other things do we need to know in order to understand the claims regarding quantum computers?</span></p>
<p><strong>Quantum supremacy</strong>  is the ability of quantum computers to demonstrate computations that classical computers cannot demonstrate. (Or that it is very very hard for classical computers  to demonstrate.)</p>
<p><strong>Quantum error correcting codes</strong> are certain quantum gadgets that a quantum computer needs to create that will be used as building blocks for larger quantum computers.</p>
<p><strong>A sampling task</strong> is  a task where the computer (quantum or classic) produces samples from a certain probability distribution <strong>D</strong>. Each sample is 0-1 vector of length n.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/john_martinis-300x256.jpg"><img src="https://gilkalai.files.wordpress.com/2019/11/john_martinis-300x256.jpg?w=640" alt="" class="alignnone size-full wp-image-18579" /></a></p>
<p><span style="color: #ff0000;"><strong>John Martinis</strong></span></p>
<h2>Google</h2>
<p><span style="color: #0000ff;">What did the Google team do?</span></p>
<p>The Google team produced a sample of a few millions 0-1 vectors of length 53 which is based on a certain “ideal” probability distribution <strong>D</strong>. They made two crucial claims regarding their sample</p>
<p>A) The statistical test for how close their sample is to the ideal distribution <strong>D</strong> will give a result above t=1/10,000</p>
<p>B) Producing a sample with similar statistical property will require 10,000 years on a supercomputer.</p>
<p>The probability distribution <strong>D</strong> depends on a quantum computation process (or by the technical jargon, a <strong>quantum circuit</strong>) denoted later by C.</p>
<p><span style="color: #0000ff;">What is the meaning of the statistical statement in part A)?</span></p>
<p>Google’s quantum computers (like any other current quantum computers) are very “noisy” so what the computer is producing are not samples from <strong>D</strong> but rather a noisy version which could roughly be described as follows: a fraction <em>t</em> of the samples are from <strong>D</strong> and a fraction <em>(1-t)</em> of the samples are from a uniform distribution. The statistical test allows to estimate the value of <em>t</em> which is referred to as the fidelity.</p>
<p><span style="color: #0000ff;">Could they directly verify claims A) and B) ?</span></p>
<p>No, it was only possible to give indirect evidence for both these claims.</p>
<p><span style="color: #0000ff;">What is the logic of Google’s quantum supremacy argument?</span></p>
<p>For claim A) regarding the success of the statistical test on the sample they have two arguments:</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/gsformula.png"><img src="https://gilkalai.files.wordpress.com/2019/11/gsformula.png?w=300&amp;h=44" alt="" width="300" class="alignnone size-medium wp-image-18590" height="44" /></a></p>
<ol>
<li>Analysis based on the fidelity of the components of the quantum computer – qubits and gates (see formula (77) above),</li>
<li>Experiments that support the analysis in the regime where they can be tested by a classical computer.</li>
</ol>
<p>According to the paper  the fidelity of entire circuits agrees perfectly with the prediction of the simple mathematical formula (77) with deviation under 10-20 percents. There are several reported experiments in the classically tractable regime including on simplified circuits (that are easier to simulate on classical computers) to support the assumption that the prediction given by formula (77) for the fidelity applies to the 53-qubit circuit in the supremacy regime.</p>
<p>For claim B) For the classical difficulty they rely on:</p>
<ol>
<li>Extrapolation from the running time of a specific algorithm that they use.</li>
<li>Computational complexity support for the assertion that the task they consider is asymptotically difficult.</li>
</ol>
<p><span style="color: #0000ff;">What are the weak points in this logic?</span></p>
<p>A main weakness (which is crucial in my mind) of the experiment is that the experimental support from the regime where the experiments can be tested by a classical computer is too sparse. Much more could have been done and should have been done.</p>
<h2><a href="https://gilkalai.files.wordpress.com/2019/11/supremacy-figure.png"><img src="https://gilkalai.files.wordpress.com/2019/11/supremacy-figure.png?w=300&amp;h=277" alt="" width="300" class="alignnone size-medium wp-image-18586" height="277" /></a></h2>
<p><span style="color: #ff0000;">In my opinion, a major weakness of the Google experiment is that the support from experiments in the classically tractable regime (blue in the picture) is much too sparse and is unconvincing.</span></p>
<p>Another weakness is that the arguments for classical difficulty were mainly based on the performance of a specific algorithm.</p>
<p>Sources: The link to the <a href="https://www.nature.com/articles/s41586-019-1666-5">Google paper in </a><em><a href="https://www.nature.com/articles/s41586-019-1666-5">Nature.</a> </em>A <a href="https://youtu.be/FklMpRiTeTA">videotaped lecture</a> by John Martinis at Caltech.</p>
<h2>Assessment</h2>
<p><span style="color: #0000ff;">What is your assessment of the Google claims, Gil?</span></p>
<p>I think that the claims are incorrect. Specifically, I find the evidence for  the claim “the statistical test applied to the 53-qubit  sample will give a result above 1/10,000 too weak and I expect that this claim and other related claims in the paper will not stand after a closer scrutiny and further experiments in the classically tractable regime. I also doubt the perfect proximity between predictions based on the 1- and 2- qubit fidelity and the circuit fidelity.</p>
<p>The Google experiment represents a very large leap in several aspects of human ability to control noisy quantum systems and accepting their claims  requires very careful evaluation of the experiments and, of course, successful replications.</p>
<h2>The “most amazing thing” – is it real?</h2>
<p><span style="color: #0000ff;">Do you want to tell us more about Formula (77)?</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/77.png"><img src="https://gilkalai.files.wordpress.com/2019/11/77.png?w=300&amp;h=222" alt="" width="300" class="size-medium wp-image-18615 aligncenter" height="222" /></a></p>
<p style="text-align: center;"><span style="color: #ff0000;">Formula (77)</span></p>
<p>Yes, thank you. Here again is Formula (77) and its explanation in the paper. The fact that the fidelity of entire circuits agrees with the prediction of the simple mathematical Formula (77) is “most amazing” <a href="https://youtu.be/FklMpRiTeTA?t=2455">according to John Martinis (videotaped lecture at Caltech).</a>  Indeed the deviation according to the paper is at most 10-20 percents.  This perfect agreement can be seen in various other parts of the paper. The authors’ interpretation of this finding is that it validates the digital error model and shows that there are no new mechanisms for errors.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/77-explained.png"><img src="https://gilkalai.files.wordpress.com/2019/11/77-explained.png?w=640&amp;h=400" alt="" width="640" class="alignnone size-large wp-image-18641" height="400" /></a></p>
<p><span style="color: #ff0000;">John explains the significance of Formula (77) at Caltech. Amazing big surprises are often false.</span></p>
<p><span style="color: #0000ff;">And what do you think about it, Gil</span></p>
<p>I completely agree that this is most amazing and, as a matter of fact, there are reasons to consider the  predictions based on Formula (77) as <span style="color: #ff0000;"><strong>too good to be true</strong></span> even if qubit and gates fidelity account for all the errors in the system. The issue is that Formula (77) itself is very sensitive to noise. The formula estimates the fidelity as the product of hundreds of contributions from individual qubits and gates. Fairly small errors in estimating the individual terms can have large accumulative effect, well beyond the 10%-20% margin.</p>
<p>Anyway, this matter deserves further study.</p>
<p><span style="color: #0000ff;">Why?</span></p>
<p>Because this is considered by the authors as a major discovery and while looking skeptically at the Google papers this appears to be an orthogonal “miracle” to the main supremacy claim.</p>
<h2>How to proceed</h2>
<p><span style="color: #0000ff;">Let’s go back to your overall assessment. What could change your mind, Gil?</span></p>
<p>Here goes:</p>
<h3>Verification</h3>
<p>A) An independent verification of the statistical tests outcomes for the experiments in the regime where the Google team classically computed the probabilities. This looks to me like a crucial step in a verification of such an important experiment.</p>
<p>A more difficult verification that I’d also regard as necessary at a later stage would be to independently check the probability distributions for the circuits given by the Google computation.</p>
<h3>Further analysis and crucial improvements</h3>
<p>B) Experiments with the quantum computers giving sufficiently many samples to understand the noisy probability distribution for circuits in the 10-25 qubit range.  See <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">my first post</a>, <a href="https://www.scottaaronson.com/blog/?p=4372#comment-1822400" rel="nofollow ugc">this comment by Ryan O’Donnell</a>, and this <a href="https://www.scottaaronson.com/blog/?p=4317#comment-1819915">earlier one</a>.  We need to understand the noisy probability distribution produced by the Google quantum computer, the actual fidelity, and the quality of the statistical tests used by the Google team.</p>
<h3>Replications</h3>
<p>C) Experiments in the 10-30 qubit range on the IBM (and other) quantum computers.  It is quite possible that experimenting of this kind with random quantum circuits was already carried out.</p>
<p>D) Since improved classical algorithms were found by the IBM team (but analyzing the 53 qubits samples still seems practically beyond reach).   Google can produce samples for 41-49 qubits for which IBM (or others) can compute the probabilities quickly and test Google’s prediction for the fidelity.</p>
<h3>Follow-ups</h3>
<p>E) Success in demonstrating distance-3 and distance-5 surface codes and other quantum error-correcting codes.</p>
<p><span style="color: #0000ff;">So what precisely will convince you and what is the time-schedule that you expect for matters to be clarified?</span></p>
<p>A successful and convincing combination of<strong> three or more</strong> from A), B), C), D) or E) will go a long way to convince me. The verification part A) is important, and I don’t expect problems there, I expect that the Google claims will be verified and I consider it as very important that the data will be public and that various groups will verify the claims. This may take several months and certainly it should take less than a year.</p>
<p>At present, I expect parts B)-D) will not support Google’s supremacy claims. So outcomes of experiments in the next couple of years both by the Google group and by other groups will be crucial. One direction that I <em>do not</em> regard, at present, as useful for strengthening the quantum supremacy claims is increasing the number of qubits of the quantum computer.</p>
<p><span style="color: #0000ff;">What is required for the (easy) verification stage?</span></p>
<p>(1) Right now the raw samples of Google’s sampling experiments are public. There are altogether 300 files with samples.</p>
<p>(2) For every circuit that they experiment, the Google team also plans to upload  the 2^n probabilities that they obtained by the Feynman-Schrodinger algorithm.  This will  allow verifying their statistical tests, making subsequent analysis, and for other researchers to test other algorithms for computing the same probabilities.</p>
<p>(3) A convenient form of the data from (2) is a file that will give for every experiment the probabilities that the Google team computed for the samples. (For a large <em>n</em> those are much smaller files.)</p>
<p>(4) For each of the 300 experiments the estimated fidelity that formula (77) gave and the contribution of each qubit and gate to the RHS of (77).</p>
<p><span style="color: #0000ff;">Do you plan to take part yourself?</span></p>
<p>I plan to get involved myself with the “easy” verification and analysis of the “raw data” once it will become available. I do expect that the statistical tests will agree with the assertions in the Google paper, and at the same time, as I said,  I think it is important that this and other aspects of the experiments will be double checked and triple checked. This basic data already allows interesting analysis and indeed Google’s supplementary paper describes such analysis (that the Google people kindly pointed me to) on how the data fits the theory and on their statistical tests. See Figures S32-S36, table V and associated materials around pages 37-40.</p>
<h2>IBM</h2>
<p><span style="color: #0000ff;">What did the IBM rebuttal paper show?</span></p>
<p>Recall that the Google claim is based on two assertions:</p>
<p>A) The statistical test applied to the sample will give a result above 1/10,000</p>
<p>B) that producing a sample with similar statistical property will require 10,000 years on a supercomputer.</p>
<p>The IBM team described a different algorithm (on an even stronger current supercomputer) that would take only 2.5 days rather than 10,000 years.</p>
<p><span style="color: #0000ff;">Can the 2.5 days be further reduced?</span></p>
<p>As far as I can see the IBM claim is about a full computation of all the 2^53 probabilities. It is reasonable to think that producing a sample (or even a complete list of 2^53 probabilities) with fidelity t reduces the classical effort linearly with t. (This is the claim about the specific algorithm used by the Google team.) If this holds for the IBM algorithm then the 2.5 days will go down to less than a minute. (This will still be a notable “quantum speed up” in terms of the number of operations.) I don’t have an opinion as to whether  we should expect considerably better than IBM’s classical algorithms for computing the exact probabilities.</p>
<p><span style="color: #0000ff;">But lack of enthusiasm and skepticism of researchers from IBM about the Google paper appears to go beyond this particular point of the 2.5 computing days. Do you think that the objection by IBM people is motivated by fierce competition or envy?</span></p>
<p>No, I tend to think that there is a genuine interest by researchers who question the Google paper to understand the scientific matter, and carefully, critically and skeptically examine  Google’s claims. Maybe Google’s claims might seem to some other researchers who are working on quantum computers with superconducting qubits as remote from their own experimental experience, and this may give a strong reason for skepticism. It is also possible that in time people in IBM and elsewhere will change their mind and will become  more enthusiastic about the Google results.</p>
<p>IBM <a href="https://arxiv.org/abs/1910.09534">paper</a> and <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">blog post</a> responding to Google’s announcement.</p>
<h2>Noise</h2>
<p><span style="color: #0000ff;">Tell us a little more about noise</span></p>
<p>Here is a nice toy model (which I think is quite realistic) to understand what the noise is.  Suppose that you ran a circuit C on your quantum computer with n qubits and the ideal probability distribution is <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" />. The fidelity <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> noisy version of <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" /> will be <img src="https://s0.wp.com/latex.php?latex=N_C%28x%29%3Dt+D_C%28x%29+%2B+%281-t%29S_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_C(x)=t D_C(x) + (1-t)S_C" class="latex" title="N_C(x)=t D_C(x) + (1-t)S_C" />. And here <img src="https://s0.wp.com/latex.php?latex=S_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_C" class="latex" title="S_C" /> is the average (or weighted average) of values of <img src="https://s0.wp.com/latex.php?latex=D_C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C(y)" class="latex" title="D_C(y)" /> where y is a vector in the neighborhood of x.</p>
<p>Here is a concrete version: We look at the expected value of <img src="https://s0.wp.com/latex.php?latex=D_C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C(y)" class="latex" title="D_C(y)" /> where y is a new vector and <img src="https://s0.wp.com/latex.php?latex=y_i%3Dx_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i=x_i" class="latex" title="y_i=x_i" /> with probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> <img src="https://s0.wp.com/latex.php?latex=y_i%3D%281-x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i=(1-x_i)" class="latex" title="y_i=(1-x_i)" /> with probability <img src="https://s0.wp.com/latex.php?latex=%281-p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1-p)" class="latex" title="(1-p)" /> <span style="color: #008000;"><strong>independently</strong></span>. We choose <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> so that <img src="https://s0.wp.com/latex.php?latex=p%5En%3Dt&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p^n=t" class="latex" title="p^n=t" />. There are cases where positive correlation of errors for 2-qubit gates lead to correlated errors. (This is especially relevant in the context of quantum error correction.) To add this effect to the toy noise model replace the  word <span style="color: #008000;">independently</span> by “<span style="color: #008000;">positively correlated</span>“.</p>
<h2>Your argument, Gil</h2>
<p><span style="color: #0000ff;">Why do you think that quantum supremacy is not possible at all?</span></p>
<p>The gist of my argument against the possibility of achieving quantum supremacy by Noisy intermediate scale quantum computers is quite simple: “<em>NISQ</em> <em>devices can’t outperform classical computers, for the simple reason that they <span style="color: #ff0000;">are</span> primitive classical computers.” </em></p>
<p>(Note the similarity to Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=4342">critique</a> on a <a href="https://www.nature.com/articles/s41586-019-1557-9">paper</a> published by <em>Nature</em> claiming implementation of a Shor-like algorithm on a classical device called “p-bits”.  Scott offered a very quick debunking: “ ‘<em>p-bit’ devices can’t scalably outperform classical computers, for the simple reason that they <span style="color: #ff0000;">are</span> classical computers.)</em></p>
<p><span style="color: #0000ff;">If Google’s claim are correct – does it falsify your argument?</span></p>
<p>Yes! I predict that probability distributions described (robustly) by a noisy quantum circuit represent a polynomial time algorithm in terms of the description of the circuit. And by a polynomial time algorithm I assume small degree and modest constants. The Google claim, if true, appears to falsify this prediction. (And for this you do not need quantum supremacy in the strongest form of the term.)</p>
<p><span style="color: #0000ff;">But is there no way that Google’s huge (or at least large) computational advantage coexists with what you say?</span></p>
<p>There is an issue that I myself am not completely sure about regarding the possibility of chaotic behavior of quantum computers. Here is the classical analog: If you have <em>n</em> bits of memory inside a (classical) computer of <em>m</em> bits and ask about the complexity of the evolution on the <em>n</em> bits which may be chaotic. Of course, we cannot expect that this chaotic computer can lead to a computation that requires thousands of years in a super computer. But can it lead to a robust computation which is superpolynomial  in <em>n</em> (but polynomial in <em>m</em>)?</p>
<p>I don’t know the general answer but, in any case, I don’t think that it changes the situation here. If the Google claims  stand, I would regard it as a very strong argument against my theory. (Even if the noisy distributions themselves are not robust.) In any case, the question if the samples in the experiments represent robust distributions or are chaotic could and should be tested. (I discussed it in <a href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/">this post</a>.)</p>
<p><span style="color: #0000ff;">If Google’s claims do not stand, will it confirm or give a strong support to your position that quantum supremacy and quantum error correction are impossible?</span></p>
<p>Failure of the Google claim will mainly support the position that quantum supremacy and quantum error correction require substantial improvement of the quality of qubit and gates. It would give a noteworthy support to my position (and probably would draw some attention to it) but I would not regard it as a decisive support. Let me mention that various specific predictions that I made can be tested in the Google, IBM and other systems.</p>
<p><span style="color: #0000ff;">OK, so why <em>do you think</em> that the quality of qubits and gates <em>cannot</em> be improved?</span></p>
<p><span style="color: #ff0000;">Yes, this is the crucial point. One argument (that I already mentioned) for thinking that there is a barrier for the quality of gates and qubits is computational theoretic. Computationally speaking NISQ devices are primitive classical computing devices and this gives a strong reason to think that it will not be possible to reduce the error rate to the level allowing computational supremacy. But there is an additional argument: for a wide range of lower levels of noise, reducing the noise will have the effect of making the system more chaotic. So the first argument tells us that there is a small range of error-rates that we can hope to  achieve and the second argument tells us that for a large range  of lower error-rates all we gain is chaos!</span></p>
<p>Links to my work: <a href="https://gilkalai.files.wordpress.com/2019/09/main-pr.pdf">Three puzzles on mathematics computations, and games,</a> Proc. ICM2018; <a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers</a>, To appear in Itamar Pitowsky’s memorial volume; <a href="https://www.ams.org/journals/notices/201605/rnoti-p508.pdf">The quantum computer puzzle</a>, Notices AMS, May 2016</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">Slides</a> from my 2019 CERN lecture. My ICM 2018 <a href="https://www.youtube.com/watch?v=oR-ufBz13Eg">videotaped lecture</a>.</p>
<h2>Correlated errors and quantum error correction</h2>
<p><span style="color: #0000ff;">People mainly refer to your conjectures about correlated errors.</span></p>
<p>Yes, this reflects my work between 2005-2013 (and was a central issue in <a href="http://rjlipton.wordpress.com/2012/01/30/perpetual-motion-of-the-21st-century/">my debate</a> <a href="http://rjlipton.wordpress.com/2012/10/03/quantum-supremacy-or-classical-control/">with Aram Harrow</a>) and I think it is an important part of the overall picture. But this issue is different than my argument against quantum computers which represents my work between 2014-2019.  I think that my earlier work on error correlation is a key (or a starting point) to the question: What do we learn from failure of quantum computers on general properties of quantum noise. Indeed there are various consequences; some of them are fairly intuitive; some of them are counter-intuitive, and some of them are both. The basic intuition is that once your computation really makes use of a large portion of the Hilbert space, so will the error!</p>
<p>The major challenge is to put this intuition into formal mathematical terms and to relate it to the mathematics and physics of quantum physics.</p>
<p>I made a similar idea in  a comment to Dave Bacon in 2006 when I wrote “I believe that you may be able to approximate a rank-one matrix up to a rank-one error. I do not believe that you will be able to approximate an arbitrary matrix up to a rank one matrix.” to which Dave replied “I will never look at rank one matrices the same <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" alt="😉" style="height: 1em;" class="wp-smiley" />”. Dave Bacon is among the authors of the new Google paper.</p>
<p><span style="color: #0000ff;">What is the connection between the ability to achieve quantum supremacy and the ability to achieve quantum error-correction?</span></p>
<p>One of the main claims in my recent works is that quantum supremacy is an easier task compared to creating good quality error-correcting codes. For the attempted experiments by Google, we see  a clear demonstration that achieving good quality quantum error correction is harder than demonstrating quantum supremacy. Low fidelity circuits that Google claims to achieve are far from sufficient for quantum error-correction. The other claim in my argument is that quantum supremacy cannot be achieved without quantum error correction (and, in particular, not at all in the NISQ regime) and this claim is, of course, challenged by the Google claims.</p>
<p><span style="color: #0000ff;">You claim that without quantum error correction to start with we cannot reach quantum supremacy. But maybe John Martinis’ experimental methods have some seeds of quantum error correction inside them?</span></p>
<p>Maybe <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" />  See this 2017 cartoon from <a href="https://gilkalai.wordpress.com/2017/10/16/if-quantum-computers-are-not-possible-why-are-classical-computers-possible/">this post</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/sulam35c2.png"><img src="https://gilkalai.files.wordpress.com/2019/11/sulam35c2.png?w=300&amp;h=223" alt="" width="300" class="alignnone size-medium wp-image-18525" height="223" /></a></p>
<p>(Here is a <a href="https://youtu.be/h6p_ZeMqGIU">nice overview video from 2014 about my stance and earlier work</a>.)</p>
<h2>The Google experiment: concerns and attacks</h2>
<p><span style="color: #0000ff;">Beside the critique on experimental evidence that could be tested did you find some concrete issues with the Google experiment?</span></p>
<p>Perhaps even too many <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /> . In the first post and comments I raised quite a few objections. Some of them are relevant and some of them turned out to be irrelevant or incorrect. Anyway, here, taken from my first post, are some of my concerns and attempted attacks on the Google experiment:</p>
<ol>
<li>Not enough experiments with full histograms; not enough experiments in the regime where they can be directly tested</li>
<li>Classical supremacy argument is overstated and is based on the performance of a specific algorithm</li>
<li>Error correlation may falsify the Google noise model</li>
<li>Low degree Fourier coefficients may fool the statistical test</li>
<li>(Motivated by <a href="https://www.scottaaronson.com/blog/?p=4317#comment-1819915">a comment</a> by Ryan.) It is easier to optimize toward the new statistical test “linear cross-ratio entropy” compared to the old logarithmic one.</li>
<li>“System calibration” may reflect an optimization towards the specific required circuit.</li>
<li>The interpolation argument is unjustified (because of the calibration issue).</li>
</ol>
<p><span style="color: #0000ff;">We talked about items 1 and 2 what about 3-5. In particular, are correlated errors relevant to the Google experiment?</span></p>
<p>No! (As far as I can see.) Correlated errors mean that in the smoothing the flipped coordinates are positively correlated.  But for the random circuit and the (Porter Thomas) distribution <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" />  this makes no difference!</p>
<p>As for item 4., it turns out (and this was essentially known by the <a href="https://arxiv.org/abs/1810.03176">work of Gao and Duan</a>) that in the case of random circuits (unlike the case of Boson Sampling) there is no low degree coefficients to fool the statistical test.</p>
<p>As for item 5., the answer is “nice observation, but so what?” (Let me note that the supplementary paper of the Google team compares and analyzes the linear and logarithmic statistical measures.)</p>
<p><span style="color: #0000ff;">What about the calibration? You got a little overworked about it, no?</span></p>
<p>In almost every scientific experiment there could be concerns that there will be some sort of biased data selection toward the hoped-for result.</p>
<p>Based on the description of the calibration method I got the impression that part of the calibration/verification process (“system calibration”) was carried out towards the experimental outcome for a specific circuit, and that this does not improve the fidelity as the authors thought but rather mistakenly tweaked the experimental outcomes toward a specific probability distribution. This type of calibration would be a major (yet innocent) flaw in the experiment. However, this possibility was excluded by a clear assertion of the researchers regarding the nature of the calibration process, and also by a more careful reading of the paper itself by Peter Shor and Greg Kuperberg. I certainly was, for a short while, way overconfident about this theory.</p>
<p>One nice (and totally familiar) observation is that a blind experiment can largely eliminate the concern of biased data selection.</p>
<h2>So how do you feel, Gil?</h2>
<p><span style="color: #0000ff;">When did you hear about the Google claim?</span></p>
<p>There were certainly some reasons to think that Google’s quantum supremacy was coming for example a quanta magazine article by Kevin Hartnett entitled <a href="https://www.quantamagazine.org/quantum-supremacy-is-coming-heres-what-you-should-know-20190718/">Quantum Supremacy Is Coming: Here’s What You Should Know</a> and another article about Neven’s double exponential law. Also Scott Aaronson gave some hints about it.</p>
<p>On September 18, I met Thomas Vidick in a very nice conference of the Israeli and US academies on the future of computer science (it was mentioned in <a href="https://gilkalai.wordpress.com/2019/09/04/computer-science-and-its-impact-on-our-future/">this post</a>, links to all videos will be added here, <a href="https://www.youtube.com/watch?v=Ozo5avBdlLk">Naftali Tishby’s lecture</a> is especially recommended.) Thomas told me about the new expected Google paper. Later that day I got involved in an <a href="https://www.facebook.com/john.preskill/posts/897725643486?comment_id=897734281176">amusing Facebook discussion</a> about related matters. (See Barry Simon’s first comment to Preskill’s post and the subsequent 15 comments.)</p>
<p>When I introduced Thomas to <a href="https://gilkalai.wordpress.com/2008/09/03/the-prisonner-dilemma-sympathy-and-yaaris-challenge/">Menachem Yaari</a> (who was the president of the Israeli Academy), describing the former as a young superstar in quantum computation, Menachem’s reaction was: “but you do not believe in quantum computers.” I replied that I believe it is a fascinating intellectual area, and that perhaps I am even wrong about them being infeasible. Thomas said: “our area needs more people like Gil.” (!)</p>
<p><span style="color: #0000ff;">What about Scott?</span></p>
<p>Scott and I have been on friendly terms for many years and share a lot of interests and values. We are deeply divided regarding quantum computers and, naturally, I think that I am right and that Scott is wrong. In the context of the Google paper Scott’s references to me and my stance were <a href="https://www.scottaaronson.com/blog/?p=4312#comment-1819487">peculiar</a> and even a <a href="https://www.scottaaronson.com/blog/?p=4312#comment-1819821">little hostile</a> which was especially strange since at that time I did not have access to the paper and Scott was the referee of the paper.</p>
<p><span style="color: #0000ff;">Gil, how do you vision a situation where you are proven wrong?</span></p>
<p>If my theory of quantum computation being derailed by noise inherent in quantum gates is proven wrong, then physicists will say that I am a mathematician and mathematicians will say that I am a combinatorialist.</p>
<p><span style="color: #0000ff;">and how do you vision  a situation where you are proven right?</span><span id="more-18435"></span></p>
<p>If my theory of quantum computation being derailed by noise inherent in quantum gates is  proven successful, then physicists will say that I am a mathematician and mathematicians will say that I am a combinatorialist. <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<p><span style="color: #0000ff;">And what would you say if your theory  prevails?</span></p>
<p>Where I <em>have seen further</em> than others, it is because I stood on Peter Shor’s shoulders and looked at the opposite direction. <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<h2>Topological</h2>
<p><span style="color: #0000ff;">One last thing, Gil. Nick Read just</span> <a href="https://www.scottaaronson.com/blog/?p=4400#comment-1823614">commented</a> <span style="color: #0000ff;">that experimental evidence is gradually pointing towards you being false on the matter of topological quantum qubits.</span></p>
<p>Nick is a great guy and topological quantum computing is a great topic. The general situation is quite simple and it applies to topological quantum computing like any other form of quantum computing. The way I see it,  gradual experimental progress will hit a barrier and non-gradual experimental progress will be falsified.</p>
<p>(See this 2014 <a href="https://youtu.be/L5gSZsezhoQ">videotaped lecture of mine on topological quantum computing,</a> and also Section 3.5 of <a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers.</a>)</p>
<h2>Appendix: The probability distribution and the statistical tests</h2>
<p><span style="color: #0000ff;">Let’s have an Appendix question. Can you try to briefly describe the probability distribution and the statistical test used in the Google paper?</span></p>
<p>Let me try. We start with a probability distribution described by the density function <img src="https://s0.wp.com/latex.php?latex=z+e%5E%7B-z%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z e^{-z}" class="latex" title="z e^{-z}" /> supported on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb R_+" class="latex" title="\mathbb R_+" />. Now we consider our set <em>X</em> of <em>0-1</em> vectors of length <em>n</em>.</p>
<p>We draw a random probability distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x)" class="latex" title="p(x)" /> on <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" />. <img src="https://s0.wp.com/latex.php?latex=p%28x%29%3Dq%28x%29%2F2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x)=q(x)/2^n" class="latex" title="p(x)=q(x)/2^n" /> and the value of <img src="https://s0.wp.com/latex.php?latex=q%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q(x)" class="latex" title="q(x)" /> is drawn at random from the probability distribution <img src="https://s0.wp.com/latex.php?latex=z+e%5E%7B-z%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z e^{-z}" class="latex" title="z e^{-z}" />.  (A very slight normalization may still be needed.) A probability distribution of this kind on <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}^n" class="latex" title="\{0,1\}^n" /> is called a Porter-Thomas distribution.</p>
<p>A random quantum circuit <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> leads to a (deterministic) probability distribution <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" /> of this kind. A classical computer can compute the probabilities based on the description of the quantum circuits but this becomes increasingly hard with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />. A quantum computer can easily sample  <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{0,1\}^n" class="latex" title="x \in \{0,1\}^n" /> according to <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" />.</p>
<p>We are given <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> samples <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cdots%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1,\dots, x_t" class="latex" title="x_1,\dots, x_t" /> that our noisy quantum computer drew.</p>
<p>Our research hypothesis is that the samples are drawn from <img src="https://s0.wp.com/latex.php?latex=%5Crho+D_C+%2B%281-%5Crho%29U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho D_C +(1-\rho)U" class="latex" title="\rho D_C +(1-\rho)U" /> where <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" /> is a uniform distribution. <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> is called the fidelity. The null hypothesis is that the samples were drawn uniformly at random. (There is also a finer description of the noisy distribution with a Gaussian low order term depending on <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />. (This can be seen already from the noise toy model above but I will not discuss it here.)</p>
<p>The main test used in the Google paper is an estimator for <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B2%5En%7D%7Bt%7D+%5Csum_%7Bi%3D1%7D%5Et+D_C%28x_i%29+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac {2^n}{t} \sum_{i=1}^t D_C(x_i) -1" class="latex" title="\frac {2^n}{t} \sum_{i=1}^t D_C(x_i) -1" />.</p>
<p>They also considered a logarithmic version.</p>
<p>The samples in the experiment (at least when <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> is large) are too sparse to identify the probability distribution on <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}^n" class="latex" title="\{0,1\}^n" />. (This was one of my concerns that was also endorsed by Ryan.) But once you compute the probability distribution <img src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_C" class="latex" title="D_C" /> you can study statistically the set of probabilities that you obtained for your sample. The Google paper offers some interesting statistical studies and in particular a statistical comparison between the set of values <img src="https://s0.wp.com/latex.php?latex=%5C%7BD_C%28x_i%29%3Ai%3D1%2C%5Cdots%2Ct%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{D_C(x_i):i=1,\dots,t\}" class="latex" title="\{D_C(x_i):i=1,\dots,t\}" />.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/"><span class="datestr">at November 13, 2019 07:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4110486083097630045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2019/11/allston-students-should-speak-up.html">Allston :  Students Should Speak Up</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I have had (as is pretty usual for me) some number of lunches with students this semester, and many of them asked me about the Computer Science move to Allston.  A lot of times, these questions are concerns:  what food will we have there, how frequently will buses run, what sort of space will there be for students to hang out/study etc., what all will be over there?<br /><br />These are good questions.  I don't know the answers, I think a lot is still being worked out or decided.  I realize that's not a great response. <br /><br />I've started encouraging students to start asking these questions, more directly, to the powers that be.  Specifically, I've suggested to the small number of undergrads I've been lunching with that they should start sending e-mails to the powers that be, asking whatever questions they have, and expressing their concerns.  If you have questions, or wishes, regarding the food, safety, transportation, etc. at the new Allston building, you should ask or let your desires be known.  And, I'll be frank here, you should not (just) let the faculty know -- we're at best an indirect channel to the powers, and they may listen to all of you more than they listen to us.  Contact the powers directly.<br /><br />(Graduate students too.)<br /><br />It might be more impactful if students organize to make their questions and/or wishes known.  Or not.  That's up to you really.  But if you want Allston to be successful from where you stand, now is a pretty good time to get involved, before we all start over there next fall.  For Allston to be the academic home you want, you may have to speak up. <br /><br />I realize undergrads might not know who are the powers that be that they should contact.  So I'll provide a starting list:<br /><br />Frank Doyle, Dean of the School of Engineering and Applied Sciences<br />Claudine Gay, Dean of the Faculty of Arts and Sciences<br />Alan Garber, Provost<br />Rakesh Khurana, Dean of Harvard College<br />Amanda Claybaugh, Dean of Undergraduate Education<br /><br />And finally, yes, of course it's a <a href="https://en.wikipedia.org/wiki/Powers_That_Be_(Angel)">Buffy (well, Angel) reference</a>. <br /><br /></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2019/11/allston-students-should-speak-up.html"><span class="datestr">at November 13, 2019 06:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/12/postdoc-at-university-of-bergen-apply-by-january-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/12/postdoc-at-university-of-bergen-apply-by-january-15-2020/">postdoc at University of Bergen (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>At the Department of Informatics of the University of Bergen (UiB), Norway, there is a vacancy of up to two positions as postdoctoral researcher in algorithmic foundations of data science, associated with the newly founded Center for Data Science (CEDAS). Each position is for a fixed period of 2 years.</p>
<p>Website: <a href="https://www.jobbnorge.no/en/available-jobs/job/177613/researcher-in-informatics-algorithmic-foundations-of-data-science">https://www.jobbnorge.no/en/available-jobs/job/177613/researcher-in-informatics-algorithmic-foundations-of-data-science</a><br />
Email: fedor.fomin@uib.no</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/12/postdoc-at-university-of-bergen-apply-by-january-15-2020/"><span class="datestr">at November 12, 2019 03:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4409">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4409">Annual recruitment post</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Just like I did <a href="https://www.scottaaronson.com/blog/?p=3964">last year</a>, and <a href="https://www.scottaaronson.com/blog/?p=3508">the year before</a>, I’m putting up a post to let y’all know about opportunities in our growing <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin.</p>



<p>I’m proud to report that we’re building something pretty good here.  This fall <a href="http://sites.utexas.edu/shyamshankar/">Shyam Shankar</a> joined our Electrical and Computer Engineering (ECE) faculty to do experimental superconducting qubits, while (as I <a href="https://www.scottaaronson.com/blog/?p=4233">blogged</a> in the summer) the quantum complexity theorist <a href="http://www.mit.edu/~jswright/">John Wright</a> will join me on the CS faculty in Fall 2020.  Meanwhile, <a href="https://sites.google.com/utexas.edu/potter/home">Drew Potter</a>, an expert on topological qubits, rejoined our physics faculty after a brief leave.  Our weekly quantum information group meeting now regularly attracts around 30 participants—from the turnout, you wouldn’t know it’s not MIT or Caltech or Waterloo.  My own group now has five postdocs and six PhD students—as well as some amazing undergrads striving to meet the bar set by <a href="https://www.scottaaronson.com/blog/?p=3880">Ewin Tang</a>.  Course offerings in quantum information currently include Brian La Cour’s <a href="https://cns.utexas.edu/component/cobalt/item/3138-quantum-computing?Itemid=1971">Freshman Research Initiative</a>, my own undergrad <a href="https://www.scottaaronson.com/blog/?p=3943">Intro to Quantum Information Science</a> honors class, and graduate classes on quantum complexity theory, experimental realizations of QC, and topological matter (with more to come).  We’ll also be starting an undergraduate Quantum Information Science concentration next fall.</p>



<p>So without further ado:</p>



<p>(1) If you’re interested in pursuing a PhD focused on quantum computing and information (and/or classical theoretical computer science) at UT Austin: please apply!  If you want to work with me or John Wright on quantum algorithms and complexity, <a href="https://www.cs.utexas.edu/graduate/prospective-students/apply">apply to CS</a> (I can also supervise physics students in rare cases).  Also apply to CS, of course, if you want to work with our other CS theory faculty: David Zuckerman, Dana Moshkovitz, Adam Klivans, Anna Gal, Eric Price, Brent Waters, Vijaya Ramachandran, or Greg Plaxton.  If you want to work with Drew Potter on nonabelian anyons or suchlike, or with <a href="https://web2.ph.utexas.edu/~macdgrp/">Allan MacDonald</a>, <a href="http://order.ph.utexas.edu/people/Reichl.htm">Linda Reichl</a>, <a href="https://sites.cns.utexas.edu/liopticsut/home">Elaine Li</a>, or others on many-body quantum theory, <a href="https://ph.utexas.edu/prospective-graduate-students/admissions">apply to physics</a>.  If you want to work with Shyam Shankar on superconducting qubits, <a href="http://www.ece.utexas.edu/graduate/admissions">apply to ECE</a>.  Note that the deadline for CS and physics is <strong>December 1</strong>, while the deadline for ECE is <strong>December 15</strong>.</p>



<p>You don’t need to ask me whether I’m on the lookout for great students: I always am!  If you say on your application that you want to work with me, I’ll be sure to see it.  Emailing individual faculty members is not how it works and won’t help.  Admissions are extremely competitive, so I strongly encourage you to apply broadly to maximize your options.</p>



<p>(2) If you’re interested in a postdoc in my group, I’ll have approximately two openings starting in Fall 2020.  To apply, just send me an email by <strong>January 1, 2020</strong> with the following info:<br />– Your CV<br />– 2 or 3 of your best papers (links or PDF attachments)<br />– The names of two recommenders (who should email me their letters separately)</p>



<p>(3) If you’re on the faculty job market in quantum computing and information—well, please give me a heads-up if you’re potentially interested in Austin!  Our CS, physics, and ECE departments are all open to considering additional candidates in quantum information, both junior and senior.  I can’t take credit for this—it surely has to do with developments beyond my control, both at UT and beyond—but I’m happy to relay that, in the three years since I arrived in Texas, the appetite for strengthening UT’s presence in quantum information has undergone jaw-dropping growth at every level of the university.</p>



<p>Also, Austin-Bergstrom International Airport now has direct flights to London, Frankfurt, and (soon) Amsterdam and Paris.</p>



<p>Hook ’em Hadamards!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4409"><span class="datestr">at November 12, 2019 07:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/160">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/160">TR19-160 |  Tractable Unordered 3-CNF Games | 

	Md Lutfar Rahman, 

	Thomas Watson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The classic TQBF problem can be viewed as a game in which two players alternate turns assigning truth values to a CNF formula's variables in a prescribed order, and the winner is determined by whether the CNF gets satisfied. The complexity of deciding which player has a winning strategy in this game is well-understood: it is NL-complete for 2-CNFs and PSPACE-complete for 3-CNFs.

We continue the study of the unordered variant of this game, in which each turn consists of picking any remaining variable and assigning it a truth value. The complexity of deciding who can win on a given CNF is less well-understood; prior work by the authors showed it is in L for 2-CNFs and PSPACE-complete for 5-CNFs. We conjecture it may be efficiently solvable on 3-CNFs, and we make progress in this direction by proving the problem is in P, indeed in L, for 3-CNFs with a certain restriction, namely that each width-3 clause has at least one variable that appears in no other clause. Another (incomparable) restriction of this problem was previously shown to be tractable by Kutz.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/160"><span class="datestr">at November 11, 2019 10:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/159">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/159">TR19-159 |  SETH-hardness of Coding Problems | 

	Noah Stephens-Davidowitz, 

	Vinod Vaikuntanathan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that assuming the strong exponential-time hypothesis (SETH), there are no non-trivial algorithms for the nearest codeword problem (NCP), the minimum distance problem (MDP), or the nearest codeword problem with preprocessing (NCPP) on linear codes over any finite field. More precisely, we show that there are no NCP, MDP, or NCPP algorithms running in time $q^{(1-\epsilon)n}$ for any constant $\epsilon&gt;0$ for codes with $q^n$ codewords. (In the case of NCPP, we assume non-uniform SETH.)

We also show that there are no sub-exponential-time algorithms for $\gamma$-approximate versions of these problems for some constant $\gamma &gt; 1$, under different versions of the exponential-time hypothesis.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/159"><span class="datestr">at November 11, 2019 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/11/faculty-any-rank-at-penn-state-apply-by-january-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/11/faculty-any-rank-at-penn-state-apply-by-january-1-2020/">FACULTY (ANY RANK)  at PENN STATE (apply by January 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for multiple tenure-track positions at all levels across all areas of theoretical computer science. Our department is looking to grow rapidly in several areas, and theory is one of them.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/14482">https://academicjobsonline.org/ajo/jobs/14482</a><br />
Email: ablanca@cse.psu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/11/faculty-any-rank-at-penn-state-apply-by-january-1-2020/"><span class="datestr">at November 11, 2019 06:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
