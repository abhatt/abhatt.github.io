<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" class="message" title="internal server error">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" class="message" title="internal server error">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" class="message" title="internal server error">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at August 12, 2021 03:24 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8184">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">Replica Method for the Machine Learning Theorist: Part 2 of 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p>See <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">part 1</a> of this series, and <a href="https://boazbarak.org/Papers/replica.pdf">pdf version of both parts</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.</p>



<p>In the previous post we described the outline of the replica method, and outlined the analysis per this figure:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/eVOI8EA.png" alt="" /></figure>



<p>Specifically, we reduced the task of evaluating the expectation of a (potentially modified) log partition function to evaluating expectation over <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> replicas which in turn amount to</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \int\exp(-nN \mathcal{F}(Q))" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> is the <img src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\times n" class="latex" /> matrix of _overlaps_ (inner products between pairs of replicas), and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%28Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}(Q)" class="latex" /> is some nice analytical function depending on <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G" class="latex" /> and the log probability obtaining this overlap <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />. Then since this is an integral of exponentials, it turns out to be dominated by the maximizer <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" />, arriving at</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%5Cstar+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^\star ))." class="latex" /></p>



<p>reducing our desired quantity <img src="https://s0.wp.com/latex.php?latex=%3C-%5Clog+Z%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="&lt;-\log Z&gt;" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \mathcal F(Q^\star)" class="latex" /> and so if we’re lucky, all we need to do is run Mathemtica or Sympy to find the maximizer of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" /> over the space of all?? (not really: see below) <img src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\times n" class="latex" /> matrices.</p>



<h2>IV. Constraints on <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />: Replica Symmetry and Replica Symmetry Breaking</h2>



<p>Unfortunately, the description of <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> above is a gross simplification. <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> cannot be any matrix — it needs to satisfy particular constraints. In particular, it cannot be a matrix that appears with probability tending to zero with <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> as the overlap matrix of a tuple of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> replicas from the Gibbs distribution.<br />Hence, we need to understand the space of potential matrices <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> that could arise from the probability distribution, and <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> is the global minimum under these constraints.</p>



<p>The most important constraint on <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> is the <b>replica symmetry</b> (RS), or the lack thereof (<b>replica symmetry breaking</b>, or <b>RSB</b>). Recall that <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> encodes the overlap between <img src="https://s0.wp.com/latex.php?latex=%5C%7Bx%5E%7B%28a%29%7D%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\{x^{(a)}\}" class="latex" />, where each element is a Gibbs random variable. On a high level, the structure of <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> describes the geometry of the Gibbs distribution. An in depth description of the relationship between the two is beyond the scope of this post (check out <a href="http://michel.talagrand.net/challenge/volume1.pdf">Mean Field Models for Spin Glasses</a> by Michel Talagrand). We will give some intuitions that apply in the zero-temperature limit.</p>



<h3>A. What is the symmetry ansatz and when is it a good idea?</h3>



<p>The <b>replica symmetric ansatz</b> studies the following special form of <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> matrix</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%28q-q_0%29+%5Cdelta_%7Bab%7D+%2B+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab} = (q-q_0) \delta_{ab} + q_0" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta_{ab}" class="latex" /> is the Kroneker delta. In other words, this ansatz corresponds to the guess that if we pick <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> random replicas <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2C+x%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(1)},\ldots, x^{(n)}" class="latex" /> then they will satisfy that <img src="https://s0.wp.com/latex.php?latex=%5C%7C+x%5E%7B%28a%29%7D+%5C%7C%5E2+%5Capprox+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\| x^{(a)} \|^2 \approx q" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=a%3D1%2C%5Cldots+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a=1,\ldots n" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D+%5Ccdot+x%5E%7B%28b%29%7D+%5Capprox+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(a)} \cdot x^{(b)} \approx q_0" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=a+%5Cneq+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a \neq b" class="latex" />.<br />This ansatz is especially natural for problems with unique minimizers for a fixed problem instance <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.<br />In such a problem we might imagine that the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> replicas are all random vectors that are have the same correlation with the true minimizer <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(0)}" class="latex" /> and since they are random and in high dimension, this correlation explains their correlation with one another (see example below).</p>



<p>&gt;<b>What have we done?</b> It is worthwhile to pause and take stock of what we have done here. We have reduced computing <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \log Z \rangle" class="latex" /> into finding an expression for <img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n \rangle" class="latex" /> and then reduced this to computing <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Cexp%28-+n+N+%5Cmathcal%7BF%7D+%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} \exp(- n N \mathcal{F} (Q))" class="latex" /> whre the expectation is taken over the induced distribution of the <img src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\times n" class="latex" /> overlap matrix. Now for every fixed <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" />, we reduce the task to optimizing over just two parameters <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_0" class="latex" />. Once we find the matrix <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> that optimizes this bound, we obtain the desired quantity by taking <img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Crightarrow+0%7D+%5Ctfrac%7B1%7D%7Bn%7D%5Clog+%5Cexp%28-n+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29+%3D+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{n \rightarrow 0} \tfrac{1}{n}\log \exp(-n N \mathcal{F}(Q^\star) = N \mathcal{F}(Q^\star)" class="latex" />.</p>



<h3>B. An illustration:</h3>



<p>Annealed langevin dynamics on a convex and non-convex objective below illustrate how the geometry of the learning problem influences the structure of the overlap matrix <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />.</p>



<h4>A convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"></div>
</div></figure>



<h4>A non-convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"></div>
</div></figure>



<p>We see that even in low-dimensional problems, the structure of the loss landscape influences the resulting <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> matrices. Since the replica method works only in high dimensions, these animations cannot be taken too seriously as a justification of the symmetry ansatz, but below we discuss in what kinds of models we could expect the symmetry ansatz to be a good idea.</p>



<h3>C. Replica Symmetry in High Dimensions</h3>



<p>We will now discuss a simple model where the replica symmetry ansatz is especially natural. For a fixed problem instance <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />, suppose that the <img src="https://s0.wp.com/latex.php?latex=x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_a" class="latex" /> vectors are distributed in a point cloud about some mean vector <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in\mathbb{R}^N" class="latex" />.</p>



<p><img src="https://s0.wp.com/latex.php?latex=x_a+%3D+%5Cmu+%2B+%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_a = \mu + \epsilon_a" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon_a" class="latex" /> are zero-mean noise independently sampled across different replicas with covariance <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cepsilon_%7Ba%2Ci%7D+%5Cepsilon_%7Bb%2Cj%7D+%5Cright%3E+%3D+%5Cfrac%7B%5Csigma%5E2%7D%7BN%7D+%5Cdelta_%7Bab%7D%5Cdelta_%7Bij%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \epsilon_{a,i} \epsilon_{b,j} \right&gt; = \frac{\sigma^2}{N} \delta_{ab}\delta_{ij}" class="latex" />. This is equivalent to stipulating a Gibbs measure with energy <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+H%28x%29+%3D+-+%5Clog+p_%7B%5Cepsilon%7D%28x-%5Cmu%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta H(x) = - \log p_{\epsilon}(x-\mu)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=p_%5Cepsilon%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\epsilon(\cdot)" class="latex" /> is the distribution of each noise variable. In this case, the <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> matrix has elements</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%7C%5Cmu%7C%5E2+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_a+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_b+%2B+%5Cepsilon_a%5E%5Ctop+%5Cepsilon_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab} = |\mu|^2 + \mu^\top \epsilon_a + \mu^\top \epsilon_b + \epsilon_a^\top \epsilon_b" class="latex" /></p>



<p>By the central limit theorem, these sums of independently sampled random variables are approximately Gaussian (remember <img src="https://s0.wp.com/latex.php?latex=N+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \to\infty" class="latex" />), so we can estimate how <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> behaves in the large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> limit</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Q_%7Bab%7D+%5Cright%3E+%3D+%7C%5Cmu%7C%5E2+%2B+%5Csigma%5E2+%5Cdelta_%7Bab%7D+%5C+%2C+%5C+%5Ctext%7BVar%7D+%5C+Q_%7Bab%7D+%3D+O%281%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Q_{ab} \right&gt; = |\mu|^2 + \sigma^2 \delta_{ab} \ , \ \text{Var} \ Q_{ab} = O(1/N)" class="latex" /></p>



<p>This implies that in the thermodynamic <img src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N\to\infty" class="latex" /> limit, the <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> matrix concentrates around a replica symmetric structure. Note that the emergence of this RS structure relied on the fact that high dimensional random vectors are approximately orthogonal. For many supervised learning problems such as least squares fitting, this toy model is actually relevant by specifically taking <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Csim+%5Cmathcal+N%280%2C%5Csigma%5E2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon \sim \mathcal N(0,\sigma^2/N)" class="latex" />.</p>



<h2>V. Example Simple Problem: Learning Curve Phase Transition in Least Squares Fitting</h2>



<p>To show these tools in action we will first study the simplest possible example with that has an interesting outcome. We will study the generalization performance of ridge regression on Gaussian distributed random features. In particular we will study a thermodynamic limit where the number of samples <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" /> and the number of features <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> are both tending to infinity <img src="https://s0.wp.com/latex.php?latex=P%2CN+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P,N \to\infty" class="latex" /> but with finite ratio <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+P%2FN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = P/N" class="latex" />. We will observe a phase transition the point <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = 1" class="latex" />, where the learning problem transitions from over-parameterized (<img src="https://s0.wp.com/latex.php?latex=P+%3C+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P &lt; N" class="latex" />) to under-parameterized (<img src="https://s0.wp.com/latex.php?latex=P%3EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P&gt;N" class="latex" />). In the presence of noise this leads to an overfitting peak which can be eliminated through explicit regularization.</p>



<h3>A. Some References</h3>



<p><a href="https://iopscience.iop.org/article/10.1088/0305-4470/22/12/016">Hertz, Krogh, and Thorbergsson</a> first studied this problem and noted the phase transition at <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = 1" class="latex" />. <a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031034">Advani and Ganguli</a> examine this model as a special case of M-estimation. Analysis of this model can also be obtained as a special case of kernel regression, for which general learning curves were obtained by <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a> with the replica method. Similar overfitting peaks were recently observed in nonlinear two layer neural networks by <a href="https://www.pnas.org/content/116/32/15849">Belkin, Hsu, Ma, and Mandal</a> and modeled with the replica method by <a href="http://proceedings.mlr.press/v119/d-ascoli20a.html">d’Ascoli, Refinetti, Biroli, and, Krzakala </a> allowing them to <a href="https://arxiv.org/abs/2006.03509">clarify the two possible types of overfitting peaks</a> in random feature models. This problem can also be studied with tools from random matrix theory as in the work of <a href="https://arxiv.org/abs/1908.05355">Mei and Montanari</a> and several others.</p>



<h3>B. Problem Setup</h3>



<p>Our problem instance is a dataset <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D+%3D+%5C%7B+%28x%5E%5Cmu%2C+y%5E%5Cmu%29+%5C%7D_%7B%5Cmu%3D1%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D = \{ (x^\mu, y^\mu) \}_{\mu=1}^P" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\mu \in \mathbb{R}^N" class="latex" /> drawn i.i.d. from a Gaussian distribution <img src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu_k+%5Csim+%5Cmathcal+N%280%2C1%2FN+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\mu_k \sim \mathcal N(0,1/N )" class="latex" />. The target values <img src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y^\mu" class="latex" /> are generated by a noisy linear teacher</p>



<p><img src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+%5Csum_%7Bk%3D1%7D%5EN+w_k%5E%2A+x_k%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y^\mu = \sum_{k=1}^N w_k^* x_k^\mu + \sigma \epsilon^\mu" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%7C%7Cw%5E%2A%7C%7C%5E2+%3D+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="||w^*||^2 = N" class="latex" /> and noise is Gaussian distributed <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%5Cmu+%5Csim+%5Cmathcal+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon^\mu \sim \mathcal N(0,1)" class="latex" />. We will compute, not the generalization error for a particular problem instance <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />, but the <i>average</i> performance over random datasets! The energy function we study is the ridge regression loss function</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29+%3D+%5Cfrac%7B1%7D%7B%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5EN+w_k+x_k%5E%5Cmu+-+y%5E%5Cmu+%5Cright%29%5E2+%2B+%5Csum_k+w_k%5E2+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(w,\mathcal D) = \frac{1}{\lambda} \sum_{\mu=1}^P \left( \sum_{k=1}^N w_k x_k^\mu - y^\mu \right)^2 + \sum_k w_k^2 " class="latex" /></p>



<p>The ridge parameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> controls the trade-off between training accuracy and regularization of the weight vectors. When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \to 0" class="latex" />, the training data are fit perfectly while the <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \to \infty" class="latex" /> limit gives <img src="https://s0.wp.com/latex.php?latex=w+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w = 0" class="latex" /> as the minimizer of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />. The <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta \to \infty" class="latex" /> limit of the Gibbs distribution corresponds to studying the performance of the ridge regression solution which minimizes <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />. The generalization error is an average over possible test points drawn from the same distribution<br /><img src="https://s0.wp.com/latex.php?latex=E_g+%3D+%5Cleft%3C+%5Cleft%28+%5Csum_k+%28w_k-w%5E%2A_k%29+x_k+%5Cright%29%5E2+%5Cright%3E_x+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bk%3D1%7D%5EN+%28w_k-w_k%5E%2A%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g = \left&lt; \left( \sum_k (w_k-w^*_k) x_k \right)^2 \right&gt;_x = \frac{1}{N} \sum_{k=1}^N (w_k-w_k^*)^2" class="latex" /></p>



<h3>C. Partition Function</h3>



<p>We introduce a partition function for the Gibbs distribution on <img src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(w,\mathcal D)" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+dw+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%28+w%5E%5Ctop+x%5E%5Cmu+-+y%5E%5Cmu+%29%5E2+%2B+%5Cfrac%7B%5Cbeta+%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%5Cright%29+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z(\mathcal D) = \int dw \exp\left( - \frac{\beta}{2 \lambda} \sum_{\mu=1}^P ( w^\top x^\mu - y^\mu )^2 + \frac{\beta }{2}||w||^2 \right) ." class="latex" /></p>



<h3>D. Replicated Partition Function</h3>



<p>We can rewrite the integral through a simple change of variables <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3D+w-w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta = w-w^*" class="latex" /> since <img src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+w%5E%7B%2A+%5Ctop%7D+x%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y^\mu = w^{* \top} x^\mu + \sigma \epsilon^\mu" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta" class="latex" /> represents the discrepancy between the learned weights <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> and the target weights <img src="https://s0.wp.com/latex.php?latex=w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w^*" class="latex" />. We will now replicate and average over the training data <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />, ie compute <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt;" class="latex" />.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%28%5Cmathcal%7BD%7D%2C+J%29%5En+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint+%5Cprod_%7Ba%3D1%7D%5En+d%5CDelta_a+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Csum_%7Ba%3D1%7D%5En+%28+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Csigma+%5Cepsilon%5E%5Cmu+%29%5E2+-+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%5Csum_%7Ba%3D1%7D%5En+%7C%7C%5CDelta_a+%2B+w%5E%2A%7C%7C%5E2+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z(\mathcal{D}, J)^n \right&gt;_{\mathcal D} = \int \prod_{a=1}^n d\Delta_a \left&lt; \exp\left( - \frac{\beta}{2\lambda} \sum_{\mu=1}^P \sum_{a=1}^n ( \Delta_a \cdot x^\mu - \sigma \epsilon^\mu )^2 - \frac{\beta}{2} \sum_{a=1}^n ||\Delta_a + w^*||^2 \right) \right&gt;_{\mathcal D}" class="latex" /></p>



<p><strong>Warning:</strong> Notice that by writing these integrals, we are implicitly assuming that <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is an integer. Eventually, we need to take <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \to 0" class="latex" /> limit to obtain the generalization error <img src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \log Z \right&gt;" class="latex" />. After computation of <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt;" class="latex" /> at integer <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" />, we will get an analytic expression of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> which we will allow us to non-rigorously take <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \to 0" class="latex" />.</p>



<p>The randomness from the dataset <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> is present in the first term only appears through mean-zero Gaussian variables <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu+%3D+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Cepsilon%5E%5Cmu+%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_a^\mu = \Delta_a \cdot x^\mu - \epsilon^\mu \sigma" class="latex" /> which have covariance structure</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cgamma_a%5E%5Cmu+%5Cgamma_b%5E%5Cnu+%5Cright%3E+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%2B+%5Csigma%5E2+%5Cright%5D+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+Q_%7Bab%7D+%2B+%5Csigma%5E2+%5Cright%5D+%5Cimplies+%5Cleft%3C+%5Cgamma+%5Cgamma%5E%5Ctop+%5Cright%3E+%3D+Q+%2B+%5Csigma%5E2+1+1%5E%5Ctop+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \gamma_a^\mu \gamma_b^\nu \right&gt; = \delta_{\mu \nu} \left[ \frac{1}{N} \Delta_a \cdot \Delta_b + \sigma^2 \right] = \delta_{\mu \nu} \left[ Q_{ab} + \sigma^2 \right] \implies \left&lt; \gamma \gamma^\top \right&gt; = Q + \sigma^2 1 1^\top \in \mathbb{R}^{n \times n}" class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=1+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 \in \mathbb{R}^n" class="latex" /> is the vector of all ones and we introduced overlap order parameters <img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab}" class="latex" /> defined as</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%5C+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab} = \frac{1}{N} \Delta_a \cdot \Delta_b \ ." class="latex" /></p>



<p>The average over the randomness in the dataset <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> is therefore converted into a routine Gaussian integral. Exploiting the independence over each data point, we break the average into a product of <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" /> averages.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%2C%5Cmu%7D+%5Cleft%28+%5Cgamma_%7Ba%7D%5E%5Cmu%5Cright%29%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5E%5Cmu%5C%7D%7D+%3D+%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_%7Ba%7D%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5C%7D%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a,\mu} \left( \gamma_{a}^\mu\right)^2 \right) \right&gt;_{\{\gamma_a^\mu\}} = \left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a} \gamma_{a}^2 \right) \right&gt;_{\{\gamma_a\}}^P" class="latex" /></p>



<p>Each average is a multivariate Gaussian integral of the form<br /><img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7Bd%5Cgamma_1+d%5Cgamma_2+...+d%5Cgamma_n%7D%7B%5Csqrt%7B%5Cleft%28+2%5Cpi+%5Cright%29%5En+%5Cdet%28Q%2B%5Csigma%5E2+I%29%7D%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+%5Cgamma_a+%5Cgamma_b+%5Cleft%28+Q+%2B+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%7D_%7Bab%7D+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_a%5E2+%5Cright%29+%3D+%5Cdet%5Cleft%28I+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+Q+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int \frac{d\gamma_1 d\gamma_2 ... d\gamma_n}{\sqrt{\left( 2\pi \right)^n \det(Q+\sigma^2 I)}} \exp\left( -\frac{1}{2} \sum_{ab} \gamma_a \gamma_b \left( Q + \sigma^2 11^\top \right)^{-1}_{ab} - \frac{\beta}{2 \lambda} \sum_{a} \gamma_a^2 \right) = \det\left(I + \frac{\beta}{\lambda} Q + \frac{\beta}{\lambda} \sigma^2 11^\top \right)^{-1/2}" class="latex" /></p>



<p>This integral can be derived by routine integration of Gaussian functions, which we derive in the Appendix.</p>



<h3>E. Enforcing Order Parameter Definition</h3>



<p>To enforce the definition of the order parameters, we insert delta-functions into the expression for <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt;" class="latex" /> which we write as Fourier integrals over dual order parameters <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{Q}_{ab}" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28i+%5Chat%7BQ%7D_%7Bab%7D+%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta( N Q_{ab} - \Delta_a \cdot \Delta_b) = \frac{1}{2\pi} \int d\hat{Q}_{ab} \exp\left(i \hat{Q}_{ab} ( N Q_{ab} - \Delta_a \cdot \Delta_b)\right) " class="latex" /></p>



<p>This trick is routine and is derived in the Appendix of this post.</p>



<p>After integration over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_a" class="latex" />, we are left with an expression of the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+%5Cprod_%7Bab%7D+dQ_%7Bab%7D+%5Cprod_%7Bab%7D+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+-+P+G_E%28Q%29+%2B+i+N+%5Csum_%7Bab%7D+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+N+G_S%28%5Chat%7BQ%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt; = \int \prod_{ab} dQ_{ab} \prod_{ab} d\hat{Q}_{ab} \exp\left( - P G_E(Q) + i N \sum_{ab} Q_{ab} \hat{Q}_{ab} - N G_S(\hat{Q}) \right)" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G_E" class="latex" /> is a function which arises from the average over <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_a^\mu" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G_S" class="latex" /> is calculated through integration over the <img src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_a" class="latex" /> variables.</p>



<p><strong>Warning:</strong> The functions <img src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G_E" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G_S" class="latex" /> have complicated formulas and we omit them here to focus on the conceptual steps in the replica method. Interested readers can find explicit expressions for these functions in the references above.</p>



<h3>F. Replica Symmetry</h3>



<p>To make progress on the integral above, we will make the replica symmetry assumption, leveraging the fact that the ridge regression loss is convex and has unique minimizer for <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda &gt; 0" class="latex" />. Based on our simulations and arguments above, we will assume that the <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{Q}" class="latex" /> matrices satisfy <i>replica symmetry</i><br /><img src="https://s0.wp.com/latex.php?latex=%7BQ%7D_%7Bab%7D+%3D+q+%5Cdelta_%7Bab%7D+%2B+q_0+%5C+%2C+%5C+%5Chat%7BQ%7D_%7Bab%7D+%3D+%5Chat%7Bq%7D+%5Cdelta_%7Bab%7D+%2B+%5Chat%7Bq%7D_0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{Q}_{ab} = q \delta_{ab} + q_0 \ , \ \hat{Q}_{ab} = \hat{q} \delta_{ab} + \hat{q}_0 " class="latex" /></p>



<h3>G. Saddle Point Equations and Final Result</h3>



<p>After the replica symmetry ansatz, the replicated partition function has the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dq+d%5Chat%7Bq%7D+dq_0+d%5Chat%7Bq%7D_0+%5Cexp%28+-+n+N+%5Cmathcal+F%28q%2C%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt; = \int dq d\hat{q} dq_0 d\hat{q}_0 \exp( - n N \mathcal F(q,\hat{q},q_0,\hat{q}_0))" class="latex" /></p>



<p>In the <img src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \to \infty" class="latex" /> limit, this integral is dominated by the order parameters <img src="https://s0.wp.com/latex.php?latex=q%2C+%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q, \hat{q},q_0,\hat{q}_0" class="latex" /> which satisfy the saddle point equations</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q_0%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D_0%7D+%3D+0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\partial \mathcal F}{\partial q} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}} = 0 \ , \ \frac{\partial \mathcal F}{\partial q_0} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}_0} = 0 " class="latex" /></p>



<p><strong>Warning</strong>:Notice that <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is small (we are working in <img src="https://s0.wp.com/latex.php?latex=n%5Cto0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\to0" class="latex" /> limit to study <img src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Z" class="latex" />) but <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> is large (we are studying the “thermodynamic” <img src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N\to\infty" class="latex" /> limit). The order of taking these limits matters. It is important that we take <img src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \to \infty" class="latex" /> first before taking <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \to 0" class="latex" /> so that, at finite value of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" />, the integral for <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt;" class="latex" /> is dominated by the saddle point of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" />.</p>



<p>We can solve the saddle point equations symbolically with Mathematica (see <a href="https://www.dropbox.com/s/m0pje9mr0gp0x1n/saddle_point_demo.nb?dl=1">this notebook</a>) in the <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta \to \infty" class="latex" /> limit. We notice that <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> must scale like <img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(1/\beta)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{Q}" class="latex" /> must scale like <img src="https://s0.wp.com/latex.php?latex=O%28%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(\beta)" class="latex" />. After factoring out the dependence on the temperature, we can compute the saddle point conditions through partial differentiation.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Ps8WGh6.png" alt="" /></figure>



<p>This symbolically gives us the order parameters at the saddle point. For example, the overlap parameter <img src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cfrac%7B1%7D%7B2%7D%5B1-%5Clambda-%5Calpha+%2B+%5Csqrt%7B%281-%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%7D+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q = \frac{1}{2}[1-\lambda-\alpha + \sqrt{(1-\lambda-\alpha)^2 + 4\lambda} ]" class="latex" />. After solving the saddle point equations, the generalization error can be written entirely in terms of the first order parameter <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> at the saddle point. For replica <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a" class="latex" />, the generalization error is merely <img src="https://s0.wp.com/latex.php?latex=%7C%7C%5CDelta_a%7C%7C%5E2+%3D+%7C%7Cw_a+-+w%5E%2A%7C%7C%5E2+%3D+Q_%7Baa%7D+%3D+q%2Bq_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="||\Delta_a||^2 = ||w_a - w^*||^2 = Q_{aa} = q+q_0" class="latex" />. Thus</p>



<p><img src="https://s0.wp.com/latex.php?latex=E_g+%3D+q%2Bq_0+%3D+%5Cfrac%7B%28q%2B%5Clambda%29%5E2+%2B+%5Csigma%5E2+%5Calpha%7D%7B%28q%2B%5Clambda+%2B+%5Calpha%29%5E2+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g = q+q_0 = \frac{(q+\lambda)^2 + \sigma^2 \alpha}{(q+\lambda + \alpha)^2 - \alpha}" class="latex" /></p>



<p>Where <img src="https://s0.wp.com/latex.php?latex=q+%2B+%5Clambda+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%5B+1+%2B+%5Clambda+-+%5Calpha+%2B+%5Csqrt%7B%281%2B%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%5Calpha%7D+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q + \lambda = \frac{1}{2} \left[ 1 + \lambda - \alpha + \sqrt{(1+\lambda-\alpha)^2 + 4\lambda\alpha} \right]" class="latex" /> at the saddle point.</p>



<h3>H. Noise Free Estimation</h3>



<p>When <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%2C+%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2, \lambda \to 0" class="latex" /> the generalization error decreases linearly with <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" />: <img src="https://s0.wp.com/latex.php?latex=E_g+%3D+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g = 1-\alpha" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3C+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha &lt; 1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=E_g%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g=0" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha &gt; 1" class="latex" />. This indicates the target weights are perfectly estimated when the number of samples equals the number of features <img src="https://s0.wp.com/latex.php?latex=P+%5Cto+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P \to N" class="latex" />. A finite ridge parameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda &gt; 0" class="latex" /> increases the generalization error when noise is zero <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2=0" class="latex" />. Asymptotically, the generalization error scales like <img src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" /> for large <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" />.</p>



<h3>I. Phase transition and overfitting peaks</h3>



<p>In the presence of noise <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2 &gt; 0" class="latex" />, the story is different. In this case, the generalization error exhibits a peak at <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1%2B%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha \approx 1+\lambda" class="latex" /> before falling at a rate <img src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Csigma%5E2%2F%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g \sim \sigma^2/\alpha" class="latex" /> at large <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" />. In this regime, accurate estimation requires reducing the variance of the estimator by increasing the number of samples.</p>



<p>In small <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \to 0" class="latex" /> limit, the order parameter behaves like <img src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q+\lambda \sim 1-\alpha" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Calpha%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha&lt;1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q+\lambda \sim \lambda" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha &gt; 1" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/nmDihyt.png" alt="" /></figure>



<p>The free energy <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> exhibits a discontinuous first derivative as <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha \to 1" class="latex" />, a phenomenon known as <a href="https://en.wikipedia.org/wiki/Phase_transition#Ehrenfest_classification">first-order phase transition</a>. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal F^*" class="latex" /> be the value of the free energy at the saddle point <img src="https://s0.wp.com/latex.php?latex=%28Q%5E%5Cstar%2C+%5Chat%7Bq%7D%5E%2A%2C+q_0%5E%2A%2C+%5Chat%7Bq%7D_0%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(Q^\star, \hat{q}^*, q_0^*, \hat{q}_0^*)" class="latex" />. Then we find</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%5E%2A%7D%7B%5Cpartial+%5Calpha%7D+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B2%5Clambda%7D+%5CTheta%28%5Calpha+-+1%29+%2B+%5Cmathcal%7BO%7D_%5Clambda%281%29+%5C+%2C+%5C+%28%5Clambda+%5Cto+0%2C+%5Calpha+%5Cto+1+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\partial \mathcal F^*}{\partial \alpha} \sim \frac{\sigma^2}{2\lambda} \Theta(\alpha - 1) + \mathcal{O}_\lambda(1) \ , \ (\lambda \to 0, \alpha \to 1 )" class="latex" /></p>



<p>which indicates a discontinous first derivative in the <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \to 0" class="latex" /> limit as <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha \to 1" class="latex" />. We plot this free energy <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal F^*(\alpha)" class="latex" /> for varying values of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />, showing that as <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda\to 0" class="latex" /> a discontinuity in the free energy occurs at <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha \to 1" class="latex" />. The non-zero ridge parameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda &gt; 0" class="latex" /> prevents the strict phase transition at <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = 1" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/f4t9Tun.png" alt="" /></figure>



<h3>J. Putting it all together</h3>



<p>Using the analysis of the saddle point, we are now prepared to construct a full picture of the possibilities. A figure below from <a href="https://arxiv.org/abs/2006.13198">this paper</a> provies all of the major insights. We plot experimental values of generalization error <img src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g" class="latex" /> in a <img src="https://s0.wp.com/latex.php?latex=N%3D800&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N=800" class="latex" /> dimensional problem to provide a comparison with the replica prediction.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Vy7GwUs.png" alt="" /></figure>



<p>( a ) When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda = 0" class="latex" />, the generalization error either falls like <img src="https://s0.wp.com/latex.php?latex=1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\alpha" class="latex" /> if noise is zero, or it exhibits a divergence at <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha \to 1" class="latex" /> if noise is non-zero.</p>



<p>( b ) When noise is zero, increasing the explicit ridge <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> increases the generalization error. At large <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" />.</p>



<p>( c ) When there is noise, explicit regularization can prevent the overfitting peak and give optimal generalization. At large <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g \sim \frac{\sigma^2}{\alpha}" class="latex" />.</p>



<p>( d ) In the <img src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\lambda,\sigma^2)" class="latex" /> plane, there are multiple possibilities for the learning curve <img src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g(\alpha)" class="latex" />. Monotonic learning curves <img src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%29+%3C0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g'(\alpha) &lt;0" class="latex" /> are guaranteed provided <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> is sufficiently large compared to <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2" class="latex" />. If regularization is too small, then two-critical points can exist in the learning curve, ie two values <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha^*" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%5E%2A%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g'(\alpha^*) = 0" class="latex" /> (sample wise double descent). For very large noise, a single local maximum exists in the learning curve <img src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E_g(\alpha)" class="latex" />, which is followed by monotonic decreasing error.</p>



<h2>VI. Example Problem 2: Spiked Matrix Recovery</h2>



<p>_Detailed calculations can be found in this excellent <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">introduction of the problem by Song Mei</a>._</p>



<p>Suppose we have a <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />-by-<img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> rank-1 matrix, <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cmathbf+u+%5Cmathbf+u+%5ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \mathbf u \mathbf u ^T" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf u" class="latex" /> is a norm-1 column vector constituting the signal that we would like to recover. The input <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf A" class="latex" /> we receive is corrupted by symmetric Gaussian i.i.d. noise, i.e.,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A+%3D+%5Clambda+%5Cmathbf%7Buu%7D%5ET+%2B+%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf A = \lambda \mathbf{uu}^T + \mathbf W" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=W_%7Bij%7D%3DW_%7Bji%7D%5Csim+%5Cmathcal+N%280%2C+1%2FN%29%2C+W_%7Bii%7D%5Csim+%5Cmathcal+N+%280%2C+2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W_{ij}=W_{ji}\sim \mathcal N(0, 1/N), W_{ii}\sim \mathcal N (0, 2/N)" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> is drawn from a Gaussian Orthogonal Ensemble). At large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />, eigenvalues of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> are distributed uniformly on a unit disk in the complex plane. Thus, the best estimate (which we call <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf v" class="latex" />) of<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf u" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf A" class="latex" /> is the eigenvector associated with the largest eigenvalue. In other words</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%3D+%5Carg+%5Cmax_%7B%5Cmathbf+x%5Cin+%5Cmathbb+S%5E%7BN-1%7D%7D+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BA%7D+%5Cmathbf+%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf v = \arg \max_{\mathbf x\in \mathbb S^{N-1}} \mathbf{x}^T \mathbf{A} \mathbf {x}." class="latex" /></p>



<p>The <i>observable</i> of interest is how well the estimate, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf v" class="latex" />, matches <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf u" class="latex" />, as measured by <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mathbf v \cdot \mathbf u)^2" class="latex" />. We would like to know its average over different <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{W}" class="latex" />.</p>



<p>In the problem setup, <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> is a constant controlling the signal-to-noise ratio. Intuitively, the larger <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> is, the better the estimate should be (when averaged over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" />). This is indeed true. Remarkably, for large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%5Ccdot+%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf v \cdot \mathbf u" class="latex" /> is almost surely <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda &lt;1" class="latex" />. For <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \geq 1" class="latex" />, it grows quickly as <img src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\lambda^{-2}" class="latex" />. This discontinuity at <img src="https://s0.wp.com/latex.php?latex=%5Clambda%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda=1" class="latex" /> is a <b>phase transition</b>. This dependence on <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> can be derived using the replica method.<br /><img src="https://i.imgur.com/ysQ4W3Y.png" /></p>



<p>In the simulations above, we see two trend with increasing <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />. First, the average curve approaches the theory, which is good. In addition, trial-to-trial variability (as reflected by the error bars) shrinks. This reflects the fact that our observable is indeed self-averaging.</p>



<p>Here, we give a brief overview of how the steps of a replica calculation can be set up and carried out.</p>



<h3>Step 1</h3>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{W}" class="latex" /> is the problem parameter (<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal P" class="latex" />) that we average over. The minimized function is</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%3D+-%5Cmathbf+x%5ET+%28%5Clambda+%5Cmathbf+u+%5Cmathbf+u%5ET+%2B+%5Cmathbf+W%29+%5Cmathbf%7Bx%7D+%3D+-%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf%7Bu+%7D%29%5E2+-+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BW%7D+%5Cmathbf%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(\mathbf x, \mathbf W) = -\mathbf x^T (\lambda \mathbf u \mathbf u^T + \mathbf W) \mathbf{x} = -\lambda (\mathbf x \cdot \mathbf{u })^2 - \mathbf{x}^T \mathbf{W} \mathbf{x}." class="latex" /></p>



<p>This energy function already contains a “source term” for our observable of interest. Thus, the vanilla partition function will be used as the augmented partition function. In addition, this function does not scale with <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />. To introduce the appropriate <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> scaling, we add an <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> factor to <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />, yielding the partition function</p>



<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf+W%2C+%5Clambda%29+%3D+%5Cint_%7B%5Cmathbb+S%5E%7BN-1%7D%7Dd+%5Cmathbf+x+%5Cexp+%5Cleft%28+-%5Cbeta+N+H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z(\mathbf W, \lambda) = \int_{\mathbb S^{N-1}}d \mathbf x \exp \left( -\beta N H(\mathbf x, \mathbf W) \right)." class="latex" /></p>



<p>It follows that (again using angular brackets to denote average over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" />)</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2+%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+%5CBig%7C_%7B%5Clambda%3D0%7D.+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle (\mathbf x \cdot \mathbf u)^2 \rangle = \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle \Big|_{\lambda=0}. " class="latex" /><br />Since we are ultimately interested in this observable for the best estimate, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf v" class="latex" />, at the large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />, limit, we seek to compute</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D%5Cmathbb+E+%5B%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2%5D+%3D%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{N\rightarrow \infty}\mathbb E [(\mathbf v \cdot \mathbf u)^2] =\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle ." class="latex" /></p>



<p>Why don’t we evaluate the derivative only at <img src="https://s0.wp.com/latex.php?latex=%5Clambda%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda=0" class="latex" />? Because <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda (\mathbf x \cdot \mathbf u)^2" class="latex" /> is not a source term that we introduced. Another way to think about it is that this result needs to be a function of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />, so of course we don’t just evaluate it at one value.</p>



<h3>Step 2</h3>



<p>Per the replica trick, we need to compute</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/7f2JdBt.png" alt="" /></figure>



<p><strong>Warning:</strong> Hereafter, our use of “<img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n\rangle =" class="latex" />” is loose. When performing integrals, we will ignore the constants generated and only focus on getting the exponent right. This is because we will eventually take <img src="https://s0.wp.com/latex.php?latex=%5Clog&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n \rangle" class="latex" /> and take the derivative w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />. A constant in <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> in front of the integral expression for <img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n\rangle" class="latex" /> does not affect this integral. This is often the case in replica calculations.</p>



<p>where <img src="https://s0.wp.com/latex.php?latex=D%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D\mathbf x" class="latex" /> is a uniform measure on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb S^{N-1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=D%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D\mathbf{W}" class="latex" /> is the probability measure for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{W}" class="latex" />, as described above.</p>



<p>We will not carry out the calculation in detail in this note as the details are problem-specific. But the overall workflow is rather typical of replica calculations:</p>



<p>1. Integrate over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" />. This can be done by writing <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> as the sum of a Gaussian i.i.d. matrix with its own transpose. The integral is then over the i.i.d. matrix and thus a standard Gaussian integral. After this step, we obtain an expression that no longer contains <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" />,a major simplification.<br />2. Introduce the order parameter <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" />. After the last integral, the exponent only depends on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf x^{(a)}" class="latex" /> through <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf u \cdot \mathbf x^{(a)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%5E%7B%28a%29%7D%7D+%5Ccdot+%5Cmathbf%7Bx%5E%7B%28b%29%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{x^{(a)}} \cdot \mathbf{x^{(b)}}" class="latex" />. These dot products can be described by a matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q+%5Cin+%5Cmathbb+R%5E%7BN%2B1+%5Ctimes+N%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q \in \mathbb R^{N+1 \times N+1}" class="latex" />, where we define <img src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3DQ_%7Ba%2C0%7D%3D%5Cmathbf%7Bu%7D+%5Ccdot+%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{0,a}=Q_{a,0}=\mathbf{u} \cdot \mathbf{x}^{(a)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cgeq+1%2C+b%5Cgeq1%7D%3D%5Cmathbf+x%5E%7B%28a%29%7D+%5Ccdot+%5Cmathbf+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{a\geq 1, b\geq1}=\mathbf x^{(a)} \cdot \mathbf x^{(b)}" class="latex" />.<br />3. Replace the integral over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf x" class="latex" /> with one over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" />. A major inconvenience of the integral over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf x" class="latex" /> is that it is not over the entire real space but over a hypersphere. However, we can demand <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D+%5Cin+%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf x^{(a)} \in \mathbb S^{N-1}" class="latex" /> by requiring <img src="https://s0.wp.com/latex.php?latex=Q_%7Baa%7D%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{aa}=1" class="latex" />. Now, we rewrite the exponent in terms of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" /> and integrate over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" /> instead, but we add many Dirac delta functions to enforce the definition of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" />. We get an expression in the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d%5Cmathbf+Q+%5Cexp+%28f%28Q%29%29+%5Cprod_%7Bi%3D1%7D%5EN+%5Cdelta+%28%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7B0i%7D%29+%5Cprod+_%7B1%5Cleq+i+%5Cleq+j+%5Cleq+N%7D+%5Cdelta+%28%5Cmathbf+x%5E%7B%28j%29%7D+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7Bji%7D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n\rangle = \int d\mathbf Q \exp (f(Q)) \prod_{i=1}^N \delta (\mathbf u \cdot \mathbf x^{(i)} - Q_{0i}) \prod _{1\leq i \leq j \leq N} \delta (\mathbf x^{(j)} u \cdot \mathbf x^{(i)} - Q_{ji})." class="latex" /></p>



<p>4. After some involved simplifications, we have</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d+%5Cmathbf+Q+%5Cexp%28N+g%28%5Cmathbf+Q%29+%2B+C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n\rangle = \int d \mathbf Q \exp(N g(\mathbf Q) + C)" class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" /> does not depend on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf+Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(\mathbf Q)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(1)" class="latex" />. By the saddle point method,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cmax_%5Cmathbf%7BQ%7D+%5Cexp%28N+g%28%5Cmathbf+Q%29%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle Z^n\rangle = \max_\mathbf{Q} \exp(N g(\mathbf Q))," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{Q}" class="latex" /> needs to satisfy the various constraints we proposed (e.g., its diagonal is all <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" /> and it is symmetric).</p>



<h3>Step 3</h3>



<p>The optimization over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q" class="latex" /> is not trivial. Hence, we make some guesses about the structure of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf Q^\star" class="latex" />, which maximizes the exponent. This is where the *replica symmetry (RS) ansatz* comes in. Since the indices of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{x}^{(a)}" class="latex" /> are arbitrary, one guess is that for all <img src="https://s0.wp.com/latex.php?latex=a%2Cb%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a,b\geq 1" class="latex" /> <img src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cneq+b%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{a\neq b}" class="latex" /> has the same value ,<img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />. In addition, for all <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3D%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{0,a}=\mu" class="latex" />. This is the RS ansatz — it assumes an equivalency between replicas. Rigorously showing whether this is indeed the case is challenging, but we can proceed with this assumption and see if the results are correct.</p>



<p>The maximization of <img src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf%7BQ%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(\mathbf{Q})" class="latex" /> is now over two scalars, <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />. Writing the maximum as <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^*" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> and using the replica identity</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle+%3D+%5Clim_%7Bn%5Crightarrow+0%7D+%5Clog+%5Clangle+Z%5En+%5Crangle+%2Fn%3D+%5Clim_%7B+n+%5Crightarrow+0%7D+Ng%28%5Cmu%5E%2A%2C+Q%5E%5Cstar%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \log Z \rangle = \lim_{n\rightarrow 0} \log \langle Z^n \rangle /n= \lim_{ n \rightarrow 0} Ng(\mu^*, Q^\star)." class="latex" /></p>



<p>Setting the derivative of <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" /> w.r.t. them to zero yields two solutions.</p>



<p><strong>Bad Math Warning:</strong> Maximizing <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" /> w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Cmu%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu,q" class="latex" /> requires checking that the solutions are indeed global minima, a laborious effort that has done for some models. We will assume them to be global minima.</p>



<p>For each solution, we can compute <img src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle" class="latex" />, which will become what we are looking for after being differentiated w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />. We obtain an expression of <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \log Z \rangle" class="latex" />. The two solutions yield <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+%5Clambda+%2B+1%2F%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \log Z \rangle= \lambda + 1/\lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \log Z \rangle= 2" class="latex" />, respectively. Differentiating each w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> to get <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle (v \cdot u)^2 \rangle" class="latex" />, we have <img src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \lambda^{-2}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ciDxSHD.png" alt="" /></figure>



<p>Which one is correct? We decide by checking whether the solutions (black line and blue line above) are sensible (“physical”). It can be verified that <img src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle%3D%5Clangle+%5Clambda_%5Ctext%7Bmax%7D+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle=\langle \lambda_\text{max} \rangle" class="latex" />, which is the largest eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf A" class="latex" />. Clearly, it should be non-decreasing as a function of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />. Thus, for <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \leq 1" class="latex" />, we choose the <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> solution, and for <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda \geq 1" class="latex" /> the <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%2B+1+%2F+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda + 1 / \lambda" class="latex" /> solution. Thus, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle (v \cdot u)^2 \rangle" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\lambda^{-2}" class="latex" /> in the two regimes, respectively.</p>



<h2>Appendix: Gaussian Integrals and Delta Function Representation</h2>



<p>We frequently encounter Gaussian integrals when using the replica method and it is often convenient to rely on basic integration results which we provide in this Appendix.</p>



<h4>Single Variable</h4>



<p>The simplest Gaussian integral is the following one dimensional integral</p>



<p><img src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(a) = \int_{-\infty}^\infty \exp\left( -\frac{a}{2} x^2 \right) dx" class="latex" /></p>



<p>We can calculate the square of this quantity by changing to polar coordinates <img src="https://s0.wp.com/latex.php?latex=x%3Dr%5Ccos%5Cphi%2C+y+%3D+r+%5Csin+%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x=r\cos\phi, y = r \sin \phi" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=I%28a%29%5E2+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5E2%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%28+x%5E2+%2B+y%5E2+%5Cright%29+%5Cright%29+dx+dy+%3D+%5Cint_%7B0%7D%5E%7B2%5Cpi%7D+d%5Cphi+%5Cint_%7B0%7D%5E%7B%5Cinfty%7D+r+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+r%5E2+%5Cright%29+dr+%3D+2+%5Cpi+a%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(a)^2 = \int_{\mathbb{R}^2} \exp\left(-\frac{a}{2} \left( x^2 + y^2 \right) \right) dx dy = \int_{0}^{2\pi} d\phi \int_{0}^{\infty} r \exp\left( - \frac{a}{2} r^2 \right) dr = 2 \pi a^{-1}" class="latex" /></p>



<p>We thus conclude that <img src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Csqrt%7B2%5Cpi+%2F+a%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(a) = \sqrt{2\pi / a}" class="latex" />. Thus we find that the function <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+e%5E%7B-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\frac{a}{2\pi}} e^{-\frac{a}{2} x^2 }" class="latex" /> is a normalized function over <img src="https://s0.wp.com/latex.php?latex=%28-%5Cinfty%2C+%5Cinfty%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(-\infty, \infty)" class="latex" />. This integral can also be calculated with Mathematica or Sympy. Below is the result in Sympy.</p>



<p><code>from sympy import *<br />from sympy.abc import a, b, x, y<br />x = Symbol('x')<br />integrate( exp( -a/2 <i> x</i>*2 ) , (x, -oo,oo))<br /></code></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/k8OcjPa.png" alt="" /></figure>



<p>This agrees with our result since we were implicitly assuming <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a" class="latex" /> real and positive (<img src="https://s0.wp.com/latex.php?latex=%5Carg+a+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\arg a = 0" class="latex" />).</p>



<p>We can generalize this result to accomodate slightly more involved integrals which contain both quadratic and linear terms in the exponent. This exercise reduces to the previous case through simple completion of the square</p>



<p><img src="https://s0.wp.com/latex.php?latex=I%28a%2Cb%29%3D%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+bx+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%5B+x+%5Cmp+%5Cfrac%7Bb%7D%7Ba%7D+%5Cright%5D%5E2+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Csqrt%7B2%5Cpi%2Fa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(a,b)=\int_{-\infty}^{\infty} \exp\left(-\frac{a}{2} x^2 \pm bx \right) dx = \exp\left( \frac{b^2}{2a} \right) \int_{-\infty}^{\infty} \exp\left( - \frac{a}{2} \left[ x \mp \frac{b}{a} \right]^2 \right) dx = \exp\left( \frac{b^2}{2a} \right) \sqrt{2\pi/a}" class="latex" /></p>



<p>We can turn this equality around to find an expression</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+b+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp\left( \frac{b^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm b x \right) dx" class="latex" /></p>



<p>Viewed in this way, this formula allows one to transform a term quadratic in <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b" class="latex" /> in the exponential function into an integral involving a term linear in <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b" class="latex" />. This is known as the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> transformation. Taking <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b" class="latex" /> be imaginary (<img src="https://s0.wp.com/latex.php?latex=b%3Dik&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b=ik" class="latex" /> for real <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" />), we find an alternative expression of a Gaussian function</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7Bk%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+k+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp\left( - \frac{k^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm i k x \right) dx" class="latex" /></p>



<h4>Delta Function Integral Representation</h4>



<p>A delta function <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta(z)" class="latex" /> can be considered as a limit of a normalized mean-zero Gaussian function with variance taken to zero</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Csqrt%7B%5Cfrac%7B1%7D%7B2%5Cpi+a%7D%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta(z) = \lim_{a \to 0} \sqrt{\frac{1}{2\pi a}} \exp\left( - \frac{1}{2 a} z^2 \right)" class="latex" /></p>



<p>We can now use the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> trick to rewrite the Gaussian function</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+z+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp\left( - \frac{1}{2 a} z^2 \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( - \frac{a}{2} x^2 \pm i z x \right) dx" class="latex" /></p>



<p>Thus we can relate the delta function to an integral</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+x+z+%5Cright%29+dx+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+%5Cpm+i+x+z+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta(z) = \lim_{a \to 0} \frac{1}{2\pi} \int \exp\left( - \frac{a}{2} x^2 \pm i x z \right) dx = \frac{1}{2\pi} \int \exp\left( \pm i x z \right) dx" class="latex" /></p>



<p>This trick is routinely utilized to represent delta functions with integrals over exponential functions during a replica calculation. In particular, this identity is often used to enforce defintions of the order parameters <img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab}" class="latex" /> in the problem. For example, in the least squares problem where <img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5CDelta_a+%5Ccdot+%5CDelta_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab} = \frac{1}{N}\Delta_a \cdot \Delta_b" class="latex" /> we used</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta%28NQ_%7Bab%7D+-+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi+%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+i+N+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+i+%5Chat%7BQ%7D_%7Bab%7D+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b+%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta(NQ_{ab} - \Delta_{a} \cdot \Delta_b) = \frac{1}{2\pi } \int d\hat{Q}_{ab} \exp\left( i N Q_{ab} \hat{Q}_{ab} - i \hat{Q}_{ab} \Delta_{a} \cdot \Delta_b \right) " class="latex" /></p>



<h4>Multivariate Gaussian integrals</h4>



<p>We commonly encounter integrals of the following form<br /><img src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+M_%7Bab%7D+x_a+x_b+%5Cright%29+dx_1+dx_2+...+dx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(M) = \int_{\mathbb{R}^n} \exp\left( - \frac{1}{2} \sum_{ab} M_{ab} x_a x_b \right) dx_1 dx_2 ... dx_n" class="latex" /></p>



<p>where matrix <img src="https://s0.wp.com/latex.php?latex=M_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{ab}" class="latex" /> is symmetric and positive definite. An example is the data average in the least squares problem studied in this blog post where <img src="https://s0.wp.com/latex.php?latex=M+%3D+%28Q+%2B+%5Csigma%5E2+11%5E%5Ctop%29%5E%7B-1%7D+%2B+%5Cbeta+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M = (Q + \sigma^2 11^\top)^{-1} + \beta I" class="latex" />. We can reduce this to a collection of one dimensional problems by computing the eigendecomposition of <img src="https://s0.wp.com/latex.php?latex=M+%3D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+u_%5Crho+u_%5Crho%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M = \sum_{\rho} \lambda_\rho u_\rho u_\rho^\top" class="latex" />. From this decomposition, we introduce variables</p>



<p><img src="https://s0.wp.com/latex.php?latex=z_%5Crho+%3D+%5Csum_%7Ba%3D1%7D%5En+u_%7B%5Crho%2Ca%7D+x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z_\rho = \sum_{a=1}^n u_{\rho,a} x_a" class="latex" /></p>



<p>The transformation from <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" /> is orthogonal so the determinant of the Jacobian has absolute value one. After changing variables, we therefore obtain the following decoupled integrals</p>



<p><img src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+z_%5Crho%5E2+%5Cright%29+dz_1...dz_n+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Clambda_%5Crho%7D%7B2+%7D+z_%5Crho%5E2+%5Cright%29+dz_%5Crho+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Csqrt%7B%5Cfrac%7B2%5Cpi%7D%7B%5Clambda_%5Crho%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(M) = \int_{\mathbb{R}^n} \exp\left( -\frac{1}{2} \sum_{\rho} \lambda_\rho z_\rho^2 \right) dz_1...dz_n = \prod_{\rho=1}^n \int \exp\left( -\frac{\lambda_\rho}{2 } z_\rho^2 \right) dz_\rho = \prod_{\rho=1}^n \sqrt{\frac{2\pi}{\lambda_\rho}}" class="latex" /></p>



<p>Using the fact that the determinant is the product of eigenvalues <img src="https://s0.wp.com/latex.php?latex=%5Cdet+M+%3D+%5Cprod_%7B%5Crho%7D+%5Clambda_%5Crho&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\det M = \prod_{\rho} \lambda_\rho" class="latex" />, we have the following expression for the multivariate Gaussian integral</p>



<p><img src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cleft%282+%5Cpi+%5Cright%29%5E%7Bn%2F2%7D+%5Cdet%5Cleft%28+M+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(M) = \left(2 \pi \right)^{n/2} \det\left( M \right)^{-1/2}" class="latex" />.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/"><span class="datestr">at August 11, 2021 04:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">Replica Method for the Machine Learning Theorist: Part 1 of 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p><em>[Boaz’s note: Blake and Haozhe were students in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">the ML theory seminar</a> this spring; in that seminar we touched on the replica method in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">lecture on inference and statistical physics</a> but here Blake and Haozhe (with a little help from the rest of us) give a great overview of the method and its relations to ML. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.]</em></p>



<p>See also: <a href="https://boazbarak.org/Papers/replica.pdf">PDF version of both parts</a> and <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part 2 of this post</a>.</p>



<h2>I. Analysis of Optimization Problems with Statistical Physics</h2>



<p>In computer science and machine learning, we are often interested in solving optimization problems of the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%5Cin+%5Cmathcal%7BS%7D%7D+H%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{x \in \mathcal{S}} H(x,\mathcal D)" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is an objective function which depends on our decision variables <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal+S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in \mathcal S" class="latex" /> as well as on a set of problem-specific parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />. Frequently, we encounter problems relevant to machine learning, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> is a random variable. The replica method is a useful tool to analyze <em>large problems</em> and their <em>typical</em> behavior over the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.</p>



<p>Here are a few examples of problems that fit this form:</p>



<ol><li>In supervised learning, <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> may be a training loss, <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> a set of neural network weights and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> the data points and their labels</li><li>We may want to find the most efficient way to visit all nodes on a graph. In this case <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> describes nodes and edges of the graph, <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is a representation of the set of chosen edges, and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> can be the cost of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> encodes a valid path and <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\infty" class="latex" /> (or a very large number ) if it doesn’t encode a valid path.</li><li>Satisfiability: <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in \{0,1\}^N" class="latex" /> is a collection booleans which must satisfy a collection of constraints. In this case the logical constraints (clauses) are the parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=H%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(x)" class="latex" /> can be the number of constraints violated by <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />.</li><li>Recovery of structure in noisy data: <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is our guess of the structure and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> are instances of the observed noisy data. For example PCA attempts to identify the directions of maximal variation in the data. With the replica method, we could ask how the accuracy of the estimated top eigenvector degrades with noise.</li></ol>



<h2>II. The Goal of the Replica Method</h2>



<p>The <strong>replica method</strong> is a way to calculate the value of some statistic (<strong>observable</strong> in physics-speak) <img src="https://s0.wp.com/latex.php?latex=O%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(x)" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is a “typical” minimizer of <img src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(x,\mathcal{D})" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" /> is a “typical” value for the parameters (which are also known in physics-speak as the <strong>disorder</strong>).</p>



<p>In Example 1 (supervised learning), the observable may be the generalization error of a chosen algorithm (e.g. a linear classifier) on a given dataset. For Example 2 (path), this could be the cost of the best path. For Example 3 (satisfiability), the observable might be whether or not a solution exists at all for the problem. In Example 4 (noisy data), the observable might be the quality of decoded data (distance from ground truth under some measure).</p>



<p>An observable like generalization error obviously depends on <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />, the problem instance. However, can we say something more general about this <em>type of problem</em>? In particular, if <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> obeys some probability distribution, is it possible to characterize the the typical observable over different problem instances <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />?</p>



<p>For instance, in Example 1, we can draw all of our training data from a distribution. For each random sample of data points <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />, we find the set of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> which minimize <img src="https://s0.wp.com/latex.php?latex=H%28x%2C+%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(x, \mathcal D)" class="latex" /> and compute a generalization error. We then repeat this procedure many times and average the results. Sometimes, there are multiple <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> that minimize <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> for a given sample of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />; this requires averaging the observable over all global minima for each <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" /> first, before averaging over different <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.</p>



<p>To give away a “spolier”, towards the end of this note, we will see how to use the replica method to give accurate predictions of performance for noisy least square fitting and spiked matrix recovery.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/BKjGG3O.png" alt="" /></figure>



<p><em>Generalization gap in least squares ridge regression, figure taken from <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a></em></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Jrth8N3.png" alt="" /></figure>



<p><em>Performance (agreement with planted signal) as function of signal strength for spiked matrix recovery, as the dimension grows, the experiment has stronger agreement with theory. See also <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">Song Mei’s exposition</a></em></p>



<h3>A. What do we actually do?</h3>



<p>Now that we are motivated, let’s see what quantities the replica method attempts to obtain. In general, given some observable <img src="https://s0.wp.com/latex.php?latex=O%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(x,\mathcal D)" class="latex" />, the average of <img src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O" class="latex" /> over a minimizer <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> chosen at random from the set <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Barg%7D+%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{arg} \min H(x,\mathcal{D})" class="latex" />, and take the average of this quantity over the choice of the disorder <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />.<br />In other words, we want to compute the following quantity:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cmathbb%7BE%7D_%7B%5Cmathcal%7BD%7D%7D+%5Cmathbb%7BE%7D_%7Bx+%5Cin+%5Ctext%7Barg%7D%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29%7D+%5Cleft%5B+O%28x%2C+%5Cmathcal%7BD%7D%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \mathbb{E}_{\mathcal{D}} \mathbb{E}_{x \in \text{arg}\min H(x,\mathcal{D})} \left[ O(x, \mathcal{D}) \right]" class="latex" /></p>



<p>The above equation has two types of expectation- over the disorder <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />, and over the minimizers <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />.<br />The physics convention is to</p>



<ul><li>use <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+f+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; f \right&gt;_{\mathcal D}" class="latex" /> for the expectation of a function <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\mathcal{D})" class="latex" /> over the disorder <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" /></li><li>use <img src="https://s0.wp.com/latex.php?latex=%5Cint+g%28x%29+%5Cmu%28x%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int g(x) \mu(x) dx" class="latex" /> for the expectation of a function <img src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x)" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> chosen according to some measure <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />.</li></ul>



<p>Using this notation, we can write the above as</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \right&gt;_{\mathcal D}" class="latex" /> denotes an average over the probability measure for problem parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p^*(x;\mathcal D)" class="latex" /> is a uniform distribution over the set of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> that minimize <img src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(x,\mathcal{D})" class="latex" /> with zero probability mass placed on sub-optimal points.</p>



<p>The ultimate goal of the replica method is to express</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Ctext%7Bsolution+of+optimization+on+constant+number+of+variables%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \text{solution of optimization on constant number of variables}" class="latex" /></p>



<p>but it will take some time to get there.</p>



<h3>B. The concept of “self-averaging” and concentration</h3>



<p>Above, we glossed over an important distinction between the “typical” value of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29+%3D+%5Cint+p%5E%2A%28x%3B+%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\mathcal D) = \int p^*(x; \mathcal D) O(x,\mathcal D) dx" class="latex" /> and the *average* value of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\mathcal D)" class="latex" />. This is OK only when we have <em>concentration</em> in the sense that with high probability over the choice of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\mathcal{D})" class="latex" /> is close to its expected value. We define this as the property that with probability at least <img src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\epsilon" class="latex" />, the quantity <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\mathcal{D})" class="latex" /> is within a <img src="https://s0.wp.com/latex.php?latex=1%5Cpm+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1\pm \epsilon" class="latex" /> multiplicative factor of its expectation, whwere <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> is some quantity that goes to zero as the system size grows. A quantity <img src="https://s0.wp.com/latex.php?latex=f%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(\cdot)" class="latex" /> that concentrates in this sense is called <strong>self averaging</strong>.</p>



<p>For example, suppose that <img src="https://s0.wp.com/latex.php?latex=X%3D+%5Csum_%7Bi%3D1%7D%5En+X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X= \sum_{i=1}^n X_i" class="latex" /> where each <img src="https://s0.wp.com/latex.php?latex=X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X_i" class="latex" /> equals <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" /> with probabilty <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/2" class="latex" /> independently. Standard Chernoff bounds show that with high probability <img src="https://s0.wp.com/latex.php?latex=X+%5Cin+%5Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X \in [n/2 \pm O(\sqrt{n})]" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7BX%7D%7B%5Cmathbb%7BE%7D+X%7D+%5Cin+%5Cleft%281+%2B+O%28%5Ctfrac%7B1%7D%7B%5Csqrt+n%7D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{X}{\mathbb{E} X} \in \left(1 + O(\tfrac{1}{\sqrt n})\right)" class="latex" />. Hence <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" /> is a self averaging quantity.</p>



<p>In contrast the random variable <img src="https://s0.wp.com/latex.php?latex=Y%3D2%5EX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y=2^X" class="latex" /> is not self averaging. Since <img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+2%5E%7BX_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y = \prod_{i=1}^n 2^{X_i}" class="latex" /> and these random variables are independent, we know that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+%5Cmathbb%7BE%7D+2%5E%7BX_i%7D+%3D+%5Cleft%28%5Ctfrac%7B1%7D%7B2%7D+2%5E1+%2B+%5Ctfrac%7B1%7D%7B2%7D+2%5E0+%5Cright%29%5En+%3D+%283%2F2%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} Y = \prod_{i=1}^n \mathbb{E} 2^{X_i} = \left(\tfrac{1}{2} 2^1 + \tfrac{1}{2} 2^0 \right)^n = (3/2)^n" class="latex" />. However, with high probability a typical value of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" /> will be of the form <img src="https://s0.wp.com/latex.php?latex=2%5E%7Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%7D+%3D+%5Csqrt%7B2%7D%5E%7Bn+%5Cpm+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^{n/2 \pm O(\sqrt{n})} = \sqrt{2}^{n \pm O(\sqrt n)}" class="latex" />. Since <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D+%3C+3%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{2} &lt; 3/2" class="latex" /> we see that the typical value of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" /> is exponentially smaller than the expected value of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" />.</p>



<p>The example above is part of a more general pattern. Often even if a variable <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" /> is not self averaging, the variable <img src="https://s0.wp.com/latex.php?latex=X+%3D+%5Clog+Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X = \log Y" class="latex" /> will be self-averaging. Hence if we are interested in the typical value of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" />, the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cmathbb%7BE%7D+%5B%5Clog+Y%5D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp\left( \mathbb{E} [\log Y] \right)" class="latex" /> is more representative than the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BY%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[Y]" class="latex" />.</p>



<h3>C. When is using the replica method a good idea?</h3>



<p>Suppose that we want to compute a quantity of the form above. When is it a good idea to use the replica method to do so?<br />Generally, we would want it to satisfy the following conditions:</p>



<ol><li>The learning problem is high dimensional with a large budget of data. The replica method describes a <em>thermodynamic limit</em> where the system size and data budget are taken to infinity with some fixed ratio between the two quantities. Such a limit is obviously never achieved in reality, but in practice sufficiently large learning problems can be accurately modeled by the method.</li><li>The loss or the constraints are convenient functions of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />. Typically <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> will be a low degree polynomial or a sum of local functions (each depending on small number of variables) in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />.</li><li>Averages over the disorder in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> are tractable analytically. That is, we can compute marginals of the distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.</li><li>The statistic that we are interested in is self-averaging.</li></ol>



<p>If the above conditions aren’t met, it is unlikely that this problem will gain much analytical insight from the replica method.</p>



<h2>III. The Main Conceptual Steps Behind Replica Calculations</h2>



<p>We now describe the conceptual steps that are involved in calculating a quantity using the replica method.<br />They are also outlined in this figure:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/eVOI8EA.png" alt="" /></figure>



<h3>Step 1:”Softening” Constraints with the Gibbs Measure</h3>



<p>The uniform measure on minimizers <img src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p^*(x;\mathcal D)" class="latex" /> is often difficult to work with. To aid progress, we can think of it as a special case of what is known as the <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs measure</a>, defined as<br /><img src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28x%3B%5Cmathcal+D%29dx%3D+%5Cfrac%7B1%7D%7BZ%28%5Cmathcal+D%29%7D%5Cexp+%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal%7BD%7D%29%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\beta(x;\mathcal D)dx= \frac{1}{Z(\mathcal D)}\exp \left( -\beta H(x,\mathcal{D})\right)dx" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+%5Cexp%5Cleft%28-%5Cbeta+%7BH%7D%28x%2C+%5Cmathcal+D%29+%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z(\mathcal D) = \int \exp\left(-\beta {H}(x, \mathcal D) \right)dx" class="latex" /> is the normalization factor, or <strong>partition function</strong>. <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta" class="latex" /> is called the <strong>inverse temperature</strong>, a name from thermodynamics. It is easy to see that when <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta\to\infty" class="latex" /> (i.e., when the temperature tends to the absolute zero), the Gibbs measure converges to a uniform distribution on the minimizers of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />: <img src="https://s0.wp.com/latex.php?latex=p_%7B%5Cbeta%7D+%5Cto+p%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{\beta} \to p^*" class="latex" />.</p>



<p>Hence we can write</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E%7B%5Cmathcal+D%7D+%3D+%5Cleft%3C+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;{\mathcal D} = \left&lt; \lim_{\beta \rightarrow \infty} \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" /></p>



<p>Physicists often exchange the order of limits and expectations at will, which generally makes sense in this setting, and so assume</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cleft%3C+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \lim_{\beta \rightarrow \infty} \left&lt; \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" /></p>



<p>Thus general approach taken in the replica method is to derive an expression for the average observable for any <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta" class="latex" /> and then take the <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta \rightarrow \infty" class="latex" /> limit. The quantity <img src="https://s0.wp.com/latex.php?latex=%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int p_\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" /> is also known as the <em>thermal average</em> of <img src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O" class="latex" />, since it is taken with respect to the Gibbs distribution at some positive temperature.</p>



<p>To compute the thermal average of <img src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O" class="latex" />, we define the following <em>augmented partition function</em>:</p>



<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%2C+J%29+%3D+%5Cint_%7BS%7D+%5Cexp%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%2B+J+O%28x%3B+%5Cmathcal+D%29+%5Cright%29+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z(\mathcal D, J) = \int_{S} \exp\left( -\beta H(x,\mathcal D) + J O(x; \mathcal D) \right) dx." class="latex" /></p>



<p>One can then check that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7BdJ%7D+%5Clog+Z%28%5Cmathcal%7BD%7D%2C+J%29%5CBig%7C_%7BJ%3D0%7D%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Cint+O%28x%2C%5Cmathcal+D%29+%5Cexp%28-+%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29+dx+%3D+%5Cint+p%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{d}{dJ} \log Z(\mathcal{D}, J)\Big|_{J=0}= \frac{1}{Z} \int O(x,\mathcal D) \exp(- \beta H(x,\mathcal D) ) dx = \int p\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" /></p>



<p>Hence our desired quantity can be obtained as</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+J%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E_%7B%5Cmathcal+D%7D%280%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \lim_{\beta \to \infty} \frac{\partial}{\partial J} \left&lt; \log Z(\mathcal D, J) \right&gt;_{\mathcal D}(0)" class="latex" /></p>



<p>or (assuming we can again exchange limits at will):</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cepsilon+%5Cto+0%7D+%5Ctfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%5B+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+%5Cepsilon%29+%5Cright%3E%7B%5Cmathcal+D%7D+-+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+0%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Desired quantity } = \lim_{\epsilon \to 0} \tfrac{1}{\epsilon}\left[ \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, \epsilon) \right&gt;{\mathcal D} - \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, 0) \right&gt;_{\mathcal D}\right]" class="latex" /></p>



<p>We see that ultimately computing the desired quantity reduces to computing quantities of the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%27%28%5Cmathcal%7BD%7D%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5C%3B%5C%3B+%28%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \log Z'(\mathcal{D}) \right&gt;_{\mathcal D}\;\; (\star)" class="latex" /></p>



<p>for the original or modified partition function <img src="https://s0.wp.com/latex.php?latex=Z%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z'" class="latex" />. Hence our focus from now on will be on computing (<img src="https://s0.wp.com/latex.php?latex=%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\star" class="latex" />).<br />Averaging over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> is known as “configurational average” or <strong>quenched average</strong>. All together, we obtain the observable, first thermal averaged to get <img src="https://s0.wp.com/latex.php?latex=O%5E%2A%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O^*(\mathcal D)" class="latex" /> (averaged over <img src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p^*(x;\mathcal D)" class="latex" />) and then quenched averaged over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.</p>



<blockquote class="wp-block-quote"><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;" class="wp-smiley" alt="⚠" /> <em>What Concentrates?</em>: It is not just an algebraic convenience to average <img src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Z" class="latex" /> instead of averaging <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" /> itself. When the system size <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> is large, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{1}{N} \log Z" class="latex" /> concentrates around its average. Thus, the typical behavior of the system can be understood by studying the quenched average <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{1}{N} \left&lt; \log Z \right&gt;" class="latex" /> The partition function <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" /> itself often does not concentrate and in general the values <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+%5Cleft%3C+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{1}{N} \log \left&lt; Z \right&gt;" class="latex" /> (known as the “annealed average”) and <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{1}{N}\left&lt; \log Z \right&gt;" class="latex" /> (known as the “quenched average”) could differ subtantially. For more information, please consult <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">Mezard and Montanari’s</a> excellent book, Chapter 5.</p></blockquote>



<h3>Step 2: The Replica Trick</h3>



<p>Hereafter, we use <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Ccdot+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \cdot \rangle" class="latex" /> to denote the average and drop the dependence of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" />. To compute <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \log Z(\mathcal D, J) \right&gt;{\mathcal D}" class="latex" />, we use an identity <img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E+%3D+%5Clim_%7Bn+%5Cto+0%7D+%5Cfrac%7B1%7D%7Bn%7D+%5Clog+%5Cleft%3C+Z%5En+%5Cright%3E.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \log Z \right&gt; = \lim_{n \to 0} \frac{1}{n} \log \left&lt; Z^n \right&gt;." class="latex" /></p>



<p>For the limit to make sense, <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> should be any real number. However, the expression for <img src="https://s0.wp.com/latex.php?latex=Z%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z^n" class="latex" /> is only easily computable for natural numbers. <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;" class="wp-smiley" alt="⚠" /> This step is non-rigorous: we will obtain an expression for <img src="https://s0.wp.com/latex.php?latex=%5Clog+%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log \langle Z^n \rangle" class="latex" /> for natural number <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" />, and then take the <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow 0" class="latex" /> limit after the fact.</p>



<p>Recall that under the Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\beta(\mathcal D)" class="latex" />, the probability density on state <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is equal to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29%2FZ%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-\beta H(x,\mathcal D) )/Z(\mathcal D)" class="latex" />. Denote by <img src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\beta(\mathcal D)^n" class="latex" /> the probability distribution over a tuple <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%3D+%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{x} = (x^{(1)},\ldots,x^{(n)})" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> independent samples (also known as <strong>replicas</strong>) chosen from <img src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\beta(\mathcal D)" class="latex" />.</p>



<p>Since the partition function <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" /> is an integral (or sum in the discrete case) of the form <img src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cexp%28-%5Cbeta+H%28x%3B%5Cmathcal+D%29%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int \exp(-\beta H(x;\mathcal D)) dx" class="latex" />, we can write <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z(\mathcal D)^n" class="latex" /> as the integral of <img src="https://s0.wp.com/latex.php?latex=%5Cprod_%7Ba%3D1%7D%5En+%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%3B%5Cmathcal+D%29%29%3D+%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\prod_{a=1}^n \exp(-\beta H(x^{(a)};\mathcal D))= \exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D)\right)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(1)},\ldots,x^{(n)}" class="latex" /> are independent variables.</p>



<p>Now since each <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(a)}" class="latex" /> is weighed with a factor of <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%29%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-\beta H(x^{(a)});\mathcal D)" class="latex" />, this expression can be shown as equal to taking expectation of some exponential function <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%3B+%5Cmathcal+D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp( \sum_{a=1}^n G(x^{(a)}; \mathcal D))" class="latex" /> over a tuple <img src="https://s0.wp.com/latex.php?latex=%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x^{(1)},\ldots,x^{(n)})" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> independent samples of <strong>replicas</strong> all coming from the same Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_\beta(\mathcal D)" class="latex" /> corresponding to the same instance <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.<br />(The discussion on <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G" class="latex" /> is just for intuition – we will not care about the particular form of this <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="G" class="latex" />, since soon average it over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.)</p>



<p>Hence</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cleft%3C+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+dx+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt; = \left&lt; \int_{\vec{x} \sim p_*(\mathcal D)^n} \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) dx \right) \right&gt;_{\mathcal D}." class="latex" /></p>



<h3>Step 3: The Order Parameters</h3>



<p>The above expression is an expectation of an integral, and so we can switch the order of summation, and write it also as</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt; = \int_{\vec{x} \sim p_*(\mathcal D)^n} \left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} dx." class="latex" /></p>



<p>It turns out that for natural energy functions (for example when <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is quadratic in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> such as when it corresponds to Mean Squared Error loss), for any tuple of <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(1)},\ldots,x^{(n)}" class="latex" />, the expectation over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D) \right)" class="latex" /> only depends on the angles between the <img src="https://s0.wp.com/latex.php?latex=x%5E%28a%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^(a)" class="latex" />‘s.<br />That is, rather than depending on all of these <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />-dimensional vectors, it only depends on the <img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^2" class="latex" /> coefficients <img src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D%5Cfrac%7B1%7D%7BN%7D+x%5E%7B%28a%29%7D%5Ccdot+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{ab} =\frac{1}{N} x^{(a)}\cdot x^{(b)}" class="latex" />. The <img src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\times n" class="latex" /> matrix Q is known as the <strong>overlap matrix</strong> or <strong>order parameters</strong> and one can often find a nice analytical function <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" /> whose values are bounded (independently of <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" />) such that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \exp(-nN \mathcal{F}(Q))" class="latex" />.</p>



<p>Hence we can replace the integral over <img src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{(1)},\ldots,x^{(n)}" class="latex" /> with an integral over <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> and write</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dQ+%5Cexp%5Cleft%28-+n+N+%5Cmathcal+F%28Q%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left&lt; Z^n \right&gt; = \int dQ \exp\left(- n N \mathcal F(Q) \right)" class="latex" /></p>



<p>where the measure <img src="https://s0.wp.com/latex.php?latex=dQ&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="dQ" class="latex" /> is the one induced by the overlap distribution of a tuple <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Csim+p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{x} \sim p_\beta(\mathcal D)" class="latex" /> taken for a random choice of the parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal D" class="latex" />.</p>



<p>Since <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> only ranges over a small (<img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^2" class="latex" /> dimensional set), at the large <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> limit, the integral is dominated by the maximum of its integrand (“method of steepest descent” / “saddle point method”). Let <img src="https://s0.wp.com/latex.php?latex=Q%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^*" class="latex" /> be the global minimum of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> (within some space of matrices). We have</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%2A+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^* ))." class="latex" /></p>



<p>Once we arrive at this expression, the configurational average of <img src="https://s0.wp.com/latex.php?latex=-%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\log Z" class="latex" /> is simply <img src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \mathcal F(Q^*)" class="latex" />. These steps constitute the replica method. The ability to compute the configurational average by creating an appropriate <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> is one of the factors determining whether the replica method can be used. For example, in the supervised learning example, <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is almost always assumed to be quadratic in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />; cross-entropy loss, for instance, is generally not amendable.</p>



<blockquote class="wp-block-quote"><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;" class="wp-smiley" alt="⚠" /> <em>Bad Math Warning</em>: there are three limits, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta \rightarrow \infty" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N \rightarrow \infty" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow 0" class="latex" />. In replica calculations, we assume that we take take these limits in whichever order that is arithmetically convenient.</p></blockquote>



<p><strong>Coming up:</strong> In <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part two of this blog post</a>, we will explain the replica symmetric assumption (or “Ansatz” in Physics-speak) on the order parameters <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> and then demonstrate how to use the replica method for two simple examples: <em>least squares regression</em> and <em>spiked matrix model</em>.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/"><span class="datestr">at August 11, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-7046389272941836220">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/08/the-first-movie.html">The First: A Movie</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I had the great pleasure to watch <a href="https://pantar.com/" target="_blank">Ali Hossaini</a> and <a href="http://www.lucavigano.com/" target="_blank">Luca Viganò</a>'s short movie "<a href="https://www.nationalgallery.org.uk/national-gallery-x/the-ai-gallery" target="_blank">The First</a>" that has been released by National Gallery X. "The First" is a coproduction of <a href="https://www.tas.ac.uk/" target="_blank">UKRI TAS Hub</a>, <a href="https://rusi.org/" target="_blank">RUSI </a>and <a href="https://www.nationalgallery.org.uk/national-gallery-x" target="_blank">National Gallery X</a>. It was produced as a keynote for <a href="https://www.tas.ac.uk/bigeventscpt/trusting-machines/" target="_blank">Trusting Machines?</a>, a conference on how to develop trustworthy AI. </p><p>For the little that it may be worth, I strongly recommend the movie. Do watch also the <a href="https://vimeo.com/577412746" target="_blank">conversation</a> between National Gallery X co-director Ali Hossaini and Luca Viganò, possibly before enjoying a second viewing of the movie. You can also read the paper "<a href="https://arxiv.org/pdf/1807.06078.pdf" target="_blank">Gnirut: The Trouble With Being Born Human In An Autonomous World</a>" mentioned in that conversation.  </p><p>I fully subscribe to Luca Viganò's vision of using artistic tools to explain computer science concepts to the public, whose members will have to make use of the technological artifacts based on those concepts. Indeed, we live in a world in which technologists will increasingly have to be great humanists. IMHO, we are lucky to have people like Luca Viganò, who is also a playwright, paving the way in connecting "<a href="https://en.wikipedia.org/wiki/The_Two_Cultures" target="_blank">The Two Cultures</a>". (In case any of you is interested, I recommend Luca Viganò's  <a href="https://us02web.zoom.us/rec/play/3uNdz1F0g1JCAir0C10Y3_jVs7k6fJgrIwr5RomnzBJGTjqBMOYv6SJgD2jINwMTuBvx-CSsHclof5Vn.29XNpJA7FKbfL5dG?autoplay=true&amp;startTime=1614787555000" target="_blank">GSSI+ICE-TCS webinar</a>.)</p><p> </p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/08/the-first-movie.html"><span class="datestr">at August 11, 2021 09:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04798">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04798">Pointwise distance distributions of periodic sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Widdowson:Daniel.html">Daniel Widdowson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04798">PDF</a><br /><b>Abstract: </b>The fundamental model of a periodic structure is a periodic set of points
considered up to rigid motion or isometry in Euclidean space. The recent work
by Edelsbrunner et al defined the new isometry invariants (density functions),
which are continuous under perturbations of points and complete for generic
sets in dimension 3. This work introduces much faster invariants called higher
order Pointwise Distance Distributions (PDD). The new PDD invariants are
simpler represented by numerical matrices and are also continuous under
perturbations important for applications. Completeness of PDD invariants is
proved for distance-generic sets in any dimension, which was also confirmed by
distinguishing all 229K known molecular organic structures from the world's
largest Cambridge Structural Database. This huge experiment took only seven
hours on a modest desktop due to the proposed algorithm with a near linear or
small polynomial complexity in terms of key input sizes. Most importantly, the
above completeness allows one to build a common map of all periodic structures,
which are continuously parameterized by PDD and explicitly reconstructible from
PDD. Appendices include first tree-based maps for several thousands of real
structures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04798"><span class="datestr">at August 11, 2021 11:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04755">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04755">FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Haoyu.html">Haoyu Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zhize.html">Zhize Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Richt=aacute=rik:Peter.html">Peter Richtárik</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04755">PDF</a><br /><b>Abstract: </b>Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)
is a classical federated learning algorithm in which clients run multiple local
SGD steps before communicating their update to an orchestrating server. We
propose a new federated learning algorithm, FedPAGE, able to further reduce the
communication complexity by utilizing the recent optimal PAGE method (Li et
al., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer
communication rounds than previous local methods for both federated convex and
nonconvex optimization. Concretely, 1) in the convex setting, the number of
communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD (Karimireddy et
al.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients
(usually is very large in federated learning), $S$ is the sampled subset of
clients in each communication round, and $\epsilon$ is the target error; 2) in
the nonconvex setting, the number of communication rounds of FedPAGE is
$O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving the best-known result
$O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by
a factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\leq \sqrt{N}$. Note
that in both settings, the communication cost for each round is the same for
both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art
results in terms of communication complexity for both federated convex and
nonconvex optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04755"><span class="datestr">at August 11, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04734">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04734">Tutorial on the Robust Interior Point Method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh_S=.html">Santosh S. Vempala</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04734">PDF</a><br /><b>Abstract: </b>We give a short, self-contained proof of the interior point method and its
robust version.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04734"><span class="datestr">at August 11, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04729">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04729">Correlation Clustering Reconstruction in Semi-Adversarial Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chierichetti:Flavio.html">Flavio Chierichetti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panconesi:Alessandro.html">Alessandro Panconesi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Re:Giuseppe.html">Giuseppe Re</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trevisan:Luca.html">Luca Trevisan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04729">PDF</a><br /><b>Abstract: </b>Correlation Clustering is an important clustering problem with many
applications. We study the reconstruction version of this problem in which one
is seeking to reconstruct a latent clustering that has been corrupted by random
noise and adversarial modifications.
</p>
<p>Concerning the latter, we study a standard "post-adversarial" model, in which
adversarial modifications come after the noise, and also introduce and analyze
a "pre-adversarial" model in which adversarial modifications come before the
noise. Given an input coming from such a semi-adversarial generative model, the
goal is to reconstruct almost perfectly and with high probability the latent
clustering.
</p>
<p>We focus on the case where the hidden clusters have equal size and show the
following. In the pre-adversarial setting, spectral algorithms are optimal, in
the sense that they reconstruct all the way to the information-theoretic
threshold beyond which no reconstruction is possible. In contrast, in the
post-adversarial setting their ability to restore the hidden clusters stops
before the threshold, but the gap is optimally filled by SDP-based algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04729"><span class="datestr">at August 11, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04604">Stochastic Games with Disjunctions of Multiple Objectives (Technical Report)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Winkler:Tobias.html">Tobias Winkler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weininger:Maximilian.html">Maximilian Weininger</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04604">PDF</a><br /><b>Abstract: </b>Stochastic games combine controllable and adversarial non-determinism with
stochastic behavior and are a common tool in control, verification and
synthesis of reactive systems facing uncertainty. In this paper, we study
turn-based stochastic two-player games on graphs where the winning condition is
to satisfy at least one reachability or safety objective from a given set of
alternatives with at least some desired probability. These objectives are also
known as disjunctive queries (DQs). We present a fine-grained overview of
strategy and computational complexity and provide new lower and upper bounds
for several variants of the problem. These results extend the previous
literature on DQs significantly. We also propose a novel value iteration-style
algorithm for approximating the set of Pareto optimal thresholds for a given
DQ.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04604"><span class="datestr">at August 11, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04590">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04590">Parallel Computation of Combinatorial Symmetries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anders:Markus.html">Markus Anders</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schweitzer:Pascal.html">Pascal Schweitzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04590">PDF</a><br /><b>Abstract: </b>In practice symmetries of combinatorial structures are computed by
transforming the structure into an annotated graph whose automorphisms
correspond exactly to the desired symmetries. An automorphism solver is then
employed to compute the automorphism group of the constructed graph. Such
solvers have been developed for over 50 years, and highly efficient sequential,
single core tools are available. However no competitive parallel tools are
available for the task. We introduce a new parallel randomized algorithm that
is based on a modification of the individualization-refinement paradigm used by
sequential solvers. The use of randomization crucially enables parallelization.
We report extensive benchmark results that show that our solver is competitive
to state-of-the-art solvers on a single thread, while scaling remarkably well
with the use of more threads. This results in order-of-magnitude improvements
on many graph classes over state-of-the-art solvers. In fact, our tool is the
first parallel graph automorphism tool that outperforms current sequential
tools.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04590"><span class="datestr">at August 11, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04588">Distinguishing classes of intersection graphs of homothets or similarities of two convex disks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abrahamsen:Mikkel.html">Mikkel Abrahamsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walczak:Bartosz.html">Bartosz Walczak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04588">PDF</a><br /><b>Abstract: </b>For smooth convex disks $A$, i.e., convex compact subsets of the plane with
non-empty interior, we classify the classes $G^{\text{hom}}(A)$ and
$G^{\text{sim}}(A)$ of intersection graphs that can be obtained from homothets
and similarities of $A$, respectively. Namely, we prove that
$G^{\text{hom}}(A)=G^{\text{hom}}(B)$ if and only if $A$ and $B$ are affine
equivalent, and $G^{\text{sim}}(A)=G^{\text{sim}}(B)$ if and only if $A$ and
$B$ are similar.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04588"><span class="datestr">at August 11, 2021 11:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04587">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04587">On Learning and Testing Decision Tree</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bshouty:Nader_H=.html">Nader H. Bshouty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haddad=Zaknoon:Catherine_A=.html">Catherine A. Haddad-Zaknoon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04587">PDF</a><br /><b>Abstract: </b>In this paper, we study learning and testing decision tree of size and depth
that are significantly smaller than the number of attributes $n$.
</p>
<p>Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms
with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that
distinguish between functions that are decision trees of size $s$ from
functions that are $\epsilon$-far from any decision tree of size
$\phi(s,1/\epsilon)$, for some function $\phi &gt; s$. The best known result is
the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that
gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give
a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$.
</p>
<p>Moreover, we study the testability of depth-$d$ decision tree and give a {\it
distribution free} tester that distinguishes between depth-$d$ decision tree
and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In
particular, for decision trees of size $s$, the above result holds in the
distribution-free model when the tree depth is $O(\log(s/\epsilon))$.
</p>
<p>We also give other new results in learning and testing of size-$s$ decision
trees and depth-$d$ decision trees that follow from results in the literature
and some results we prove in this paper.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04587"><span class="datestr">at August 11, 2021 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04566">Algorithm Engineering for Cut Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Noe:Alexander.html">Alexander Noe</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04566">PDF</a><br /><b>Abstract: </b>Graphs are a natural representation of data from various contexts, such as
social connections, the web, road networks, and many more. In the last decades,
many of these networks have become enormous, requiring efficient algorithms to
cut networks into smaller, more readily comprehensible blocks. In this work, we
aim to partition the vertices of a graph into multiple blocks while minimizing
the number of edges that connect different blocks. There is a multitude of cut
or partitioning problems that have been the focus of research for multiple
decades. This work develops highly-efficient algorithms for the (global)
minimum cut problem, the balanced graph partitioning problem and the
multiterminal cut problem. All of these algorithms are efficient in practice
and freely available for use.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04566"><span class="datestr">at August 11, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04564">Random Rank-Based, Hierarchical or Trivial: Which Dynamic Graph Algorithm Performs Best in Practice?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Noe:Alexander.html">Alexander Noe</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04564">PDF</a><br /><b>Abstract: </b>Fully dynamic graph algorithms that achieve polylogarithmic or better time
per operation use either a hierarchical graph decomposition or random-rank
based approach. There are so far two graph properties for which efficient
algorithms for both types of data structures exist, namely fully dynamic (Delta
+ 1) coloring and fully dynamic maximal matching. In this paper we present an
extensive experimental study of these two types of algorithms for these two
problems together with very simple baseline algorithms to determine which of
these algorithms are the fastest. Our results indicate that the data structures
used by the different algorithms dominate their performance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04564"><span class="datestr">at August 11, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04563">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04563">The Parameterized Complexity of Finding MinimumBounded Chains</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blaser:Nello.html">Nello Blaser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brun:Morten.html">Morten Brun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salbu:Lars_M=.html">Lars M. Salbu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V=aring=gset:Erlend_Raa.html">Erlend Raa Vågset</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04563">PDF</a><br /><b>Abstract: </b>Finding the smallest $d$-chain with a specific $(d-1)$-boundary in a
simplicial complex is known as the \textsc{Minimum Bounded Chain} (MBC$_d$)
problem. The MBC$_d$ problem is NP-hard for all $d\geq 2$. In this paper, we
prove that it is also W[1]-hard for all $d\geq 2$, if we parameterize the
problem by solution size. We also give an algorithm solving the MBC$_1$ problem
in polynomial time and introduce and implemented two fixed parameter tractable
(FPT) algorithms solving the MBC$_d$ problem for all $d$. The first algorithm
is a generalized version of Dijkstra's algorithm and is parameterized by
solution size and coface degree. The second algorithm is a dynamic programming
approach based on treewidth, which has the same runtime as a lower bound we
prove under the exponential time hypothesis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04563"><span class="datestr">at August 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04559">Symmetries of discrete curves and point clouds via trigonometric interpolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bizzarri:Michal.html">Michal Bizzarri</a>, Miroslav Lávička, Jan Vršek <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04559">PDF</a><br /><b>Abstract: </b>We formulate a simple algorithm for computing global exact symmetries of
closed discrete curves in plane. The method is based on a suitable
trigonometric interpolation of vertices of the given polyline and consequent
computation of the symmetry group of the obtained trigonometric curve. The
algorithm exploits the fact that the introduced unique assigning of the
trigonometric curve to each closed discrete curve commutes with isometries. For
understandable reasons, an essential part of the paper is devoted to
determining rotational and axial symmetries of trigonometric curves. We also
show that the formulated approach can be easily applied on nonorganized clouds
of points. A functionality of the designed detection method is presented on
several examples.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04559"><span class="datestr">at August 11, 2021 11:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04520">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04520">Fast and Fair Lock-Free Locks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=David:Naama.html">Naama Ben-David</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blelloch:Guy_E=.html">Guy E. Blelloch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04520">PDF</a><br /><b>Abstract: </b>We present a randomized approach for lock-free locks with strong bounds on
time and fairness in a context in which any process can be arbitrarily delayed.
Our approach supports a tryLock operation that is given a set of locks, and
code to run when all the locks are acquired. Given an upper bound $\kappa$
known to the algorithm on the point contention for a tryLock it will succeed in
acquiring its locks and running the code with probability at least $1/\kappa$.
It is thus fair. If the algorithm does not know the bound $\kappa$, we present
a variant that can guarantee a probability of at least $1/\kappa\log\kappa$ of
success. Furthermore, if the maximum step complexity for the code in any lock
is $T$, and the point contentions are constant, the attempt will take $O(T)$
steps. The attempts are independent, thus if the tryLock is repeatedly retried
on failure, it will succeed in $O(T)$ expected steps, and with high probability
in not much more. Importantly, however, retrying is not mandatory, and a
process may choose to execute different code upon failure.
</p>
<p>We assume an oblivious adversarial scheduler, which does not make decisions
based on the operations, but can predetermine any schedule for the processes,
which is unknown to our algorithm. Furthermore, to account for applications
that change their future requests based on the results of previous lock
attempts, we strengthen the adversary by allowing decisions of the start times
and lock sets of tryLock attempts to be made adaptively, given the history of
the execution so far.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04520"><span class="datestr">at August 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04458">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04458">A Tight Analysis of Slim Heaps and Smooth Heaps</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinnamon:Corwin.html">Corwin Sinnamon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tarjan:Robert_E=.html">Robert E. Tarjan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04458">PDF</a><br /><b>Abstract: </b>The smooth heap and the closely related slim heap are recently invented
self-adjusting implementations of the heap (priority queue) data structure. We
analyze the efficiency of these data structures. We obtain the following
amortized bounds on the time per operation: $O(1)$ for make-heap, insert,
find-min, and meld; $O(\log\log n)$ for decrease-key; and $O(\log n)$ for
delete-min and delete, where $n$ is the current number of items in the heap.
These bounds are tight not only for smooth and slim heaps but for any heap
implementation in Iacono and \"{O}zkan's pure heap model, intended to capture
all possible "self-adjusting" heap implementations. Slim and smooth heaps are
the first known data structures to match Iacono and \"{O}zkan's lower bounds
and to satisfy the constraints of their model. Our analysis builds on Pettie's
insights into the efficiency of pairing heaps, a classical self-adjusting heap
implementation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04458"><span class="datestr">at August 11, 2021 10:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04422">A $D$-competitive algorithm for the Multilevel Aggregation Problem with Deadlines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McMahan:Jeremy.html">Jeremy McMahan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04422">PDF</a><br /><b>Abstract: </b>In this paper, we consider the multi-level aggregation problem with deadlines
(MLAPD) previously studied by Bienkowski et al. (2015), Buchbinder et al.
(2017), and Azar and Touitou (2019). This is an online problem where the
algorithm services requests arriving over time and can save costs by
aggregating similar requests. Costs are structured in the form of a rooted
tree. This problem has applications to many important areas such as
multicasting, sensor networks, and supply-chain management. In particular, the
TCP-acknowledgment problem, joint-replenishment problem, and assembly problem
are all special cases of the delay version of the problem.
</p>
<p>We present a $D$-competitive algorithm for MLAPD. This beats the
$6(D+1)$-competitive algorithm given in Buchbinder et al. (2017). Our approach
illuminates key structural aspects of the problem and provides an algorithm
that is simpler to implement than previous approaches. We also give improved
competitive ratios for special cases of the problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04422"><span class="datestr">at August 11, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04416">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04416">A Parallel Algorithm for Minimum Cost Submodular Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ran:Yingli.html">Yingli Ran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Zhao.html">Zhao Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04416">PDF</a><br /><b>Abstract: </b>In a minimum cost submodular cover problem (MinSMC), given a monotone
non-decreasing submodular function $f\colon 2^V \rightarrow \mathbb{Z}^+$, a
cost function $c: V\rightarrow \mathbb R^{+}$, an integer $k\leq f(V)$, the
goal is to find a subset $A\subseteq V$ with the minimum cost such that
$f(A)\geq k$. MinSMC has a lot of applications in machine learning and data
mining. In this paper, we design a parallel algorithm for MinSMC which obtains
a solution with approximation ratio at most
$\frac{H(\min\{\Delta,k\})}{1-5\varepsilon}$ with probability $1-3\varepsilon$
in $O(\frac{\log m\log n\log^2 mn}{\varepsilon^4})$ rounds, where
$\Delta=\max_{v\in V}f(v)$, $H(\cdot)$ is the Hamornic number, $n=f(V)$,
$m=|V|$ and $\varepsilon$ is a constant in $(0,\frac{1}{5})$. This is the first
paper obtaining a parallel algorithm for the weighted version of the MinSMC
problem with an approximation ratio arbitrarily close to $H(\min\{\Delta,k\})$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04416"><span class="datestr">at August 11, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.04297">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.04297">Small Parsimony for Natural Genomes in the DCJ-Indel Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Doerr:Daniel.html">Daniel Doerr</a>, Cedric Chauve <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.04297">PDF</a><br /><b>Abstract: </b>Reconstructing ancestral gene orders is an important step towards
understanding genome evolution. The Small Parsimony Problem (SPP) has been
extensively studied in this regard. The problem aims at finding the gene orders
at internal nodes of a given phylogenetic tree such that the overall genome
rearrangement distance along the tree branches is minimized. However, this
problem is intractable in most genome rearrangement models, especially when
gene duplication and loss are considered. In this work, we describe an Integer
Linear Program algorithm to solve the SPP for natural genomes, i.e., genomes
that contain conserved, unique, and duplicated markers. The evolutionary model
that we consider is the DCJ-indel model that includes the Double-Cut and Join
rearrangement operation and the insertion and deletion of genome segments. We
evaluate our algorithm on simulated data and show that it is able to
reconstruct very efficiently and accurately ancestral gene orders in a very
comprehensive evolutionary model.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.04297"><span class="datestr">at August 11, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14323">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14323">Improved Reconstruction of Random Geometric Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dani:Varsha.html">Varsha Dani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=iacute=az:Josep.html">Josep Díaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hayes:Thomas_P=.html">Thomas P. Hayes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moore:Cristopher.html">Cristopher Moore</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14323">PDF</a><br /><b>Abstract: </b>Embedding graphs in a geographical or latent space, i.e., inferring locations
for vertices in Euclidean space or on a smooth submanifold, is a common task in
network analysis, statistical inference, and graph visualization. We consider
the classic model of random geometric graphs where $n$ points are scattered
uniformly in a square of area $n$, and two points have an edge between them if
and only if their Euclidean distance is less than $r$. The reconstruction
problem then consists of inferring the vertex positions, up to symmetry, given
only the adjacency matrix of the resulting graph. We give an algorithm that, if
$r=n^\alpha$ for $\alpha &gt; 0$, with high probability reconstructs the vertex
positions with a maximum error of $O(n^\beta)$ where $\beta=1/2-(4/3)\alpha$,
until $\alpha \ge 3/8$ where $\beta=0$ and the error becomes $O(\sqrt{\log
n})$. This improves over earlier results, which were unable to reconstruct with
error less than $r$. Our method estimates Euclidean distances using a hybrid of
graph distances and short-range estimates based on the number of common
neighbors. We sketch proofs that our results also apply on the surface of a
sphere, and (with somewhat different exponents) in any fixed dimension.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14323"><span class="datestr">at August 11, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/08/10/relandscaping">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/08/10/relandscaping.html">Relandscaping</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>We recently redid our front yard, after spending too long with a boring flat weedy mostly-unused and water-thirsty lawn and after our neighbor took out the only part of it that we actually cared for, a liquidambar tree between our yards. This week’s WADS/CCCG conference, with its four-hour time difference from Pacific, gave me an excuse to catch it in the early morning light:</p>

<p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/frontyard/frontyard-m.jpg" style="border-style: solid; border-color: black;" alt="My house with its old front lawn" /></p>

<p>I don’t have a photo of the old front, so for comparison here’s a similar angle snagged from Google Street View.</p>

<p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="My house with its old front lawn" src="https://www.ics.uci.edu/~eppstein/pix/frontyard/lawn.jpg" /></p>

<p>More for my own later benefit than because I imagine anyone else cares:</p>

<p>The frontmost tree is a <a href="https://en.wikipedia.org/wiki/Parkinsonia_florida">palo verde</a>; a neighbor across the street has a much bigger one, a little messy but quite pretty, especially when covered with its yellow flowers. The tree in the back corner with the blue bistro table under it is some kind of mesquite, I think maybe a black mesquite; the flagstone path makes a loop around it. The two tallest bushes, forming a quadrilateral with the two trees, are <a href="https://en.wikipedia.org/wiki/Feijoa_sellowiana">pineapple guavas / feijoas</a>.</p>

<p>Behind the palo verde and against the house are some <a href="https://en.wikipedia.org/wiki/Kangaroo_paw">kangaroo paws</a>. The closest medium-sized bush on the bottom right (and elsewhere) is <a href="https://en.wikipedia.org/wiki/Salvia_yangii">Russian sage</a>. There are also some <a href="https://en.wikipedia.org/wiki/Buddleja_davidii">butterfly bushes</a>, and several other low bushes and flowers whose names I already lost track of or didn’t get. I think the tall bunchgrass may be a <a href="https://en.wikipedia.org/wiki/Lomandra">lomandra</a>, and the low blue ones <a href="https://en.wikipedia.org/wiki/Festuca_glauca">blue fescue</a>.</p>

<p>Lining the path to the door is <a href="https://en.wikipedia.org/wiki/Curio_repens">senecio / blue chalksticks</a>, and there’s more of it around the base of the mesquite. The groundcover in front is <a href="https://en.wikipedia.org/wiki/Dymondia">dymondia / silver carpet</a> (well, and some crabgrass and spurge, but let’s not count that), with some <a href="https://11011110.github.io/blog/2021/08/10/Myoporum parvifolium">myoporum parvifolium</a> behind it (the low spreading groundcover with white flowers); there’s more myoporum barely visible near the porch bench.</p>

<p>The neighbor ended up replacing the liquidambar (at the far left of the Street View shot) with a <a href="https://11011110.github.io/blog/2021/08/10/Podocarpus henkelii">podocarpus henkelii</a> and some bottlebrushes.</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/08/10/relandscaping.html"><span class="datestr">at August 10, 2021 09:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19041">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/">P vs NP Proof Claims</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
Ken Ribet once was sent a freebie book that he looked at and decided he didn’t want, so took it to a second hand bookstore on his lunch break, sold it, and bought lunch with the proceeds. On the way back to the math department he realized he’d turned theorems into coffee.<br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p></p><p>
</p><p></p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/dijkgraafias/" rel="attachment wp-att-19043"><img width="120" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/DijkgraafIAS.jpg?resize=120%2C144&amp;ssl=1" class="alignright size-full wp-image-19043" height="144" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">IAS <a href="https://www.ias.edu/scholars/dijkgraaf">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Robbert Dijkgraaf is a mathematical physicist and the current Director of the Institute for Advanced Study in Princeton. He is also a down-to-earth communicator of mathematics and a <a href="https://www.ias.edu/news/dijkgraaf-blackhole-mural">spacy</a> surrealist artist. He wrote a guest <a href="https://www.quantamagazine.org/the-subtle-art-of-the-mathematical-conjecture-20190507/">column</a> for <em>Quanta</em> two years ago titled, “The Subtle Art of the Mathematical Conjecture.”</p>
<p>
Today I talk again about what I feel is a meta-error in all the claims to resolve our favorite conjecture, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" />. Or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" /> if you are so inclined.</p>
<p>
I have an issue with them that goes beyond well-noted <a href="https://www.scottaaronson.com/blog/?p=458">advice</a> by Scott Aaronson on how to tell claimed proofs of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" /> are wrong. It is not about whether they are incorrect. They all are incorrect. And I believe that they will continue to be so into the future.</p>
<p>
My problem with these claims is: they are always all or nothing. </p>
<p>
The claims never improve what we know about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> versus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />. They always resolve the entire question. There is no partial result, no improvement of what we know. They always solve the conjecture and are ready to get the $1,000,000 dollars. Of course the prize money is not in any jeopardy. </p>
<p>
</p><p></p><h2> Climbing a Mountain </h2><p></p>
<p></p><p>
This is where Dijkgraaf’s article comes in. He talks first about mountain climbing (as we have <a href="https://rjlipton.wpcomstaging.com/2010/02/02/climbing-mountains-and-proving-theorems/">also</a> <a href="https://rjlipton.wpcomstaging.com/2012/12/08/mounting-or-solving-open-problems/">done</a>) rather than art:</p>
<blockquote><p><b> </b> <em> Mountain climbing is a beloved metaphor for mathematical research. … [T]he role of these highest peaks is played by the great conjectures—sharply formulated statements that are most likely true but for which no conclusive proof has yet been found.</em></p><em>
</em><p><em>
The highest summits are not conquered in a single effort. Climbing expeditions carefully lay out base camps and fixed ropes, then slowly work their way to the peak. Similarly, in mathematics one often needs to erect elaborate structures to attack a major problem. A direct assault is seen as foolish and naive. These auxiliary mathematical constructions can sometimes take centuries to build and in the end often prove to be more valuable than the conquered theorem itself. The scaffold then becomes a permanent addition to the architecture of mathematics. </em>
</p></blockquote>
<p></p><p>
What strikes me and Ken about the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> versus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> claims we see so often is that they try to reach the summit in one bound. There is no scaffolding, no rope, no new handhold. The reader learns little.</p>
<p>
The term that most needs expounding is <em>base camp</em>. A base camp is not at the bottom. The main <a href="https://en.wikipedia.org/wiki/Everest_base_camps">base camps</a> for Mount Everest are halfway up to the summit. Getting to a base camp takes substantial work by itself. <em>Building</em> a base camp certainly does. But getting to one is essential. This is what is missing for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> versus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />.</p>
<p>
</p><p></p><h2> P&lt;NP Base Camps </h2><p></p>
<p></p><p>
Let’s look at <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" />. Suppose you claim that you can show that CLIQUE requires super polynomial time. This is what you need to do to prove <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" />. This is way beyond anything we can imagine proving. </p>
<p>
Suppose rather you claimed that CLIQUE requires <img src="https://s0.wp.com/latex.php?latex=%7Bm%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m^2}" class="latex" /> deterministic time where <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m}" class="latex" /> is the number of edges. This would be the best result ever on the difficulty of CLIQUE. It would easily be the best paper in complexity theory in decades. Would win prizes of all kinds. </p>
<p>
It is even worse. If one could prove that CLIQUE is not in deterministic time 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5Clog%5Clog%5Clog+m+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  m\log\log\log m " class="latex" /></p>
<p>that would also be a major result. Forget about proving a lower bound of 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5E%7B1000%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  m^{1000} " class="latex" /></p>
<p>and more that is needed to solve <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" />. Just the above would be major.</p>
<p>
If we skirt around some technical issues with time on Turing machines, we can pitch our camp right at going beyond linear time. Or certainly linear size circuits. Nobody knows a language in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> that does not have linear size circuits. Proving one would bring enough renown for anyone.</p>
<p>
</p><p></p><h2> P=NP Base Camps </h2><p></p>
<p></p><p>
Let’s look at <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" />. Most regard this as the far side of the mountain but please bear with us to the end. Suppose you claim that CLIQUE is in polynomial time. </p>
<p>
The usual paper of this type claims it is doable by some straightforward polynomial time algorithm. The method might use linear programming in some standard manner. This might lead to a <em>practical</em> algorithm, but none of those has ever been observed in the wild. Even worse, any proof that CLIQUE can be resolved in time 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7BC%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n^{C} " class="latex" /></p>
<p>would be also the best result ever on this problem. This applies even if <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" /> is an unknown constant, or is equal to some astronomically large value. Think <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a> or the <a href="https://en.wikipedia.org/wiki/Skewes%27s_number">Skewes</a> number: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+10%5E%7B10%5E%7B10%5E%7B502%7D%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle 10^{10^{10^{502}}} " class="latex" /></p>
<p>Nor do we need <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" /> to be constant. Say it is <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\log n)}" class="latex" /> or some polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log n}" class="latex" />. This would be a <em>quasi-polynomial</em> time algorithm. Here a really big campsite was built by László Babai, who <a href="https://www.quantamagazine.org/graph-isomorphism-vanquished-again-20170114/">proved</a> that Graph Isomorphism is in quasi-polynomial time. His <a href="http://people.cs.uchicago.edu/~laci/17groups/version2.1.pdf">paper</a> has a wealth of ideas that might be extended.</p>
<p>
But what really might be attractive about this side is that you can make progress without “shaking the Earth.” These results would be in the direction of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" /> but not opposed by as many factors:</p>
<ul>
<li>
<em>Refute the Strong Exponential Time Hypothesis</em> (SETH). That is, find an algorithm in time <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%28cn%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{O(cn)}}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &lt; 1}" class="latex" /> for CNF-SAT. We have written about SETH <a href="https://rjlipton.wpcomstaging.com/2015/06/01/puzzling-evidence/">here</a>. <p></p>
</li><li>
<em>Refute the Unique Games Conjecture</em> (UGC). Unlike SETH, this technically does not imply <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}.}" class="latex" /> But refuting it does reduce the reach of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-hardness. A large special case of UGC was, however, <a href="https://ieeexplore.ieee.org/document/8555140">proved</a> three years ago.
</li></ul>
<p>
Or prove that they cannot both be true simultaneously. My old <a href="https://rjlipton.wpcomstaging.com/2010/05/05/unique-games-a-three-act-play/">post</a> on UGC covered a sense in which there is no “SETH for UGC.” </p>
<p>
</p><p></p><h2> But … </h2><p></p>
<p></p><p>
The trouble with our insight is that in the past, sometimes a full conjecture has been solved. That is, partial progress did not happen first—the mountain was scaled in one go. Or at least a lot of it, from a relatively low base camp. For <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" /> there is an essence of “polynomial” versus “exponential” that is sharply defined in other ways, for instance Mikhail Gromov’s theorem about growth rates in groups, which we wrote about <a href="https://rjlipton.wpcomstaging.com/2013/06/20/three-theorems-about-growth/">here</a>.</p>
<p>
Ken and I have differed on how long and steep and sudden the final ascent was for the <a href="https://en.wikipedia.org/wiki/Four_color_theorem">Four Color Theorem</a> and <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem">Fermat’s Last Theorem</a> (FLT). The proof of the former by Kenneth Appel and Wolfgang Haken was a watershed for its use of computers, but drew on ideas that had gone through the non-computer proofs-and-refutations process. Andrew Wiles’s announcement of FLT was a shock even with a three-day buildup of his lectures at a conference in 1993 having hinted it to several attendees. But he drew on partial progress that had been ramped up by Ribet and others since the mid-1980s.</p>
<p>
Maybe if Évariste Galois had beaten Niels Abel to showing the unsolvability of the quintic, his invention of group theory for the proof used today would have been a single bound. But Abel got a big lift from Paolo Ruffini’s 500 pages of work in 1799. (Évariste is the same name as <a href="https://appellationmountain.net/baby-name-of-the-day-everest/">Everest</a>, go figure.)</p>
<p>
The proof of the Boolean Sensitivity Conjecture two years ago by Hao Huang was short and sudden. But along lines remarked also in Dijkgraaf’s article, perhaps it was “more of a foothill.” Or maybe a base camp for harder problems, such as improving the upper bound from quartic to cubic or quadratic, as we discussed <a href="https://rjlipton.wpcomstaging.com/2019/07/12/tools-and-sensitivity/">here</a> and <a href="https://rjlipton.wpcomstaging.com/2019/07/25/discrepancy-games-and-sensitivity/">here</a>.</p>
<p>
This leads Ken into a historical daydream, taking over from here.</p>
<p>
</p><p></p><h2> A Fermat Fantasy </h2><p></p>
<p></p><p>
Pierre de Fermat famously <a href="https://www.maths-et-tiques.fr/index.php/detentes/la-conjecture-de-fermat">wrote</a> the following in French in the margin of his copy of the famous book on arithmetic by Diophantus:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de deux cubes, une puissance quatrième n’est jamais la somme de deux puissances quatriémes et plus généralement aucune puissance supérieure à 2 n’est la somme de deux puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, mais la marge est trop étroite pour la contenir. </em>
</p></blockquote>
<p></p><p>
I (Ken) think he could just as easily have written the following—and in place of the margin being too narrow, he could have given a more reasonable excuse, one I know all too well:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de moins que trois cubes, une puissance quatrième n’est jamais la somme de moins que quatre puissances quatriémes, et plus généralement aucune puissance n’est la somme de un moindre nombre de puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, que je rédigerai après avoir traité onze nouveaux cas de triche aux échecs en ligne. </em>
</p></blockquote>
<p></p><p>
The stronger statement here is that no cube can be a nontrivial sum of fewer than three cubes (such as <img src="https://s0.wp.com/latex.php?latex=%7B6%5E3+%3D+3%5E3+%2B+4%5E3+%2B+5%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6^3 = 3^3 + 4^3 + 5^3}" class="latex" />), no fourth power a sum of fewer than four other like powers, and so on. This also was a hallowed conjecture that stood for centuries, <a href="https://en.wikipedia.org/wiki/Euler's_sum_of_powers_conjecture">named for</a> the giant Leonhard Euler no less. Well, if Pierre had just changed a few of his words, then <em>this</em> is what we would have known as FLT. Call it EFLT. It could have been just as worthy. That there were reservations known to Euler, even as he lent his name to it, might have made it all the more Olympian. </p>
<p>
Then what we actually know as FLT would have been a hard-work base camp for EFLT. Would we have seen the vast number of unsuccessful FLT proofs directed at EFLT instead? By people claiming to climb this peak in one bound—without first trying to prove that a sum of <b>just two</b> <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-th powers cannot be a higher <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-th power, for all <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 3}" class="latex" />? Well, there would have been a problem with that, one we <a href="https://rjlipton.wpcomstaging.com/2015/09/03/open-problems-that-might-be-easy/">discussed</a> in connection with solutions that might be easy after all:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++144%5E5+%26%3D%26+27%5E5+%2B+84%5E5+%2B+110%5E5+%2B+133%5E5.%5C%5C+20615673%5E4+%26%3D%26+2682440%5E4+%2B+15365639%5E4+%2B+18796760%5E4.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  144^5 &amp;=&amp; 27^5 + 84^5 + 110^5 + 133^5.\\ 20615673^4 &amp;=&amp; 2682440^4 + 15365639^4 + 18796760^4. \end{array} " class="latex" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do you have your own issues with these claimed proofs? Or, do you see other cases of people having suddenly scaled a mountain in one stride?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/"><span class="datestr">at August 10, 2021 07:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5706">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5706">Yet more mistakes in papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In my <a href="https://www.scottaaronson.com/blog/?p=5675">last post</a>, I came down pretty hard on the blankfaces: people who relish their power to persist in easily-correctable errors, to the detriment of those subject to their authority.  The sad truth, though, is that <em>I</em> don’t obviously do better than your average blankface in my ability to resist falsehoods on early encounter with them.  As one of many examples that readers of this blog might know, I didn’t think covid seemed like a big deal in early February 2020—although by mid-to-late February 2020, I’d repented of my doofosity.  If I have <em>any</em> tool with which to unblank my face, then it’s only my extreme self-consciousness when confronted with evidence of my own stupidities—the way I’ve trained myself over decades in science to see error-correction as a or even <em>the</em> fundamental virtue.</p>



<p>Which brings me to today’s post.  Continuing what’s become a <em>Shtetl-Optimized</em> tradition—see <a href="https://www.scottaaronson.com/blog/?p=2072">here from 2014</a>, <a href="https://www.scottaaronson.com/blog/?p=2854">here from 2016</a>, <a href="https://www.scottaaronson.com/blog/?p=3256">here from 2017</a>—I’m going to fess up to two serious mistakes in research papers on which I was a coauthor.</p>



<hr class="wp-block-separator" />



<p>In 2015, Andris Ambainis and I had a STOC paper entitled <a href="https://arxiv.org/abs/1411.5729">Forrelation: A Problem that Optimally Separates Quantum from Classical Computing</a>.  We gave two main results there:</p>



<ol><li>A Ω((√N)/log(N)) lower bound on the randomized query complexity of my “Forrelation” problem, which was known to be solvable with only a single quantum query.</li><li>A proposed way to take any k-query quantum algorithm that queries an N-bit string, and simulate it using only O(N<sup>1-1/2k</sup>) classical randomized queries.</li></ol>



<p>Later, <a href="https://arxiv.org/abs/2008.07003">Bansal and Sinha</a> and independently <a href="https://arxiv.org/abs/2008.10223">Sherstov, Storozhenko, and Wu</a> showed that a k-query generalization of Forrelation, which I’d also defined, requires ~Ω(N<sup>1-1/2k</sup>) classical randomized queries, in line with my and Andris’s conjecture that k-fold Forrelation <em>optimally</em> separates quantum and classical query complexities.</p>



<p>A couple months ago, alas, my former grad school officemate <a href="https://www.cse.cuhk.edu.hk/~andrejb/">Andrej Bogdanov</a>, along with Tsun Ming Cheung and Krishnamoorthy Dinesh, emailed me and Andris to say that they’d discovered an error in result 2 of our paper (result 1, along with the Bansal-Sinha and Sherstov-Storozhenko-Wu extensions of it, remained fine).  So, adding our own names, we’ve now posted a <a href="https://eccc.weizmann.ac.il/report/2021/115/">preprint on ECCC</a> that explains the error, while also showing how to recover our result for the special case k=1: that is, any 1-query quantum algorithm really can be simulated using only O(√N) classical randomized queries.</p>



<p>Read the preprint if you really want to know the details of the error, but to summarize it in my words: Andris and I used a trick that we called “variable-splitting” to handle variables that have way more influence than average on the algorithm’s acceptance probability.  Alas, variable-splitting fails to take care of a situation where there are a bunch of variables that are non-influential individually, but that on some unusual input string, can “conspire” in such a way that their signs all line up and their contribution overwhelms those from the other variables.  A single mistaken inequality fooled us into thinking such cases were handled, but an explicit counterexample makes the issue obvious.</p>



<p>I <em>still</em> conjecture that my original guess was right: that is, I conjecture that any problem solvable with k quantum queries is solvable with O(N<sup>1-1/2k</sup>) classical randomized queries, so that k-fold Forrelation is the extremal example, and so that no problem has constant quantum query complexity but linear randomized query complexity.  More strongly, I reiterate the conjecture that any bounded degree-d real polynomial, p:{0,1}<sup>N</sup>→[0,1], can be approximated by querying only O(N<sup>1-1/d</sup>) input bits drawn from some suitable distribution.  But proving these conjectures, if they’re true, will require a new algorithmic idea.</p>



<hr class="wp-block-separator" />



<p>Now for the second <em>mea culpa</em>.  Earlier this year, my student Sabee Grewal and I posted a short preprint on the arXiv entitled <a href="https://arxiv.org/abs/2102.10458">Efficient Learning of Non-Interacting Fermion Distributions</a>.  In it, we claimed to give a classical algorithm for reconstructing any “free fermionic state” |ψ⟩—that is, a state of n identical fermionic particles, like electrons, each occupying one of m&gt;n possible modes, that can be produced using only “fermionic beamsplitters” and no interaction terms—and for doing so in polynomial time and using a polynomial number of samples (i.e., measurements of where all the fermions are, given a copy of |ψ⟩).  Alas, after trying to reply to confused comments from readers and reviewers (albeit, none of them <em>exactly</em> putting their finger on the problem), Sabee and I were able to figure out that we’d done no such thing.</p>



<p>Let me explain the error, since it’s actually really interesting.  In our underlying problem, we’re trying to find a collection of unit vectors, call them |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩, in C<sup>n</sup>.  Here, again, n is the number of fermions and m&gt;n is the number of modes.  By measuring the “2-mode correlations” (i.e., the probability of finding a fermion in both mode i and mode j), we can figure out the approximate value of |⟨v<sub>i</sub>|v<sub>j</sub>⟩|—i.e., the absolute value of the inner product—for any i≠j.  From that information, we want to recover |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ themselves—or rather, their relative configuration in n-dimensional space, isometries being irrelevant.</p>



<p>It seemed to me and Sabee that, if we knew ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for all i≠j, then we’d get linear equations that iteratively constrained each |v<sub>j</sub>⟩ in terms of ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for j&lt;i, so all we’d need to do is solve those linear systems, and then (crucially, and this was the main work we did) show that the solution would be <em>robust</em> with respect to small errors in our estimates of each ⟨v<sub>i</sub>|v<sub>j</sub>⟩.  It seemed further to us that, while it was true that the measurements only revealed |⟨v<sub>i</sub>|v<sub>j</sub>⟩| rather than ⟨v<sub>i</sub>|v<sub>j</sub>⟩ itself, the “phase information” in ⟨v<sub>i</sub>|v<sub>j</sub>⟩ was manifestly irrelevant, as it in any case depended on the irrelevant global phases of |v<sub>i</sub>⟩ and |v<sub>j</sub>⟩ themselves.</p>



<p>Alas, it turns out that the phase information <em>does</em> matter.  As an example, suppose I told you only the following about three unit vectors |u⟩,|v⟩,|w⟩ in R<sup>3</sup>:</p>



<p>|⟨u|v⟩| = |⟨u|w⟩| = |⟨v|w⟩| = 1/2.</p>



<p>Have I thereby determined these vectors up to isometry?  Nope!  In one class of solution, all three vectors belong to the same plane, like so:</p>



<p>|u⟩=(1,0,0),<br />|v⟩=(1/2,(√3)/2,0),<br />|w⟩=(-1/2,(√3)/2,0).</p>



<p>In a completely different class of solution, the three vectors <em>don’t</em> belong to the same plane, and instead look like three edges of a tetrahedron meeting at a vertex:</p>



<p>|u⟩=(1,0,0),<br />|v⟩=(1/2,(√3)/2,0),<br />|w⟩=(1/2,1/(2√3),√(2/3)).</p>



<p>These solutions correspond to different sign choices for |⟨u|v⟩|, |⟨u|w⟩|, and |⟨v|w⟩|—choices that <em>collectively</em> matter, even though each of them is individually irrelevant.</p>



<p>It follows that, even in the special case where the vectors are all real, the 2-mode correlations are <em>not </em>enough information to determine the vectors’ relative positions.  (Well, it takes some more work to convert this to a counterexample that could actually arise in the fermion problem, but that work can be done.)  And alas, the situation gets even gnarlier when, as for us, the vectors can be complex.</p>



<p>Any possible algorithm for our problem will have to solve a system of <em>non</em>linear equations (albeit, a massively overconstrained system that’s guaranteed to have a solution), and it will have to use <em>3-mode</em> correlations (i.e., statistics of <em>triples</em> of fermions), and quite possibly 4-mode correlations and above.</p>



<p>But now comes the good news!  Googling revealed that, for reasons having nothing to do with fermions or quantum physics, problems <em>extremely</em> close to ours had already been studied in classical machine learning.  The key term here is <a href="https://en.wikipedia.org/wiki/Determinantal_point_process">“Determinantal Point Processes”</a> (DPPs).  A DPP is a model where you specify an m×m matrix A (typically symmetric or Hermitian), and then the probabilities of various events are given by the determinants of various principal minors of A.  Which is <em>precisely</em> what happens with fermions!  In terms of the vectors |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ that I was talking about before, to make this connection we simply let A be the m×m <em>covariance matrix</em>, whose (i,j) entry equals ⟨v<sub>i</sub>|v<sub>j</sub>⟩.</p>



<p>I first learned of this remarkable correspondence between fermions and DPPs a decade ago, from a talk on DPPs that <a href="https://www.cs.washington.edu/people/faculty/taskar">Ben Taskar</a> gave at MIT.  Immediately after the talk, I made a mental note that Taskar was a rising star in theoretical machine learning, and that his work would probably be relevant to me in the future.  While researching this summer, I was devastated to learn that Taskar died of heart failure in 2013, in his mid-30s and only a couple of years after I’d heard him speak.</p>



<p>The most relevant paper for me and Sabee was called <a href="https://www.alexkulesza.com/pubs/spmap_laa14.pdf">An Efficient Algorithm for the Symmetric Principal Minor Assignment Problem</a>, by Rising, Kulesza, and Taskar.  Using a combinatorial algorithm based on minimum spanning trees and chordless cycles, this paper <em>nearly</em> solves our problem, except for two minor details:</p>



<ol><li>It doesn’t do an error analysis, and</li><li>It considers complex <em>symmetric</em> matrices, whereas our matrix A is <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a> (i.e., it equals its <em>conjugate</em> transpose, not its transpose).</li></ol>



<p>So I decided to email <a href="https://www.alexkulesza.com/">Alex Kulezsa</a>, one of Taskar’s surviving collaborators who’s now a research scientist at Google NYC, to ask his thoughts about the Hermitian case.  Alex kindly replied that they’d been meaning to study that case—a reviewer had even asked about it!—but they’d ran into difficulties and didn’t know what it was good for.  I asked Alex whether he’d like to join forces with me and Sabee in tackling the Hermitian case, which (I told him) was enormously relevant in quantum physics.  To my surprise and delight, Alex agreed.</p>



<p>So we’ve been working on the problem together, making progress, and I’m optimistic that we’ll have <em>some</em> nice result.  By using the 3-mode correlations, at least “generically” we can recover the entries of the matrix A <em>up to complex conjugation</em>, but further ideas will be needed to resolve the complex conjugation ambiguity, to whatever extent it actually matters.</p>



<p>In short: on the negative side, there’s much more to the problem of learning a fermionic state than we’d realized.  But on the positive side, there’s much more to the problem than we’d realized!  As with the simulation of k-query quantum algorithms, my coauthors and I would welcome any ideas.  And I apologize to anyone who was misled by our premature (and hereby retracted) claims.</p>



<hr class="wp-block-separator" />



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 11):</span></strong> Here’s a third bonus retraction, which I thank my colleague <a href="https://www.markwilde.com/">Mark Wilde</a> for bringing to my attention.  Way back in 2005, in my <a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a> survey article, I “left it as an exercise for the reader” to prove that BQP<sub>CTC</sub>, or quantum polynomial time augmented with Deutschian closed timelike curves, is contained in a complexity class called SQG (Short Quantum Games).  While it turns out to be <em>true</em> that BQP<sub>CTC</sub> ⊆ SQG—as follows from <a href="https://arxiv.org/abs/0808.2669">my and Watrous’s 2008 result</a> that BQP<sub>CTC</sub> = PSPACE, combined with <a href="https://arxiv.org/abs/1011.2787">Gutoski and Wu’s 2010 result</a> that SQG = PSPACE—it’s not something for which I could possibly have had a correct proof back in 2005.  I.e., it was a harder exercise than I’d intended!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5706"><span class="datestr">at August 10, 2021 07:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/116">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/116">TR21-116 |  Quantum Meets the Minimum Circuit Size Problem | 

	Nai-Hui Chia, 

	Chi-Ning  Chou, 

	Jiayu Zhang, 

	Ruizhe Zhang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, we initiate the study of the Minimum Circuit Size Problem (MCSP) in the quantum setting. MCSP is a problem to compute the circuit complexity of Boolean functions. It is a fascinating problem in complexity theory---its hardness is mysterious, and a better understanding of its hardness can have surprising implications to many fields in computer science.

We first define and investigate the basic complexity-theoretic properties of minimum quantum circuit size problems for three natural objects: Boolean functions, unitaries, and quantum states. We show that these problems are not trivially in NP but in QCMA (or have QCMA protocols). Next, we explore the relations between the three quantum MCSPs and their variants. We discover that some reductions that are not known for classical MCSP exist for quantum MCSPs for unitaries and states, e.g., search-to-decision reduction and self-reduction. Finally, we systematically generalize results known for classical MCSP to the quantum setting (including quantum cryptography, quantum learning theory, quantum circuit lower bounds, and quantum fine-grained complexity) and also find new connections to tomography and quantum gravity. Due to the fundamental differences between classical and quantum circuits, most of our results require extra care and reveal properties and phenomena unique to the quantum setting. Our findings could be of interest for future studies, and we post several open problems for further exploration along this direction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/116"><span class="datestr">at August 10, 2021 06:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-480455417739937086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/08/queues-with-small-advice.html">Queues with Small Advice</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I have had papers rejected, with comments of the form that the results seem too easy, and are at the level of a homework assignment.  Generally, I think these reviewers miss the point.  The fact that the results seem easy may be because the point isn't the derivation but the conception and framing of the problem.  I actually think that generally it's an interesting subclass of good papers that can be and are turned into homework assignments.</p><p>A new-ish paper of mine, Queues with Small Advice, was recently accepted to the very new SIAM Conference on Applied and Computational Discrete Algorithms (<a href="https://www.siam.org/conferences/cm/conference/acda21">ACDA21</a>), which took place July 19-21.  This conference focuses on algorithms with a close tie to applications.  Some people unfamiliar with theory conferences might think that algorithms work would naturally be tied to applications, but I've generally found that algorithmic work tied to applications is more negatively reviewed in theory conferences.  Indeed, that type of work is much more likely to receive comments of the form that the results seem too easy, and are at the level of a homework assignment.  So perhaps this new conference will fill an important role and hole in the current slate of theory conferences. </p><p>In any case, I actually do think this paper is in some ways easy (in that the analysis is readily approachable with standard tools), and parts of it would, I believe, make a great homework assignment.  The goal was to show the potential power of using even very simple advice, such as from machine-learning algorithms, in queueing systems.  This seems to me to be a very understudied topic, and fits into the recently growing theme of <a href="https://arxiv.org/abs/2006.09123">Algorithms with Predictions</a>.  (The paper was rejected previously from a conference, where the most negative review said "Very accessible and well written paper, which certainly provides motivation to consider problems of this type." but also said "The mathematical analysis in this paper is fairly standard, and in that sense not novel... the paper is interesting, but not advancing sufficiently the state of the art.")  </p><p>The paper focuses on the case of 1 bit of advice -- essentially, is the job "short" or "long".  I think this type is advice is a good approach to look at for queueing -- it corresponds naturally to putting a job at the front of the queue, or the back.  And it may be easier for machine-learning algorithms to generate accurately.  Simple is often good in practice.  </p><p>Rather than describe the paper further, I'll go ahead and turn it directly into a collection of homework problems.  Feel free to use them or variations you come up with;  hopefully, the students won't find the paper for answers. I personally would be thrilled if one outcome of this paper was that prediction-based problems of this form made their way into problem sets.  (Although, serious question:  who still teaches queueing theory any more?)  </p><p><b>Section 1:  One-Bit Advice (Single Queue)</b></p><p>a)  Consider the standard M/M/1 queue, with Poisson arrivals at rate λ, and exponentially distributed service times of mean 1;  the expected time a job spends in the queue in equilibrium is 1/(1-λ).  Now suppose each job comes with one bit advice;  if the job has service time greater than T, the bit is 1, and if it is smaller than T, the bit is 0.  A "big" job goes to the end of the queue, a "small" job goes to the front.  (Assume the queue is non-preemptive.)  Find the expected time for a job in this queue in equilibrium, as a function of T and λ.</p><p>b)  What is the optimal value for T (as a function of λ)? </p><p>c)  Repeat parts a and b, but this time with a preemptive queue.  Does preemption help or hurt performance?</p><p>Harder variation:  The above questions, but with an M/G/1 queue (that is, for a general, given service distribution);  derive a formula for the expected time in the system, where the formula may involve terms based on the service distribution.</p><p>Easier variation:  Write a simulation, experimentally determine the best threshold, and the improvements from one bit of advice.  Different service time distributions can be tried.  </p><p><b>Section 2:  One-Bit Advice with Predictions (Single Queue)</b></p><p>Where would possibly get a bit of advice in real life?  Perhaps from a machine learning predictor.  But in that case, the advice might turn out to be wrong.  What if our bit of advice is just right most of the time?</p><p>a)  Consider the (non-preemptive) M/M/1 queue variation from Section 1 part a above, but now the advice is correct with some probability p.  Find the expected time for a job in this queue in equilibrium, as a function of p, T, and λ.</p><p>b)  Repeat part a with a preemptive queue.</p><p>Harder variations:  The above questions, but have the probability the advice is correct depend on the size of the job.  A particularly fun example is when the "predicted service time" for a job with true time x is exponentially distributed with mean x, and the prediction bit is 1 if the predicted time is larger than T, and 0 otherwise.  Also, one can again consider general service times.  </p><p>Easier variation:  Again, write a simulation and derive experimental results/insights.  </p><p><b>Section 3:  One-Bit Advice with Prediction (Power of 2 Choices)</b>  <i>[harder, grad student level;  needs to know fluid limit models;  I'd stick with sections 1 and 2!]</i></p><p>a)  Derive fluid limit equations for a collection of N queues, where there are two types of jobs:  "large" jobs arrive as a Poisson stream of rate λ₁N and have exponentially distributed service times with mean μ₁ and "small" jobs arrive as a Poisson stream of rate λ₂N and have exponentially distributed service times of mean μ₂.  Each job comes with a bit of advice determining whether it is large or small, but large jobs are mislabelled with probability p₁ and small jobs are mislabelled with probability p₂.  An incoming job selects a queue using "the power of two choices" -- it is up to you to describe how a job determines what is the better of the two choices (there are multiple possibilities) and how jobs are processed within a queue (non-preemptive is suggested).   </p><p>[Hint:  the queue state can be represented by the number of jobs that are labelled short that are waiting, the number of jobs that are labelled long that are waiting, and the type of the job currently being serviced.]  </p><p>b)  Compare fluid limit results to simulations for 1000 queues to see if your equations seem accurate.  </p><p><br /></p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/08/queues-with-small-advice.html"><span class="datestr">at August 09, 2021 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/one-shot-top-k/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/one-shot-top-k/">One-shot DP Top-k mechanisms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the last <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><em>blog post</em></a>, we showed that the exponential mechanism enjoys improved composition bounds over general pure DP mechanisms due to a property called <strong>bounded range</strong>.  For this post, we will present another useful, and somewhat surprising, property of the exponential mechanism in its application of top-\(k\) selection.</p>

<h2 id="differentially-private-top-k-selection">Differentially Private Top-\(k\) Selection</h2>

<p>We will focus on datasets that are a vector of counts \(h = (h_1, \cdots, h_d) \in \mathbb{N}^d\), which consist of counts \(h_i\) for elements from a universe \(\mathcal{U}\) where \(|\mathcal{U}| = d\).  Let’s assume that a user’s data can modify each count by at most 1, yet can change all \(d\) counts, i.e. the \(\ell_\infty\)-sensitivity is 1 and the \(\ell_0\)-sensitivity is \(d\).  The task here is to return the top-\(k\) elements from the input counts in a differentially private way.</p>

<p>For top-\(1\), this is simply returning the element with the max count, and this is precisely the problem that the exponential mechanism is set up to solve.  Let’s write out the exponential mechanism \(M^{(1)}: \mathbb{N}^d \to [d]\) for this instance:
\[
\mathbb{P}[M^{(1)}(h) = i] = \frac{e^{ \varepsilon h_i }}{\sum_{j \in [d] } e^{ \varepsilon h_j } }, \qquad \forall i \in [d].
\]
For those wondering why this formula omits the factor of \(1/2\) in the exponent, we are using the <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">stronger result</a> of the exponential mechanism which replaces global sensitivity with the range of the loss function \(\ell(i,h) = - h_i\), which is \(1\) in this case.  Recall from the last blog post that the exponential mechanism is \(\varepsilon^2/8\)-CDP.</p>

<p>Hence, to generalize this to top-\(k\) selection, we can simply iteratively apply this exponential mechanism by removing the <em>discovered</em> element from each previous round.  That is, we write \(M^{(k)}: \mathbb{N}^d \to [d]^k\) as the following for any outcome \( (i_1, i_2, \cdots, i_k) \in [d]^k\),</p>

<p>\[
\mathbb{P}[M^{(k)}(h) = (i_1, i_2, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
<a name="eq:peelingEM"></a>
\[\qquad = \frac{e^{ \varepsilon h_{i_1} }}{\sum_{j \in [d] } e^{ \varepsilon h_j } } \cdot  \frac{ e^{\varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1\}} e^{\varepsilon h_j } } \cdot \cdots \cdot \frac{ e^{ \varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1, \cdots, i_{k-1}\}} e^{ \varepsilon h_j } }. 
\tag{1}
\]</p>

<p>We can then apply composition to conclude that \(M^{(k)}\) is \(k \varepsilon^2/8\)-CDP.</p>

<h2 id="gumbel-noise-and-the-exponential-mechanism">Gumbel Noise and the Exponential Mechanism</h2>

<p>As we discussed in our last post, we can implement the exponential mechanism by adding <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel</a> noise to each count and reporting the noisy max element.  A Gumbel random variable \(X \sim \text{Gumbel}(\beta) \), parameterized by scale parameter \(\beta&gt;0\), has the following density function
<a name="eq:GumbelDensity"></a>
\[
p(x;\beta) = \frac{1}{\beta} \exp\left( - x/\beta - e^{-x/\beta} \right), \qquad \forall x \in \mathbb{R}.
\tag{2}
\]</p>

<p>Hence, we can write the exponential mechanism in the following way
\[
M^{(1)}(h) = \arg\max \{ h_i + X_i : i \in [d] \}, \qquad \{X_i \} \stackrel{i.i.d.}{\sim} \text{Gumbel}(1/\varepsilon).
\]</p>

<p>We can then extend this to top-\(k\) by repeatedly adding independent Gumbel noise to each count and removing the discovered element for the next round.  However, something that would significantly improve run time would be to add Gumbel noise to each count <em>once</em> and then take the elements with the top-\(k\) noisy counts.  We could then add only \(d\) many noise terms, rather than \(O(d^k)\) noise terms if we were to iteratively run \(k\) different exponential mechanisms.  The question is, does this one-shot top-\(k\) Gumbel noise mechanism ensure the same level of privacy?</p>

<p>Let’s denote the one-shot Gumbel mechanism as \(\tilde{M}^{(k)}\).  At first glance, it does not seem like the one-shot Gumbel mechanism \(\tilde{M}^{(k)}\) should be just as private as the iterative exponential mechanism \(M^{(k)}\), but it turns out they are exactly the same mechanism!  The following result is due to <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a>.</p>

<blockquote>
  <p><strong>Theorem 1</strong>
For any input vector of counts \(h \in \mathbb{N}^d\), the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) and iteratively applying the exponential mechanism \(M^{(k)}(h)\) are equal in distribution.</p>
</blockquote>

<p><em>Proof.</em> 
Recall the distribution of the iterative exponential mechanism \(M^{(k)}(h)\) from <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>.
Now we consider the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) where we use the density of \(X \sim \) Gumbel\( (1/\varepsilon)\) from <a href="https://differentialprivacy.org/feed.xml#eq:GumbelDensity">(2)</a>.<br />
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad = \int_{-\infty}^\infty p(u_1 - h_{i_1}) \int_{-\infty}^{u_1}  p(u_2 - h_{i_2}) \cdots \int_{-\infty}^{u_{k-1}} p(u_k - h_k) 
\]
<a name="eq:integral"></a>
\[
\qquad \qquad \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[ X &lt; u_k - h_j]du_k \cdots du_2 du_1.
\tag{3}
\]
Note that we have 
\[
\mathbb{P}[X &lt; y] = \exp\left( - \exp\left( -\varepsilon y \right) \right).
\]
Let’s focus on the inner integral over \(u_k\) in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.<br />
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\quad = \int_{-\infty}^{u_{k-1}} \varepsilon \cdot  \exp\left( - \varepsilon (u_k - h_{i_k}) - e^{ -\varepsilon (u_k - h_{i_k}) } \right) \cdot  \exp\left( -e^{-\varepsilon u_k}  \sum_{j \in [d] \setminus \{i_1, \cdots, i_k \} } e^{\varepsilon h_j}  \right) du_k 
\]
\[
\quad  = \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left( e^{\varepsilon h_{i_k}} + \sum_{j \in [d] \setminus \{i_1, \cdots i_k \} } e^{\varepsilon h_j} \right) \right) du_k \qquad
\]
<a name="eq:lastLine"></a>
\[
\qquad =  \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left(\sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j} \right) \right) du_k. \qquad \qquad
\tag{4}
\]
We now integrate with a \(v\)-substitution,
\[
v =e^{-\varepsilon u_{k}} \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}<br />
\]
\[
dv = - \varepsilon \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}  \cdot e^{-\varepsilon u_{k}} du_{k}.
\]</p>

<p>Continuing with <a href="https://differentialprivacy.org/feed.xml#eq:lastLine">(4)</a>, we get
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} \cdot \exp\left( - e^{-\varepsilon u_{k-1}} \cdot \sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j} \right)
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}}  \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \} } \mathbb{P}[X &lt; u_{k-1} - h_j ] .
\] 
Note how this line has the last term in the expression for \(M^{(k)}(h)\) in <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>, which is independent of \(u_{k-1}\) and can hence be pulled out of the larger integral in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.  By induction, we have
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad  = \frac{e^{\varepsilon h_{i_1} }}{\sum_{j \in [d]} e^{\varepsilon h_j}}  \cdot \frac{e^{\varepsilon h_{i_2} }}{\sum_{j \in [d] \setminus \{ i_1\}} e^{\varepsilon h_j}}  \cdot \cdots \cdot \frac{e^{\varepsilon h_k }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} 
\]
\[
\qquad = \mathbb{P}[M^{(k)}(h) =(i_1, \cdots, i_k)].
\] ∎</p>

<p>So that’s great!  We can now run the one-shot Gumbel mechanism for top-\(k\) and still get the improved composition bounds of the exponential mechanism.  In addition to this achieving better runtime, this analysis can help with proving top-\(k\) DP algorithms over a large domain universe despite giving access to only the true top-\(\bar{k}\) items and their counts where \(\bar{k} &gt; k \), see <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> for more details.</p>

<h2 id="report-noisy-max-for-dp-top-k">Report Noisy Max for DP Top-\(k\)</h2>

<p>We now turn to comparing this algorithm to some natural alternatives.  As we discussed in the last post, there is a family of mechanisms, report noisy max (RNM) mechanisms, that ensure differential privacy for the selection problem, and hence the top-\(1\) problem.  We showed that the exponential mechanism is equivalent to RNM with Gumbel noise, there is also RNM with Laplace and with Exponential noise, the last being the recently discovered <em>permute-and-flip</em> mechanism <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>.</p>

<p>To then use RNM mechanisms for top-\(k\), we can again iteratively apply them and use composition to get the overall privacy guarantee.  However, it turns out that you can also use the Laplace noise version of RNM in one-shot <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.</p>

<p>We can compare the relative noise that is added to each count in both the Laplace and Gumbel versions.  Since <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a> gives their privacy guarantee in terms of approximate \((\varepsilon,\delta )\)-DP, we will now make the comparison there.  We first look at the standard deviation for Laplace \( \sigma_{\text{Lap}}\) (using Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>).
\[
\sigma_{\text{Lap}} =  \frac{8 \sqrt{2k \ln(d/\delta)}}{\varepsilon}.
\]
Note that the one-shot Laplace mechanism returns counts as well as the indices of the top-\(k\), both of which use Laplace noise with standard deviation \(\sigma_{\text{Lap}}\), so we will also include Laplace noise to the discovered elements in the Gumbel version. That is, we add Gumbel noise with scale \(\sqrt{k}/\varepsilon’\) for the discovery portion and Laplace noise with scale \(2\sqrt{k}/\varepsilon’\) for obtaining their counts, resulting in standard deviation noise \(\sigma_{\text{Gumb}}’\) and \(\sigma_{\text{Lap}}’\), respectively.<br />
\[
\sigma_{\text{Gumb}}’  = \frac{\pi \sqrt{k} }{\sqrt{6}\varepsilon’}, \qquad 
\sigma_{\text{Lap}}’ = \frac{2\sqrt{2k}}{\varepsilon’}.
\]
Recall that adding this scale of Gumbel noise and Laplace noise will ensure \(\tfrac{\varepsilon’^2}{8} \)-CDP each, so combining will ensure \(\tfrac{\varepsilon’^2}{4}\)-CDP.  We could also use Gaussian noise to return the counts since we are using CDP, but we will analyze it with Laplace noise for comparison.  To ensure \((\varepsilon,\delta)\)-DP, we use the CDP to DP conversion from Lemma 3.5 in <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and solve for \(\varepsilon’\).  Hence, we get for any \(\delta&gt;0\)
\[
\varepsilon’^2/4 = \left( \sqrt{\ln(1/\delta) + \varepsilon} - \sqrt{\ln(1/\delta)} \right)^2 
\]
\[ \implies \varepsilon’ = 2 \sqrt{\ln(1/\delta)} \left( \sqrt{1 + \tfrac{\varepsilon}{\ln(1/\delta)}} - 1 \right).
\]</p>

<p>Let’s consider a typical privacy setting where \(\varepsilon &lt;  \ln(1/\delta)\), and use the inequality \(\sqrt{1+x} \geq 1 + x/4\) for \(0&lt;x&lt;1\).  Here is a short proof of this inequality:
\[
(1 + x/4)^2 = 1 + x/2  + x^2/16  \leq 1 + x/2 + x/2 = 1 + x.<br />
\]
Note that the privacy guarantee for one-shot Laplace noise only holds when \(\varepsilon &lt; 0.2\) and \(\delta &lt; 0.05\) as stated in Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.  In this case, we have 
\[
\varepsilon’ \geq 2 \sqrt{\ln(1/\delta)} \left( 1 + 1/4 \cdot \tfrac{\varepsilon}{\ln(1/\delta)} - 1 \right) = 1/2 \cdot \tfrac{\varepsilon}{\sqrt{\ln(1/\delta)}}.
\]
Plugging \(\varepsilon’\) into the standard deviation of Gumbel and Laplace, we get
\[
\sigma_{\text{Gumb}}’  \leq \frac{2\pi\sqrt{k \ln(1/\delta)}}{\sqrt{6}\cdot \varepsilon}, \qquad \sigma_{\text{Lap}}’ \leq \frac{4\sqrt{2k\ln(1/\delta)}}{\varepsilon}.
\]</p>

<p>Putting this together, we can show that we add significantly less noise for the discovery and releasing noisy count phases, 
\[
\sigma_{\text{Gumb}}’\leq \sigma_{\text{Lap}}/4 ,\qquad  \sigma_{\text{Lap}}’ \leq \sigma_{\text{Lap}}/2.
\]
Note that these bounds can be improved further with similar analysis.</p>

<p>Although it has not been studied yet whether the permute-and-flip mechanism \(M_{\text{PF}} \) can also ensure DP in one shot by using Exponential noise, we briefly discuss whether it can be bounded range for a similar parameter as the Exponential Mechanism, and hence achieve similar composition bounds.  Consider running permute-and-flip on two items \(\{1,2 \}\) with a monotonic quality score \(q: \mathcal{X} \times \{1,2 \} \to \mathbb{R} \) whose sensitivity is 1.  Let \(x, x’ \in \mathcal{X} \) be neighbors where 
\[
q(x,1) = q(x,2) = 0
\]
\[
q(x’,1) = 0 , \quad q(x’,2) = 1.
\]
Hence, permute-and-flip will return outcome \(1\) or \(2\) with half probability each on dataset \(x\), while with dataset \(x’\) outcome \(1\) occurs with probability \(1/2 \cdot  e^{-\varepsilon}\) and outcome \(2\) occurs with probability \( 1/2 + 1/2 \cdot (1 - e^{-\varepsilon})\).  We can then compute the bounded range parameter \(\alpha\) as
\[
\frac{\mathbb{P}[M_{\text{PF}}(x’) = 2 ] }{\mathbb{P}[M_{\text{PF}}(x) = 2 ]}\leq e^{\alpha}\frac{\mathbb{P}[M_{\text{PF}}(x’) = 1 ]}{\mathbb{P}[M_{\text{PF}}(x) = 1 ]} \implies  \alpha \geq \varepsilon + \ln(2 - e^{-\varepsilon} ).
\]
Note that with \(\varepsilon \gg 1\), we get \(\alpha \) close to \(\varepsilon\), which would be the same bounded range parameter as the exponential mechanism.  However, with \(\varepsilon&lt; 1\), we get \(\alpha\) close to \(2\varepsilon\).  This example provides a lower bound on the BR parameter for permute-and-flip.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have looked at the top-\(k\) selection problem subject to differential privacy and although there are many different mechanisms to use, the exponential mechanism stands out for several reasons:</p>
<ol>
  <li>The exponential mechanism is \(\varepsilon\)-DP and \( \varepsilon^2/8\)-CDP and hence gets improved composition.</li>
  <li>Iteratively applying the exponential mechanism for top-\(k\) can be implemented by adding Gumbel noise to each count and returning the elements with the top-\(k\) noisy counts in one-shot.</li>
  <li>The one-shot Gumbel mechanism returns a ranked list of \(k\) elements, rather than a set of \(k\) elements.</li>
</ol></div>







<p class="date">
by Ryan Rogers <a href="https://differentialprivacy.org/one-shot-top-k/"><span class="datestr">at August 09, 2021 05:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1648249705477846335">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html">Combing two posts: Blankface (Scott Aa) and Is Science Slowing Down? (Scott Al)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(I also posted this to the Less Wrong Website. At least I tried to- I don't quite know if or when it will appear there as its my first post there.) </p><p>Some papers result from taking two papers and combining them. Perhaps nobody else had read both of them so you can say something new! Or (looking over this post) it may guide people to two really good papers, or in this case two really good posts. </p><p>This blog will draw from two excellent blog posts.</p><p>Scott Aaronson  blogged on  his website Aug 2, 2021 about <a href="https://www.scottaaronson.com/blog/?p=5675#comments">blankfaces</a>, people who let stupid or undefined rules dictate what you can do  without apology (see his post for a better explanation). One example that struck me I quote</p><p><i>No, I never applied for that grant. I spend two hours struggling to log in to a web portal designed by the world's top blankfaces until I finally gave up in despair. </i></p><p><i><br /></i></p><p>Scott Alexander blogged  on LessWrong on Nov 26, 2018 about <a href="https://www.lesswrong.com/posts/v7c47vjta3mavY3QC/is-science-slowing-down">Is science slowing down?</a> which answers with an emphatic <i>yes.</i> His point is science-per-researcher is much less than it used to be, and he has graphs and stats to prove it (see his post for the evidence and some speculation as to why this is) One of the reasons he gave struck me which I quote</p><p><i>Certain features of the modern academic system like undepaid PhD's, interminably long postdocs, endless grant writing drudgery, and clueless funders have lowered productivity. The 1930's academic system was ineed 25x more effective at getting researchers to actually do good research.</i></p><p>(A commenter reminded me that Scott Alexander himself dismisses this reason. I do not.) </p><p>(I note that he gives other reasons as well, most notably for our field that the low hanging fruit is gone. Our lack of progress on P vs NP is likely that its a hard problem, rather than the reason above. Of course, if its solved tomorrow by an outsider without funding, I will happily be proven wrong.) </p><p>Scott Alexander hits upon two types of blankfaces (without using the term).</p><p><i>Grant writing drudgery</i>: the rules for how to submit get more and more detailed an onerous. This is  what Scott Aaronson was alluding to. There are other ways its drudgery as well. </p><p><i>Clueless Funders</i>: the people deciding who gets funded might not know the area (actually in my experience the grant I've reviews have been quite good and the problem is more not enough money to award all that are deserving.) </p><p>SO I pose the following non-rhetorically as always</p><p>1) How big a factor is the slowing down of science that blankfaces get in the way?</p><p>2) What can we do about it?</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><i><br /></i></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html"><span class="datestr">at August 09, 2021 01:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">School on Modern Directions in Discrete Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 13-17, 2021 Online https://www.him.uni-bonn.de/programs/future-programs/future-trimester-programs/discrete-optimization/discrete-optimization-school/ Aims and Scope: The school provides an introduction to some of the main topics of the trimester program on discrete optimization. The lectures will address the interface between tropical geometry and discrete optimization; recent developments in continuous optimization with applications to combinatorial problems; topics in approximation algorithms; and fixed parameter … <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/" class="more-link">Continue reading <span class="screen-reader-text">School on Modern Directions in Discrete Optimization</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/"><span class="datestr">at August 08, 2021 02:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/115">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/115">TR21-115 |  On quantum versus classical query complexity | 

	Scott Aaronson, 

	Andris Ambainis, 

	Andrej Bogdanov, 

	Krishnamoorthy Dinesh, 

	Cheung Tsun Ming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Aaronson and Ambainis (STOC 2015, SICOMP 2018) claimed that the acceptance probability of every quantum algorithm that makes $q$ queries to an $N$-bit string can be estimated to within $\epsilon$ by a randomized classical algorithm of query complexity $O_q((N/\epsilon^2)^{1-1/2q})$.  We describe a flaw in their argument but prove that the dependence on $N$ in this upper bound is correct for one-query quantum algorithms ($q = 1$).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/115"><span class="datestr">at August 08, 2021 12:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1563">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/08/workshop-on-algorithms-for-large-data-we-found-waldo-and-so-can-you/">Workshop on Algorithms for Large Data: We found WALD(O), and so can you!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Ainesh Bakshi, Rajesh Jayaram, and Samson Zhou are organizing a 3-day <a href="https://waldo2021.github.io/">Workshop on Algorithms for Large Data</a> (nicely abbreviated as WALD(O), the O standing for Online), featuring many talks which should be of interest to the readers of this blog, as well as an open problems and a poster sessions, and a junior/senior lunch. As the organizers describe it:</p>



<blockquote class="wp-block-quote"><p>This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</p></blockquote>



<p>The workshop will take place on <strong>August 23 — August 25</strong> (ET). Attendance is free, but <a href="https://docs.google.com/forms/d/1VMtDFay1MoiKMAErfkg2ZkAswQQdNWhiDQUKGtPBrzA/viewform">registration</a> is required by <strong>August 20th</strong>. More details at <a href="https://waldo2021.github.io/">https://waldo2021.github.io/</a></p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/08/workshop-on-algorithms-for-large-data-we-found-waldo-and-so-can-you/"><span class="datestr">at August 07, 2021 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1560">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/08/new-for-july-2021/">New for July 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This month saw three papers appear online, together covering a rather broad range of topics: testing of regular languages, distribution testing under differential privacy, and local testability from high-dimensional expanders. Let’s dive in!</p>



<p><strong>Property Testing of Regular Languages with Applications to Streaming Property Testing of Visibly Pushdown Languages</strong>, by Gabriel Bathie and Tatiana Starikovskaya (<a href="https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=14188">paper</a>). Let \(L\in \Sigma^\ast\) be a regular language recognized by an automation with \(m\) states and \(k\) connected components: given as input a word \(u\in \Sigma^n\), what is the query complexity to test membership to \(L\) in Hamming distance? Edit distance? Or, more generally, <em>weighted</em> edit distance, where each letter of the word \(u\) comes with a weight? In this paper, the authors focus on non-adaptive, one-sided errors testing algorithms, for which they show an upper bound of \(q=O(k m \log(m/\varepsilon)/\varepsilon)\) queries (with running time \(O(m^2 q)\)), which they complement by a query complexity lower bound of \(\Omega(\log(1/\varepsilon)/\epsilon)\), thus matching the upper bound for languages recognized by constant-size automata. The guarantee for the upper bound is with respected to weighted edit distance, and thus implies the same upper bound for testing with respect to Hamming distance. <br />To conclude, the authors use an existing connection to streaming property testing to obtain new algorithms for property testing of visibly pushdown languages (VPL) in the <em>streaming</em> model, along with a new lower bound in that model.</p>



<p><strong>High dimensional expansion implies amplified local testability</strong>, by Tali Kaufman and Izhar Oppenheim (<a href="https://arxiv.org/abs/2107.10488">arXiv</a>). This paper sets out to show that codes that arise from high-dimensional expanders are locally testable (membership to the code can be tested using very few queries). To do so, the authors define a new notion of <em>high-dimensional expanding system</em> (HDE system), as well as that of <em>amplified</em> local testability, a stronger notion than local testability; they then prove that a code based on a HDE system satisfies this stronger notion. Moreover, they show that many well-known families of codes are, in fact, HDE system codes, and therefore satisfy this stronger notion of local testability as well.</p>



<p>Finally, a survey on differential privacy, with a foray into distribution testing:</p>



<p><strong>Differential Privacy in the Shuffle Model: A Survey of Separations</strong>, by Albert Cheu (<a href="https://arxiv.org/abs/2107.11839">arXiv</a>). If you are familiar with differential privacy (DP), you may recall that there are several notions of DP, each meant to address a different “threat model” (depending on whom you trust with your data). <em>Shuffle DP</em> is one of them, intermediate between “central” DP and the more stringent “local” DP. Long story short: with shuffle DP, the tradeoff between privacy and accuracy can be strictly in-between what’s achievable in central and local DP, and that’s the case for one of the usual suspects of distribution testing, uniformity testing (<em>“I want to test if the data uniformly distributed, but now, with privacy of that data in mind”</em>). The survey discusses what is known about this in Sections 3.3 and 6, and what the implications are; but there are quite a few questions left unanswered… Long story short: a very good introduction to shuffle privacy, and to open problems in that area!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/08/new-for-july-2021/"><span class="datestr">at August 07, 2021 06:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-551003207026816768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award.html">Interview with CONCUR 2021 ToT Award Recipients: Uwe Nestmann and Benjamin Pierce</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I am pleased to re-post <a href="https://www.imperial.ac.uk/people/n.yoshida" target="_blank">Nobuko Yoshida</a>'s splendid <a href="http://mrg.doc.ic.ac.uk/concur-tot/" target="_blank">interview</a> with CONCUR 2021 Test-of-Time Award recipients <a href="https://www.mtv.tu-berlin.de/nestmann/" target="_blank">Uwe Nestmann</a> and <a href="https://www.cis.upenn.edu/~bcpierce/" target="_blank">Benjamin Pierce</a>. I thoroughly enjoyed reading it and learnt much from the many pearls of wisdom that pepper the interview. </p><p>Thanks to Benjamin and Uwe for their answers and to Nobuko for conducting such an inspiring interview. Enjoy!<br /></p><p><br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award.html"><span class="datestr">at August 06, 2021 09:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">Workshop on Algorithms for Large Data (Online) 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 23-25, 2021 Online https://waldo2021.github.io/ Registration deadline: August 20, 2021 This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/"><span class="datestr">at August 06, 2021 09:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2754">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">Average-Case Fine-Grained Hardness, Part III</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Continuing our previous discussion, I will show another application of the new recipe described in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> (i.e., constructing a “good” polynomial for a problem of interest), which will establish average-case hardness of a problem related to the orthogonal vector (OV) problem. (Recall the OV problem: Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3D%5Comega%28%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\omega(\log n)" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. The reader can look up the worst-case hardness of the OV problem in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">first post</a> of the series.)</p>



<p>The motivating question is can we show average-case hardness for counting the number of orthogonal pairs in the OV problem by constructing a “good” polynomial? (One motivation is that such average-case hardness result could serve as the source of reductions for proving average-case hardness for many other problems, because OV is a main source of fine-grained hardness.) There is a good reason to believe the answer is no. Specifically, an <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-%5Cdelta%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-\delta})" class="latex" />-time algorithm for counting orthogonal pairs for average-case OV instances (here “average-case” is Erdős–Rényi random input model, and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta" class="latex" /> is a constant that depends on the parameter of the input model) was given in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>, while constructing a “good” polynomial would prove <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%5E%7B2-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(n^{2-\varepsilon})" class="latex" /> average-case hardness for any constant <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon&gt;0" class="latex" /> assuming randomized SETH. This indicates that sometimes constructing a “good” polynomial might be a little too ambitious goal.</p>



<p>Instead, can we first come up with a nice combinatorial problem that encodes OV on a slightly larger binary input space and then construct a “good” polynomial for counting solutions of this combinatorial problem? (The motivation is that since this combinatorial problems encodes OV, it is at least as hard as OV for worst case, and moreover, if we can construct a “good” polynomial for counting solutions of this combinatorial problem, by the new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>, counting solutions of this combinatorial problem for average case is (almost) as hard as that for worst case. Therefore, counting solutions of this combinatorial problem for average case is at least as hard as OV for worst case. More importantly, this combinatorial problem could serve as the source of reductions thanks to its combinatorial structure.) The factored OV problem introduced in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> gives a positive answer to this question. Analogously, they proposed factored variants for many other flagship fine-grained hard problems. By reductions to these factored problems, they managed to prove average-case fine-grained hardness for many natural combinatorial problems such as counting regular expression matchings (the featured image of this post is the web of reductions in their paper).</p>



<p>Next, for the purpose of exposition, I briefly sketch the high-level idea behind the factored OV problem (without even explicitly describing its combinatorial interpretation, since I will not show any reduction from this problem). </p>



<p><strong>Counting solutions for factored OV.</strong>  Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" /> (actually, <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog%5E2+n%2F%5Clog%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log^2 n/\log\log n)" class="latex" /> would also work, and the choice here is for simplicity), we encode <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%3A%3D%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29%5Ccirc+%5Ctextrm%7BLONG%7D%28x%5B%5Csqrt%7Bd%7D%2B1%3A2%5Csqrt%7Bd%7D%5D%29%5Ccirc%5Cdots%5Ccirc%5Ctextrm%7BLONG%7D%28x%5Bd-%5Csqrt%7Bd%7D%2B1%3Ad%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x):=\textrm{LONG}(x[1:\sqrt{d}])\circ \textrm{LONG}(x[\sqrt{d}+1:2\sqrt{d}])\circ\dots\circ\textrm{LONG}(x[d-\sqrt{d}+1:d])" class="latex" />,<br />where <img src="https://s0.wp.com/latex.php?latex=x%5Bi%3Aj%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x[i:j]" class="latex" /> represents the subvector (block) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> from the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th to the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="j" class="latex" />-th coordinate, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> is the long code encoding (specifically, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> maps a <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sqrt{d}" class="latex" />-dimensional binary vector <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> to a <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^{\sqrt{d}}" class="latex" />-dimensional vector binary vector <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x')" class="latex" /> of which all the coordinates are zero except the coordinate indexed by <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" />). Namely, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> is the concatenation of the long code encoding of each block of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, and thus, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" />, and for our choice of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />, we have that <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7Bn%5E%7Bo%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{n^{o(1)}}" class="latex" />. Therefore, by taking such encoding for the vectors in the OV instance, we blow up the size of input (and hence weaken the worst-case hardness of OV) very mildly.</p>



<p>The key advantage of such encoding is that the indicator function <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%5Cperp+y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[1:\sqrt{d}]\perp y[1:\sqrt{d}])" class="latex" /> (which outputs <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> if the two vectors are orthogonal and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0" class="latex" /> otherwise) can be represented as the sum of degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> monomials, of which the variables are the coordinates of <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" />. Indeed, we can first enumerate all the orthogonal pairs of vectors <img src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2%5Cin+%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1,v_2\in \{0,1\}^{\sqrt{d}}" class="latex" />, and we check whether <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_2" class="latex" />, by taking product of these two coordinates, and then, we take the sum of all the products.</p>



<p>Using this approach, for each <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, for each <img src="https://s0.wp.com/latex.php?latex=i%5Cin%5B%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i\in[\sqrt{d}]" class="latex" />, we get a degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> polynomial that computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%5Cperp+y%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[(i-1)\sqrt{d}+1:i\sqrt{d}]\perp y[(i-1)\sqrt{d}+1:i\sqrt{d}])" class="latex" /> on input <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(y)" class="latex" />. The product of these polynomials obviously computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5Cperp+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x\perp y)" class="latex" />, and moreover, this product is a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial (<img src="https://s0.wp.com/latex.php?latex=d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'" class="latex" />-partite polynomial is defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>), where each part corresponds to the coordinates of the long code encoding of a block (subvector) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />. If we sum up these <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomials for all pairs <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, we get a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial that precisely counts orthogonal pairs for the OV instance. We can let the field size of this polynomial be a prime <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^2" class="latex" /> is a trivial upper bound of the number of orthogonal pairs) such that the output of this polynomial is indeed the number of orthogonal pairs.</p>



<p>Notice that (i) Since the encoding only blows up the input size mildly, the worst-case hardness of OV (almost) carries over to evaluating this polynomial. (ii) Since <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" />, the <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial is a “good” polynomial (defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>). It follows from our new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> that evaluating this polynomial on binary input for average case (here “average case” means Erdős–Rényi random input model) is (almost) as hard as worst case. (iii) Last but not least, evaluating this polynomial on any <img src="https://s0.wp.com/latex.php?latex=z%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" /> (not just <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" />) can be interpreted as counting solutions for a combinatorial problem, which is the factored OV problem in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>. As I mentioned earlier, I will not explain the combinatorial interpretation in details. The takeaway is that counting solutions of such factored problem is average-case fine-grained hard, and its combinatorial structure allows possible reductions to other natural combinatorial problems.</p>



<p>Finally, I mention two broad research directions in this area: (i) design cryptographic primitives, e.g., one-way functions, based on these fine-grained average-case hardness results (or show complexity barriers) and (ii) prove fine-grained average-case hardness for decision problems (or design more efficient algorithms).</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk. </p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/"><span class="datestr">at August 06, 2021 03:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks.html">Predicting weighted ranks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This week’s events have included Olympic sport climbing, for the first time. The NBC livestream of the women’s qualification suffered a bit of an embarrassment, though, as the computer display of the results incorrectly showed some competitors as being guaranteed to qualify when they were not. (Rest of post contains spoilers; don’t click if you don’t want to know about the outcomes of the event.)</p>

<p>For example, the screenshot below shows Viktoriia Meshkova as a qualifier, with Janja Garnbret still to climb, but Meshkova was actually eliminated after Garnbret’s climb. The commentators noticed the problem and had to tell the audience not to pay attention to that part of the display. What went wrong, and how could it have been done correctly?</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" /></p>

<h1 id="background">Background</h1>

<p>The important things to know about this event are:</p>

<ul>
  <li>
    <p>It involves three disciplines, speed, bouldering, and lead, in that order. One discipline finishes before the next one starts (in fact there is a long rest period between each two disciplines).</p>
  </li>
  <li>
    <p>The scores from each discipline are turned into orderings, giving each of the competitors a number from 1 to 20, before combining them into a single outcome.
Each of 20 competitors gets a rank from 1 to 20 in each of the three disciplines.</p>
  </li>
  <li>
    <p>These numbers are multiplied and the eight competitors with the <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">smallest product of ranks</a> advance to the final round.</p>
  </li>
  <li>
    <p>In lead climbing, the ranks are based on how high each competitor climbs, with ties broken by time, so any competitor who has not yet climbed can slot into the ranking in any position.</p>
  </li>
</ul>

<p>In screens like the one shown above, the livestream showed the standings of the competitor, ordered by their product of ranks: the product of two ranks for competitors who had not yet climbed and the product of all three current ranks for competitors who had already climbed. It predicted qualification for competitors who had already climbed and were in the top eight in this ordering. This seems reasonable, at first glance: the ordering among the people who have already climbed is set, and the people who have not yet climbed can only go down in the ordering, so they will stay in the top eight.</p>

<p>But the ordering among competitors who had already climbed is <em>not</em> set. In the example shown above, before Janja Garnbret climbed, Meshkova was ahead of two other climbers, Aleksandra Mirosław and Anouck Jaubert, both of whom had done well in speed and badly in lead. Garnbret climbed better than Meshkova, bumping Meshkova’s lead ranking down from fourth to fifth. That hurt Meshkova’s combined score a lot more than it hurt Mirosław’s and Jaubert’s, because Mirosław and Jaubert already had very high ranks in lead. At the end of the competition, Meshkova was behind Mirosław and Jaubert, and out of the competition.</p>

<h1 id="analysis">Analysis</h1>

<p>We can formulate this as an algorithms problem: Suppose we have \(n\) competitors, numbered \(1\dots n\), each with a weight \(w_i\) (their combined score from the previous disciplines). We have selected an ordering on a set \(X\) of the competitors, while the remaining set \(Y\) have yet to be ordered. The eventual ordering on \(X\cup Y\) must be consistent with the ordering we already know on \(X\). If competitor \(i\) ends up in position \(p_i\), they get a combined score \(w_i\cdot p_i\) and a combined rank based on sorting these combined scores. (To keep the terminology from being confused, I’ll stick to “ordering” for the result of a single discipline, and “ranking” for the combined result of the whole competition.) What we want to know, for each competitor number \(i\), is: what is the maximum possible combined rank \(r_i\)? Before formulating an algorithm for this problem, let’s do some analysis to find simplifying assumptions that can make the algorithm fast.</p>

<p>For the climbing competition, \(n=20\) and we only care whether \(r_i\le 8\) or \(r_i&gt;8\): is competitor \(i\) guaranteed a spot in the final or not? More generally, we can ask the same question for any \(n\) and any threshold on the combined rank. We can also ask this question regardless of whether competitor \(i\) has already competed (that is, whether \(i\in X\)): if not, it’s safe to assume that they will end up last in the ordering, because that’s the slot that will give them the maximum combined rank.</p>

<p>For each competitor \(j\) in the set \(Y\) of not-yet-ordered competitors, it’s always safe to assume that \(j\) will slot in somewhere above \(i\) in the final ordering. Slotting in immediately above \(i\) is always worse for \(i\) than slotting in anywhere below \(i\), because it hurts the ranking for \(i\) more than it hurts anyone else. Therefore, it can only cause \(i\) to go down among the combined rankings of competitors who are already ordered. Once we make this assumption, we know the final position \(p_i\) of competitor \(i\) in our ordering, and therefore we also know the final combined score \(w_i\cdot p_i\). This assumption lets us determine which final scores beat \(i\), and (because the orderings are also fixed by this assumption) determines which of the competitors ordered later than \(i\) in \(X\) end up beating \(i\). What remains to be determined is which of the competitors ordered earlier than \(i\) in \(X\) might beat \(i\), and which of the competitors in \(Y\) might <span style="white-space: nowrap;">beat \(i\).</span></p>

<p>We don’t know how many competitors in \(Y\) beat \(i\), but we can guess; there are only \(\vert Y\vert\lt n\) possibilities to try. Suppose that we guess that this number is \(k\). If our guess is correct, then we can safely assume that these \(k\) better competitors are the ones in \(Y\) with the \(k\) smallest weights. Any other outcome that puts \(k\) competitors in \(Y\) ahead of \(i\) can be swapped to an outcome that puts these \(k\) competitors ahead, without changing the ranks of any competitors <span style="white-space: nowrap;">in \(X\).</span></p>

<p>Once we know which \(k\) competitors in \(Y\) beat \(i\), we also know how good a position \(p_j\) each of these competitors will need to attain, to beat \(i\). We can assign these competitors to these positions, breaking ties by moving some up into higher positions, determining where they all slot into the overall ordering. Among all assignments of positions to these competitors that puts them ahead of \(i\), this is the one that hurts the other competitors of \(i\) the least. Once this is done, we can safely assign all of the remaining competitors in \(Y\) to an ordering that slots them in just ahead of \(i\), again hurting \(i\) while causing the least hurt to the competitors <span style="white-space: nowrap;">of \(i\).</span></p>

<p>With the ordering of competitors completely determined by the choice of \(k\), all we need to do is try all choices of \(k\) and test which ones put the largest number of competitors ahead <span style="white-space: nowrap;">of \(i\)!</span></p>

<p>There is a complication here with tiebreaks that I am not handling. If two competitors get the same combined score, the one who is ahead in two of the three disciplines gets the higher combined rank. If three competitors get the same combined score, and they have a cyclic ordering on tiebreaks, I don’t know what happens, and I suspect the rules don’t cover that situation. To simplify things I will just assume that all tiebreaks go against the candidate (so we might not guarantee qualification until the ties are resolved).</p>

<h1 id="algorithm">Algorithm</h1>

<p>Based on these simplifications, our algorithm for computing the maximum combined rank \(r_i\) of competitor \(i\) performs the following steps:</p>

<ul>
  <li>
    <p>If \(i\) is not already in \(X\), add it to \(X\) with a position after all of the other competitors <span style="white-space: nowrap;">in \(X\).</span></p>
  </li>
  <li>
    <p>Loop through all choices of \(k\) in \(0\dots\vert Y\vert\). For each choice:</p>

    <ul>
      <li>
        <p>Determine, for each \(j\) in the \(k\) smallest-weight competitors in \(Y\), the position \(p_j\) that \(j\) would need to obtain to <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
      <li>
        <p>While any two of these competitors have the same value for \(p_j\), decrement one of these two values. If this causes any value to become non-positive, continue the outer loop with the next choice <span style="white-space: nowrap;">of \(k\).</span></p>
      </li>
      <li>
        <p>Place the competitors in \(Y\) that are not among the \(k\) smallest immediately above \(i\) in the ordering</p>
      </li>
      <li>
        <p>Place the \(k\) smallest-weight competitors into positions \(p_j\), preserving the ordering of the remaining competitors.</p>
      </li>
      <li>
        <p>In the resulting ordering, determine how many competitors <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Return the maximum number of competitors beating \(i\) in all of the orderings that have been examined</p>
  </li>
</ul>

<p>With some care it should be possible to do all of this in time \(O(nk)\). <a href="https://11011110.github.io/blog/assets/2021/rank-product/qualify.py">My implementation</a> is slower because I was more interested in getting it to work than in optimizing it, and for \(n=20\) it is blazingly fast regardless.</p>

<h1 id="outcomes">Outcomes</h1>

<p>With all that in mind, and assuming that (somehow) I’ve implemented this correctly, let’s look at what this algorithm predicts for the actual data.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with five climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/2.jpg" /></p>

<p>At this stage of the qualifications, five competitors are left to climb. NBC predicted that Seo, Raboutou, and Pilz had already qualified, and my implementation agrees for Seo and Raboutou (both can finish at worst 8th) but it was incorrect for Pilz, who could drop to 9th if Mirosław miraculously finished 2nd and Garnbret 3rd. More surprising to me, Garnbret was still not an automatic qualifier: if she finished 20th, enough other competitors could better her score to put her into 9th place.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with three climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/3.jpg" /></p>

<p>After two more climbs, the faulty NBC algorithm claims that five climbers have qualified. My program agrees for Seo (worst rank 6), Nonaka (worst rank 5), Raboutou (worst rank 7), but still not Pilz (worst rank 9) or Jaubert (worst rank 10). My code now thinks Garnbret has locked in a rank of at worst 7th, qualifying without even climbing yet.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with two climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/4.jpg" /></p>

<p>Shauna Coxsey, climbing with an injured knee, has climbed into the middle of the pack, falling off the top ten ranking, and her place has been taken by Kyra Condie. Garnbret is now at worst 6th and should have been marked as qualified. Noguchi is not quite guaranteed yet, with a scenario in which she could rank 9th. Seo is now at worst 5th, Nonaka at worst 4th, Raboutou at worst 5th, and Pilz at worst 8th (now guaranteeing her spot). But Jaubert and Mirosław each could end 9th; at most one of Jaubert, Mirosław, or Noguchi will be eliminated, but we can’t yet guarantee any of their spots.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" /></p>

<p>Until now, all of the “Q” markings shown on the livestream, while mathematically incorrect in many cases, were at least correct in hindsight: the people marked that way did end up qualifying. This one, though, a repeat of the first image in this post, gets it wrong in practice as well as in theory. Meshkova is marked as qualifying, but did not. Coxsey has moved back into the top ten. Mirosław and Jaubert could still have ended up 9th (under different scenarios, obviously) and should not have been marked as qualifying. Seo is at worst 4th, Nonaka is at most 3rd, Noguch is at most 4th, Raboutou is at most 5th, and Pilz is at most 6th.</p>

<p>And the final ranking:</p>

<p style="text-align: center;"><img width="80%" alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, top ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6a.jpg" /></p>

<p style="text-align: center;"><img width="80%" alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, bottom ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6b.jpg" /></p>

<h1 id="the-future">The future</h1>

<p>I think the moral of the story is that this ranking system is too hard to understand and a little too random (with players trading places too much depending on what other players do). To some extent this is unavoidable (a version of <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem">Arrow’s impossibility theorem for rank aggregation</a>), but the scuttlebutt seems to be that this system will be replaced for future competitions. Which, sadly, makes all of this algorithm design a little redundant…</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106707759220927820">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks.html"><span class="datestr">at August 05, 2021 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5445216218770736629">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html">Pole Vault Live Blogging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As I write this I'm watching the women's pole vault final in the Olympics. Of the 15 women who made the finals, only four remain after two heights.</p><p>To expand on my <a href="https://twitter.com/fortnow/status/1421510617878437891">tweet</a>, I find the pole vault the purest of the Olympic Sports. No electronic monitors and timers, no biased judges, no video review. No points deducted for bad form or failing to stick the landing. No disqualification for a false start or stepping over a line. Either you clear the bar without knocking it down, or you don't.</p><p>The high jump has similar properties, but just not as cool looking.</p><p>All four made the third height. Now onto 4.85 meters. An American, a Greek, a Brit and a Russian (sorry I meant member of the Russian Olympic Committee).</p><p>Back in the day, the TV coverage was rather limited. We'd only see the Americans and the medal winners with too much time spend on human interest backgrounds. Now in the streaming world I can watch every competitor. The good and the bad. Live as it happens.</p><p>The Russian Anzhelika Sidorova just cleared 4.85 on her first attempt. So did the Brit Holly Bradshaw and the American Katie Nageotte. The Greek Katerina Stefanidi missed her first attempt but decided to pass on the rest. All now go to 4.90 but Stefanidi only gets two attempts while the rest get three.</p><p>Stefanidi missed her first attempt at 4.90. She gets one attempt left.</p><p>Sidorova and Bradshaw fail to even reach the bar. Nageotte can't clear the bar.</p><p>Now the moment that means everything for Stefanidi. Her last attempt. Make it or the rest get the medals. Stefaidi fails to get a good plant and doesn't get into the air at all. Her Olympics are over.</p><p>Second attempt for the others. Sidorva and Bardshaw knock down the bar. Nageotte clears the bar, putting her in prime position. Go USA!</p><p>Imagine if we judged research papers this way. Either they get into a conference or they don't. Wait, that is they way they happen, although not always without biased judging.</p><p>Sidorova is passing on her last attempt at 4.90. Bradshaw goes for it but hits the bar. She has to settle for Bronze.</p><p>Bar is now at 4.95 meters. </p><p>Sidorova gets only one attempt at 4.95. If she makes it, she takes the lead, if she misses, she gets the silver. </p><p>Sidorova doesn't clear and the gold goes to the American Katie Nageotte! </p><p>Just for excitement Nageotte is going for 5.01 meters, which would be her first over five meters in competition. In the men's pole vault, the Swede Armand Duplantis (great pole vault name!) easily won the gold. He moved the bar to 6.19 meters to break his own world record. Came all so close in his first attempt but failed to clear. </p><p>Nageotte is just too excited winning the gold to focus enough to make a serious attempt at 5.01. Can't blame her.</p><p>Thus ends the best sport in the Olympics.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/s797/polevault.png"><img src="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/w400-h229/polevault.png" border="0" width="400" height="229" /></a></div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html"><span class="datestr">at August 05, 2021 01:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21878">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/">Let me tell you about three of my recent papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img width="605" alt="michaelrabin" src="https://gilkalai.files.wordpress.com/2021/08/michaelrabin.jpg" class="alignnone size-full wp-image-21893" height="403" /></p>
<p> </p>
<p>Let me tell you briefly about three of my papers that were recently accepted for publication. <a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i≥d and S⊂V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V∖σ];F)=0 for all i≥d and σ∈Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a> with Roy Meshulam, <a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly-type problems</a> with Imre Bárány, and <a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a> with Yosi Rinott and Tomer Shoham.</p>
<h3><a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i≥d and S⊂V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V∖σ];F)=0 for all i≥d and σ∈Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a></h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice. </em></span></p>
</blockquote>
<p>Roy and I have a<a href="https://scholar.google.com/scholar?hl=iw&amp;as_sdt=0%2C5&amp;q=kalai+and+meshulam&amp;btnG="> long term project</a> of studying topological Helly type theorems. Often, results from convexity give a simple and strong manifestation of theorems from topology: For example, Helly’s theorem manifests the nerve theorem from algebraic topology, and Radon’s theorem can be regarded as an early “linear” version of the Borsuk–Ulam theorem. We have a few more “linear” theorems in need of topologizing on our list. Actually the paper <a href="https://londmathsoc.onlinelibrary.wiley.com/doi/full/10.1112/mtk.12103">already appeared in Mathematika</a> on June 26, 2021. It is dedicated to our dear teacher, colleague and friend  Michael O. Rabin. </p>
<blockquote>
<h3><span style="color: #993300;"> Dedicated to Michael O. Rabin, a trailblazing mathematician and computer scientist</span></h3>
</blockquote>
<h3><a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly type problems</a>, to appear in the Bulletin of the American Mathematical Society </h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We present a variety of problems in the interface between combinatorics and geometry around the theorems of Helly, Radon, Carath ́eodory, and Tverberg. Through these problems we describe the fascinating area of Helly-type theorems, and explain some of its main themes and goals.</em></span></p>
</blockquote>
<p>Imre and I have long term common interest in Helly-type problems and often discussed it since we first met in 1982.  We wrote a first joint paper in 2016 and last year we wrote two additional papers with Attila Por.  Last year Imre wrote a great book  “Combinatorial convexity” (AMS, 2021, in press) largely devoted to Helly-type theorems. As for me, I plan on gradually writing on open problems related to my areas of interest. (See <a href="https://www.renyi.hu/conferences/erdos100/slides/kalai.pdf">these slides</a> for some problems.)   </p>
<h3><a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a></h3>
<p>Yosi Rinott, Tomer Shoham and I started this project about a year an a half ago. Our paper have now been accepted to Statistical Science where you can <a href="https://www.e-publications.org/ims/submission/STS/user/submissionFile/47360?confirm=ed13d436">download the accepted version</a> along <a href="https://imstat.org/journals-and-publications/statistical-science/statistical-science-future-papers/">many other future papers</a>. This is my second paper in Statistical Science. The first one was “<a href="https://projecteuclid.org/journals/statistical-science/volume-14/issue-2/Solving-the-Bible-Code-Puzzle/10.1214/ss/1009212243.full">Solving the bible code puzzle</a>” with Brendan McKay, Dror Bar-Nathan and Maya Bar-Hillel, that appeared in 1999.     </p>
<blockquote>
<p><span style="color: #ff0000;"><em>In quantum computing, a demonstration of quantum supremacy (or quantum advantage) consists of presenting a task, possibly of no practical value, whose computation is feasible on a quantum device, but cannot be performed by classical computers in any feasible amount of time. The notable claim of quantum supremacy presented by Google’s team in 2019 consists of demonstrating the ability of a quantum circuit to generate, albeit with considerable noise, bitstrings from a distribution that is considered hard to simulate on classical computers. Very </em></span><span style="color: #ff0000;"><em>recently, in 2020, a quantum supremacy claim was presented by a group from the University of Science and Technology of China, using a different technology and generating a different distribution, but sharing some statistical principles with Google’s demonstration. </em></span></p>
<p><span style="color: #ff0000;"><em>Verifying that the generated data is indeed from the claimed distribution and assessing the circuit’s noise level and its fidelity is a statistical undertaking. The objective of this paper is to explain the relations between quantum computing and some of the statistical aspects involved in demonstrating quantum supremacy in terms that are accessible to statisticians, computer scientists, and mathematicians. Starting with the statistical modeling and analysis in Google’s demonstration, which we explain, we study various estimators of the fidelity, and different approaches to testing the distributions generated by the quantum computer. We propose different noise models, and discuss their implications. A preliminary study of the Google data, focusing mostly on circuits of 12 and 14 qubits is given in different parts of the paper</em></span></p>
</blockquote>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/"><span class="datestr">at August 05, 2021 11:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19028">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/">Turning the Tables on Cheating?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Colonel Stok: Do you play chess?</em></font></p><font color="#0044cc"><em>
</em><p><em>
Harry Palmer: Yes, but I prefer a game with a better chance of cheating.</em><br />
<font color="#000000"></font></p><font color="#000000">
<p></p><p></p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/sleuth-1972-screencaps-michael-caine-5575427-550-330/" rel="attachment wp-att-19031"><img width="154" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/Sleuth-1972-Screencaps-michael-caine-5575427-550-330.jpg?resize=154%2C100&amp;ssl=1" class="alignright size-full wp-image-19031" height="100" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><i>Sleuth</i> <a href="https://www.storypick.com/memorable-michael-caine-roles/">source</a></font></td>
</tr>
</tbody>
</table>
<p></p><p>
Michael Caine played <a href="https://en.wikipedia.org/wiki/Harry_Palmer">Harry Palmer</a> in the movie <a href="https://www.quotes.net/mquote/35048">Funeral in Berlin</a>. This was released in 1966—long before computers could play terrific chess. Perhaps he would have a different answer today?</p>
<p>
Today I thought we might look at chess cheating in a way that complements what Ken does.<br />
<span id="more-19028"></span></p>
<p>
Of course Ken refers to Ken Regan. He is one of the world experts at detecting chess cheating. Detection is based solely on the statistics of move choice. The only data given to Ken about the games are the moves that were played and the overall time allowance. But this ignores the players and equipment on the scene. People can cheat in ways that are closer to issues in computer security and protocols.</p>
<p>
Caine knows a lot about the latter. He is a doyen of <a href="https://filmsane.com/5-of-my-favorite-michael-caine-heist-movies/">heist movies</a> and those increasingly involve security. In the 1966 comedy <a href="https://en.wikipedia.org/wiki/Gambit_(1966_film)">Gambit</a> it is as simple as working around an alarm, but the 1969 version of <a href="https://en.wikipedia.org/wiki/The_Italian_Job">The Italian Job</a> has a switch of computer data reels and jamming traffic cameras. His latest movie <a href="https://en.wikipedia.org/wiki/Tenet_(film)">Tenet</a> centers on an algorithm for inverting time and entropy on Earth. He is also a fan of chess. He tangles with his co-star Laurence Olivier <a href="http://www.chess-in-the-cinema.de/showfilm.php?filmfile=7258.txt&amp;pfad=7079">amid</a> chess sets in the movie <a href="https://en.wikipedia.org/wiki/Sleuth_(1972_film)">Sleuth</a>. Most of all, his character in 2009’s <a href="https://en.wikipedia.org/wiki/Harry_Brown_(film)">Harry Brown</a> is a chess player, who <a href="https://www.dailymotion.com/video/xqr41b">discourses</a> on the <a href="https://www.chessgames.com/perl/chessgame?gid=1044731">17th</a> (not 7th as said) game of the 1972 championship between Bobby Fischer and Boris Spassky. </p>
<p>
</p><p></p><h2> Cheating at Chess—the Easy Way </h2><p></p>
<p></p><p>
Derren Brown is an <a href="https://derrenbrown.co.uk">illusionist</a>—a magician. He claims that he is a weak chess player. But he had Britain’s Channel 4 broadcast him playing nine strong players, including two grandmasters. Yet he won the match <b>5-4</b>. </p>
<p>
This is how he did it. He used an ancient <a href="https://en.wikipedia.org/wiki/Cheating_in_chess#Simultaneous_games">trick</a>. Say he plays two games: one against Alice and one against Bob. He plays black against Alice and white against Bob. After he gets Alice’s first move he plays that exact move against Bob. Then after he gets Bob’s move he plays that one against Alice. And so on.</p>
<p>
Suppose he loses both games. Thus Alice wins and Bob wins. But that means that Bob’s answer to Alices’ move was a winner and so on. This is impossible and so he must win at least one of the games. Thus he wins one game unless both end in a tie. </p>
<p>
The illusion is, how could he win five games against nine players given two were grandmasters? Brown played one of the nine games for real—he won that one. The “table trick” only works for an even number of games. It is not, of course, a new <a href="https://en.chessbase.com/post/the-magical-che-experiment">idea</a>: </p>
<blockquote><p><b> </b> <em> Alekhine and his twice world championship challenger Bogoljubov were once challenged separately by a relative patzer to games of correspondence chess at money odds. In effect, of course, the anonymous opportunist was playing in neither game. The story goes that the two players, who were friends away from the board, met up one day and latched on to what was happening. </em>
</p></blockquote>
<p></p><p>
The imitation trick is a real potential issue in <em>Basque chess</em>, where two players play two games simultaneously, one as white and one as black. By copying each other’s moves, they would always tie. An early <a href="https://en.chessbase.com/post/che-magazine-basque-che-does-it-work-for-you-">description</a> noted the issue but Ken has not been able to find how the rules forbid it. </p>
<p>
A similar situation <a href="https://www.chess.com/article/view/chess-arbiters">happened</a> recently in a real tournament—a world championship qualifier, no less. Two games at adjacent tables played almost twenty of the same opening moves. The chief arbiter—someone Ken corresponds with several times a week—moved one of the games to a different area. International Master Danny Rensch, who heads the major online playing site <a href="https://www.chess.com/">Chess.com</a>, made a <a href="https://youtu.be/-6QX53BmDbg">video</a> “Are You Copying Me?” of the incident. None of the four players involved was cheating, but this illustrates the kind of people dynamics one needs to watch for.</p>
<p>
</p><p></p><h2> Cheating at Chess—the Too Easy Way </h2><p></p>
<p></p><p>
This is to cheat by consulting a computer program that is stronger than all human players, such as the free program <a href="https://en.wikipedia.org/wiki/Stockfish_(chess)">Stockfish</a>, without anything impeding one’s ability to access the program’s recommendations during the game. This is often the case in online chess without sophisticated measures to detect the access. </p>
<p>
Ken’s statistical model can still judge the moves, but this is after the fact. We would like to <em>prevent</em> cheating. This hasn’t happened. Already last year, Ken was <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">quoted</a> in the UK Guardian newspaper saying, “The pandemic has brought me as much work in a single day as I have had in a year previously.” A Wall Street Journal <a href="https://www.wsj.com/articles/the-real-queens-gambit-catching-chess-cheaters-11607439491">story</a> that featured Ken also noted:</p>
<blockquote><p><b> </b> <em> The data showed something curious. More people were playing chess. Yet the fair play violations were surging even faster than the number of overall games. “Which makes us think that there has been an uptick in the rate of cheating,” said Gerard Le-Marechal, head of cheat detection for Chess.com. </em>
</p></blockquote>
<p></p><p>
This year has brought no letup—see Ken’s statement prefacing a non-chess <a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/">post</a> that June and July were the worst. Chess.com and Lichess and other playing sites have the final say but Ken is often used for both early warning (his “screening” step is agile and gives officials an informative snapshot of an entire tournament) and for explaining verdicts afterwards, since his model is transparent and not compromised by divulging explanations.</p>
<p>
But again, this is after the fact. I recall a <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> we wrote about ways computer security is like “closing the barn door after the horse has already left.” The open question that I find interesting is not how cheaters can be detected, but is there some way to make it hard for them to cheat at all. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/cainebbb/" rel="attachment wp-att-19037"><img width="295" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/CaineBBB.jpg?resize=295%2C279&amp;ssl=1" class="aligncenter size-full wp-image-19037" height="279" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2">ARPANET history <a href="https://computer.howstuffworks.com/arpanet.htm#pt1">source</a><br />
</font>
</td>
</tr>
</tbody></table>
<p>
Of course, this is not just about chess, or other games forced online by the pandemic. It extends to administering courses and tests, at a time when the prospect of a “normal” in-person Fall semester is being roiled by the surge we’ve <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> and <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">tracked</a> on this blog. </p>
<p>
</p><p></p><h2> Cheating at Chess—the Harder Way </h2><p></p>
<p></p><p>
The number of ways people have cheated at in-person chess is legion. Wikipedia has a long <a href="https://en.wikipedia.org/wiki/Cheating_in_chess">list</a>. Ken put the ways in cases he’d encountered to a Dr. Seuss rhyme midway through his 2014 TEDx Buffalo <a href="https://www.youtube.com/watch?v=9W3D8xVAKao">talk</a>. </p>
<p>
On March 15, 2020, the New York Times published an <a href="https://www.nytimes.com/2020/03/15/sports/chess-cheating.html">article</a> on tech in chess cheating. It drew analogy to the Houston Astros scandal, including the same example we just <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">covered</a> of whether José Altuve was wired for his series-winning home run in 2019. It touched on online chess and quotes Le-Marechal but <em>showed no inkling of</em> the impending pandemic and its effect on chess. Its first sentence about chess alludes to the 1978 incident in which Viktor Korchnoi alleged that Anatoly Karpov could receive coded information about their match games via the flavor of yogurt delivered to the table. </p>
<p>
I, Ken writing this part, have been part of discussions of how a yogurt <em>spoon</em> dropped audibly could be one of myriad possible signals from the audience. The pandemic caused this year’s Tata Steel tournament to be played <a href="https://www.dutchnews.nl/news/2021/01/no-women-at-tata-steel-chess-this-year-but-game-is-growing-in-popularity/">without</a> audience, while some other elite events are played in an “<a href="https://www.chess.com/news/view/bilbao-chess-outside-in-a-glass-cube">aquarium</a>” with one-way glass. But that does not work for larger-scale Open tournaments. Jamming RF signals is generally illegal. I agree with those recommending that an illusionist like Brown—someone with an eagle eye for watching people—be employed to help the arbiters at large events.</p>
<p>
Yet for all the ways and means out there, it is still <em>hard</em> to cheat at in-person chess. Its state is one that organizers of <em>online chess</em> would gladly reach if they could. FIDE has promoted a <a href="https://en.chessbase.com/post/the-rise-of-hybrid-chess">hybrid</a> form in which players travel to regional rooms watched by arbiters, but this is hard to manage on large scale. Dick and I have debated all year what to do for online chess, and we’ve converged on two poles of answers.</p>
<p>
</p><p></p><h2> Way #1: Standardized Playing Tabletops </h2><p></p>
<p></p><p>
The paradox, noted this week by International Master Nisha Mohota in her recent <a href="https://youtu.be/hpLiG1UgIus">video</a> on cheating, is that the popularity of chess online has burgeoned during the pandemic. But this also enhances the following dilemma:</p>
<ul>
<li>
Having a second camera—side view supplementing screen view—has been an effective measure. <p></p>
</li><li>
But requiring even one camera has been an acknowledged obstacle to expanding the reach of chess tournaments. <p></p>
</li><li>
And it takes extra human resources to monitor two video feeds per player.
</li></ul>
<p>
Online education has also <a href="https://www.erasmusmagazine.nl/en/2021/01/15/students-will-have-to-use-phone-as-a-second-camera-in-proctored-online-exams/">recognized</a> the importance of a second camera, <a href="https://www.theabr.org/announcements/remote-exam-information">requiring</a> one in some cases. Yet allowing the user to control how the side camera is positioned may allow circumventions, and mandating one connotes distrust and negativity in a bare sense. </p>
<p>
My suggestion is to try to turn around the negative aspect into a positive by marketing a standardized and hopefully-inexpensive “Online Tabletop Arena.” It would have three walls to feel like an alcove. The walls, one with a side camera built in, would limit hand movements as well as sight lines. Standardization would lessen stigma and help monitoring. It could also be used for online test taking.</p>
<p>
</p><p></p><h2> Way #2: Give In </h2><p></p>
<p></p><p>
The real import of our mentioned security <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> is to stop trying to stick thumbs in all the dam holes. Mohota in her video laments kids being exposed to chess programs and advocates training without them, but that strikes us as trying to close ten thousand barn doors while a million free horses are out there. </p>
<p>
So let’s give in: Allow the players to use computers freely. The more, the merrier. But as in the 1967 Caine-as-Harry-Palmer movie <a href="https://en.wikipedia.org/wiki/Billion_Dollar_Brain">Billion Dollar Brain</a>, we reward the humans for how they <em>disobey</em> the computer calling the shots.</p>
<p>
One way to implement this would be to have the chess playing site appoint one unknown (say, randomly selected) strong chess program <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> as the official scorer of all games. A player’s score for a won or drawn game would be proportional to the total difference from <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> in Ken’s metrics. Perhaps credit could be given also for a valiantly lost game.</p>
<p>
This scheme would directly reward players for the amount of <em>non-</em>cheating they do. Or put more positively, human ingenuity apart from computers would bring the reward. The ability to sleuth strategy beyond computer moves was already <a href="https://www.psychologytoday.com/us/blog/seeing-what-others-dont/201710/the-age-centaurs">demonstrated</a> in so-called <a href="https://en.wikipedia.org/wiki/Advanced_chess">freestyle</a> tournaments held in 2007-08 and 2014. A particularly nice example of playing a sacrifice that the computer does not like was executed at turn 18 by Magnus Carlsen in his World Cup <a href="https://www.chessbomb.com/arena/2021-fide-world-cup/08-01-Fedoseev_Vladimir-Carlsen_Magnus">win</a> today over Vladimir Fedoseev.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We admit the tabletop suggestion is far from electrifying, but has anyone come up with better? As always we welcome suggestions from our readers, or pointers to forum discussions that you agree with.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/"><span class="datestr">at August 04, 2021 09:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">Complexity Postdoctoral Fellowship at Santa Fe Institute (apply by October 24, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Unique among postdocs, these 3-year fellowships offer the opportunity to join a collaborative community that nurtures creative, transdisciplinary thought in pursuit of key insights about the complex systems that matter most for science and society. Benefits include research/collaboration funds, paid family leave, and a professional leadership &amp; development program.</p>
<p>Website: <a href="https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021">https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021</a><br />
Email: sfifellowship@santafe.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/"><span class="datestr">at August 02, 2021 10:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
