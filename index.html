<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at September 08, 2020 09:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03260">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03260">A Potential Reduction Inspired Algorithm for Exact Max Flow in Almost $\widetilde{O}(m^{4/3})$ Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kathuria:Tarun.html">Tarun Kathuria</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03260">PDF</a><br /><b>Abstract: </b>We present an algorithm for computing $s$-$t$ maximum flows in directed
graphs in $\widetilde{O}(m^{4/3+o(1)}U^{1/3})$ time. Our algorithm is inspired
by potential reduction interior point methods for linear programming. Instead
of using scaled gradient/Newton steps of a potential function, we take the step
which maximizes the decrease in the potential value subject to advancing a
certain amount on the central path, which can be efficiently computed. This
allows us to trace the central path with our progress depending only
$\ell_\infty$ norm bounds on the congestion vector (as opposed to the $\ell_4$
norm required by previous works) and runs in $O(\sqrt{m})$ iterations. To
improve the number of iterations by establishing tighter bounds on the
$\ell_\infty$ norm, we then consider the weighted central path framework of
Madry \cite{M13,M16,CMSV17} and Liu-Sidford \cite{LS20}. Instead of changing
weights to maximize energy, we consider finding weights which maximize the
maximum decrease in potential value. Finally, similar to finding weights which
maximize energy as done in \cite{LS20} this problem can be solved by the
iterative refinement framework for smoothed $\ell_2$-$\ell_p$ norm flow
problems \cite{KPSW19} completing our algorithm. We believe our potential
reduction based viewpoint provides a versatile framework which may lead to
faster algorithms for max flow.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03260"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03242">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03242">PolyAdd: Polynomial Formal Verification of Adder Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Drechsler:Rolf.html">Rolf Drechsler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03242">PDF</a><br /><b>Abstract: </b>Only by formal verification approaches functional correctness can be ensured.
While for many circuits fast verification is possible, in other cases the
approaches fail. In general no efficient algorithms can be given, since the
underlying verification problem is NP-complete.
</p>
<p>In this paper we prove that for different types of adder circuits polynomial
verification can be ensured based on BDDs. While it is known that the output
functions for addition are polynomially bounded, we show in the following that
the entire construction process can be carried out in polynomial time. This is
shown for the simple Carry Ripple Adder, but also for fast adders like the
Conditional Sum Adder and the Carry Look Ahead Adder. Properties about the
adder function are proven and the core principle of polynomial verification is
described that can also be extended to other classes of functions and circuit
realizations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03242"><span class="datestr">at September 08, 2020 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03218">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03218">Fast simulation of planar Clifford circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gosset:David.html">David Gosset</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grier:Daniel.html">Daniel Grier</a>, Alex Kerzner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schaeffer:Luke.html">Luke Schaeffer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03218">PDF</a><br /><b>Abstract: </b>A general quantum circuit can be simulated in exponential time on a classical
computer. If it has a planar layout, then a tensor-network contraction
algorithm due to Markov and Shi has a runtime exponential in the square root of
its size, or more generally exponential in the treewidth of the underlying
graph. Separately, Gottesman and Knill showed that if all gates are restricted
to be Clifford, then there is a polynomial time simulation. We combine these
two ideas and show that treewidth and planarity can be exploited to improve
Clifford circuit simulation. Our main result is a classical algorithm with
runtime scaling asymptotically as $ n^{\omega/2}&lt;n^{1.19}$ which samples from
the output distribution obtained by measuring all $n$ qubits of a planar graph
state in given Pauli bases. Here $\omega$ is the matrix multiplication
exponent. We also provide a classical algorithm with the same asymptotic
runtime which samples from the output distribution of any constant-depth
Clifford circuit in a planar geometry. Our work improves known classical
algorithms with cubic runtime. A key ingredient is a mapping which, given a
tree decomposition of some graph $G$, produces a Clifford circuit with a
structure that mirrors the tree decomposition and which emulates measurement of
the quantum graph state corresponding to $G$. We provide a classical simulation
of this circuit with the runtime stated above for planar graphs and otherwise
$n t^{\omega-1}$ where $t$ is the width of the tree decomposition. The
algorithm also incorporates a matrix-multiplication-time version of the
Gottesman-Knill simulation of multi-qubit measurement on stabilizer states,
which may be of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03218"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03158">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03158">Efficient Network Reliability Computation in Uncertain Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Yuya Sasaki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujiwara:Yasuhiro.html">Yasuhiro Fujiwara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Onizuka:Makoto.html">Makoto Onizuka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03158">PDF</a><br /><b>Abstract: </b>Network reliability is an important metric to evaluate the connectivity among
given vertices in uncertain graphs. Since the network reliability problem is
known as #P-complete, existing studies have used approximation techniques. In
this paper, we propose a new sampling-based approach that efficiently and
accurately approximates network reliability. Our approach improves efficiency
by reducing the number of samples based on stratified sampling. We
theoretically guarantee that our approach improves the accuracy of
approximation by using lower and upper bounds of network reliability, even
though it reduces the number of samples. To efficiently compute the bounds, we
develop an extended BDD, called S2BDD. During constructing the S2BDD, our
approach employs dynamic programming for efficiently sampling possible graphs.
Our experiment with real datasets demonstrates that our approach is up to 51.2
times faster than the existing sampling-based approach with higher accuracy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03158"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03078">Achieving anonymity via weak lower bound constraints for k-median and k-means</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Anna Arutyunova, Melanie Schmidt <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03078">PDF</a><br /><b>Abstract: </b>We study $k$-clustering problems with lower bounds, including $k$-median and
$k$-means clustering with lower bounds. In addition to the point set $P$ and
the number of centers $k$, a $k$-clustering problem with (uniform) lower bounds
gets a number $B$. The solution space is restricted to clusterings where every
cluster has at least $B$ points. We demonstrate how to approximate $k$-median
with lower bounds via a reduction to facility location with lower bounds, for
which $O(1)$-approximation algorithms are known.
</p>
<p>Then we propose a new constrained clustering problem with lower bounds where
we allow points to be assigned multiple times (to different centers). This
means that for every point, the clustering specifies a set of centers to which
it is assigned. We call this clustering with weak lower bounds. We give an
$8$-approximation for $k$-median clustering with weak lower bounds and an
$O(1)$-approximation for $k$-means with weak lower bounds.
</p>
<p>We conclude by showing that at a constant increase in the approximation
factor, we can restrict the number of assignments of every point to $2$ (or, if
we allow fractional assignments, to $1+\epsilon$). This also leads to the first
bicritera approximation algorithm for $k$-means with (standard) lower bounds
where bicriteria is interpreted in the sense that the lower bounds are violated
by a constant factor.
</p>
<p>All algorithms in this paper run in time that is polynomial in $n$ and $k$
(and d for the Euclidean variants considered).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03078"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03076">Ray Tracing Structured AMR Data Using ExaBricks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wald:Ingo.html">Ingo Wald</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zellmann:Stefan.html">Stefan Zellmann</a>, Will Usher, Nate Morrical, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lang:Ulrich.html">Ulrich Lang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pascucci:Valerio.html">Valerio Pascucci</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03076">PDF</a><br /><b>Abstract: </b>Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to
adapt the domain resolution to save computation and storage, and has become one
of the dominant data representations used by scientific simulations; however,
efficiently rendering such data remains a challenge. We present an efficient
approach for volume- and iso-surface ray tracing of Structured AMR data on
GPU-equipped workstations, using a combination of two different data
structures. Together, these data structures allow a ray tracing based renderer
to quickly determine which segments along the ray need to be integrated and at
what frequency, while also providing quick access to all data values required
for a smooth sample reconstruction kernel. Our method makes use of the RTX ray
tracing hardware for surface rendering, ray marching, space skipping, and
adaptive sampling; and allows for interactive changes to the transfer function
and implicit iso-surfacing thresholds. We demonstrate that our method achieves
high performance with little memory overhead, enabling interactive high quality
rendering of complex AMR data sets on individual GPU workstations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03076"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03052">Faster motif counting via succinct color coding and adaptive sampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bressan:Marco.html">Marco Bressan</a>, Stefano Leucci, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panconesi:Alessandro.html">Alessandro Panconesi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03052">PDF</a><br /><b>Abstract: </b>We address the problem of computing the distribution of induced connected
subgraphs, aka \emph{graphlets} or \emph{motifs}, in large graphs. The current
state-of-the-art algorithms estimate the motif counts via uniform sampling, by
leveraging the color coding technique by Alon, Yuster and Zwick. In this work
we extend the applicability of this approach, by introducing a set of
algorithmic optimizations and techniques that reduce the running time and space
usage of color coding and improve the accuracy of the counts. To this end, we
first show how to optimize color coding to efficiently build a compact table of
a representative subsample of all graphlets in the input graph. For $8$-node
motifs, we can build such a table in one hour for a graph with $65$M nodes and
$1.8$B edges, which is $2000$ times larger than the state of the art. We then
introduce a novel adaptive sampling scheme that breaks the ``additive error
barrier'' of uniform sampling, guaranteeing multiplicative approximations
instead of just additive ones. This allows us to count not only the most
frequent motifs, but also extremely rare ones. For instance, on one graph we
accurately count nearly $10.000$ distinct $8$-node motifs whose relative
frequency is so small that uniform sampling would literally take centuries to
find them. Our results show that color coding is still the most promising
approach to scalable motif counting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03052"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03038">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03038">Multi-Pass Graph Streaming Lower Bounds for Cycle Counting, MAX-CUT, Matching Size, and Other Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assadi:Sepehr.html">Sepehr Assadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kol:Gillat.html">Gillat Kol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Raghuvansh_R=.html">Raghuvansh R. Saxena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Huacheng.html">Huacheng Yu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03038">PDF</a><br /><b>Abstract: </b>Consider the following gap cycle counting problem in the streaming model: The
edges of a $2$-regular $n$-vertex graph $G$ are arriving one-by-one in a stream
and we are promised that $G$ is a disjoint union of either $k$-cycles or
$2k$-cycles for some small $k$; the goal is to distinguish between these two
cases. Verbin and Yu [SODA 2011] introduced this problem and showed that any
single-pass streaming algorithm solving it requires $n^{1-\Omega(\frac{1}{k})}$
space. This result and the technique behind it---the Boolean Hidden
Hypermatching communication problem---has since been used extensively for
proving streaming lower bounds for various problems.
</p>
<p>Despite its significance and broad range of applications, the lower bound
technique of Verbin and Yu comes with a key weakness that is inherited by all
subsequent results: the Boolean Hidden Hypermatching problem is hard only if
there is exactly one round of communication and can be solved with logarithmic
communication in two rounds. Therefore, all streaming lower bounds derived from
this problem only hold for single-pass algorithms.
</p>
<p>We prove the first multi-pass lower bound for the gap cycle counting problem:
Any $p$-pass streaming algorithm that can distinguish between disjoint union of
$k$-cycles vs $2k$-cycles---or even $k$-cycles vs one Hamiltonian
cycle---requires $n^{1-\frac{1}{k^{\Omega(1/p)}}}$ space. As a corollary of
this result, we can extend many of previous lower bounds to multi-pass
algorithms. For instance, we can now prove that any streaming algorithm that
$(1+\epsilon)$-approximates the value of MAX-CUT, maximum matching size, or
rank of an $n$-by-$n$ matrix, requires either $n^{\Omega(1)}$ space or
$\Omega(\log{(\frac{1}{\epsilon})})$ passes. For all these problems, prior work
left open the possibility of even an $O(\log{n})$ space algorithm in only two
passes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03038"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02945">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02945">A Simpler NP-Hardness Proof for Familial Graph Compression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Ammar.html">Ammar Ahmed</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassan:Zohair_Raza.html">Zohair Raza Hassan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shabbir:Mudassir.html">Mudassir Shabbir</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02945">PDF</a><br /><b>Abstract: </b>This document presents a simpler proof showcasing the NP-hardness of Familial
Graph Compression.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02945"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02815">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02815">Optimal Inapproximability of Satisfiable $k$-LIN over Non-Abelian Groups</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhangale:Amey.html">Amey Bhangale</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khot:Subhash.html">Subhash Khot</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02815">PDF</a><br /><b>Abstract: </b>A seminal result of H\r{a}stad [J. ACM, 48(4):798--859, 2001] shows that it
is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$
fraction of the constraints of a given $k$-LIN instance over an abelian group,
even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the
constraints, for any constant $\varepsilon&gt;0$. Engebretsen et al. [Theoretical
Computer Science, 312(1):17--45, 2004] later showed that the same hardness
result holds for $k$-LIN instances over any finite non-abelian group.
</p>
<p>Unlike the abelian case, where we can efficiently find a solution if the
instance is satisfiable, in the non-abelian case, it is NP-complete to decide
if a given system of linear equations is satisfiable or not, as shown by
Goldmann and Russell [Information and Computation, 178(1):253--262. 2002].
</p>
<p>Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN
instance over $G$, one can in fact do better than just outputting a random
assignment using a simple but clever algorithm. The approximation factor
achieved by this algorithm varies with the underlying group. In this paper, we
show that this algorithm is {\em optimal} by proving a tight hardness of
approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$,
assuming $P \neq NP$.
</p>
<p>As a corollary, we also get $3$-query probabilistically checkable proofs with
perfect completeness over large alphabets with improved soundness.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02815"><span class="datestr">at September 08, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02778">On Hardness of Approximation of Parameterized Set Cover and Label Cover: Threshold Graphs from Error Correcting Codes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/S=:Karthik_C=.html">Karthik C. S.</a>, Inbal Livni-Navon <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02778">PDF</a><br /><b>Abstract: </b>In the $(k,h)$-SetCover problem, we are given a collection $\mathcal{S}$ of
sets over a universe $U$, and the goal is to distinguish between the case that
$\mathcal{S}$ contains $k$ sets which cover $U$, from the case that at least
$h$ sets in $\mathcal{S}$ are needed to cover $U$. Lin (ICALP'19) recently
showed a gap creating reduction from the $(k,k+1)$-SetCover problem on universe
of size $O_k(\log |\mathcal{S}|)$ to the
$\left(k,\sqrt[k]{\frac{\log|\mathcal{S}|}{\log\log |\mathcal{S}|}}\cdot
k\right)$-SetCover problem on universe of size $|\mathcal{S}|$. In this paper,
we prove a more scalable version of his result: given any error correcting code
$C$ over alphabet $[q]$, rate $\rho$, and relative distance $\delta$, we use
$C$ to create a reduction from the $(k,k+1)$-SetCover problem on universe $U$
to the $\left(k,\sqrt[2k]{\frac{2}{1-\delta}}\right)$-SetCover problem on
universe of size $\frac{\log|\mathcal{S}|}{\rho}\cdot|U|^{q^k}$.
</p>
<p>Lin established his result by composing the input SetCover instance (that has
no gap) with a special threshold graph constructed from extremal combinatorial
object called universal sets, resulting in a final SetCover instance with gap.
Our reduction follows along the exact same lines, except that we generate the
threshold graphs specified by Lin simply using the basic properties of the
error correcting code $C$.
</p>
<p>We use the same threshold graphs mentioned above to prove inapproximability
results, under W[1]$\neq$FPT and ETH, for the $k$-MaxCover problem introduced
by Chalermsook et al. (SICOMP'20). Our inapproximaiblity results match the
bounds obtained by Karthik et al. (JACM'19), although their proof framework is
very different, and involves generalization of the distributed PCP framework.
Prior to this work, it was not clear how to adopt the proof strategy of Lin to
prove inapproximability results for $k$-MaxCover.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02778"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02717">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02717">Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chattopadhyay:Arkadev.html">Arkadev Chattopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sherif:Suhail.html">Suhail Sherif</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02717">PDF</a><br /><b>Abstract: </b>We give improved separations for the query complexity analogue of the
log-approximate-rank conjecture i.e. we show that there are a plethora of total
Boolean functions on $n$ input bits, each of which has approximate Fourier
sparsity at most $O(n^3)$ and randomized parity decision tree complexity
$\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and
Sherif (JACM '20) both qualitatively (in terms of designing a large number of
examples) and quantitatively (improving the gap from quartic to cubic). We
leave open the problem of proving a randomized communication complexity lower
bound for XOR compositions of our examples. A linear lower bound would lead to
new and improved refutations of the log-approximate-rank conjecture. Moreover,
if any of these compositions had even a sub-linear cost randomized
communication protocol, it would demonstrate that randomized parity decision
tree complexity does not lift to randomized communication complexity in general
(with the XOR gadget).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02717"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02710">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02710">Multi-Way Number Partitioning: an Information-Theoretic View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Niloufar Ahmadypour, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gohari:Amin.html">Amin Gohari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02710">PDF</a><br /><b>Abstract: </b>The number partitioning problem is the problem of partitioning a given list
of numbers into multiple subsets so that the sum of the numbers in each subset
are as nearly equal as possible. We introduce two closely related notions of
the "most informative" and "most compressible" partitions. Most informative
partitions satisfy a principle of optimality property. We also give an exact
algorithm (based on Huffman coding) with a running time of O(nlog(n)) in input
size n to find the most compressible partition.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02710"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02702">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02702">The Mixture Graph-A Data Structure for Compressing, Rendering, and Querying Segmentation Histograms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Khaled Al-Thelaya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agus:Marco.html">Marco Agus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schneider:Jens.html">Jens Schneider</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02702">PDF</a><br /><b>Abstract: </b>In this paper, we present a novel data structure, called the Mixture Graph.
This data structure allows us to compress, render, and query segmentation
histograms. Such histograms arise when building a mipmap of a volume containing
segmentation IDs. Each voxel in the histogram mipmap contains a convex
combination (mixture) of segmentation IDs. Each mixture represents the
distribution of IDs in the respective voxel's children. Our method factorizes
these mixtures into a series of linear interpolations between exactly two
segmentation IDs. The result is represented as a directed acyclic graph (DAG)
whose nodes are topologically ordered. Pruning replicate nodes in the tree
followed by compression allows us to store the resulting data structure
efficiently. During rendering, transfer functions are propagated from sources
(leafs) through the DAG to allow for efficient, pre-filtered rendering at
interactive frame rates. Assembly of histogram contributions across the
footprint of a given volume allows us to efficiently query partial histograms,
achieving up to 178$\times$ speed-up over na$\mathrm{\"{i}}$ve parallelized
range queries. Additionally, we apply the Mixture Graph to compute correctly
pre-filtered volume lighting and to interactively explore segments based on
shape, geometry, and orientation using multi-dimensional transfer functions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02702"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02668">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02668">A Framework for Private Matrix Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Upadhyay:Jalaj.html">Jalaj Upadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Upadhyay:Sarvagya.html">Sarvagya Upadhyay</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02668">PDF</a><br /><b>Abstract: </b>We study private matrix analysis in the sliding window model where only the
last $W$ updates to matrices are considered useful for analysis. We give first
efficient $o(W)$ space differentially private algorithms for spectral
approximation, principal component analysis, and linear regression. We also
initiate and show efficient differentially private algorithms for two important
variants of principal component analysis: sparse principal component analysis
and non-negative principal component analysis. Prior to our work, no such
result was known for sparse and non-negative differentially private principal
component analysis even in the static data setting. These algorithms are
obtained by identifying sufficient conditions on positive semidefinite matrices
formed from streamed matrices. We also show a lower bound on space required to
compute low-rank approximation even if the algorithm gives multiplicative
approximation and incurs additive error. This follows via reduction to a
certain communication complexity problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02668"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02664">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02664">Strong rainbow disconnection in graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bai:Xuqing.html">Xuqing Bai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xueliang.html">Xueliang Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02664">PDF</a><br /><b>Abstract: </b>Let $G$ be a nontrivial edge-colored connected graph. An edge-cut $R$ of $G$
is called a {\it rainbow edge-cut} if no two edges of $R$ are colored with the
same color. For two distinct vertices $u$ and $v$ of $G$, if an edge-cut
separates them, then the edge-cut is called a {\it $u$-$v$-edge-cut}. An
edge-colored graph $G$ is called \emph{strong rainbow disconnected} if for
every two distinct vertices $u$ and $v$ of $G$, there exists a both rainbow and
minimum $u$-$v$-edge-cut ({\it rainbow minimum $u$-$v$-edge-cut} for short) in
$G$, separating them, and this edge-coloring is called a {\it strong rainbow
disconnection coloring} (srd-{\it coloring} for short) of $G$. For a connected
graph $G$, the \emph{strong rainbow disconnection number} (srd-{\it number} for
short) of $G$, denoted by $\textnormal{srd}(G)$, is the smallest number of
colors that are needed in order to make $G$ strong rainbow disconnected.
</p>
<p>In this paper, we first characterize the graphs with $m$ edges such that
$\textnormal{srd}(G)=k$ for each $k \in \{1,2,m\}$, respectively, and we also
show that the srd-number of a nontrivial connected graph $G$ equals the maximum
srd-number among the blocks of $G$. Secondly, we study the srd-numbers for the
complete $k$-partite graphs, $k$-edge-connected $k$-regular graphs and grid
graphs. Finally, we show that for a connected graph $G$, to compute
$\textnormal{srd}(G)$ is NP-hard. In particular, we show that it is already
NP-complete to decide if $\textnormal{srd}(G)=3$ for a connected cubic graph.
Moreover, we show that for a given edge-colored (with an unbounded number of
colors) connected graph $G$ it is NP-complete to decide whether $G$ is strong
rainbow disconnected.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02664"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02660">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02660">Length-optimal tool path planning for freeform surfaces with preferred feed directions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Qiang.html">Qiang Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Charlie_C=_L=.html">Charlie C. L. Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Hsi=Yung.html">Hsi-Yung Feng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02660">PDF</a><br /><b>Abstract: </b>This paper presents a new method to generate tool paths for machining
freeform surfaces represented either as parametric surfaces or as triangular
meshes. This method allows for the optimal tradeoff between the preferred feed
direction field and the constant scallop height, and yields a minimized overall
path length. The optimality is achieved by formulating tool path planning as a
Poisson problem that minimizes a simple, quadratic energy. This Poisson
formulation considers all tool paths at once, without resorting to any
heuristic sampling or initial tool path choosing as in existing methods, and is
thus a globally optimal solution. Finding the optimal tool paths amounts to
solving a well-conditioned sparse linear system, which is computationally
convenient and efficient. Tool paths are represented with an implicit scheme
that can completely avoid the challenging topological issues of path
singularities and self-intersections seen in previous methods. The presented
method has been validated with a series of examples and comparisons.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02660"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02618">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02618">A Tensor Network based Decision Diagram for Representation of Quantum Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hong:Xin.html">Xin Hong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Xiangzhen.html">Xiangzhen Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Sanjiang.html">Sanjiang Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Yuan.html">Yuan Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ying:Mingsheng.html">Mingsheng Ying</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02618">PDF</a><br /><b>Abstract: </b>Tensor networks have been successfully applied in simulation of quantum
physical systems for decades. Recently, they have also been employed in
classical simulation of quantum computing, in particular, random quantum
circuits. This paper proposes a decision-diagram style data structure, called
TDD (Tensor Decision Diagram), for more principled and convenient applications
of tensor networks. This new data structure provides a compact and canonical
representation for quantum circuits. By exploiting circuit partition, the TDD
of a quantum circuit can be computed efficiently. Furthermore, we show that the
operations of tensor networks essential in their applications (e.g., addition
and contraction), can also be implemented efficiently in TDDs. A
proof-of-concept implementation of TDDs is presented and its efficiency is
evaluated on a set of benchmark quantum circuits. It is expected that TDDs will
play an important role in various design automation tasks related to quantum
circuits, including but not limited to equivalence checking, error detection,
synthesis, simulation, and verification.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02618"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02595">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02595">Explicit near-fully X-Ramanujan graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ryan O'Donnell, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Xinyu.html">Xinyu Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02595">PDF</a><br /><b>Abstract: </b>Let $p(Y_1, \dots, Y_d, Z_1, \dots, Z_e)$ be a self-adjoint noncommutative
polynomial, with coefficients from $\mathbb{C}^{r \times r}$, in the
indeterminates $Y_1, \dots, Y_d$ (considered to be self-adjoint), the
indeterminates $Z_1, \dots, Z_e$, and their adjoints $Z_1^*, \dots, Z_e^*$.
Suppose $Y_1, \dots, Y_d$ are replaced by independent random $n \times n$
matching matrices, and $Z_1, \dots, Z_e$ are replaced by independent random $n
\times n$ permutation matrices. Assuming for simplicity that $p$'s coefficients
are $0$-$1$ matrices, the result can be thought of as a kind of random
$rn$-vertex graph $G$. As $n \to \infty$, there will be a natural limiting
infinite graph $X$ that covers any finite outcome for $G$. A recent landmark
result of Bordenave and Collins shows that for any $\varepsilon &gt; 0$, with high
probability the spectrum of a random $G$ will be $\varepsilon$-close in
Hausdorff distance to the spectrum of $X$ (once the suitably defined "trivial"
eigenvalues are excluded). We say that $G$ is "$\varepsilon$-near fully
$X$-Ramanujan". Our work has two contributions: First we study and clarify the
class of infinite graphs $X$ that can arise in this way. Second, we derandomize
the Bordenave-Collins result: for any $X$, we provide explicit, arbitrarily
large graphs $G$ that are covered by $X$ and that have (nontrivial) spectrum at
Hausdorff distance at most $\varepsilon$ from that of $X$. This significantly
generalizes the recent work of Mohanty et al., which provided explicit
near-Ramanujan graphs for every degree $d$ (meaning $d$-regular graphs with all
nontrivial eigenvalues bounded in magnitude by $2\sqrt{d-1} + \varepsilon$). As
an application of our main technical theorem, we are also able to determine the
"eigenvalue relaxation value" for a wide class of average-case degree-$2$
constraint satisfaction problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02595"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02584">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02584">Deterministic Decremental Reachability, SCC, and Shortest Paths via Directed Expanders and Congestion Balancing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernstein:Aaron.html">Aaron Bernstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gutenberg:Maximilian_Probst.html">Maximilian Probst Gutenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02584">PDF</a><br /><b>Abstract: </b>Let $G = (V,E,w)$ be a weighted, digraph subject to a sequence of adversarial
edge deletions. In the decremental single-source reachability problem (SSR), we
are given a fixed source $s$ and the goal is to maintain a data structure that
can answer path-queries $s \rightarrowtail v$ for any $v \in V$. In the more
general single-source shortest paths (SSSP) problem the goal is to return an
approximate shortest path to $v$, and in the SCC problem the goal is to
maintain strongly connected components of $G$ and to answer path queries within
each component. All of these problems have been very actively studied over the
past two decades, but all the fast algorithms are randomized and, more
significantly, they can only answer path queries if they assume a weaker model:
they assume an oblivious adversary which is not adaptive and must fix the
update sequence in advance. This assumption significantly limits the use of
these data structures, most notably preventing them from being used as
subroutines in static algorithms. All the above problems are notoriously
difficult in the adaptive setting. In fact, the state-of-the-art is still the
Even and Shiloach tree, which dates back all the way to 1981 and achieves total
update time $O(mn)$. We present the first algorithms to break through this
barrier:
</p>
<p>1) deterministic decremental SSR/SCC with total update time $mn^{2/3 + o(1)}$
</p>
<p>2) deterministic decremental SSSP with total update time $n^{2+2/3+o(1)}$.
</p>
<p>To achieve these results, we develop two general techniques of broader
interest for working with dynamic graphs: 1) a generalization of expander-based
tools to dynamic directed graphs, and 2) a technique that we call congestion
balancing and which provides a new method for maintaining flow under
adversarial deletions. Using the second technique, we provide the first
near-optimal algorithm for decremental bipartite matching.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02584"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02553">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02553">Revisiting Co-Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luo:Luo.html">Luo Luo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Cheng.html">Cheng Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xie:Guangzeng.html">Guangzeng Xie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Haishan.html">Haishan Ye</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02553">PDF</a><br /><b>Abstract: </b>We study the streaming model for approximate matrix multiplication (AMM). We
are interested in the scenario that the algorithm can only take one pass over
the data with limited memory. The state-of-the-art deterministic sketching
algorithm for streaming AMM is the co-occurring directions (COD), which has
much smaller approximation errors than randomized algorithms and outperforms
other deterministic sketching methods empirically. In this paper, we provide a
tighter error bound for COD whose leading term considers the potential
approximate low-rank structure and the correlation of input matrices. We prove
COD is space optimal with respect to our improved error bound. We also propose
a variant of COD for sparse matrices with theoretical guarantees. The
experiments on real-world sparse datasets show that the proposed algorithm is
more efficient than baseline methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02553"><span class="datestr">at September 08, 2020 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02480">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02480">Trimmed Spline Surfaces with Accurate Boundary Control</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Florian Martin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reif:Ulrich.html">Ulrich Reif</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02480">PDF</a><br /><b>Abstract: </b>We introduce trimmed NURBS surfaces with accurate boundary control, briefly
called ABC-surfaces, as a solution to the notorious problem of constructing
watertight or smooth ($G^1$ and $G^2)$ multi-patch surfaces within the function
range of standard CAD/CAM systems and the associated file exchange formats. Our
construction is based on the appropriate blend of a base surface, which traces
out the intended global shape, and a series of reparametrized ribbons, which
dominate the shape near the boundary.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02480"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02452">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02452">A Lower Bound on Determinantal Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Mrinal.html">Mrinal Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Volk:Ben_Lee.html">Ben Lee Volk</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02452">PDF</a><br /><b>Abstract: </b>The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1, \ldots,
x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$
whose entries are affine functions in $\mathbb{F}[x_1, \ldots, x_n]$ such that
$P = Det(M)$. We prove that the determinantal complexity of the polynomial
$\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$.
</p>
<p>For every $n$-variate polynomial of degree $d$, the determinantal complexity
is trivially at least $d$, and it is a long standing open problem to prove a
lower bound which is super linear in $\max\{n,d\}$. Our result is the first
lower bound for any explicit polynomial which is bigger by a constant factor
than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved
by Alper, Bogart and Velasco [ABV17] for the same polynomial.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02452"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02388">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02388">On Communication Compression for Distributed Optimization on Heterogeneous Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stich:Sebastian_U=.html">Sebastian U. Stich</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02388">PDF</a><br /><b>Abstract: </b>Lossy gradient compression, with either unbiased or biased compressors, has
become a key tool to avoid the communication bottleneck in centrally
coordinated distributed training of machine learning models. We analyze the
performance of two standard and general types of methods: (i) distributed
quantized SGD (D-QSGD) with arbitrary unbiased quantizers and (ii) distributed
SGD with error-feedback and biased compressors (D-EF-SGD) in the heterogeneous
(non-iid) data setting.
</p>
<p>Our results indicate that D-EF-SGD is much less affected than D-QSGD by
non-iid data, but both methods can suffer a slowdown if data-skewness is high.
We propose two alternatives that are not (or much less) affected by
heterogenous data distributions: a new method that is only applicable to
strongly convex problems, and we point out a more general approach that is
applicable to linear compressors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02388"><span class="datestr">at September 08, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">Eberhard’s theorem for bipartite polyhedra with one big face</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://en.wikipedia.org/wiki/Eberhard%27s_theorem">Eberhard’s theorem</a> is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after <a href="https://en.wikipedia.org/wiki/Victor_Eberhard">Victor Eberhard</a>, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a>, <a href="https://en.wikipedia.org/wiki/Lev_Pontryagin">Lev Pontryagin</a>, and <a href="https://en.wikipedia.org/wiki/Bernard_Morin">Bernard Morin</a> also come to mind, and there are more.</p>

<p>Anyway, Eberhard’s theorem concerns the following question. Suppose I tell you that a polyhedron has a certain number of faces of certain types. For instance, after Archimedes’ work on polytopes was lost, all we knew about the <a href="https://en.wikipedia.org/wiki/Archimedean_solid">Archimedean solids</a> until their rediscovery in the Renaissance was a brief listing from <a href="https://en.wikipedia.org/wiki/Pappus_of_Alexandria">Pappus of Alexandria</a> giving this information: there is one with 8 triangles and 6 squares, etc. How can we tell that these counts of faces actually determine a polyhedron?</p>

<p>The given information for Eberhard’s theorem, then, is just a collection of counts of face types (triangles, quadrilaterals, etc.), without specifying the exact shapes of these faces. The goal is to use these faces to build a simple polyhedron, one for which three edges meet at every vertex (like a cube, unlike an octahedron). One necessary condition for this to be possible is that the polyhedron must obey Euler’s polyhedral formula \(v-e+f=2\). And it’s easy to calculate the numbers of vertices, edges, and faces appearing in this formula, from the face counts. Plugging these numbers into Euler’s formula leads to a linear equation that the face counts must obey. Crucially, this linear equation omits the count of hexagons: adding or removing hexagons will not change whether Euler’s formula holds. What Eberhard’s theorem states is that, as long as the face counts obey Euler’s formula in this way, there is always some number of hexagons that can be added or removed so that the remaining faces will form a polyhedron.</p>

<p>However, calculating the fewest number of hexagons needed, or even determining whether a given number of faces of all types (including hexagons) can be put together into a polyhedron, remains somewhat mysterious. So I thought I’d play with a case that would be both simple enough to solve and still interesting: the bipartite simple polyhedra (famous from <a href="https://en.wikipedia.org/wiki/Barnette%27s_conjecture">Barnette’s conjecture</a>), with one big face (a \(2n\)-gon for some \(n&gt;3\)), many small faces (\(n+3\) quadrilaterals, the number needed to make Euler’s formula hold), and a mysterious number of hexagons. What is the smallest number of hexagons that will allow the construction of a simple polyhedron with these face counts? The answer turns out to be \(\lfloor (3n-6)/2\rfloor\), achieved with polyhedra (or polyhedral graphs) in which the outer \(2n\)-gon surrounds a <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus tree</a> of 6-vertex cycles (and possibly one 4-vertex cycle), connected to each other by bridge edges:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/eberhard.svg" alt="Three hexagon-minimal bipartite simple polyhedra" /></p>

<p>The central cactus tree can be rearranged, as long as no two bridge edges have adjacent endpoints. For instance, in the graph with the dodecagon outer face, at the bottom of the figure, it’s possible for the middle six-vertex loop to have three connections to the outside polygon on one side and only one connection on the other side, or to have the four-vertex loop in the middle. But I can prove that all optimal solutions have the same overall central cactus tree structure.</p>

<p>I find it easier to think about the following equivalent rephrasing of the optimization problem: instead of finding a minimum number of hexagons that will allow us to build a polyhedron with those face counts, let’s build a polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons, and concentrate on minimizing the number of vertices in this polyhedron. The number of quadrilaterals will automatically come out right, and the number of hexagons will be minimized if the number of vertices is minimized.</p>

<p>Now suppose that we have any simple polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons. Remove the outer \(2n\)-gon from the graph, leaving a conncted subgraph, and look at the biconnected components of this subgraph. For any one component, its outer face in its induced planar embedding must be a simple cycle, with some vertices having degree two in the component (the endpoints of edges connecting the component to the rest of the graph) and some having degree three. If the component is a 4-cycle or 6-cycle, then all of its vertices have degree two. But if not, then at most four consecutive vertices of its outer cycle can have degree two, because they and the two vertices connected to them on both sides form part of the boundary of a face interior to the component, which can have at most six vertices. And the degree-three vertices of the outer cycle must come in consecutive pairs, which cannot be adjacent to the endpoints of bridge edges connecting to other biconnected components, because a degree-three vertex next to a bridge edge or next to two other degree-three vertices would combine with part of the outer \(2n\)-gon to form a face with seven or more vertices, and a degree-three vertex by itself would form a pentagon, neither of which is allowed.</p>

<p>So in a component that is not a 4-cycle or 6-cycle, the degree-two and degree-three vertices alternate around the outer cycle of the component in consecutive sequences of at most four and exactly two vertices. This implies that the number of degree-two vertices is even (because the whole cycle is even by bipartiteness) and that the number of degree-three vertices in the component (even just counting the ones on its boundary) is at least half of the number of degree-two vertices on its boundary. For the cactus trees that we’ve been using, on the other hand, the number of degree-three vertices in each cactus tree is strictly less than half of the number of degree-two vertices. So if we replace a whole non-cycle component by a cactus tree, we can get a graph with the same number of exposed degree-2 vertices, but fewer total vertices. After repeated replacement of biconnected components, at each step reducing the number of vertices, we would reach a state where the subgraph inside the \(2n\)-gon is a cactus tree. It might not meet the requirement that its bridge edges have nonadjacent endpoints, but it could always be rearranged to do so. And it might not be a cactus with at most one 4-cycle, but if not we could replace two 4-cycles by one 6-cycle and make it even smaller. So the only graphs that cannot be made smaller are the ones we started with, the cactus trees of 6-cycles and at most one 4-cycle, surrounded by an outer \(2n\)-gon.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104827671950147352">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html"><span class="datestr">at September 07, 2020 10:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1108">IGAFIT Algorithmic Colloquium</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" target="_blank" rel="noreferrer noopener">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" target="_blank" rel="noreferrer noopener">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br />Vera Traub, University of Bonn<br />Title: An improved approximation algorithm for ATSP<br />Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br />Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br />Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br />Nathan Klein, University of Bonn<br />Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br />Nikhil Bansal<br />Artur Czumaj<br />Andreas Feldmann<br />Adi Rosén<br />Eva Rotenberg<br />Piotr Sankowski<br />Christian Sohler <br /></p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1108"><span class="datestr">at September 07, 2020 08:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2727">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/">The many faces of integration by parts – II : Randomized smoothing and score functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="556" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" class="wp-image-4633" height="254" />Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="357" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" class="wp-image-4598" height="292" />Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="491" alt="" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" class="wp-image-4545" height="322" />Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="446" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" class="wp-image-4605" height="225" />Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img width="432" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" class="wp-image-4624" height="288" />Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br />[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br />[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br />[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br />[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br />[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br />[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br />[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br />[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br />[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br />[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br />[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br />[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br />[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br />[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br />[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br />[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br />[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br />[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br />[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br />[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br />[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br />[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br />[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/"><span class="datestr">at September 07, 2020 07:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/132">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/132">TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/132"><span class="datestr">at September 07, 2020 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02233">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02233">Access-Adaptive Priority Search Tree</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Massa:Haley.html">Haley Massa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlmann:Jeffrey.html">Jeffrey Uhlmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02233">PDF</a><br /><b>Abstract: </b>In this paper we show that the priority search tree of McCreight, which was
originally developed to satisfy a class of spatial search queries on
2-dimensional points, can be adapted to the problem of dynamically maintaining
a set of keys so that the query complexity adapts to the distribution of
queried keys. Presently, the best-known example of such a data structure is the
splay tree, which dynamically reconfigures itself during each query so that
frequently accessed keys move to the top of the tree and thus can be retrieved
with fewer queries than keys that are lower in the tree. However, while the
splay tree is conjectured to offer optimal adaptive amortized query complexity,
it may require O(n) for individual queries. We show that an access-adaptive
priority search tree (AAPST) can provide competitive adaptive query performance
while ensuring O(log n) worst-case query performance, thus potentially making
it more suitable for certain interactive (e.g.,online and real-time)
applications for which the response time must be bounded.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02233"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02207">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02207">Fair and Useful Cohort Selection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Niklas Smedemark-Margulies, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langton:Paul.html">Paul Langton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02207">PDF</a><br /><b>Abstract: </b>As important decisions about the distribution of society's resources become
increasingly automated, it is essential to consider the measurement and
enforcement of fairness in these decisions. In this work we build on the
results of Dwork and Ilvento ITCS'19, which laid the foundations for the study
of fair algorithms under composition. In particular, we study the cohort
selection problem, where we wish to use a fair classifier to select $k$
candidates from an arbitrarily ordered set of size $n&gt;k$, while preserving
individual fairness and maximizing utility. We define a linear utility function
to measure performance relative to the behavior of the original classifier. We
develop a fair, utility-optimal $O(n)$-time cohort selection algorithm for the
offline setting, and our primary result, a solution to the problem in the
streaming setting that keeps no more than $O(k)$ pending candidates at all
time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02207"><span class="datestr">at September 07, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01986">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01986">Smoothed analysis of the condition number under low-rank perturbations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Rikhav.html">Rikhav Shah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01986">PDF</a><br /><b>Abstract: </b>Let $M$ be an arbitrary $n$ by $n$ matrix of rank $n-k$. We study the
condition number of $M$ plus a \emph{low rank} perturbation $UV^T$ where $U, V$
are $n$ by $k$ random Gaussian matrices. Under some necessary assumptions, it
is shown that $M+UV^T$ is unlikely to have a large condition number. The main
advantages of this kind of perturbation over the well-studied dense Gaussian
perturbation where every entry is independently perturbed is the $O(nk)$ cost
to store $U,V$ and the $O(nk)$ increase in time complexity for performing the
matrix-vector multiplication $(M+UV^T)x$. This improves the $\Omega(n^2)$ space
and time complexity increase required by a dense perturbation, which is
especially burdensome if $M$ is originally sparse. We experimentally validate
our approach and consider generalizations to symmetric and complex settings.
Lastly, we show barriers in applying our low rank model to other problems
studied in the smoothed analysis framework.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01986"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01947">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01947">Nearly Linear-Time, Parallelizable Algorithms for Non-Monotone Submodular Maximization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01947">PDF</a><br /><b>Abstract: </b>We study parallelizable algorithms for maximization of a submodular function,
not necessarily monotone, with respect to a cardinality constraint $k$. We
improve the best approximation factor achieved by an algorithm that has optimal
adaptivity and query complexity, up to logarithmic factors in the size $n$ of
the ground set, from $0.039 - \epsilon$ to $0.193 - \epsilon$. We provide two
algorithms; the first has approximation ratio $1/6 - \epsilon$, adaptivity $O(
\log n )$, and query complexity $O( n \log k )$, while the second has
approximation ratio $0.193 - \epsilon$, adaptivity $O( \log^2 n )$, and query
complexity $O(n \log k)$. Heuristic versions of our algorithms are empirically
validated to use a low number of adaptive rounds and total queries while
obtaining solutions with high objective value in comparison with highly
adaptive approximation algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01947"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01928">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01928">Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Quintino Francesco Lotito, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montresor:Alberto.html">Alberto Montresor</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01928">PDF</a><br /><b>Abstract: </b>Over the last decade, there has been an increasing interest in temporal
graphs, pushed by a growing availability of temporally-annotated network data
coming from social, biological and financial networks. Despite the importance
of analyzing complex temporal networks, there is a huge gap between the set of
definitions, algorithms and tools available to study large static graphs and
the ones available for temporal graphs. An important task in temporal graph
analysis is mining dense structures, i.e., identifying high-density subgraphs
together with the span in which this high density is observed. In this paper,
we introduce the concept of $(k, \Delta)$-truss (span-truss) in temporal
graphs, a temporal generalization of the $k$-truss, in which $k$ captures the
information about the density and $\Delta$ captures the time span in which this
density holds. We then propose novel and efficient algorithms to identify
maximal span-trusses, namely the ones not dominated by any other span-truss
neither in the order $k$ nor in the interval $\Delta$, and evaluate them on a
number of public available datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01928"><span class="datestr">at September 07, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01874">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01874">Sum-of-Squares Lower Bounds for Sherrington-Kirkpatrick via Planted Affine Planes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Mrinalkanti.html">Mrinalkanti Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeronimo:Fernando_Granha.html">Fernando Granha Jeronimo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Chris.html">Chris Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potechin:Aaron.html">Aaron Potechin</a>, Goutham Rajendran <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01874">PDF</a><br /><b>Abstract: </b>The Sum-of-Squares (SoS) hierarchy is a semi-definite programming
meta-algorithm that captures state-of-the-art polynomial time guarantees for
many optimization problems such as Max-$k$-CSPs and Tensor PCA. On the flip
side, a SoS lower bound provides evidence of hardness, which is particularly
relevant to average-case problems for which NP-hardness may not be available.
</p>
<p>In this paper, we consider the following average case problem, which we call
the \emph{Planted Affine Planes} (PAP) problem: Given $m$ random vectors
$d_1,\ldots,d_m$ in $\mathbb{R}^n$, can we prove that there is no vector $v \in
\mathbb{R}^n$ such that for all $u \in [m]$, $\langle v, d_u\rangle^2 = 1$? In
other words, can we prove that $m$ random vectors are not all contained in two
parallel hyperplanes at equal distance from the origin? We prove that for $m
\leq n^{3/2-\epsilon}$, with high probability, degree-$n^{\Omega(\epsilon)}$
SoS fails to refute the existence of such a vector $v$.
</p>
<p>When the vectors $d_1,\ldots,d_m$ are chosen from the multivariate normal
distribution, the PAP problem is equivalent to the problem of proving that a
random $n$-dimensional subspace of $\mathbb{R}^m$ does not contain a boolean
vector. As shown by Mohanty--Raghavendra--Xu [STOC 2020], a lower bound for
this problem implies a lower bound for the problem of certifying energy upper
bounds on the Sherrington-Kirkpatrick Hamiltonian, and so our lower bound
implies a degree-$n^{\Omega(\epsilon)}$ SoS lower bound for the certification
version of the Sherrington-Kirkpatrick problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01874"><span class="datestr">at September 07, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-83517349672236531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">Two Math Problems of interest (at least to me)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br /></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br /></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br /></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br /></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html"><span class="datestr">at September 06, 2020 09:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/131">TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/131"><span class="datestr">at September 06, 2020 05:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/130">TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/130"><span class="datestr">at September 06, 2020 04:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/">Closing An Open Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Crawl, then walk, then run.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img width="150" alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" /></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.</p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p></p>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br />
<a href="https://rjlipton.wordpress.com/2011/02/01/godel’s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p></p><h2> Claiming A Solution </h2><p></p>
<p></p><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p></p>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Of course this implies P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph of edge probability <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" />, the size of the maximum clique is one of the two integers flanking <img src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\log_{1/p}(n)}" class="latex" title="{2\log_{1/p}(n)}" />. <p></p>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" title="{(1+\epsilon)\log_{1/p}(n)}" />, for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" /> and sufficiently large <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.
</li></ul>
<p>
You only need to close a gap of a factor of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p></p><h2> Some Examples </h2><p></p>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3n+1}" class="latex" title="{3n+1}" /> problem. It asks for the long-term behavior of the function: <img src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f(n) " class="latex" title="f(n) " /> which is equal to <img src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n/2 " class="latex" title="n/2 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> even and <img src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="3n+1 " class="latex" title="3n+1 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> odd. </p>
<p>The conjecture is that for every <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, iterating the function eventually hits <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, i.e., <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n) = 1}" class="latex" title="{f^i(n) = 1}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" />. <p></p>
</li><li>
For some <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n)}" class="latex" title="{f^i(n)}" /> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> achieve: <img src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f^i(n) &lt; \log^* n " class="latex" title="f^i(n) &lt; \log^* n " /> for some <img src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i " class="latex" title="i " />.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-\epsilon)}" class="latex" title="{(1-\epsilon)}" /> proportion of values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(n)}" class="latex" title="{g(n)}" /> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> so there is another prime <img src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q&gt;p}" class="latex" title="{q&gt;p}" /> bounded above by <img src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p +C}" class="latex" title="{p +C}" />. His <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was <img src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{70}" class="latex" title="{70}" /> million, but this was still a breakthrough. Previously no bounded <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{n}-1}" class="latex" title="{2^{n}-1}" /> vertex induced subgraph of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-dimensional cube graph has maximum degree at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{n}}" class="latex" title="{\sqrt{n}}" />. The previous best was only order logarithm in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p></p><h2> Advice To Claimers </h2><p></p>
<p></p><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p></p><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP show that you have a exponential algorithm that is better than known. Or for P<img src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{&lt;}" class="latex" title="{&lt;}" />NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{2^{n^{1/3}}}" class="latex" title="{2^{n^{1/3}}}" /> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> </em>
</p></blockquote>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/"><span class="datestr">at September 05, 2020 05:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/129">TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/129"><span class="datestr">at September 05, 2020 03:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5519">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/">Mathematics for Human Flourishing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/"><span class="datestr">at September 04, 2020 09:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
