<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 10, 2019 07:26 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/06/10/intro-tcs-rebooted/">Intro-TCS rebooted</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This Spring and Summer I am doing some major editing to my text on <a href="https://introtcs.org">introduction to theoretical computer science</a>. I am adding figures (176 so far and counting..), examples, exercises, simplifying explanations, reducing footnotes, and mainly trying to make it more “user friendly” and less “idiosyncratic”. I am now adding in all chapters figures such as the following that outline the main results and how they are connected to one another.<br /></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://windowsontheory.files.wordpress.com/2019/06/codedataoverview.png?w=600" alt="Overview of the results in &quot;Code and Data&quot; chapter presenting universal circuit and the counting lower bound" class="wp-image-7519" /><em>Overview of the results in <a href="https://introtcs.org/public/lec_04_code_and_data.html">“Code and Data” chapter</a> presenting universal circuit and the counting lower bound</em></figure></div>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/chaploopoverview.png?w=600" alt="Overview of the results in the &quot;loops and infinity&quot; chapter defining Turing Machines. A running theme in the book is the emphasis on distinguishing specification (the mathematical function being computed) from implementation (the algorithm, machine, or program doing the computation)." class="wp-image-7520" /><em>Overview of the results in the <a href="https://introtcs.org/public/lec_06_loops.html">“loops and infinity” chapter</a> defining Turing Machines. A running theme in the book is the emphasis on distinguishing <strong>specification</strong> (the mathematical function being computed) from <strong>implementation</strong> (the algorithm, machine, or program doing the computation).</em></figure>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/universalchapoverview.png?w=600" alt="Overview of the results in the chapter on universality and uncomputability. We use Sipser's metaphor on reductions as transforming a &quot;pig that can whistle&quot; (e.g., an algorithm for the deciding if a function halts on the zero input) into a &quot;horse that can fly&quot; (e.g., an algorithm for the general halting problem). " class="wp-image-7522" /><em>Overview of the results in the chapter on <a href="https://introtcs.org/public/lec_08_uncomputability.html">universality and uncomputability</a>. We use Sipser’s metaphor on reductions as transforming a “pig that can whistle” (e.g., an algorithm for the deciding if a function halts on the zero input) into a “horse that can fly” (e.g., an algorithm for the general halting problem). </em></figure>



<p><br /><br /><br />Specifically, in the previous version of the book I used <em>programming languages </em>as the main computational models. While I still think this is the right way if we were to “start from scratch”, these idiosyncratic models made it harder for students to use other resources such as textbooks and lecture notes. They also make it more difficult for instructors to use individual chapters in their courses without committing to using the full book.</p>



<p>Hence in the new revision the standard models of <strong>Turing Machines</strong> and <strong>Boolean Circuits</strong> are front and center. We do talk about the programming-language equivalents as well, since I think they are important for the connection to practice and some concepts such as the duality of code and data are better explained in these terms.  I also use the programming-language variants to <a href="https://github.com/boazbk/tcscode">demonstrate concepts to students in code</a> including compilers from circuits to straightline programs, various “syntactic sugar” transformations, and the Cook-Levin Theorem and NP reductions. </p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/aoncircequiv.png?w=600" alt="" class="wp-image-7521" /><em>An equivalent description of a finite computation using straight-line programs and Boolean circuits. The <a href="https://nbviewer.jupyter.org/github/boazbk/tcscode/blob/master/Chap_03_Computation.ipynb">supplementary code</a> contains a Python implementation of the transformation between these two representations.</em></figure>



<p><br /><br />One thing did not change – we still start with <em>Boolean Circuits </em>rather than automata as the initial computational model. Boolean circuits are closer to actual implementations of computing, are a finite (and hence simpler) model, but one that is non-trivial enough to allow showing some important theorems early in the course including existence of a circuit for computing every finite function, the existence of a circuit to evaluate other circuits, and the counting lower bound as well as the counting lower bound. </p>



<p>Circuits are also crucial for later material in the course since they make the proof of the Cook-Levin Theorem much simpler and cleaner,  allow talking about results such as <img src="https://s0.wp.com/latex.php?latex=BPP+%5Csubseteq+P_%7B%2Fpoly%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="BPP \subseteq P_{/poly}" class="latex" title="BPP \subseteq P_{/poly}" /> and Sipser-Gacs, and are crucial to be even able to state results in advanced topics such as derandomization, cryptography, and quantum computing.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/3sat2isreduction.png?w=600" alt="" class="wp-image-7523" /><em>Reduction of SAT to independent set from <a href="https://introtcs.org/public/lec_12_NP.html">Chapter 13</a> in the book. On the right is the Python code implementing the reduction, on the left is the resulting independent set.</em></figure>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/indsetfromnandsat.png?w=600" alt="" class="wp-image-7524" /><em>An instance of independent set obtained by chaining together the proof of the Cook-Levin Theorem together with the reduction of 3SAT to independent set. Figure taken from <a href="https://introtcs.org/public/lec_13_Cook_Levin.html">Chapter 14</a>.</em></figure>



<p><br /><br />We do cover automata as well, including the equivalence of regular expressions and deterministic finite automata. We also cover context-free grammars (though not pushdown automata) and the λ calculus, including its equivalence with Turing Machines and the Y combinator (see also <a href="https://nbviewer.jupyter.org/github/boazbk/tcscode/blob/master/lambda_calculus.ipynb">this notebook</a>)</p>



<p>I have also done some work on the technical side of producing the book. The book is written in markdown. Markdown has many advantages but it wasn’t designed for 600-page technical books full of equations and cross-references so I did need to use some extensions to it. I am using pandoc (and my own <a href="https://github.com/boazbk/tcs/blob/master/scripts/book-filter.py">filter</a>) to produce both the HTML and LaTeX/PDF versions of the book. </p>



<p>There is still more work to do. I plan to add a chapter on space complexity and on proofs and computation (including both interactive and zero knowledge proofs, as well as the “propositions as types” correspondence between proofs and programs). I need to add more examples and exercises. There are also still several chapters where the text is “rough around the edges”.</p>



<p>As usual, the latest version of the book is available on  <a href="https://introtcs.org/">https://introtcs.org</a>  . If you see any typo, problem, etc.., please post an issue on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a>  (you can also make a <a href="https://github.com/boazbk/tcs/pulls">pull request</a> for small typo fixes if you prefer)</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/06/10/intro-tcs-rebooted/"><span class="datestr">at June 10, 2019 05:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blogs.princeton.edu/imabandit/?p=1359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/bubeck.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/">Amazing progress in adversarially robust stochastic multi-armed bandits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In this post I briefly discuss some recent stunning progress on robust bandits (for more background on bandits see these two posts, <a href="https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/" class="liinternal">part 1</a> and <a href="https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/" class="liinternal">part 2</a>, in particular what is described below gives a solution to Open Problem 3 at the end of part 2).</p>
<p> </p>
<p><strong>Stochastic bandit and adversarial examples</strong></p>
<p>In multi-armed bandit problems the gold standard property, going back to <a href="https://core.ac.uk/download/pdf/82425825.pdf" class="lipdf">a seminal paper</a> of Lai and Robbins in 1985 is to have a regret upper bounded by:</p>
<p style="line-height: 50px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2ac69fee3f1e4890b09202da232941f6_l3.png?resize=91%2C50&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="50" width="91" alt="\begin{equation*} \sum_{i \neq i^*} \frac{\log(T)}{\Delta_i} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>Let me unpack this a bit: this is for the scenario where the reward process for each action is simply an i.i.d. sequence from some fixed distribution, <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9af0b76a90462b68c1d83fca9cc6604d_l3.png?resize=12%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="12" alt="i^*" class="ql-img-inline-formula " /> is the index of the (unique) best action, and <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9ca6025283881ee4c29a1f9f236d72ba_l3.png?resize=20%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="20" alt="\Delta_i" class="ql-img-inline-formula " /> is the gap between the mean value of the best action and the one of <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " />. Such guarantee is extremely strong, as in particular it means that actions whose average performance is a constant away from the optimal arm are very rarely played (only of order <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " />). On the other hand, the price to pay for such an aggressive behavior (by this I mean focusing on good actions very quickly) is that all the classical algorithms attaining the above bound are extremely sensitive to <em>adversarial examples</em>: that is if there is some deviation from the i.i.d. assumption (even very brief in time), the algorithms can suddenly suffer linear in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-58f18d11e5ffdd11dd9095c427922c8b_l3.png?resize=13%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="13" alt="T" class="ql-img-inline-formula " /> regret.</p>
<p> </p>
<p><strong>Adversarial bandit</strong></p>
<p>Of course there is an entirely different line of works, on <em>adversarial multi-armed bandits</em>, where the whole point is to prove regret guarantee for <em>any</em> reward process. In this case the best one can hope for is a regret of order <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " />. The classical algorithm in this model, Exp3, attains a regret of order <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fc756065e4451f8bccc02a40b732c969_l3.png?resize=103%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="103" alt="\sqrt{K T \log(K)}" class="ql-img-inline-formula " />. In joint work with Jean-Yves Audibert we showed <a href="http://sbubeck.com/COLT09_AB.pdf" class="lipdf">back in 2009</a> that the following strategy, which we called PolyINF, attains the optimal <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3cf8321c07956bc2bd76714eabd30a0a_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{KT}" class="ql-img-inline-formula " />: view Exp3 as Mirror Descent with the (negative) entropy as a regularizer, and now replace the entropy by a simple rational function namely <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac7b6228954b4b6f9ceee99faf3d7776_l3.png?resize=77%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="77" alt="- \sum_{i=1}^K x_i^p" class="ql-img-inline-formula " /> with <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-de24a5fa6badcd9e8c80a7d72640dcc6_l3.png?resize=70%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="70" alt="p \in (0,1)" class="ql-img-inline-formula " /> (this mirror descent view was actually derived in <a href="https://pubsonline.informs.org/doi/10.1287/moor.2013.0598" class="liinternal">a later paper</a> with Gabor Lugosi). The proof becomes one line (given the appropriate knowledge of mirror descent and estimation in bandit games): the radius part of the bound is of the form <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a3d6ffad60b5151ac3866f9fa9213b65_l3.png?resize=120%2C27&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="27" width="120" alt="\frac{1}{\eta} \sum_{i=1}^K x_i^p \leq \frac{K^{p}}{\eta}" class="ql-img-inline-formula " />, while the variance is of the form (since the inverse of the Hessian of the mirror map is a diagonal matrix with entries <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0b3188b0bc4d641381b7ba4be936c230_l3.png?resize=35%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="23" width="35" alt="x_i^{2-p}" class="ql-img-inline-formula " />):</p>
<p style="line-height: 46px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5356f94049f314f34752afbac2277f16_l3.png?resize=177%2C46&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="46" width="177" alt="\[ \eta \sum_{i=1} x_i^{2-p} \frac{1}{x_i} \leq \eta K^{1-p} \,. \]" class="ql-img-displayed-equation " /></p>
<p>Thus optimizing over <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-86bde2afc6c0858d01ac505267801f02_l3.png?resize=9%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="12" width="9" alt="\eta" class="ql-img-inline-formula " /> yields <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " /> for any <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-de24a5fa6badcd9e8c80a7d72640dcc6_l3.png?resize=70%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="70" alt="p \in (0,1)" class="ql-img-inline-formula " />. Interestingly, the best numerical constant in the bound is obtained for <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " />.</p>
<p> </p>
<p><strong>Best of both worlds</strong></p>
<p>This was the state of affairs back in 2011, when with Alex Slivkins we started working on a <em>best of both worlds</em> type algorithm (which in today’s language is exactly a stochastic MAB robust to adversarial examples): namely one that gets the <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " /> guarantee (in fact <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-00cc550d710d51cc64227da3b639018e_l3.png?resize=56%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="56" alt="\log^2(T)" class="ql-img-inline-formula " /> in <a href="http://sbubeck.com/COLT12_BS.pdf" class="lipdf">our original paper</a>) if the environment is the nice i.i.d. one, and also <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " /> (in fact <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4d85e820be8841c565042231480be3ac_l3.png?resize=107%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="107" alt="\sqrt{K T \log^3(T)}" class="ql-img-inline-formula " />) in the worst case. This original best of both worlds algorithm was of the following form: be aggressive as if it was a stochastic environment, but still sample sufficiently often the bad actions to make sure there isn’t an adversary trying to hide some non-stochastic behavior on these seemingly bad performing actions. Of course the whole difficulty was to show that it is possible to implement such a defense without hurting the stochastic performance too much (remember that bad actions can only be sampled of order <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " /> times!). Since this COLT 2012 paper there has been many improvements to the original strategy, as well as many variants/refinements (one such variant worth mentioning are the works trying to do a smooth transition between the stochastic and adversarial models, see e.g. <a href="https://arxiv.org/abs/1803.09353" class="liinternal">here</a> and <a href="https://arxiv.org/abs/1902.08647" class="liinternal">here</a>).</p>
<p> </p>
<p><strong>A stunning twist</strong></p>
<p>The amazing development that I want to talk about in this post is the following: <a href="https://arxiv.org/abs/1807.07623" class="liinternal">about a year ago</a> Julian Zimmert and Yevgeny Seldin proved that the 2009 PolyINF (crucially with <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " />) strategy actually gets the 2012 best of both worlds bound! This is truly surprising, as in principle mirror descent does not “know” anything about stochastic environments, it does not make any sophisticated concentration reasoning (say as in Lai and Robbins), yet it seems to automatically and optimally pick up on the regularity in the data. This is really amazing to me, and of course also a total surprise that the polynomial regularizer has such strong adaptivity property, while it was merely introduced to remove a benign log term.</p>
<p>The crucial observation of Zimmert and Seldin is that a a certain <em>self-bounding</em> property of the regret implies (in a one-line calculation) the best of both worlds result:</p>
<blockquote><p><strong>Lemma 1:</strong> Consider a strategy whose regret with respect to the optimal action <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9af0b76a90462b68c1d83fca9cc6604d_l3.png?resize=12%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="12" alt="i^*" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 56px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (2) </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-58ac492ce88752df4dbc5bccc5f7c9ff_l3.png?resize=129%2C56&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="56" width="129" alt="\begin{equation*} C \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>(Recall that for multi-armed bandit one selects a probability distribution <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ae5b82123ba785d8c8153a037675bc56_l3.png?resize=15%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="11" width="15" alt="x_t" class="ql-img-inline-formula " /> over the actions, so <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-65be0e3045aefe9ef0dd0bd9531c0572_l3.png?resize=24%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="24" alt="x_{i,t}" class="ql-img-inline-formula " /> denote here the probability of playing action <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " /> at time <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6b97bb0f65c75b6cc0fba1868749478d_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="t" class="ql-img-inline-formula " />.) Then one has that the regret is in fact bounded by <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6dc96815294ab91f00db7fc32adcc459_l3.png?resize=68%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="68" alt="2 C \sqrt{K T}" class="ql-img-inline-formula " /> (this follows trivially by Jensen on the <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " /> sum), and moreover if the environment is stochastic one has that the regret is in fact bounded by <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4b609168e948e0de36c4f357b700408_l3.png?resize=21%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="21" alt="C^2" class="ql-img-inline-formula " /> times <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5048b7c449e02713d954071d1a80df0f_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(1)" class="ql-img-inline-formula " />.</p></blockquote>
<p><em>Proof:</em> Assuming that the environment is stochastic we can write the regret as <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1e451f2e1cd6b24d6f4ceb19592378a8_l3.png?resize=80%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="21" width="80" alt="\sum_{i,t} \Delta_i x_{i,t}" class="ql-img-inline-formula " />, so by assumption and using that <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-37d8baadc3f1897c077c0f1bc6da832f_l3.png?resize=205%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="205" alt="C \sqrt{\frac{x_{i,t}}{t}} \leq \frac{1}{2} \left( \Delta_i x_{i,t} + \frac{C^2}{t \Delta_i} \right)" class="ql-img-inline-formula " /> one has:</p>
<p style="line-height: 52px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-19259c6dbd863aa77917fbddee306ca7_l3.png?resize=295%2C52&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="52" width="295" alt="\[ \sum_{i \neq i^*,t} \Delta_i x_{i,t} \leq \frac{1}{2} \sum_{i \neq i^*,t} \left(\Delta_i x_{i,t} + \frac{C^2}{t \Delta_i} \right) \,, \]" class="ql-img-displayed-equation " /></p>
<p>which means that the left hand side is smaller than <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3a170cbde4999b8a9aaab3ebc7698acb_l3.png?resize=82%2C26&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="26" width="82" alt="\sum_{i \neq i^*,t} \frac{C^2}{t \Delta_i}" class="ql-img-inline-formula " /> which is indeed smaller than <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4b609168e948e0de36c4f357b700408_l3.png?resize=21%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="21" alt="C^2" class="ql-img-inline-formula " /> times <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5048b7c449e02713d954071d1a80df0f_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(1)" class="ql-img-inline-formula " />.</p>
<p> </p>
<p><strong>Yet another one-line proof (okay, maybe 5 lines)</strong></p>
<p>Zimmert and Seldin proved that PolyINF with <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " /> actually satisfies the self-bounding property of Lemma 1 (and thus obtains the best of both worlds guarantee). In <a href="https://arxiv.org/abs/1901.08779" class="liinternal">another recent paper</a> by Zimmert, in joint work with Haipeng Luo and Chen-Yu Wei, they simplify the analysis by using a very mild variation of the PolyINF regularizer, namely <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-83b29164db23b237061cd205df4a585d_l3.png?resize=183%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="183" alt="- \sum_{i=1}^K (\sqrt{x_i} + \sqrt{1-x_i})" class="ql-img-inline-formula " />. In my view it’s the proof from the book for the best of both worlds result (or very close to it)! Here it is:</p>
<blockquote><p><strong>Lemma 2:</strong> Equation <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8a996880dc323ee4b8cf323009c02635_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(2)" class="ql-img-inline-formula " /> with <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ae0daffd6d9302e8baf8d6432a69c5ca_l3.png?resize=56%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="56" alt="C=10" class="ql-img-inline-formula " /> is an upper bound on the regret of mirror descent with learning <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-27f9c61a5d83fe6d2fca62581c4d1b6f_l3.png?resize=57%2C27&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="27" width="57" alt="\eta_t = \frac{1}{\sqrt{t}}" class="ql-img-inline-formula " />, mirror map <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e28928d4c9de69492e5f786c52e80f3e_l3.png?resize=245%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="245" alt="\Phi(x) = - \sum_{i=1}^K (\sqrt{x_i} + \sqrt{1-x_i})" class="ql-img-inline-formula " />, and standard multi-armed bandit loss estimator.</p></blockquote>
<p><em>Proof:</em> The classical mirror descent analysis from any good book will tell you that the regret is controlled by (for <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f1d27d479269ff49c66d302354f84569_l3.png?resize=144%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="144" alt="\Phi(x) = \sum_{i=1}^K \phi(x_i)" class="ql-img-inline-formula " /> and with the convention <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-db68debdd0cb300a9b79aed311c05d28_l3.png?resize=71%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="71" alt="\eta_0 = + \infty" class="ql-img-inline-formula " />):</p>
<p style="line-height: 54px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (3) </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d3de9d0c55a8d8aca491d928972530ed_l3.png?resize=437%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="54" width="437" alt="\begin{equation*} \sum_{t=1}^T \left(\frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}\right) (\Phi(x^*) - \Phi(x_t)) + \sum_{t=1}^T \eta_t \sum_{i=1}^K \frac{1}{x_{i,t} \phi''(x_t)} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>We now consider those terms for the specific <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8581dbc45f448345dcc6bd9caed502e9_l3.png?resize=12%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="12" alt="\Phi" class="ql-img-inline-formula " /> and <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f489909a6f16a3b5a6ee23901d48d9b6_l3.png?resize=14%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="\eta_t" class="ql-img-inline-formula " /> suggested in the lemma. First notice that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7f8f2af48129af0fd204515d14d0f6f5_l3.png?resize=105%2C25&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="25" width="105" alt="\frac{1}{\eta_t} - \frac{1}{\eta_{t-1}} \leq \eta_t" class="ql-img-inline-formula " />. Moreover <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e59356dfe693c9414f1e407c034a4a4d_l3.png?resize=88%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="19" width="88" alt="\phi(x^*_i) = - 1" class="ql-img-inline-formula " /> (since <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b92fd5178a7e53e81d04a267f41c9d8_l3.png?resize=16%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="16" alt="x^*" class="ql-img-inline-formula " /> is integral) so that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a13944e230b8362a3c996cdb9d42f634_l3.png?resize=298%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="298" alt="\phi(x^*_i) - \phi(x_{i,t}) \leq \min(\sqrt{x_{i,t}}, \sqrt{1-x_{i,t}})" class="ql-img-inline-formula " />. In other words the first term in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c9264e4ae133cc333e4e98394c7e0656_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(3)" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 57px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c048790fe35996074b87ea676060d91d_l3.png?resize=350%2C57&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="57" width="350" alt="\[ \sum_{t=1}^T \sum_{i=1}^K \sqrt{\frac{\min(x_{i,t}, 1-x_{i,t})}{t}} \leq 2 \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \, \]" class="ql-img-displayed-equation " /></p>
<p>where the inequality simply comes from <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-01c4dcfc661631d69e71024a0340222d_l3.png?resize=307%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="307" alt="\sqrt{1-x_{i^*,t}} = \sqrt{\sum_{i \neq i^*} x_{i,t}} \leq \sum_{i \neq i^*} \sqrt{x_{i,t}}" class="ql-img-inline-formula " />.</p>
<p>Next note that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-83fb6f1ccbc0d202e6020dc0884a140f_l3.png?resize=358%2C28&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="28" width="358" alt="\phi''(s) = \frac{1}{4} (s^{-3/2} + (1-s)^{-3/2}) \geq \frac{1}{4 \min(s,1-s)^{3/2}}" class="ql-img-inline-formula " />, so that the second term in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c9264e4ae133cc333e4e98394c7e0656_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(3)" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 57px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-98a3e5f8a743148f5f6919b1dd53a8b9_l3.png?resize=442%2C57&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="57" width="442" alt="\[ 4 \sum_{t=1}^T \sum_{i=1}^K \sqrt{\frac{x_{i,t}}{t}} \min(1, (1-x_{i,t})/x_{i,t})^{3/2} \leq 8 \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \, \]" class="ql-img-displayed-equation " /></p>
<p>where the inequality follows trivially by considering the two cases whether <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0bcf369611e215890249a1b1bb94089d_l3.png?resize=31%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="31" alt="x_{i^*,t}" class="ql-img-inline-formula " /> is smaller or larger than <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1c652ece8cc629e4e659c41eeed4d410_l3.png?resize=25%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="25" alt="1/2" class="ql-img-inline-formula " />.</p></div>







<p class="date">
by Sebastien Bubeck <a href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/"><span class="datestr">at June 10, 2019 03:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1126">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1126">News for May 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We were able to find four new papers for May 2019 — as usual, please let us know if we missed any!</p>



<p><strong>On Local Testability in the Non-Signaling Setting</strong>, by Alessandro Chiesa, Peter Manohar, and Igor Shinkar (<a href="https://eccc.weizmann.ac.il/report/2019/070/">ECCC</a>).  This paper studies testability of a certain generalization of (distributions over) functions, known as \(k\)-non-signalling functions, objects which see use in hardness of approximation and delegation of computation. Prior work by the authors show the effectiveness of the linearity test in this setting, leading to the design of PCPs. On the other hand, in this work, the authors show that two types of bivariate tests are ineffective in revealing low-degree structure of these objects.</p>



<p><strong>Computing and Testing Small Vertex Connectivity in Near-Linear Time and Queries</strong>, by Danupon Nanongkai, Thatchaphol Saranurak, and Sorrachai Yingchareonthawornchai (<a href="https://arxiv.org/abs/1905.05329">arXiv</a>). This work, apparently simultaneous with the one by Forster and Yang that we covered <a href="https://ptreview.sublinear.info/?p=1116">last month</a>, also studies the problem of locally computing cuts in a graph. The authors also go further, and study approximation algorithms for the same problems. Inspired by the connections to property testing in the work of Forster and Yang, they apply these approximation algorithms to get even more query-efficient algorithms for the problems of testing \(k\)-edge- and \(k\)-vertex-connectivity.</p>



<p><strong>Testing Graphs against an Unknown Distribution</strong>, by Lior Gishboliner and Asaf Shapira (<a href="https://arxiv.org/abs/1905.09903">arXiv</a>). This paper studies graph property testing, under the vertex-distribution-free (VDF) model, as <a href="http://www.wisdom.weizmann.ac.il/~oded/p_vdf.html">recently introduced</a> by Goldreich. In the VDF model, rather than the ability to sample a random node, the algorithm has the ability to sample a node from some unknown distribution, and must be accurate with respect to the same distribution (reminiscent of the PAC learning model). In Goldreich’s work, it was shown that every property which is testable in the VDF model is semi-hereditary. This work strengthens this statement and proves a converse, thus providing a characterization: a property is testable in the VDF model if and only if it is both hereditary and extendable. These descriptors roughly mean that the property is closed under both removal and addition of nodes (with the choice of addition of edges in the latter case). This is a far simpler characterization than that of properties which are testable in the standard model, which is a special case of the VDF model.</p>



<p><strong>Private Identity Testing for High-Dimensional Distributions</strong>, by Clément L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou (<a href="https://arxiv.org/abs/1905.11947">arXiv</a>). This work continues a recent line on distribution testing under the constraint of differential privacy. The settings of interest are multivariate distributions: namely, product distributions over the hypercube and Gaussians with identity covariance. An application of a statistic of <a href="https://arxiv.org/abs/1612.03156">CDKS</a>, combined with a Lipschitz extension from the set of datasets likely to be generated by such structured distributions, gives a sample-efficient algorithm. A time-efficient version of this extension is also provided, at the cost of some loss in the sample complexity. Some tools of independent interest include reductions between Gaussian mean and product uniformity testing, balanced product identity to product uniformity testing, and an equivalence between univariate and “extreme” product identity testing. </p></div>







<p class="date">
by Gautam Kamath <a href="https://ptreview.sublinear.info/?p=1126"><span class="datestr">at June 10, 2019 07:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.03242">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.03242">Zooming Cautiously: Linear-Memory Heuristic Search With Node Expansion Guarantees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Orseau:Laurent.html">Laurent Orseau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lelis:Levi_H=_S=.html">Levi H. S. Lelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lattimore:Tor.html">Tor Lattimore</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.03242">PDF</a><br /><b>Abstract: </b>We introduce and analyze two parameter-free linear-memory tree search
algorithms. Under mild assumptions we prove our algorithms are guaranteed to
perform only a logarithmic factor more node expansions than A* when the search
space is a tree. Previously, the best guarantee for a linear-memory algorithm
under similar assumptions was achieved by IDA*, which in the worst case expands
quadratically more nodes than in its last iteration. Empirical results support
the theory and demonstrate the practicality and robustness of our algorithms.
Furthermore, they are fast and easy to implement.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.03242"><span class="datestr">at June 10, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.03113">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.03113">Optimal algebraic Breadth-First Search for sparse graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burkhardt:Paul.html">Paul Burkhardt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.03113">PDF</a><br /><b>Abstract: </b>There has been a rise in the popularity of algebraic methods for graph
algorithms given the development of the GraphBLAS library and other sparse
matrix methods. These are useful in practice because many graph algorithms are
amenable to sparse matrix multiplication. An exemplar for these approaches is
Breadth-First Search (BFS). Despite many redundant operations over nonzeros
that ultimately lead to suboptimal performance, the algebraic BFS is appealing
for practical implementations because it is simple and embarrassingly parallel.
Therefore an optimal algebraic BFS should be of keen interest especially if it
is easily integrated with existing matrix methods.
</p>
<p>Current methods, notably in the GraphBLAS, use a Sparse Matrix Sparse Vector
(SpMSpV) multiplication in which the input vector is kept in a sparse
representation in each step of the BFS. But simply applying SpMSpV in BFS does
not lead to optimal runtime. Each nonzero in the vector must be masked in
subsequent steps. This has been an area of recent recent in GraphBLAS and other
libraries. While in theory these masking methods are asymptotically optimal on
sparse graphs, many add work that leads to suboptimal runtime. We give a new
optimal, algebraic BFS for sparse graphs that is also a constant factor faster
than theoretically optimal SpMSpV methods.
</p>
<p>Our method multiplies progressively smaller submatrices of the adjacency
matrix at each step. The matrix remains unchanged, rather we are masking the
rows and columns in the matrix. The input vector in each step is also
effectively masked, thus our method ultiplies a sparse submatrix by a sparse
vector in decreasing size each step. Our algebraic BFS algorithm takes $O(m)$
algebraic operations on a sparse graph with $O(m)$ edges as opposed to $O(mn)$
operations of other sparse matrix approaches.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.03113"><span class="datestr">at June 10, 2019 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.03027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.03027">CrossFill: Foam Structures with Graded Density for Continuous Material Extrusion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuipers:Tim.html">Tim Kuipers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Jun.html">Jun Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Charlie_C=_L=.html">Charlie C. L. Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.03027">PDF</a><br /><b>Abstract: </b>The fabrication flexibility of 3D printing has sparked a lot of interest in
designing structures with spatially graded material properties. In this paper,
we propose a new type of density graded structure that is particularly designed
for 3D printing systems based on filament extrusion. In order to ensure
high-quality fabrication results, extrusion-based 3D printing requires not only
that the structures are self-supporting, but also that extrusion toolpaths are
continuous and free of self-overlap. The structure proposed in this paper,
called CrossFill, complies with these requirements. In particular, CrossFill is
a self-supporting foam structure, for which each layer is fabricated by a
single, continuous and overlap-free path of material extrusion. Our method for
generating CrossFill is based on a space-filling surface that employs spatially
varying subdivision levels. Dithering of the subdivision levels is performed to
accurately reproduce a prescribed density distribution. We demonstrate the
effectiveness of CrossFill on a number of experimental tests and applications.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.03027"><span class="datestr">at June 10, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02842">Quasi-automatic semigroups</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Benjamin Blanchette, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choffrut:Christian.html">Christian Choffrut</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reutenauer:Christophe.html">Christophe Reutenauer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02842">PDF</a><br /><b>Abstract: </b>A quasi-automatic semigroup is defi0ned by a finite set of generators, a
rational (regular) set of representatives, such that if a is a generator or
neutral, then the graph of right multiplication by a on the set of
representatives is a rational relation. This class of semigroups contains
previously considered semigroups and groups (Sakarovitch, Epstein et al.,
Campbell et al.). Membership of a semigroup to this class does not depend on
the choice of the generators. These semigroups are rationally presented.
Representatives may be computed in exponential time. Their word problem is
decidable in exponential time. They enjoy a property similar to the so-called
Lipschitz property, or fellow traveler property. If graded, they are automatic.
In the case of groups, they are finitely presented with an exponential
isoperimetric inequality and they are characterized by the weak Lipschitz
property.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02842"><span class="datestr">at June 10, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02830">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02830">Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bun:Mark.html">Mark Bun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steinke:Thomas.html">Thomas Steinke</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02830">PDF</a><br /><b>Abstract: </b>The simplest and most widely applied method for guaranteeing differential
privacy is to add instance-independent noise to a statistic of interest that is
scaled to its global sensitivity. However, global sensitivity is a worst-case
notion that is often too conservative for realized dataset instances. We
provide methods for scaling noise in an instance-dependent way and demonstrate
that they provide greater accuracy under average-case distributional
assumptions.
</p>
<p>Specifically, we consider the basic problem of privately estimating the mean
of a real distribution from i.i.d.~samples. The standard empirical mean
estimator can have arbitrarily-high global sensitivity. We propose the trimmed
mean estimator, which interpolates between the mean and the median, as a way of
attaining much lower sensitivity on average while losing very little in terms
of statistical accuracy.
</p>
<p>To privately estimate the trimmed mean, we revisit the smooth sensitivity
framework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a
framework for using instance-dependent sensitivity. We propose three new
additive noise distributions which provide concentrated differential privacy
when scaled to smooth sensitivity. We provide theoretical and experimental
evidence showing that our noise distributions compare favorably to others in
the literature, in particular, when applied to the mean estimation problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02830"><span class="datestr">at June 10, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02640">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02640">Near Neighbor: Who is the Fairest of Them All?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Har=Peled:Sariel.html">Sariel Har-Peled</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahabadi:Sepideh.html">Sepideh Mahabadi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02640">PDF</a><br /><b>Abstract: </b>$\newcommand{\ball}{\mathbb{B}}\newcommand{\dsQ}{{\mathcal{Q}}}\newcommand{\dsS}{{\mathcal{S}}}$In
this work we study a fair variant of the near neighbor problem. Namely, given a
set of $n$ points $P$ and a parameter $r$, the goal is to preprocess the
points, such that given a query point $q$, any point in the $r$-neighborhood of
the query, i.e., $\ball(q,r)$, have the same probability of being reported as
the near neighbor.
</p>
<p>We show that LSH based algorithms can be made fair, without a significant
loss in efficiency. Specifically, we show an algorithm that reports a point in
the $r$-neighborhood of a query $q$ with almost uniform probability. The query
time is proportional to $O\bigl( \mathrm{dns}(q.r) \dsQ(n,c) \bigr)$, and its
space is $O(\dsS(n,c))$, where $\dsQ(n,c)$ and $\dsS(n,c)$ are the query time
and space of an LSH algorithm for $c$-approximate near neighbor, and
$\mathrm{dns}(q,r)$ is a function of the local density around $q$.
</p>
<p>Our approach works more generally for sampling uniformly from a
sub-collection of sets of a given collection and can be used in a few other
applications. Finally, we run experiments to show performance of our approach
on real data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02640"><span class="datestr">at June 10, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/086">TR19-086 |  Perfect zero knowledge for quantum multiprover interactive proofs | 

	Alex Bredariol Grilo, 

	William Slofstra, 

	Henry Yuen</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work we consider the interplay between multiprover interactive proofs, quantum
entanglement, and zero knowledge proofs — notions that are central pillars of complexity theory,
quantum information and cryptography. In particular, we study the relationship between the
complexity class MIP$^*$ , the set of languages decidable by multiprover interactive proofs with
quantumly entangled provers, and the class PZK-MIP$^*$ , which is the set of languages decidable
by MIP$^*$ protocols that furthermore possess the perfect zero knowledge property.

Our main result is that the two classes are equal, i.e., MIP$^*$ = PZK-MIP$^*$ . This result provides
a quantum analogue of the celebrated result of Ben-Or, Goldwasser, Kilian, and Wigderson (STOC
1988) who show that MIP = PZK-MIP (in other words, all classical multiprover interactive
protocols can be made zero knowledge). We prove our result by showing that every MIP$^*$
protocol can be efficiently transformed into an equivalent zero knowledge MIP$^*$ protocol in a
manner that preserves the completeness-soundness gap. Combining our transformation with
previous results by Slofstra (Forum of Mathematics, Pi 2019) and Fitzsimons, Ji, Vidick and
Yuen (STOC 2019), we obtain the corollary that all co-recursively enumerable languages (which
include undecidable problems as well as all decidable problems) have zero knowledge MIP$^*$
protocols with vanishing promise gap.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/086"><span class="datestr">at June 09, 2019 10:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/06/09/max-plank-summer-school-games-brains-and-distributed-computing/">Max Plank Summer School: Games, Brains, and Distributed Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The 20th <a href="http://resources.mpi-inf.mpg.de/conferences/adfocs/">Max Planck Advanced Course on the Foundations of Computer Science focused on “Games, Brains, and Distributed Computing”</a> will take place on 19 – 23 August 2019, Saarbrücken, Germany.</p>
<p> </p>
<p>The instructors are Christos Papadimitriou, Eva Tardos, and Pierre Fraigniaud.</p></div>







<p class="date">
by algorithmicgametheory <a href="https://agtb.wordpress.com/2019/06/09/max-plank-summer-school-games-brains-and-distributed-computing/"><span class="datestr">at June 09, 2019 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02511">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02511">On the distribution of runners on a circle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hrubes:Pavel.html">Pavel Hrubes</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02511">PDF</a><br /><b>Abstract: </b>Consider $n$ runners running on a circular track of unit length with constant
speeds such that $k$ of the speeds are distinct. We show that, at some time,
there will exist a sector $S$ which contains at least $|S|n+ \Omega(\sqrt{k})$
runners. The result can be generalized as follows. Let $f(x,y)$ be a complex
bivariate polynomial whose Newton polytope has $k$ vertices. Then there exists
$a\in {\mathbb C}\setminus\{0\}$ and a complex sector $S=\{re^{\imath \theta}:
r&gt;0, \alpha\leq \theta \leq \beta\}$ such that the univariate polynomial
$f(x,a)$ contains at least $\frac{\beta-\alpha}{2\pi}n+\Omega(\sqrt{k})$
non-zero roots in $S$ (where $n$ is the total number of such roots and $0\leq
(\beta-\alpha)\leq 2\pi$). This shows that the Real $\tau$-Conjecture of Koiran
implies the conjecture on Newton polytopes of Koiran et al.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02511"><span class="datestr">at June 09, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02315">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02315">Greed is Not Always Good: On Submodular Maximization over Independence Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02315">PDF</a><br /><b>Abstract: </b>In this work, we consider the maximization of submodular functions
constrained by independence systems. Because of the wide applicability of
submodular functions, this problem has been extensively studied in the
literature. When the independence system is a $p$-system, prior literature has
claimed that the greedy algorithm achieves a $1/(p+1)$-approximation if the
submodular function is monotone. We show that, on the contrary, for any
$\epsilon &gt; 0$, the problem is hard to approximate within $(2/n)^{1-\epsilon}$,
where $n$ is the size of the ground set, even when the independence system is a
$1$-system. This result invalidates prior work on constant-factor algorithms
for non-monotone submodular maximization over $p$-systems as well. On the
positive side, we provide the first nearly linear-time algorithm for
maximization of non-monotone submodular functions over $p$-extendible
independence systems, which are a subclass of $p$-systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02315"><span class="datestr">at June 09, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02229">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02229">Quantum Algorithms for Solving Dynamic Programming Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ronagh:Pooya.html">Pooya Ronagh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02229">PDF</a><br /><b>Abstract: </b>We present quantum algorithms for solving finite-horizon and infinite-horizon
dynamic programming problems. The infinite-horizon problems are studied using
the framework of Markov decision processes. We prove query complexity lower
bounds for classical randomized algorithms for the same tasks and consequently
demonstrate a polynomial separation between the query complexity of our quantum
algorithms and best-case query complexity of classical randomized algorithms.
Up to polylogarithmic factors, our quantum algorithms provide quadratic
advantage in terms of the number of states $|S|$, and the number of actions
$|A|$, in the Markov decision process when the transition kernels are
deterministic. This covers all discrete and combinatorial optimization problems
solved classically using dynamic programming techniques. In particular, we show
that our quantum algorithm solves the travelling salesperson problem in
$O^*(c^4 \sqrt{2^n})$ where $n$ is the number of nodes of the underlying graph
and $c$ is the maximum edge-weight of it. For stochastic transition kernels the
quantum advantage is again quadratic in terms of the numbers of actions but
less than quadratic (from $|S|^2$ to $|S|^{3/2}$) in terms of the numbers of
states. In all cases, the speed-up achieved is at the expense of appearance of
other polynomial factors in the scaling of the algorithm. Finally we prove
lower bounds for the query complexity of our quantum algorithms and show that
no more-than-quadratic speed-up in either of $|S|$ or $|A|$ can be achieved for
solving dynamic programming and Markov decision problems using quantum
algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02229"><span class="datestr">at June 09, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8872438004336745499">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/ray-miller-one-of-our-founders-passes.html">Ray Miller, one of our founders, Passes away at the age of 90</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Ray Miller, one of the founders of our field, passed away recently at the age of 90.<br />
<br />
He has associations with both GA Tech and The University of Maryland, so both Lance and I have a connection to him. As does Dick Lipton who has posted about him <a href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/">here</a>.<br />
<br />
I present two guest blog-posts about him<br />
<br />
<br />
<b>Post One</b>: From<br />
<br />
<a href="https://www.cs.umd.edu/people/lin">Ming C Lin</a><br />
<br />
Elizabeth Stevinson Chair of Computer Science<br />
<br />
University of Maryland at College Park<br />
<br />
<br />
Dear CS Alumni and Friends,<br />
<br />
We are deeply saddened to learn that Professor Emeritus Ray Miller passed away two nights ago around 9 pm.<br />
<br />
A Memorial Service at St. Andrews Lutheran Church (15300 New Hampshire Ave., Silver Spring MD  20905) for Dr. Miller will be held on Saturday, June 15th at 10:30 am.<br />
<br />
Dr. Ray Miller received his Ph.D. from University of Illinois in 1957.  He was a professor and the former Director of the School of Information and Computer Science at the Georgia Institute of Technology before joining our department in 1988 as the Director of the Center of Excellence in Space Data and Information Science (CESDIS).   Dr. Miller was well known for his research on communication protocols, networks, distributed systems, parallel computation, and theory.<br />
<br />
In 1970, he became the Fellow of IEEE for the advancement of the theoretical understanding of computation through work in switching theory and theoretical models.<br />
<br />
In 1997, he was elevated to be a Fellow of ACM for research contributions to the theory of parallel computation and for his distinguished service to the Computer Science community as an educator and leader.<br />
<br />
In 2003, Dr. Miller was designated as a Fellow by the Computer Science Accreditation Board<br />
<br />
<i>"in recognition of his outstanding professional volunteer contributions to computing sciences </i><i>and accreditation”.</i><br />
<br />
Dr. Miller was also an AAAS Fellow,  and a Charter Member of IEEE Computer Society Golden Core;he received the IEEE Third Millennium Medal in 2000 and ACM Distinguished Service Award in 2002.<br />
<br />
Beyond his outstanding research contribution and devotion to education, Dr. Ray Miller has been known for his kindness as a colleague, supportiveness as a mentor, and effectiveness as a leader. Dr. Miller will be forever remembered warmly by his friends, colleagues and students for his dedication and service to our department, the University, and the field of computing at large.<br />
<br />
<br />
<b>Post Two</b>: From<br />
<br />
<a href="http://www.cs.umd.edu/users/ben/">Ben Shneiderman</a><br />
<br />
Emeritus Professor, University of Maryland at College Park.<br />
<br />
I was saddened to hear about the death of Ray Miller at age 90.  He was a dear colleague who contributed a great deal to computer science and to our department.  You can read his 84-page personal memoir at the IEEE Computer Society History Committee website: <a href="https://history.computer.org/pubs/ray-miller.pdf">here</a>.<br />
<br />
His memoirs tells his story in detail, describing his research collaborations in computational complexity, parallel algorithms, and program optimization and his leadership roles. You can see more about Ray’s work on his ACM author page: <a href="https://dl.acm.org/author_page.cfm?id=81332515760">here</a><br />
<br />
<div>
This is the best source as he had no Google Scholar page or Wikipedia article that I could find. Ray’s quiet and modest style was a valuable asset, but his contributions come through in his memoir. He describes working with Turing Awardees John Cocke, Fran Allen, John Backus, Dick Karp, and other key figures, so maybe Ray should have received that award too.  Ray was also an excellent administrator and leader, who contributed to building the institutions (conferences, ACM, IEEE, etc.) that supported the growth of computer science.</div>
<div>
<div>
<br /></div>
<div>
Ray was especially kind to me in the early 1970s, when I was working on my Phd, developing a graph theoretic model of data structures.  As Assistant Director of the IBM Mathematical Science Department at the T. J. Watson Labs in Yorktown Heights, NY. This legendary IBM Research Lab was equal to Bell Labs and filled with computing pioneers in hardware, software, and applications.<br />
<br /></div>
<div>
Ray invited me to give a talk about my work, drawing interest from Arnold Rosenberg, who had been developing related ideas. With Ray’s support I returned for monthly visits with Arnie and Ray to refine my esoteric ideas leading to my May 1973 Phd.</div>
<div>
<br /></div>
<div>
Ray’s kindness as a colleague and supportiveness as a mentor will always be remembered warmly. Here are a few photos of Ray giving a talk in the CS Department, probably in 1985: <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray1.jpg">here,</a> <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray2.jpg">here</a>, and</div>
</div>
<div>
<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray3.jpg">here</a></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/ray-miller-one-of-our-founders-passes.html"><span class="datestr">at June 08, 2019 06:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15969">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/">Raymond Edward Miller Just Passed Away</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Miller just passed away at 90</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/ray/" rel="attachment wp-att-15970"><img src="https://rjlipton.files.wordpress.com/2019/06/ray.jpeg?w=600" alt="" class="alignright size-full wp-image-15970" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ GIT ]</font></td>
</tr>
</tbody>
</table>
<p>
Ray Miller just passed away. He had been a researcher and leader at IBM Research, Georgia Tech, and University of Maryland. At all he did important research and also was a leader: a group head, a director, and a chair.</p>
<p>
Today we remember Ray.</p>
<p>
For starters you can read <a href="https://cacm.acm.org/news/237370-in-memoriam-raymond-e-ray-miller-1928-2019/fulltext">this</a> or his memoir <a href="https://history.computer.org/pubs/ray-miller.pdf">here</a>. Ray started in the field with an electrical engineering PhD <a href="https://www.ideals.illinois.edu/handle/2142/5131/filter-search">thesis</a> tilted <i>Formal Analysis and Synthesis of Bilateral Switching Networks</i>. See <a href="https://ieeexplore.ieee.org/document/5222582">this</a> for his paper.</p>
<p>
Ray’s memoir does not mention tennis. But we played tennis when I visited IBM, and also when we could while at a conference. Ray was not slim, not fast, not obviously athletic. But he was the best tennis player in our group. He was miles above me. His trick was he could control the ball, especially on his serve so it was untouchable. He could spin it so that it landed and bounced at right angles. He was so good that he hardly ever had to move. When I played him in singles, we had to agree “no spinning serves”. When we played doubles there was a small chance that someone could return his ball, but not much.</p>
<p>
I think this is a fair way to summarize Ray: It was easy to underestimate him. Ray always had a smile on his face. I cannot remember him being anything but happy. This may led some to think he was not serious. But Ray was. He did wonderful research and was a leader in our field. He changed, for the better, all the places he called “home”. I can directly attest to his impact at Tech; others I believe can attest to IBM and Maryland. He was a great editor for <i>Journal of the ACM</i>, and did much more for our field. Thanks Ray.</p>
<p>
</p><p></p><h2> Some Thoughts </h2><p></p>
<p></p><p>
Here are just a few comments from friends of Ray.</p>
<p>
<i>Rich DeMillo</i>:</p>
<blockquote><p><b> </b> <em> Ray was a quiet but strong and effective leader and a good friend to many of us. Nancy Lynch and I recruited him to be School of ICS Director, bringing immediate stature—both internal and external—to computer science at Georgia Tech. Up until that time Information and Computer Science was an odd duck interdisciplinary graduate program in a very traditional engineering school that had not invested in the field despite the early success of the School of Information and Computer Science and a world-class Burroughs installation (Georgia Tech had played a key role in Algol development in the ’60s). Ray was an engineer with impeccable credentials and was taken seriously by the administration in ways that the library scientists, linguists, philosophers, psychologists, cyberneticists, and mathematicians who founded the School never managed to achieve. </em></p><em>
<p>
Dick Lipton and I were long term collaborators on theory research with Ray, and Rays’ presence helped shine a national spotlight on the School. He was the editor-in-chief of the Journal of the ACM and palled around with luminaries like Sam Winograd and Dick Karp. </p>
<p>
In addition to his own work in switching theory and automata, Ray had co-edited the volume in which Karp’s NP-completeness paper appeared, and so his name was forever associated with that ground-breaking paper. From that point on, Atlanta became a mandatory stop on the national research circuit that was the hallmark of theory research in those days. </p>
<p>
One of his first accomplishments was snagging the Computer Science Conference, the large, research-oriented conference for the field. It gave Georgia Tech a chance to showcase its work for the rest of the world. There was a steady rise in rankings and a steady flow of visitors like Michael Rabin, Leslie Lamport, Mike Fisher, Andy Yao, Ravi Kannan, in addition to Karp, Winograd, and Lipton. Ray tried to entice Lipton to Georgia Tech with what was at the time the university’s juiciest startup package. He was ultimately successful, but it took nearly twenty years, and by that time, Tech had replaced the School of ICS with the College of Computing, and Ray was in semi-retirement in Maryland. </p>
</em><p><em>
</em>
</p></blockquote>
<p></p><p>
<i>Umakishore Ramachandran</i>:</p>
<blockquote><p><b> </b> <em> I have fond memories of my early years at Tech after being recruited by Ray to join the small but vibrant group of faculty in ICS specializing in theory, systems, and AI. Ray was extremely caring and supportive in nurturing junior faculty. One could say that Ray’s leadership was instrumental in the transformation of CS at Georgia Tech and putting GT on the map to compete with other more established CS departments around the nation.</em></p><em>
<p>
Ray helped create a sense of family in the department. I recall every day he would be at lunch in the faculty club which in those days served coffee at $0.10. He would be there eating a healthy meal featuring a giant sausage and reserving an entire table for the ICS faculty to join him for lunch. It created such a friendly and amicable environment for discussing any issue.</p>
<p>
I feel compelled to share a lighter anecdote when I got hired by Ray. Those were the days of terminals connected to mini computers, DEC VAX 750 and 780, and I wanted Ray to promise me that he will give me a terminal and modem for connecting to the campus computers from home. Even as a grad student at UW-Madison I had a 2400 baud modem at home. When I arrived at Tech, Ray gave me a 300-baud acoustic coupler since I had not specified what speed modem I wanted in my startup package. Ray had a giant infectious smile all the time that made it difficult to get mad at him for anything. Note: <img src="https://s0.wp.com/latex.php?latex=%7B300+%5Cll+2400%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{300 \ll 2400}" class="latex" title="{300 \ll 2400}" />.</p>
</em><p><em>
He will be missed by anyone and everyone whose lives he touched. </em>
</p></blockquote>
<p></p><p>
<i>Bill Gasarch</i></p>
<p>
Here is a short quote from Bill and see his post for more details.</p>
<blockquote><p><b> </b> <em> I was saddened to hear of Ray Miller’s death. He was at University of Maryland for many years. Since he got his PhD in 1957 and was still active into the 2000’s he had a broad view of computer science. He did theory AND systems AND other things. It was great to talk to him about the early days of computer science before we had these labels. </em>
</p></blockquote>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Ray is missed. Our condolences to his family and his many friends for their loss.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/"><span class="datestr">at June 08, 2019 05:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/07/little-knowledge-can">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/07/little-knowledge-can.html">A little knowledge can make the next step harder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Suppose you have a fill-in-the-unknowns puzzle, like Sudoku. Can making some deductions and filling in those parts of the puzzle make the whole thing harder to solve than it was before you started? Sometimes, yes!</p>

<p>I have in mind human-style puzzle-deduction rules, where you see a piece of the puzzle that matches some pattern and use that to deduce what one of the unknowns should be. And by “harder” I mean that a puzzle that was previously possible to solve by some set of deduction rules, after the deduction, stopped being possible for those rules to solve. Of course this is possible if you have a bad set of deduction rules. But normally, at least for the kinds of patterns I think about when solving puzzles, if a pattern matches in a partially completed puzzle, then it or a simplification of it will continue to match no matter how I fill in more of the unknowns. Most of the deduction pattern that I use are monotonic, in this sense. If you had a collection of patterns that was not monotonic, you could add all ways of partially filling them in to your collection, and get a better set of patterns, right?</p>

<p>Wrong! There can be valid deduction patterns for which this extension to monotonic sets would produce invalid patterns. I’m pretty sure this can happen in Sudoku, actually, but the example I have in mind comes from a different puzzle game I’ve been playing lately, part of Simon Tatham’s puzzle collection, where it’s called “map”. It’s based on the problem of <a href="https://en.wikipedia.org/wiki/Precoloring_extension">precoloring extension</a>: you’re given a partially 4-colored planar map, and you have to fill in the rest of the colors.
And it’s trivially NP-complete, by a reduction from planar 3-coloring (augment a 3-coloring instance by extra vertices of the fourth color, preventing any of the given instance vertices from having that color) but the puzzles usually presented by the puzzle app are solvable by hand, even when they’re large and at the highest of its levels of difficulty.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/map-screenshot.png" alt="Screenshot of the map puzzle from Simon Tatham's puzzle collection" /></p>

<p>If that were all, then I think all deduction rules could be made monotonic. But I’m going to tell you one more thing about the puzzle, and this one thing makes it non-monotonic. It is that, like Sudoku, every puzzle has a unique solution.
And <a href="https://11011110.github.io/blog/2005/10/15/assuming-uniqueness-in.html">like Sudoku, the assumption of uniqueness leads to new deduction rules</a>.
You can infer that certain regions have to have certain colors, because if they could be colored anything else then there would be more than one solution.</p>

<p>To see how this works, suppose I had a map like the one shown below (where the white squares have not yet been colored, and I’m only showing a small piece of a larger puzzle):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon1.svg" alt="Uncolored pocket in a map puzzle" /></p>

<p>There’s a pocket of uncolored squares extending into the colored region on the left. If I colored the square at the mouth of the pocket yellow, the inner square of the puzzle would be ambiguous: it has only yellow and blue neighbors, so it could be either red or black.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon2.svg" alt="Ambiguously colored pocket in a map puzzle" /></p>

<p>To prevent this ambiguity, the square at the mouth of the pocket must be black. And to force it to be black, the square one step beyond the mouth must be yellow. So from the initial state and the assumption of a unique solution, it’s possible to infer the colors of three previously-blank squares:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon3.svg" alt="Ambiguously colored pocket in a map puzzle" /></p>

<p>But if I have multiple rules at hand, it’s natural for me to try the weaker and easier ones first. Suppose I had done this, and used a weaker inference rule telling me that the square at the mouth was black.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon4.svg" alt="Partially colored pocket in a map puzzle" /></p>

<p>Or suppose I had used a rule that produced the valid (but even weaker) inference that the inner square must be red.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon5.svg" alt="Even more partially colored pocket in a map puzzle" /></p>

<p>Now I can’t use my strong inference rule and color the outer squares! I’ve lost the information about why I colored the inner pocket squares the way I did, and so I’ve lost the ability to make deductions about how the outer squares should be colored to avoid ambiguities. It would not be a valid pattern to see a puzzle in these states and deduce the color of the remaining squares associated with the pocket. So the extension from my initial (valid) rule, which filled in all three squares when they were all blank, to a monotonic rule that fills in the partially-filled-in pocket in the same way, would be invalid. Of course, if the puzzle solution was unique before, it must still be unique after partially filling in the pocket. But with fewer squares colored, my deductive abilities might not be up to the task of reasoning from the remaining parts of the puzzle to its unique solution.</p>

<p>For the same reason, I might not actually want to color yellow the square beyond the mouth, forgetting why it needs to be yellow. Because what I can infer from the initial state is not merely that it should be colored yellow: it’s that the three outer neighbors of this square must have a permutation of the three other colors, so that this square is forced to be yellow, so that the rest of the pocket will have an unambiguous coloring.</p>

<p>I think what this means is that my knowledge representation (consisting only of blank or filled-in puzzle regions) is inadequate. In practice, I actually use a more complex knowledge representation where (either in my head or with markers provided in the puzzle app) I keep track of which colors are still available for the blank regions, but it’s still inadequate, in the same way. It’s not clear to me what the right knowledge representation is, to allow me to keep track of chains of inferences like “one of these squares must be red to prevent this square from becoming yellow to prevent its neighbor from becoming ambiguous” without the complexity of what I remember for each square blowing up to non-constant.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102234384857906663">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/07/little-knowledge-can.html"><span class="datestr">at June 07, 2019 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e20240a48c6624200d">
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><br />
<br />
<a style="display: inline;" href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-pi" class="asset-img-link"> <img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-800wi" alt="It-exists" border="0" class="asset  asset-image at-xid-6a00d83452383469e20240a4b0ee61200b image-full img-responsive" title="It-exists" /> </a><br /></p></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2019/06/my-entry.html"><span class="datestr">at June 07, 2019 06:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15957">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">A Rank Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>More on restricted quantum circuits</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/images-191/" rel="attachment wp-att-15961"><img src="https://rjlipton.files.wordpress.com/2019/06/images.png?w=600" alt="" class="alignright size-full wp-image-15961" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ The Daily Grail ]</font></td>
</tr>
</tbody>
</table>
<p>
Ken Regan just wrote about his paper with Chaowen Guan (GR). The <a href="https://arxiv.org/abs/1904.00101">paper</a> is titled, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today I thought I would add some additional comments on their result.</p>
<p>
The post Ken wrote is thorough, detailed, and geared for an expert in quantum computation. I think he did a service to the field of quantum complexity theory with his write-up. But I thought too that there some things that he neglected to say: things that could be interesting to a wider community. So with all due respect I hope this short discussion is useful.</p>
<p>
</p><p></p><h2> Quantum Circuits </h2><p></p>
<p></p><p>
In studying classic computations, we can restrict our attention to Boolean circuits. Moreover, these circuits can be assumed to consist of one type of gate. A formal way to say this is: Circuits that use NAND gates are <i>universal</i>. Any computation can be converted into a circuit that only uses this type of gate. It is now viewed as a trivial observation, but was not obvious in the beginning. Note, AND and OR and NOT can be implemented with just NAND gates.</p>
<p>
The understanding of what gate types are universal is not only of interest to theorists. Some technologies support directly some types of gates, while others support different types. For example the dominant CMOS technology efficiently implements NAND gates. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/cmos/" rel="attachment wp-att-15958"><img src="https://rjlipton.files.wordpress.com/2019/06/cmos.png?w=150&amp;h=300" alt="" width="150" class="aligncenter size-medium wp-image-15958" height="300" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Wikipedia ]</font>
</td>
</tr>
</tbody></table>
<p>
In studying quantum computations, we can also restrict our attention to <i>quantum circuits</i>. These circuits need only use gates from a small family. These gate types are more complex than just NAND gates, as you might expect since quantum circuits manipulate qubits and not just bits. I believe that it is fair to say that the fact that there are universal gates for quantum circuits is not trivial. There are many types of gates that are studied: Clifford, NOT, Fredkin, Hadamard, Toffoli, and many others.</p>
<p>
Again the understanding of what gate types are universal is not only of interest to theorists. Quantum circuits are not easy to build, the technology is still evolving. There is yet no CMOS answer to the question: How do we build quantum hardware? Errors may be the limiting factor for large quantum circuits. We will see. One consequence of the difficulty of building universal quantum circuits is the interest in circuits that use gates that are from a family that is <i>not universal</i>.</p>
<p>
The hope is several fold: </p>
<ul>
<li>
Perhaps understanding gates that are not universal will help us understand general quantum computations. <p></p>
</li><li>
Perhaps some important problems in chemistry, for example, may be solvable with these non-universal circuits. <p></p>
</li><li>
Perhaps the mathematics of these restricted circuits will be interesting in its own right.
</li></ul>
<p>
The third point is where GR’s work falls.</p>
<p>
</p><p></p><h2> Stabilizer Circuits </h2><p></p>
<p></p><p>
The class of quantum circuits that GR studied are the <i>stabilizer circuits</i>. See this for an earlier discussion on these <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">circuits</a>. The key is that these circuits are not universal. And more importantly they can be simulated by classical computers in polynomial time. In the upside-down world of quantum complexity theory, the ability to efficiently simulate them is bad. In order to show that quantum computations are new and exciting, being able to simulate them on your laptop is not good.</p>
<p>
The punch-line is: how efficient is this simulation? There are polynomial algorithms and there are useful polynomial algorithms. The 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman (AG) says that the time complexity is <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />. Looks good.</p>
<p>
</p><p></p><h2> A Breakthrough? </h2><p></p>
<p></p><p>
For a few hours, maybe days, GR thought they could show that stabilizer circuits solve the matrix rank problem. In part this was based on a misunderstanding, but it was also based on not needing the full generality of maintaining the system between single-qubit measurements. If they could calculate or even estimate the probability of just one all-qubits measurement in time <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> then the same time would apply to computing matrix rank over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. This would have been an immense result. The best known is that the rank of a matrix can be computed in <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{\omega}}" class="latex" title="{n^{\omega}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> is the current exponent for multiplying <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrices—over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" /> or any field. Today <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> stands at <img src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2.3728\dots}" class="latex" title="{2.3728\dots}" />. </p>
<p>
Clearly, they were excited. I still recall Ken calling me with the possible news. I thought that it was not a crazy idea. Their plan was: </p>
<ol>
<li>
Reduce matrix rank to a quantum problem; <p></p>
</li><li>
Show that problem could be computed by stabilizer circuits; <p></p>
</li><li>
Then invoke the simulation theorem for these circuits to get a classical algorithm.
</li></ol>
<p>Wow. This trip from a classic problem, to a quantum one, and back was intriguing. </p>
<p>
</p><p></p><h2> Saving Grace </h2><p></p>
<p></p><p>
Quickly GR figured out the issue. They could in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> total time determine the probability of each individual qubit being <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. That probability would be either <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> since their reduction from rank gives a circuit in which the probability of getting all <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />s is guaranteed to be positive. If <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> values are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> that does not make the answer <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2^k}}" class="latex" title="{\frac{1}{2^k}}" />, because of entanglements. They had thought that the Aaronson-Gottesman algorithm took care of the entanglement bookkeeping in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, but it services only one qubit in that time. Ironically, AG expressly note that they improved Gottesman’s earlier <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time for this task to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />—exactly what GR thought they would get—but AG still only get <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> time for operations involving all the qubits.</p>
<p>
GR worked out the full picture from a more-recent <a href="https://arxiv.org/abs/1712.03554">paper</a> by Héctor García-Ramírez and Igor Markov of Michigan, which describes detailed software for simulating stabilizer circuits. Thus the quantum method would only put matrix rank into cubic time—not new. But then they realized that the goalposts for a result in the <em>other</em> direction were only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" />. This they could beat with most of their hard work already done. Thus they can improve <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> cases by AG and some subsequent papers to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />. In theory, that is—the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> algorithms are galactic. But what GR also have is a pretty result that is not galactic at all:</p>
<blockquote><p><b>Theorem 1</b> <em> If membership in a certain class of undirected <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graphs can be decided in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, then the following problems all have the same time complexity: </em></p><em>
<ol>
<li>
The strong simulation of quantum stabilizer circuits; <p></p>
</li><li>
The computation of the rank of matrices over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />; <p></p>
</li><li>
The counting of solutions to classical quadratic forms modulo <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" />.
</li></ol>
</em><p><em></em>
</p></blockquote>
<p></p><p>
Note that all these problems have upper bounds of <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" /> by GR; the point is that their bounds would coincide even if matrix rank turns out to be easier than matrix multiplication. The next post will talk about the class of graphs. Just to be clear:</p>
<ul>
<li>
If the graphs are in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, this does not mean the problems are all in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time—they could still need <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" /> time. <p></p>
</li><li>
The graphs are in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. This is for dense graphs with order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" /> edges. <p></p>
</li><li>
The reductions from rank to the other problems run in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time unconditionally—the graphs involved are all bipartite so their status is known.
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I hope that this discussion helps shed some light on the new result of GR. I hope that raising the covers on how they found their theorem is useful. Research is not smooth, is not a straight-line from start to finish, and their journey is not atypical.</p>
<p>[Edited wrong word: thorough and fixed qubits]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/"><span class="datestr">at June 07, 2019 12:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/085">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/085">TR19-085 |  Approximate Degree-Weight and Indistinguishability | 

	Emanuele Viola, 

	Xuangui Huang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that the Or function on $n$ bits can be point-wise approximated with error $\eps$ by a polynomial of degree $O(k)$ and weight $2^{O(n \log (1/\eps)/k)}$, for any $k \geq \sqrt{n \log 1/\eps}$.  This result is tight for all $k$.  Previous results were either not tight or had $\eps = \Omega(1)$.  In general we obtain an approximation result for any symmetric function, also tight.  Building on this we also obtain an approximation result for bounded-width CNF.  For these two classes no such result was known.

One motivation for such results comes from the study of indistinguishability.
Two distributions $P$, $Q$ over $n$-bit strings are $(k,\delta)$-indistinguishable if their projections on any $k$ bits have statistical distance at most $\delta$.
The above approximations give values of $(k,\delta)$ that suffice to fool symmetric functions and bounded-width CNF, and the first result is tight.
Finally, we show that any two $(k, \delta)$-indistinguishable distributions are $O(n)^{k/2}\delta$-close to two distributions that are $(k,0)$-indistinguishable, improving the previous bound of $O(n)^k \delta$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/085"><span class="datestr">at June 06, 2019 11:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/06/06/workshop-on-simplicity-and-robustness-in-complex-markets-stony-brook-university-july-11-12-2019/">Workshop on Simplicity and Robustness in Complex Markets, Stony Brook University, July 11-12, 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="font-weight: 400;">As part of the <a href="http://www.gtcenter.org/?page=Conference.html">30<sup>th</sup> Stony Brook International Conference on Game Theory</a>, we will be holding a workshop on Simplicity and Robustness in Complex Markets.  The workshop will be held July 11-12 at Stony Brook University.</p>
<p style="font-weight: 400;">The goal of this workshop is to bring together researchers from Computer Science, Game Theory, Economics, and Operations Research to focus on issues related to simplicity, robustness, and approximation in economic problems.  For more information, including the workshop program, please visit</p>
<p style="font-weight: 400;"><a href="http://www.gtcenter.org/?page=Workshops.html">http://www.gtcenter.org/?page=Workshops.html</a></p>
<p style="font-weight: 400;">Early registration is open until June 16 at <a href="http://www.gtcenter.org/?page=Registration.html">http://www.gtcenter.org/?page=Registration.html</a></p>
<p style="font-weight: 400;">We hope to see you there!</p>
<p style="font-weight: 400;">Michal Feldman and Brendan Lucier</p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2019/06/06/workshop-on-simplicity-and-robustness-in-complex-markets-stony-brook-university-july-11-12-2019/"><span class="datestr">at June 06, 2019 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/">TCS+ talk: Wednesday, June 12th — John Wright, MIT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk—and last of the season!—will take place this coming Wednesday, June 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). <strong>John Wright</strong> from MIT will speak about “<em>NEEXP in MIP*</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote>
<p style="text-align: justify;">Abstract: A long-standing puzzle in quantum complexity theory is to understand the power of the class <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP*}" class="latex" title="\textsf{MIP*}" /> of multiprover interactive proofs with shared entanglement. This question is closely related to the study of entanglement through non-local games, which dates back to the pioneering work of Bell. In this work we show that <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP*}" class="latex" title="\textsf{MIP*}" /> contains <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEEXP}" class="latex" title="\textsf{NEEXP}" /> (non-deterministic doubly-exponential time), exponentially improving the prior lower bound of <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEXP}" class="latex" title="\textsf{NEXP}" /> due to Ito and Vidick. Our result shows that shared entanglement exponentially increases the power of these proof systems, as the class <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP}" class="latex" title="\textsf{MIP}" /> of multiprover interactive proofs without shared entanglement is known to be equal to <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEXP}" class="latex" title="\textsf{NEXP}" />.</p>
</blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/"><span class="datestr">at June 06, 2019 04:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1381501092894849452">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html">What Happened to the Surprising Theorems?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Twenty-five years ago Peter Shor presented a polynomial-time factoring algorithms for quantum computers. For Peter, it was a simple translation of a <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">quantum algorithm</a> due to Dan Simon. For the rest of us, it was a shock, while we knew quantum could do some seemingly artificial problems exponentially faster, no one expected a natural problem like factoring to fall so quickly. I remember remarking at the time that Shor bought quantum computing twenty years, now I would say fifty.<br />
<div>
<br /></div>
<div>
That may have been the last time I was truly shocked by a theorem in theoretical computer science. I've been shocked by proofs, that Primes are in P, Undirected connectivity in Log space, NEXP not in ACC<sup>0</sup>, Graph Isomorphism in quasi-polynomial time. But the theorems themselves all went in the directions we expected.<br />
<br />
In the ten years before Shor we had plenty of surprises, interactive proofs, zero-knowledge proofs, probabilistically checkable proofs, nondeterministic space closed under complementation, hardness versus randomness, the permanent hard for the polynomial-time hierarchy. It seemed to come to a hard stop after Shor.<br />
<br />
There have been some mild surprises, the Hadamard isn't rigid, holographic algorithms, the complexity of Nash equilibrium, QIP = PSPACE, and many others. But nothing that has made us  rethink the complexity world.<br />
<br />
This reflects the maturity of our field. How many shocking theorems have we seen recently in math in general? We're shocked by proofs of the Poincaré conjecture and Fermat's last theorem but both went in the expected direction.<br />
<br />
We will have some shocking theorem in the future, maybe Factoring in P or L = NL. To be truly shocked it would have to be something I can't even imagine being true today.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html"><span class="datestr">at June 06, 2019 02:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/robust_apps/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/robust_apps/">Robustness beyond Security&amp;#58; Computer Vision Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://gradientscience.org/robust-apps.pdf" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left;" href="http://git.io/robust-apps" class="bbutton">
<i class="fab fa-github"></i>
   Notebooks
</a>
<a style="float: left;" href="http://bit.ly/robustness_demo" class="bbutton">
<i class="fas fa-code"></i>
   Live Demo
</a></p>

<p><i>We discuss our <a href="https://gradientscience.org/robust-apps.pdf">latest paper</a>
on computer vision applications of robust classifiers.
We are able to leverage the features learned by a single classifier to 
develop a rich toolkit for diverse computer vision applications.
Our results suggest the robust classification framework as a viable alternative
to more complex or task-specific approaches.
</i></p>

<p>In our <a href="https://gradientscience.org/robust_reps/">previous post</a>, we saw how robust models 
capture high-level, human-aligned features that can be directly manipulated through 
gradient descent. In this post, we demonstrate how to leverage these robust models 
to perform a wide range of computer vision tasks. In fact, we perform all these tasks 
by simply optimizing the predicted class scores of a <i>single</i> robustly trained 
classifier (per dataset). The resulting toolkit is simple, versatile and 
reliable—to highlight its consistency, in this post we visualize the performance 
of our method using <i>random</i> (not cherry-picked) samples.</p>

<p>Our corresponding paper can be found <a href="https://gradientscience.org/robust-apps.pdf">here</a>, and you 
can reproduce all of our results using open-source IPython notebooks 
<a href="http://git.io/robust-apps">here</a>.
We also have a live <a href="http://bit.ly/robustness_demo">demo</a> where you play with a 
trained robust model like this:</p>

<video align="right" style="width: 90%;">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.mp4" type="video/mp4">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.webm" type="video/webm">
</source></source></video>

<h3 id="robust-models-as-a-tool-for-input-manipulation">Robust Models as a Tool for Input Manipulation</h3>
<p>The key primitive of our approach is <i>class maximization</i>: maximization of class 
log-probabilities (scores) from an <a href="https://gradientscience.org/robust_opt_pt1/">adversarially robust model</a> 
using gradient descent in input space.</p>

<p>As discussed in our <a href="https://gradientscience.org/robust_reps/">last post</a>, robust models learn 
representations that are more aligned with human perception. As it turns out, 
performing class maximization on these models actually introduces class-relevant 
characteristics in the corresponding input (after all, these class log-probabilities 
are just linear combinations of learned representations). To visualize this, here is 
the result of class maximization for a few random inputs:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/targeted.jpg" alt="Targeted adversarial examples for a robust model" /></p>
<div class="footnote"> For a robust
model, maximizing the predicted probability of a specific class enhances key
features of that class in the input. </div>

<p>These results are also in line with <a href="https://arxiv.org/abs/1805.12152">our previous
work</a>, where we observed that large
adversarial perturbations for robust models often actually resemble natural
examples of the corresponding incorrect class.</p>

<p>In the rest of this post, we will explore how to perform a variety of computer
vision tasks using only class maximization. It turns out that robustness is all
you need!</p>

<h3 id="generation">Generation</h3>
<p>We begin by leveraging robust models to generate diverse,
realistic images. As we saw in the previous section, it is possible to introduce
salient features of a target class into an input through class maximization.
This simple operation alone turns out to also suffice to (class-conditionally)
generate images.</p>

<p>To generate an image, we randomly sample a starting point (seed) and then
execute (starting from that seed) the projected gradient ascent operation
underlying class maximization. The key question here is: how do we sample the
seed input? A natural idea is to just fit a Gaussian distribution to each class
(in image space), and sample seeds from that distribution.. Despite its
simplicity, this approach already leads to fairly diverse and realistic samples:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <img style="width: 38%; margin: 8px;" class="image-container" id="gen1" />
        <img style="width: 60%;" class="image-container" id="gen2" />
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>We expect that there is still room for improvement; for example, one could
replace Gaussians with a sophisticated class of distributions for seed sampling.</p>

<h3 id="image-to-image-translation">Image-to-Image translation</h3>

<p>The ability to introduce perceptually meaningful, class-relevant features in
image space (via class maximization) enables a very intuitive approach to
performing <a href="https://arxiv.org/abs/1703.10593">image-to-image translation</a>, the
task of transforming inputs from a source to a target domain (e.g., transforming
horses into zebras in photos). To perform this task, we first train a classifier
to <em>robustly</em> distinguish between the two domains. This process encourages the
classifier to learn key characteristics of each domain. We then perform image
translation on an image from a given domain simply by using class maximization
towards the target domain—this suffices! Here are some instances of our
approach applied to typical datasets:</p>

<div class="widget">
    <div class="choices_one" id="translate">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="translatedclass">Translated Image</span>
    <div class="beer-slider selected_one" id="translate_slider">
    <img style="width: 336px;" id="translate1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="translate2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: choose an image on the left to see the
result of image-to-image translation.
</div>
<p>This is what a few particularly nice samples produced by our
method look like:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/translation.jpeg" alt="Select image-to-image translations" /></p>

<p>Just as with image generation, we find reasonable solutions to the task using
only class maximization on a robust classifier.</p>

<h3 id="inpainting">Inpainting</h3>

<p>Next, we consider the task of image inpainting—recovering images with large
missing or corrupted regions. At a high level, we would like to fill in the
damaged regions with features that are human-meaningful and consistent with the
rest of the image. Within our framework, the most natural way to do this is to
perform class maximization (towards the original class)  while also penalizing
large changes to the uncorrupted regions of the image. The intuition being that
this process restores the “missing” features while only minimally modifying the
rest of the image. This is how a few random inpainted images produced by our
method look like:</p>

<div class="widget">
    <div class="choices_one" id="inpaint">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Inpainted Image</span>
    <div class="beer-slider selected_one" id="inpaint_slider">
    <img style="width: 336px;" id="inpaint1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="inpaint2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on an image on the left to see
an instance of inpainting for that image.
</div>

<p>Interestingly, even when our method produces reconstructions that differ from
the original, they are often still perceptually plausible to a human. Here are a
few select samples:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/inpainting_errors.jpeg" alt="Failure modes of inpainting using robust models" /></p>
<div class="footnote">
Even when inpainting fails to recover the original uncorrupted image, the result is 
often still perceptually plausible to a human.
</div>

<h3 id="superresolution">Superresolution</h3>

<p>To perform inpainting, we used class maximization to restore relevant features
in corrupted images. The exact same intuition applies to the task of
superresolution, i.e., improving the resolution of an image in a human-
meaningful way. Specifically, using class maximization towards the underlying
true class, we can accentuate image features that are distorted in the low-
resolution image. Here we apply this method to images from the CIFAR10 dataset
(32x32 pixels) to a resolution of 224x224 (7-fold upsampling):</p>

<div class="widget">
    <div class="choices_one" id="upsample">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Upsampled Image</span>
    <div class="beer-slider selected_one" id="upsample_slider">
    <img style="width: 336px;" id="upsample1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="upsample2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
7-fold superresolution.
</div>

<p>Since the starting point of the underlying class maximization comes from a crude
upsampling (i.e., nearest neighbor interpolation) of the low-resolution image,
the final images exhibit some pixelation artifacts. We expect, however, that
combining this approach with a more sophisticated initialization will yield even
more realistic samples.</p>

<h3 id="interactive-image-manipulation">Interactive Image Manipulation</h3>

<p>Finally, using this simple primitive, one can build an interactive toolkit for
performing input space manipulations.</p>

<h4 id="sketch-to-image">Sketch-to-image</h4>

<p>Class maximization with robust classifiers turns out to yield human-meaningful
transformations even for <em>arbitrary</em> inputs. This enables us to use this
primitive to even transform hand-drawn sketches into realistic images. Here is
the result of maximizing a chosen class probability from a very crude sketch:</p>

<div class="widget">
    <div class="choices_one" id="sketch">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="sketchclass">Enhanced Image</span>
    <div class="beer-slider selected_one" id="sketch_slider">
    <img style="width: 336px;" id="sketch1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="sketch2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: select any of the sketches on the left to see it 
converted into a realistic image.
</div>

<p>You can draw realistic looking images with this method interactively
<a href="http://bit.ly/robustness_demo">here</a>, without any artistic skill!</p>

<h4 id="paint-with-features">Paint-with-Features</h4>

<p>In fact, we can achieve an even more fine-grained level of manipulation if we
directly perform maximization on the <em>representations</em> learned by the robust
model, instead of the class probabilities. (Recall that in the <a href="https://gradientscience.org/robust_reps/">last
post</a> we saw how individual components can
correspond to human-level features such as “stripes”.) By adding a human in the
loop, we can choose particular regions of the image to modify and specific
features to add. This leads to a versatile paint tool (inspired by
<a href="http://gandissect.res.ibm.com/ganpaint.html">GANpaint</a>) that can perform
manipulation such as this:</p>

<div class="widget">
    <div class="choices_one_paint" id="paint_left1">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <div class="selected_three" id="paintclass">
    
    <video style="width: 100%;" id="paint_video">
        <source type="video/mp4" id="paint_selected_mp4">
        <source type="video/webm" id="paint_selected_webm">
    </source></source></video>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: Choose one of the images on the left to see
a demonstration of painting high-level features onto the image.
</div>

<p>Here, we added a feature to a given region of the image by simply maximize the
corresponding activation (a single component of the robust representation
vector) while penalizing changes to the rest of the image. By successively
performing such <i>activation maximization</i> in different parts of the image,
we can paint with high-level concepts (e.g., grass or stripes).</p>

<h3 id="takeaways">Takeaways</h3>

<p>In this blog post, we applied simple, first-order manipulations of the
representation learned by a <em>single</em> robust classifier to perform a number of
computer vision tasks. This is contrast to prior approaches that
often required specialized
and sophisticated techniques. Crucially, to highlight the potential of the core
methodology itself, we used the same simple toolkit for all tasks and datasets,
and with minimal tuning and no task-specific optimizations. We expect that the
addition of domain knowledge and leveraging more perceptually-aligned notions of
robustness will further boost the performance of this toolkit. Importantly,
the models we use here are truly off-the-shelf and
are trained in a standard (and stable) manner (via
<a href="https://gradientscience.org/robust_opt_pt1/">robust optimization</a>).</p>

<p>Furthermore, our results highlight the utility of the basic classification
toolkit outside of classification tasks. We hope that our framework will expand to
offer ways to perform other vision tasks, on par with the existing
state-of-the-art techniques (e.g., based on generative models). Finally, our
findings highlight the merits of adversarial robustness as a goal that goes
beyond the security and reliability contexts this goal was considered in so far.</p></div>







<p class="date">
<a href="http://gradientscience.org/robust_apps/"><span class="datestr">at June 06, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/084">TR19-084 |  Resolution Lower Bounds for Refutation Statements | 

	Michal Garlik</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For any unsatisfiable CNF formula we give an exponential lower bound on the size of resolution refutations of a propositional statement that the formula has a resolution refutation. We describe three applications. (1) An open question in [Atserias-Müller,2019] asks whether a certain natural propositional encoding of the above statement is hard for Resolution. We answer by giving an exponential size lower bound. (2) We show exponential resolution size lower bounds for reflection principles, thereby improving a result in [Atserias-Bonet,2004]. (3) We provide new examples of CNFs that exponentially separate Res(2) from Resolution (an exponential separation of these two proof systems was originally proved in [Segerlind-Buss-Impagliazzo,2004]).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/084"><span class="datestr">at June 05, 2019 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/">Postdoc position in TCS at Lund University (apply by June 14, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CS Department at Lund University invites applications for a postdoc position in TCS. The application deadline is June 14, 2019. See <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a><br />
Email: jakob.nordstrom@cs.lth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/"><span class="datestr">at June 05, 2019 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/083">TR19-083 |  Testing Graphs against an Unknown Distribution | 

	Lior Gishboliner, 

	Asaf Shapira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The area of graph property testing seeks to understand the relation between the global properties of a graph and its local statistics. In the classical model, the local statistics of a graph is defined relative to a uniform distribution over the graph’s vertex set. A graph property $\mathcal{P}$ is said to be testable if the local statistics of a graph can allow one to distinguish between graphs satisfying $\mathcal{P}$ and those that are far from satisfying it.

Goldreich recently introduced a generalization of this model in which one endows the vertex set of the input graph with an arbitrary and unknown distribution, and asked which of the properties that can be tested in the classical model can also be tested in this more general setting. We completely resolve this problem by giving a (surprisingly ``clean'') characterization of these properties. To this end, we prove a removal lemma for vertex weighted graphs which is of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/083"><span class="datestr">at June 04, 2019 09:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7512">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/">ITCS 20 call for papers (guest post by Thomas Vidick)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>

We invite you to submit your papers to the 11th Innovations in<br />Theoretical Computer Science (ITCS). The conference will be held at<br />the University of Washington in Seattle, Washington from January 12-14,<br />2020.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message<br />(e.g., introducing a new concept, model or understanding, opening a new<br />line of inquiry within traditional or interdisciplinary areas,<br />introducing new mathematical techniques and methodologies, or new<br />applications of known techniques). ITCS welcomes both conceptual and<br />technical contributions whose contents will advance and inspire the<br />greater theory community.</p>



<p>Submission deadline: September 9, 2019 (05:59pm PDT)<br />Notification to authors: October 31, 2019<br />Conference dates: January 12-14, 2020</p>



<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" target="_blank" rel="noreferrer noopener">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for<br />detailed information regarding submissions.</p>



<p>Program committee</p>



<p>Nikhil Bansal, CWI + TU Eindhoven<br />Nir Bitansky, Tel-Aviv University<br />Clement Canonne, Stanford<br />Timothy Chan, University of Ilinois at Urbana-Champaign<br />Edith Cohen, Google and Tel-Aviv University<br />Shaddin Dughmi, University of Southern California<br />Sumegha Garg, Princeton<br />Ankit Garg, Microsoft research<br />Ran Gelles, Bar-Ilan University<br />Elena Grigorescu, Purdue<br />Tom Gur, University of Warwick<br />Sandy Irani, UC Irvine<br />Dakshita Khurana, University of Illinois at Urbana-Champaign<br />Antonina Kolokolova, Memorial University of Newfoundland.<br />Pravesh Kothari, Carnegie Mellon University<br />Rasmus Kyng, Harvard<br />Katrina Ligett, Hebrew University<br />Nutan Limaye, IIT Bombay<br />Pasin Manurangsi, UC Berkeley<br />Tamara Mchedlidze, Karlsruhe Institute of Technology<br />Dana Moshkovitz, UT Austin<br />Jelani Nelson, UC Berkeley<br />Merav Parter, Weizmann Institute<br />Krzysztof Pietrzak, IST Austria<br />Elaine Shi, Cornell<br />Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br />Li-Yang Tan, Stanford<br />Madhur Tulsiani, TTIC<br />Gregory Valiant, Stanford<br />Thomas Vidick, California Institute of Technology (chair)<br />Virginia Vassilevska Williams, MIT<br />Ronald de Wolf, CWI and University of Amsterdam<br />David Woodruff, Carnegie Mellon University

</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/"><span class="datestr">at June 04, 2019 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15923">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">A Quantum Connection For Matrix Rank</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>A new paper with Chaowen Guan</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/chaowenwhiteboard/" rel="attachment wp-att-15934"><img src="https://rjlipton.files.wordpress.com/2019/06/chaowenwhiteboard.jpg?w=180&amp;h=130" alt="" width="180" class="alignright wp-image-15934" height="130" /></a></p>
<p>
Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our <a href="https://arxiv.org/abs/1904.00101">paper</a>, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today we emphasize new connections we have found between simulating special quantum circuits and computing matrix rank over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />.<span id="more-15923"></span></p>
<p>
The quantum circuits involved have been known as polynomial-time solvable <a href="https://en.wikipedia.org/wiki/Gottesman-Knill_theorem">since</a> <a href="https://arxiv.org/abs/quant-ph/9807006v1">1998</a>. They are not universal but form important building blocks of quantum systems people intend to build. They impact the problem of showing quantum circuits are more powerful than classical circuits—the <i>quantum advantage problem</i>—in terms of how much harder quantum stuff must be added to them. </p>
<p>
The question is: How efficiently can we simulate these special circuits? Our answer improves the bound from order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^3}" class="latex" title="{n^3}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{\omega}}" class="latex" title="{n^{\omega}}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> here means the current best-known exponent for multiplying <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrices (over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" /> or any field). Today <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> stands at <img src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2.3728\dots}" class="latex" title="{2.3728\dots}" />. The non-quantum problem of counting solutions to a quadratic polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x_1,\dots,x_n)}" class="latex" title="{f(x_1,\dots,x_n)}" /> modulo 2 is likewise improved from the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.9881">shown</a> by Andrzej Ehrenfeucht and Marek Karpinski to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />.</p>
<p>
This comes at a price, however, because the matrix multiplication algorithms that optimize the exponent are <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic</a>. In this post we’ll emphasize what is <em>not</em> galactic: reductions to and from the problem of computing matrix rank that run in linear time—meaning <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time for dense matrices—except for the need to check a yes/no condition in one of them. All this builds on the algebraic methods in our <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">paper</a> last year with Amlan Chakrabarti of the University of Calcutta.</p>
<p>
Chaowen has contributed a <a href="https://rjlipton.wordpress.com/2018/07/02/local-hams-in-la-jolla/">post</a> and some other materials for this blog. His work first came up in a <a href="https://rjlipton.wordpress.com/2016/06/29/getting-to-the-roots-of-factoring/">post</a> three years ago that saluted Dick and Kathryn’s wedding. Today is their third anniversary—so this post also comes with happy anniversary wishes.</p>
<p>
</p><p></p><h2> Strong Simulation Problems </h2><p></p>
<p></p><p>
We have <a href="https://rjlipton.wordpress.com/2010/08/02/quantum-algorithms-via-linear-algebra/">covered</a> <a href="https://rjlipton.wordpress.com/2010/08/25/quantum-algorithms-a-different-view-again/">quantum</a> <a href="https://rjlipton.wordpress.com/2011/10/26/quantum-chocolate-boxes/">algorithms</a> <a href="https://rjlipton.wordpress.com/2011/11/14/more-quantum-chocolate-boxes/">several</a> <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">times</a>. We discussed <em>stabilizer circuits</em> in an early <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a> on the work with Amlan and <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">covered</a> them more recently in connection with the work of Jin-Yi Cai’s group. Suffice it to say that stabilizer circuits—which extend Clifford circuits by allowing intermediate measurement gates—form the most salient case that classical computers can simulate in polynomial time.</p>
<p>
The simulation time is sometimes cited as <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> going back to a 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman, but there is a catch: this is only for one measurement of one qubit. For general (non-sparse) instances, all of <a href="https://arxiv.org/abs/quant-ph/0504117">various</a> <a href="https://arxiv.org/abs/1305.6190">other</a> <a href="https://web.eecs.umich.edu/~imarkov/pubs/conf/iccd13-quipu.pdf">algorithms</a> need order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" /> time to re-organize their data structures after each single-qubit measurement. This is so even if one merely wants to measure all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> qubits in one shot: the time becomes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" />. This is one case of what is generally called a <em>strong</em> simulation. It is precisely this time that Chaowen and I improved to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />.</p>
<p>
In wider contexts, strong simulation of a quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> means the ability to compute the probability of a given output to high precision. When the input and output are both in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{0,1\}^n}" class="latex" title="{\{0,1\}^n}" /> we may suppose both are <img src="https://s0.wp.com/latex.php?latex=%7B0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^n}" class="latex" title="{0^n}" /> since we can prepend and append <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNOT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NOT}}" class="latex" title="{\mathsf{NOT}}" /> gates to <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. Then strong simulation means computing the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> (or computing <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" title="{|\langle 0^n |C| 0^n \rangle|^2}" /> which is the output probability) to <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-place precision. It doesn’t take much for this to be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-hard, often <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{\#P}}" class="latex" title="{\mathsf{\#P}}" />-complete. If we take the Clifford generating set </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCZ%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+i+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " class="latex" title="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " /></p>
<p>then we can get universal circuits by adding any any one of the following gates: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BT%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Csqrt%7Bi%7D+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+i+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BTof%7D+%3D+%5Cmathit%7Bdiag%7D%281%2C1%2C1%2C1%2C1%2C1%2C%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " class="latex" title="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " /></p>
<p>In the last one we’ve portrayed the <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8 \times 8}" class="latex" title="{8 \times 8}" /> matrix of the <em>Toffoli gate</em> as being <em>block-diagonal</em>. We will later consider block-diagonal matrices permuted so that all <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" /> “blocks” are at upper left.</p>
<p>
There is <a href="https://arxiv.org/abs/1601.07601">much</a> <a href="https://arxiv.org/pdf/1808.00128.pdf">recent</a> <a href="https://arxiv.org/pdf/1712.03554.pdf">literature</a> on trying to simulate circuits with limited numbers of non-Clifford gates, and on how many such gates may be needed for <a href="https://arxiv.org/pdf/1902.04764.pdf">exponential</a> lower bounds—even just to tell whether <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle \neq 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle \neq 0}" />. This plays against a wider context of <a href="https://arxiv.org/abs/1608.00263">efforts</a> <a href="https://arxiv.org/abs/1807.10749">toward</a> <a href="https://arxiv.org/pdf/1905.00444.pdf">quantum</a> <a href="https://arxiv.org/abs/1203.5813">advantage</a>. Chaowen and I have been trying to apply algebraic-geometric techniques for new lower bounds at the high end, but this time we found new upper bounds at the low end.</p>
<p>
</p><p></p><h2> From Matrix Rank to Quantum </h2><p></p>
<p></p><p>
It is not known how to compute the rank <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A)}" class="latex" title="{rk(A)}" /> of a dense matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> in better than matrix-multiplication time, even over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. We may suppose <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is square and symmetric, since we can always form the block matrix </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%27+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+A%5E%5Ctop+%5C%5C+A+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " class="latex" title="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " /></p>
<p>and then <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29+%3D+%5Cfrac%7B1%7D%7B2%7Drk%28A%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A) = \frac{1}{2}rk(A')}" class="latex" title="{rk(A) = \frac{1}{2}rk(A')}" />. In the case of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />, <img src="https://s0.wp.com/latex.php?latex=%7BA%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A'}" class="latex" title="{A'}" /> is the adjacency matrix <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_G}" class="latex" title="{A_G}" /> of an undirected bipartite graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. The rank of <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_G}" class="latex" title="{A_G}" /> for any undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G = (V,E)}" class="latex" title="{G = (V,E)}" /> must be even. Whereas the rank of the <img src="https://s0.wp.com/latex.php?latex=%7B%7CV%7C+%5Ctimes+%7CE%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|V| \times |E|}" class="latex" title="{|V| \times |E|}" /> vertex-edge incidence matrix always equals <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> minus the number of connected components of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, less is <a href="http://web.cs.elte.hu/~lovasz/kurzusok/adjrank16.pdf">known</a> about characterizing <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A_G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A_G)}" class="latex" title="{rk(A_G)}" />. Our first main theorem brings quantum strong simulation into the picture. Let <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> stand for <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" />.</p>
<blockquote><p><b>Theorem 1</b> <em><a name="rank2QC"></a> Given any <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%5Cmathbb%7BF%7D_2%5E%7Bn+%5Ctimes+n%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A \in \mathbb{F}_2^{n \times n}}" class="latex" title="{A \in \mathbb{F}_2^{n \times n}}" /> we can construct in <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time a stabilizer circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> on <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" /> qubits such that </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++rk%28A%29+%3D+%5Clog_2%28%7C%5Clangle+0%5E%7B2n%7D+%7CC%7C+0%5E%7B2n%7D+%5Crangle%7C%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " class="latex" title="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
One interpretation is that if you believe matrix rank is a “mildly hard” function (with regard to <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time computability) then predicting the result of measuring all the qubits in a stabilizer circuit is also “mildly hard.” Such mild hardness would represent a <em>gap</em> between the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time for weak simulation and the time for strong simulation. Such gaps have been noted and proved for extensions of stabilizer circuits but those are between “polynomial” and an intractable hardness notion.</p>
<p>
One can also view Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">1</a> as a possible avenue toward computing matrix rank without doing either matrix multiplication or Gaussian elimination. This is the view Chaowen and I have had all along. </p>
<p>
</p><p></p><h2> From Quantum to Rank </h2><p></p>
<p></p><p>
The distinguishing point of our converse reduction to the rank <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is <em>knowledge of normal forms that depend on <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /></em> where one can use the knowledge <em>to delay or avoid computing them explicitly</em>. The normal forms are for polynomials <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> associated to quantum circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> in our <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">earlier</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">work</a>. Stabilizer circuits yield <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> as a <em>classical quadratic form</em> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />, the integers modulo <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" />. That is, all cross terms <img src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i x_j}" class="latex" title="{x_i x_j}" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> have even coefficients—here, <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. Thus quantum computing enters a debate that occupied Carl Gauss and others over two hundred years ago:</p>
<blockquote><p><b> </b> <em> Should every homogeneous quadratic polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f(x_1,\dots,x_n)}" class="latex" title="{f(x_1,\dots,x_n)}" /> with integer coefficients be called a <b>quadratic form</b>, or only those whose cross terms <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7Dx_i+x_j%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{c_{i,j}x_i x_j}" class="latex" title="{c_{i,j}x_i x_j}" /> all have even coefficients <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{c_{i,j}}" class="latex" title="{c_{i,j}}" />? </em>
</p></blockquote>
<p></p><p>
The point of even coefficients is that they enable having a symmetric <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> <em>integer</em> matrix <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+x%5E%5Ctop+S+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = x^\top S x " class="latex" title="\displaystyle  f(x) = x^\top S x " /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Without that condition, <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> might only be half-integral. This old difference turns out to mirror that between universal quantum computing and classical, because the non-Clifford <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CS}}" class="latex" title="{\mathsf{CS}}" />-gate noted above yields circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> whose <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> have terms <img src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i x_j}" class="latex" title="{x_i x_j}" /> and/or <img src="https://s0.wp.com/latex.php?latex=%7B3+x_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 x_i x_j}" class="latex" title="{3 x_i x_j}" />. While counting solutions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4^n}" class="latex" title="{\mathbb{Z}_4^n}" /> for those polynomials is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P}}" class="latex" title="{\mathsf{P}}" />, counting their <em>binary</em> solutions is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{\#P}}" class="latex" title="{\mathsf{\#P}}" />-complete—an amazing dichotomy we expounded <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">here</a>.</p>
<p>
We hasten to add that for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 2}" class="latex" title="{k = 2}" /> the classical forms coincide with those over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_{2^k}}" class="latex" title="{\mathbb{Z}_{2^k}}" /> whose nonzero cross terms all have coefficient <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k-1}}" class="latex" title="{2^{k-1}}" />. Those are called <em>affine</em> in the work by Jin-Yi and others noted above, and our above-mentioned <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">post</a> noted his 2017 <a href="https://arxiv.org/abs/1705.00942">paper</a> with Heng Guo and Tyson Williams giving another proof of polynomial-time simulation of stabilizer circuits via <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> being affine. Our work improving the polynomial bounds, however, draws on a 2009 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.730.2154">paper</a> by Kai-Uwe Schmidt and further theory of classical quadratic forms. This paper uses work going back to 1938 that decomposes a classical (affine) quadratic form <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_4}" class="latex" title="{\mathsf{Z}_4}" /> further as <a name="repn"></a></p><a name="repn">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+f_0%28x%29+%2B+2%28x+%5Cbullet+v%29+%5Cquad%5Ctext%7Bwith%7D%5Cquad+f_0%28x%29+%3D+x%5E%5Ctop+B+x%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" class="latex" title="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" /></p>
</a><p><a name="repn"></a> for binary arguments <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Here <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> is a binary vector with <img src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i = 1}" class="latex" title="{v_i = 1}" /> if <img src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S[i,i] = 2}" class="latex" title="{S[i,i] = 2}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S[i,i] = 3}" class="latex" title="{S[i,i] = 3}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i =0}" class="latex" title="{v_i =0}" /> otherwise, and the operations including the inner product <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> are mod-2 except that the final <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is <em>alternating</em> if the diagonal of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is all-zero, <em>non-alternating</em> otherwise. Now take <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> to be the rank of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. The key normal-form lemma is:</p>
<blockquote><p><b>Lemma 2</b> <em> There is a change of basis to <img src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{y_1,\dots,y_n}" class="latex" title="{y_1,\dots,y_n}" /> such that if <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is non-alternating then <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> is transformed to </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+%5Ccdots+%2B+y_r%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " class="latex" title="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " /></p>
<p>whereas if <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is alternating then <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is even and <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> is transformed to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+2y_1+y_2+%2B+2y_3+y_4+%2B+%5Ccdots+%2B+2y_%7Br-1%7D+y_r.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " class="latex" title="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " /></p>
</em><p><em>In either case, there is a binary vector <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28y%29+%3D+f%27_0%28y%29+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f(y) = f'_0(y) + 2(y \bullet w)}" class="latex" title="{f(y) = f'_0(y) + 2(y \bullet w)}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. </em>
</p></blockquote>
<p></p><p>
The point is that to evaluate the quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, we don’t need to evaluate <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" />, but can make inferences about the structure of the solution sets to <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x%29+%3D+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C(x) = a}" class="latex" title="{f_C(x) = a}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%2C1%2C2%2C3+%5Cpmod%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a = 0,1,2,3 \pmod{4}}" class="latex" title="{a = 0,1,2,3 \pmod{4}}" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in \{0,1\}^n}" class="latex" title="{x \in \{0,1\}^n}" />. Given the knowledge of <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />, the normal form goes a long way to this. The vector <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is also needed, but the fact of its having only <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits gives hope of finding it in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29+%3D+O%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2) = O(N)}" class="latex" title="{O(n^2) = O(N)}" /> time. That—plus an analysis of the normal form <img src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%2Cw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f'_0,w}" class="latex" title="{f'_0,w}" /> itself of course—would complete an <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time reduction from computing the amplitude to computing <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />.</p>
<p>
</p><p></p><h2> The Needed Piece—For Now </h2><p></p>
<p></p><p>
Chaowen took the lead all through the Fall 2018 term in trying multiple attacks. In the non-alternating case, the change of basis converts <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> into a diagonal matrix <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. In the alternating case, the same process makes <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> a block-diagonal matrix of the kind we mentioned above. The conversion <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+Q+B+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = Q B Q^\top}" class="latex" title="{D = Q B Q^\top}" /> in both cases also yields <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. Of course <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> can be computed by Gaussian elimination in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> time, but this is what we wanted to avoid.</p>
<p>
After poring over older literature on <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" />-time methods, including a 1974 <a href="https://www.ams.org/journals/mcom/1974-28-125/S0025-5718-1974-0331751-8/">paper</a> by James Bunch and John Hopcroft (see also <a href="http://renatoppl.com/blog/2014/08/12/solving-linear-systems-and-inverting-a-matrix-is-equivalent-to-matrix-multiplication/">this</a>), we found a <a href="https://arxiv.org/abs/1802.10453">paper</a> from last year by Jean-Guillaume Dumas and Clément Pernet that gives exactly what we needed: an LDU-type decomposition that yields <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. We only needed to apply the change-of-basis analysis in Schmidt’s paper to this decomposition and combine with the normal-form analysis to establish our algorithm for computing the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" />: </p>
<ol>
<li>
Convert <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> to the classical quadratic form <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> with matrix <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> and associate the <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" /> as above. This needs only <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time. <p></p>
</li><li>
Compute the Dumas-Pernet decomposition <img src="https://s0.wp.com/latex.php?latex=%7BB+%3D+PLDL%5E%5Ctop+P%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B = PLDL^\top P^\top}" class="latex" title="{B = PLDL^\top P^\top}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" /> where <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> is a permutation matrix, <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> is lower-triangular, and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is block-diagonal with blocks that are either <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" />. Of course, this involves computing the rank <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> and takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. Think of it as <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+QBQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = QBQ^\top}" class="latex" title="{D = QBQ^\top}" />. This takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time—indeed, <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2+r%5E%7B%5Comega+-+2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2 r^{\omega - 2})}" class="latex" title="{O(n^2 r^{\omega - 2})}" /> time according to Dumas and Pernet. <p></p>
</li><li>
Compute <img src="https://s0.wp.com/latex.php?latex=%7BD%27+%3D+Q+S+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D' = Q S Q^\top}" class="latex" title="{D' = Q S Q^\top}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />. This, too, takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. <p></p>
</li><li>
If any diagonal <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> block of the original <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> has become <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" />, output <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle = 0}" />. Else, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> is nonzero and we have enough information about <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> to find it—in only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n)}" class="latex" title="{O(n)}" /> time, in fact.
</li></ol>
<p>
This proves our main theorem:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="rank2QC"></a> For stabilizer circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> is computable in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. So is counting binary solutions to a classical quadratic form over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />, or any quadratic polynomial mod 2. </em>
</p></blockquote>
<p></p><p>
Because we use the decomposition, the above is not a clean <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time reduction to computing <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />. It does not make Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">3</a> into a linear-time equivalence. By further analysis, however, we show that the only impediment is needing <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> in step 4 of our algorithm to tell whether <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle = 0}" />. If we are <a href="https://rjlipton.wordpress.com/2010/09/05/promise-problems-and-twopaths/">promised</a> that it is nonzero, then we obtain the probability <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" title="{|\langle 0^n |C| 0^n \rangle|^2}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time from <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> alone. This is actually where the power of Chaowen’s analysis of the normal forms is brightest and neatest. We will devote further posts to this and to illuminating further connections in graph and matroid theory.</p>
<p>
</p><p></p><h2> A Three-Part Example </h2><p></p>
<p></p><p>
Consider the following quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. OK, this is a very low-tech drawing. Besides the six Hadamard gates it has two <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> gates, which are shown as simple bars since they are symmetric:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c1/" rel="attachment wp-att-15927"><img src="https://rjlipton.files.wordpress.com/2019/06/c1.png?w=240&amp;h=112" alt="" width="240" class="aligncenter wp-image-15927" height="112" /></a></p>
<p>
By the rules given <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">here</a>, the three Hadamard gates at left introduce “nondeterministic variables” <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_2%2Cx_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,x_2,x_3}" class="latex" title="{x_1,x_2,x_3}" />. The three Hadamard gates at right also give nondeterministic variables, but they are immediately equated to the output variables <img src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z_1,z_2,z_3}" class="latex" title="{z_1,z_2,z_3}" /> so we skip them. The polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bq_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_C}" class="latex" title="{q_C}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2u_1+x_1+%2B+2u_2+x_2+%2B+2u_3+x_3+%2B+2x_1+x_2+%2B+2+x_2+x_3+%2B+2+x_1+z_1+%2B+2+x_2+z_2+%2B+2+x_3+z_3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " class="latex" title="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " /></p>
<p>Upon substituting <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for all of <img src="https://s0.wp.com/latex.php?latex=%7Bu_1%2Cu_2%2Cu_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u_1,u_2,u_3}" class="latex" title="{u_1,u_2,u_3}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z_1,z_2,z_3}" class="latex" title="{z_1,z_2,z_3}" /> this gives simply <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" class="latex" title="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" />. This is an alternating form with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " class="latex" title="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " /></p>
<p>which is the adjacency matrix of the path graph of length 2 on <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n = 3}" class="latex" title="{n = 3}" /> vertices. Gaussian elimination does not need any prior swaps, so the permutation matrix <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> in the decomposition is the identity and we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D%2C+%5Cquad%5Ctext%7Bgiving%7D%5Cquad+D+%3D+QBQ%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " class="latex" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " /></p>
<p>as the block-diagonal matrix over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" />. Now we re-compute the products over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> to get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+2+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " /></p>
<p>Now <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> has entries that are <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> but they are off-diagonal, and hence cancel when <img src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+D%27+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y^\top D' y}" class="latex" title="{y^\top D' y}" /> is computed in the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis. Since <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is likewise the zero vector, this gives the transformed form as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y_1%2Cy_2%2Cy_3%29+%3D+2y_1+y_2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " class="latex" title="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " /></p>
<p>It is easy to compute that <img src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f'_0}" class="latex" title="{f'_0}" /> has six values of 0 and two values of 2, which gives the amplitude as the difference <img src="https://s0.wp.com/latex.php?latex=%7B6+-+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{6 - 2}" class="latex" title="{6 - 2}" /> divided by the square root of <img src="https://s0.wp.com/latex.php?latex=%7B2%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^6}" class="latex" title="{2^6}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" />, The probability of getting <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> as the result of the measurement is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{4}}" class="latex" title="{\frac{1}{4}}" />.</p>
<p>
Now suppose we insert a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" />-gate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" class="latex" title="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" /> on the first qubit to make a new circuit <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" />. Since <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> are diagonal in the standard basis it does not matter where between the Hadamard gates it goes, say:</p>
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c2/" rel="attachment wp-att-15928"><img src="https://rjlipton.files.wordpress.com/2019/06/c2.png?w=240&amp;h=106" alt="" width="240" class="aligncenter wp-image-15928" height="106" /></a></p>
<p></p><p><br />
After substituting zeroes the form over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}}" class="latex" title="{\mathbb{Z}}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bg+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+2x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" class="latex" title="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" />. This gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" title="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " /></p>
<p>The matrix <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is the same as in the first example, hence so are the matrices <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> and the alternating status of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />. The difference made by <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> and the resulting <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> makes itself felt when we re-compute over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+2+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+2+%5C%5C+1+%26+0+%26+2+%5C%5C+2+%26+2+%26+2+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " /></p>
<p>Well, <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> is far from diagonal—perhaps we shouldn’t use that name—but again the off-diagonal <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />s are innocuous so we really have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%27%27+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+2+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " class="latex" title="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " /></p>
<p>The <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> at upper left does not zero out the amplitude, because it is within a <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" /> block. The <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> at lower right, however, constitutes a <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> block of <img src="https://s0.wp.com/latex.php?latex=%7BD%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D''}" class="latex" title="{D''}" />, so it signifies that <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> is not a possible measurement outcome. Essentially what has happened is that in the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis the form has become </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%27%28y%29+%3D+2y_1%5E2+%2B+2y_1+y_2+%2B+2y_3%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " class="latex" title="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " /></p>
<p>The isolated term in <img src="https://s0.wp.com/latex.php?latex=%7By_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_3}" class="latex" title="{y_3}" /> contributes <img src="https://s0.wp.com/latex.php?latex=%7B%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+2}" class="latex" title="{+2}" /> mod <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> to half the <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />–<img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> assignments so as to cancel the other half, leaving a difference of <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> in the numerator of the amplitude.</p>
<p>
For the third example, let us insert a phase gate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" /> after the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" /> to make a circuit <img src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_3}" class="latex" title="{C_3}" />:</p>
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c3/" rel="attachment wp-att-15929"><img src="https://rjlipton.files.wordpress.com/2019/06/c3.png?w=240&amp;h=90" alt="" width="240" class="aligncenter wp-image-15929" height="90" /></a></p>
<p></p><p><br />
The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ZS}}" class="latex" title="{\mathsf{ZS}}" /> combination is the same as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%5E%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S^*}}" class="latex" title="{\mathsf{S^*}}" />, the adjoint (and inverse) of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" />. Now after substitutions we have <img src="https://s0.wp.com/latex.php?latex=%7Bh_%7BC_3%7D%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+3x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" class="latex" title="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" />, giving: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" title="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " /></p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is still a 0-1 matrix. This <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> has full rank. Again it helps our exposition that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is diagonalizable without swaps (and that the inverse of an invertible lower-triangular matrix is lower-triangular), so we can find <img src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QBQ^\top = D = I}" class="latex" title="{QBQ^\top = D = I}" /> with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+1+%26+1+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " class="latex" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " /></p>
<p>In the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis we get <img src="https://s0.wp.com/latex.php?latex=%7Bh%27%28y%29+%3D+y_1+%2B+y_2+%2B+y_3+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" class="latex" title="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. To test for zero amplitude—before we know what <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is—we compute in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%7B%5Ctop%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+0+%26+0+%5C%5C+0+%26+1+%26+2+%5C%5C+0+%26+2+%26+3+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " /></p>
<p>Again we can ignore the off-diagonal <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />‘s. There is no <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> on the main diagonal, so we know the amplitude is non-zero. To compute it, we only need the information on the diagonal, which tells us <img src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+y_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'_0(y) = y_1 + y_2 + y_3}" class="latex" title="{h'_0(y) = y_1 + y_2 + y_3}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = (1,0,1)}" class="latex" title="{w = (1,0,1)}" /> in the transformed basis. Note that we could have written <img src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'_0(y)}" class="latex" title="{h'_0(y)}" /> down the moment we learned that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> has rank <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is the only rigmarole. The final analysis—using a recursion detailed in the appendix of our paper—gives the amplitude as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2+-+2i%7D%7B8%7D+%3D+%5Cfrac%7B1+-+i%7D%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " class="latex" title="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " /></p>
<p>and so the probability of the output <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B8%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{8}}" class="latex" title="{\frac{1}{8}}" />. </p>
<p>
We remark finally that <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is generally not the same as <img src="https://s0.wp.com/latex.php?latex=%7BQv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Qv}" class="latex" title="{Qv}" />. To see where it comes from, let us now compute <img src="https://s0.wp.com/latex.php?latex=%7BQ+B+Q%5E%7B%5Ctop%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q B Q^{\top}}" class="latex" title="{Q B Q^{\top}}" /> (not <img src="https://s0.wp.com/latex.php?latex=%7BQSQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QSQ^\top}" class="latex" title="{QSQ^\top}" />) over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> to get <img src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%2B+2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QBQ^\top = D + 2U}" class="latex" title="{QBQ^\top = D + 2U}" />. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++f%28x%29+%26%3D%26+x%5E%5Ctop+B+x+%2B+2x%5E%5Ctop+v+%3D+x%5E%5Ctop+Q%5E%7B-1%7D+%28D%2B2U%29+%28Q%5E%5Ctop%29%5E%7B-1%7D+x+%2B+2x%5E%5Ctop+%28Q%5E%7B-1%7D+Q%29+v%5C%5C+%26%3D%26+y%5E%5Ctop+%28D+%2B+2U%29+y+%2B+2+y%5E%5Ctop+Qv%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+%28Q%5E%5Ctop%29%5E%7B-1%7D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y = (Q^\top)^{-1} x}" class="latex" title="{y = (Q^\top)^{-1} x}" />. Now off-diagonal elements in <img src="https://s0.wp.com/latex.php?latex=%7B2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2U}" class="latex" title="{2U}" /> will cancel when taking <img src="https://s0.wp.com/latex.php?latex=%7B2+y%5E%5Ctop+U+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 y^\top U y}" class="latex" title="{2 y^\top U y}" /> modulo 4, so we need only retain the diagonal <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> as a binary vector. Since <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> is binary, <img src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+%5Cmathit%7Bdiag%7D%28u%29+y+%3D+y%5E%5Ctop+u%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y^\top \mathit{diag}(u) y = y^\top u}" class="latex" title="{y^\top \mathit{diag}(u) y = y^\top u}" />. This finally gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+y%5E%5Ctop+D+y+%2B+2y%5E%5Ctop+%28u+%2B+Qv%29+%3D+y%5E%5Ctop+D+y+%2B+2%28y+%5Cbullet+w%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " class="latex" title="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " /></p>
<p>with <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = u + Qv}" class="latex" title="{w = u + Qv}" />. In the third example we have <img src="https://s0.wp.com/latex.php?latex=%7BQv+%3D+%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Qv = (1,1,1)}" class="latex" title="{Qv = (1,1,1)}" /> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++QBQ%5E%7B%5Ctop%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5Ccdot+Q%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+2+%26+1+%26+1+%5C%5C+2+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+2+%26+2+%5C%5C+2+%26+3+%26+0+%5C%5C+2+%26+0+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " class="latex" title="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " /></p>
<p>The diagonal gives <img src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+%280%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u = (0,1,0)}" class="latex" title="{u = (0,1,0)}" /> and so <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv+%5Cpmod%7B2%7D+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = u + Qv \pmod{2} = (1,0,1)}" class="latex" title="{w = u + Qv \pmod{2} = (1,0,1)}" />. This agrees with what we read off above by comparing <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> with <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />. There is a different worked-out example for the triangle graph on three vertices in the paper.</p>
<p>
</p><p></p><h2> Looking Ahead </h2><p></p>
<p></p><p>
Chaowen and I continue to be interested in shortcuts to computing the amplitude and/or probability. Here we take a cue from how Volker Strassen titled his famous 1969 <a href="https://eudml.org/doc/131927">paper</a> on matrix multiplication:</p>
<blockquote><p><b> </b> <em> “Gaussian Elimination is not Optimal.” </em>
</p></blockquote>
<p></p><p>
We would like to find cases where we can say, “Matrix Multiplication is not Optimal.” In view of recent papers blunting efforts to show <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega = 2}" class="latex" title="{\omega = 2}" />—see this <a href="https://rjlipton.wordpress.com/2018/08/30/limits-on-matrix-multiplication/">post</a>—the question may shift to which computations may not need the full power of matrix multiplication and be achievable in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time after all. This applies to computing the rank (over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" />) itself, and the question extends to sparse cases like those considered in the <a href="http://www-scf.usc.edu/~hoyeeche/papers/matrix-rank.pdf">paper</a>, “Fast Matrix Rank Algorithms and Applications,” by Ho Yee Cheung, Tsz Chiu Kwok, and Lap Chi Lau.</p>
<p>
The second circuit in the above example corresponds to a graph with a self-loop at node 1—or, depending on how one counts incidence of self-loops in undirected graphs, one could call it a double self-loop. It exemplifies circuits used to create quantum <a href="https://en.wikipedia.org/wiki/Graph_state">graph states</a>, and those circuits are representative of stabilizer circuits in general. The third circuit can be said to have a “triple loop,” or maybe better, a “3/2-loop”—while if the original <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" />-gate were a single <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" />-gate giving the form <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%5E2+%2B+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" class="latex" title="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" />, we would face the ambiguity of calling it a “loop” or a “half-loop.” Sorting this out properly needs going beyond graph theory. In upcoming posts, Chaowen and I will say more about how all this yields new problems in graph theory and new connections between quantum computing and <em>matroid theory</em>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do our results say about the problem of computing the rank of a matrix, and possibly separating it from dependence on matrix multiplication?</p>
<p>
We hope that we have begun to convey how our paper uncovers a lot of fun computational mathematics. We are grateful for communications from people we’ve approached (some acknowledged in our paper) about possible known connections, but there may be more we don’t know. Our next posts will say more about combinatorial aspects of quantum circuits.</p>
<p>
[fixed name]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/"><span class="datestr">at June 04, 2019 07:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/">ITCS’20 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;"><em>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception!</em></p>
<p><em><strong>tl;dr:</strong> the ITCS’20 CFP has been <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" target="_blank" rel="noopener">posted</a>. Read it, and submit your work there!</em></p>
<hr />
<p> </p>
<p style="text-align: justify;">We invite you to submit your papers to the <a href="http://itcs-conf.org/" target="_blank" rel="noopener">11th Innovations in</a> <a href="http://itcs-conf.org/" target="_blank" rel="noopener">Theoretical Computer Science</a> (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020.</p>
<p style="text-align: justify;">ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the<br />
greater theory community.</p>
<p><strong>Important dates</strong></p>
<ul>
<li><em>Submission deadline:</em> September 9, 2019 (05:59pm PDT)</li>
<li><em>Notification to authors:</em> October 31, 2019</li>
<li><em>Conference dates:</em> January 12-14, 2020</li>
</ul>
<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for detailed information regarding submissions.</p>
<p><strong>Program committee</strong></p>
<p>Nikhil Bansal, CWI + TU Eindhoven<br />
Nir Bitansky, Tel-Aviv University<br />
Clement Canonne, Stanford<br />
Timothy Chan, University of Ilinois at Urbana-Champaign<br />
Edith Cohen, Google and Tel-Aviv University<br />
Shaddin Dughmi, University of Southern California<br />
Sumegha Garg, Princeton<br />
Ankit Garg, Microsoft research<br />
Ran Gelles, Bar-Ilan University<br />
Elena Grigorescu, Purdue<br />
Tom Gur, University of Warwick<br />
Sandy Irani, UC Irvine<br />
Dakshita Khurana, University of Illinois at Urbana-Champaign<br />
Antonina Kolokolova, Memorial University of Newfoundland.<br />
Pravesh Kothari, Carnegie Mellon University<br />
Rasmus Kyng, Harvard<br />
Katrina Ligett, Hebrew University<br />
Nutan Limaye, IIT Bombay<br />
Pasin Manurangsi, UC Berkeley<br />
Tamara Mchedlidze, Karlsruhe Institute of Technology<br />
Dana Moshkovitz, UT Austin<br />
Jelani Nelson, UC Berkeley<br />
Merav Parter, Weizmann Institute<br />
Krzysztof Pietrzak, IST Austria<br />
Elaine Shi, Cornell<br />
Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br />
Li-Yang Tan, Stanford<br />
Madhur Tulsiani, TTIC<br />
Gregory Valiant, Stanford<br />
Thomas Vidick, California Institute of Technology (chair)<br />
Virginia Vassilevska Williams, MIT<br />
Ronald de Wolf, CWI and University of Amsterdam<br />
David Woodruff, Carnegie Mellon University</p></div>







<p class="date">
by ccanonne <a href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/"><span class="datestr">at June 04, 2019 06:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3116995407343145604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">IMU's non-controversial changing the name of the Nevanlinna Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
(I want to thank Alexander Soifer for supplying me with some of the documents I point to in this post. We should all thank him for getting the ball rolling on changing the name of the Nevanlinna Prize.)<br />
<br />
The <i>Nevanlinna Prize </i>was essentially a Fields Medal for Theoretical Computer Science.  I do not know why it is a<i> Prize </i>instead of a <i>Medal.</i><br />
<div>
<br /></div>
<div>
It has been renamed <i>The Abacus Medal. </i>If you want to know why the IMU (International Mathematics Union) thinks the new name is good <i>but do not </i><i>care even a little about why the original name was bad</i> then see this article: <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a>.</div>
<div>
<br /></div>
<div>
So why is <i>The Nevanlinna Prize</i> a bad name? In brief, Rolf Nevanlinna was an enthusiastic Nazi sympathizer. How enthused? He served as the chair of the Finish SS recruitment committee.<br />
<br />
That would seem like enough to get the name changed. In fact, it makes one wonder why the prize originally had the name.<br />
<br />
1) Why the change now?  It began when Alexander Soifer came across this information about Nevanlinna while working on his book<br />
<br />
<i>The Scholar and the State: In Search of Van der Waerdan</i> (see <a href="https://amzn.to/2WnfDYh">here</a> to buy it, see <a href="https://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/47-1.pdf">here</a> for a book review column that includes my review of it).<br />
<br />
He then wrote a letter to the IMU which sponsors the <i>Nevanlinna Prize</i>. The letter is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu.pdf">here</a>. Note that Alexander offered to pay for the prize ($15,000 every four years) if that will help get the name changed.<br />
<br />
After a response that lamely said (I paraphrase): <i>Gee, we didn't know. Oh well</i>. Alex wrote another letter which is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu2.pdf">here</a>.<br />
<br />
The story has a happy ending: the name was changed.  (No, Alexander is not paying for the award.)<br />
<br />
2) For a full summary of why the award was originally named Nevanlinna  and why it was changed see the article, <i>Yes We Can,  </i>by Alexander Soifer,<i> </i>in an issue of the journal <i>Mathematical Competition</i>s, see <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/yeswecan.pdf">here</a>.</div>
<div>
<br /></div>
<div>
3) When is change possible?<br />
<br /></div>
<div>
 Assume Y did X and X is awful (e.g., I assume for most of my readers believing and spreading Nazi propaganda). Assume there is a Y-prize. What does it take to have the name changed?<br />
<br /></div>
<div>
<br /></div>
<div>
a) You need someone pushing hard for it. Kudos to Alexander Soifer who started this.</div>
<div>
<br /></div>
<div>
b) There is no really good reason to use that name in the first place. </div>
<div>
<br /></div>
<div>
What was Nevanlinna's contribution to mathematical aspects of computer science? The IMU (International Mathematics Union) internet page answers:</div>
<div>
<br /></div>
<div>
<i>The prize was named in honors of Rolf Nevanlinna ... who in the 1950's had taken the initiative to the computer organization at Finnish Universities. </i></div>
<div>
<i><br /></i></div>
<div>
That's all. If there was a Gauss Prize (actually there IS a Gauss Prize) and we later found out that Gauss was X, I doubt we would change the name of the award. Gauss's name is on it since he is a great mathematician. </div>
<div>
<br /></div>
<div>
c) The person on the award is not the one giving the money. If we found out that Nobel was an X,  I doubt the name would change since he is paid for it. </div>
<div>
<br /></div>
<div>
d) If the award name is well known then it might not change. Nobel is a good example. I think the Nevanlinna prize is mostly unknown to the public. The Field's medal is better known, though still not that well known. The general public became briefly aware of the Field's medal twice: when it was mentioned in the movie <i>Good Will Hunting,</i> and when Perelman turned it down. Fame is fleeting for both prizes and people.</div>
<div>
<br /></div>
<div>
e) Organizations don't like to change things. Hence X would need to be particularly bad to warrant a name change. </div>
<div>
<br /></div>
<div>
OTHER THOUGHTS</div>
<div>
<br /></div>
<div>
1) Why <i>The Abacus Medal</i>? Perhaps they are worried that if they name it after someone and that someone turns out to be an X they'll have to change it again. I find the explanation given <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a> to be unsatisfying. I find the fact that they make <b>NO MENTION</b> of why they are no longer naming it <i>The</i> <i>Nevanlinna prize </i>appalling and insulting.</div>
<div>
<br /></div>
<div>
2) Lets turn to people who get the awards. If someone solved two Millennium problems and clearly deserved a Field's Medal, but was an X, should they be denied the prize on that basis. I would tend to think no (that is, they should get the prize) but it does trouble me. What would happen?  I honestly don't know.  </div>
<div>
<br /></div>
<div>
3) X will change over time.</div>
<div>
<br /></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html"><span class="datestr">at June 04, 2019 03:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-32902056.post-2757759457141834790">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/goldberg.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html">plans for WINE conferences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="text-align: left;" dir="ltr">Update on the annual Conference on Web and Internet Economics (I am on the steering committee).<br /><br /><a href="http://wine2019.cs.columbia.edu/">WINE 2019</a> (the 15th) will be at Columbia University, December 10-12. We could not avoid the clash with NeurIPS, due to Columbia’s exam schedule. Submission deadline is July 15th.<br /><br />The plan is for WINE 2020 to take place at Peking University, following the tradition to rotate between Europe, USA, and Asia.<br /><br />WINE 2021 is under discussion; one idea it to hold it in Addis Ababa, Ethiopia, which is not as novel as it may seem at first sight, it would be following ICLR 2020 (see <a href="https://venturebeat.com/2018/11/19/major-ai-conference-is-moving-to-africa-in-2020-due-to-visa-issues/">this link</a> (noting the visa issues) and others). The rationale is that Africa has a burgeoning AI community including people who are interested in algorithmic game theory, and for them, Ethiopia is an easy destination (administratively, in particular). WINE 2018 (at Oxford, UK) had (I think) 5 African participants, and about 10 more would have liked to come but were denied visas. These participants brought home to me the point that there is this developing AI community in Africa. At WINE 2018, Eric Sodomka (from Facebook) gave a well-received presentation on the idea of holding a future WINE in Africa. Are there other places in Africa we should be thinking of? I welcome feedback and comments!<br /><br /></div></div>







<p class="date">
by Paul Goldberg (noreply@blogger.com) <a href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html"><span class="datestr">at June 04, 2019 10:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/06/03/trajectories/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/06/03/trajectories/">Is Optimization a Sufficient Language for Understanding Deep Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this Deep Learning era, machine learning usually boils down to defining a suitable objective/cost function for the learning task at hand, and then optimizing this function using some variant of gradient descent (implemented via backpropagation).  Little wonder that hundreds of ML papers each year are devoted to various aspects of optimization. Today I will suggest that if our goal is mathematical understanding of deep learning, then  the optimization viewpoint is potentially insufficient —at least in the conventional view:</p>

<blockquote>
  <p><strong>Conventional View (CV) of Optimization</strong>: Find a solution of minimum possible value of the objective, as fast as possible.</p>
</blockquote>

<p>Note that <em>a priori</em> it is not obvious if all learning should involve optimizing a single objective. Whether or not this is true for  learning in the brain is a longstanding open question in neuroscience. Brain components appear to have been repurposed/cobbled together through various accidents of evolution and the whole assemblage may or may not boil down to optimization of an objective. See <a href="https://arxiv.org/pdf/1606.03813.pdf">this survey by Marblestone et al</a>.</p>

<p>I am suggesting that deep learning algorithms also have important properties that are not always reflected in the objective value. Current deep nets, being vastly overparametrized, have multiple optima. They are trained until the objective is almost zero (i.e., close to optimality) and training is said to succeed if the optimum (or near-optimum) model thus found also performs well on unseen/held-out data —i.e., <em>generalizes.</em> The catch here is that the value of the objective may imply nothing about generalization (see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>).</p>

<p>Of course experts will now ask: “Wasn’t generalization theory invented precisely for this reason as the “second leg” of machine learning,  where optimization is the first leg?” For instance this theory shows how to add regularizers to the training objective to ensure the solution generalizes. Or that <em>early stopping</em> (i.e., stopping before reaching the optimum) or even adding noise to the gradient (e.g. by playing with batch sizes and learning rates) can be preferable to perfect optimization, even in simple settings such as regression.</p>

<p>However, in practice explicit regularizers  and noising tricks can’t prevent deep nets from attaining low training objective even on data with random labels; see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>. Current generalization theory is designed to give <em>post hoc</em> explanations for why a particular model generalized. It is agnostic about <em>how</em> the solution was obtained, and thus makes few prescriptions —apart from recommending some regularization— for optimization.   (See my earlier <a href="http://www.offconvex.org/2017/12/08/generalization1/">blog post</a>, which explains the distinction between descriptive and prescriptive methods, and  that generalization theory is primarily descriptive.) The fundamental mystery is:</p>

<blockquote>
  <p>Even vanilla gradient descent (GD) is good at finding models with reasonable generalization. Furthermore, methods to speed up gradient descent (e.g., acceleration or adaptive regularization) can sometimes lead to worse generalization.</p>
</blockquote>

<p>In other words, GD has an innate bias towards finding solutions with good generalization. Magic happens along the GD trajectory and is not captured in the objective value per se. We’re reminded of the old adage.</p>

<blockquote>
  <p>The journey matters more than the destination.</p>
</blockquote>

<p>I will illustrate this viewpoint by sketching new  rigorous analyses of gradient descent in two simple but suggestive settings. I  hope more  detailed writeups will appear in future blog posts.</p>

<p>Acknowledgements: My views on this topic were initially shaped by the excellent papers from TTI Chicago group regarding the implicit bias of gradient descent (<a href="https://arxiv.org/pdf/1709.01953.pdf">Behnam Neyshabur’s thesis</a> is a good starting point), and then of course by  various coauthors.</p>

<h2 id="computing-with-infinitely-wide-deep-nets">Computing with Infinitely Wide Deep Nets</h2>

<p>Since overparametrization does not appear to hurt deep nets too much, researchers have wondered what happens in the infinite limit of overparametrization: use a fixed training set such as CIFAR10 to train a classic deep net architecture like AlexNet or VGG19 whose “width” —namely, number of channels in the convolutional filters, and number of nodes in fully connected internal layers—- is allowed to increase to <strong>infinity</strong>. Note that initialization (using sufficiently small Gaussian weights) and training makes sense for any finite width, no matter how large. We assume $\ell_2$ loss at the output.</p>

<p>Understandably, such questions can seem hopeless and pointless: all the computing in the world is insufficient to train an infinite net, and we theorists already have our hands full trying to figure out finite nets.  But sometimes in math/physics one can derive insight into questions by studying them in the infinite limit.  Here where an infinite net is training on a finite dataset like CIFAR10, the number of optima is infinite and we are trying to understand what GD does.</p>

<p>Thanks to insights in recent papers on provable learning by overparametrized deep nets (some of the key papers are: <a href="https://arxiv.org/abs/1811.04918">Allen-Zhu et al 1</a>, <a href="https://arxiv.org/abs/1811.03962">Allen-Zhu et al 2</a> <a href="https://arxiv.org/abs/1811.03804">Du et al</a>, <a href="https://arxiv.org/abs/1811.08888">Zou et al</a>) researchers have realized that a nice limiting structure emerges:</p>

<blockquote>
  <p>As width $\rightarrow \infty$, trajectory approaches the trajectory of GD for a kernel regression problem, where the (fixed) kernel in question is the so-called  <em>Neural Tangent Kernel</em> (NTK). (For convolutional nets the kernel is <em>Convolutional NTK or CNTK.</em> )</p>
</blockquote>

<p>The kernel was identified and named by <a href="https://arxiv.org/abs/1806.07572">Jacot et al.</a>, and also implicit in some of the above-mentioned papers on overparametrized nets, e.g. <a href="https://arxiv.org/abs/1810.02054">Du et al</a>.</p>

<p>The definition of this fixed kernel uses the infinite net at its random initialization. For  two inputs $x_i$ and $x_j$ the kernel inner product  $K(x_i, x_j)$  is the inner product of the gradient $\nabla_x$ of the output with respect to the input, evaluated at $x=x_i$, and $x= x_j$ respectively. As the net size increases to infinity this kernel inner product can be shown to converge to a limiting value (there is a technicality about how to define the limit, and the series of new papers have improved the formal statement here; eg <a href="https://arxiv.org/abs/1902.04760">Yang2019</a> and our paper below.).</p>

<p>Our <a href="https://arxiv.org/abs/1904.11955">new paper with Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov and Ruosang Wang</a> shows that the CNTK can be efficiently computed via dynamic programming, giving us a way to efficiently compute the answer of the trained net for any desired input,  <em>even though training the infinite net directly is of course computationally infeasible.</em> (Aside: Please do not confuse these new results with some earlier papers which view infinite nets as kernels or Gaussian Processes —see citations/discussion in our paper—  since they correspond to training only the top layer while freezing the lower layers to a random initialization.) Empirically we find that this infinite net (aka kernel regression with respect to the NTK) yields better performance on CIFAR10 than any previously known kernel —not counting kernels that were  hand-tuned or designed by training on image data. For instance we can compute the kernel corresponding to a 10-layer convolutional net (CNN) and obtain 77.4% success rate on CIFAR10.</p>

<h2 id="deep-matrix-factorization-for-solving-matrix-completion">Deep Matrix Factorization for solving Matrix Completion</h2>

<p><a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix completion</a>, motivated by design of recommender systems, is well-studied for over a decade: given $K$ random entries of an unknown matrix, we wish to recover the unseen entries. Solution is not unique in general. But if the unknown matrix is low rank or approximately low rank and satisfies some additional technical assumptions (eg <em>incoherence</em>) then various algorithms can recover the unseen entries approximately or even exactly. A famous algorithm  based upon <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">nuclear/trace norm</a>  minimization is as follows: find matrix that fits all the known observations and has minimum nuclear norm. (Note that nuclear norm is a convex relaxation of rank.) It is also possible to rephrase this as a single objective in the form required by the Conventional View as follows where $S$ is the subset of indices of revealed entries,  $\lambda$ is a multiplier:</p>



<p>In case you didn’t know about nuclear norms, you will like the interesting suggestion made by <a href="http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization">Gunasekar et al. 2017</a>: let us just forget about the nuclear norm penalty term  altogether. Instead try to recover the missing entries by  simply training (via simple gradient descent/backpropagation) a linear net with two layers on the first term in the loss. This linear net is just a multiplication of two $n\times n $ matrices (you can read about linear deep nets in this <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">earlier blog post by Nadav Cohen</a>) so we obtain the following  where $e_i$ is the vector with all entries $0$ except for $1$ in the $i$th position:</p>



<p>The “data” now corresponds to indices $(i, j) \in S$, and the training loss captures how well the end-to-end model $M_2M_1$ fits the revealed entries.  Since $S$ was chosen randomly among all entries,  “generalization” corresponds exactly to doing well at predicting the remaining entries. Empirically, soving matrix completion this way via deep learning  (i.e., gradient descent to solve for $M_1, M_2$, and entirely forgetting about ensuring low rank) works as well as the classic algorithm, leading to the following conjecture, which if true would imply that the implicit regularization effect of gradient descent in this case is captured exactly by the nuclear norm.</p>

<blockquote>
  <p>(Conjecture by Gunasekar et al.; Rough Statement) When solving matrix completion as above using a depth-$2$ linear net, the solution obtained is exactly the  one obtained by the nuclear norm minimization method.</p>
</blockquote>

<p>But as you may have already guessed, this turns out to be too simplistic. In <a href="https://arxiv.org/abs/1905.13655">a new paper with Nadav Cohen, Wei Hu and Yuping Luo</a>, we report new experiments suggesting that the above conjecture is false. (I hedge by saying “suggest” because some fine print in the conjecture statement makes it pretty hard to refute definitively.) More interesting, we find that if we overparametrize the problem by further increasing the number of layers from two to $3$ or even higher —which we call Deep Matrix Factorization—then this empirically solves matrix completion even better than nuclear norm minimization. (Note that we’re working in the regime where $S$ is slightly smaller than what it needs to be for nuclear norm algorithm to exactly recover the matrix. Inductive bias is most important precisely in such data-poor settings!) We provide partial analysis for this improved performance of depth $N$ nets by analysing —surprise surprise!—the trajectory of gradient descent and showing how it biases strongly toward finding solutions of low rank, and this bias is stronger than simple nuclear norm. Furthermore our analysis suggests that this bias toward low rank  cannot be captured by nuclear norm or any obvious Schatten quasi-norm of the end-to-end matrix.</p>

<p>NB: Empirically we find that Adam, the celebrated  acceleration method for deep learning, speeds up optimization a lot here as well, but slightly hurts generalization. This relates to what I said above about the  Conventional View being insufficient to capture generalization.</p>

<h2 id="conclusionstakeways">Conclusions/Takeways</h2>

<p>Though the above settings are simple, they suggest that to understand deep learning we have to go beyond the Conventional View of optimization, which focuses only on the value of the objective and the rate of convergence.</p>

<p>(1): Different optimization strategies —GD, SGD, Adam, AdaGrad etc. —-lead to different learning algorithms. They induce different trajectories, which may lead to solutions with different generalization properties.</p>

<p>(2) We need to develop a new vocabulary (and mathematics) to reason about trajectories. This goes beyond the usual “landscape view” of stationary points, gradient norms, Hessian norms, smoothness etc. Caution: trajectories depend on initialization!</p>

<p>(3): I wish I had learnt a few tricks about ODEs/PDEs/Dynamical Systems/Lagrangians in college, to be in better shape to reason about trajectories!</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/06/03/trajectories/"><span class="datestr">at June 03, 2019 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/robust_reps/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/robust_reps/">Robustness beyond Security&amp;#58; Representation Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://arxiv.org/abs/1906.00945" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Read the paper
</a>
<a style="float: right;" href="http://git.io/robust-reps" class="bbutton">
<i class="fab fa-github"></i>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/visualization.png" alt="" /></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png" alt="Standard representations are brittle" /></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1" />
	<div style="border-right: 3px white solid;" class="beer-reveal">
	    <img class="slider_img" id="selectedinv2" />
	</div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;" /></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"></div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1" /> 
	    <div style="border-right: 3px white solid;" class="beer-reveal">
		<img class="slider_img" id="man_selected2" /> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source type="video/mp4" id="int_selected">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;" /></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>







<p class="date">
<a href="http://gradientscience.org/robust_reps/"><span class="datestr">at June 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4199">NP-complete Problems and Physics: A 2019 View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr />

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center (TACC), a leading supercomputing facility in North Austin that’s part of the University of Texas, is seeking to hire a Research Scientist focused on quantum computing.  Such a person would be a full participant in our <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin, with plenty of opportunities for collaboration.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting!</a></p>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4199"><span class="datestr">at June 02, 2019 01:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/082">TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/082"><span class="datestr">at June 02, 2019 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/081">TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/081"><span class="datestr">at June 02, 2019 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/080">TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/080"><span class="datestr">at June 01, 2019 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/079">TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/079"><span class="datestr">at June 01, 2019 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/078">TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/078"><span class="datestr">at June 01, 2019 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
