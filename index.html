<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.minimizingregret.com/" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at January 28, 2019 08:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-6555947.post-8169216749697822290">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/suresh.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://feedproxy.google.com/~r/TheGeomblog/~3/TROyy-Os39k/fat-blogging.html">FAT* blogging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I'll be blogging about each session of papers from the FAT* Conference. So as not to clutter your feed, the posts will be housed at the fairness blog that I co-write along with Sorelle Friedler and Carlos Scheidegger.<br /><br />The first post is on <a href="https://algorithmicfairness.wordpress.com/2019/01/27/fat-papers-framing-and-abstraction/">Session 1: Framing and Abstraction</a>.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=TROyy-Os39k:gYq646_XYaU:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=TROyy-Os39k:gYq646_XYaU:63t7Ie-LG7Y"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=63t7Ie-LG7Y" border="0" /></a>
</div><img src="http://feeds.feedburner.com/~r/TheGeomblog/~4/TROyy-Os39k" alt="" width="1" height="1" /></div>







<p class="date">
by Suresh Venkatasubramanian (noreply@blogger.com) <a href="http://feedproxy.google.com/~r/TheGeomblog/~3/TROyy-Os39k/fat-blogging.html"><span class="datestr">at January 28, 2019 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=600">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/">Selling your town to the marijuana industry</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;">I vowed to quit with marijuana, but I just can’t.  It’s addictive.</p>
<p style="text-align: justify;">We can go back to 2016, when voters were hit with legalese that can only be described as a trap.  Basically, under the mask of legalizing the consumption of marijuana, the ballot question was really about opening recreational pot shops around the corner.  No doubt many, many people voted for legalization without knowledge of this and with no desire to have pot shops in their town.  What exultation must have come from the lawyers working for the industry, when their masterstroke made it to the fine print:</p>
<h3 style="text-align: justify;">A town voting to legalize marijuana <del>may</del> MUST open pot shops.</h3>
<p style="text-align: justify;">At the same time, the administration of Newton changed.  Councilors who liked the place the way it is and wanted to protect it lost to others who wanted it more vibrant.  The new councilors and the new mayor sided with the marijuana industry.</p>
<p style="text-align: justify;">The way in which they eventually won is sinister.  The context was that everybody in Newton wants at least some restriction on the number of marijuana stores.  But don’t take my word for this claim: even the pro-pot councilors believe so, and in fact almost unanimously they put a question on the ballot about restricting the number of stores.  At the same time, many people in Newton wanted zero stores.  In another masterstroke of the saga, the councilors were able to put one group against the other.  They added another question about having zero stores, following a massive, grassroots petition which however should have put the question at a different time. Then they forced the people who wanted zero stores to vote against restricting the number of stores. This is genius.  Also, if it isn’t illegal I believe it should be.  And in perfect coup style, media outlets censored several pieces explaining the situation to the voters. The end result was what the administration had always wanted: no restriction on the number of stores. Ignore the alarms of the doctors, the police officers, and the people.  What do they know about what’s best for Newton? A joint is like a pint of beer!  Except beer does not give you permanent brain damage. Whatever, the bottom line is that the revenue will do good things for the city! Oh yes, the revenue.  Newton has 1 billion dollars in deficit.  You read well, 1 billion.  For decades we will have a fraction of the city budget wiped out to repay that. I guess they can say we are so desperately in debt that we should rake in every penny we can zone in town.  But I think a more accurate perspective is that even in their wildest dreams, cannabis sales won’t make a dent in that.  And maybe they should spend a couple of minutes thinking about the dozens of other ways we can bring money to the city without bringing the drugs.</p>
<p style="text-align: justify;">Executing their sophisticated plan cost in the neighborhood of $100k, mostly spent on a political strategy group which helped win the election.  To add insult to injury, key members of this marijuana combine, including the political strategists and those who funded them, don’t live in Newton but in towns where recreational pot stores are banned.  The marijuana combine is effectively carving out suburban Boston in areas where it’s good to live and areas where it’s good to sell pot.</p>
<p style="text-align: justify;">As is well known, nobody has any problem with legalizing marijuana consumption.  Moreover, there is absolutely no problem with buying this stuff over the internet, or stocking up at out-of-the-way stores.  Well, absolutely no problem except one.  The money wouldn’t go into the pockets of X, Y, and Z.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/"><span class="datestr">at January 28, 2019 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09017">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09017">Finding a Mediocre Player</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dumitrescu:Adrian.html">Adrian Dumitrescu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09017">PDF</a><br /><b>Abstract: </b>Consider a totally ordered set $S$ of $n$ elements; as an example, a set of
tennis players and their rankings. Further assume that their ranking is a total
order and thus satisfies transitivity and anti-symmetry.Following Frances Yao
(1974), an element (player) is said to be $(i,j)$-\emph{mediocre} if it is
neither among the top $i$ nor among the bottom $j$ elements of $S$. More than
$40$ years ago, Yao suggested a stunningly simple algorithm for finding an
$(i,j)$-mediocre element: Pick $i+j+1$ elements arbitrarily and select the
$(i+1)$-th largest among them. She also asked: "Is this the best algorithm?" No
one seems to have found such an algorithm ever since.
</p>
<p>We first provide a deterministic algorithm that beats the worst-case
comparison bound in Yao's algorithm for a large range of values of $i$ (and
corresponding suitable $j=j(i)$). We then repeat the exercise for randomized
algorithms; the average number of comparisons of our algorithm beats the
average comparison bound in Yao's algorithm for another large range of values
of $i$ (and corresponding suitable $j=j(i)$); the improvement is most notable
in the symmetric case $i=j$. Moreover, the tight bound obtained in the analysis
of Yao's algorithm allows us to give a definite answer for this class of
algorithms. In summary, we answer Yao's question as follows: (i) "Presently
not" for deterministic algorithms and (ii) "Definitely not" for randomized
algorithms. (In fairness, it should be said however that Yao posed the question
in the context of deterministic algorithms.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09017"><span class="datestr">at January 28, 2019 02:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.09007">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.09007">The conjugate gradient algorithm on well-conditioned Wishart matrices is almost deteriministic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deift:Percy.html">Percy Deift</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trogdon:Thomas.html">Thomas Trogdon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.09007">PDF</a><br /><b>Abstract: </b>We prove that the number of iterations required to solve a random positive
definite linear system with the conjugate gradient algorithm is almost
deterministic for large matrices. We treat the case of Wishart matrices.
Precisely, we prove that for most choices of error tolerance, as the matrix
increases in size, the probability that the iteration count deviates from an
explicit deterministic value tends to zero. In addition, for a fixed iteration,
we show that the norm of the error vector and the norm of the residual converge
exponentially fast in probability, converge in mean, converge almost surely
and, after centering and rescaling, converge in distribution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.09007"><span class="datestr">at January 28, 2019 02:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08836">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08836">A Laplacian Approach to $\ell_1$-Norm Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonifaci:Vincenzo.html">Vincenzo Bonifaci</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08836">PDF</a><br /><b>Abstract: </b>We propose a novel differentiable reformulation of the linearly-constrained
$\ell_1$ minimization problem, also known as the basis pursuit problem. The
reformulation is inspired by the Laplacian paradigm of network theory and leads
to a new family of gradient-based, matrix-free methods for the solution of
$\ell_1$ minimization problems. We analyze the iteration complexity of a
natural solution approach to the reformulation, based on a multiplicative
weights update scheme, as well as the iteration complexity of an accelerated
gradient scheme. The accelerated method, in particular, yields an improved
worst-case bound on the complexity of matrix-free methods of basis pursuit.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08836"><span class="datestr">at January 28, 2019 02:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08805">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08805">Metric Spaces with Expensive Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kerber:Michael.html">Michael Kerber</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nigmetov:Arnur.html">Arnur Nigmetov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08805">PDF</a><br /><b>Abstract: </b>In algorithms for finite metric spaces, it is common to assume that the
distance between two points can be computed in constant time, and complexity
bounds are expressed only in terms of the number of points of the metric space.
We introduce a different model where we assume that the computation of a single
distance is an expensive operation and consequently, the goal is to minimize
the number of such distance queries. This model is motivated by metric spaces
that appear in the context of topological data analysis.
</p>
<p>We consider two standard operations on metric spaces, namely the construction
of a $1+\varepsilon$-spanner and the computation of an approximate nearest
neighbor for a given query point. In both cases, we partially explore the
metric space through distance queries and infer lower and upper bounds for yet
unexplored distances through triangle inequality. For spanners, we evaluate
several exploration strategies through extensive experimental evaluation. For
approximate nearest neighbors, we prove that our strategy returns an
approximate nearest neighbor after a logarithmic number of distance queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08805"><span class="datestr">at January 28, 2019 02:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08711">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08711">Optimal Bribery in Voting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08711">PDF</a><br /><b>Abstract: </b>Studying complexity of various bribery problems has been one of the main
research focus in computational social choice. In all the models of bribery
studied so far, the briber has to pay every voter some amount of money
depending on what the briber wants the voter to report and the briber has some
budget at her disposal. Although these models successfully capture many real
world applications, in many other scenarios, the voters may be unwilling to
deviate too much from their true preferences. In this paper, we study the
computational complexity of the problem of finding a preference profile which
is as close to the true preference profile as possible and still achieves the
briber's goal subject to budget constraints. We call this problem Optimal
Bribery. We consider three important measures of distances, namely, swap
distance, footrule distance, and maximum displacement distance, and resolve the
complexity of the optimal bribery problem for many common voting rules. We show
that the problem is polynomial time solvable for the plurality and veto voting
rules for all the three measures of distance. On the other hand, we prove that
the problem is NP-complete for a class of scoring rules which includes the
Borda voting rule, maximin, Copeland$^\alpha$ for any $\alpha\in[0,1]$, and
Bucklin voting rules for all the three measures of distance even when the
distance allowed per voter is $1$ for the swap and maximum displacement
distances and $2$ for the footrule distance even without the budget constraints
(which corresponds to having an infinite budget). For the $k$-approval voting
rule for any constant $k&gt;1$ and the simplified Bucklin voting rule, we show
that the problem is NP-complete for the swap distance even when the distance
allowed is $2$ and for the footrule distance even when the distance allowed is
$4$ even without the budget constraints.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08711"><span class="datestr">at January 28, 2019 02:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08708">Almost Boltzmann Exploration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Harsh.html">Harsh Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kong:Seo_Taek.html">Seo Taek Kong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srikant:R=.html">R. Srikant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Weina.html">Weina Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08708">PDF</a><br /><b>Abstract: </b>Boltzmann exploration is widely used in reinforcement learning to provide a
trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et
al., 2017) it has been shown that pure Boltzmann exploration does not perform
well from a regret perspective, even in the simplest setting of stochastic
multi-armed bandit (MAB) problems. In this paper, we show that a simple
modification to Boltzmann exploration, motivated by a variation of the standard
doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB
problem with $K$ arms, where $\alpha&gt;0$ is a parameter of the algorithm. This
improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm
inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also
show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in
stochastic MAB problems with graph-structured feedback, without knowledge of
the graph structure, where $\beta(G)$ is the independence number of the
feedback graph. Additionally, we present extensive experimental results on real
datasets and applications for multi-armed bandits with both traditional bandit
feedback and graph-structured feedback. In all cases, our algorithm performs as
well or better than the state-of-the-art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08708"><span class="datestr">at January 28, 2019 02:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08686">On the Complexity of Approximating Wasserstein Barycenter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kroshnin:Alexey.html">Alexey Kroshnin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dvinskikh:Darina.html">Darina Dvinskikh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dvurechensky:Pavel.html">Pavel Dvurechensky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gasnikov:Alexander.html">Alexander Gasnikov</a>, Nazarii Tupitsa, Cesar Uribe <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08686">PDF</a><br /><b>Abstract: </b>We study the complexity of approximating Wassertein barycenter of $m$
discrete measures, or histograms of size $n$ by contrasting two alternative
approaches, both using entropic regularization. The first approach is based on
the Iterative Bregman Projections (IBP) algorithm for which our novel analysis
gives a complexity bound proportional to $\frac{mn^2}{\varepsilon^2}$ to
approximate the original non-regularized barycenter.
</p>
<p>Using an alternative accelerated-gradient-descent-based approach, we obtain a
complexity proportional to $\frac{mn^{2.5}}{\varepsilon} $. As a byproduct, we
show that the regularization parameter in both approaches has to be
proportional to $\varepsilon$, which causes instability of both algorithms when
the desired accuracy is high. To overcome this issue, we propose a novel
proximal-IBP algorithm, which can be seen as a proximal gradient method, which
uses IBP on each iteration to make a proximal step. We also consider the
question of scalability of these algorithms using approaches from distributed
optimization and show that the first algorithm can be implemented in a
centralized distributed setting (master/slave), while the second one is
amenable to a more general decentralized distributed setting with an arbitrary
network topology.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08686"><span class="datestr">at January 28, 2019 02:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08668">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08668">Guarantees for Spectral Clustering with Fairness Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleindessner:Matth=auml=us.html">Matthäus Kleindessner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samadi:Samira.html">Samira Samadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morgenstern:Jamie.html">Jamie Morgenstern</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08668">PDF</a><br /><b>Abstract: </b>Given the widespread popularity of spectral clustering (SC) for partitioning
graph data, we study a version of constrained SC in which we try to incorporate
the fairness notion proposed by Chierichetti et al. (2017). According to this
notion, a clustering is fair if every demographic group is approximately
proportionally represented in each cluster. To this end, we develop variants of
both normalized and unnormalized constrained SC and show that they help find
fairer clusterings on both synthetic and real data. We also provide a rigorous
theoretical analysis of our algorithms. While there have been efforts to
incorporate various constraints into the SC framework, theoretically analyzing
them is a challenging problem. We overcome this by proposing a natural variant
of the stochastic block model where h groups have strong inter-group
connectivity, but also exhibit a "natural" clustering structure which is fair.
We prove that our algorithms can recover this fair clustering with high
probability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08668"><span class="datestr">at January 28, 2019 02:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08639">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08639">Dolha - an Efficient and Exact Data Structure for Streaming Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fan.html">Fan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Lei.html">Lei Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeng:Li.html">Li Zeng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gou:Xiangyang.html">Xiangyang Gou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08639">PDF</a><br /><b>Abstract: </b>A streaming graph is a graph formed by a sequence of incoming edges with time
stamps. Unlike static graphs, the streaming graph is highly dynamic and time
related. In the real world, the high volume and velocity streaming graphs such
as internet traffic data, social network communication data and financial
transfer data are bringing challenges to the classic graph data structures. We
present a new data structure: double orthogonal list in hash table (Dolha)
which is a high speed and high memory efficiency graph structure applicable to
streaming graph. Dolha has constant time cost for single edge and near linear
space cost that we can contain billions of edges information in memory size and
process an incoming edge in nanoseconds. Dolha also has linear time cost for
neighborhood queries, which allow it to support most algorithms in graphs
without extra cost. We also present a persistent structure based on Dolha that
has the ability to handle the sliding window update and time related queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08639"><span class="datestr">at January 28, 2019 02:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08628">Fair k-Center Clustering for Data Summarization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleindessner:Matth=auml=us.html">Matthäus Kleindessner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morgenstern:Jamie.html">Jamie Morgenstern</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08628">PDF</a><br /><b>Abstract: </b>In data summarization we want to choose k prototypes in order to summarize a
data set. We study a setting where the data set comprises several demographic
groups and we are restricted to choose k_i prototypes belonging to group i. A
common approach to the problem without the fairness constraint is to optimize a
centroid-based clustering objective such as k-center. A natural extension then
is to incorporate the fairness constraint into the clustering objective.
Existing algorithms for doing so run in time super-quadratic in the size of the
data set. This is in contrast to the standard k-center objective that can be
approximately optimized in linear time. In this paper, we resolve this gap by
providing a simple approximation algorithm for the k-center problem under the
fairness constraint with running time linear in the size of the data set and k.
If the number of demographic groups is small, the approximation guarantee of
our algorithm only incurs a constant-factor overhead. We demonstrate the
applicability of our algorithm on both synthetic and real data sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08628"><span class="datestr">at January 28, 2019 02:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/012">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/012">TR19-012 |  Multi-pseudodeterministic algorithms | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, dedicated to Shafi Goldwasser, we consider a relaxation of the notion of pseudodeterministic algorithms, which was put forward by Gat and Goldwasser ({\em ECCC}, TR11--136, 2011). 


Pseudodeterministic algorithms are randomized algorithms that solve search problems by almost always providing the same canonical solution (per each input). 
Multi-pseudodeterministic algorithms relax the former notion by allowing the algorithms to output one of a bounded number  of canonical solutions (per each input). 
We show that efficient multi-seudodeterministic algorithms can solve natural problems that are not solveable by efficient pseudodeterministic algorithms, present a composition theorem regarding multi-pseudodeterministic algorithms,
and relate them to other known notions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/012"><span class="datestr">at January 27, 2019 05:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/011">TR19-011 |  Sampling Graphs without Forbidden Subgraphs and Almost-Explicit Unbalanced Expanders | 

	Benny Applebaum, 

	Eliran Kachlon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the study of the following hypergraph sampling problem: Sample a $d$-uniform hypergraph over $n$ vertices and $m$ hyperedges from some pseudorandom distribution $\mathcal{G}$ conditioned on not having some small predefined $t$-size hypergraph $H$ as a subgraph. The algorithm should run in $\mathrm{poly}(n)$-time even when the size of the subgraph $H$ is super-constant.

We solve the problem by carefully designing a sampling algorithm for $k$-wise independent hypergraphs $\mathcal{G}$ that supports efficient testing for subgraph-freeness. We use our algorithm to obtain the first probabilistic construction of constant-degree polynomially-unbalanced expander graphs whose failure probability is negligible in $n$ (i.e., $n^{-\omega(1)}$). In particular, given constants $d&gt;c$, we output a bipartite graph that has $n$ left nodes, $n^c$ right nodes with right-degree of $d$ so that any right set of size at most $n^{\Omega(1)}$ expands by factor of $\Omega(d)$. This result is extended to the setting of unique expansion as well.

We argue that such an ``almost-explicit'' construction can be employed in many useful settings, and present applications in coding theory (batch codes and LDPC codes), pseudorandomness (low-bias generators and randomness extractors) and cryptography. Notably, we show that our constructions yield a collection of polynomial-stretch locally-computable cryptographic pseudorandom generators based on Goldreich's one-wayness assumption resolving a long-standing open problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/011"><span class="datestr">at January 27, 2019 05:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/010">TR19-010 |   Stoquastic PCP vs. Randomness | 

	Alex Bredariol Grilo, 

	Dorit Aharonov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The derandomization of MA, the probabilistic version of NP, is a long standing open question. In this work, we connect this problem to a variant of another major problem: the quantum PCP conjecture. Our connection goes through the surprising quantum characterization of MA by Bravyi and Terhal. They proved the MA-completeness of the problem of deciding whether the groundenergy of a uniform stoquastic local Hamiltonian is zero or inverse polynomial. We show that the gapped version of this problem, i.e. deciding if a given uniform stoquastic local Hamiltonian is frustration-free or has energy at least some constant $\varepsilon$, is in NP. Thus, if there exists a gap-amplification procedure for uniform stoquastic Local Hamiltonians (in analogy to the gap amplification procedure for constraint satisfaction problems in the original PCP theorem), then MA = NP (and vice versa). Furthermore, if this gap amplification procedure exhibits some additional (natural) properties, then P = RP. We feel this work opens up a rich set of new directions to explore, which might lead to progress on both quantum PCP and derandomization. As a small side result, we also show that deciding if commuting stoquastic Hamiltonian is frustration free is in NP.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/010"><span class="datestr">at January 27, 2019 12:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/009">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/009">TR19-009 |  The Fine-Grained Complexity of Strengthenings of First-Order Logic | 

	Jiawei Gao, 

	Russell Impagliazzo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The class of model checking for first-order formulas on sparse graphs has a complete problem with respect to fine-grained reductions, Orthogonal Vectors (OV) [GIKW17]. This paper studies extensions of this class or more lenient parameterizations. We consider classes obtained by allowing function symbols;
first-order on ordered structures; adding various notions of transitive closure operations; and stratifications of first-order properties by quantifier depth and variable complexity, rather than number of quantifiers. For some of these classes, OV is still a complete problem, in that significant improvement for the entire class is equivalent to significant improvement for OV algorithms.  For these classes, we can also use the improved OV algorithm of [AWY16, CW16] to get moderate improvements on algorithms for the entire class. For other classes, we show that model checking becomes harder than for first-order, under well-studied conjectures such as SETH.  For other classes, we show hardness follows from weaker assumptions than SETH. 

Surprisingly, whether an extension increases the complexity of model checking seems independent of whether it increases the expressive power of the logic. For example, adding function symbols does not change which problems are expressible by first-order, but does increase the time for model checking under SETH. On the other hand, adding an ordering does not change the fine-grained complexity
of model checking, although it increases the logic's expressive power.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/009"><span class="datestr">at January 27, 2019 12:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08575">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08575">Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot Compute</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Durand=Lose:J=eacute=r=ocirc=me.html">Jérôme Durand-Lose</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoogeboom:Hendrik_Jan.html">Hendrik Jan Hoogeboom</a>, Nataša Jonoska <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08575">PDF</a><br /><b>Abstract: </b>We consider non cooperative binding in so called `temperature 1', in
deterministic (here called {\it confluent}) tile self-assembly systems (1-TAS)
and prove the standing conjecture that such systems do not have universal
computational power. We call a TAS whose maximal assemblies contain at least
one ultimately periodic assembly path {\it para-periodic}. We observe that a
confluent 1-TAS has at most one maximal producible assembly, $\alpha_{max}$,
that can be considered a union of path assemblies, and we show that such a
system is always para-periodic. This result is obtained through a superposition
and a combination of two paths that produce a new path with desired properties,
a technique that we call \emph{co-grow} of two paths. Moreover we provide a
characterization of an $\alpha_{max}$ of a confluent 1-TAS as one of two
possible cases, so called, a grid or a disjoint union of combs. To a given
$\alpha_{max}$ we can associate a finite labeled graph, called \emph{quipu},
such that the union of all labels of paths in the quipu equals $\alpha_{max}$,
therefore giving a finite description for $\alpha_{max}$. This finite
description implies that $\alpha_{max}$ is a union of semi-affine subsets of
$\mathbb{Z}^2$ and since such a finite description can be algorithmicly
generated from any 1-TAS, 1-TAS cannot have universal computational power.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08575"><span class="datestr">at January 27, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08564">Infinite All-Layers Simple Foldability</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akitaya:Hugo_A=.html">Hugo A. Akitaya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avery:Cordelia.html">Cordelia Avery</a>, Joseph Bergeron, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopinsky:Justin.html">Justin Kopinsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ku:Jason.html">Jason Ku</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08564">PDF</a><br /><b>Abstract: </b>We study the problem of deciding whether a crease pattern can be folded by
simple folds (folding along one line at a time) under the infinite all-layers
model introduced by [Akitaya et al., 2017], in which each simple fold is
defined by an infinite line and must fold all layers of paper that intersect
this line. This model is motivated by folding in manufacturing such as
sheet-metal bending. We improve on [Arkin et al., 2004] by giving a
deterministic $O(n)$-time algorithm to decide simple foldability of 1D crease
patterns in the all-layers model. Then we extend this 1D result to 2D, showing
that simple foldability in this model can be decided in linear time for
unassigned axis-aligned orthogonal crease patterns on axis-aligned 2D
orthogonal paper. On the other hand, we show that simple foldability is
strongly NP-complete if a subset of the creases have a mountain-valley
assignment, even for an axis-aligned rectangle of paper.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08564"><span class="datestr">at January 27, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08544">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08544">Learning Sublinear-Time Indexing for Nearest Neighbor Search</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Yihe.html">Yihe Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, Ilya Razenshteyn, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Tal.html">Tal Wagner</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08544">PDF</a><br /><b>Abstract: </b>Most of the efficient sublinear-time indexing algorithms for the
high-dimensional nearest neighbor search problem (NNS) are based on space
partitions of the ambient space $\mathbb{R}^d$. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing
such partitions that reduces the problem to balanced graph partitioning
followed by supervised classification. We instantiate this general approach
with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural
networks, respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions found by Neural LSH consistently
outperform partitions found by quantization- and tree-based methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08544"><span class="datestr">at January 27, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08525">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08525">Solving linear program with Chubanov queries and bisection moves</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan=Hon=Tong:Adrien.html">Adrien Chan-Hon-Tong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08525">PDF</a><br /><b>Abstract: </b>This short article focus on the link between linear feasibility and generic
linear program. An algorithm is presented to solve generic linear program using
linear feasibility queries and working at constraint level instead of raw
values level. Even if the number of required linear feasibility queries is not
established, this algorithm may be especially interesting, since, thank to
Chubanov algorithm, there is a strongly polynomial time algorithm to solve
linear feasibility problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08525"><span class="datestr">at January 27, 2019 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08419">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08419">Spherical sampling methods for the calculation of metamer mismatch volumes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mackiewicz:Michal.html">Michal Mackiewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rivertz:Hans_Jakob.html">Hans Jakob Rivertz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Finlayson:Graham_D=.html">Graham D. Finlayson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08419">PDF</a><br /><b>Abstract: </b>In this paper, we propose two methods of calculating theoretically maximal
metamer mismatch volumes. Unlike prior art techniques, our methods do not make
any assumptions on the shape of spectra on the boundary of the mismatch
volumes. Both methods utilize a spherical sampling approach, but they calculate
mismatch volumes in two different ways. The first method uses a linear
programming optimization, while the second is a computational geometry approach
based on half-space intersection. We show that under certain conditions the
theoretically maximal metamer mismatch volume is significantly larger than the
one approximated using a prior art method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08419"><span class="datestr">at January 27, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08246">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08246">Reachability Problem in Non-uniform Cellular Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adak:Sumit.html">Sumit Adak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mukherjee:Sukanya.html">Sukanya Mukherjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Sukanta.html">Sukanta Das</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08246">PDF</a><br /><b>Abstract: </b>This paper deals with the CREP (Configuration REachability Problem) for
non-uniform cellular automata (CAs). The cells of non-uniform CAs, we have
considered here, can use different Wolfram's rules to generate their next
states. We report an algorithm which decides whether or not a configuration of
a given (non-uniform) cellular automaton is reachable from another
configuration. A characterization tool, named Reachability tree, is used to
develop theories and the decision algorithm for the CREP. Though the worst case
complexity of the algorithm is exponential in time and space, but the average
performance is very good.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08246"><span class="datestr">at January 27, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08235">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08235">Multi-Frequency Phase Synchronization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Tingran.html">Tingran Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Zhizhen.html">Zhizhen Zhao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08235">PDF</a><br /><b>Abstract: </b>We propose a novel formulation for phase synchronization -- the statistical
problem of jointly estimating alignment angles from noisy pairwise comparisons
-- as a nonconvex optimization problem that enforces consistency among the
pairwise comparisons in multiple frequency channels. Inspired by harmonic
retrieval in signal processing, we develop a simple yet efficient two-stage
algorithm that leverages the multi-frequency information. We demonstrate in
theory and practice that the proposed algorithm significantly outperforms
state-of-the-art phase synchronization algorithms, at a mild computational
costs incurred by using the extra frequency channels. We also extend our
algorithmic framework to general synchronization problems over compact Lie
groups.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08235"><span class="datestr">at January 27, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08219">Greedy Strategy Works for Clustering with Outliers and Coresets Construction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08219">PDF</a><br /><b>Abstract: </b>We study the problems of clustering with outliers in high dimension. Though a
number of methods have been developed in the past decades, it is still quite
challenging to design quality guaranteed algorithms with low complexities for
the problems. Our idea is inspired by the greedy method, Gonzalez's algorithm,
for solving the problem of ordinary $k$-center clustering. Based on some novel
observations, we show that this greedy strategy actually can handle
$k$-center/median/means clustering with outliers efficiently, in terms of
qualities and complexities. We further show that the greedy approach yields
small coreset for the problem in doubling metrics, so as to reduce the time
complexity significantly. Moreover, a by-product is that the coreset
construction can be applied to speedup the popular density-based clustering
approach DBSCAN.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08219"><span class="datestr">at January 27, 2019 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.08210">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.08210">Pseudo-Polynomial Time Algorithm for Computing Moments of Polynomials in Free Semicircular Elements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Rei Mizuta <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08210">PDF</a><br /><b>Abstract: </b>We consider about calculating $M$th moments of a given polynomial in free
independent semicircular elements in free probability theory. By a naive
approach, this calculation requires exponential time with respect to $M$. We
explicitly give an algorithm for calculating them in polynomial time by
rearranging Sch\"utzenberger's algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.08210"><span class="datestr">at January 27, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-2559156841800187715">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html">Algorithmic Unfairness Without Any Bias Baked In</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Discussion of (un)fairness in machine learning hit mainstream political discourse this week, when Representative Alexandria Ocasio-Cortez discussed the possibility of algorithmic bias, and was clumsily "called out" by Ryan Saavedra on twitter: <br /><center><blockquote class="twitter-tweet"><div lang="en" dir="ltr">Socialist Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist <a href="https://t.co/X2veVvAU1H">pic.twitter.com/X2veVvAU1H</a>— Ryan Saavedra (@RealSaavedra) <a href="https://twitter.com/RealSaavedra/status/1087627739861897216?ref_src=twsrc%5Etfw">January 22, 2019</a></div></blockquote></center>It was gratifying to see the number of responses pointing out how wrong he was --- awareness of algorithmic bias has clearly become pervasive! But most of the pushback focused the possibility being "baked in" by the designer of the algorithm, or because of latent bias embedded in the data, or both:  <br /><center><blockquote class="twitter-tweet"><div lang="en" dir="ltr">You know algorithms are written by people right? And that the data they are trained on is made and selected by people? And that the problems algorithms solve are decided...again...by people? And that people can be and many times are racist? Ok now you do the math</div>— kade (@onekade) <a href="https://twitter.com/onekade/status/1087853353000939521?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote></center>Bias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is "unfair".   Here is the (toy) scenario -- the specifics aren't important. High school students are applying to college, and each student has some innate "talent" $I$, which we will imagine is normally distributed, with mean 100 and standard deviation 15: $I \sim N(100,15)$. The college would like to admit students who are sufficiently talented --- say one standard deviation above the mean (so, it would like to admit students with $I \geq 115$). The problem is that talent isn't directly observable. Instead, the college can observe <i>grades</i> $g$ and <i>SAT scores $s$</i>, which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student's talent level, and also with standard deviation 15: $g \sim N(I, 15)$, $s \sim N(I, 15)$.<br /><br />In this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose <i>predicted </i>talent is at least 115. This is indeed "driven by math" --- since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college.<br /><br />Ok. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population --- the Blues's only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above.<br /><br />But there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds. Here are the grades and exam scores for the two populations, plotted:<br /><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s1600/gradesexams.png"><img width="400" src="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s400/gradesexams.png" border="0" height="261" /></a></div><div style="clear: both; text-align: center;" class="separator"></div>So what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don't see much difference:<br /><br />The Red classifier makes errors approximately 11% of the time. The Blue classifier does about the same --- it makes errors about 10.4% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate!<br /><br />And since we are interested in fairness, lets think about the <i>false negative rate</i> of our classifiers. "False Negatives" in this setting are the people who are qualified to attend the college ($I &gt; 115$), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier's mistakes. And the False Negative <i>Rate</i> is the probability that a randomly selected qualified person is mistakenly rejected from college --- i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier's mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness --- <a href="http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning">sometimes referred to as "equal opportunity."</a><br /><br />So how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 50% on the blues, and the Red model has a false negative rate of 47% on the reds --- so the difference between these two is a satisfyingly small 3%.<br /><br />But you might reasonably object: because we have learned separate models for the Blues and the Reds, we are <i>explicitly </i>making admissions decisions as a function of a student's color! This might sound like a form of discrimination, baked in by the algorithm designer --- and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending.<br /><br />So what happens if we don't allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.5%, and the overall error rate ticks up. This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population.<br /><br />What happened? There wasn't any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population --- which contributed much more to the <i>average</i>. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them --- all in their favor.<br /><br /><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto; text-align: center;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s1600/classifier.png"><img width="400" src="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s400/classifier.png" border="0" height="262" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">The combined admissions rule takes everyone above the black line. Since the Blues are shifted up relative to the Reds, they are admitted at a disproportionately higher rate. </td></tr></tbody></table><div style="clear: both; text-align: center;" class="separator"></div><br /><br />This is the kind of thing that happens <i>all the time</i>: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, <i>it is exacerbated if we artificially force the algorithm to be group blind</i>. Well intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time.<br /><br /><br /><br /><br /></div>







<p class="date">
by Aaron Roth (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html"><span class="datestr">at January 26, 2019 10:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/">Introduction to AMP and the Replica Trick</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(This post from the lecture by <span class="qu"><span class="gD">Yueqi </span></span><span class="qu"><span class="gD">Sheng)</span></span></em></p>
<p>In this post, we will talk about detecting phase transitions using<br />
Approximate-Message-Passing (AMP), which is an extension of<br />
Belief-Propagation to “dense” models. We will also discuss the Replica<br />
Symmetric trick, which is a heuristic method of analyzing phase<br />
transitions. We focus on the Rademacher spiked Wigner model (defined<br />
below), and show how both these methods yield the same phrase transition<br />
in this setting.</p>
<p>The Rademacher spiked Wigner model (RSW) is the following. We are given<br />
observations <img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" title="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> (sampled uniformly) is the true signal and <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> is a<br />
Gaussian-Orthogonal-Ensemble (GOE) matrix:<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, j} \sim \mathbb{N}(0, 1)" class="latex" title="W_{i, j} \sim \mathbb{N}(0, 1)" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \neq j" class="latex" title="i \neq j" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, i} \sim \mathbb{N}(0, 2)" class="latex" title="W_{i, i} \sim \mathbb{N}(0, 2)" />. Here <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> is the signal to noise<br />
ratio. The goal is to approximately recover <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />.</p>
<p>The question here is: how small can <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> be such that it is<br />
impossible to recover anything reasonably correlated with the<br />
ground-truth <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />? And what do the approximate-message-passing algorithm<br />
(or the replica method) have to say about this?</p>
<p>To answer the first question, one can think of the task here is to<br />
distinguish <img src="https://s0.wp.com/latex.php?latex=Y+%5Csim+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" title="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" /> vs<br />
<img src="https://s0.wp.com/latex.php?latex=Y+%5Csim+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \sim W" class="latex" title="Y \sim W" />. One approach to distinguishing these distributions is to<br />
look at the spectrum of the observation matrix <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" />. (In fact, it turns<br />
out that this is an asymptotically optimal distinguisher [1]). The spectrum of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> behaves as ([2]):</p>
<ul>
<li>When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda \leq 1" class="latex" title="\lambda \leq 1" />, the empirical distribution of eigenvalues in<br />
spiked model still follows the semicircle law, with the top<br />
eigenvalues <img src="https://s0.wp.com/latex.php?latex=%5Capprox+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\approx 2" class="latex" title="\approx 2" /><p></p>
</li>
<li>
<p>When <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &gt; 1" class="latex" title="\lambda &gt; 1" />, we start to see an eigenvalue <img src="https://s0.wp.com/latex.php?latex=%3E+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="&gt; 2" class="latex" title="&gt; 2" /> in the<br />
planted model.</p>
</li>
</ul>
<h1>Approximate message passing</h1>
<p>This section approximately follows the exposition in [3].</p>
<p>First, note that in the Rademacher spiked Wigner model, the posterior<br />
distribution of the signal <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" /> conditioned on the observation <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><br />
is: <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D+%5Cpropto+%5CPr%5BY+%7C+%5Csigma%5D+%5Cpropto+%5Cprod_%7Bi+%5Cneq+j%7D+%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i+%5Csigma_j+%2F2+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" class="latex" title="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" /> This<br />
defines a graphical-model (or “factor-graph”), over which we can perform<br />
Belief-Propogation to infer the posterior distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" />.<br />
However, in this case the factor-graph is dense (the distribution is a<br />
product of potentials <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i%5Csigma_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" class="latex" title="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" /> for all<br />
pairs of <img src="https://s0.wp.com/latex.php?latex=i%2C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i, j" class="latex" title="i, j" />).</p>
<p>In the previous <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">blog post</a>, we saw belief propagation works great when the underlying interaction<br />
graph is sparse. Intuitively, this is because <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is locally tree like,<br />
which allows us to assume each messages are independent random<br />
variables. In dense model, this no longer holds. One can think of dense<br />
model as each node receive a weak signal from all its neighbors.</p>
<p>In the dense model setting, a class of algorithms called Approximate<br />
message passing (AMP) is proposed as an alternative of BP. We will<br />
define AMP for RWM in terms of its state evolution.</p>
<h2>State evolution of AMP for Rademacher spiked Wigner model</h2>
<p>Recall that in BP, we wish to infer the posterior distributon of<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" />, and the messages we pass between nodes correspond to marginal<br />
probability distribution over values on nodes. In our setting, since the<br />
distributions are over <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}" class="latex" title="\{\pm 1\}" />, we can represent distributions by<br />
their expected values. Let <img src="https://s0.wp.com/latex.php?latex=m%5Et_%7Bu+%5Cto+v%7D+%5Cin+%5B-1%2C+1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t_{u \to v} \in [-1, 1]" class="latex" title="m^t_{u \to v} \in [-1, 1]" /> denote the<br />
message from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" /> to <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />. That is, <img src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{u \to v}" class="latex" title="m_{u \to v}" /> corresponds<br />
to the expected value <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_u]" class="latex" title="{{\mathbb{E}}}[\sigma_u]" />.</p>
<p>To derive the BP update rules, we want to compute the expectation<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_v]" class="latex" title="{{\mathbb{E}}}[\sigma_v]" /> of a node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />, given the<br />
messages <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_u]" class="latex" title="{{\mathbb{E}}}[\sigma_u]" /> for <img src="https://s0.wp.com/latex.php?latex=u+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u \neq v" class="latex" title="u \neq v" />. We can<br />
do this using the posterior distribution of the RWM, <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma | Y]" class="latex" title="\Pr[\sigma | Y]" />,<br />
which we computed above.<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5B%5Csigma_v+%3D+1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D+%3D+%5Cfrac%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+-+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%2B+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" class="latex" title="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" /></p>
<p>And similarly for <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma_v+%3D+-1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" class="latex" title="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" />.<br />
From the above, we can take expectations over <img src="https://s0.wp.com/latex.php?latex=%5Csigma_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_u" class="latex" title="\sigma_u" />, and express<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[\sigma_v]" class="latex" title="{{\mathbb{E}}}[\sigma_v]" /> in terms of<br />
<img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D%5C%7D_%7Bu+%5Cneq+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" class="latex" title="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" />. Doing this (and<br />
using the heuristic assumption that the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" /> is a<br />
product distribution), we find that the BP state update can be written<br />
as:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%28%5Csum_%7Bw+%5Cneq+v%7Df%5E%7B-1%7D%28A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" class="latex" title="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" /><br />
where the interaction matrix <img src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D+%3D+%5Clambda+Y_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{w, u} = \lambda Y_{w, u}" class="latex" title="A_{w, u} = \lambda Y_{w, u}" />, and<br />
<img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+tanh%28x%29+%3D+%5Cfrac%7B%5Cexp%28x%29+-+%5Cexp%28-x%29%7D%7B%5Cexp%28x%29+%2B+%5Cexp%28x%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" class="latex" title="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" />.</p>
<p>Now, Taylor expanding <img src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f^{-1}" class="latex" title="f^{-1}" /> around <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />, we find<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%5Cleft%28+%28%5Csum_%7Bw+%5Cneq+v%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29+%2B+O%281%2F%5Csqrt%7Bn%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" class="latex" title="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" /><br />
since the terms <img src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{w, u}" class="latex" title="A_{w, u}" /> are of order <img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1/\sqrt{n})" class="latex" title="O(1/\sqrt{n})" />.</p>
<p>At this point, we could try dropping the “non-backtracking” condition<br />
<img src="https://s0.wp.com/latex.php?latex=w+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w \neq v" class="latex" title="w \neq v" /> from the above sum (since the node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> contributes at most<br />
<img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1/\sqrt{n})" class="latex" title="O(1/\sqrt{n})" /> to the sum anyway), to get the state update:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu%7D+%3D+f%5Cleft%28+%5Csum_%7Bw%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" class="latex" title="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" /> (note the messages no longer<br />
depend on receiver – so we write <img src="https://s0.wp.com/latex.php?latex=m_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_u" class="latex" title="m_u" /> in place of <img src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{u \to v}" class="latex" title="m_{u \to v}" />).<br />
However, this simplification turns out not to work for estimating the<br />
signal. The problem is that the “backtracking” terms which we added<br />
amplify over two iterations.</p>
<p>In AMP, we simply perform the above procedure, except we add a<br />
correction term to account for the backtracking issue above. Given <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" />,<br />
for all <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />, the AMP update is:<br />
<img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+m%5E%7Bt%7D_u+%3D+f%28%5Csum_%7Bw%7DA_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%2B+%5B%5Ctext%7Bsome+correction+term%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" class="latex" title="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" /></p>
<p>The correction term corresponds to error introduced by the backtracking<br />
terms. Suppose everything is good until step <img src="https://s0.wp.com/latex.php?latex=t+-+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t - 2" class="latex" title="t - 2" />. We will examine<br />
the influence of backtracking term to a node <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> through length 2 loops.<br />
At time <img src="https://s0.wp.com/latex.php?latex=t+-+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t - 1" class="latex" title="t - 1" />, <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> exert <img src="https://s0.wp.com/latex.php?latex=Y_%7Bv%2C+u%7Dm%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{v, u}m^{t - 2}_v" class="latex" title="Y_{v, u}m^{t - 2}_v" /> additional influence to<br />
each of it’s neighbor <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u" class="latex" title="u" />. At time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> receive roughly<br />
<img src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{u, v}^2m^{t - 2}_v" class="latex" title="Y_{u, v}^2m^{t - 2}_v" />. Since <img src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y_{u, v}^2" class="latex" title="Y_{u, v}^2" /> has magnitude<br />
<img src="https://s0.wp.com/latex.php?latex=%5Capprox+%5Cfrac%7B1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\approx \frac{1}{n}" class="latex" title="\approx \frac{1}{n}" /> and we need to sum over all of <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />’s neighbors,<br />
this error term is to large to ignore. To characterize the exact form of<br />
correction, we simply do a taylor expansion</p>
<p><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_v+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+1%7D_u%29+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7D+%5Cleft%28%5Csum_%7Bw%7Df%28Y_%7Bw%2C+u%7Dm%5E%7Bt+-+2%7D_w%29+-+f%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+2%7D_w%29%5Cright%29+%29%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+Y_%7Bu%2C+v%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bu%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" class="latex" title="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" /></p>
<h2>State evolution of AMP</h2>
<p>In this section we attempt to obtain the phase transition of Rademacher<br />
spiked Wigner model via looking at <img src="https://s0.wp.com/latex.php?latex=m%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{\infty}" class="latex" title="m^{\infty}" />.</p>
<p>We assume that each message could be written as a sum of signal term and<br />
noise term. <img src="https://s0.wp.com/latex.php?latex=m%5Et+%3D+%5Cmu_t+x+%2B+%5Csigma_t+g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t = \mu_t x + \sigma_t g" class="latex" title="m^t = \mu_t x + \sigma_t g" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=g+%5Csim+%5Cmathbb%7BN%7D%280%2C+I%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g \sim \mathbb{N}(0, I)" class="latex" title="g \sim \mathbb{N}(0, I)" />. To the dynamics of AMP (and find its phase<br />
transition), we need to look at how the signal <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t" class="latex" title="\mu_t" /> and noise<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t" class="latex" title="\sigma_t" /> evolves with <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />.</p>
<p>We do the following simplification: ignore the correction term and<br />
assume each time we obtain an independent noise <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g" class="latex" title="g" />.</p>
<p><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D+%3D+Yf%28m%5E%7Bt+-+1%7D%29+%3D+%28%5Cfrac%7B%5Clambda%7D%7Bn%7Dx%5ETx+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW%29f%28m%5E%7Bt+-+1%7D%29+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x+%3E+x+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D+Wf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" class="latex" title="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" /></p>
<p>Here, we see that <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" class="latex" title="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" /><br />
and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_t+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DWf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" class="latex" title="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" />.</p>
<p><em>Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{t}" class="latex" title="\mu_{t}" /> is essentially proportional to overlap between<br />
ground truth and current belief</em>, since the function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> keeps the<br />
magnitude of the current beliefs bounded.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28m%5E%7Bt+-+1%7D%29%2C+x%3E%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28%5Cmu_%7Bt+-+1%7Dx+%2B+%5Csigma_%7Bt+-+1%7Dg%29%2C+x%3E+%5Capprox%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX+%5Csim+unif%28%5Cpm+1%29%2C+G%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29%7D%5BX+f%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D+%3D+%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_G%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" class="latex" title="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" /></p>
<p>For the noise term, each coordinate of <img src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t" class="latex" title="\sigma_t" /> is a gaussian random<br />
variable with <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> mean and variance</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_v+f%28m%5E%7Bt+-+1%7D%29_v%5E2+%5Capprox+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX%2C+G%7D%5Bf%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D+%3D+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BG%7D%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" class="latex" title="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" /></p>
<p>It was shown in [4] that we can introduce a new<br />
parameter <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t" class="latex" title="\gamma_t" /> s.t.<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%3D+%5Clambda%5E2+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28%5Cgamma_%7Bt+-+1%7D+%2B+%5Csqrt%7B%5Cgamma_%7Bt+-+1%7D%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" class="latex" title="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" /><br />
As <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, turns out <img src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Cgamma_t%7D%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_t = \frac{\gamma_t}{\lambda}" class="latex" title="\mu_t = \frac{\gamma_t}{\lambda}" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csigma_t%5E2+%3D+%5Cfrac%7B%5Csigma_t%7D%7B%5Clambda%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" class="latex" title="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" />. To study the behavior of<br />
<img src="https://s0.wp.com/latex.php?latex=m%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m^t" class="latex" title="m^t" /> as <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, it is enough to track the evolution of<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t" class="latex" title="\gamma_t" />.</p>
<p>This heuristic analysis of AMP actually gives a phase transition at<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda = 1" class="latex" title="\lambda = 1" /> (in fact, the analysis of AMP can be done rigorously as in [5]):</p>
<ul>
<li>For <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &lt; 1" class="latex" title="\lambda &lt; 1" />: If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_t \approx 0" class="latex" title="\gamma_t \approx 0" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cgamma_t+%2B+%5Csqrt%7B%5Cgamma_t%7DG%7C+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" class="latex" title="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" /> w.h.p., thus we have <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bt+%2B+1%7D+%5Capprox+%5Clambda%5E2+%28%5Cgamma_t%29+%3C+%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" class="latex" title="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" />. Taking <img src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \to \infty" class="latex" title="t \to \infty" />, we have <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7B%5Cinfty%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma_{\infty} = 0" class="latex" title="\gamma_{\infty} = 0" />, which means there AMP solution has no overlap with the ground truth.<p></p>
</li>
<li>
<p>For <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda &gt; 1" class="latex" title="\lambda &gt; 1" />: In this case, AMP’s solution has some correlation with the ground truth.</p>
</li>
</ul>
<p><img src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.49.39.png?w=600" alt="screenshot 2019-01-26 13.49.39" class="alignnone size-full wp-image-7422" /></p>
<p>(Figure from [6])</p>
<h1>Replica symmetry trick</h1>
<p>Another way of obtaining the phase transition is via a non-rigorous<br />
analytic method called the replica method. Although non-rigorous, this<br />
method from statistical physics has been used to predict the fixed point<br />
of many message passing algorithms and has the advantage of being easy<br />
to simulate. In our case, we will see that we obtain the same phase<br />
transition temperature as AMP above. The method is non-rigorous due to<br />
several assumptions made during the computation.</p>
<h2>Outline of replica method</h2>
<p>Recall that we are interested in minizing the free energy of a given<br />
system <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%2C+Y%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%2C+Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" class="latex" title="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" /> where <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> is<br />
the partition function as before:<br />
<img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+exp%28-%5Cbeta+H%28Y%2C+x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" class="latex" title="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=H%28Y%2C+x%29+%3D+-%3CY%2C+x%5ETx%3E+%3D+-xYx%5ET+%3D+-%5Csum_%7Bi%2C+j%7D+Y_%7Bi%2C+j%7Dx_ix_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" class="latex" title="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" />.</p>
<p>In replica method, <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> is not fixed but a random variable. The<br />
assumption is that as <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" />, free energy doesn’t vary with <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><br />
too much, so we will look at the mean of <img src="https://s0.wp.com/latex.php?latex=f_Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_Y" class="latex" title="f_Y" /> to approximate free<br />
energy of the system.</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BY%7D%5B%5Clog+Z%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" class="latex" title="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is called the free energy density and the goal now is to<br />
compute the free energy density as a function of only <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" /> , the<br />
temperature of the system.</p>
<p>The <strong>replica method</strong> is first proposed as a simplification of the<br />
computation of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /></p>
<p>It is a generally hard problem to compute <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> in a clear way. A<br />
naive attempt of approximate <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is to simply pull the log out<br />
<img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%5Clog+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" class="latex" title="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" /><br />
Unfortunately <img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta)" class="latex" title="g(\beta)" /> and <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> are quite different quantities,<br />
at least when temperature is low. Intuitively, <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> is looking at<br />
system with a fixed <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> while in <img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta)" class="latex" title="g(\beta)" />, <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> are allowed to<br />
fluctuate together. When the temperature is high, <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> doesn’t play a big<br />
roll in system thus they could be close. However, when temperature is<br />
low, there could be a problems. Let <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to \infty" class="latex" title="\beta \to \infty" />,<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%5Capprox+%5Cint_Y+%28%5Cbeta+x_Y+Y+x_Y%29%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" class="latex" title="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" />,<br />
<img src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%5Capprox+%5Clog+%5Cint_Y+exp%28%5Cbeta+x_J+Y+x_Y%29%5Cmu%28Y%29dY+%5Capprox+%5Cbeta+x%5E%2A+Yx%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" class="latex" title="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" />.</p>
<p>While <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_X%5B%5Clog%28f%28X%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_X[\log(f(X))]" class="latex" title="{{\mathbb{E}}}_X[\log(f(X))]" /> is hard to compute,<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28X%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[f(X)^r]" class="latex" title="{{\mathbb{E}}}[f(X)^r]" /> is a much easier quantity. The<br />
replica trick starts from rewriting <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> with moments of <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" />:<br />
Recall that <img src="https://s0.wp.com/latex.php?latex=x%5Er+%5Capprox+1+%2B+r+%5Clog+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^r \approx 1 + r \log x" class="latex" title="x^r \approx 1 + r \log x" /> for <img src="https://s0.wp.com/latex.php?latex=r+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \approx 0" class="latex" title="r \approx 0" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cln%281+%2B+x%29%5Capprox+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ln(1 + x)\approx x" class="latex" title="\ln(1 + x)\approx x" />, using this we can rewrite <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x)" class="latex" title="f(x)" /> in the following<br />
way:</p>
<p><strong>Claim 1.</strong> <em>Let <img src="https://s0.wp.com/latex.php?latex=f_r%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br+%5Cbeta+n%7D%5Cln%5B%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" class="latex" title="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" /></em><br />
<em>Then, <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Br+%5Cto+0%7Df_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{r \to 0}f_r(\beta)" class="latex" title="f(\beta) = \lim_{r \to 0}f_r(\beta)" /></em></p>
<p>The idea of replica method is quite simple</p>
<ul>
<li>Define a function <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" /> for <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in \mathbb{Z}_+" class="latex" title="r \in \mathbb{Z}_+" /> s.t. <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+f_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta) = f_r(\beta)" class="latex" title="f(r, \beta) = f_r(\beta)" /> for all such <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />.<p></p>
</li>
<li>
<p>Extend <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" /> analytically to all <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%7B%7B%5Cmathbb%7BR%7D%7D%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in {{\mathbb{R}}}_+" class="latex" title="r \in {{\mathbb{R}}}_+" /> and take the limit of <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />.</p>
</li>
</ul>
<p>The second step may sound crazy, but for some unexplained reason, it has<br />
been surprisingly effective at making correct predictions.</p>
<p>The term replica comes from the way used to compute<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}[Z^r]" class="latex" title="{{\mathbb{E}}}[Z^r]" /> in Claim 1. We expand the <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />-th moment<br />
in terms of <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> replicas of the system</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29%5Er+%3D+%28%5Csum_x+exp%28-%5Cbeta+H%28Y%2C+x%29%29%29%5Er+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5CPi_%7Bk+%3D+1%7D%5Er+exp%28-%5Cbeta+H%28Y%2C+x%5Ei%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" class="latex" title="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" /></p>
<h2>For Rademacher spiked Wigner model</h2>
<p>In this section, we will see how one can apply the replica trick to<br />
obtain phase transition in the Rademacher spiked Wigner model. Recall<br />
that given a hidden <img src="https://s0.wp.com/latex.php?latex=a+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a \in \{\pm 1\}^n" class="latex" title="a \in \{\pm 1\}^n" />, the observable<br />
<img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Da%5ETa+%2B+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" class="latex" title="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" /> where<br />
<img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, j} \sim \mathcal{N}(0, 1)" class="latex" title="W_{i, j} \sim \mathcal{N}(0, 1)" /> and <img src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{i, i} \sim \mathcal{N}(0, 2)" class="latex" title="W_{i, i} \sim \mathcal{N}(0, 2)" />.<br />
We are interested in finding the smallest <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> where we can still<br />
recover a solution with some correlation to the ground truth <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" />. <em>Note<br />
that <img src="https://s0.wp.com/latex.php?latex=%5C%7BW_%7Bi%2C+i%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{W_{i, i}\}" class="latex" title="\{W_{i, i}\}" /> is not so important here as <img src="https://s0.wp.com/latex.php?latex=x_i%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i^2" class="latex" title="x_i^2" /> doesn’t carry<br />
any information in this case.</em></p>
<p>Given by the posterior <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BP%7D%7D%7D%5Bx%7CY%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{P}}}[x|Y]" class="latex" title="{{\mathbb{P}}}[x|Y]" />, the system we<br />
set up corresponding to Rademacher spiked Wigner model is the following:</p>
<ul>
<li>the system consists of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> particles and the interactions between<br />
each particle are give by <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /><p></p>
</li>
<li>
<p>the signal to noise ratio <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> as the inverse temperature<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />.</p>
</li>
</ul>
<p>Following the steps above, we begin by computing<br />
<img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" class="latex" title="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" /><br />
for <img src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \in \mathbb{Z}_+" class="latex" title="r \in \mathbb{Z}_+" />: Denote <img src="https://s0.wp.com/latex.php?latex=X%5Ek+%3D+%28x%5Ek%29%5ETx%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X^k = (x^k)^Tx^k" class="latex" title="X^k = (x^k)^Tx^k" /> where <img src="https://s0.wp.com/latex.php?latex=x%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^k" class="latex" title="x^k" /> is the<br />
<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />th replica of the system.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%5Csum_k+%3CY%2C+X%5Ek%3E+%5Cmu%28Y%29+dY%5C%5C+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%3CY%2C+%5Csum_k+X%5Ek%3E%29+%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" class="latex" title="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" /></p>
<p>We then simplify the above expression with a technical claim.</p>
<p><strong>Claim 2.</strong><em> Let <img src="https://s0.wp.com/latex.php?latex=Y+%3D+A+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y = A + \frac{1}{\sqrt{n}}W" class="latex" title="Y = A + \frac{1}{\sqrt{n}}W" /> where <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> is a fixed matrix and</em><br />
<em><img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> is the GOE matrix defined as above. Then,</em><br />
<em><img src="https://s0.wp.com/latex.php?latex=%5Cint_Y+exp%28%5Cbeta%3CY%2C+X%3E%29+%5Cmu%28Y%29+dY+%3D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%3CA%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" class="latex" title="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" /></em><br />
<em>for some constant <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> depending on distribution of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" />.</em></p>
<p>Denote <img src="https://s0.wp.com/latex.php?latex=X+%3D+%5Csum_k+X%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X = \sum_k X^k" class="latex" title="X = \sum_k X^k" />. Apply Claim 2 with<br />
<img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cfrac%7B%5Cbeta%7D%7Bn%7Da%5ETa&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A = \frac{\beta}{n}a^Ta" class="latex" title="A = \frac{\beta}{n}a^Ta" />, we have<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D+%3Ca%5ETa%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" class="latex" title="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" /><br />
To understand the term inside exponent better, we can rewrite the inner<br />
sum in terms of overlap between replicas:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7DX_%7Bi%2C+j%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%5E2+%3D%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%28%5Csum_%7Bl+%3D+1%7D%5Er+x%5El_ix%5El_j%29%5C%5C+%3D+%5Csum_%7Bk%2C+l%7D+%28%5Csum_%7Bi+%3D+1%7D%5En+x%5Ek_ix%5E%7Bl%7D_i%29%5E2+%3D+%5Csum_%7Bk%2C+l%7D+%3Cx%5Ek%2C+x%5El%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" class="latex" title="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" /></p>
<p>where the last equality follows from rearranging and switch the inner<br />
and outer summations.</p>
<p>Using a similar trick, we can view the other term as</p>
<p><img src="https://s0.wp.com/latex.php?latex=%3Ca%5ETa%2C+X%3E+%3D+%5Csum_%7Bi%2C+j%7D%5Csum_%7Bk+%3D+1%7D%5Erx%5Ek_ix%5Ek_ja_ia_j+%3D+%5Csum_%7Bk+%3D+1%7D%5Er+%28%5Csum_%7Bi+%3D+1%7D%5En+a_ix%5Ek_i%29%5E2+%3D+%5Csum_%7Bk%7D%3Ca%2C+x%5Ek%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" class="latex" title="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" /></p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+%3Cx%5Ek%2C+x%5El%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = &lt;x^k, x^l&gt;" class="latex" title="Q_{k, l} = &lt;x^k, x^l&gt;" /> represents overlaps between the<br />
<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> and <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" />th replicas and <img src="https://s0.wp.com/latex.php?latex=Q_k+%3D+%3Ca%2C+x%5Ek%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_k = &lt;a, x^k&gt;" class="latex" title="Q_k = &lt;a, x^k&gt;" /> represents the<br />
overlaps between the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />th replica and the ground truth vector.</p>
<p>In the end, we get for any integer <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />, (Equation 1):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%28%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29+%5Clabel%7Be%3A1%7D%5C%5C+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D+%5Cln%28%5Csum_%7BQ%7D%5Cnu_%7Bx%5Ek%7D%28Q%29exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" class="latex" title="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" /></p>
<p>Our goal becomes to approximate this quantity. Intuitively, if we think<br />
of <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l}" class="latex" title="Q_{k, l}" /> as indices on a <img src="https://s0.wp.com/latex.php?latex=%28r+%2B+1%29+%5Ctimes+%28r+%2B+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(r + 1) \times (r + 1)" class="latex" title="(r + 1) \times (r + 1)" /> matrices, <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" />,<br />
with <img src="https://s0.wp.com/latex.php?latex=Q%28i%2Ci%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(i,i) = 1" class="latex" title="Q(i,i) = 1" />, then <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" /> is the average of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> i.i.d matrices. So we<br />
expect <img src="https://s0.wp.com/latex.php?latex=Q_%7Bj%2C+k%7D+%5Cin+%5B%5Cpm+%5Cfrac%7B1%7D%7Bn%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{j, k} \in [\pm \frac{1}{n}]" class="latex" title="Q_{j, k} \in [\pm \frac{1}{n}]" /> for <img src="https://s0.wp.com/latex.php?latex=j+%5Cneq+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j \neq k" class="latex" title="j \neq k" /> w.h.p. In the<br />
remaining part, We find the correct <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q" class="latex" title="Q" /> via rewriting Equation 1.</p>
<p>Observe that by introducing a new variable <img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k, l}" class="latex" title="Z_{k, l}" /> for <img src="https://s0.wp.com/latex.php?latex=k+%5Cneq+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \neq l" class="latex" title="k \neq l" /> and<br />
using the property of gaussian intergal (Equation 4):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A4%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7Bn%7D%7B4%5Cpi%7D%7D%5Cint_%7BZ_%7Bk%2C+l%7D%7D+exp%28-%5Cfrac%7Bn%7D%7B4%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cbeta+Q_%7Bk%2C+l%7DZ_%7Bk%2C+l%7D%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" class="latex" title="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7DQ_k%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7B8%5Cpi+n%7D%7D%5Cint_%7BZ_k%7Dexp%28-%282n%29Z_k%5E2+%2B+2%5Cbeta+Q_%7Bk%7DZ_k%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" class="latex" title="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" /><br />
Replace each <img src="https://s0.wp.com/latex.php?latex=exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="exp(\frac{\beta^2}{n}Q_{k, l}^2)" class="latex" title="exp(\frac{\beta^2}{n}Q_{k, l}^2)" /> by a such integral, we<br />
have (Equation 2):</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bgathered%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29+%5Clabel%7Be%3A2%7D%5C%5C+%3D+C%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5Cexp%28%5Cbeta%5E2+n%29%5Cint_%7BZ_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29+dZ+%5C%5C+%3DC%5Cexp%28%5Cbeta%5En%29+%5Cint_%7BY_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kY_k+Q_k%29%29+dY+%5Clabel%7Be%3A2%7D%5Cend%7Bgathered%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" class="latex" title="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> is the constant given by introducing gaussian intergals.</p>
<p>To compute the integral in (Equation 2), we need to cheat a little bit and take<br />
<img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" /> before letting <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />. Note that free energy density<br />
is defined as<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" class="latex" title="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" /><br />
This is the second assumption made in the replica method and it is<br />
commonly believed that switching the order is okay here. Physically,<br />
this is plausible because we believe intrinsic physical quantities<br />
should not depend on the system size.</p>
<p>Now the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace method</a> tells us when <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" />, the integral in (Equation 2) is dominated by the max of the exponent.</p>
<p><strong>Theorem 1 (Laplace Method).</strong> <em>Let <img src="https://s0.wp.com/latex.php?latex=h%28x%29%3A+%7B%7B%5Cmathbb%7BR%7D%7D%7D%5En+%5Cto+%7B%7B%5Cmathbb%7BR%7D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" class="latex" title="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" />, </em><em>then </em></p>
<p><em><img src="https://s0.wp.com/latex.php?latex=%5Cint+e%5E%7Bnh%28x%29%7D+%5Capprox+e%5E%7Bnh%28x%5E%2A%29%7D%28%5Cfrac%7B2%5Cpi%7D%7Bn%7D%29%5E%7B%5Cfrac%7Bd%7D%7B2%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7Bdet%28H%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" class="latex" title="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" /></em></p>
<p><em>where <img src="https://s0.wp.com/latex.php?latex=x%5E%2A+%3D+argmax_x+%5C%7Bh%28x%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^* = argmax_x \{h(x)\}" class="latex" title="x^* = argmax_x \{h(x)\}" /> and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is the Hessian of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" /> evaluated at the point <img src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^*" class="latex" title="x^*" />.</em></p>
<p>Fix a pair of <img src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k, l" class="latex" title="k, l" /> and apply Laplace method with<br />
<img src="https://s0.wp.com/latex.php?latex=h%28Z_%7Bk%2C+l%7D%29+%3D+-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7B0+%5Cleq+k+%3C+l+%5Cleq+r%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" class="latex" title="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" /><br />
what’s left to do is to find the critical point of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" />. Taking the<br />
derivatives gives<br />
<img src="https://s0.wp.com/latex.php?latex=-Y_%7Bk%2C+l%7D+%2B+%5Cfrac%7BA%28Z_%7Bk%2C+l%7D%29%5Cbeta+Q_%7Bk%2C+l%7D%7D%7Bn+A%28Z_%7Bk%2C+l%7D%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" class="latex" title="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" /><br />
where<br />
<img src="https://s0.wp.com/latex.php?latex=A%28Z_%7Bk%2C+l%7D%29+%3D+%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+%5Cbeta%5Csum_kY_k+Q_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" class="latex" title="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" />.</p>
<p>We now need to find a saddle point of <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" /> where the hessian is PSD. To<br />
do that, we choose to assume the order of the replicas does not matter,<br />
which is refer to as the replica symmetry case. <sup id="fnref-7420-1"><a href="https://windowsontheory.org/feed/#fn-7420-1" class="jetpack-footnote">1</a></sup> One simplest form<br />
of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> is the following: <img src="https://s0.wp.com/latex.php?latex=%5Cforall+k%2C+l+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall k, l &gt; 0" class="latex" title="\forall k, l &gt; 0" />, <img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k, l} = y" class="latex" title="Z_{k, l} = y" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=Z_%7Bk%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{k} = y" class="latex" title="Z_{k} = y" /> for some <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />. This also implies that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = q" class="latex" title="Q_{k, l} = q" /> for some<br />
<img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and <img src="https://s0.wp.com/latex.php?latex=y+%3D%5Cfrac%7B%5Cbeta%7D%7Bn%7D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y =\frac{\beta}{n} q" class="latex" title="y =\frac{\beta}{n} q" /></p>
<p>Plug this back in to Equation 2 gives: (Equation 3)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A3%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+C%5Cexp%28%5Cbeta+n%29%5Cexp%28-%5Cfrac%7Bn%7D%7B2%7D%28%5Cfrac%7Br%5E2+-+r%7D%7B2%7D%29y%5E2+-+%5Cfrac%7Bn%5E2%7D%7B2%7D+%2B+%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+2y%5Cbeta+%5Csum_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" class="latex" title="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" /></p>
<p>To obtain <img src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(r, \beta)" class="latex" title="f(r, \beta)" />, we only need to deal with the last term in<br />
(Equation 3) as <img src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r \to 0" class="latex" title="r \to 0" />. Using the fact that <img src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_{k, l} = y" class="latex" title="Q_{k, l} = y" /> for all<br />
<img src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k, l" class="latex" title="k, l" /> and using the same trick of introducing new gaussain integral as<br />
in (Equation 4) we have<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%7D%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+n%5Cbeta+%5Csum_k+Q_k%29%29+%3D+-%5Cbeta+%2B+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7Bz+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29%7D%5B%5Clog%282cosh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" class="latex" title="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" /></p>
<p>Using the fact that we want the solution to minimizes free energy,<br />
taking the derivative of the current <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> w.r.t. <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> gives<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7By%7D%7B%5Cbeta%7D+%3D+n%7B%7B%5Cmathbb%7BE%7D%7D%7D_z%5Btanh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" class="latex" title="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" /><br />
which matches the fixed point of AMP. Plug in <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> will give us<br />
<img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" />. The curve of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> looks like the Figure below, where<br />
the solid line is the curve of <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> with the given <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and the<br />
dotted line is the curve given by setting all variables <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.54.49.png?w=600" alt="screenshot 2019-01-26 13.54.49" class="alignnone size-full wp-image-7423" /></p>
<p> </p>
<p><strong>References</strong></p>
<div>[1] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-optimality of pca for spiked random matrices and synchronization.</div>
<div>arXiv preprint arXiv:1609.05573, 2016.</div>
<div></div>
<div>[2] D. Feral and S. Pech e. The Largest Eigenvalue of Rank One Deformation of Large Wigner Matrices. Communications in Mathematical Physics, 272:185–228, May 2007.</div>
<div></div>
<div>[3] Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.</div>
<div></div>
<div>[4] Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for thebinary stochastic block model. In</div>
<div>Information Theory (ISIT), 2016 IEEE International Symposium on, pages 185–189. IEEE, 2016.</div>
<div></div>
<div>[5] Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.</div>
<div></div>
<div>[6] A. Perry, A. S. Wein, and A. S. Bandeira. Statistical limits of spiked tensor models.</div>
<div>ArXiv e-prints, December 2016.</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn-7420-1">
Turns out for this problem, replica symmetry is the only case. We<br />
will not talk about replica symmetry breaking here, which<br />
intuitively means we partition replicas into groups and re-curse. <a href="https://windowsontheory.org/feed/#fnref-7420-1">↩</a>
</li>
</ol>
</div></div>







<p class="date">
by preetum <a href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/"><span class="datestr">at January 26, 2019 07:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7413">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/">Quantum circuits and their role in demonstrating quantum supremacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid proof that quantum computers do have an <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">advantage</a> over classical computers. </p>



<p>What’s rather tricky about showing such a result is that, rather than a direct argument about the capability of quantum computers, what we really need to demonstrate is the incapability of classical computers to achieve tasks that can be done with quantum computers. </p>



<p>One of the major leaps forward in demonstrating quantum supremacy was taken by Terhal and DiVincenzo in their 2008 paper “<a href="https://arxiv.org/abs/quant-ph/0205133">Adaptive quantum computation, constant depth quantum circuits and arthur-merlin games</a>“. Their approach was to appeal to a complexity-theoretic argument: they gave evidence that there exists a certain class of quantum circuits that cannot be simulated classically by proving that if a classical simulation existed, certain complexity classes strongly believed to be distinct would collapse to the same class. While this doesn’t quite provide a proof of quantum supremacy – since the statement about the distinction between complexity classes upon which it hinges is not a proven fact – because the complexity statement appears overwhelmingly likely to be true, so too does the proposed existence of non-classically-simulatable quantum circuits. The Terhal and DiVincenzo paper is a complex and highly technical one, but in this post I hope to explain a little bit and give some intuition for the major points. </p>



<p>Now, let’s start at the beginning. What is a <a href="https://en.wikipedia.org/wiki/Quantum_circuit">quantum circuit</a>? I’m going to go ahead and assume you already know what a <a href="https://en.wikipedia.org/wiki/Circuit_(computer_science)">classical circuit</a> is – the extension to a quantum circuit is rather straightforward: it’s a circuit in which all gates are <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate">quantum gates</a>, where a quantum gate can be thought of as a classical gate whose output is, rather than a deterministic function of the inputs, instead a probability distribution over all possible outputs given the size of the inputs. For example, given two single-bit inputs, a classical AND gate outputs 0 or 1 deterministically given the inputs. A quantum AND gate on the analogous single-<a href="https://en.wikipedia.org/wiki/Qubit">qubit</a> inputs would output 0 with some probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and 1 with some probability <img src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1-p" class="latex" title="1-p" />. Similarly, a classical AND gate on two 4-bit inputs outputs the bitwise AND, while the quantum analog has associated with it a 4-qubit output: some probability distribution over all 4-bit binary strings. A priori there is no particular string that is the “output” of the computation by the quantum gate; it’s only after taking a <a href="https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics">quantum measurement</a> of the output that we get an actual string that we can think of as the outcome of the computation done by the gate. The actual string we “observe” upon taking the measurement follows the probability distribution computed by the gate on its inputs. In this way, a quantum circuit can then be thought of as producing, via a sequence of probabilistic classical gates (i.e., quantum gates) some probability distribution over possible outputs given the input lengths. It’s not hard to see that in this way, we can compose circuits: suppose we have a quantum circuit <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> and another quantum circuit <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />. Let <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> have an input of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits and an output of <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> qubits; suppose we measure <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> of the output qubits of <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_1" class="latex" title="c_1" /> – then we can feed the remaining <img src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m-k" class="latex" title="m-k" /> unmeasured qubits as inputs into <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />(assuming that those <img src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m-k" class="latex" title="m-k" /> qubits do indeed constitute a valid input to <img src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_2" class="latex" title="c_2" />). </p>



<p>Consider, then, the following sort of quantum circuit: it’s a composition of <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" /> quantum circuits, such that after each <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th circuit we take a measurement some of its output qubits (so that the remaining unmeasured qubits become inputs to the <img src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i+1)" class="latex" title="(i+1)" />-th circuit), and then the structure of the <img src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i+1)" class="latex" title="(i+1)" />-th circuit is dependent on this measurement. That is, it’s as though, given a quantum circuit, we’re checking every so often at intermediate layers over the course of the circuit’s computation what the value of some of the variables are (leaving the rest to keep going along through the circuit to undergo more computational processing), and based on what we measure is the current computed value, the remainder of the circuit “adapts” in a way determined by that measurement. Aptly enough, this is called an “adaptive circuit”. But since the “downstream” structure of the circuit depends on the outcomes of all the measurements made “upstream”, each adaptive circuit actually comprises a family of circuits, each of which is specified by the sequence of intermediate measurement outcomes. That is, we can alternatively characterize an adaptive circuit as a set of ordinary quantum circuits that is parameterized by a list of measurement outcomes. Terhal and DiVincenzo call this way of viewing an adaptive circuit, as a family of circuits parametrized by a sequence of measurement values, a “non-adaptive circuit” – since we replace the idea that the circuit “adapts” to intermediate measurements with the idea that there are just many regular circuits, one for each possible sequence of measurements. It’s this non-adaptive circuit concept that’ll be our main object of study going forward.</p>



<h2>Simulating quantum circuits</h2>



<p>Now, the result we wanted to demonstrate about quantum circuits had to do with their efficient simulatability by classical circuits – and so we should establish some notion of what we mean when we talk about an “efficient simulation”. </p>



<p>Terhal and DiVincenzo offer the following notion of a classical simulation – which in their paper they call an “efficient density computation”: consider a quantum circuit with some output of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits. Recall that to actually obtain an output value, we need to take a measurement of the circuit output – imagine doing this in disjoint subsets of cubits at a time. That is, we can break up the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits into <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> disjoint subsets and consider the entire output measurement as a process of taking <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> measurements, subset by subset. An efficient density computation exists if there’s a classical procedure for computing, in time polynomial in the width and depth of the quantum circuit, the conditional probability distribution over the set of possible measurement outcomes of a particular subset of qubits, given any subset of the <img src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k-1" class="latex" title="k-1" /> other measurement outcomes. Intuitively, this is a good notion of what a classical simulation should consist of, or at least what data it should contain, since if you know the conditional probabilities given any (possibly empty) subset of the other measurements, you can just flip coins for the outputs according to the conditional probabilities as a way of actually exhibiting a “working” simulation.</p>



<p>It’s with this notion of simulation, along with our concept of an adaptive quantum circuit as a family of regular circuits parameterized by a sequence of intermediate measurement outcomes, we may now arrive at the main result of Terhal and DiVincenzo’s paper. Recall that what we wanted to show from the very beginning is that there exists some quantum circuit that can’t be simulated classically. The argument for this proceeds like a proof by contradiction: suppose the contrary, and that all quantum circuits can be simulated classically. We want to show that we can find, then, a quantum circuit which, if it were possible to be simulated classically (as per our assumption), we’d wind up with some strange consequences that we believe are false, leading us to conclude that those circuits probably can’t be simulated classically.</p>



<p>Thus, we shall now exhibit such a quantum circuit whose classical simulatability leads (as far as we believe) to a contradiction. Consider a special case of adaptive quantum circuits, considered as a parameterized family of regular circuits, in which the circuit’s output distribution is independent of the intermediate measurement outcomes; that is, the case in which the entire family of circuits corresponding to an adaptive circuit is logically the same – that is, is the same logical circuit on input qubits independent of intermediate measurements. I’d like to point out, just for clarification’s sake, the subtlety here, which makes this consideration non-redundant, and not simply a reduction of an adaptive quantum circuit (again, thought of as a family) to a single fixed circuit (i.e., a family of one): the situation in which the family is reduced to a single fixed circuit occurs when the<em> structure of the circuit</em> is independent of the intermediate measurement outcomes. If the structure were independent of the measurements, then no matter what we observed in the measurements, we’d get the same circuit – hence a trivial family of one. What we’re considering instead is the case in which the <em>structure</em> of the circuit is still dependent on the intermediate measurements (and so the circuit is still adaptive), but where the <em>distribution over the possible outputs of the circuit</em> is identical no matter what the intermediate measurements are. In this case, the circuit can still be considered as a parameterized and in general non-trivial family of circuits, but for which each member produces the same distribution over outputs – hence, a family of potentially <em>structurally</em> different circuits, but which are <em>logically</em> identical.</p>



<p>Suppose there’s some set of such circuits that’s universal – that is, that’s sufficient to implement all polynomial-time quantum computations. (This is a reasonable assumption to make, since there do in fact exist <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate\#Universal_quantum_gates">universal quantum gate sets.</a> But now if a simulation of the kind we defined (an efficient density computation) existed for every circuit in this set, then we could calculate the outcome probability of any polynomial-depth quantum circuit, since any polynomial-depth quantum circuit could be realized as some composition of circuits in this universal set (and in particular as a composition of particular family members of each adaptive circuit in the universal set), and an efficient density computation, as we mentioned above, precisely gives us a way to compute the output distribution. </p>



<p>But now here is where our believed contradiction lies: </p>



<p><em>Theorem:</em> Suppose there exists a universal set of adaptive quantum circuits whose output distributions are independent of intermediate measurements. If there is an efficient density computation for each family member of each adaptive circuit in this universal set, then for the polynomial hierarchy PH we have PH = BPP = BQP. </p>



<p>The proof goes something like this: if we can do our desired efficient density computations (as we assumed, for the sake of contradiction, we could for all quantum circuits), this is equivalent to being able to determine the acceptance probability of a quantum computation, which was shown in the paper “<a href="https://arxiv.org/abs/quant-ph/9812056">Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy”</a> by Fenner, Green, Homer and Pruim to be equivalent to the class <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{coC=P}" class="latex" title="\text{coC=P}" />. Thus, we have that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D+%5Csubseteq+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{coC=P} \subseteq \text{BPP}" class="latex" title="\text{coC=P} \subseteq \text{BPP}" />. But it’s known that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BcoC%3DP%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" class="latex" title="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" /> and so <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BBPP%7D%7D+%3D+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" class="latex" title="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" />. That is, we have <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%3D+%5Ctext%7BBPP%7D+%3D+%5Ctext%7BBQP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{PH} = \text{BPP} = \text{BQP}" class="latex" title="\text{PH} = \text{BPP} = \text{BQP}" />, and so the polynomial hierarchy would collapse to <img src="https://s0.wp.com/latex.php?latex=%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma^P_2" class="latex" title="\Sigma^P_2" /> since <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBPP%7D+%5Csubseteq+%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{BPP} \subseteq \Sigma^P_2" class="latex" title="\text{BPP} \subseteq \Sigma^P_2" /> (for more on these more obscure complexity classes, see <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo">here</a>). Again, this is our “contradiction”: while it hasn’t been quite proven, it is widely believed, with strong supporting evidence, that the polynomial hierarchy does not collapse as would be the case if all quantum circuits were classically simulatable. Thus this provides a strong argument that not all quantum circuits are classically simulatable, which was precisely what we were looking to demonstrate.</p>



<h2>Conclusion</h2>



<p>Terhal and DiVincenzo actually go even further and show that there is a certain class of constant-depth quantum circuits that are unlikely to be simulatable by classical circuits – this, indeed, seems to provide even stronger evidence for quantum supremacy. This argument, which is somewhat more complex, uses the idea of teleportation and focuses on a particular class of circuits implementable by a certain restricted set of quantum gates. If you’re interested, I highly recommend reading their paper, where this is explained. </p>



<h2>Recommended reading</h2>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li><li>Harrow, Aram Wettroth and Ashley Montanaro. “Quantum computational supremacy.” <em>Nature</em> 549 (2017): 203-209.</li><li>Boixo, Sergio et. al. “Characterizing quantum supremacy in near-term devices.” <em>Nature Physics</em> 14 (2018); 595-600.</li></ul>



<h2>References</h2>



<p>\begin{enumerate}</p>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Fenner, Stephen et. al. “Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy.” <a href="https://arxiv.org/abs/quant-ph/9812056" rel="nofollow">https://arxiv.org/abs/quant-ph/9812056</a>. (1998).</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li></ul></div>







<p class="date">
by hksorens <a href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/"><span class="datestr">at January 25, 2019 11:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/">postdoc at UC San Diego (apply by March 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/"><span class="datestr">at January 25, 2019 07:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7411">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/">Looking a postdoc opportunity?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out <a href="https://cstheory-jobs.org/">https://cstheory-jobs.org/</a> . </p>



<p>(For example, a new ad was just posted on Wednesday by <a href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">Venkat Guruswami and Pravesh Kothari </a> )</p></div>







<p class="date">
by windowsontheory <a href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/"><span class="datestr">at January 25, 2019 05:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/">faculty at UC San Diego (apply by February 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Apply to be an Assistant Professor, with a focus on Algorithmic Approaches to Socially Innovative Product Architectures and Business Models. This is a joint search by the Computer Science department and the Rady business school.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF01987">https://apol-recruit.ucsd.edu/JPF01987</a><br />
Email: slovett@ucsd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/"><span class="datestr">at January 24, 2019 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7530001663397385709">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html">Machine Learning and Wind Turbines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator">
<a style="margin-left: 1em; margin-right: 1em;" href="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s1600/Turbines.png"><img width="320" src="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s320/Turbines.png" border="0" height="180" /></a></div>
<br />
My daughter Molly spent six weeks on an environmental program in China last summer. When she got back she had to do a report on machine learning and wind turbines used for clean energy generation. What does machine learning have to do with wind turbines? Plenty it turns out and it tell us a lot about the future of programming.<br />
<br />
Sudden changes in wind can cause damage to the blades of the turbine. Maintenance is very expensive especially for turbines in the sea and a broken turbine generates no electricity. To catch these changes ahead of time you can mount a Lidar on top of the turbine.<br />
<br />
<div style="clear: both; text-align: center;" class="separator">
<a style="margin-left: 1em; margin-right: 1em;" href="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s1600/Turbine%2BLidar.jpg"><img width="320" src="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s320/Turbine%2BLidar.jpg" border="0" height="180" /></a></div>
<br />
The Lidar can detect wind gusts from about 100 meters ahead, giving about 10 seconds to react. In that time you can rotate the blades, or the whole turbine itself to minimize any damage. Here's a <a href="https://www.youtube.com/watch?v=j5zLp6UuC70">video</a> describing the situation.<br />
<br />
<center>

</center>
<br />
How do you do the computations to convert the Lidar data into accurate representations of wind gusts and then how to best adjust for them? You could imagine some complex fluid dynamics computation, which gets even more complex when you several wind turbines in front of each other. Instead you can use the massive amount of data you have collected by sensors on the turbine and the Lidar information and train a neural network. Training takes a long time but a trained network can quickly determine a good course of action. Now neural networks can always make mistakes but unlike self-driving cars, a mistake won't kill anyone, just possibly cause more damage. Since on average you can save considerable maintenance costs, using ML here is a big win.<br />
<br />
I've obviously over simplified the above but I really like this example. This is not an ML solution to a standard AI question like image recognition or playing chess. Rather we are using ML to make a difficult computation tractable mostly by using ML on available data and that changes how we think about programming complex tasks.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html"><span class="datestr">at January 24, 2019 01:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1054">Prophet inequality and auction design</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Suppose you want to sell a car and there are 10 agents willing to buy it. You are not sure how much they could pay but for each of them you know a probability distribution of how high the offer will be. For example, a car salon would always pay 10K  but some person might offer 5K or 15K with equal probability. The best you could do is to first negotiate with all of them and then pick the highest bid. Unfortunately, you cannot do so - after seeing each offer you must irrevocably choose either to sell the car or to refuse the offer. What is the best strategy to maximize your revenue in this case?</p>
<p>In turns out that this is a well-studied optimization problem with a simple strategy that guarantees you can (on expectation) earn at least half as much as a hypothetical prophet, who knows all the bids in advance. This result is known as the prophet inequality. What is more surprising, this strategy would work even if you are a car retailer and want to sell five cars. Moreover, you might want to have some constraints, for example you do not want to sell two cars to buyers from the same city, or have multiple kinds of cars with different evaluations, and you can always guarantee an expected revenue comparable to the one of the prophets.</p>
<p>This problem not only exploits beautiful math but also has important applications in internet ad display. Actually, whenever you type a query into a web search engine, the ad system performs this kind of car-selling game with the ad suppliers, who offer different bids for their ad to be displayed to you.</p>
<p>Here is a link to our recent work with new developments in this theory: <a href="http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf">http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf</a></p>
<p>Michał Włodarczyk</p></div>







<p class="date">
by Renata Czarniecka <a href="http://corner.mimuw.edu.pl/?p=1054"><span class="datestr">at January 24, 2019 10:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">Postdoctoral Fellow at Carnegie Mellon University (apply by February 28, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Postdoctoral Position at the computer science department, CMU. Hosted by Venkatesan Gurusawami and Pravesh Kothari. Start Date: Fall 2019 (Flexible)<br />
Application Deadline: Feb 28, 2019 (earlier applications encouraged).</p>
<p>To apply, send CV, a research statement and arrange for 2 letters of recommendation to be set to be sent to bcook@cs.cmu.edu with subject line mentioning “CMU theory postdoc.”</p>
<p>Website: <a href="https://www.cs.princeton.edu/~kothari/theory-postdoc.html">https://www.cs.princeton.edu/~kothari/theory-postdoc.html</a><br />
Email: kothari@cs.princeton.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/"><span class="datestr">at January 23, 2019 10:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/">Postdoc at Charles University in Prague (apply by February 28, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Several one‐year post‐doc positions are available at the Computer Science Institute of Charles University, with a possibility of one-year extension. The positions are in areas of algorithms, cryptography, computational complexity, combinatorics and graph theory. Starting date is in Fall 2019, and can be negotiated.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html">https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/"><span class="datestr">at January 22, 2019 12:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/22/phd-student-at-charles-university-in-prague-apply-by-february-28-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/22/phd-student-at-charles-university-in-prague-apply-by-february-28-2019/">PhD student at Charles University in Prague (apply by February 28, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Positions for PhD students are available at the Computer Science Institute of Charles University within the framework of the project “Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. Applications are invited from candidates who have strong background in theoretical computer science or discrete mathematics. Starting date: Fall 2019.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/EPAC/positions.html">https://iuuk.mff.cuni.cz/~koucky/EPAC/positions.html</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/22/phd-student-at-charles-university-in-prague-apply-by-february-28-2019/"><span class="datestr">at January 22, 2019 12:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019/">Postdoc at Charles University in Prague (apply by February 28, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The position is supported by a five year project “EPAC: Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. The project is focused on approximation algorithms in fine-grained and parameterized complexity and on lower bound techniques. Starting date is in Fall 2019, and can be negotiated.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/EPAC/">https://iuuk.mff.cuni.cz/~koucky/EPAC/</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019/"><span class="datestr">at January 22, 2019 12:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3377">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/01/22/guest-post-like-a-swarm-of-locusts-vijay-vazirani/">Guest post: “Like a Swarm of Locusts…” (Vijay Vazirani)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em><img src="https://www.ics.uci.edu/~vazirani/Vijay2.png" align="RIGHT" height="161" width="135" alt="My Picture" border="1" class=" alignleft" /></em></p>
<p> </p>
<p style="text-align: right;"><em>[The following guest post was written by Vijay Vazirani.] </em></p>
<p> </p>
<p> </p>
<p> </p>
<p><em> </em></p>
<p> </p>
<blockquote><p><em>What the cutting locust left, the swarming locust has eaten. What the swarming locust left, the hopping locust has eaten, and what the hopping locust left, the destroying locust has eaten.</em></p>
<p style="text-align: right;">Joel 1:4</p>
</blockquote>
<p>A few years ago, while attending a Dagstuhl program on Equilibrium Computation, I embarked on one of the traditional long hikes through the beautiful woods surrounding Schloss Dagstuhl and happened to be walking next to a senior member of the Operations Research community. In no time we got immersed in a lively discussion on the style of research going on in Algorithmic Game Theory. We both agreed that the progress made in a matter of a few years was nothing short of phenomenal. At this point, my colleague remarked that AGT was no exception and that when TCS researchers enter a new area, they go in with such energies and enthusiasm that in no time not only is all low hanging fruit gone but in fact the entire area is devoid of any reasonable open problems! “They attack the area like swarms of locusts devouring foliage in biblical lands, consuming not only fruit and leaves but even shrubs and twigs,” he added. “Yes, and only sh*t is left behind!” I chimed in.</p>
<p>As AGT is reaching that stage, with researchers yearning for new issues/problems for their students and their own research and grant proposals, there is a reprieve emerging on the horizon: a program at Simons on “<a href="https://simons.berkeley.edu/programs/market2019">Online and Matching-Based Market Design</a>” in Fall 2019.</p>
<p>This is by no means a new area. In fact, the first such market, for matching medical residents to hospitals, dates back to 1920s, well before the classic — and by now Nobel Prize winning — work of Gale and Shapley on the stable matching problem, which provided the canonical algorithm for this market. In recent years, the advent of the Internet and the relocation of our most important activities to online platforms have led to an explosion of such marketplaces (for examples, see the Simons web page) and they have been occupying an ever-increasing fraction of our economy.</p>
<p>AGT, CS and Economics have already had a massive impact via these markets. My own experience comes from Google’s multi-billion dollar Adwords market in which sophisticated algorithmic ideas have also played a central role, e.g., see the Simons talk “<a href="https://simons.berkeley.edu/talks/vijay-vazirani-4-30-18">Google’s AdWords Market: How Theory Influenced Practice</a>“.</p>
<p>Clearly, the stakes are high. There is a real opportunity of extending the already existing highly inter-disciplinary theory in substantial ways and making an even bigger impact. Considering this and the enthusiasm of the researchers who have agreed to be participants in this program, Federico Echenique, Nicole Immorlica and I have launched the project of publishing a comprehensive volume of contributed chapters on this topic. More information on this will be coming soon.</p></div>







<p class="date">
by robertkleinberg <a href="https://agtb.wordpress.com/2019/01/22/guest-post-like-a-swarm-of-locusts-vijay-vazirani/"><span class="datestr">at January 21, 2019 10:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15597">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/01/21/a-simple-fact/">A Simple Fact</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Can we find a simplest proof?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/wedderburndickson.png"><img src="https://rjlipton.files.wordpress.com/2019/01/wedderburndickson.png?w=193&amp;h=142" alt="" width="193" class="alignright wp-image-15598" height="142" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite crop from <a href="https://en.wikipedia.org/wiki/Joseph_Wedderburn">src1</a>, <a href="http://www.ams.org/about-us/presidents/14-dickson">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Joseph Wedderburn and Leonard Dickson proved Wedderburn’s “Little” Theorem: that every finite ring with the zero-product property is a field. Which of them proved it first is hard to tell.</p>
<p>
Today we discuss the issue of finding simple proofs for simple facts—not just Wedderburn’s theorem but the zero-product property on which it leans.</p>
<p>
Dickson was a professor at the University of Chicago when Wedderburn visited on a Carnegie Fellowship in 1904–05. To <a href="https://en.wikipedia.org/wiki/Wedderburn's_little_theorem#History">judge</a> <a href="https://books.google.com/books?id=0CApaWJwry8C&amp;pg=PA318&amp;lpg=PA318&amp;dq=In+pursuit+of+the+finite+division+algebra+theorem+and+beyond&amp;source=bl&amp;ots=esoyfvVJAl&amp;sig=ACfU3U0N05-c467jVhDz1ElYLcsE4e_9Og&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjRvPv41_3fAhVOnuAKHdy3DgcQ6AEwBXoECAIQAQ#v=onepage&amp;q=In pursuit of the finite division algebra theorem and beyond&amp;f=false">from</a> <a href="https://books.google.com/books?id=O7R-DwAAQBAJ&amp;pg=PA105&amp;lpg=PA105&amp;dq=wedderburn+dickson+proofs&amp;source=bl&amp;ots=VdkQrceyrN&amp;sig=ACfU3U3u7Qn3ibDSszBPg0AitsrXUNUZpA&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwixkqzl0f3fAhWMTt8KHVo9DVgQ6AEwBnoECAUQAQ#v=onepage&amp;q=wedderburn dickson proofs&amp;f=false">several</a> <a href="https://api.research-repository.uwa.edu.au/portalfiles/portal/9699048/BambergPenttilaRevised.pdf">sources</a>, what happened is that Wedderburn first claimed a proof. Dickson did not believe the <em>result</em> and tried to build a counterexample. Instead he found a lemma that convinced him it was true and used it to give a simpler proof. They gave back-to-back presentations on 20 January, 1905, Wedderburn wrote up his paper with his proof and two more based on the lemma, which Wedderburn ascribed to an earlier paper by George Birkhoff and Harry Vandiver. Dickson wrote a paper with his approach, saying in a footnote:</p>
<blockquote><p><b> </b> <em> First proved by Wedderburn … Following my simpler proof above, and using the same lemma, Wedderburn constructed two further simpler proofs. </em>
</p></blockquote>
<p></p><p>
However, Wedderburn’s first proof was found to have a gap. As <a href="https://www.math.uni-bielefeld.de/lag/man/099.pdf">detailed</a> by Michael Adam and Birte Specht, the gap was a statement that was not false <em>per se</em> but whose vague argument used only properties from a class of weaker structures in which it can fail. So:</p>
<blockquote><p><b> </b> <em> Who first proved the theorem? </em>
</p></blockquote>
<p>
</p><p></p><h2> Other Proofs, Other Properties </h2><p></p>
<p></p><p>
Our sources linked above differ on whether the gap was noticed at the time or anytime before Emil Artin remarked on it in 1927. Artin didn’t discuss the gap but gave his own proof instead. There <a>seems</a> <a href="https://www.theoremoftheday.org/Docs/WedderburnShamil.pdf">to</a> <a href="https://ysharifi.wordpress.com/2011/09/24/wedderburns-little-theorem-1/">be</a> <a href="https://en.wikipedia.org/wiki/Wedderburn's_little_theorem#Proof">consensus</a> that the “proof from the Book” was <a href="http://www.math.leidenuniv.nl/~hfinkeln/seminarium/eindige_delingsring_is_lichaam.pdf">given</a> by Ernst Witt four years later in 1931. But two other proofs, <a href="https://www.jstor.org/stable/2311457?seq=1#page_scan_tab_contents">by</a> Israel Herstein and <a href="https://www.cambridge.org/core/journals/glasgow-mathematical-journal/article/grouptheoretic-proof-of-a-theorem-of-maclaganwedderburn/7BBA5D9EA9EDEE000BAE4859EDE64DA6">by</a> Hans Zassenhaus, receive prominent mention. </p>
<p>
As our sources attest, interest in finding other proofs continues. The surprise in the theorem, which is that finiteness and the zero-product property force the multiplication to be commutative, informs what happens in other mathematical structures. Proofs that draw on results about these structures create connections among many areas. The shortest proof drawing on deeper results was a two-page <a href="https://www.jstor.org/stable/2312328">paper</a> in the summer 1964 <em>Monthly</em> by Theodore Kaczynski (of <a href="https://www.fbi.gov/history/famous-cases/unabomber">ill</a> <a href="https://www.biography.com/news/unabomber-ted-kaczynski-today">note</a>). We won’t try to compare all these proofs, but will try to flip the question by focusing on the other property involved—the zero-product property:</p>
<blockquote><p><b> </b> <em> If <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Ccdot+b+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{a \cdot b = 0}" class="latex" title="{a \cdot b = 0}" /> then <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{a = 0}" class="latex" title="{a = 0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bb+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{b = 0}" class="latex" title="{b = 0}" />. </em>
</p></blockquote>
<p></p><p>
Given a ring <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> with this property, how easy is it to prove? If <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> has inverses then it is immediate, else multiplying both sides of <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Ccdot+b+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\cdot b = 0}" class="latex" title="{a\cdot b = 0}" /> by <img src="https://s0.wp.com/latex.php?latex=%7Ba%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a^{-1}}" class="latex" title="{a^{-1}}" /> in front and <img src="https://s0.wp.com/latex.php?latex=%7Bb%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b^{-1}}" class="latex" title="{b^{-1}}" /> in back would yield the contradiction <img src="https://s0.wp.com/latex.php?latex=%7B1+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 = 0}" class="latex" title="{1 = 0}" />. And we know if <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is finite then this property makes it a field. So our quest for a different angle leads us to thinking about infinite rings. </p>
<p>
Incidentally, a ring with the zero-product property is called a <em>domain</em>. If the multiplication is commutative then it is an <em>integral domain</em>, though Serge Lang preferred the term <em>entire ring</em>. Our example goes beyond the integers though.</p>
<p>
</p><p></p><h2> No Zero-Divisors I </h2><p></p>
<p></p><p>
A natural example that arises is the ring of integer polynomials over multiple variables. Thus for two variables <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> the elements of this ring are 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%2Cj%7D+c_%7Bij%7Dx%5E%7Bi%7Dy%5E%7Bj%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{i,j} c_{ij}x^{i}y^{j}. " class="latex" title="\displaystyle  \sum_{i,j} c_{ij}x^{i}y^{j}. " /></p>
<p>It is easy to see that they form a ring with the usual high school rules for adding and multiplying polynomials. The ring is an integral domain in general.</p>
<p>
To show this, we claim that it has no zero-divisors. Suppose that it does: let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29+%5Ccdot+g%28x%2Cy%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x,y) \cdot g(x,y) = 0}" class="latex" title="{f(x,y) \cdot g(x,y) = 0}" />. Then let 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Cy%29+%3D+%5Csum_%7Bk%7D+f_%7Bk%7D%28x%29+y%5E%7Bk%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x,y) = \sum_{k} f_{k}(x) y^{k}, " class="latex" title="\displaystyle  f(x,y) = \sum_{k} f_{k}(x) y^{k}, " /></p>
<p>and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%2Cy%29+%3D+%5Csum_%7Bk%7D+g_%7Bk%7D%28x%29+y%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x,y) = \sum_{k} g_{k}(x) y^{k}. " class="latex" title="\displaystyle  g(x,y) = \sum_{k} g_{k}(x) y^{k}. " /></p>
<p>Assume that the maximum degree in <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x,y)}" class="latex" title="{f(x,y)}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bdf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{df}" class="latex" title="{df}" /> and is <img src="https://s0.wp.com/latex.php?latex=%7Bdg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{dg}" class="latex" title="{dg}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(x,y)}" class="latex" title="{g(x,y)}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bdf%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{df}(x)}" class="latex" title="{f_{df}(x)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bg_%7Bdf%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g_{df}(x)}" class="latex" title="{g_{df}(x)}" /> are both non-zero polynomials in one variable. It is clear by induction that <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bdf%7D%28x%29+%5Ccdot+g_%7Bdg%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{df}(x) \cdot g_{dg}(x)}" class="latex" title="{f_{df}(x) \cdot g_{dg}(x)}" /> is not zero. This proves the claim.</p>
<p>
</p><p></p><h2> No Zero-Divisors II </h2><p></p>
<p></p><p>
The trouble with the above property of polynomials is that it is only about formal objects. In many applications to computer science we want to view polynomials as objects that can be evaluated. So a natural issue is a slightly different property: Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%2Cy%2C%5Cdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x,y,\dots)}" class="latex" title="{F(x,y,\dots)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%28x%2Cy%2C%5Cdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(x,y,\dots)}" class="latex" title="{G(x,y,\dots)}" /> are two integral polynomials. Suppose further that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28a%2Cb%2C%5Cdots%29+%5Ccdot+G%28a%2Cb%2C%5Cdots%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(a,b,\dots) \cdot G(a,b,\dots) = 0, " class="latex" title="\displaystyle  F(a,b,\dots) \cdot G(a,b,\dots) = 0, " /></p>
<p>for all integers <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b,\dots}" class="latex" title="{a,b,\dots}" />. Then it clearly must be the case that either <img src="https://s0.wp.com/latex.php?latex=%7BF%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F=0}" class="latex" title="{F=0}" /> identically or <img src="https://s0.wp.com/latex.php?latex=%7BG%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=0}" class="latex" title="{G=0}" /> identically. Right?</p>
<p>
The trouble is that this seems to be obvious but how do we prove that it is true? Note, it must be the case that some use of the fact that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> are polynomials. The fact is not true for more complex functions. For example, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos%28%5Cpi+x%29+%5Ccdot+%5Csin%28%5Cpi+x%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \cos(\pi x) \cdot \sin(\pi x) = 0, " class="latex" title="\displaystyle  \cos(\pi x) \cdot \sin(\pi x) = 0, " /></p>
<p>for all integers <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. But neither the function <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccos%28%5Cpi+x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cos(\pi x)}" class="latex" title="{\cos(\pi x)}" /> nor <img src="https://s0.wp.com/latex.php?latex=%7B%5Csin%28%5Cpi+x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sin(\pi x)}" class="latex" title="{\sin(\pi x)}" /> is identically zero for all integers.</p>
<p>
</p><p></p><h2> A Proof </h2><p></p>
<p></p><p>
Here is a relatively simple proof of the fact. It uses the famous Schwarz-Zippel (SZ) Theorem. See <a href="https://rjlipton.wordpress.com/2009/11/30/the-curious-history-of-the-schwartz-zippel-lemma/">here</a> for our discussion of the theorem. </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> be in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%5Bx_1%2Cx_2%2C%5Cldots%2Cx_n%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{F}[x_1,x_2,\ldots,x_n]}" class="latex" title="{\mathbb{F}[x_1,x_2,\ldots,x_n]}" /> be a non-zero polynomial of total degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> over a field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{F}}" class="latex" title="{\mathbb{F}}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> be a finite subset of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{F}}" class="latex" title="{\mathbb{F}}" /> and let <img src="https://s0.wp.com/latex.php?latex=%7Br_%7B1%7D%2C%5Cdots%2Cr_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{r_{1},\dots,r_{n}}" class="latex" title="{r_{1},\dots,r_{n}}" /> be selected at random independently and uniformly from <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />. Then 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr%5BP%28r_1%2Cr_2%2C%5Cldots%2Cr_n%29%3D0%5D+%5Cleq+%5Cfrac%7Bd%7D%7B%7CS%7C%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \Pr[P(r_1,r_2,\ldots,r_n)=0] \leq \frac{d}{|S|}. " class="latex" title="\displaystyle  \Pr[P(r_1,r_2,\ldots,r_n)=0] \leq \frac{d}{|S|}. " /></p>
</em><p><em></em>
</p></blockquote>
<p>
The S-Z lemma of course has manifest applications throughout complexity theory. Often it is used in design of randomized algorithms. What we found interesting is that here we use it for a different purpose: to prove a structural property of polynomials.</p>
<p>
Back to our fact. Assume that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28a%2Cb%29+%5Ccdot+G%28a%2Cb%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(a,b) \cdot G(a,b) = 0, " class="latex" title="\displaystyle  F(a,b) \cdot G(a,b) = 0, " /></p>
<p>for all integers <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a \in S}" class="latex" title="{a \in S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b \in S}" class="latex" title="{b \in S}" /> for some finite set. Now either <img src="https://s0.wp.com/latex.php?latex=%7BF%28a%2C+b%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(a, b)=0}" class="latex" title="{F(a, b)=0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BG%28a%2C+b%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(a, b)=0}" class="latex" title="{G(a, b)=0}" /> must be true for at least one half of the values in <img src="https://s0.wp.com/latex.php?latex=%7BS+%5Ctimes+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S \times S}" class="latex" title="{S \times S}" />. Assume that it is <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. Then by the S-Z theorem it must be that <img src="https://s0.wp.com/latex.php?latex=%7BF%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F=0}" class="latex" title="{F=0}" /> always if the set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> is large enough. Therefore, the fact is proved.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is there a simpler proof of this key fact? Is it possible to find an easier reference in the literature of this basic fact of polynomials? We have yet to discover one—any help would be appreciated.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/01/21/a-simple-fact/"><span class="datestr">at January 21, 2019 06:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-800609330387332388">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/01/acm-prize-and-some-thoughts-on-godel.html">ACM prize and some thoughts on the Godel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
<br />
As Lance Tweeted, and I will re-iterate, nominations for the following prizes<br />
are due soon and you can nominate people <a href="https://sigact.org/articles/prizes.html">here</a><br />
<br />
Godel Prize for outstanding paper in TCS. (Godel mentioned P vs NP in a letter to Von Neumann. I've heard it said that its too bad they didn't work on it-- either it would be solved or we'd know its hard. Frankly, I think enough smart people have worked on it that we already know its hard.)<br />
<br />
Knuth Prize for outstanding contributions to foundations of Computer Science. (its a greater honor to have a prize  named after you in your lifetime then to win a prize!)<br />
<br />
Dijkstra Prize (I wonder if having `ijk' in his name inspired him to work in algorithms)<br />
<br />
Kanellakis Theory and Practice Award.<br />
<br />
Lawler Award for Humanitarian Contributions within CS and Informatics.<br />
<br />
ACM Distinguished Service Award<br />
<br />
Danny Lewin Best Student Paper Award (best student paper at STOC)<br />
<br />
The Best Paper award (Best paper at STOC, Best paper at FOCS)<br />
<br />
(The last two I doubt you can nominate someone for.)<br />
<br />
A few thoughts on the Godel Prize:<br />
<div>
<br /></div>
<div>
<div>
<br /></div>
<div>
1) You can win the Godel Prize twice and some people have: Goldwasser, Hastad, Arora, Szegedy, Spielman, Teng. Spielman-Teng have won it as a team twice.</div>
<div>
<br /></div>
<div>
2) GLAD there is no limit to how many can win. If a paper has a lot of people on it (and this has happened) then FINE, they're all winners! According to The Big Bang Theory (the TV show, not the theory) in Physics at most 3 can win a Nobel Prize in Physics for the same breakthrough in a given year. The show itself shows how stupid the policy is.</div>
<div>
<br /></div>
<div>
3) I either knew and forgot or never knew that DPDA Equiv is decidable! Glad to now it just in time for teaching Automata theory this spring.</div>
<div>
<br /></div>
<div>
4) Looking over the list reminded me that there are some papers in the intersection of those I want to read and those I am able to read! Though not many. Most I want to read but they seem hard.</div>
<div>
<br /></div>
<div>
5) The Kanellakis award is for theory that is PRACTICAL. Could someone win a Godel AND a Kannellakis award for the same paper (or set of papers). I found one sort-of case ( (a) below) and one definite case ( (b) below).</div>
<div>
<br /></div>
<div>
a) Moshe and Wolper won the 2000 Godel Prize for Temporal Logic and Finite Automata (I should also read that before my class starts)</div>
<div>
<br /></div>
<div>
Holtzmann, Kurshan, Vardi, and Wolpert won the 2005 Kanellakis prize for Formal Verification Tools.</div>
<div>
<br /></div>
<div>
I assume the two works are related.</div>
<div>
<br /></div>
<div>
b) Freund and Schapire won the 2003 Godel Prize and the 2004 Kanellakis Award, both for their work on boosting in Machine Learning.</div>
<div>
<br /></div>
<div>
6) Why is it the  Godel <i>Prize </i>and the Kanellakis <i>Award? </i>What is the diff between a prize and an award? A quick Google Search says that an Award is a token of effort and merit, while a Prize is something you win in a competition. I doubt that applies. I suspect they are called Prize and Award from historical accident. Does anyone know?</div>
<div>
<br /></div>
</div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/01/acm-prize-and-some-thoughts-on-godel.html"><span class="datestr">at January 20, 2019 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
