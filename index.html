<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc ‚Äì QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science ‚Äì Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs ‚Äì Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I‚Äôm a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Caf√©: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 24, 2020 03:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7779">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/07/24/ryan-odonnels-tcs-toolkit-and-other-resources/">Ryan O‚ÄôDonnell‚Äôs ‚ÄúTCS Toolkit‚Äù and other resources</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>When I was in grad school a common advice for beginning grad students was to leaf through the (paper) STOC or FOCS proceedings to see papers that you are interested in. This is still a decent advice (and requires less physical strength these days <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="üôÇ" /> ) but papers are not always the best source for people starting out. More often than we‚Äôd like to, the author of the 10th paper on a topic writes it to the audience of people that read (in fact probably wrote) the previous 9 papers.</p>



<p>Talks often do a better job of giving an overview of the field, and one great resource is the <a href="https://simons.berkeley.edu/videos">videos</a> from the Simons Institute. If you want to get more in-depth information about a particular topic, it‚Äôs hard to beat the extended surveys in <a href="https://www.nowpublishers.com/TCS">Foundations and Trends in TCS</a>, as well as the related areas such as <a href="https://www.nowpublishers.com/MAL">Machine Learning</a> and <a href="https://www.nowpublishers.com/CIT">Information Theory</a>. </p>



<p>But if you are not yet sure what topic you‚Äôre interested in, or perhaps not even sure if you want to go to grad school, but you just know that you are interested in theory, there is now a new great resource. As I learned from <a href="https://twitter.com/BooleanAnalysis/status/1286658578049359873">Twitter</a>, Ryan O‚ÄôDonnell has just finished his <a href="https://www.diderot.one/course/28/">TCS Toolkit course</a>. All 99(!) lectures are on <a href="https://www.youtube.com/watch?v=prI35GmCon4&amp;list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX">YouTube</a>.</p>



<p>The topics are the following (these links are to the handwritten notes, for the lecture videos see <a href="https://www.youtube.com/watch?v=prI35GmCon4&amp;list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX">YouTube channel</a>):</p>



<p><a href="https://www.diderot.one/course/28/chapters/1824/">1. ¬† Course Overview, and How to TCS</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1881/">2. ¬† Basic Asymptotics</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1889/">3. ¬† Factorials and Binomial Coefficients</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1923/">4. ¬† Central Limit Theorem</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1956/">5. ¬† Chernoff Bounds</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1973/">6. ¬† Computational Models</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1981/">7. ¬† Fast Multiplication with the DFT</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1990/">8. ¬† Analysis of Boolean Functions</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2003/">9. ¬† Quantum Computation</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2037/">10. ¬† Fields and Polynomials</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2038/">11. ¬† Error-Correcting Codes</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2064/">12. ¬† Derandomization</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2065/">13. ¬† Spectral Graph Theory I</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2094/">14. ¬† Spectral Graph Theory II</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2099/">15. ¬† Spectral Graph Theory III</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2101/">15.1. ¬† Cheeger‚Äôs Inequality (Spectral Graph Theory bonus)</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2112/">16. ¬† Expander Graphs</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2149/">17. ¬† Linear Programming I</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2162/">18. ¬† Linear Programming II</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2173/">19. ¬† The Ellipsoid Algorithm</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2190/">20. ¬† CSPs and Approximation</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2197/">21. ¬† LP Hierarchies and Proof Systems</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2209/">22. ¬† Treewidth</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2227/">23. ¬† Communication Complexity</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2250/">24. ¬† Information Theory</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2251/">25. ¬† Cryptography</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2290/">26. ¬† Hardness Assumptions</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2291/">27. ¬† The PCP Theorem</a></p>



<p>p.s. For giving a high level taste of theory to beginning undergraduates, a great resource is Aaronson‚Äôs <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing since Democritus</a> or <a href="https://www.math.ias.edu/avi/book">Wigderson‚Äôs Math and Computation</a> if they‚Äôre more math inclined. </p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/07/24/ryan-odonnels-tcs-toolkit-and-other-resources/"><span class="datestr">at July 24, 2020 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.12120">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.12120">The Asymmetric Travelling Salesman Problem in Sparse Digraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>≈Åukasz Kowalik, Konrad Majewski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12120">PDF</a><br /><b>Abstract: </b>Asymmetric Travelling Salesman Problem (ATSP) and its special case Directed
Hamiltonicity are among the most fundamental problems in computer science. The
dynamic programming algorithm running in time $O^*(2^n)$ developed almost 60
years ago by Bellman, Held and Karp, is still the state of the art for both of
these problems.
</p>
<p>In this work we focus on sparse digraphs. First, we recall known approaches
for Undirected Hamiltonicity and TSP in sparse graphs and we analyse their
consequences for Directed Hamiltonicity and ATSP in sparse digraphs, either by
adapting the algorithm, or by using reductions. In this way, we get a number of
running time upper bounds for a few classes of sparse digraphs, including
$O^*(2^{n/3})$ for digraphs with both out- and indegree bounded by 2, and
$O^*(3^{n/2})$ for digraphs with outdegree bounded by 3.
</p>
<p>Our main results are focused on digraphs of bounded {\em average} outdegree
$d$. The baseline for ATSP here is a simple enumeration of cycle covers which
can be done in time bounded by $O^*(\mu(d)^n)$ for a function
$\mu(d)\le(\lceil{d}\rceil!)^{n/{\lceil{d}\rceil}}$. One can also observe that
Directed Hamiltonicity can be solved in randomized time $O^*((2-2^{-d})^n)$ and
polynomial space, by adapting a recent result of Bj\"{o}rklund [ISAAC 2018]
stated originally for Undirected Hamiltonicity in sparse bipartite graphs.
</p>
<p>We present two new deterministic algorithms: the first running in time
$O(2^{0.441(d-1)n})$ and polynomial space, and the second in exponential space
with running time of $O^*(\tau(d)^{n/2})$ for a function $\tau(d)\le d$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.12120"><span class="datestr">at July 24, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.12117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.12117">Silhouette Vectorization by Affine Scale-space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Yuchen.html">Yuchen He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Sung_Ha.html">Sung Ha Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morel:Jean=Michel.html">Jean-Michel Morel</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12117">PDF</a><br /><b>Abstract: </b>Silhouettes or 2D planar shapes are extremely important in human
communication, which involves many logos, graphics symbols and fonts in vector
form. Many more shapes can be extracted from image by binarization or
segmentation, thus in raster form that requires a vectorization. There is a
need for disposing of a mathematically well defined and justified shape
vectorization process, which in addition provides a minimal set of control
points with geometric meaning. In this paper we propose a silhouette
vectorization method which extracts the outline of a 2D shape from a raster
binary image, and converts it to a combination of cubic B\'{e}zier polygons and
perfect circles. Starting from the boundary curvature extrema computed at
sub-pixel level, we identify a set of control points based on the affine
scale-space induced by the outline. These control points capture similarity
invariant geometric features of the given silhouette and give precise locations
of the shape's corners.of the given silhouette. Then, piecewise B\'{e}zier
cubics are computed by least-square fitting combined with an adaptive splitting
to guarantee a predefined accuracy. When there are no curvature extrema
identified, either the outline is recognized as a circle using the
isoperimetric inequality, or a pair of the most distant outline points are
chosen to initiate the fitting. Given their construction, most of our control
points are geometrically stable under affine transformations. By comparing with
other feature detectors, we show that our method can be used as a reliable
feature point detector for silhouettes. Compared to state-of-the-art image
vectorization software, our algorithm demonstrates superior reduction on the
number of control points, while maintaining high accuracy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.12117"><span class="datestr">at July 24, 2020 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.12102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.12102">Faster algorithms for sampling connected induced subgraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bressan:Marco.html">Marco Bressan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12102">PDF</a><br /><b>Abstract: </b>We consider the problem of sampling connected induced $k$-node subgraphs
($k$-graphlets) uniformly at random from a given graph $G=(V,E)$. The best
algorithms known achieve $\epsilon$-uniformity and are based on random walks or
color coding. The random walk approach has a worst-case running time of
$\Delta^{\!\Theta(k)}\log\frac{n}{\epsilon}$ where $n=|V|$ and $\Delta$ is the
maximum degree of $G$, while color coding requires a preprocessing that takes
time and space $2^{\Theta(k)}O(m \log\frac{1}{\epsilon})$ where $m=|E|$. Our
main result is an algorithm for $\epsilon$-uniform sampling with the following
guarantees. The preprocessing runs in time $k^{O(k)}O\big(\frac{1}{\epsilon}\,
n \log n \big)$ and space $O(n)$. After, the algorithm yields independent
$\epsilon$-uniform $k$-graphlets in
$k^{{O}(k)}{O}\big((\frac{1}{\epsilon})^{10}\log \frac{1}{\epsilon} \big)$
expected time per sample. The preprocessing phase computes in one pass an
approximate ordering of $G$ that makes rejection sampling efficient in the
sampling phase, and the $\epsilon$-uniformity is based on estimating cuts and
coupling arguments. In fact, the algorithm derives from a much simpler
algorithm which has ${O}(m \log \Delta)$ preprocessing time and returns
perfectly uniform $k$-graphlets from $G$ in $k^{{O}(k)}{O}(\log \Delta)$
expected time per sample. In addition, we give an almost-tight bound for the
random walk technique. More precisely, we show that the most commonly used
random walk has mixing time $k^{{O}(k)} {O}(t(G)(\frac{\Delta}{\delta})^{k-1}\!
\log n)$ where $t(G)$ is the mixing time of $G$ and $\delta$ is its minimum
degree. This improves on recent results and is tight up to a factor $k^{{O}(k)}
\delta \log n$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.12102"><span class="datestr">at July 24, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.12077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.12077">Detecting and Enumerating Small Induced Subgraphs in $c$-Closed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koana:Tomohiro.html">Tomohiro Koana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nichterlein:Andr=eacute=.html">Andr√© Nichterlein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12077">PDF</a><br /><b>Abstract: </b>Fox et al. [SIAM J. Comp. 2020] introduced a new parameter, called
$c$-closure, for a parameterized study of clique enumeration problems. A graph
$G$ is $c$-closed if every pair of vertices with at least $c$ common neighbors
is adjacent. The $c$-closure of $G$ is the smallest $c$ such that $G$ is
$c$-closed. We systematically explore the impact of $c$-closure on the
computational complexity of detecting and enumerating small induced subgraphs.
More precisely, for each graph $H$ on three or four vertices, we investigate
parameterized polynomial-time algorithms for detecting $H$ and for enumerating
all occurrences of $H$ in a given $c$-closed graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.12077"><span class="datestr">at July 24, 2020 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.12048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.12048">Lower Bounds and Hardness Magnification for Sublinear-Time Shrinking Cellular Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Modanese:Augusto.html">Augusto Modanese</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12048">PDF</a><br /><b>Abstract: </b>The minimum circuit size problem (MCSP) is a string compression problem with
a parameter $s$ in which, given the truth table of a Boolean function over
inputs of length $n$, one must answer whether it can be computed by a Boolean
circuit of size at most $s(n) \ge n$. Recently, McKay, Murray, and Williams
(STOC, 2019) proved a hardness magnification result for MCSP involving
(one-pass) streaming algorithms: For any reasonable $s$, if there is no
$\mathsf{poly}(s(n))$-space streaming algorithm with $\mathsf{poly}(s(n))$
update time for $\mathsf{MCSP}[s]$, then $\mathsf{P} \neq \mathsf{NP}$. We
prove an equivalent result for the (provably) strictly less capable model of
shrinking cellular automata (SCAs), which are cellular automata whose cells can
spontaneously delete themselves. We show every language accepted by an SCA can
also be accepted by a streaming algorithm of similar complexity, and we
identify two different aspects in which SCAs are more restricted than streaming
algorithms. We also show there is a language which cannot be accepted by any
SCA in $o(n / \log n)$ time, even though it admits an $O(\log n)$-space
streaming algorithm with $O(\log n)$ update time, where $n$ is the input
length.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.12048"><span class="datestr">at July 24, 2020 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11997">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11997">Total Domination in Unit Disk Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jena:Sangram_K=.html">Sangram K. Jena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Gautam_K=.html">Gautam K. Das</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11997">PDF</a><br /><b>Abstract: </b>Let $G=(V,E)$ be an undirected graph. We call $D_t \subseteq V$ as a total
dominating set (TDS) of $G$ if each vertex $v \in V$ has a dominator in $D$
other than itself. Here we consider the TDS problem in unit disk graphs, where
the objective is to find a minimum cardinality total dominating set for an
input graph. We prove that the TDS problem is NP-hard in unit disk graphs.
Next, we propose an 8-factor approximation algorithm for the problem. The
running time of the proposed approximation algorithm is $O(n \log k)$, where
$n$ is the number of vertices of the input graph and $k$ is output size. We
also show that TDS problem admits a PTAS in unit disk graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11997"><span class="datestr">at July 24, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11964">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11964">Sign-curing local Hamiltonians: termwise versus global stoquasticity and the use of Clifford transformations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ioannou:Marios.html">Marios Ioannou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Piddock:Stephen.html">Stephen Piddock</a>, Milad Marvian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klassen:Joel.html">Joel Klassen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Terhal:Barbara_M=.html">Barbara M. Terhal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11964">PDF</a><br /><b>Abstract: </b>We elucidate the distinction between global and termwise stoquasticity for
local Hamiltonians and prove several complexity results. We prove coNP-hardness
of deciding global stoquasticity in a fixed basis and $\Sigma_2^p$-hardness of
deciding global stoquasticity under single-qubit transformations. We expand the
class of sign-curing transformations by showing how Clifford transformations
can sign-cure a class of disordered 1D XYZ Hamiltonians.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11964"><span class="datestr">at July 24, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11868">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11868">Two-way Greedy: Algorithms for Imperfect Rationality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferraioli:Diodato.html">Diodato Ferraioli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penna:Paolo.html">Paolo Penna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Ventre:Carmine.html">Carmine Ventre</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11868">PDF</a><br /><b>Abstract: </b>The realization that selfish interests need to be accounted for in the design
of algorithms has produced many contributions in computer science under the
umbrella of algorithmic mechanism design. Novel algorithmic properties and
paradigms have been identified and studied. Our work stems from the observation
that selfishness is different from rationality; agents will attempt to
strategize whenever they perceive it to be convenient according to their
imperfect rationality. Recent work has focused on a particular notion of
imperfect rationality, namely absence of contingent reasoning skills, and
defined obvious strategyproofness (OSP) as a way to deal with the selfishness
of these agents. Essentially, this definition states that to care for the
incentives of these agents, we need not only pay attention about the
relationship between input and output, but also about the way the algorithm is
run. However, it is not clear what algorithmic approaches must be used for OSP.
In this paper, we show that, for binary allocation problems, OSP is fully
captured by a combination of two well-known algorithmic techniques: forward and
reverse greedy. We call two-way greedy this algorithmic design paradigm. Our
main technical contribution establishes the connection between OSP and two-way
greedy. We build upon the recently introduced cycle monotonicity technique for
OSP. By means of novel structural properties of cycles and queries of OSP
mechanisms, we fully characterize these mechanisms in terms of extremal
implementations. These are protocols that ask each agent to consistently
separate one extreme of their domain at the current history from the rest.
Through the connection with the greedy paradigm, we are able to import a host
of approximation bounds to OSP and strengthen the strategic properties of this
family of algorithms. Finally, we begin exploring the power of two-way greedy
for set systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11868"><span class="datestr">at July 24, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11863">Plane augmentation of plane graphs to meet parity constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>J. C. Catana, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garc=iacute=a:A=.html">A. Garc√≠a</a>, J. Tejel, J. Urrutia <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11863">PDF</a><br /><b>Abstract: </b>A plane topological graph $G=(V,E)$ is a graph drawn in the plane whose
vertices are points in the plane and whose edges are simple curves that do not
intersect, except at their endpoints. Given a plane topological graph $G=(V,E)$
and a set $C_G$ of parity constraints, in which every vertex has assigned a
parity constraint on its degree, either even or odd, we say that $G$ is
\emph{topologically augmentable} to meet $C_G$ if there exits a plane
topological graph $H$ on the same set of vertices, such that $G$ and $H$ are
edge-disjoint and their union is a plane topological graph that meets all
parity constraints.
</p>
<p>In this paper, we prove that the problem of deciding if a plane topological
graph is topologically augmentable to meet parity constraints is
$\mathcal{NP}$-complete, even if the set of vertices that must change their
parities is $V$ or the set of vertices with odd degree. In particular, deciding
if a plane topological graph can be augmented to a Eulerian plane topological
graph is $\mathcal{NP}$-complete. Analogous complexity results are obtained,
when the augmentation must be done by a plane topological perfect matching
between the vertices not meeting their parities.
</p>
<p>We extend these hardness results to planar graphs, when the augmented graph
must be planar, and to plane geometric graphs (plane topological graphs whose
edges are straight-line segments). In addition, when it is required that the
augmentation is made by a plane geometric perfect matching between the vertices
not meeting their parities, we also prove that this augmentation problem is
$\mathcal{NP}$-complete for plane geometric trees and paths.
</p>
<p>For the particular family of maximal outerplane graphs, we characterize
maximal outerplane graphs that are topological augmentable to satisfy a set of
parity constraints.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11863"><span class="datestr">at July 24, 2020 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11773">FPT Approximation for Constrained Metric $k$-Median/Means</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Dishant.html">Dishant Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaiswal:Ragesh.html">Ragesh Jaiswal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11773">PDF</a><br /><b>Abstract: </b>The Metric $k$-median problem over a metric space $(\mathcal{X}, d)$ is
defined as follows: given a set $L \subseteq \mathcal{X}$ of facility locations
and a set $C \subseteq \mathcal{X}$ of clients, open a set $F \subseteq L$ of
$k$ facilities such that the total service cost, defined as $\Phi(F, C) \equiv
\sum_{x \in C} \min_{f \in F} d(x, f)$, is minimised. The metric $k$-means
problem is defined similarly using squared distances. In many applications
there are additional constraints that any solution needs to satisfy. This gives
rise to different constrained versions of the problem such as $r$-gather,
fault-tolerant, outlier $k$-means/$k$-median problem. Surprisingly, for many of
these constrained problems, no constant-approximation algorithm is known. We
give FPT algorithms with constant approximation guarantee for a range of
constrained $k$-median/means problems. For some of the constrained problems,
ours is the first constant factor approximation algorithm whereas for others,
we improve or match the approximation guarantee of previous works. We work
within the unified framework of Ding and Xu that allows us to simultaneously
obtain algorithms for a range of constrained problems. In particular, we obtain
a $(3+\varepsilon)$-approximation and $(9+\varepsilon)$-approximation for the
constrained versions of the $k$-median and $k$-means problem respectively in
FPT time. In many practical settings of the $k$-median/means problem, one is
allowed to open a facility at any client location, i.e., $C \subseteq L$. For
this special case, our algorithm gives a $(2+\varepsilon)$-approximation and
$(4+\varepsilon)$-approximation for the constrained versions of $k$-median and
$k$-means problem respectively in FPT time. Since our algorithm is based on
simple sampling technique, it can also be converted to a constant-pass
log-space streaming algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11773"><span class="datestr">at July 24, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11651">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11651">R*-Grove: Balanced Spatial Partitioning for Large-scale Datasets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vu:Tin.html">Tin Vu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eldawy:Ahmed.html">Ahmed Eldawy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11651">PDF</a><br /><b>Abstract: </b>The rapid growth of big spatial data urged the research community to develop
several big spatial data systems. Regardless of their architecture, one of the
fundamental requirements of all these systems is to spatially partition the
data efficiently across machines. The core challenges of big spatial
partitioning are building high spatial quality partitions while simultaneously
taking advantages of distributed processing models by providing load balanced
partitions. Previous works on big spatial partitioning are to reuse existing
index search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree,
by building a temporary tree for a sample of the input and use its leaf nodes
as partition boundaries. However, we show in this paper that none of those
techniques has addressed the mentioned challenges completely. This paper
proposes a novel partitioning method, termed R*-Grove, which can partition very
large spatial datasets into high quality partitions with excellent load balance
and block utilization. This appealing property allows R*-Grove to outperform
existing techniques in spatial query processing. R*-Grove can be easily
integrated into any big data platforms such as Apache Spark or Apache Hadoop.
Our experiments show that R*-Grove outperforms the existing partitioning
techniques for big spatial data systems. With all the proposed work publicly
available as open source, we envision that R*-Grove will be adopted by the
community to better serve big spatial data research.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11651"><span class="datestr">at July 24, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11643">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11643">FPT-space Graph Kernelizations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kammer:Frank.html">Frank Kammer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sajenko:Andrej.html">Andrej Sajenko</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11643">PDF</a><br /><b>Abstract: </b>Let $n$ be the size of a parametrized problem and $k$ the parameter. We
present polynomial-time kernelizations for Cluster Editing/Deletion, Path
Contractions and Feedback Vertex Set that run with $O(\mathrm{poly}(k) \log n)$
bits and compute a kernel of size polynomial in $k$. By first executing the new
kernelizations and subsequently the best known polynomial-time kernelizations
for the problem under consideration, we obtain the best known kernels in
polynomial time with $O(\mathrm{poly}(k) \log n)$ bits.
</p>
<p>Our kernelization for Feedback Vertex Set computes in a first step an
approximated solution, which can be used to build a simple algorithm for
undirected $s$-$t$-connectivity (USTCON) that runs in polynomial time and with
$O(\mathrm{poly}(k) \log n)$ bits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11643"><span class="datestr">at July 24, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11636">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11636">Light Euclidean Spanners with Steiner Points</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11636">PDF</a><br /><b>Abstract: </b>The FOCS'19 paper of Le and Solomon, culminating a long line of research on
Euclidean spanners, proves that the lightness (normalized weight) of the greedy
$(1+\epsilon)$-spanner in $\mathbb{R}^d$ is $\tilde{O}(\epsilon^{-d})$ for any
$d = O(1)$ and any $\epsilon = \Omega(n^{-\frac{1}{d-1}})$ (where $\tilde{O}$
hides polylogarithmic factors of $\frac{1}{\epsilon}$), and also shows the
existence of point sets in $\mathbb{R}^d$ for which any $(1+\epsilon)$-spanner
must have lightness $\Omega(\epsilon^{-d})$. Given this tight bound on the
lightness, a natural arising question is whether a better lightness bound can
be achieved using Steiner points.
</p>
<p>Our first result is a construction of Steiner spanners in $\mathbb{R}^2$ with
lightness $O(\epsilon^{-1} \log \Delta)$, where $\Delta$ is the spread of the
point set. In the regime of $\Delta \ll 2^{1/\epsilon}$, this provides an
improvement over the lightness bound of Le and Solomon [FOCS 2019]; this regime
of parameters is of practical interest, as point sets arising in real-life
applications (e.g., for various random distributions) have polynomially bounded
spread, while in spanner applications $\epsilon$ often controls the precision,
and it sometimes needs to be much smaller than $O(1/\log n)$. Moreover, for
spread polynomially bounded in $1/\epsilon$, this upper bound provides a
quadratic improvement over the non-Steiner bound of Le and Solomon [FOCS 2019],
We then demonstrate that such a light spanner can be constructed in
$O_{\epsilon}(n)$ time for polynomially bounded spread, where $O_{\epsilon}$
hides a factor of $\mathrm{poly}(\frac{1}{\epsilon})$. Finally, we extend the
construction to higher dimensions, proving a lightness upper bound of
$\tilde{O}(\epsilon^{-(d+1)/2} + \epsilon^{-2}\log \Delta)$ for any $3\leq d =
O(1)$ and any $\epsilon = \Omega(n^{-\frac{1}{d-1}})$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11636"><span class="datestr">at July 24, 2020 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11582">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11582">The importance of the spectral gap in estimating ground-state energies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deshpande:Abhinav.html">Abhinav Deshpande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gorshkov:Alexey_V=.html">Alexey V. Gorshkov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fefferman:Bill.html">Bill Fefferman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11582">PDF</a><br /><b>Abstract: </b>The field of quantum Hamiltonian complexity lies at the intersection of
quantum many-body physics and computational complexity theory, with deep
implications to both fields. The main object of study is the LocalHamiltonian
problem, which is concerned with estimating the ground-state energy of a local
Hamiltonian and is complete for the class QMA, a quantum generalization of the
class NP. A major challenge in the field is to understand the complexity of the
LocalHamiltonian problem in more physically natural parameter regimes. One
crucial parameter in understanding the ground space of any Hamiltonian in
many-body physics is the spectral gap, which is the difference between the
smallest two eigenvalues. Despite its importance in quantum many-body physics,
the role played by the spectral gap in the complexity of the LocalHamiltonian
is less well-understood. In this work, we make progress on this question by
considering the precise regime, in which one estimates the ground-state energy
to within inverse exponential precision. Computing ground-state energies
precisely is a task that is important for quantum chemistry and quantum
many-body physics.
</p>
<p>In the setting of inverse-exponential precision, there is a surprising result
that the complexity of LocalHamiltonian is magnified from QMA to PSPACE, the
class of problems solvable in polynomial space. We clarify the reason behind
this boost in complexity. Specifically, we show that the full complexity of the
high precision case only comes about when the spectral gap is exponentially
small. As a consequence of the proof techniques developed to show our results,
we uncover important implications for the representability and circuit
complexity of ground states of local Hamiltonians, the theory of uniqueness of
quantum witnesses, and techniques for the amplification of quantum witnesses in
the presence of postselection.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11582"><span class="datestr">at July 24, 2020 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4916">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4916">The Busy Beaver Frontier</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A life that was all covid, cancellations, and Trump, all desperate rearguard defense of the beleaguered ideals of the Enlightenment, would hardly be worth living.  So it was an exquisite delight, these past two weeks, to forget current events and write an <a href="https://www.scottaaronson.com/papers/bb.pdf">18-page survey article</a> about the <a href="https://en.wikipedia.org/wiki/Busy_beaver">Busy Beaver function</a>: the staggeringly quickly-growing function that probably encodes a huge portion of all interesting mathematical truth in its first hundred values, if only we could know those values or exploit them if we did.</p>



<p>Without further ado, here‚Äôs the title, abstract, and link:</p>



<blockquote class="wp-block-quote"><p><a href="https://www.scottaaronson.com/papers/bb.pdf"><strong>The Busy Beaver Frontier</strong></a><br />by Scott Aaronson</p><p>The Busy Beaver function, with its incomprehensibly rapid growth, has captivated generations of computer scientists, mathematicians, and hobbyists.  In this survey, I offer a personal view of the BB function 58 years after its introduction, emphasizing lesser-known insights, recent progress, and especially favorite open problems.  Examples of such problems include: when does the BB function first exceed the Ackermann function?  Is the value of BB(20) independent of set theory?  Can we prove that BB(n+1)&gt;2<sup>BB(n)</sup> for large enough n?  Given BB(n), how many advice bits are needed to compute BB(n+1)?  Do all Busy Beavers halt on all inputs, not just the 0 input?  Is it decidable whether BB(n) is even or odd?</p></blockquote>



<p>The article is slated to appear soon in <em>SIGACT News</em>.  I‚Äôm grateful to Bill Gasarch for suggesting it‚Äîeven with everything else going on, this was a commission I felt I couldn‚Äôt turn down!</p>



<p>Besides Bill, I‚Äôm grateful to the various Busy Beaver experts who answered my inquiries, to Marijn Heule and Andy Drucker for suggesting some of the open problems, to Marijn for creating a figure, and to Lily, my 7-year-old daughter, for raising the question about the first value of n at which the Busy Beaver function exceeds the Ackermann function.  (Yes, Lily‚Äôs covid homeschooling has included multiple lessons on very large positive integers.)</p>



<p>There are still a few days until I have to deliver the final version. So if you spot anything wrong or in need of improvement, don‚Äôt hesitate to leave a comment or send an email.  Thanks in advance!</p>



<p>Of course Busy Beaver has been an obsession that I‚Äôve returned to many times in my life: for example, in that <a href="https://www.scottaaronson.com/writings/bignumbers.html">Who Can Name the Bigger Number?</a> essay that I wrote way back when I was 18, in <em><a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a></em>, in my <a href="https://www.scottaaronson.com/blog/?p=3445">public lecture at Festivaletteratura</a>, and in my <a href="https://www.scottaaronson.com/blog/?p=2725">2016 paper with Adam Yedidia</a> that showed that the values of all Busy Beaver numbers beyond the 7910<sup>th</sup> are independent of the axioms of set theory (Stefan O‚ÄôRear has since shown that independence starts at the 748<sup>th</sup> value or sooner).  This survey, however, represents the first time I‚Äôve tried to take stock of BusyBeaverology <em>as a research topic</em>‚Äîcollecting in one place all the lesser-known theorems and empirical observations and open problems that I found the most striking, in the hope of inspiring not just contemplation or wonderment but actual progress.</p>



<p>Within the last few months, the world of <em>deep mathematics that you can actually explain to a child</em> lost two of its greatest giants: <a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John Conway</a> (who died of covid, and who I <a href="https://www.scottaaronson.com/blog/?p=4732">eulogized here</a>) and <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ron Graham</a>.  One thing I found poignant, and that I didn‚Äôt know before I started writing, is that Conway and Graham <em>both</em> play significant roles in the story of the Busy Beaver function.  Conway, because most of the best known candidates for Busy Beaver Turing machines turn out, when you analyze them, to be testing variants of the notorious <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz Conjecture</a>‚Äîand Conway is the one who proved, in 1972, that the set of ‚ÄúCollatz-like questions‚Äù is Turing-undecidable.  And Graham because of <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham‚Äôs number</a> from <a href="https://en.wikipedia.org/wiki/Ramsey_theory">Ramsey theory</a>‚Äîa candidate for the biggest number that‚Äôs ever played a role in mathematical research‚Äîand because of the <a href="https://googology.wikia.org/wiki/User_blog:Wythagoras/The_nineteenth_Busy_Beaver_number_is_greater_than_Graham%27s_Number!">discovery</a>, four years ago, that the 18<sup>th</sup> Busy Beaver number exceeds Graham‚Äôs number.</p>



<p>(‚ÄúJust how big is Graham‚Äôs number?  So big that the <em>17<sup>th</sup> Busy Beaver number</em> is not yet known to exceed it!‚Äù)</p>



<p>Anyway, I tried to make the survey pretty accessible, while still providing enough technical content to sink one‚Äôs two overgrown front teeth into (don‚Äôt worry, there are no such puns in the piece itself).  I hope you like reading it at least 1/BB(10) as much as I liked writing it.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (July 24):</span></strong> Longtime commenter Joshua Zelinsky gently reminded me that one of the main questions discussed in the survey‚Äînamely, whether we can prove BB(n+1)&gt;2<sup>BB(n)</sup> for all large enough n‚Äîwas <a href="https://www.scottaaronson.com/blog/?p=1385#comment-73298">first brought to my attention</a> by him, Joshua, in a 2013 Ask-Me-Anything session on this blog!  I apologize to Joshua for the major oversight, which has now been corrected.  On the positive side, we just got a powerful demonstration <em>both</em> of the intellectual benefits of blogging, and of the benefits of sharing paper drafts on one‚Äôs blog before sending them to the editor!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4916"><span class="datestr">at July 23, 2020 06:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/">Tenure-track or Tenured Faculty Positions at Center on Frontiers of Computing Studies, Peking University (apply by September 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Center on Frontiers of Computing Studies (CFCS), Peking University (PKU), China, is a university new initiative co-founded by Professors John Hopcroft and Wen Gao.</p>
<p>We are seeking applicants from all areas of Computer Science, spanning theoretical foundations, systems, software, and applications, with special interests in artificial intelligence and machine learning.</p>
<p>Website: <a href="https://cfcs.pku.edu.cn/english/people/joinus/236979.htm">https://cfcs.pku.edu.cn/english/people/joinus/236979.htm</a><br />
Email: cfcs_recruiting@pku.edu.cn</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/"><span class="datestr">at July 23, 2020 02:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11559">An Improved Approximation Algorithm for the Matching Augmentation Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>J. Cheriyan, R. Cummings, J. Dippel, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:J=.html">J. Zhu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11559">PDF</a><br /><b>Abstract: </b>We present a $\frac53$-approximation algorithm for the matching augmentation
problem (MAP): given a multi-graph with edges of cost either zero or one such
that the edges of cost zero form a matching, find a 2-edge connected spanning
subgraph (2-ECSS) of minimum cost.
</p>
<p>A $\frac74$-approximation algorithm for the same problem was presented
recently, see Cheriyan, et al., "The matching augmentation problem: a
$\frac{7}{4}$-approximation algorithm," {\em Math. Program.}, 182(1):315--354,
2020; <a href="http://export.arxiv.org/abs/1810.07816">arXiv:1810.07816</a>.
</p>
<p>Our improvement is based on new algorithmic techniques, and some of these may
lead to advances on related problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11559"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11532">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11532">Online Adaptive Bin Packing with Overflow</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perez=Salazar:Sebastian.html">Sebastian Perez-Salazar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Mohit.html">Mohit Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Toriello:Alejandro.html">Alejandro Toriello</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11532">PDF</a><br /><b>Abstract: </b>Motivated by bursty bandwidth allocation and by the allocation of virtual
machines into servers in the cloud, we consider the online problem of packing
items with random sizes into unit-capacity bins. Items arrive sequentially, but
upon arrival an item's actual size is unknown; only its probabilistic
information is available to the decision maker. Without knowing this size, the
decision maker must irrevocably pack the item into an available bin or place it
in a new bin. Once packed in a bin, the decision maker observes the item's
actual size, and overflowing the bin is a possibility. An overflow incurs a
large penalty cost and the corresponding bin is unusable for the rest of the
process. In practical terms, this overflow models delayed services, failure of
servers, and/or loss of end-user goodwill. The objective is to minimize the
total expected cost given by the sum of the number of opened bins and the
overflow penalty cost. We present an online algorithm with expected cost at
most a constant factor times the cost incurred by the optimal packing policy
when item sizes are drawn from an i.i.d. sequence of unknown length. We give a
similar result when item size distributions are exponential with arbitrary
rates.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11532"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11495">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11495">Improved Distance Sensitivity Oracles with Subcubic Preprocessing Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ren:Hanlin.html">Hanlin Ren</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11495">PDF</a><br /><b>Abstract: </b>We consider the problem of building Distance Sensitivity Oracles (DSOs).
Given a directed graph $G=(V, E)$ with edge weights in $\{1, 2, \dots, M\}$, we
need to preprocess it into a data structure, and answer the following queries:
given vertices $u,v\in V$ and a failed vertex or edge $f\in (V\cup E)$, output
the length of the shortest path from $u$ to $v$ that does not go through $f$.
Our main result is a simple DSO with $\tilde{O}(n^{2.7233}M)$ preprocessing
time and $O(1)$ query time. Moreover, if the input graph is undirected, the
preprocessing time can be improved to $\tilde{O}(n^{2.6865}M)$. The
preprocessing algorithm is randomized with correct probability $\ge 1-1/n^C$,
for a constant $C$ that can be made arbitrarily large. Previously, there is a
DSO with $\tilde{O}(n^{2.8729}M)$ preprocessing time and
$\operatorname{polylog}(n)$ query time [Chechik and Cohen, STOC'20].
</p>
<p>At the core of our DSO is the following observation from [Bernstein and
Karger, STOC'09]: if there is a DSO with preprocessing time $P$ and query time
$Q$, then we can construct a DSO with preprocessing time $P+\tilde{O}(n^2)\cdot
Q$ and query time $O(1)$. (Here $\tilde{O}(\cdot)$ hides
$\operatorname{polylog}(n)$ factors.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11495"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11476">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11476">Approximate Covering with Lower and Upper Bounds via LP Rounding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Aniket_Basu.html">Aniket Basu Roy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11476">PDF</a><br /><b>Abstract: </b>In this paper, we study the lower- and upper-bounded covering (LUC) problem,
where we are given a set $P$ of $n$ points, a collection $\mathcal{B}$ of
balls, and parameters $L$ and $U$. The goal is to find a minimum-sized subset
$\mathcal{B}'\subseteq \mathcal{B}$ and an assignment of the points in $P$ to
$\mathcal{B}'$, such that each point $p\in P$ is assigned to a ball that
contains $p$ and for each ball $B_i\in \mathcal{B}'$, at least $L$ and at most
$U$ points are assigned to $B_i$. We obtain an LP rounding based constant
approximation for LUC by violating the lower and upper bound constraints by
small constant factors and expanding the balls by again a small constant
factor. Similar results were known before for covering problems with only the
upper bound constraint. We also show that with only the lower bound constraint,
the above result can be obtained without any lower bound violation.
</p>
<p>Covering problems have close connections with facility location problems. We
note that the known constant-approximation for the corresponding lower- and
upper-bounded facility location problem, violates the lower and upper bound
constraints by a constant factor.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11476"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11451">Point-Location in The Arrangement of Curves</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aghamolaei:Sepideh.html">Sepideh Aghamolaei</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11451">PDF</a><br /><b>Abstract: </b>An arrangement of $n$ curves in the plane is given. The query is a point $q$
and the goal is to find the face of the arrangement that contains $q$. A
data-structure for point-location, preprocesses the curves into a data
structure of polynomial size in $n$, such that the queries can be answered in
time polylogarithmic in $n$.
</p>
<p>We design a data structure for solving the point location problem queries in
$O(\log C(n)+\log S(n))$ time using $O(T(n)+S(n)\log(S(n)))$ preprocessing
time, if a polygonal subdivision of total size $S(n)$, with cell complexity at
most $C(n)$ can be computed in time $T(n)$, such that the order of the parts of
the curves inside each cell has a monotone order with respect to at least one
segment of the boundary of the cell. We call such a partitioning a
curve-monotone polygonal subdivision.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11451"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11410">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11410">Sum-of-squares chordal decomposition of polynomial matrix inequalities</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Yang.html">Yang Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fantuzzi:Giovanni.html">Giovanni Fantuzzi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11410">PDF</a><br /><b>Abstract: </b>We prove three decomposition results for sparse positive (semi-)definite
polynomial matrices. First, we show that a polynomial matrix $P(x)$ with
chordal sparsity is positive semidefinite for all $x\in \mathbb{R}^n$ if and
only if there exists a sum-of-squares (SOS) polynomial $\sigma(x)$ such that
$\sigma(x)P(x)$ can be decomposed into a sum of sparse SOS matrices, each of
which is zero outside a small principal submatrix. Second, we establish that
setting $\sigma(x)=(x_1^2 + \cdots + x_n^2)^\nu$ for some integer $\nu$
suffices if $P(x)$ is even, homogeneous, and positive definite. Third, we prove
a sparse-matrix version of Putinar's Positivstellensatz: if $P(x)$ has chordal
sparsity and is positive definite on a compact semialgebraic set
$\mathcal{K}=\{x:g_1(x)\geq 0,\ldots,g_m(x)\geq 0\}$ satisfying the Archimedean
condition, then $P(x) = S_0(x) + g_1(x)S_1(x) + \cdots + g_m(x)S_m(x)$ for
matrices $S_i(x)$ that are sums of sparse SOS matrices, each of which is zero
outside a small principal submatrix. Using these decomposition results, we
obtain sparse SOS representation theorems for polynomials that are quadratic
and correlatively sparse in a subset of variables. We also obtain new
convergent hierarchies of sparsity-exploiting SOS reformulations to convex
optimization problems with large and sparse polynomial matrix inequalities.
Analytical examples illustrate all our decomposition results, while large-scale
numerical examples demonstrate that the corresponding sparsity-exploiting SOS
hierarchies have significantly lower computational complexity than traditional
ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11410"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11402">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11402">Independent Set on C$_{\geq k}$-Free Graphs in Quasi-Polynomial Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gartland:Peter.html">Peter Gartland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11402">PDF</a><br /><b>Abstract: </b>We give an algorithm that takes as input a graph $G$ with weights on the
vertices and outputs a maximum weight independent set of $G$. If $G$ does not
contain any cycle on $k$ or more vertices as an induced subgraph ($G$ is a
$C_{\geq k}$-free graph), the algorithm runs in time $n^{O(k^3 \log^5 n)}$, and
therefore for fixed $k$ is a quasi-polynomial time algorithm. $C_{\geq 4}$-free
graphs (also known as chordal graphs) have a well known polynomial time
algorithm. A subexponetial time algorithm for $C_{\geq 5}$-free graphs (also
known as long-hole-free graphs) was found in 2019 [Chudnovsky et al., Arxiv'19]
followed by a polynomial time algorithm for $C_{\geq 5}$-free graphs in 2020
[Abrishami et al., Arxiv'20]. For $k &gt; 5$ only a quasi-polynomial time
approximation scheme [Chudnovsky et al., SODA'20] was known. Our work is the
first to exhibit conclusive evidence that Independent Set on $C_{\geq k}$-free
graphs is not NP-complete for any integer $k$. This also generalizes previous
work of ours [Gartland and Lokshtanov, FOCS'20], with an additional factor of
$\log^2(n)$ in the exponent, where we provided a quasi-polynomial time
algorithm for graphs that exclude a path on $k$ vertices as an induced
subgraph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11402"><span class="datestr">at July 23, 2020 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11398">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11398">A Framework for Consistency Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chini:Peter.html">Peter Chini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saivasan:Prakash.html">Prakash Saivasan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11398">PDF</a><br /><b>Abstract: </b>We present a framework that provides deterministic consistency algorithms for
given memory models. Such an algorithm checks whether the executions of a
shared-memory concurrent program are consistent under the axioms defined by a
model. For memory models like SC and TSO, checking consistency is NP-complete.
Our framework shows, that despite the hardness, fast deterministic consistency
algorithms can be obtained by employing tools from fine-grained complexity. The
framework is based on a universal consistency problem which can be instantiated
by different memory models. We construct an algorithm for the problem running
in time O*(2^k), where k is the number of write accesses in the execution that
is checked for consistency. Each instance of the framework then admits an
O*(2^k)-time consistency algorithm. By applying the framework, we obtain
corresponding consistency algorithms for SC, TSO, PSO, and RMO. Moreover, we
show that the obtained algorithms for SC, TSO, and PSO are optimal in the
fine-grained sense: there is no consistency algorithm for these running in time
2^o(k) unless the exponential time hypothesis fails.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11398"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11389">A 3/2-Approximation for the Metric Many-visits Path TSP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=eacute=rczi:Krist=oacute=f.html">Krist√≥f B√©rczi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mnich:Matthias.html">Matthias Mnich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vincze:Roland.html">Roland Vincze</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11389">PDF</a><br /><b>Abstract: </b>In the Many-visits Path TSP, we are given a set of $n$ cities along with
their pairwise distances (or cost) $c(uv)$, and moreover each city $v$ comes
with an associated positive integer request $r(v)$.
</p>
<p>The goal is to find a minimum-cost path, starting at city $s$ and ending at
city $t$, that visits each city $v$ exactly $r(v)$ times.
</p>
<p>We present a $\frac32$-approximation algorithm for the metric Many-visits
Path TSP, that runs in time polynomial in $n$ and poly-logarithmic in the
requests $r(v)$.
</p>
<p>Our algorithm can be seen as a far-reaching generalization of the
$\frac32$-approximation algorithm for Path TSP by Zenklusen (SODA 2019), which
answered a long-standing open problem by providing an efficient algorithm which
matches the approximation guarantee of Christofides' algorithm from 1976 for
metric TSP.
</p>
<p>One of the key components of our approach is a polynomial-time algorithm to
compute a connected, degree bounded multigraph of minimum cost.
</p>
<p>We tackle this problem by generalizing a fundamental result of Kir\'aly, Lau
and Singh (Combinatorica, 2012) on the Minimum Bounded Degree Matroid Basis
problem, and devise such an algorithm for general polymatroids, even allowing
element multiplicities.
</p>
<p>Our result directly yields a $\frac32$-approximation to the metric
Many-visits TSP, as well as a $\frac32$-approximation for the problem of
scheduling classes of jobs with sequence-dependent setup times on a single
machine so as to minimize the makespan.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11389"><span class="datestr">at July 23, 2020 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11278">The mergegram of a dendrogram and its stability</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Yury Elkin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11278">PDF</a><br /><b>Abstract: </b>This paper extends the key concept of persistence within Topological Data
Analysis (TDA) in a new direction. TDA quantifies topological shapes hidden in
unorganized data such as clouds of unordered points. In the 0-dimensional case
the distance-based persistence is determined by a single-linkage (SL)
clustering of a finite set in a metric space. Equivalently, the 0D persistence
captures only edge-lengths of a Minimum Spanning Tree (MST). Both SL dendrogram
and MST are unstable under perturbations of points. We define the new
stable-under-noise mergegram, which outperforms previous isometry invariants on
a classification of point clouds by PersLay.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11278"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11094">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11094">New Data Structures for Orthogonal Range Reporting and Range Minima Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11094">PDF</a><br /><b>Abstract: </b>In this paper we present new data structures for two extensively studied
variants of the orthogonal range searching problem.
</p>
<p>First, we describe a data structure that supports two-dimensional orthogonal
range minima queries in $O(n)$ space and $O(\log^{\varepsilon} n)$ time, where
$n$ is the number of points in the data structure and $\varepsilon$ is an
arbitrarily small positive constant. Previously known linear-space solutions
for this problem require $O(\log^{1+\varepsilon} n)$ (Chazelle, 1988) or
$O(\log n\log \log n)$ time (Farzan et al., 2012). A modification of our data
structure uses space $O(n\log \log n)$ and supports range minima queries in
time $O(\log \log n)$. Both results can be extended to support
three-dimensional five-sided reporting queries.
</p>
<p>Next, we turn to the four-dimensional orthogonal range reporting problem and
present a data structure that answers queries in optimal $O(\log n/\log \log n
+ k)$ time, where $k$ is the number of points in the answer. This is the first
data structure that achieves the optimal query time for this problem.
</p>
<p>Our results are obtained by exploiting the properties of three-dimensional
shallow cuttings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11094"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.11093">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.11093">Improved lower and upper bounds on the tile complexity of uniquely self-assembling a thin rectangle non-cooperatively in 3D</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furcy:David.html">David Furcy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Summers:Scott_M=.html">Scott M. Summers</a>, Logan Withers <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11093">PDF</a><br /><b>Abstract: </b>We investigate a fundamental question regarding a benchmark class of shapes
in one of the simplest, yet most widely utilized abstract models of algorithmic
tile self-assembly. Specifically, we study the directed tile complexity of a $k
\times N$ thin rectangle in Winfree's abstract Tile Assembly Model, assuming
that cooperative binding cannot be enforced (temperature-1 self-assembly) and
that tiles are allowed to be placed at most one step into the third dimension
(just-barely 3D). While the directed tile complexities of a square and a
scaled-up version of any algorithmically specified shape at temperature 1 in
just-barely 3D are both asymptotically the same as they are (respectively) at
temperature 2 in 2D, the bounds on the directed tile complexity of a thin
rectangle at temperature 2 in 2D are not known to hold at temperature 1 in
just-barely 3D. Motivated by this discrepancy, we establish new lower and upper
bounds on the directed tile complexity of a thin rectangle at temperature 1 in
just-barely 3D. We develop a new, more powerful type of Window Movie Lemma that
lets us upper bound the number of "sufficiently similar" ways to assign glues
to a set of fixed locations. Consequently, our lower bound,
$\Omega\left(N^{\frac{1}{k}}\right)$, is an asymptotic improvement over the
previous best lower bound and is more aesthetically pleasing since it
eliminates the $k$ that used to divide $N^{\frac{1}{k}}$. The proof of our
upper bound is based on a just-barely 3D, temperature-1 counter, organized
according to "digit regions", which affords it roughly fifty percent more
digits for the same target rectangle compared to the previous best counter.
This increase in digit density results in an upper bound of
$O\left(N^{\frac{1}{\left\lfloor\frac{k}{2}\right\rfloor}}+\log N\right)$, that
is an asymptotic improvement over the previous best upper bound and roughly the
square of our lower bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.11093"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.10307">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.10307">Optimal $\ell_1$ Column Subset Selection and a Fast PTAS for Low Rank Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Arvind V. Mahankali, David P. Woodruff Carnegie Mellon University) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10307">PDF</a><br /><b>Abstract: </b>We study the problem of entrywise $\ell_1$ low rank approximation. We give
the first polynomial time column subset selection-based $\ell_1$ low rank
approximation algorithm sampling $\tilde{O}(k)$ columns and achieving an
$\tilde{O}(k^{1/2})$-approximation for any $k$, improving upon the previous
best $\tilde{O}(k)$-approximation and matching a prior lower bound for column
subset selection-based $\ell_1$-low rank approximation which holds for any
$\text{poly}(k)$ number of columns. We extend our results to obtain tight upper
and lower bounds for column subset selection-based $\ell_p$ low rank
approximation for any $1 &lt; p &lt; 2$, closing a long line of work on this problem.
</p>
<p>We next give a $(1 + \varepsilon)$-approximation algorithm for entrywise
$\ell_p$ low rank approximation, for $1 \leq p &lt; 2$, that is not a column
subset selection algorithm. First, we obtain an algorithm which, given a matrix
$A \in \mathbb{R}^{n \times d}$, returns a rank-$k$ matrix $\hat{A}$ in
$2^{\text{poly}(k/\varepsilon)} + \text{poly}(nd)$ running time such that:
$$\|A - \hat{A}\|_p \leq (1 + \varepsilon) \cdot OPT +
\frac{\varepsilon}{\text{poly}(k)}\|A\|_p$$ where $OPT = \min_{A_k \text{ rank
}k} \|A - A_k\|_p$. Using this algorithm, in the same running time we give an
algorithm which obtains error at most $(1 + \varepsilon) \cdot OPT$ and outputs
a matrix of rank at most $3k$ --- these algorithms significantly improve upon
all previous $(1 + \varepsilon)$- and $O(1)$-approximation algorithms for the
$\ell_p$ low rank approximation problem, which required at least
$n^{\text{poly}(k/\varepsilon)}$ or $n^{\text{poly}(k)}$ running time, and
either required strong bit complexity assumptions (our algorithms do not) or
had bicriteria rank $3k$. Finally, we show hardness results which nearly match
our $2^{\text{poly}(k)} + \text{poly}(nd)$ running time and the above additive
error guarantee.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.10307"><span class="datestr">at July 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/07/22/three-cccg-videos">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html">Three CCCG videos</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Three new ten-minute research talk videos by me are now up on YouTube as part of the collection of videos to be presented in the 2020 Canadian Conference on Computational Geometry. The <a href="http://vga.usask.ca/cccg2020/program.pdf">pdf conference program</a> has links to all the videos. The conference includes an opportunity to interact with the speakers online, on the August 5‚Äì7 dates of the actual conference, with free registration at <a href="http://vga.usask.ca/cccg2020/">the CCCG web site</a>.</p>

<p>My three videos are on <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">dynamic products of ranks, discussed in my previous post</a>, polyhedra that are difficult to unfold, and mathematics inspired by <a href="https://en.wikipedia.org/wiki/Lusona">the sona drawings of southwest Africa</a>. For your convenience here they are more directly:</p>

<div style="text-align: center;">

<p>¬†</p>

<p>¬†</p>

<p>¬†</p>
</div>

<p>(Apologies for the weird aspect ratio. I should probably aim for a more standard 16x9 format in future.)</p>

<p>I also have a fourth CCCG paper, on unfolding orthogonal polyhedra, with a video produced by Joe O‚ÄôRourke. I posted about its preprint version, ‚Äú<a href="https://11011110.github.io/blog/2019/07/29/zipless-polycube.html">Some polycubes have no edge-unzipping</a>‚Äù, a year ago. Here‚Äôs Joe‚Äôs talk:</p>

<div style="text-align: center;">
<p>¬†</p>

</div></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html"><span class="datestr">at July 22, 2020 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/average-case-dp/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/average-case-dp/">The Pitfalls of Average-Case Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Differential privacy protects against extremely strong adversaries‚Äîeven ones who know the entire dataset except for one bit of information about one individual.  Since its inception, people have considered ways to relax the definition to assume a more realistic adversary.  A natural way to do so is to incorporate some distributional assumptions. That is, rather than considering a worst-case dataset, assume the dataset is drawn from some distribution and provide some form of ‚Äúaverage-case‚Äù or ‚ÄúBayesian‚Äù privacy guarantee with respect to this distribution. This is especially tempting as it is common for statistical analysis to work under distributional assumptions.</p>

<p>In this post and in a planned follow-up post, we will discuss some pitfalls of average-case or Bayesian versions of differential privacy.  To avoid keeping you in suspense:</p>

<ul>
  <li>The average-case assumptions in relaxations of differential privacy are qualitatively different to and much more brittle than the typical assumptions made about how the data is generated.</li>
  <li>Average-case relaxations do not satisfy the strong composition properties that have made differential privacy so successful.</li>
  <li>It is safer to use distributional assumptions in the accuracy analysis instead of the privacy analysis. That is, we can provide average-case utility and worst-case privacy. Recent work has shown that this model can capture most of the advantages of distributional assumptions.</li>
</ul>

<p>We will show some illustrative examples for each of these points, but we will be purposefully vague as to exactly which alternative definition we are considering, as these issues arise in a wide variety of definitions.  Our hope is not to shut down discussion of these relaxations, or to single out specific definitions as flawed.  There are specific concrete applications where average-case differential privacy might be useful, and our goal is to highlight some issues that must be carefully considered in each application.</p>

<h3 id="assumptions-about-nature-vs-assumptions-about-the-adversary">Assumptions about nature vs. assumptions about the adversary?</h3>

<p>In any reasonable definition of privacy, we have to think about whom we are hiding sensitive information from.  This person‚Äî‚Äúthe adversary‚Äù‚Äîcould be a stranger, a close friend, a relative, a corporation we do business with, or the government, and who they are affects what information they have access to and what defenses are appropriate.   How the adversary can access the private system defines the <a href="https://differentialprivacy.org/\trustmodels">trust model</a>. Distributional assumptions correspond to the adversary‚Äôs side information.  Our key point is:</p>

<blockquote>
  <p>Assumptions incorporated into the definition of privacy are assumptions about the adversary and these are qualitatively different from assumptions about ‚Äúnature,‚Äù which is the process that generates the data.</p>
</blockquote>

<p>For example, suppose an employer learns that two of its employees have expensive medical conditions. On its own, this information does not identify those employees and this privacy intuition could be formalized via distributional assumptions. But these distributional assumptions will break if the employer later receives some side information. For example, the other healthy employees may voluntarily disclose their medical status or the employer may find out that, before you were hired, that number was only one. (Incidentally, this is an example of a failure of composition, which we will discuss in another post.)</p>

<p>This example illustrates how assumptions about the adversary that might seem reasonable in a vacuum can be invalidated by context. Plus, assumptions about the adversary can be invalidated by <em>future</em> side information, and you can‚Äôt retract a privacy leak once it happens the way you can a medical study. So assumptions about the adversary are much less future-proof than assumptions about nature.</p>

<h3 id="all-models-are-wrong-but-some-are-useful">All models are wrong, but some are useful</h3>

<p>One justification for incorporating distributional assumptions into the privacy definition is that the person using the data is often making these assumptions anyway‚Äîfor example, that the data is i.i.d. Gaussian, or that two variables have some underlying linear relationship to be discovered.  So, if the assumption were false, wouldn‚Äôt we already be in trouble?  Not really.</p>

<blockquote>
  <p>It‚Äôs important to remember the old saw ‚Äúall models are wrong, but some are useful.‚Äù  Some models have proven themselves useful for statistical purposes, but that does not mean they are useful as a basis for privacy.</p>
</blockquote>

<p>For example, our methods may be robust to the relatively friendly ways that nature deviates from the model, but we can‚Äôt trust adversaries to be as friendly.</p>

<p>For a toy example, suppose we model our data as coming from a normal distribution \( N(\mu,\sigma^2) \), but actually the data is collected at two different testing centers, one of which rounds its measurements to the nearest integer and the other of which provides two decimal places of precision.  This rounding makes the model wrong, but won‚Äôt significantly affect our estimate of the mean.  However, just looking at the estimate of the mean might reveal that someone in the dataset went to the second testing center, potentially compromising that person‚Äôs privacy.</p>

<p>A more natural setting where this issue arises is in dealing with <em>outliers</em> or other extreme examples, which we will discuss in the next section.</p>

<h3 id="privacy-for-outliers">Privacy for outliers</h3>

<p>The usual worst-case definition of differential privacy provides privacy for everyone, including outliers.  Although there are lots of ways to achieve differential privacy, in order to compare definitions, it will help to restrict attention to the basic approach based on calibrating noise to sensitivity:</p>

<p>Suppose we have a private dataset \( x \in \mathcal{X}^n \) containing the data of \( n \) individuals, and some real-valued query \( q : \mathcal{X}^n \to \mathbb{R} \).  The standard way to release an estimate of \( q(x) \) is to compute
\[
M(x) = q(x) + Z \cdot \sup_{\textrm{neighboring}~x‚Äô,x‚Äù} |q(x‚Äô) - q(x‚Äù)|
\]
where 
\(
\sup_{\textrm{neighboring}~x‚Äô,x‚Äù} |q(x‚Äô) - q(x‚Äù)|
\)
is called the ‚Äúworst-case sensitivity‚Äù of \( q \) and \( Z \) is some noise, commonly drawn from a Laplace or Gaussian distribution.</p>

<p>Unfortunately, the worst-case sensitivity may be large or even infinite for basic statistics of interest, such as the mean \( q(x) = \frac{1}{n} \sum_{i} x_i \) of unbounded real values.  There are a variety of differentially private algorithms for addressing this problem,<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote">1</a></sup> but that is not what this post is about.  It‚Äôs tempting to, instead, try to scale the noise to some notion of ‚Äúaverage-case sensitivity,‚Äù with the goal of satisfying some average-case version of differential privacy.  For example, suppose the data is drawn from some normal distribution \( N(\mu,\sigma^2) \) and the neighboring datasets \( x', x'' \) are each \( n \) i.i.d. samples from this distribution, but differing on exactly one random sample.  Then the worst-case sensitivity of the mean is infinite:
\[
\sup_{\textrm{neighboring}~x‚Äô,x‚Äù} |q(x‚Äô) - q(x‚Äù)| = \infty,
\]
but the average-sensitivity is proportional to \(1/n\):
\[
\mathbb{E}_{\textrm{neighboring}~x‚Äô, x‚Äù}(|q(x‚Äô) - q(x‚Äù)|) \approx \frac{\sigma}{n}.
\]
Thus, under an average-case privacy guarantee, we can estimate the mean with very little noise.</p>

<p>But what happens to privacy if this assumption fails, perhaps because of outliers?  Imagine computing the average wealth of a subset of one hundred Amazon employees who test positive for COVID-19, and discovering that it‚Äôs over one billion dollars.  Maybe Jeff Bezos isn‚Äôt feeling well?<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote">2</a></sup></p>

<p>Yes, this example is a little contrived, since you probably shouldn‚Äôt have computed the empirical mean of such skewed data anyway. But, if this fact leaks out, you can‚Äôt just go back in time and truncate the data or compute the median instead. Privacy tends to be high-stakes both because of the potential consequences of a breach and the inability to retract or correct a privacy violation after it is discovered.</p>

<p>In the next section we‚Äôll see a slightly more complex example where average-case privacy / average-case sensitivity fails to protect privacy even when the distributional assumptions hold.</p>

<h3 id="example-pairwise-correlations-and-linear-regression">Example: pairwise correlations and linear regression</h3>

<p>Suppose our dataset \( X \) is a matrix \( \{-1,+1\}^{n \times (d+1)} \) where each row \( X_i \) corresponds to one person‚Äôs data and each column corresponds to one feature.  For simplicity, let‚Äôs suppose our distributional assumption is that the dataset is completely uniform‚Äîeach bit is sampled independently and uniformly from \( \{-1,+1\} \).  We‚Äôll think of the first \( d \) columns as ‚Äúfeatures‚Äù and the last column as a ‚Äúsecret label.‚Äù</p>

<p>First, consider the set of pairwise correlations between each feature and the secret label:
\[
q_j(X) = \sum_{i = 1}^{n} X_{i,j} X_{i,d+1}
\]
for \( j = 1,\dots,d \).  Note that \( q_j(X) \) has mean 0 and variance \( n \) under our distributional model of the data.</p>

<p>Now, suppose we have a weight vector \( w \in \mathbb{R}^{d} \) and want to estimate the weighted average of correlations
\[
q(X) = \sum_{j = 1}^{d} w_j q_j(X) = \sum_{i=1}^{n} \sum_{j=1}^{d} w_{j} X_{i,j} X_{i,d+1}
\]
This statistic may look a little odd, but it‚Äôs pretty close to computing the average squared error of the linear predictor \( \hat X_{i,d+1} = \sum_{j=1}^{d} w_j X_{i,j} \) given by the weight vector \( w \), which is a natural thing to estimate.</p>

<p>The worst-case sensitivity of \(q\) is proportional to \( \|w\|_1 \).<br />
However, it‚Äôs not too hard to show that, under our distributional model, the average-case sensitivity is much lower; it is proportional to \( \| w \|_2 \).  Thus, using average-case privacy may allow us to add significantly less noise.</p>

<p>What could go wrong here?  Well, we‚Äôve implicitly assumed that the weights \( w \) are independent of the data \( X \).  That is, the person specifying the weights has no knowledge of the data itself, only its distribution.  Suppose the weights are specified by an adversary who has learned the \( d \) features of the first individual (although there is nothing special about considering the first individual), who sets the weights to \(w = (X_{1,1},\dots,X_{1,d}) \).  Another calculation shows that, in this case, <em>even when our model of the data is exactly correct</em>, the query \( q(X) \) has mean \( d \cdot X_{1,d+1} \) and standard deviation approximately \( \sqrt{nd} \).  Thus, if \( d \gg n \) we can confidently determine the secret label \( X_{1,d+1} \)  of the first individual from the value \( q(X) \).  Moreover, adding noise of standard deviation \( \ll d \) will not significantly affect the adversary‚Äôs ability to learn the secret label.  But, earlier, we argued that average-case sensitivity is proportional to \( \|w \|_2 = \sqrt{d} \), so this form of average-case privacy fails to protect a user‚Äôs data in this scenario!  Note that adding noise proportional to \( \|w\|_1 = d \) would satisfy (worst-case) differential privacy and would thwart this adversary.</p>

<blockquote>
  <p>What went wrong is that the data satisfied our assumptions, but the adversary‚Äôs beliefs about the data did not!</p>
</blockquote>

<p>The set of reasonable distributions to consider for the adversary‚Äôs beliefs may look very different from the set of reasonable distributions to consider for your analysis of the data.  You may think that it‚Äôs not reasonable for the attacker to choose this weight vector \( w \) containing a lot of prior information about an individual, but assuming that the attacker cannot obtain or specify such a vector is very different from assuming that the data is uniform, and requires its own justification.</p>

<p>Before wrapping up, let‚Äôs just make a couple more observations about this example:</p>

<ul>
  <li>This attack is pretty robust. The assumption that the data is uniform with independent features can be relaxed significantly.  It‚Äôs also not necessary for the adversary to exactly know all the features of the first user, all we need is for the weights to have correlation \( \gg \sqrt{nd} \) with the features.  For example, if the dataset is genomic data, having the data of a relative might suffice.</li>
  <li>This problem isn‚Äôt specific to high-dimensional data with \( d \gg n \).  If we allow more general types of ‚Äúqueries‚Äù, then a similar attack is possible when there are only \( d \approx \log n \) features.</li>
  <li>To make this example as crisp as possible, we allowed an adversarial data analyst to specify the weight vector \( w \).  You might think examples like this can‚Äôt arise if the algorithm designer specifies all of the queries internally, but ensuring that requires great care (as we‚Äôll see in our upcoming post about composition).</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p>As we have discussed, the main issue that arises in average-case or Bayesian versions of differential privacy is that we must make strong assumptions about the adversary. A simple distributional assumption about the data, which may be entirely reasonable for statistical analysis, entails assuming a na√Øve adversary with essentially no side information, which is not reasonable from a privacy perspective.</p>

<p>In a future post, we will discuss <em>composition</em>, which is a key robustness property and really the secret to differential privacy‚Äôs success.  As we‚Äôll see, average-case versions of differential privacy do not enjoy strong composition properties the way worst-case differential privacy does, which makes them much harder to deploy.</p>

<p>Incorporating assumptions about the adversary into the privacy guarantee requires great care; and it is safest to make fewer assumptions, which quickly pushes us towards the worst-case definition of differential privacy. Nevertheless, assumptions about the adversary are often made implicitly and it is worth studying how to make these explicit.</p>

<p>So, is there are role for distributional assumptions in differential privacy? Yes! Although we‚Äôve discussed the pitfalls of making the <em>privacy guarantee</em> contingent on distributional assumptions, none of these pitfalls apply to making the <em>utility guarantee</em> contingent on distributional assumptions, as is normally done in statistical analysis.  In recent years, this combination‚Äîworst-case privacy, average-case utility‚Äîhas been fruitful, and seems to allow many of the benefits that average-case privacy definitions seek to capture.  For example, recent work has shown that worst-case differential privacy permits accurate mean and covariance estimation of unbounded data under natural modeling assumptions <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a>, <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a>, <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, <a href="https://arxiv.org/abs/2001.02285" title="Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, Adam Groce. Differentially Private Confidence Intervals. 2020."><strong>[DFMBG20]</strong></a>, <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a>, <a href="https://arxiv.org/abs/2006.06618" title="Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman. CoinPress: Practical Private Mean and Covariance Estimation. 2020."><strong>[BDKU20]</strong></a>, but this remains an active area of research.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For example, there are approaches based on various paradigms like Smooth Sensitivity <a href="http://www.cse.psu.edu/~ads22/pubs/NRS07/NRS07-full-draft-v1.pdf" title="Kobbi Nissim, Sofya Raskhodnikova, Adam Smith. Smooth Sensitivity and Sampling in Private Data Analysis. STOC 2007."><strong>[NRS07]</strong></a> <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, Propose-Test-Release <a href="http://www.stat.cmu.edu/~jinglei/dl09.pdf" title="Cynthia Dwork, Jing Lei. Differential Privacy and Robust Statistics. STOC 2009."><strong>[DL09]</strong></a>, or Truncation/Winsorization <a href="http://www.cse.psu.edu/~ads22/pubs/2011/stoc194-smith.pdf" title="Adam Smith. Privacy-preserving Statistical Estimation with Optimal Convergence Rates. STOC 2011."><strong>[S11]</strong></a> <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a> <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a> <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a> to name a few.¬†<a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">‚Ü©</a></p>
    </li>
    <li id="fn:2">
      <p>If you are confident that Jeff Bezos or other extremely high-wealth individuals are not in the sample, then you could <em>truncate</em> each sample and compute the mean of the truncated samples.  This would give worst-case privacy, and, if you are correct in your assumption, would not affect the mean.¬†<a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">‚Ü©</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/average-case-dp/"><span class="datestr">at July 22, 2020 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/110">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/110">TR20-110 |  Capacity Lower Bounds via Productization | 

	Leonid Gurvits, 

	Jonathan Leake</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The purpose of this note is to state and prove a lower bound on the capacity of a real stable polynomial $p(x)$ which is based only on its value and gradient at $x=1$. This result implies a sharp improvement to a similar inequality proved by Linial-Samorodnitsky-Wigderson in 2000. Such inequalities have played an important role in the recent work on operator scaling and its generalizations and applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/110"><span class="datestr">at July 22, 2020 04:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4912">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4912">Is this blog obsolete?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Longtime friend-of-the-blog <a href="http://michaelnielsen.org/">Michael Nielsen</a>, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following <a href="https://twitter.com/michael_nielsen/status/1284988349418098688">exchange</a> with the astounding <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> text generator / chatbot:</p>



<blockquote class="wp-block-quote"><p>Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?<br />A: It is not possible to use quantum computers to solve NP-complete problems in polynomial time.</p><p>Q: Why not?<br />A: Because NP-complete problems are not in BQP.</p><p>Q: You‚Äôve just restated your earlier assertion!  What I‚Äôd like is to understand the underlying reason why not!<br />A: I‚Äôm sorry, but I don‚Äôt have time to explain the underlying reason why not.</p><p>Q: No offence, but this seems really bogus.  I don‚Äôt believe you understand the reason why!<br />A: I‚Äôm sorry, but I don‚Äôt have time to explain the underlying reason why not.</p></blockquote>



<p>(And then there are a few more rounds of the AI not having time to ‚Äúexplain the underlying reason why not.‚Äù)</p>



<p>This is, like, 80% of the way to replacing <em>Shtetl-Optimized</em>!</p>



<p>For much more discussion of GPT-3 and its implications, and samples of its output, see for example the <a href="https://www.reddit.com/r/slatestarcodex/">SSC subreddit</a>.  At the moment, as far as I can tell, the closest a person off the street can easily come to experimenting with GPT-3 themselves is using a website called <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>



<p>And yes, as many have already remarked, this is clearly the <a href="https://en.wikipedia.org/wiki/Altair_8800">MITS Altair</a> of text-generating AI, an amusing toy that‚Äôs also the start of something that will change the world.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4912"><span class="datestr">at July 21, 2020 12:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/stoc2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/stoc2020/">Conference Digest - STOC 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1911.08339">The Power of Factorization Mechanisms in Local and Central Differential Privacy</a> (<a href="https://www.youtube.com/watch?v=hSenRTxhZhM">video</a>)<br />
<a href="https://dblp.uni-trier.de/pers/hd/e/Edmonds:Alexander">Alexander Edmonds</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time</a> (<a href="https://www.youtube.com/watch?v=Tlc-z-MFAmM">video</a>)<br />
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints</a> (<a href="https://www.youtube.com/watch?v=AWgzaFOU_HM">video</a>)<br />
<a href="https://yuvaldagan.wordpress.com/">Yuval Dagan</a>, <a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail</a> (<a href="https://www.youtube.com/watch?v=sV59uoWJRnk">video</a>, <a href="https://www.youtube.com/watch?v=Fp7cgHRl8Yc">longer video</a>)<br />
<a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/stoc2020/"><span class="datestr">at July 20, 2020 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/transfer-learning/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/transfer-learning/">Transfer Learning with Adversarially Robust Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2007.08489" class="bbutton">
<i class="fas fa-file-pdf"></i>
¬† ¬† Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/Microsoft/robust-models-transfer" class="bbutton">
<i class="fab fa-github"></i>
¬†¬† Models and Code
</a>
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2007.08489">latest paper</a>, in collaboration with <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, we explore adversarial
robustness as an avenue for training computer vision models with more transferrable
features. We find that robust models outperform their standard counterparts on
a variety of transfer learning tasks.</i></p>

<h2 id="what-is-transfer-learning">What is transfer learning?</h2>

<p>Transfer learning is a paradigm where one leverages information
from a ‚Äúsource‚Äù task to better solve another ‚Äútarget‚Äù task. Particularly when there is little training data or compute available for solving the target
task, transfer learning provides a simple and efficient way to obtain performant
machine learning models.</p>

<p>Transfer learning has already proven its utility in many ML contexts. In natural language processing, for example, one can leverage language models pre-trained on large
text corpora to beat state-of-the-art performance on
tasks like query answering, entity recognition or part-of-speech classification.</p>

<p>In our work we focus on computer vision; in this context, a standard‚Äîand
remarkably successful‚Äîtransfer learning pipeline is ‚ÄúImageNet pre-training.‚Äù
This pipeline starts with a deep neural network trained on the <a href="http://image-net.org">ImageNet-1K</a>
dataset, and then refines this pre-trained model for a target task. The target task can range
from classification of smaller datasets (e.g., <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>) to more complex
tasks like object detection (e.g., <a href="http://host.robots.ox.ac.uk/pascal/VOC/">VOC</a>).</p>

<p>Although there are many ways in which one can refine a pre-trained model, we
will restrict our attention to the two most popular methods:</p>

<ul>
  <li><strong>Fixed-feature</strong>: In fixed-feature transfer learning, we replace the final
(linear) layer of the neural network with a new layer that has the correct
number of outputs for the target task. Then, keeping the rest of the layers
<em>fixed</em>, we train the newly replaced layer on the target task.</li>
  <li><strong>Full-network</strong>: In full-network transfer learning, we also replace the last
layer but do not freeze any layers afterwards. Instead, we use the pre-trained
network
as a sort of ‚Äúinitialization,‚Äù and continue training <em>all</em> the layers on the
target task.</li>
</ul>

<p>When at least a moderate amount of data is available, full-network transfer
learning typically outperforms the fixed-feature strategy.</p>

<h2 id="how-can-we-improve-transfer-learning">How can we improve transfer learning?</h2>

<p>Although we don‚Äôt have a comprehensive understanding of what makes transfer
learning algorithms tick, there has been a long line of work focused on identifying 
factors that improve (or worsen) performance (examples include
<a href="https://arxiv.org/abs/1406.5774">[1]</a>,
<a href="https://arxiv.org/abs/1608.08614">[2]</a>,
<a href="https://arxiv.org/abs/1805.08974">[3]</a>,
<a href="https://arxiv.org/abs/1804.08328">[4]</a>,
<a href="https://arxiv.org/abs/1411.1792">[5]</a>).</p>

<p>By design, the pre-trained ImageNet model itself plays a major role here:
indeed, a recent study by <a href="https://arxiv.org/abs/1805.08974">Kornblith, Shlens, and Le</a> finds that
pre-trained models which achieve a higher ImageNet accuracy also perform better when
transferred to downstream classification tasks, with a tight linear
correspondence between ImageNet accuracy and the accuracy on the target task:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/ksl.png" alt="A scatter plot of ImageNet accuracy versus downstream     transfer accuracy showing the linear relation." class="bigimg" /></p>
<div class="footnote">
Reproduced from <a href="https://arxiv.org/abs/1805.08974">[KSL19]</a>. 
Each dot is a pre-trained model whose $x$ coordinate is given by its 
ImageNet accuracy and $y$ coordinate is given by its downstream 
accuracy on the target task (after the corresponding refinement on that task).
</div>

<p>But is improving ImageNet accuracy of the pre-trained model the <em>only</em> way to improve transfer learning performance?</p>

<p>After all, we want to obtain models that have learned broadly applicable features from the
source dataset. ImageNet accuracy likely correlates with the quality of
features that a model has learned, but may not fully describe the downstream
utility of these features.
Ultimately, the nature of learned features stems from the <em>priors</em> placed on
them during training. For example, there have been studies of the (sometimes
implicit) priors imposed by architectural components (e.g., <a href="https://dmitryulyanov.github.io/deep_image_prior">convolutional layers</a>),
<a href="https://www.tandfonline.com/doi/abs/10.1198/10618600152418584">data</a>
<a href="https://arxiv.org/abs/1911.09071">augmentation</a>, 
<a href="https://arxiv.org/abs/1811.00401">loss functions</a> and even
<a href="https://stats385.github.io/assets/lectures/Stanford_Donoho_class_Nov_19.pdf">gradient descent</a> on neural network training.</p>

<p>In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, we study another prior: <em>adversarial robustness</em>.
Adversarial robustness‚Äîa rather frequent subject on this blog‚Äîrefers to
model‚Äôs invariance to small (often imperceptible) perturbations of natural
inputs, called <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.</p>

<p>Standard neural networks (i.e., trained with the goal of maximizing
accuracy) are extremely vulnerable to such adversarial examples. For example,
with just a tiny perturbation to the pig image below, a pre-trained ImageNet
classifier will predict it as an ‚Äúairliner‚Äù with 99% confidence:</p>

<p><img src="https://gradientscience.org/images/piggie.png" alt="An adversarial example: a pig on the left which is imperceptibly perturbed to be classified as an airliner on the right." /></p>
<div class="footnote">
A "pigs-can-fly" adversarial example: The "pig" image on the left is correctly classified by a standard ML model, but its imperceptibly perturbed counterpart on the right is classified as an "airliner" with 99% confidence.
</div>

<p>Adversarial robustness is thus typically induced at training time by replacing
the standard loss minimization objective with a <em>robust optimization</em> objective
(see our <a href="https://gradientscience.org/robust_opt_pt1">post on robust optimization</a> for more background):</p>



<p>The above objective trains models to be robust to image perturbations that are
small in (pixel-wise) $\ell_2$ (Euclidean) normIn reality, an $\ell_2$ ball doesn't perfectly capture the
set of imperceptible perturbations we want models to be robust to‚Äîbut robustness with respect to this fairly rudimentary notion of perturbations turns out to be already non-trivial and very helpful.. 
The parameter $\varepsilon$ is a hyperparameter
governing the intended degree of invariance of the resulting models to the
corresponding perturbations. Setting 
$\varepsilon = 0$ corresponds to standard training, and increasing $\varepsilon$
asks the model to be robust to increasingly large perturbations.
In short, the objective asks the model to minimize risk on not only the 
training datapoints but also the entire radius-$\varepsilon$
neighbourhood around them.</p>

<p><em>[A quick plug: Our <a href="https://github.com/MadryLab/robustness"><code class="language-plaintext highlighter-rouge">robustness</code> Python library</a>, used for the code release of this paper, enables one to easily train and manipulate both standard and adversarially robust models.]</em></p>

<p>Although adversarial robustness has been initially studied solely through the lens of machine learning security, a line
of recent work (including some that‚Äôs been <a href="https://gradientscience.org/adv">previously</a> 
<a href="https://gradientscience.org/robust_apps">covered</a> on this blog) has begun to study
adversarially robust models in their own right, framing adversarial robustness
as a prior that forces models to learn features that are locally stable.
These works have found that on the one hand, adversarially robust models tend
to attain lower accuracy than their standardly-trained
counterparts.</p>

<p>On the other hand, recent work suggests that the feature
representations of robust models carry several advantages over those of
standard models, such as <a href="https://arxiv.org/abs/1805.12152">better-behaved</a>
<a href="https://arxiv.org/abs/1905.09797">gradients</a>, <a href="https://arxiv.org/abs/1910.08640">representation
invertibility</a>, and more <a href="https://arxiv.org/abs/2005.10190">specialized
features</a>.
We‚Äôve actually discussed some of these observations in earlier posts on this
blog‚Äîsee, e.g., our posts about 
<a href="https://gradientscience.org/robust_reps">representation learning</a> and 
<a href="https://gradientscience.org/robust_apps">image synthesis</a>.</p>

<p>These desirable properties
might suggest that robust neural networks are learning better feature
representations than standard networks, which could improve transfer
performance.</p>

<h3 id="adversarial-robustness-and-transfer-learning">Adversarial robustness and transfer learning</h3>

<p>So in summary, we have standard models with high accuracy on the source task but
little (or no) robustness; and we have adversarially robust models, which are
worse in terms of ImageNet accuracy, but have the ‚Äúnice‚Äù
representational properties identified and discussed by prior works. Which
models are better for transfer learning?</p>

<p>To answer this question, we trained and examined a large collection
of standard and robust ImageNet models, while grid searching over a wide range of
hyperparameters and architectures to find the best model of each type. (All
models are available for download via our <a href="https://github.com/microsoft/robust-models-transfer">code/model
release</a> and more
details on our training procedure can be found there and in <a href="https://arxiv.org/abs/2007.08489">our
paper</a>). We then performed transfer
learning (using both fixed-feature and full-network refinement) from each
trained model to 12 downstream classification tasks.</p>

<p>It turns out that 
adversarially robust source models fairly consistently outperform their standard counterparts in
terms of downstream accuracy. In the table below, we compare the accuracies of
the best standard model (searching over hyperparameters and
architecture) and the best robust model (searching over the
previous factors as well as robustness level $\varepsilon$):</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/results-table.svg" style="width: 100%;" class="bigimg" alt="Table showing that robust models     perform better than their standard counterparts." /></p>
<div class="footnote">
    The main result: Adversarially robust models outperform their standard counterparts when transferred to downstream classification tasks.
</div>

<p>This difference in performance tends to be particularly striking in the context of fixed-feature transfer learning. The following graph shows, for each architecture and
downstream classification task, the best standard model compared to the best
robust model in that setting. As we can see, adversarially robust models
improve on the performance of their standard counterparts, and the gap tends to
<em>increase</em> as networks increase in width:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/LogisticRegression.svg" alt="A bar chart showing that robust models improve on     standard ones even without taking the maximum over architectures." class="bigimg" /></p>
<div class="footnote">
    Adversarially robust models tend to improve over standard networks for
    individual architectures too. (An analogous graph for full-network
    transfer learning is given in Figure 3 of <a href="https://arxiv.org/abs/2007.08489">our paper</a>.)
</div>

<p>Adversarial robustness improved downstream transfer
performance even when the target task was not a classification one. For example, the
following table compares standard and robust pre-training for use in downstream
object detection and instance segmentation:</p>

<p><img src="https://gradientscience.org/../assets/robust-transfer-learning/obj-det-results.svg" style="width: 80%;" class="bigimg" /></p>
<div class="footnote">
</div>

<h3 id="robustness-versus-accuracy">Robustness versus accuracy</h3>

<p>So it seems like robust models, despite being less accurate on the source task, are actually
better for transfer learning purposes. Indeed, the linear relation between
 ImageNet accuracy and transfer performance observed in prior work (see our discussion above) doesn‚Äôt seem
 to hold when the robustness parameter is varied. Compare the graphs below to the ones at the very start of this post:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/wide_resnet50_4_LogisticRegression.svg" class="bigimg" /></p>
<div class="footnote">
    Source-task (ImageNet) versus target (fixed-feature) accuracy for models with the same
    architecture while varying the robustness levels. Each dot is a
    WideResNet-50x4 model with $x$ coordinate given by source-task accuracy and
    $y$ coordinate given by fixed-feature transfer learning accuracy.
    Contrast the trends here with the "fixed-feature" trend in the first
    figure of this post‚Äîthe linear trend depicted there largely disappears as less
    accurate but more robust models perform better in terms of transfer.
</div>

<p>How do we reconcile our observations with these trends observed by prior work?</p>

<p>We hypothesize that robustness and accuracy have <em>disentangled</em> effects on
transfer performance. That is, for a fixed level of robustness, higher
accuracy on the source task helps transfer, and for a fixed level of
accuracy, increased robustness helps transfer. Indeed, as shown below, for a
fixed level of robustness, the accuracy-transfer relation tends to hold
strongly:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/fixed-robustness.svg" class="bigimg" /></p>
<div class="footnote">
Even though robust models appear to break the linear
accuracy-transfer trend, this trend is actually preserved for a fixed value of
robustness. Each dot in the graph is a different architecture, trained for the same level of robustness ($\varepsilon = 3.0$). The $x$ coordinate is source task (ImageNet) accuracy, and the $y$ coordinate is the downstream accuracy on each target dataset.
</div>

<p>In addition to reconciling our results with those of prior work, these findings suggest that ongoing work on developing more accurate robust models
may have the added benefit of further improving transfer learning performance.</p>

<h3 id="other-empirical-mysteries-and-future-work">Other empirical mysteries and future work</h3>

<p>This post discussed how adversarially robust models might constitute a promising
avenue for improving transfer learning, and already often outperform standard
models in terms of downstream accuracy. In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, 
we study this phenomenon more closely: for example, we examine the effects of
model width, and we compare adversarial robustness to other notions of
robustness. We also uncover a few somewhat mysterious properties: for example,
resizing images seems to have a non-trivial effect on the relationship between
robustness and downstream accuracy.</p>

<p>Finally, while our work provides evidence that adversarially
robust computer vision models transfer better, understanding precisely <em>why</em> this is the case remains open. More broadly, the results we
observe indicate that we still do not yet fully understand (even empirically)
the ingredients that make transfer learning successful. We hope that our work
prompts an inquiry into the underpinnings of modern transfer learning.</p></div>







<p class="date">
<a href="https://gradientscience.org/transfer-learning/"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2418581440113974615">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html">Erdos-Turan for k=3 is True!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
(All of the math in this post is summarized (without proofs) in a writeup by Erik Metz and myself which you can find¬†<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/3apblog.pdf">here</a>. It is a pdf file so you can click on links in it to get to the papers it refers to. There have been posts on this topic by¬†<a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">Gil Kalai</a>¬†and¬†¬†<a href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/">Luca Trevisan</a>. If you know of others then let me know so I can add them to this post.)<br />
<br />
<br />
<br />
This is a sequel to <a href="https://blog.computationalcomplexity.org/2010/12/breakthrough-result-on-density-and-3.html">A BREAKTHROUGH result on density and 3-AP's</a>¬†and¬†<a href="https://blog.computationalcomplexity.org/2017/06/big-news-on-w3r.html">Big news on W(3,r)!</a><br />
<br />
For this post N is large, and all inequalites have a big-O or a big-Omega.<br />
<br />
For this post [N] is {1,...,N}<br />
<br />
Let<br />
<br />
r(N) be the least w such that if A is a subset of [N] and |A|¬† &gt;¬† w, then A has a 3-AP.<br />
<br />
There has been a long sequence of results getting smaller and smaller upper bounds on r(N).<br />
<br />
The motivation for getting these results is that if r(N) is &lt; N/(log N)^{1+\delta} with delta&gt;0 then the following holds:<br />
<br />
If sum_{x\in A} 1/x diverges then A has a 3-AP.<br />
<br />
This is the k=3 case of one of the Erdos-Turan Conjectures.<br />
<br />
Bloom and Sisack HAVE gotten N/(log N)^{1+delta} so they HAVE gotten ET k=3. Wow!<br />
<br />
1) I am NOT surprised that its true.<br />
<br />
2) I am SHOCKED and DELIGHTED that it was proven.¬† Shocked because the results leading up to it (see the write up referenced at the beginning of this post) seemed Zeno-like, approaching the result needed got but not getting there. Delighted because... uh, as the kids say, just cause.<br />
<br />
I've heard that k=4 really is much harder (see my comments and Gil's response on his blog post, pointed to at the beginning of this post)¬† and it is true that there has been far less progress on that case (the write up I pointed to at the beginning of this post says what is known). Hence I will again be <i>shocked </i>if it is proven.¬† So, unlike The Who (see¬†<a href="https://www.youtube.com/watch?v=UDfAdHBtK_Q">here</a>) I CAN be fooled again. That's okay--- I will¬† be <i>delighted</i>. (ADDED LATER- there are more comments no Gil's website, from Thomas Bloom and Ben Green about what is likely to happen in the next 10 years.)<br />
<br />
Erdos offered a prize of $3000 for a proof that A has, for all k, a k-AP.¬† The prize is now $5000. After Erdos passed away Ronald Graham became the Erdos-Bank and paid out the money when people solved a problem Erdos put a bounty on. What happens now? (If I have the facts wrong and/or if you know the answer, please leave a polite and enlightening comment.)<br />
<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html"><span class="datestr">at July 19, 2020 07:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/109">TR20-109 |  On Testing Hamiltonicity in the Bounded Degree Graph Model | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that testing Hamiltonicity in the bounded-degree graph model requires a linear number of queries. This refers to both the path and the cycle versions of the problem, and similar results hold also for the directed analogues.
In addition, we present an alternative proof for the known fact that testing Independent Set Size (in this model) requires a linear number of queries.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/109"><span class="datestr">at July 19, 2020 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/108">TR20-108 |  Query Complexity of Global Minimum Cut | 

	Arijit Bishnu, 

	Arijit Ghosh, 

	Gopinath Mishra, 

	Manaswi Paraashar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, we resolve the query complexity of global minimum cut problem for a graph by designing a randomized algorithm for approximating the size of minimum cut in a graph, where the graph can be accessed through local queries like \textsc{Degree}, \textsc{Neighbor}, and \textsc{Adjacency} queries.

Given $\epsilon \in (0,1)$, the algorithm with high probability outputs an estimate $\hat{t}$ satisfying the following $(1-\epsilon) t \leq \hat{t} \leq (1+\epsilon) t$, where $m$ is the number of edges in the graph and $t$ is the size of minimum cut in the graph. The expected number of local queries used by our algorithm is $\min\left\{m+n,\frac{m}{t}\right\}\mbox{poly}\left(\log n,\frac{1}{\epsilon}\right)$ where $n$ is the number of vertices in the graph. Eden and Rosenbaum showed that $\Omega(m/t)$ many local queries are required for approximating the size of minimum cut in graphs. These two results together resolve the query complexity of the problem of estimating the size of minimum cut in graphs using local queries.

Building on the lower bound of Eden and Rosenbaum, we show that, for all $t \in \mathbb{N}$, $\Omega(m)$ local queries are required to decide if the size of the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t \in \mathbb{N}$, $\Omega(m)$ local queries are required to find all the minimum cut edges even if it is promised that the input graph has a minimum cut of size $t$. Both of our lower bound results are randomized, and hold even if we can make \textsc{Random Edge} query apart from local queries.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/108"><span class="datestr">at July 19, 2020 01:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/107">TR20-107 |  Testing linear inequalities of subgraph statistics | 

	Lior Gishboliner, 

	Asaf Shapira, 

	Henrique Stagni</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Property testers are fast randomized algorithms whose task is to distinguish between inputs satisfying some predetermined property ${\cal P}$ and those that are far from satisfying it. Since these algorithms operate by inspecting a small randomly selected portion of the input, the most natural property one would like to be able to test is whether the input does not contain certain forbidden small substructures. In the setting of graphs, such a result was obtained by Alon et al., who proved that for any finite family of graphs ${\cal F}$, the property of being induced ${\cal F}$-free (i.e. not containing an induced copy of any $F \in {\cal F}$) is testable.

It is natural to ask if one can go one step further and prove that more elaborate properties involving induced subgraphs are also testable. One such generalization of the result of Alon et al. was formulated by Goldreich and Shinkar who conjectured that for any finite family of graphs ${\cal F}$, and any linear inequality involving the densities of the graphs $F \in {\cal F}$ in the input graph,
the property of satisfying this inequality can be tested in a certain restricted model of graph property testing. Our main result in this paper disproves this conjecture in the following strong form: some properties of this type are not testable even in the classical (i.e. unrestricted) model of graph property testing.

The proof deviates significantly from prior non-testability results in this area. The main idea is to use a linear inequality relating induced subgraph densities in order to encode the property of being a quasirandom graph.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/107"><span class="datestr">at July 19, 2020 01:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
