<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 26, 2020 10:12 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/158">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/158">TR20-158 |  A Note on Hardness under Projections for Graph Isomorphism and Time-Bounded Kolmogorov Complexity | 

	Eric Allender, 

	Azucena Garvia Bosshard, 

	Amulya Musipatla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This paper focuses on a variant of the circuit minimization problem (MCSP), denoted MKTP, which studies resource-bounded Kolmogorov complexity in place of circuit size. MCSP is not known to be hard for any complexity class under any kind of m-reducibility, but recently MKTP was shown to be hard for DET under m-reductions computable in NC0, by presenting an AC0 reduction from the rigid graph isomorphism problem to MKTP, and combining that with a theorem of Toran, showing that DET AC0-reduces to the rigid graph isomorphism problem, and then appealing to the "Gap Theorem" of [Agrawal, Allender, Rudich]. Here, we show that these reductions can be accomplished by means of projections. Thus MKTP is hard for DET under projections, and the rigid graph isomorphism problem is hard for DET under uniform projections.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/158"><span class="datestr">at October 26, 2020 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12455">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12455">Primal-Dual Mesh Convolutional Neural Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milano:Francesco.html">Francesco Milano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Loquercio:Antonio.html">Antonio Loquercio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosinol:Antoni.html">Antoni Rosinol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scaramuzza:Davide.html">Davide Scaramuzza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlone:Luca.html">Luca Carlone</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12455">PDF</a><br /><b>Abstract: </b>Recent works in geometric deep learning have introduced neural networks that
allow performing inference tasks on three-dimensional geometric data by
defining convolution, and sometimes pooling, operations on triangle meshes.
These methods, however, either consider the input mesh as a graph, and do not
exploit specific geometric properties of meshes for feature aggregation and
downsampling, or are specialized for meshes, but rely on a rigid definition of
convolution that does not properly capture the local topology of the mesh. We
propose a method that combines the advantages of both types of approaches,
while addressing their limitations: we extend a primal-dual framework drawn
from the graph-neural-network literature to triangle meshes, and define
convolutions on two types of graphs constructed from an input mesh. Our method
takes features for both edges and faces of a 3D mesh as input and dynamically
aggregates them using an attention mechanism. At the same time, we introduce a
pooling operation with a precise geometric interpretation, that allows handling
variations in the mesh connectivity by clustering mesh faces in a task-driven
fashion. We provide theoretical insights of our approach using tools from the
mesh-simplification literature. In addition, we validate experimentally our
method in the tasks of shape classification and shape segmentation, where we
obtain comparable or superior performance to the state of the art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12455"><span class="datestr">at October 26, 2020 01:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12397">Quickly excluding a non-planar graph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawarabayashi:Ken=ichi.html">Ken-ichi Kawarabayashi</a>, Robin Thomas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wollan:Paul.html">Paul Wollan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12397">PDF</a><br /><b>Abstract: </b>A cornerstone theorem in the Graph Minors series of Robertson and Seymour is
the result that every graph $G$ with no minor isomorphic to a fixed graph $H$
has a certain structure. The structure can then be exploited to deduce
far-reaching consequences. The exact statement requires some explanation, but
roughly it says that there exist integers $k,n$ depending on $H$ only such that
$0&lt;k&lt;n$ and for every $n\times n$ grid minor $J$ of $G$ the graph $G$ has a a
$k$-near embedding in a surface $\Sigma$ that does not embed $H$ in such a way
that a substantial part of $J$ is embedded in $\Sigma$. Here a $k$-near
embedding means that after deleting at most $k$ vertices the graph can be drawn
in $\Sigma$ without crossings, except for local areas of non-planarity, where
crossings are permitted, but at most $k$ of these areas are attached to the
rest of the graph by four or more vertices and inside those the graph is
constrained in a different way, again depending on the parameter $k$.
</p>
<p>The original and only proof so far is quite long and uses many results
developed in the Graph Minors series. We give a proof that uses only our
earlier paper [A new proof of the flat wall theorem, {\it J.~Combin.\ Theory
Ser.\ B \bf 129} (2018), 158--203] and results from graduate textbooks.
</p>
<p>Our proof is constructive and yields a polynomial time algorithm to construct
such a structure. We also give explicit constants for the structure theorem,
whereas the original proof only guarantees the existence of such constants.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12397"><span class="datestr">at October 26, 2020 01:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12265">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12265">Model Interpretability through the Lens of Computational Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barcel=oacute=:Pablo.html">Pablo Barceló</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Monet:Mika=euml=l.html">Mikaël Monet</a>, Jorge Pérez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subercaseaux:Bernardo.html">Bernardo Subercaseaux</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12265">PDF</a><br /><b>Abstract: </b>In spite of several claims stating that some models are more interpretable
than others -- e.g., "linear models are more interpretable than deep neural
networks" -- we still lack a principled notion of interpretability to formally
compare among different classes of models. We make a step towards such a notion
by studying whether folklore interpretability claims have a correlate in terms
of computational complexity theory. We focus on local post-hoc explainability
queries that, intuitively, attempt to answer why individual inputs are
classified in a certain way by a given model. In a nutshell, we say that a
class $\mathcal{C}_1$ of models is more interpretable than another class
$\mathcal{C}_2$, if the computational complexity of answering post-hoc queries
for models in $\mathcal{C}_2$ is higher than for those in $\mathcal{C}_1$. We
prove that this notion provides a good theoretical counterpart to current
beliefs on the interpretability of models; in particular, we show that under
our definition and assuming standard complexity-theoretical assumptions (such
as P$\neq$NP), both linear and tree-based models are strictly more
interpretable than neural networks. Our complexity analysis, however, does not
provide a clear-cut difference between linear and tree-based models, as we
obtain different results depending on the particular post-hoc explanations
considered. Finally, by applying a finer complexity analysis based on
parameterized complexity, we are able to prove a theoretical result suggesting
that shallow neural networks are more interpretable than deeper ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12265"><span class="datestr">at October 26, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12227">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12227">Geometric Separability using Orthogonal Objects</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Abidha V P, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashok:Pradeesha.html">Pradeesha Ashok</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12227">PDF</a><br /><b>Abstract: </b>Given a bichromatic point set $P=\textbf{R}$ $ \cup$ $ \textbf{B}$ of red and
blue points, a separator is an object of a certain type that separates
$\textbf{R}$ and $\textbf{B}$. We study the geometric separability problem when
the separator is a) rectangular annulus of fixed orientation b) rectangular
annulus of arbitrary orientation c) square annulus of fixed orientation d)
orthogonal convex polygon. In this paper, we give polynomial time algorithms to
construct separators of each of the above type that also optimizes a given
parameter. Specifically, we give an $O(n^3 \log n)$ algorithm that computes
(non-uniform width) separating rectangular annulus in arbitrary orientation, of
minimum possible width. Further, when the orientation is fixed, we give an
$O(n\log n)$ algorithm that constructs a uniform width separating rectangular
annulus of minimum possible width and an $O(n\log^2 n)$ algorithm that
constructs a minimum width separating concentric square annulus. We also give
an optimal algorithm that computes a separating orthogonal convex polygon with
minimum number of edges, that runs in $O(n\log n)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12227"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12122">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12122">Quantum Meets Fine-grained Complexity: Sublinear Time Quantum Algorithms for String Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gall:Fran=ccedil=ois_Le.html">François Le Gall</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seddighin:Saeed.html">Saeed Seddighin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12122">PDF</a><br /><b>Abstract: </b>Longest common substring (LCS), longest palindrome substring (LPS), and Ulam
distance (UL) are three fundamental string problems that can be classically
solved in near linear time. In this work, we present sublinear time quantum
algorithms for these problems along with quantum lower bounds. Our results shed
light on a very surprising fact: Although the classic solutions for LCS and LPS
are almost identical (via suffix trees), their quantum computational
complexities are different. While we give an exact $\tilde O(\sqrt{n})$ time
algorithm for LPS, we prove that LCS needs at least time $\tilde
\Omega(n^{2/3})$ even for 0/1 strings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12122"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12081">Codes over integers, and the singularity of random matrices with large entries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karingula:Sankeerth_Rao.html">Sankeerth Rao Karingula</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12081">PDF</a><br /><b>Abstract: </b>The prototypical construction of error correcting codes is based on linear
codes over finite fields. In this work, we make first steps in the study of
codes defined over integers. We focus on Maximum Distance Separable (MDS)
codes, and show that MDS codes with linear rate and distance can be realized
over the integers with a constant alphabet size. This is in contrast to the
situation over finite fields, where a linear size finite field is needed.
</p>
<p>The core of this paper is a new result on the singularity probability of
random matrices. We show that for a random $n \times n$ matrix with entries
chosen independently from the range $\{-m,\ldots,m\}$, the probability that it
is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12081"><span class="datestr">at October 26, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12000">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12000">Computationally and Statistically Efficient Truncated Regression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daskalakis:Constantinos.html">Constantinos Daskalakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gouleakis:Themis.html">Themis Gouleakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzamos:Christos.html">Christos Tzamos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12000">PDF</a><br /><b>Abstract: </b>We provide a computationally and statistically efficient estimator for the
classical problem of truncated linear regression, where the dependent variable
$y = w^T x + \epsilon$ and its corresponding vector of covariates $x \in R^k$
are only revealed if the dependent variable falls in some subset $S \subseteq
R$; otherwise the existence of the pair $(x, y)$ is hidden. This problem has
remained a challenge since the early works of [Tobin 1958, Amemiya 1973,
Hausman and Wise 1977], its applications are abundant, and its history dates
back even further to the work of Galton, Pearson, Lee, and Fisher. While
consistent estimators of the regression coefficients have been identified, the
error rates are not well-understood, especially in high dimensions.
</p>
<p>Under a thickness assumption about the covariance matrix of the covariates in
the revealed sample, we provide a computationally efficient estimator for the
coefficient vector $w$ from $n$ revealed samples that attains $l_2$ error
$\tilde{O}(\sqrt{k/n})$. Our estimator uses Projected Stochastic Gradient
Descent (PSGD) without replacement on the negative log-likelihood of the
truncated sample. For the statistically efficient estimation we only need
oracle access to the set $S$.In order to achieve computational efficiency we
need to assume that $S$ is a union of a finite number of intervals but still
can be complicated. PSGD without replacement must be restricted to an
appropriately defined convex cone to guarantee that the negative log-likelihood
is strongly convex, which in turn is established using concentration of
matrices on variables with sub-exponential tails. We perform experiments on
simulated data to illustrate the accuracy of our estimator.
</p>
<p>As a corollary, we show that SGD learns the parameters of single-layer neural
networks with noisy activation functions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12000"><span class="datestr">at October 26, 2020 01:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11983">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11983">Learnability and Complexity of Quantum Samples</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niu:Murphy_Yuezhen.html">Murphy Yuezhen Niu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Andrew_M=.html">Andrew M. Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Li.html">Li Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Odena:Augustus.html">Augustus Odena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Zhengli.html">Zhengli Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smelyanskyi:Vadim.html">Vadim Smelyanskyi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neven:Hartmut.html">Hartmut Neven</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boixo:Sergio.html">Sergio Boixo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11983">PDF</a><br /><b>Abstract: </b>Given a quantum circuit, a quantum computer can sample the output
distribution exponentially faster in the number of bits than classical
computers. A similar exponential separation has yet to be established in
generative models through quantum sample learning: given samples from an
n-qubit computation, can we learn the underlying quantum distribution using
models with training parameters that scale polynomial in n under a fixed
training time? We study four kinds of generative models: Deep Boltzmann machine
(DBM), Generative Adversarial Networks (GANs), Long Short-Term Memory (LSTM)
and Autoregressive GAN, on learning quantum data set generated by deep random
circuits. We demonstrate the leading performance of LSTM in learning quantum
samples, and thus the autoregressive structure present in the underlying
quantum distribution from random quantum circuits. Both numerical experiments
and a theoretical proof in the case of the DBM show exponentially growing
complexity of learning-agent parameters required for achieving a fixed accuracy
as n increases. Finally, we establish a connection between learnability and the
complexity of generative models by benchmarking learnability against different
sets of samples drawn from probability distributions of variable degrees of
complexities in their quantum and classical representations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11983"><span class="datestr">at October 26, 2020 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11981">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11981">A novel auction system for selecting advertisements in Real-Time bidding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miralles=Pechu=aacute=n:Luis.html">Luis Miralles-Pechuán</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jim=eacute=nez:Fernando.html">Fernando Jiménez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garc=iacute=a:Jos=eacute=_Manuel.html">José Manuel García</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11981">PDF</a><br /><b>Abstract: </b>Real-Time Bidding is a new Internet advertising system that has become very
popular in recent years. This system works like a global auction where
advertisers bid to display their impressions in the publishers' ad slots. The
most popular system to select which advertiser wins each auction is the
Generalized second-price auction in which the advertiser that offers the most
wins the bet and is charged with the price of the second largest bet. In this
paper, we propose an alternative betting system with a new approach that not
only considers the economic aspect but also other relevant factors for the
functioning of the advertising system. The factors that we consider are, among
others, the benefit that can be given to each advertiser, the probability of
conversion from the advertisement, the probability that the visit is
fraudulent, how balanced are the networks participating in RTB and if the
advertisers are not paying over the market price. In addition, we propose a
methodology based on genetic algorithms to optimize the selection of each
advertiser. We also conducted some experiments to compare the performance of
the proposed model with the famous Generalized Second-Price method. We think
that this new approach, which considers more relevant aspects besides the
price, offers greater benefits for RTB networks in the medium and long-term.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11981"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9110359134467856278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html">Do not become obsessed with the Polls unless...</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I know someone who checks the polls 3 times a day to see who looks like they will be elected prez. She cares A LOT about the election. It is irrelevant to this post who she supports. <div><br /></div><div>I've asked her `if you find out that, say, Biden is up  8 points instead of 9 in Penn. or that Georgia is looking pretty safe for Trump, or that Texas is in play (really?) how will that change your life? What will YOU do differently?'</div><div><br /></div><div>She had no answer. Unfortunately she is still a poll-watcher (I know that means something else usually, but you know what I mean.)</div><div><br /></div><div>So who should be poll-watching or poll-obsessing?</div><div><br /></div><div>1) To be fair to my friend, she might decide to GIVE MORE to her candidate if the polls are saying that he will lose (whoops- by saying `he' I gave away that her candidate is NOT Jo Jorgenson- Libertarian).  I doubt my friend could say to the campaign `I want to you to spend it in state X since I read that its close there' (I read that some big donors in 2016 demanded more say in where the money was spend. I doubt that's a good idea since I suspect the party knows more about how to best spend the money then the donor does.) </div><div><br /></div><div>2) The Biden and the Trump Campaigns SHOULD be poll-watching to decide where to put their efforts. And I suspect they are doing just that.</div><div><br /></div><div>3) A really big donor (my friend is not one of those) MIGHT want to poll watch to decide if the candidate they want needs money. (I wonder if EITHER candidate needs money since they get so much free media.)</div><div><br /></div><div>4) Nate Silver-being a poll-watcher is kind-of his job. And of course writing columns about them and making predictions based on what he sees. My friend is not Nate Silver. </div><div><br /></div><div>5) Other people who have Nate Silver's job. I can't name any- is Nate Silver the most famous... Gee, not sure what job title he has... SO this is now two questions: What is his job title, call it X, and is he the most famous person who does X?</div><div><br /></div><div><br /></div><div>SO- my point- DO NOT be a poll-obsessive unless the information you get will lead to an action you can take. And I suspect that mostly it does not. </div><div><br /></div><div>The primaries are different: If a poll says A can beat X but B cannot beat X, that might guide who you vote for. </div><div><br /></div><div>Misc thought: </div><div><br /></div><div> I've heard the phrase `democratic pollster' and `republican pollster' These terms do not make sense. Would I call myself a `democratic Muffin Mathematician' ? My political leanings do not affect my search for truth about mathematical Muffins. Similarly, one would think that a pollster wants to find the TRUTH, even if its bad news for their employer, ESPECIALLY if its bad news for their employer, so they can help their employer fix it. The phrase `pollster employed by the X party' would make more sense-- however, whenever they are on TV they seem to always say that their candidate is doing well, even when they are not. </div><div><br /></div><div>ADDED LATER: Lance had a great tweet about this post: <i>do not obsess about polls, but DO obsess bout prediction markets. </i>I think in the past prediction markets have been better predictors but some group-think has set in so its no longer clear. (I could be wrong- but thats why I have heard.) </div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html"><span class="datestr">at October 25, 2020 08:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/157">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/157">TR20-157 |  Batch Verification and Proofs of Proximity with Polylog Overhead | 

	Guy Rothblum, 

	Ron Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose Alice wants to convince Bob of the correctness of k NP statements. Alice could send k witnesses to Bob, but as k grows the communication becomes prohibitive. Is it possible to convince Bob using smaller communication (without making cryptographic assumptions or bounding the computational power of a malicious Alice)? This is the question of batch verification for NP statements.

Our main result is a new interactive proof protocol for verifying the correctness of k UP statements (NP statements with a unique witness) using communication that is poly-logarithmic in k (and a fixed polynomial in the length of a single witness).

This result is obtained by making progress on a different question in the study of interactive proofs. Suppose Alice wants to convince Bob that a huge dataset has some property. Can this be done if Bob can't even read the entire input? In other words, what properties can be verified in sublinear time? An Interactive Proof of Proximity guarantees that Bob accepts if the input has the property, and rejects if the input is far (say in Hamming distance) from having the property. Two central complexity measures of such a protocol are the query and communication complexities (which should both be sublinear). For every query parameter $q$, and for every language in logspace uniform NC, we construct an interactive proof of proximity with query complexity $q$ and communication complexity $(n/q) \cdot \polylog(n)$.

Both results are optimal up to poly-logarithmic factors, under reasonable complexity-theoretic or cryptographic assumptions. The second result, which is our main technical contribution, builds on a distance amplification technique introduced in a beautiful recent work of Ben-Sasson, Kopparty and Saraf [CCC 2018].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/157"><span class="datestr">at October 25, 2020 10:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11880">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11880">Fast Approximate CoSimRanks via Random Projections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Renchi.html">Renchi Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11880">PDF</a><br /><b>Abstract: </b>Given a graph G with n nodes, and two nodes u,v in G, the CoSim-Rank value
s(u,v) quantifies the similarity between u and v based on graph topology.
Compared to SimRank, CoSimRank has been shown to be more accurate and effective
in many real-world applications including synonym expansion, lexicon
extraction, and entity relatedness in knowledge graphs. The computation of
all-pair CoSimRank values in G is highly expensive and challenging. Existing
methods all focus on devising approximate algorithms for the computation of
all-pair CoSimRanks. To attain the desired absolute error delta, the
state-of-the-art approximate algorithm for computing all-pair CoSimRank values
requires O(n^3log2(ln(1/delta))) time. In this paper, we propose RP-CoSim, a
randomized algorithm for computing all-pair CoSimRank values. The basic idea of
RP-CoSim is to reduce the n*n matrix multiplications into a k-dimensional(k&lt;&lt;n)
subspace via a random projection such that the pairwise inner product is
preserved within a certain error, and then iteratively approximate CoSimRank
values in the k-dimensional subspace in O(n^2k) time. Theoretically,
RP-CoSimruns in O(n^2*ln(n)*ln(1/delta)/delta^2) time, and meanwhile ensures an
absolute error of at most delta in the CoSimRank value of every two nodes in G
with a high probability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11880"><span class="datestr">at October 25, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11788">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11788">Equation satisfiability in solvable groups</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Paweł Idziak, Piotr Kawałek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krzaczkowski:Jacek.html">Jacek Krzaczkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Armin.html">Armin Weiß</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11788">PDF</a><br /><b>Abstract: </b>The study of the complexity of the equation satisfiability problem in finite
groups had been initiated by Goldmann and Russell (2002) where they showed that
this problem is in polynomial time for nilpotent groups while it is NP-complete
for non-solvable groups. Since then, several results have appeared showing that
the problem can be solved in polynomial time in certain solvable groups $G$
having a nilpotent normal subgroup $H$ with nilpotent factor $G/H$. This paper
shows that such normal subgroup must exist in each finite group with equation
satisfiability solvable in polynomial time, unless the Exponential Time
Hypothesis fails.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11788"><span class="datestr">at October 25, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11754">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11754">Separation Results for Boolean Function Classes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Aniruddha Biswas, Palash Sarkar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11754">PDF</a><br /><b>Abstract: </b>We show (almost) separation between certain important classes of Boolean
functions. The technique that we use is to show that the total influence of
functions in one class is less than the total influence of functions in the
other class. In particular, we show (almost) separation of several classes of
Boolean functions which have been studied in the coding theory and cryptography
from classes which have been studied in combinatorics and complexity theory.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11754"><span class="datestr">at October 25, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11658">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11658">On the Compressed-Oracle Technique, and Post-Quantum Security of Proofs of Sequential Work</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chung:Kai=Min.html">Kai-Min Chung</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fehr:Serge.html">Serge Fehr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Yu=Hsuan.html">Yu-Hsuan Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liao:Tai=Ning.html">Tai-Ning Liao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11658">PDF</a><br /><b>Abstract: </b>We revisit the so-called compressed oracle technique, introduced by Zhandry
for analyzing quantum algorithms in the quantum random oracle model (QROM). To
start off with, we offer a concise exposition of the technique, which easily
extends to the parallel-query QROM, where in each query-round the considered
algorithm may make several queries to the QROM in parallel. This variant of the
QROM allows for a more fine-grained query-complexity analysis.
</p>
<p>Our main technical contribution is a framework that simplifies the use of
(the parallel-query generalization of) the compressed oracle technique for
proving query complexity results. With our framework in place, whenever
applicable, it is possible to prove quantum query complexity lower bounds by
means of purely classical reasoning. More than that, for typical examples the
crucial classical observations that give rise to the classical bounds are
sufficient to conclude the corresponding quantum bounds.
</p>
<p>We demonstrate this on a few examples, recovering known results (like the
optimality of parallel Grover), but also obtaining new results (like the
optimality of parallel BHT collision search). Our main target is the hardness
of finding a $q$-chain with fewer than $q$ parallel queries, i.e., a sequence
$x_0, x_1,\ldots, x_q$ with $x_i = H(x_{i-1})$ for all $1 \leq i \leq q$.
</p>
<p>The above problem of finding a hash chain is of fundamental importance in the
context of proofs of sequential work. Indeed, as a concrete cryptographic
application of our techniques, we prove that the "Simple Proofs of Sequential
Work" proposed by Cohen and Pietrzak remains secure against quantum attacks.
Such an analysis is not simply a matter of plugging in our new bound; the
entire protocol needs to be analyzed in the light of a quantum attack. Thanks
to our framework, this can now be done with purely classical reasoning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11658"><span class="datestr">at October 25, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11632">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11632">The Primal-Dual method for Learning Augmented Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamas:=Eacute=tienne.html">Étienne Bamas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggiori:Andreas.html">Andreas Maggiori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11632">PDF</a><br /><b>Abstract: </b>The extension of classical online algorithms when provided with predictions
is a new and active research area. In this paper, we extend the primal-dual
method for online algorithms in order to incorporate predictions that advise
the online algorithm about the next action to take. We use this framework to
obtain novel algorithms for a variety of online covering problems. We compare
our algorithms to the cost of the true and predicted offline optimal solutions
and show that these algorithms outperform any online algorithm when the
prediction is accurate while maintaining good guarantees when the prediction is
misleading.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11632"><span class="datestr">at October 25, 2020 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11629">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11629">Learning Augmented Energy Minimization via Speed Scaling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamas:=Eacute=tienne.html">Étienne Bamas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggiori:Andreas.html">Andreas Maggiori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11629">PDF</a><br /><b>Abstract: </b>As power management has become a primary concern in modern data centers,
computing resources are being scaled dynamically to minimize energy
consumption. We initiate the study of a variant of the classic online speed
scaling problem, in which machine learning predictions about the future can be
integrated naturally. Inspired by recent work on learning-augmented online
algorithms, we propose an algorithm which incorporates predictions in a
black-box manner and outperforms any online algorithm if the accuracy is high,
yet maintains provable guarantees if the prediction is very inaccurate. We
provide both theoretical and experimental evidence to support our claims.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11629"><span class="datestr">at October 25, 2020 11:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11620">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11620">From trees to barcodes and back again: theoretical and statistical perspectives</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanari:Lida.html">Lida Kanari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garin:Ad=eacute=lie.html">Adélie Garin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hess:Kathryn.html">Kathryn Hess</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11620">PDF</a><br /><b>Abstract: </b>Methods of topological data analysis have been successfully applied in a wide
range of fields to provide useful summaries of the structure of complex data
sets in terms of topological descriptors, such as persistence diagrams. While
there are many powerful techniques for computing topological descriptors, the
inverse problem, i.e., recovering the input data from topological descriptors,
has proved to be challenging. In this article we study in detail the
Topological Morphology Descriptor (TMD), which assigns a persistence diagram to
any tree embedded in Euclidean space, and a sort of stochastic inverse to the
TMD, the Topological Neuron Synthesis (TNS) algorithm, gaining both theoretical
and computational insights into the relation between the two. We propose a new
approach to classify barcodes using symmetric groups, which provides a concrete
language to formulate our results. We investigate to what extent the TNS
recovers a geometric tree from its TMD and describe the effect of different
types of noise on the process of tree generation from persistence diagrams. We
prove moreover that the TNS algorithm is stable with respect to specific types
of noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11620"><span class="datestr">at October 25, 2020 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11571">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11571">A 4-Approximation of the $\frac{2\pi}{3}$-MST</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashur:Stav.html">Stav Ashur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katz:Matthew_J=.html">Matthew J. Katz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11571">PDF</a><br /><b>Abstract: </b>Bounded-angle (minimum) spanning trees were first introduced in the context
of wireless networks with directional antennas. They are reminiscent of
bounded-degree spanning trees, which have received significant attention. Let
$P = \{p_1,\ldots,p_n\}$ be a set of $n$ points in the plane, let $\Pi$ be the
polygonal path $(p_1,\ldots,p_n)$, and let $0 &lt; \alpha &lt; 2\pi$ be an angle. An
$\alpha$-spanning tree ($\alpha$-ST) of $P$ is a spanning tree of the complete
Euclidean graph over $P$, with the following property: For each vertex $p_i \in
P$, the (smallest) angle that is spanned by all the edges incident to $p_i$ is
at most $\alpha$. An $\alpha$-minimum spanning tree ($\alpha$-MST) is an
$\alpha$-ST of $P$ of minimum weight, where the weight of an $\alpha$-ST is the
sum of the lengths of its edges. In this paper, we consider the problem of
computing an $\alpha$-MST, for the important case where $\alpha =
\frac{2\pi}{3}$. We present a simple 4-approximation algorithm, thus improving
upon the previous results of Aschner and Katz and Biniaz et al., who presented
algorithms with approximation ratios 6 and $\frac{16}{3}$, respectively.
</p>
<p>In order to obtain this result, we devise a simple $O(n)$-time algorithm for
constructing a $\frac{2\pi}{3}$-ST\, ${\cal T}$ of $P$, such that ${\cal T}$'s
weight is at most twice that of $\Pi$ and, moreover, ${\cal T}$ is a 3-hop
spanner of $\Pi$. This latter result is optimal in the sense that for any
$\varepsilon &gt; 0$ there exists a polygonal path for which every
$\frac{2\pi}{3}$-ST has weight greater than $2-\varepsilon$ times the weight of
the path.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11571"><span class="datestr">at October 25, 2020 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11497">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11497">Cluster-and-Conquer: When Randomness Meets Graph Locality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giakkoupis:George.html">George Giakkoupis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kermarrec:Anne=Marie.html">Anne-Marie Kermarrec</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ruas:Olivier.html">Olivier Ruas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Ta=iuml=ani:Fran=ccedil=ois.html">François Taïani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11497">PDF</a><br /><b>Abstract: </b>K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining
and machine-learning applications. Some of the most efficient KNN graph
algorithms are incremental and local: they start from a random graph, which
they incrementally improve by traversing neighbors-of-neighbors links.
Paradoxically, this random start is also one of the key weaknesses of these
algorithms: nodes are initially connected to dissimilar neighbors, that lie far
away according to the similarity metric. As a result, incremental algorithms
must first laboriously explore spurious potential neighbors before they can
identify similar nodes, and start converging. In this paper, we remove this
drawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts
the starting configuration of greedy algorithms thanks to a novel lightweight
clustering mechanism, dubbed FastRandomHash. FastRandomHash leverages
random-ness and recursion to pre-cluster similar nodes at a very low cost. Our
extensive evaluation on real datasets shows that Cluster-and-Conquer
significantly outperforms existing approaches, including LSH, yielding
speed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN
quality.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11497"><span class="datestr">at October 25, 2020 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11462">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11462">Polynomial Delay Enumeration for Minimal Steiner Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurita:Kazuhiro.html">Kazuhiro Kurita</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wasa:Kunihiro.html">Kunihiro Wasa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11462">PDF</a><br /><b>Abstract: </b>Let $G = (V, E)$ be a undirected graph and let $W \subseteq V$ be a set of
terminals. A \emph{Steiner subgraph} of $(G, W)$ is a subgraph of $G$ that
contains all vertices of $W$ and there is a path between every pair of vertices
of $W$ in the subgraph. We say that a Steiner subgraph is minimal if it has no
proper Steiner subgraph. It is easy to observe that every minimal Steiner
subgraph forms a tree, which is called a minimal Steiner tree. We propose a
linear delay and polynomial space algorithm for enumerating all minimal Steiner
trees of $(G, W)$, which improves a previously known polynomial delay
enumeration algorithm in [Kimelfeld and Sagiv, Inf. Syst., 2008]. Our
enumeration algorithm can be extended to other Steiner problems: minimal
Steiner forests, minimal terminal Steiner trees, minimal directed Steiner
trees. As another variant of the minimal Steiner subgraph problem, we study the
problem of enumerating minimal induced Steiner subgraphs. We propose a
polynomial delay and exponential space enumeration algorithm of minimal induced
Steiner subgraphs for claw-free graphs, whereas the problem on general graphs
is shown to be at least as hard as the problem of enumerating minimal
transversals in hypergraphs. Contrary to these tractable results, we show that
the problem of enumerating minimal group Steiner trees is at least as hard as
the minimal transversal enumeration problem on hypergraphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11462"><span class="datestr">at October 25, 2020 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11456">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11456">An Investigation of the Recoverable Robust Assignment Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Dennis.html">Dennis Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Tim_A=.html">Tim A. Hartmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lendl:Stefan.html">Stefan Lendl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woeginger:Gerhard_J=.html">Gerhard J. Woeginger</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11456">PDF</a><br /><b>Abstract: </b>We investigate the so-called recoverable robust assignment problem on
balanced bipartite graphs with $2n$ vertices, a mainstream problem in robust
optimization: For two given linear cost functions $c_1$ and $c_2$ on the edges
and a given integer $k$, the goal is to find two perfect matchings $M_1$ and
$M_2$ that minimize the objective value $c_1(M_1)+c_2(M_2)$, subject to the
constraint that $M_1$ and $M_2$ have at least $k$ edges in common.
</p>
<p>We derive a variety of results on this problem. First, we show that the
problem is W[1]-hard with respect to the parameter $k$, and also with respect
to the recoverability parameter $k'=n-k$. This hardness result holds even in
the highly restricted special case where both cost functions $c_1$ and $c_2$
only take the values $0$ and $1$. (On the other hand, containment of the
problem in XP is straightforward to see.) Next, as a positive result we
construct a polynomial time algorithm for the special case where one cost
function is Monge, whereas the other one is Anti-Monge. Finally, we study the
variant where matching $M_1$ is frozen, and where the optimization goal is to
compute the best corresponding matching $M_2$, the second stage recoverable
assignment problem. We show that this problem variant is contained in the
randomized parallel complexity class $\text{RNC}_2$, and that it is at least as
hard as the infamous problem \probl{Exact Matching in Red-Blue Bipartite
Graphs} whose computational complexity is a long-standing open problem
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11456"><span class="datestr">at October 25, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11450">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11450">Optimal Approximation -- Smoothness Tradeoffs for Soft-Max Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epasto:Alessandro.html">Alessandro Epasto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahdian:Mohammad.html">Mohammad Mahdian</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11450">PDF</a><br /><b>Abstract: </b>A soft-max function has two main efficiency measures: (1) approximation -
which corresponds to how well it approximates the maximum function, (2)
smoothness - which shows how sensitive it is to changes of its input. Our goal
is to identify the optimal approximation-smoothness tradeoffs for different
measures of approximation and smoothness. This leads to novel soft-max
functions, each of which is optimal for a different application. The most
commonly used soft-max function, called exponential mechanism, has optimal
tradeoff between approximation measured in terms of expected additive
approximation and smoothness measured with respect to R\'enyi Divergence. We
introduce a soft-max function, called "piecewise linear soft-max", with optimal
tradeoff between approximation, measured in terms of worst-case additive
approximation and smoothness, measured with respect to $\ell_q$-norm. The
worst-case approximation guarantee of the piecewise linear mechanism enforces
sparsity in the output of our soft-max function, a property that is known to be
important in Machine Learning applications [Martins et al. '16, Laha et al.
'18] and is not satisfied by the exponential mechanism. Moreover, the
$\ell_q$-smoothness is suitable for applications in Mechanism Design and Game
Theory where the piecewise linear mechanism outperforms the exponential
mechanism. Finally, we investigate another soft-max function, called power
mechanism, with optimal tradeoff between expected \textit{multiplicative}
approximation and smoothness with respect to the R\'enyi Divergence, which
provides improved theoretical and practical results in differentially private
submodular optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11450"><span class="datestr">at October 25, 2020 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11443">Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Alexander.html">Alexander Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fred.html">Fred Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11443">PDF</a><br /><b>Abstract: </b>We study the problem of improving the performance of online algorithms by
incorporating machine-learned predictions. The goal is to design algorithms
that are both consistent and robust, meaning that the algorithm performs well
when predictions are accurate and maintains worst-case guarantees. Such
algorithms have been studied in a recent line of works due to Lykouris and
Vassilvitskii (ICML '18) and Purohit et al (NeurIPS '18). They provide
robustness-consistency trade-offs for a variety of online problems. However,
they leave open the question of whether these trade-offs are tight, i.e., to
what extent to such trade-offs are necessary. In this paper, we provide the
first set of non-trivial lower bounds for competitive analysis using
machine-learned predictions. We focus on the classic problems of ski-rental and
non-clairvoyant scheduling and provide optimal trade-offs in various settings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11443"><span class="datestr">at October 25, 2020 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11440">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11440">Vertex deletion into bipartite permutation graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Łukasz Bożyk, Jan Derbisz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krawczyk:Tomasz.html">Tomasz Krawczyk</a>, Jana Novotná, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okrasa:Karolina.html">Karolina Okrasa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11440">PDF</a><br /><b>Abstract: </b>A permutation graph can be defined as an intersection graph of segments whose
endpoints lie on two parallel lines $l_1$ and $l_2$, one on each. A bipartite
permutation graph is a permutation graph which is bipartite. In this paper we
study the parameterized complexity of the bipartite permutation vertex deletion
problem, which asks, for a given n-vertex graph, whether we can remove at most
k vertices to obtain a bipartite permutation graph. This problem is NP-complete
by the classical result of Lewis and Yannakakis. We analyze the structure of
the so-called almost bipartite permutation graphs which may contain holes
(large induced cycles) in contrast to bipartite permutation graphs. We exploit
the structural properties of the shortest hole in a such graph. We use it to
obtain an algorithm for the bipartite permutation vertex deletion problem with
running time $O(9^k\cdot n^9)$, and also give a polynomial-time 9-approximation
algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11440"><span class="datestr">at October 25, 2020 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11420">Deterministic Approximation for Submodular Maximization over a Matroid in Nearly Linear Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Han:Kai.html">Kai Han</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Zongmai.html">Zongmai Cao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cui:Shuang.html">Shuang Cui</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Benwei.html">Benwei Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11420">PDF</a><br /><b>Abstract: </b>We study the problem of maximizing a non-monotone, non-negative submodular
function subject to a matroid constraint. The prior best-known deterministic
approximation ratio for this problem is $\frac{1}{4}-\epsilon$ under
$\mathcal{O}(({n^4}/{\epsilon})\log n)$ time complexity. We show that this
deterministic ratio can be improved to $\frac{1}{4}$ under $\mathcal{O}(nr)$
time complexity, and then present a more practical algorithm dubbed
TwinGreedyFast which achieves $\frac{1}{4}-\epsilon$ deterministic ratio in
nearly-linear running time of
$\mathcal{O}(\frac{n}{\epsilon}\log\frac{r}{\epsilon})$. Our approach is based
on a novel algorithmic framework of simultaneously constructing two candidate
solution sets through greedy search, which enables us to get improved
performance bounds by fully exploiting the properties of independence systems.
As a byproduct of this framework, we also show that TwinGreedyFast achieves
$\frac{1}{2p+2}-\epsilon$ deterministic ratio under a $p$-set system constraint
with the same time complexity. To showcase the practicality of our approach, we
empirically evaluated the performance of TwinGreedyFast on two network
applications, and observed that it outperforms the state-of-the-art
deterministic and randomized algorithms with efficient implementations for our
problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11420"><span class="datestr">at October 25, 2020 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11381">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11381">Query strategies for priced information, revisited</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11381">PDF</a><br /><b>Abstract: </b>We consider the problem of designing query strategies for priced information,
introduced by Charikar et al. In this problem the algorithm designer is given a
function $f : \{0,1\}^n \to \{-1,1\}$ and a price associated with each of the
$n$ coordinates. The goal is to design a query strategy for determining $f$'s
value on unknown inputs for minimum cost.
</p>
<p>Prior works on this problem have focused on specific classes of functions. We
analyze a simple and natural strategy that applies to all functions $f$, and
show that its performance relative to the optimal strategy can be expressed in
terms of a basic complexity measure of $f$, its influence. For $\varepsilon \in
(0,\frac1{2})$, writing $\mathsf{opt}$ to denote the expected cost of the
optimal strategy that errs on at most an $\varepsilon$-fraction of inputs, our
strategy has expected cost $\mathsf{opt} \cdot \mathrm{Inf}(f)/\varepsilon^2$
and also errs on at most an $O(\varepsilon)$-fraction of inputs. This
connection yields new guarantees that complement existing ones for a number of
function classes that have been studied in this context, as well as new
guarantees for new classes.
</p>
<p>Finally, we show that improving on the parameters that we achieve will
require making progress on the longstanding open problem of properly learning
decision trees.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11381"><span class="datestr">at October 25, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11252">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11252">On Adaptive Distance Estimation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherapanamjeri:Yeshwanth.html">Yeshwanth Cherapanamjeri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nelson:Jelani.html">Jelani Nelson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11252">PDF</a><br /><b>Abstract: </b>We provide a static data structure for distance estimation which supports
{\it adaptive} queries. Concretely, given a dataset $X = \{x_i\}_{i = 1}^n$ of
$n$ points in $\mathbb{R}^d$ and $0 &lt; p \leq 2$, we construct a randomized data
structure with low memory consumption and query time which, when later given
any query point $q \in \mathbb{R}^d$, outputs a $(1+\epsilon)$-approximation of
$\lVert q - x_i \rVert_p$ with high probability for all $i\in[n]$. The main
novelty is our data structure's correctness guarantee holds even when the
sequence of queries can be chosen adaptively: an adversary is allowed to choose
the $j$th query point $q_j$ in a way that depends on the answers reported by
the data structure for $q_1,\ldots,q_{j-1}$. Previous randomized Monte Carlo
methods do not provide error guarantees in the setting of adaptively chosen
queries. Our memory consumption is $\tilde O((n+d)d/\epsilon^2)$, slightly more
than the $O(nd)$ required to store $X$ in memory explicitly, but with the
benefit that our time to answer queries is only $\tilde O(\epsilon^{-2}(n +
d))$, much faster than the naive $\Theta(nd)$ time obtained from a linear scan
in the case of $n$ and $d$ very large. Here $\tilde O$ hides
$\log(nd/\epsilon)$ factors. We discuss applications to nearest neighbor search
and nonparametric estimation.
</p>
<p>Our method is simple and likely to be applicable to other domains: we
describe a generic approach for transforming randomized Monte Carlo data
structures which do not support adaptive queries to ones that do, and show that
for the problem at hand, it can be applied to standard nonadaptive solutions to
$\ell_p$ norm estimation with negligible overhead in query time and a factor
$d$ overhead in memory.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11252"><span class="datestr">at October 25, 2020 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/">Tenure-Track/Tenured position at University of California, Davis (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of California, Davis, seeks an Assistant Professor in any area of theoretical computer science; exceptional candidates at the Associate and Full ranks may be considered.</p>
<p>Website: <a href="https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic">https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic</a><br />
Email: amenta@cs.ucdavis.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/"><span class="datestr">at October 24, 2020 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/">Simons-Berkeley Research Fellowships for Fall 2021 and Spring 2022 at The Simons Institute for the Theory of Computing (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons Institute invites applications for Simons-Berkeley Postdoctoral Fellowships and Simons-Berkeley Research Fellowships, to collaborate with UC Berkeley faculty and to participate in the semester-long programs in Fall 2021 and Spring 2022: “Computational Complexity of Statistical Inference”, “Geometric Methods in Optimization and Sampling”, “Causality”, and “Learning and Games”.</p>
<p>Website: <a href="https://simons.berkeley.edu">https://simons.berkeley.edu</a><br />
Email: simonsvisitorservices@berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/"><span class="datestr">at October 24, 2020 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17707">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/">Can We Solve It?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>It is a Friday</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/maynard/" rel="attachment wp-att-17709"><img src="https://rjlipton.files.wordpress.com/2020/10/maynard.jpg?w=600" alt="" class="alignright size-full wp-image-17709" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
James Maynard is a number theorist. He attended Cambridge as an undergrad and then moved to do his grad work at Oxford at Balliol College. He is now a professor at Oxford. He is one of the world experts on prime density type theorems. </p>
<p></p><p>
Today, since it is Friday, I thought we would discuss a <em>timely</em> idea of Maynard. Not an idea about time complexity or time in physics, but involving the use of time.</p>
<p>
</p><p></p><h2> Decimal Digits </h2><p></p>
<p></p><p>
No it’s not a technical idea of his. He has had many ideas, for instance, that shed light on the beautiful structure of primes. For example, he <a href="https://arxiv.org/abs/1604.01041">proved</a> in 2016 that</p>
<blockquote><p><b>Theorem 1</b> <em> For each decimal digit <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" />, there are infinitely many prime numbers that do not have <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" /> in their decimal expansion. </em>
</p></blockquote>
<p></p><p>
This is not known for all digit systems: For binary, our favorite system as complexity theorists, this is still an open problem. Of course a binary prime with only <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" title="{1}" />‘s must be of the form: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Bp%7D+-+1+%3D+111%5Cdots+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^{p} - 1 = 111\dots 1, " class="latex" title="\displaystyle  2^{p} - 1 = 111\dots 1, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> must be a prime.</p>
<p>These are the famous <a href="https://en.wikipedia.org/wiki/Mersenne_prime">Mersenne primes</a> named for Marin Mersenne. The largest prime is <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B82%2C589%2C933%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{82,589,933} - 1}" class="latex" title="{2^{82,589,933} - 1}" /> as of today—at least I believe this is true. For further discussion, see a 2001 <a href="https://projecteuclid.org/euclid.em/999188636">paper</a> by Samuel Wagstaff titled, “Prime Numbers with a Fixed Number of One Bits or Zero Bits in Their Binary Representation.” </p>
<p></p><h2> Maynard’s Friday Rule </h2><p></p>
<p></p><p>
Maynard’s idea is based on his quest to understand whether known techniques can solve some problem. Of course the best way to understand this is to solve the problem. His above theorem is a perfect example of this. In the abstract he says: </p>
<blockquote><p><b> </b> <em> The proof is an application of the Hardy-Littlewood circle method to a binary problem, and rests on obtaining suitable `Type I’ and `Type II’ arithmetic information for use in Harman’s sieve to control the minor arcs. </em>
</p></blockquote>
<p>The proof may be based on known techniques, but is still very hard. He needs <img src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{70}" class="latex" title="{70}" /> pages to make it work. </p>
<p>
Maynard’s idea is to set aside time to remind himself why existing techniques have not worked against math’s biggest open problems.</p>
<blockquote><p><b> </b> <em> I often spend Friday afternoons just thinking about trying to directly attack some famous problem. This is much less because I think there’s a realistic way of solving the problem, but more because I think it’s important for me to understand where plausible techniques fail. </em>
</p></blockquote>
<p></p><p>
One can imagine that he had a Friday afternoon think. During it he asked himself:</p>
<blockquote><p><b> </b> <em> Suppose I try to show that there are primes without some particular digit. This is a density type theorem. Well could I use the Hardy-Littlewood method. But it cannot work because <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /> Wait here is a possible way around the roadblock. Hmmm. </em>
</p></blockquote>
<p></p><p>
Maybe he looked at Terence Tao’s blog post on this very <a href="https://terrytao.wordpress.com/2012/05/20/heuristic-limitations-of-the-circle-method/">issue</a>. It helped that Maynard is an expert on the Hardy-Littlewood method, but perhaps thinking why it could not work helped him figure out how it could work. </p>
<p>
</p><p></p><h2> Our Friday Rule </h2><p></p>
<p></p><p>
Today is Friday, so I though what should I think about? What problems and what techniques? Here is a possible example. Let’s look at the <a href="https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/">On Lower Bounds for the Separating Word Problem</a>. </p>
<p>
An approach is based on the following. Let <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F(n)}" class="latex" title="{F(n)}" /> be the set of all <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(x)}" class="latex" title="{f(x)}" /> degree <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> polynomials, with coefficients <img src="https://s0.wp.com/latex.php?latex=%7B-1%2C0%2C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1,0,+1}" class="latex" title="{-1,0,+1}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> be a constant. Our hypothesis <strong>H</strong><img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" /> is: For every polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(x)}" class="latex" title="{f(x)}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F(n)}" class="latex" title="{F(n)}" />, there is some prime <img src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cle+Cn%5E%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p \le Cn^{\epsilon}}" class="latex" title="{p \le Cn^{\epsilon}}" />, so that for some <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1 \le k \le n}" class="latex" title="{1 \le k \le n}" /> 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%29+%5Cnot%5Cequiv+0+%5Cbmod+p.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(k) \not\equiv 0 \bmod p. " class="latex" title="\displaystyle  f(k) \not\equiv 0 \bmod p. " /></p>
<p>How small can <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" title="{\epsilon}" /> be so that <strong>H</strong><img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" /> is true? What are the methods that we should think about? What methods can we see that cannot prove H<img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" />? Can we, for example, show that we can use a random argument? Can we should that they are not enough primes <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> in the range? Hmmm <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do you like Maynard’s Friday rule? What problems and what techniques would you think about? </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/"><span class="datestr">at October 23, 2020 07:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/">3 postdocs at University of Lyon (apply by January 6, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Excellence Laboratory Milyon (Labex Milyon) opens the 2021 campaign of postdoctoral researchers in Lyon–Saint-Etienne in Mathematics, Computer Science and their interactions (including some aspects of theoretical physics).</p>
<p>Milyon offers three postdoctoral positions of two years with no teaching load for 2021–2023. The application is open to all research areas of labex Milyon.</p>
<p>Website: <a href="https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023--130160.kjsp?RH=1571748911317">https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023–130160.kjsp?RH=1571748911317</a><br />
Email: sabot@math.univ-lyon1.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/"><span class="datestr">at October 23, 2020 07:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7831">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/">Full-replica-symmetry-breaking based algorithms for dummies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.<br />`<br />This blog post is mainly a note to myself (i.e., I’m the “dummy” <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png" style="height: 1em;" class="wp-smiley" alt="😃" />), trying to work out some basic facts in some of this line of work. it was inspired by this <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">excellent talk of Eliran Subag</a>, itself part of a great <a href="https://simons.berkeley.edu/workshops/schedule/14243">Simons institute workshop</a> which I am still planning to watch the talks of. I am posting this in case it’s useful for others, but this is quite rough, missing many references, and I imagine I have both math mistakes as well as inaccuracies in how I refer to the literature – would be grateful for comments!</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw.png"><img src="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw-e1603479127948.png" alt="" class="wp-image-7833" /></a>Screen shot from <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">Eliran Subag’s talk</a> demonstrating the difference between “easy” and “hard” instances.</figure>



<p>In computer science, <em>optimization</em> is the task of finding an assignment <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> that minimizes some function <img src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x_1,\ldots,x_n)" class="latex" title="J(x_1,\ldots,x_n)" />. In statistical physics we think of <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> as the states of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> particles, and <img src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x_1,\ldots,x_n)" class="latex" title="J(x_1,\ldots,x_n)" /> as the <em>energy</em> of this state. Finding the minimum assignment corresponds to finding the <em>ground state</em>, and another computational problem is sampling from the <em>Gibbs distribution</em> where the probability of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is proportional to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+J%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-\beta J(x))" class="latex" title="\exp(-\beta J(x))" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta&gt;0" class="latex" title="\beta&gt;0" />.</p>



<p>Two prototypical examples of such problems are:</p>



<ol><li>Random 3SAT – in this case <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in { \pm 1 }^n" class="latex" title="x\in { \pm 1 }^n" /> and <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is the number of clauses violated by the assignment <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> for a random formula.</li><li>Sherrington-Kirpatrick model – in this case <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in { \pm 1 }^n" class="latex" title="x \in { \pm 1 }^n" /> and <img src="https://s0.wp.com/latex.php?latex=J%28x%29%3D+%5Csum_%7Bi%2Cj%7D+J_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)= \sum_{i,j} J_{i,j}x_ix_j" class="latex" title="J(x)= \sum_{i,j} J_{i,j}x_ix_j" /> where <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_{i,j}" class="latex" title="J_{i,j}" /> are independent normal variables with variance <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/n" class="latex" title="1/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" /> and variance <img src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2/n" class="latex" title="2/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />. (Another way to say it is that <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> is the matrix <img src="https://s0.wp.com/latex.php?latex=A%2BA%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A+A^\top" class="latex" title="A+A^\top" /> where <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />‘s entries are chosen i.i.d from <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{2n}))" class="latex" title="N(0,\tfrac{1}{2n}))" />.)</li></ol>



<p>The physics and CS intuition is that these two problems have very different computational properties. For random 3SAT (of the appropriate density), it is believed that the set of solutions is “shattered” in the sense that it is partitioned to exponentially many clusters, separated from one another by large distance. It is conjectured that in this setting the problem will be computationally hard. Similarly from the statistical physics point of view, it is conjectured that if we were to start with the uniform distribution (i.e., a “hot” system) and “lower the temperature” (increase <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta" class="latex" title="\beta" />) at a rate that is not exponentially slow then we will get “stuck” at a “metastable” state. This is analogous how when we heat up sand and then cool it quickly then rather than returning to its original state, the sand will get stuck at the metastable state of glass.</p>



<p>In contrast for the Sherrington-Kirpatrick (SK) model, the geometry is more subtle, but interestingly this enables better algorithms. The SK model is extermely widely studied, with hundreds of papers, and was the inspiration for the simulated annealing algorithm. If memory serves me right, Sherrington and Kirpatrick made the wrong conjecture on the energy of the ground state, and then Parisi came up in 1979 with a wonderful and hugely influential way to compute this value. Parisi’s calculation was heuristic, but about 30 years later, first Talagrand and later Panchenko proved rigorously many of Parisi’s conjectures. (See this <a href="https://arxiv.org/abs/1211.1094v1">survey of Panchenko</a>.)</p>



<p>Recently <a href="https://arxiv.org/abs/1812.10897">Montanari</a> gave a polynomial time algorithm to find a state with energy that is arbitrarily close to the ground state’s. The algorithm relies on Parisi’s framework and in particular on the fact that the solution space has a property known as “full replica symmetry breaking (RSB)” / “ultrametricity”. Parisi’s derivations (and hence also Montanari’s analysis) are highly elaborate and I admit that I have not yet been able to fully follow it. The nice thing is that (as we’ll see) it is possible to describe at least some of the algorithmic results without going into this theory. In the end of the post I will discuss a bit some of the relation to this theory, which is the underlying inspiration for Subag’s results described here.</p>



<p><strong>Note:</strong> These papers and this blog post deal with the <em>search problem</em> of finding a solution that minimizes the objective. The <em>refutation problem</em> of certifying that this minimum is at least <img src="https://s0.wp.com/latex.php?latex=-C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C" class="latex" title="-C" /> for some <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" title="C" /> has often been studied. The computational complexity of these problems need not be identical. In particular there are cases where the search problem has an efficient algorithm achieving value <img src="https://s0.wp.com/latex.php?latex=-C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C^*" class="latex" title="-C^*" /> but the best refutation algorithm can only certify that the value is at most <img src="https://s0.wp.com/latex.php?latex=-C%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C'" class="latex" title="-C'" /> for <img src="https://s0.wp.com/latex.php?latex=C%27+%5Cgg+C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C' \gg C^*" class="latex" title="C' \gg C^*" />.</p>



<h2>Analysis of a simpler setting</h2>



<p>Luckily, there is a similar computational problem, for which the analysis of analogous algorithm, which was <a href="https://arxiv.org/abs/1812.04588">discovered by Subag</a> and was the partial inspiration for Montanari’s work, is much simpler. Specifically, we consider the case where <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is an element of the unit sphere, and <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is a degree <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> polynomial with random Gaussian coefficients. Specifically, for every vector <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%28%5Cgamma_2%2C%5Cldots%2C%5Cgamma_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (\gamma_2,\ldots,\gamma_d)" class="latex" title="\gamma = (\gamma_2,\ldots,\gamma_d)" />, we let <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" /> where for every <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p \in {2,\ldots, d }" class="latex" title="p \in {2,\ldots, d }" />, <img src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_p" class="latex" title="J_p" /> is a random tensor of order <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> whose <img src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^p" class="latex" title="n^p" /> coefficients are all chosen i.i.d in <img src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,1/n)" class="latex" title="N(0,1/n)" />. (We assume that polynomial does not have constant or linear components.)</p>



<p>Depending on <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma" class="latex" title="\gamma" />, the computational and geometrical properties of this problem can vary considerably. The case that <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (1,0,\ldots,0)" class="latex" title="\gamma = (1,0,\ldots,0)" /> (i.e., only <img src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^2" class="latex" title="J^2" /> has a non-zero coeffiecent) corresponds to finding the unit vector <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> minimizing <img src="https://s0.wp.com/latex.php?latex=x%5E%5Ctop+J+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\top J x" class="latex" title="x^\top J x" /> for a random matrix <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, which of course corresponds to the efficiently solveable minimum eigenvector problem. In contrast, the case <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C1%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (0,1,0,\ldots,0)" class="latex" title="\gamma = (0,1,0,\ldots,0)" /> corresponds to finding a rank one component of a random three-tensor, which is believed to be computationally difficult. The Parisi calculations give a precise condition <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> on the vector <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma" class="latex" title="\gamma" /> such that if <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds then the solution space has the “full RSB” property (and hence the problem is computationally easy) and if <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> does not hold then the solution space does not have this property (and potentially the problem is hard).</p>



<p>These calculations also give rise to the following theorem:</p>



<p><strong>Theorem (<a href="https://arxiv.org/abs/1512.08492">Chen and Sen, Proposition 2</a>):</strong> If <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds then in the limit <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow \infty" class="latex" title="n \rightarrow \infty" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%3A+%7Cx%7C%3D1%7D+J%28x%29+%3D+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" class="latex" title="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29+%3D+%5Csum_%7Bp+%5Cgeq+2%7D+%5Cgamma_p%5E2+q%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" class="latex" title="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" />. (That is, <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''" class="latex" title="\nu''" /> is the second derivative of this univariate polynomial)</p>



<p>We will not discuss the proof of this theorem, but rather how, taking it as a black box, it leads to an algorithm for minimizing <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> that achieves a near-optimal value (assuming <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds).</p>



<p>It is a nice exercise to show that for every two vectors <img src="https://s0.wp.com/latex.php?latex=x%2Cx%27%5Cin%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,x'\in\mathbb{R}^n" class="latex" title="x,x'\in\mathbb{R}^n" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_J+%5BJ%28x%29J%28x%27%29%5D+%3D+%5Cnu%28%5Clangle+x%2Cx%27+%5Crangle%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" class="latex" title="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" />. Hence for any unit vector <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is a random variable with mean zero and standard deviation <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%281%29%2Fn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\nu(1)/n}" class="latex" title="\sqrt{\nu(1)/n}" />. Since (after some coarsening) the number of unit vectors of dimension <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> can be thought of as <img src="https://s0.wp.com/latex.php?latex=c%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c^n" class="latex" title="c^n" /> for some <img src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c&gt;1" class="latex" title="c&gt;1" />, and we expect the probability of deviating <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" title="t" /> standard deviations to be <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c%27+t%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-c' t^2)" class="latex" title="\exp(-c' t^2)" />, the minimum value of <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> should be <img src="https://s0.wp.com/latex.php?latex=-c%27%27+%5Csqrt%7B%5Cnu%281%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-c'' \sqrt{\nu(1)}" class="latex" title="-c'' \sqrt{\nu(1)}" /> for some constant <img src="https://s0.wp.com/latex.php?latex=c%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c''" class="latex" title="c''" />. However determining this constant is non trivial and is the result of the Parisi theory.</p>



<p>To get a better sense for the quantity <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq" />, let’s consider two simple cases:</p>



<ul><li>If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (1,0,\ldots)" class="latex" title="\gamma = (1,0,\ldots)" /> (i.e., <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Csum_%7Bi%2Cj%7DM_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \sum_{i,j}M_{i,j}x_ix_j" class="latex" title="J(x) = \sum_{i,j}M_{i,j}x_ix_j" /> for random matrix <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" />) then <img src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29%3D+q%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu(q)= q^2" class="latex" title="\nu(q)= q^2" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28q%29+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''(q) = 2" class="latex" title="\nu''(q) = 2" />, meaning that <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-%5Csqrt%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" />. This turns out to be the actual minimum value. Indeed in this case <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7B%7Cx%7C%5E2%3D1%7D+J%28x%29+%3D+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda_%7Bmin%7D%28M+%2B+M%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" class="latex" title="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" />. But the matrix <img src="https://s0.wp.com/latex.php?latex=A%3D%5Ctfrac%7B1%7D%7B2%7D%28M%2BM%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A=\tfrac{1}{2}(M+M^\top)" class="latex" title="A=\tfrac{1}{2}(M+M^\top)" />‘s non diagonal entries are distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{2n})" class="latex" title="N(0,\tfrac{1}{2n})" /> and the diagonal entries like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{n})" class="latex" title="N(0,\tfrac{1}{n})" /> which means that <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> is distributed as <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{1}{\sqrt{2}}" class="latex" title="\tfrac{1}{\sqrt{2}}" /> times a random matrix <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> from the <em>Gaussian Orthogonal Ensemble (GOE)</em> where <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B_{i,j} \sim N(0,1/n)" class="latex" title="B_{i,j} \sim N(0,1/n)" /> for off diagonal entries and <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B_{i,i} \sim N(0,2/n)" class="latex" title="B_{i,i} \sim N(0,2/n)" /> for diagonal entries. The minimum eigenvalue of such matrices is known to be <img src="https://s0.wp.com/latex.php?latex=-2%5Cpm+o%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-2\pm o(1)" class="latex" title="-2\pm o(1)" /> with high probability.<br /></li><li>If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C%5Cldots%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (0,\ldots,1)" class="latex" title="\gamma = (0,\ldots,1)" /> (i.e. <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+T+%5Ccdot+x%5E%7B%5Cotimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = T \cdot x^{\otimes d}" class="latex" title="J(x) = T \cdot x^{\otimes d}" /> for a random <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />-tensor <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" />) then <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> does not hold. Indeed, in this case the value <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-+%5Cint_0%5E1+%5Csqrt%7Bd+%28d-1%29q%5E%7Bd-2%7D%7D+dq+%3D+-+%5Csqrt%7Bd%28d-1%29%7D%5Ctfrac%7B1%7D%7Bd%2F2-1%7D+%5Capprox+-2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" /> for large <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />. However I believe (though didn’t find the reference) that the actual minimum tends to <img src="https://s0.wp.com/latex.php?latex=-%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\infty" class="latex" title="-\infty" /> with <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />. </li></ul>



<p>While the particular form of the property <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> is not important for this post, there are several equivalent ways to state it, see Proposition 1 in <a href="https://arxiv.org/abs/1812.04588">Subag’s paper</a>. One of them is that the function <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28t%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''(t)^{-1/2}" class="latex" title="\nu''(t)^{-1/2}" /> (note the negative exponent) is concave on the interval <img src="https://s0.wp.com/latex.php?latex=%280%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(0,1]" class="latex" title="(0,1]" />.<br />It can be shown that this condition cannot be satisfied if <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 = 0" class="latex" title="\gamma_2 = 0" />, and that for every setting of <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3%2C%5Cldots%2C%5Cgamma_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3,\ldots,\gamma_d" class="latex" title="\gamma_3,\ldots,\gamma_d" />, if <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2" class="latex" title="\gamma_2" /> is large enough then it will be satisfied.</p>



<p>The central result of Subag’s paper is the following:</p>



<p><strong>Theorem:</strong> For every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" />, there is a polynomial-time algorithm <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> such that on input random <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> chosen according to the distribution above, with high probability <img src="https://s0.wp.com/latex.php?latex=A%28J%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(J)=x" class="latex" title="A(J)=x" /> such that <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%5Cleq+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" class="latex" title="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" />.</p>



<p>The algorithm itself, and the idea behind the analysis are quite simple. In some sense it’s the second algorithm you would think of (or at least the second algorithm according to some ordering).</p>



<p>The first algorithm one would think of is gradient descent. We start at some initial point <img src="https://s0.wp.com/latex.php?latex=x%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0" class="latex" title="x^0" />, and repeat the transformation <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+%5Cnabla+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" class="latex" title="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" /> for some small <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> (and normalizing the norm). Unfortunately, we will generally run into <em>saddle points</em> when we do so, with the gradient being zero. In fact, for simplicity, below we will always make the pessimistic assumption that we are constantly on a saddle point. (This assumption turns out to be true in the actual algorithm, and if it was not then we can always use gradient descent until we hit a saddle.)</p>



<p>The second algorithm one could think of would be to use the Hessian instead of the gradient. That is, repeat the transformation <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" title="x^{t+1} \leftarrow x^t - \eta u" /> where <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> is the minimal eigenvector of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> (i.e., the Hessian matrix <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> such that <img src="https://s0.wp.com/latex.php?latex=H_%7Bi%2Cj%7D+%3D+%5Ctfrac%7B%5Cpartial+J%28x%5Et%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" class="latex" title="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" /> ). By the Taylor approximation <img src="https://s0.wp.com/latex.php?latex=J%28x+-+%5Ceta+u%29+%5Capprox+J%28x%29+-+%5Ceta+%5Cnabla+J%28x%29+%5Ccdot+u+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+u%5E%5Ctop+%5Cnabla%5E2+J%28x%29+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" class="latex" title="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" /> (and since we assume the gradient is zero) the change in the objective will be roughly <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" class="latex" title="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" />. (Because we assume the gradient vanishes, it will not make a difference whether we update with <img src="https://s0.wp.com/latex.php?latex=-%5Ceta+u+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\eta u " class="latex" title="-\eta u " /> or <img src="https://s0.wp.com/latex.php?latex=%2B%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="+\eta u" class="latex" title="+\eta u" />, but we use <img src="https://s0.wp.com/latex.php?latex=-%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\eta" class="latex" title="-\eta" /> for consistency with gradient descent.)</p>



<p>The above approach is promising, but we still need some control over the norm. The way that Subag handles this is that he starts with <img src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0 = 0" class="latex" title="x^0 = 0" />, and at each step takes a step in an orthogonal direction, and so within <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta^2" class="latex" title="1/\eta^2" /> steps he will get to a unit norm vector. That is, the algorithm is as follows:</p>



<p><strong>Algorithm:</strong></p>



<p><strong>Input:</strong> <img src="https://s0.wp.com/latex.php?latex=J%3A%5Cmathbb%7BR%7D%5En+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J:\mathbb{R}^n \rightarrow \mathbb{R}" class="latex" title="J:\mathbb{R}^n \rightarrow \mathbb{R}" />.</p>



<p><strong>Goal:</strong> Find unit <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> approximately minimizing <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /></p>



<ol><li>Initialize <img src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0 = 0^n" class="latex" title="x^0 = 0^n" /></li><li>For <img src="https://s0.wp.com/latex.php?latex=t%3D0%2C%5Cldots%2C1%2F%5Ceta%5E2-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t=0,\ldots,1/\eta^2-1" class="latex" title="t=0,\ldots,1/\eta^2-1" />: i. Let <img src="https://s0.wp.com/latex.php?latex=u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^t" class="latex" title="u^t" /> be a unit vector <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> such that <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> is orthogonal to <img src="https://s0.wp.com/latex.php?latex=u%5E0%2C%5Cldots%2Cu%5E%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^0,\ldots,u^{t-1}" class="latex" title="u^0,\ldots,u^{t-1}" /> and <img src="https://s0.wp.com/latex.php?latex=u%5E%5Ctop+%5Cnabla%5E2+J%28x%5Et%29+u+%5Capprox+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" class="latex" title="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" />. (Since the bottom eigenspace of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> has large dimention, we can find a vector <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> that is not only nearly minimal eigenvector but also orthogonal to all prior ones. Also, as mentioned, we assume that <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+%5Ccdot+u+%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla \cdot u \approx 0" class="latex" title="\nabla \cdot u \approx 0" />.) ii. Set <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u^t" class="latex" title="x^{t+1} \leftarrow x^t - \eta u^t" />.</li><li>Output <img src="https://s0.wp.com/latex.php?latex=x%5E%7B1%2F%5Ceta%5E2%7D+%3D+-%5Ceta%5Csum_%7Bt%3D0%7D%5E%7B1%2F%5Ceta%5E2-1%7D+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" class="latex" title="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" /></li></ol>



<p>(The fact that the number of steps is <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta^2" class="latex" title="1/\eta^2" /> and not <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta" class="latex" title="1/\eta" /> is absolutely crucial for the algorithm’s success; without it we would not have been able to use the second order contribution that arise from the Hessian.)</p>



<p>If we define <img src="https://s0.wp.com/latex.php?latex=%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda_t" class="latex" title="\lambda_t" /> to be the minimum eigenvalue at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" title="t" />, we get that the final objective value achieved by the algorithm satisfies</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" class="latex" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" /></p>



<p>Now due to rotation invariance, the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J" class="latex" title="\nabla^2 J" /> at the point <img src="https://s0.wp.com/latex.php?latex=x%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^t" class="latex" title="x^t" /> is the same as <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J" class="latex" title="\nabla^2J" /> at the point <img src="https://s0.wp.com/latex.php?latex=%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(|x^t|,0,\ldots,0)" class="latex" title="(|x^t|,0,\ldots,0)" />. Using concentration of measure arguments, it can be shown that the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> will be close with high probability to the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(|x^t|,0,\ldots,0)" class="latex" title="\nabla^2 J(|x^t|,0,\ldots,0)" />.<br />Since <img src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5Et%5C%7C%5E2+%3D+%5Ceta%5E2+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\|x^t\|^2 = \eta^2 t" class="latex" title="\|x^t\|^2 = \eta^2 t" /> we can write</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda%28%5Csqrt%7B%5Ceta%5E2+t%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" class="latex" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda(q)" class="latex" title="\lambda(q)" /> is the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J" class="latex" title="\nabla^2 J" /> at the point <img src="https://s0.wp.com/latex.php?latex=%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(q,0,\ldots,0)" class="latex" title="(q,0,\ldots,0)" />.<br />Taking <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> to zero, we get that (approximately) the value of the solution output by the algorithm will satisfy</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Cint_0%5E1+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda%28%5Csqrt%7Bq%7D%29+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" class="latex" title="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" /></p>



<p>and hence the result will be completed by showing that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda%28%5Csqrt%7Bq%7D%29+%3D+2+%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" class="latex" title="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" /></p>



<p>To do this, let’s recall the definition of <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />:</p>



<p><img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" /> where for every <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p \in {2,\ldots, d }" class="latex" title="p \in {2,\ldots, d }" />, <img src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_p" class="latex" title="J_p" /> is a random tensor of order <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> whose <img src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^p" class="latex" title="n^p" /> coefficients are all chosen i.i.d in <img src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,1/n)" class="latex" title="N(0,1/n)" />.</p>



<p>For simplicity, let’s assume that <img src="https://s0.wp.com/latex.php?latex=d%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=3" class="latex" title="d=3" /> and hence</p>



<p><img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D%5C%3B.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." /></p>



<p>(The calculations in the general case are similar)</p>



<p>The <img src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i,j" class="latex" title="i,j" /> entry of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%5Csqrt%7Bq%7D%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(\sqrt{q},0,\ldots,0)" class="latex" title="\nabla^2 J(\sqrt{q},0,\ldots,0)" /> equals <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+J%28q%2C0%2C%5Cldots%2C0%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" class="latex" title="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" />. The contribution of the <img src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^2" class="latex" title="J^2" /> component to this term only arises from the terms corresponding to either <img src="https://s0.wp.com/latex.php?latex=x_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_ix_j" class="latex" title="x_ix_j" /> or <img src="https://s0.wp.com/latex.php?latex=x_jx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_jx_i" class="latex" title="x_jx_i" /> and hence for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" /> it equals <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%28J_%7Bi%2Cj%7D%2BJ_%7Bj%2Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 (J_{i,j}+J_{j,i})" class="latex" title="\gamma_2 (J_{i,j}+J_{j,i})" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+N%280%2C%5Ctfrac%7B2%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B2%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" class="latex" title="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" />. For <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />, since <img src="https://s0.wp.com/latex.php?latex=%28x_i%5E2%29%27%27+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_i^2)'' = 2" class="latex" title="(x_i^2)'' = 2" />, the contributioon equals <img src="https://s0.wp.com/latex.php?latex=2+%5Cgamma_2+J_%7Bi%2Ci%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2 \gamma_2 J_{i,i}" class="latex" title="2 \gamma_2 J_{i,i}" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B4%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{4\gamma_2^2}{n})" class="latex" title="N(0,\tfrac{4\gamma_2^2}{n})" />.</p>



<p>The contribution from the <img src="https://s0.wp.com/latex.php?latex=J%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^3" class="latex" title="J^3" /> component comes (in the case <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />) from all the <img src="https://s0.wp.com/latex.php?latex=6%3D3%21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="6=3!" class="latex" title="6=3!" /> terms involving <img src="https://s0.wp.com/latex.php?latex=1%2Ci%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1,i,j" class="latex" title="1,i,j" /> that is, <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+%28J_%7B1%2Ci%2Cj%7D%5Csqrt%7Bq%7D+%2B+J_%7B1%2Cj%2Ci%7D%5Csqrt%7Bq%7D+%2B+J_%7Bi%2C1%2Cj%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2C1%2Ci%7D%5Csqrt%7Bq%7D%2BJ_%7Bi%2Cj%2C1%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2Ci%2C1%7D%29%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" class="latex" title="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+N%280%2C+%5Ctfrac%7B6%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B6%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" class="latex" title="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" />. For <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" /> the contribution will be from the <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3" class="latex" title="3" /> terms involving <img src="https://s0.wp.com/latex.php?latex=1%2Ci&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1,i" class="latex" title="1,i" />, with each yielding a contribution of <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2\sqrt{q}" class="latex" title="2\sqrt{q}" />, and hence the result will be distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B12+%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{12 \gamma_3^2 q}{n})" class="latex" title="N(0,\tfrac{12 \gamma_3^2 q}{n})" />.</p>



<p>(More generally, for larger <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, the number of terms for distinct <img src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i,j" class="latex" title="i,j" /> is <img src="https://s0.wp.com/latex.php?latex=p%28p-1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(p-1)" class="latex" title="p(p-1)" />, each contributing a Gaussian of standard deviation <img src="https://s0.wp.com/latex.php?latex=%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" title="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" />, while for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" /> we have <img src="https://s0.wp.com/latex.php?latex=p%28p-1%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(p-1)/2" class="latex" title="p(p-1)/2" /> terms, each contributing a Gaussian of standard deviation <img src="https://s0.wp.com/latex.php?latex=2%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" title="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" />.)</p>



<p>Since the sum of Gaussians is a Gaussian we get that <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J(q,0,\ldots,0){i,j}" class="latex" title="\nabla^2J(q,0,\ldots,0){i,j}" /> is distributed like a Gaussian with variance <img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cgamma_p%5E2+p%28p-1%29q%5E%7Bp-2%7D%2Fn+%3D+%5Cnu%27%27%28q%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" class="latex" title="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />, and twice that for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />. This means that the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J(q,0,\ldots,0)" class="latex" title="\nabla^2J(q,0,\ldots,0)" /> equals <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\nu''(q)}" class="latex" title="\sqrt{\nu''(q)}" /> times the minimum eigenvalue of a random matrix <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" /> from the Gaussian Orthogonal Ensemble (i.e. <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" /> is sampled via <img src="https://s0.wp.com/latex.php?latex=M%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M{i,j} \sim N(0,1/n)" class="latex" title="M{i,j} \sim N(0,1/n)" />, <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,i} \sim N(0,2/n)" class="latex" title="M_{i,i} \sim N(0,2/n)" />). As mentioned above, it is known that this minimum eigenvalue is <img src="https://s0.wp.com/latex.php?latex=-2%2Bo%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-2+o(1)" class="latex" title="-2+o(1)" />, and in fact by the semi-circle law, for every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" />, the number of eigenvalues of value <img src="https://s0.wp.com/latex.php?latex=%5Cleq+-2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\leq -2+\epsilon" class="latex" title="\leq -2+\epsilon" /> is <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Omega(n)" class="latex" title="\Omega(n)" />, and so we can also pick one that is orthogonal to the previous directions. QED</p>



<h2>Full replica symmetry breaking and ultra-metricity</h2>



<p>The point of this blog post is that at least in the “mixed <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> spin” case considered by Subag, we can understand what the algorithm does and the value that it achieves without needing to go into the theory of the geometry of the solution space, but let me briefly discuss some of this theory. (As I mentioned, I am still reading through this, and so this part should be read with big grains of salt.)</p>



<p>The key object studied in this line of work is the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\xi" class="latex" title="\xi" /> of the dot product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x,x' \rangle" class="latex" title="\langle x,x' \rangle" /> for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" /> sampled independently from the Gibbs distribution induced by <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />. (This probability distribution will depend on the number of dimensions <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, but we consider the case that <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow \infty" class="latex" title="n \rightarrow \infty" />.)</p>



<p>Intuitively, there are several ways this probability distribution can behave, depending on how the solution space is “clustered”:</p>



<ul><li>If all solutions are inside a single “cluster”, in the sense that they are all of the form <img src="https://s0.wp.com/latex.php?latex=x+%3D+x_%2A+%2B+e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x = x_* + e" class="latex" title="x = x_* + e" /> where <img src="https://s0.wp.com/latex.php?latex=x_%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_*" class="latex" title="x_*" /> is the “center” of the cluster and <img src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e" class="latex" title="e" /> is some random vector, then <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x,x' \rangle" class="latex" title="\langle x,x' \rangle" /> will be concentrated on the point <img src="https://s0.wp.com/latex.php?latex=%5C%7C+x_%2A%5C%7C%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\| x_*\|^2" class="latex" title="\| x_*\|^2" />.<br /></li><li>If the solutions are inside a finite number <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> of clusters, with centers <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_k" class="latex" title="x_1,\ldots,x_k" />, then the support of the distribution will be on the <img src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k^2" class="latex" title="k^2" /> points <img src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Clangle+x_i%2Cx_j+%5Crangle+%5C%7D_%7Bi%2Cj+%5Cin+%5Bk%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" class="latex" title="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" />.<br /></li><li>Suppose that the solutions are inside a <em>hierarchy</em> of clusters. That is, suppose we have some rooted tree <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" /> (e.g., think of a depth <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> full binary tree), and we associate a vector <img src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_v" class="latex" title="x_v" /> with every vertex <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" />, with the property that <img src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_v" class="latex" title="x_v" /> is orthogonal to all vectors associated with <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />‘s ancestors on the tree. Now imagine that the distribution is obtained by taking a random leaf <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" /> and outputting <img src="https://s0.wp.com/latex.php?latex=%5Csum+x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum x_v" class="latex" title="\sum x_v" /> for all vertices <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> on the path from the root to <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. In such a case the dot product of <img src="https://s0.wp.com/latex.php?latex=x_u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_u" class="latex" title="x_u" /> and <img src="https://s0.wp.com/latex.php?latex=x_%7Bu%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{u'}" class="latex" title="x_{u'}" /> will be <img src="https://s0.wp.com/latex.php?latex=%5Csum_v+%5C%7Cx_v%5C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_v \|x_v\|^2" class="latex" title="\sum_v \|x_v\|^2" /> taken over all the common ancestors of <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />. As the dimension and depth of the tree goes to infinity, the distribution over dot product can have continuous support, and it is this setting (specifically when the support is an interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[0,q)" class="latex" title="[0,q)" />) that is known as <em>full replica symmetry breaking</em>. Because the dot product is determined by common ancestor, for every three vectors <img src="https://s0.wp.com/latex.php?latex=x_u%2Cx_v%2Cx_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_u,x_v,x_w" class="latex" title="x_u,x_v,x_w" /> in the support of the distribution <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_v%2Cx_w+%5Crangle+%5Cgeq+%5Cmin+%5C%7B+%5Clangle+x_u%2Cx_v+%5Crangle%2C+%5Clangle+x_u%2Cx_w+%5Crangle+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" class="latex" title="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" /> or <img src="https://s0.wp.com/latex.php?latex=%5C%7C+x_v+-+x_w+%5C%7C+%5Cleq+%5Cmax+%5C%7B+%5C%7Cx_u+-+x_v+%5C%7C%2C+%5C%7Cx_u+-x_w+%5C%7C%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" class="latex" title="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" />. It is this condition that known as <em>ultra-metricity</em>.</li></ul>



<p>In Subag’s algorithm, as mentioned above, at any given step we could make an update of either <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" title="x^{t+1} \leftarrow x^t - \eta u" /> or <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+%2B+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t + \eta u" class="latex" title="x^{t+1} \leftarrow x^t + \eta u" />. If we think of all the possible choices for the signs in the <img src="https://s0.wp.com/latex.php?latex=d%3D1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=1/\eta^2" class="latex" title="d=1/\eta^2" /> of the algorithms, we see that the algorithm does not only produce a single vector but actually <img src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^d" class="latex" title="2^d" /> such vectors that are arranged in an ultrametric tree just as above. Indeed, this ultrametric structure was the inspiration for the algorithm and is the reason why the algorithm produces the correct result precisely in the full replica symmetry breaking regime.</p>



<p><strong>Acknowledgements:</strong> Thanks to Tselil Schramm for helpful comments.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/"><span class="datestr">at October 23, 2020 07:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/">Tenure-Track Assistant Professor at Rutgers University (apply by January 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered.</p>
<p>Website: <a href="http://jobs.rutgers.edu/postings/120527">http://jobs.rutgers.edu/postings/120527</a><br />
Email: martin@farach-colton.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/"><span class="datestr">at October 22, 2020 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/">Professorship/Chair at TU Hamburg (apply by November 29, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1">https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1</a><br />
Email: berufungen@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/"><span class="datestr">at October 22, 2020 04:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17694">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/">Vaccines are Not Developing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>The search for a vaccine—is not a development.</em><br />
</span></p>
<p>Edward Jenner was an English physician who created the first vaccine, one for smallpox.<br />
<a href="https://rjlipton.wordpress.com/jenner/"><img src="https://rjlipton.files.wordpress.com/2020/10/jenner.jpg?w=600" alt="" class="alignright size-full wp-image-17696" /></a><br />
In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have <i>saved more lives than any other human.</i></p>
<p>Today there is an attempt to create a vaccine against our small-pox of the 21st century.</p>
<p>In his day small-pox killed 10% or more of populations. In our day there is a similar threat. and thus the immense interest in the development of a vaccine. However, there is a misunderstanding about vaccines for COVID-19 that is pervasive. Read the New York Times or watch cable news—CNN, FOX, MSNBC—where “experts” explain how AstraZeneca, Johnson &amp; Johnson, Novavax, and other drug companies are developing a vaccine. What developing means could potentially affect all of us, and a better understanding could save millions of lives.</p>
<p>They are not currently <i>developing</i> the vaccines, they are <i>testing</i> them.  The point we want to emphasize is:</p>
<blockquote>
<p><b> </b> <em> <i>The development of a vaccine does not change the vaccine. The vaccine is the same at the start of its testing trials, and remains the same throughout.</i> </em></p>
</blockquote>
<p>The Oxford vaccine AZD1222 is the same today as it was months ago when it was created. The same is true for the other vaccines currently being tested around the world.</p>
<p>A vaccine is <b>not</b> developed in the usual sense. Drug companies can modify: how the drug is made, how it is stored, how it is given, how many doses are needed, and so on. Drug companies cannot modify the vaccine without starting over—the vaccine must remain the same. Trials can lead to a vaccine being adopted, or it can cause the vaccine to be abandoned. In the later case the drug company can try again, but with a different vaccine.</p>
<h2>Not Development</h2>
<p>Think of the what development means elsewhere.</p>
<ul>
<li>In programming an app: We build a version and try it out. We find bugs and fix them. We use version numbers. Note, there is no AZD1222 version 3.</li>
<li>In writing a book: We make a draft. We have people read the draft. We fix typos and inaccuracies. Our quantum book’s <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bnd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{nd}}" class="latex" title="{2^{nd}}" /> edition is on version 11.</li>
<li>In engineering a product: You get the idea.</li>
</ul>
<p>Here is a sample explaining vaccine <a href="https://www.rivm.nl/en/novel-coronavirus-covid-19/vaccine-against-covid-19">development</a>:</p>
<ul>
<li>Phase I: The vaccine is given to healthy volunteers to see whether it is safe and what dosage (quantity) is most effective.</li>
<li>Phase II: The vaccine is given to target groups that would be vaccinated to see what dosage (quantity) is most effective and how well the immune system responds to it.</li>
<li>Phase III: The vaccine is given to an even larger group, consisting of thousands of people, to see how well the vaccine works to prevent COVID-19. People who do receive the vaccine are then compared with people who did not receive the vaccine.Note: there is no step that modifies the vaccine.
</li></ul>
<h2>Consequences</h2>
<p>There are several consequences from this insight about vaccines. For one it makes sense to order millions of doses of a vaccine, even one that has not yet been proved to be safe and effective. For example,</p>
<blockquote>
<p><b> </b> <em> The European Commission has placed its first advance order for a coronavirus vaccine, snapping up 300 million doses of AstraZeneca’s AZD1222 candidate developed by the University of Oxford, with an option on another 100 million. </em></p>
</blockquote>
<p>Note we would never order a large number of copies of a book before all editing and typos were fixed. This is a “proof” that the vaccine is the same.</p>
<p>Actually it may make sense to even begin to take the vaccine. Especially for high risk people. In the past inventors of vaccines have often taken their own new vaccine, even before they were sure they worked.</p>
<h2>Open Problems</h2>
<p>I am a computer scientist with no experience in vaccines. In 1954 I did help test the Jonas Salk polio vaccine. My help was in the form supplying an arm that got a shot of the Salk polio vaccine, I was nine years old then. But I have a math view of vaccines—a viewpoint that sheds light on this misunderstanding.</p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/"><span class="datestr">at October 22, 2020 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=499">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/">TCS+ talk: Wednesday, October 28 — Omar Montasser, TTIC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Omar Montasser</strong> from TTIC will speak about “<em>Adversarially Robust Learnability: Characterization and Reductions</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We study the question of learning an adversarially robust predictor from uncorrupted samples. We show that any VC class <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> is robustly PAC learnable, but we also show that such learning must sometimes be improper (i.e. use predictors from outside the class), as some VC classes are not robustly properly learnable.  In particular, the popular robust empirical risk minimization approach (also known as adversarial training), which is proper, cannot robustly learn all VC classes.  After establishing learnability, we turn to ask whether having a tractable non-robust learning algorithm is sufficient for tractable robust learnability and give a reduction algorithm for robustly learning any hypothesis class <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> using a non-robust PAC learner for <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" />, with nearly-optimal oracle complexity.<br />This is based on joint work with Steve Hanneke and Nati Srebro, available at <a href="https://arxiv.org/abs/1902.04217">https://arxiv.org/abs/1902.04217</a>.</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/"><span class="datestr">at October 22, 2020 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/">Computer Science, Tenured/Tenure-track at NYU Shanghai (apply by February 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on.</p>
<p>Website: <a href="https://apply.interfolio.com/80168">https://apply.interfolio.com/80168</a><br />
Email: shanghai.faculty.recruitment@nyu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/"><span class="datestr">at October 22, 2020 01:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/156">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/156">TR20-156 |  Codes over integers, and the singularity of random matrices with large entries | 

	Sankeerth Rao Karingula, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The prototypical construction of error correcting codes is based on linear codes over finite fields. In this work, we make first steps in the study of codes defined over integers. We focus on Maximum Distance Separable (MDS) codes, and show that MDS codes with linear rate and distance can be realized over the integers with a constant alphabet size. This is in contrast to the situation over finite fields, where a linear size finite field is needed.

The core of this paper is a new result on the singularity probability of random matrices. We show that for a random $n \times n$ matrix with entries chosen independently from the range $\{-m,\ldots,m\}$, the probability that it is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/156"><span class="datestr">at October 22, 2020 05:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
