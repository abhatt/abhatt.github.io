<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/3722233/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 07, 2022 10:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-06-07-approx-agreement-one/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-06-07-approx-agreement-one/">Approximate Agreement: definitions and the robust midpoint protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This post covers the basics of Approximate Agreement. We define the problem, explain in what way its an interesting variation of classic Agreement, and describe the idea behind the robust midpoint protocol. In classic consensus, the space of possible decision values is just a set and the goal is to...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-06-07-approx-agreement-one/"><span class="datestr">at June 07, 2022 07:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20150">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/">Laws and Laughs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><i>Rules are a great way to get ideas. All you have to do is break them—Jack Foster</i></p>
<p>
Roy Amara was a researcher and president of the Institute for the Future.  Among things he is known for is coining <a href="https://en.wikipedia.org/wiki/Roy_Amara">Amara’s law</a> on the effect of technology. </p>
<p>
Today Ken and I want to discuss “laws”. We hope you will like the smile that many of these give us. Perhaps they will give you some too. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/am/" rel="attachment wp-att-20153"><img width="196" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/am.png?resize=196%2C257&amp;ssl=1" class="aligncenter size-full wp-image-20153" height="257" /></a></p>
<p>
</p><p><b> Amara’s Law and More </b></p>
<p></p><p>
Amara’s law is: <br />
 <i>We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.</i> </p>
<p>
Tom Cargill’s law is: <br />
 <i>The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.</i></p>
<p>
Arthur Clarke’s law is: <br />
 <i>When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.</i></p>
<p>
Fred Brooks’s law is: <br />
 <i>Adding manpower to a late software project makes it later.</i></p>
<p>
Brooks explains it:  First, it takes the new guy some time to learn about the project before becoming productive. Teaching him takes resources that could otherwise be put into the project itself. Second, communication overheads increase as the number of people increases – sometimes, they spend more time talking to each other to keep the project in sync, rather than working on the project itself. </p>
<p>
Here is a list of many additional interesting <a href="https://www.pcmag.com/encyclopedia/term/laws">laws</a> from <i>PC Magazine</i>. Some of our favorites are:</p>
<ul>
<li>
The best way to get the right answer on the Internet is not to ask a question; it’s to post the wrong answer. <p></p>
</li><li>
If you can think of four ways that something can go wrong, it will go wrong in the fifth way. <p></p>
</li><li>
Everything takes longer than you think it will. <p></p>
</li><li>
Nature always sides with the hidden flaw. <p></p>
</li><li>
The light at the end of the tunnel is only the light of an oncoming train.
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
The third of the last five laws from <i>PC Magazine</i> was given a fuller <a href="https://en.wikipedia.org/wiki/Hofstadter%27s_law">statement</a> by Douglas Hofstadter: </p>
<blockquote><p>
<b>Hofstadter’s Law</b>: It always takes longer than you expect, even when you take into account <b>Hofstadter’s Law</b>.
</p></blockquote>
<p>
Ken notes a similarity to his “law of prediction” at the end of the “Paradox” section of his <a href="https://rjlipton.wpcomstaging.com/2018/08/25/do-you-want-to-know-a-secret/">post</a> about Mark Glickman: <i>All simple and elegant prediction models are overconfident.</i></p>
<p>
What are some of your own favorite laws?</p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/"><span class="datestr">at June 06, 2022 11:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://corner.mimuw.edu.pl/?p=1111">WOLA 2022: Call for contributed talks &amp; please register</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>

This is a reminder that the Workshop on Local Algorithms (WOLA) is<br />taking place in Warsaw this year from June 25 to 27. It is conveniently<br />scheduled right after STOC in Rome. Keynote speakers include Bernhard<br />Haeupler, Rotem Oshman, Michael Kapralov, and Vincent Cohen-Addad. More<br />information can be found on the official webpage:<br /><a href="https://ideas-ncbr.pl/en/wola/" target="_blank" rel="noreferrer noopener">https://ideas-ncbr.pl/en/wola/</a></p>



<p># Contributed talks and registration</p>



<p>Apart from keynote talks, WOLA is going back to the pre-pandemic<br />tradition of contributed talks by the workshop participants. **If you<br />are planning to attend in person and want to give a talk**, please<br />register as soon as possible at<br /><a href="https://ideas-ncbr.pl/en/wola/registration/" target="_blank" rel="noreferrer noopener">https://ideas-ncbr.pl/en/wola/registration/</a>. Even if you are not<br />planning to give a talk, registering helps the local organizers with<br />estimating the number of attendees, which is useful for events such as...</p>



<p># A night tour of Warsaw on a retro bus</p>



<p>See the attached picture for what such a bus looks like! I'm told by the<br />local organizers that this will include a tasting of Polish specialties.<br />This event is scheduled for Saturday night.</p>



<figure class="wp-block-image size-large"><img src="https://corner.mimuw.edu.pl/wp-content/uploads/2022/06/retro_bus.jpg" alt="" class="wp-image-1112" /></figure>



<p># Traveling from STOC in Rome</p>



<p>The workshop is starting at 1:30pm on Saturday to allow for more options<br />for traveling from STOC. There are direct flights with Wizz Air on both<br />Friday night and Saturday morning. You can also fly with KLM (via AMS)<br />or Air France (via CDG) on Saturday morning if you prefer a more<br />traditional air carrier.

</p></div>







<p class="date">
by sank <a href="https://corner.mimuw.edu.pl/?p=1111"><span class="datestr">at June 06, 2022 07:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2022/06/06/PGDL/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2022/06/06/PGDL/">Predicting Generalization using GANs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A central problem of generalization theory is the following: <em>Given a training dataset and a deep net trained with that dataset, give a mathematical estimate of the test error.</em></p>

<p>While this may seem useless to a practitioner (“why not just retain some holdout data for testing?”), this is of great interest to theorists trying to understand what properties of the net or the training algorithm lead to good generalization. Old posts on this blog such as <a href="http://www.offconvex.org/2018/02/17/generalization2/">this one</a> mentioned the difficulties in applying classic generalization theory to get estimates of generalization error even within a few orders of magnitude.</p>

<p>This blog post is about the topic of a 
 NeurIPS 20 competition <a href="https://sites.google.com/view/pgdl2020">Predicting Generalization in Deep Learning competition</a> which suggested using machine learning techniques to  understand network properties that promote generalization! Contestants are given datasets and a set of deep nets trained on these datasets using unknown techniques. The goal is to rank the trained nets according to generalization error. Contestants provide a python code that, given a training set and a trained net, outputs a scalar estimate of the generalization error. The contestant’s performance is computed using a ranking measure of how well their scalar score correlates with actual generalization. 
 Several interesting ideas emerged from the competition, such as measuring networks’ resilience to distortions to the input; collecting specific statistics of hidden node activations; and measuring output consistency of identically structured networks but trained by SGD with different random seeds.</p>

<p>This blog post describes our <a href="https://arxiv.org/abs/2111.14212">ICLR22 spotlight paper</a>, coauthored with Nikunj Saunshi and Arushi Gupta, that gives a surprisingly easy method to predict generalization using <a href="http://www.offconvex.org/2017/03/15/GANs/">Generative Adversarial Nets</a> or GANs.</p>

<h2 id="predicting-generalization-using-gans">Predicting Generalization using GANs</h2>
<p>Given a training dataset and a collection of nets trained using it, our method is very simple:</p>

<blockquote>
  <p><strong>Step 1)</strong> Train a Generative Adversarial Network (GAN) on the same training dataset a</p>

  <p><strong>Step 2)</strong> Draw samples from the GAN generator to obtain a synthetic dataset.</p>

  <p><strong>Step 3)</strong> For each classifier in the task, use its classification error on the synthetic data as our prediction for its true test error</p>
</blockquote>

<p>We find for a variety of image datasets that using pre-trained Studio-GANs directly downloaded from <a href="https://github.com/POSTECH-CVLab/PyTorch-StudioGAN">public repositories</a>, we obtain scores that would ‘‘not only outperforms other methods but blows them out of the water’’ (to cite one of the reviews for our paper), including the winning methods proposed at the PGDL competition (see Table 1 in the paper for quantitative results). Here we show plots of true test errors v.s. errors on the synthetic data, where we generate one synthetic dataset from a GAN trained on each dataset, and each dot in the figures represents a trained deep net classifier.</p>

<figure align="center">
<img width="80%" alt="drawing" src="http://www.offconvex.org/assets/PGDL_linear_fit.jpg" />
</figure>

<p>Not only is the correlation linear; it appears remarkably close to the $y=x$ fit! Also the trained deep net classifiers here belong to drastically different architecture families —including VGG, ResNet, DenseNet, ShuffleNet, PNASNet, and MobileNet— which don’t correspond to the discriminator of Studio-GAN.</p>

<p>Frankly, this success at predicting generalization confounded the authors, some of whom had earlier shown that GANs <a href="http://www.offconvex.org/2017/07/06/GANs3/">do not learn the distribution well, and suffer from serious mode collapse</a>).</p>

<p>Thus we conclude following three statements all appear to be true:</p>

<blockquote>
  <p><strong>Observation 1)</strong> GAN samples suffer from mode collapse (as detected by the <a href="http://www.offconvex.org/2017/07/06/GANs3/">birthday paradox test</a> and do not appear to be as diverse as the distribution of images the GANs was trained on.</p>
</blockquote>

<blockquote>
  <p><strong>Observation 2)</strong> Training deep net classifiers using only GAN samples leads to poor performance. This is another evidence that GAN samples are poor substitutes for the real thing.</p>
</blockquote>

<blockquote>
  <p><strong>Observation 3)</strong> Yet GAN samples are good enough to substitute for holdout data to give a reasonable prediction of test performance.</p>
</blockquote>

<p>Note however that there is no inherent contradiction here. For instance, suppose the GAN’s distribution has limited diversity: it only knows how to generate a $10,000$  random images, as well as $1000$ minor variations of each of these images. So long as the  $10,000$ distinct images are like random draws from the full distribution, it could  predict generalization for a trained net reasonably well, even though a million image samples from the GAN would not suffice to replace ImageNet’s 1 million images for training a deep net from scratch. (In fact this scenario was predicted and to some extent verified by <a href="https://proceedings.mlr.press/v70/arora17a.html">our earlier work on why GANs may not be able to avoid mode collapse</a>.)</p>

<p>We hope our work motivates further investigations of the power of real-life GANs and what can be done using samples from their distributions.</p>

<p>(Aside: Of course, the field has now progressed beyond simple GANs to multimodal generators like <a href="https://openai.com/blog/dall-e/">DALL-E</a>, and it would be interesting to understand how the generated images there can be leveraged to predict generalization.)</p></div>







<p class="date">
<a href="http://offconvex.github.io/2022/06/06/PGDL/"><span class="datestr">at June 06, 2022 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01706">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01706">Weighted Model Counting with Twin-Width</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ganian:Robert.html">Robert Ganian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pokr=yacute=vka:Filip.html">Filip Pokrývka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schidler:Andr=eacute=.html">André Schidler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simonov:Kirill.html">Kirill Simonov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szeider:Stefan.html">Stefan Szeider</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01706">PDF</a><br /><b>Abstract: </b>Bonnet et al. (FOCS 2020) introduced the graph invariant twin-width and
showed that many NP-hard problems are tractable for graphs of bounded
twin-width, generalizing similar results for other width measures, including
treewidth and clique-width. In this paper, we investigate the use of twin-width
for solving the propositional satisfiability problem (SAT) and propositional
model counting. We particularly focus on Bounded-ones Weighted Model Counting
(BWMC), which takes as input a CNF formula $F$ along with a bound $k$ and asks
for the weighted sum of all models with at most $k$ positive literals. BWMC
generalizes not only SAT but also (weighted) model counting.
</p>
<p>We develop the notion of "signed" twin-width of CNF formulas and establish
that BWMC is fixed-parameter tractable when parameterized by the certified
signed twin-width of $F$ plus $k$. We show that this result is tight: it is
neither possible to drop the bound $k$ nor use the vanilla twin-width instead
if one wishes to retain fixed-parameter tractability, even for the easier
problem SAT. Our theoretical results are complemented with an empirical
evaluation and comparison of signed twin-width on various classes of CNF
formulas.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01706"><span class="datestr">at June 06, 2022 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01688">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01688">L-systems for Measuring Repetitiveness*</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, Cristian Urbina University of Chile, CeBiB) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01688">PDF</a><br /><b>Abstract: </b>An L-system (for lossless compression) is a CPD0L-system extended with two
parameters $d$ and $n$, which determines unambiguously a string $w =
\tau(\varphi^d(s))[1:n]$, where $\varphi$ is the morphism of the system, $s$ is
its axiom, and $\tau$ is its coding. The length of the shortest description of
an L-system generating $w$ is known as $\ell$, and is arguably a relevant
measure of repetitiveness that builds on the self-similarities that arise in
the sequence.
</p>
<p>In this paper we deepen the study of the measure $\ell$ and its relation with
$\delta$, a better established lower bound that builds on substring complexity.
Our results show that $\ell$ and $\delta$ are largely orthogonal, in the sense
that one can be much larger than the other depending on the case. This suggests
that both sources of repetitiveness are mostly unrelated. We also show that the
recently introduced NU-systems, which combine the capabilities of L-systems
with bidirectional macro-schemes, can be asymptotically strictly smaller than
both mechanisms, which makes the size $\nu$ of the smallest NU-system the
unique smallest reachable repetitiveness measure to date.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01688"><span class="datestr">at June 06, 2022 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01568">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01568">Improved Deterministic Connectivity in Massively Parallel Computation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Manuela.html">Manuela Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giliberti:Jeff.html">Jeff Giliberti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grunau:Christoph.html">Christoph Grunau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01568">PDF</a><br /><b>Abstract: </b>A long line of research about connectivity in the Massively Parallel
Computation model has culminated in the seminal works of Andoni et al.
[FOCS'18] and Behnezhad et al. [FOCS'19]. They provide a randomized algorithm
for low-space MPC with conjectured to be optimal round complexity $O(\log D +
\log \log_{\frac m n} n)$ and $O(m)$ space, for graphs on $n$ vertices with $m$
edges and diameter $D$. Surprisingly, a recent result of Coy and Czumaj
[STOC'22] shows how to achieve the same deterministically. Unfortunately,
however, their algorithm suffers from large local computation time. We present
a deterministic connectivity algorithm that matches all the parameters of the
randomized algorithm and, in addition, significantly reduces the local
computation time to nearly linear. Our derandomization method is based on
reducing the amount of randomness needed to allow for a simpler efficient
search. While similar randomness reduction approaches have been used before,
our result is not only strikingly simpler, but it is the first to have
efficient local computation. This is why we believe it to serve as a starting
point for the systematic development of computation-efficient derandomization
approaches in low-memory MPC.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01568"><span class="datestr">at June 06, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01398">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01398">A closer look at TDFA</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Borsotti:Angelo.html">Angelo Borsotti</a>, Ulya Trafimovich <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01398">PDF</a><br /><b>Abstract: </b>We present an algorithm for regular expression parsing and submatch
extraction based on tagged deterministic finite automata. The algorithm works
with different disambiguation policies. We give detailed pseudocode for the
algorithm, covering important practical optimizations. All transformations from
a regular expression to an optimized automaton are explained on a step-by-step
example. We consider both ahead-of-time and just-in-time determinization and
describe variants of the algorithm suited to each setting. We provide
benchmarks showing that the algorithm is very fast in practice. Our research is
based on two independent implementations: an open-source lexer generator RE2C
and an experimental Java library.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01398"><span class="datestr">at June 06, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01382">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01382">Falconn++: A Locality-sensitive Filtering Approach for Approximate Nearest Neighbor Search</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pham:Ninh.html">Ninh Pham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Tao.html">Tao Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01382">PDF</a><br /><b>Abstract: </b>We present Falconn++, a novel locality-sensitive filtering (LSF) approach for
approximate nearest neighbor search on angular distance. Falconn++ can filter
out potential far away points in any hash bucket before querying, which results
in higher quality candidates compared to other hashing-based solutions.
Theoretically, Falconn++ asymptotically achieves lower query time complexity
than Falconn, an optimal locality-sensitive hashing scheme on angular distance.
Empirically, Falconn++ achieves a higher recall-speed tradeoff than Falconn on
many real-world data sets. Falconn++ is also competitive against HNSW, an
efficient representative of graph-based solutions on high search recall
regimes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01382"><span class="datestr">at June 06, 2022 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01360">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01360">Balancing Flow Time and Energy Consumption</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Sami.html">Sami Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khuller:Samir.html">Samir Khuller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Shirley.html">Shirley Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01360">PDF</a><br /><b>Abstract: </b>In this paper, we study the following batch scheduling model: find a schedule
that minimizes total flow time for $n$ uniform length jobs, with release times
and deadlines, where the machine is only actively processing jobs in at most
$k$ synchronized batches of size at most $B$. Prior work on such batch
scheduling models has considered only feasibility with no regard to the flow
time of the schedule. However, algorithms that minimize the cost from the
scheduler's perspective -- such as ones that minimize the active time of the
processor -- can result in schedules where the total flow time is arbitrarily
high \cite{ChangGabowKhuller}. Such schedules are not valuable from the
perspective of the client. In response, our work provides dynamic programs
which minimize flow time subject to active time constraints. Our main
contribution focuses on jobs with agreeable deadlines; for such job instances,
we introduce dynamic programs that achieve runtimes of O$(B \cdot k \cdot n)$
for unit jobs and O$(B \cdot k \cdot n^5)$ for uniform length jobs. These
results improve upon our modification of a different, classical dynamic
programming approach by Baptiste. While the modified DP works when deadlines
are non-agreeable, this solution is more expensive, with runtime $O(B \cdot k^2
\cdot n^7)$ \cite{Baptiste00}.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01360"><span class="datestr">at June 06, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01280">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01280">On the Parallel Parameterized Complexity of MaxSAT Variants</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannach:Max.html">Max Bannach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skambath:Malte.html">Malte Skambath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tantau:Till.html">Till Tantau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01280">PDF</a><br /><b>Abstract: </b>In the maximum satisfiability problem (MAX-SAT) we are given a propositional
formula in conjunctive normal form and have to find an assignment that
satisfies as many clauses as possible. We study the parallel parameterized
complexity of various versions of MAX-SAT and provide the first constant-time
algorithms parameterized either by the solution size or by the allowed excess
relative to some guarantee ("above guarantee" versions). For the dual
parameterized version where the parameter is the number of clauses we are
allowed to leave unsatisfied, we present the first parallel algorithm for
MAX-2SAT (known as ALMOST-2SAT). The difficulty in solving ALMOST-2SAT in
parallel comes from the fact that the iterative compression method, originally
developed to prove that the problem is fixed-parameter tractable at all, is
inherently sequential. We observe that a graph flow whose value is a parameter
can be computed in parallel and use this fact to develop a parallel algorithm
for the vertex cover problem parameterized above the size of a given matching.
Finally, we study the parallel complexity of MAX-SAT parameterized by the
vertex cover number, the treedepth, the feedback vertex set number, and the
treewidth of the input's incidence graph. While MAX-SAT is fixed-parameter
tractable for all of these parameters, we show that they allow different
degrees of possible parallelization. For all four we develop dedicated parallel
algorithms that are constructive, meaning that they output an optimal
assignment - in contrast to results that can be obtained by parallel
meta-theorems, which often only solve the decision version.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01280"><span class="datestr">at June 06, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2206.01270">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2206.01270">Max-Weight Online Stochastic Matching: Improved Approximations Against the Online Benchmark</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Mark.html">Mark Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Derakhshan:Mahsa.html">Mahsa Derakhshan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Antonio_Molina.html">Antonio Molina Lovett</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2206.01270">PDF</a><br /><b>Abstract: </b>In this paper, we study max-weight stochastic matchings on online bipartite
graphs under both vertex and edge arrivals. We focus on designing polynomial
time approximation algorithms with respect to the online benchmark, which was
first considered by Papadimitriou, Pollner, Saberi, and Wajc [EC'21].
</p>
<p>In the vertex arrival version of the problem, the goal is to find an
approximate max-weight matching of a given bipartite graph when the vertices in
one part of the graph arrive online in a fixed order with independent chances
of failure. Whenever a vertex arrives we should decide, irrevocably, whether to
match it with one of its unmatched neighbors or leave it unmatched forever.
There has been a long line of work designing approximation algorithms for
different variants of this problem with respect to the offline benchmark
(prophet). Papadimitriou et al., however, propose the alternative online
benchmark and show that considering this new benchmark allows them to improve
the 0.5 approximation ratio, which is the best ratio achievable with respect to
the offline benchmark. They provide a 0.51-approximation algorithm which was
later improved to 0.526 by Saberi and Wajc [ICALP'21]. The main contribution of
this paper is designing a simple algorithm with a significantly improved
approximation ratio of (1-1/e) for this problem.
</p>
<p>We also consider the edge arrival version in which, instead of vertices,
edges of the graph arrive in an online fashion with independent chances of
failure. Designing approximation algorithms for this problem has also been
studied extensively with the best approximation ratio being 0.337 with respect
to the offline benchmark. This paper, however, is the first to consider the
online benchmark for the edge arrival version of the problem. For this problem,
we provide a simple algorithm with an approximation ratio of 0.5 with respect
to the online benchmark.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2206.01270"><span class="datestr">at June 06, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.05760">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.05760">Co-generation of Collision-Free Shapes for Arbitrary One-Parametric Motion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morris:Clinton_B=.html">Clinton B. Morris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behandish:Morad.html">Morad Behandish</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.05760">PDF</a><br /><b>Abstract: </b>Mechanical assemblies can exhibit complex relative motions, during which
collisions between moving parts and their surroundings must be avoided. To
define feasible design spaces for each part's shape, "maximal" collision-free
pointsets can be computed using configuration space modeling techniques such as
Minkowski operations and sweep/unsweep. For example, for a pair of parts
undergoing a given relative motion, to make the problem well-posed, the
geometry of one part (chosen arbitrarily) must be fixed to compute the maximal
shape of the other part by an unsweep operation. Making such arbitrary choices
in a multi-component assembly can place unnecessary restrictions on the design
space. A broader family of collision-free pairs of parts can be explored, if
fixing the geometry of a component is not required. In this paper, we formalize
this family of collision-free shapes and introduce a generic method for
generating a broad subset of them. Our procedure, which is an extension of the
unsweep, allows for co-generation of a pair of geometries which are modified
incrementally and simultaneously to avoid collision. We demonstrate the
effectiveness and scalability of our procedure in both 2D and 3D by generating
a variety of collision-free shapes. Notably, we show that our approach can
automatically generate freeform cam and follower profiles, gear teeth, and
screw threads, starting from colliding blocks of materials, solely from a
specification of relative motion and without the use of any feature-informed
heuristics. Moreover, our approach provides continuous measures of collision
that can be incorporated into standard gradient-descent design optimization,
allowing for simultaneous collision-free and physics-informed co-design of
mechanical parts for assembly.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.05760"><span class="datestr">at June 06, 2022 11:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/06/04/maybe-powers-pi">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/06/04/maybe-powers-pi.html">Maybe powers of π don’t have unexpectedly good approximations?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>After <a href="https://11011110.github.io/blog/2022/05/31/linkage.html">I wrote recently</a> about Ramanujan’s approximation \(\pi^4\approx 2143/22\), writing “why do powers of \(\pi\) seem to have unusually good rational approximations?”, 
Timothy Chow emailed to challenge my assumption, asking what evidence I had that their approximations were unusually good. So that led me to do a little statistical experiment to test that hypothesis, and the experiment showed…that the approximations seem to be about as good as we would expect, no more, no less. Not unusually good. Chow was correct, and my earlier statement was overstated. So if Ramanujan’s approximation is not just random fluctuation (which for all I know it could be), it at least does not seem to be part of a pattern of many good rational approximations for small powers of \(\pi\). Below are some details of how I came to this conclusion.</p>

<p>First off, one gets good rational approximations by truncating the <a href="https://en.wikipedia.org/wiki/Continued_fraction">continued fraction representation</a> of a number. for instance, Ramanujan’s approximation can be obtained from the continued fraction</p>

\[\pi^4=97 + \cfrac{1}{2 + \cfrac{1}{2 + \cfrac{1}{3 + \cfrac{1}{1+\cfrac{1}{16539+\cdots}}}}}\]

<p>by stopping just before the big number \(16539\). You want to stop at those points, roughly speaking, because the accuracy of your approximation (as measured for instance by how many decimal digits you get correct) comes from combining all of the terms up to and including the big one you stopped before, but the complexity of your approximation (how many digits it takes to write it down) comes from only the terms before you stopped. When you stop just before a big term, you get a lot of accuracy for free. And so the question I was really asking was “why do the powers of \(\pi\) have unusually big terms in their continued fractions?” But the question I should have asked first was “do the powers of \(\pi\) have unusually big terms in their continued fractions?”</p>

<p>For a random number, the terms of the continued fraction will be distributed according to the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Kuzmin_distribution">Gauss–Kuzmin distribution</a>, according to which the probability of seeing a term with value exactly \(k\) is</p>

\[- \log_2 \left( 1 - \frac{1}{(1+k)^2}\right).\]

<p>This is a <a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution">heavy-tailed distribution</a>, so one should expect to see some large numbers from time to time. What I wanted to test was a <a href="https://en.wikipedia.org/wiki/Null_hypothesis">null hypothesis</a> that the continued fraction for powers of \(\pi\) are distributed in this way. If the tests revealed a low likelihood of this being true, it would suggest that they have some other distribution, confirming what I wrote in my earlier post. If they didn’t, it still might mean that there was something unusual about those terms, but it would have to be more subtle, subtle enough to be undetected by the statistical tests I used.</p>

<p>This is exactly the sort of thing <a href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test">Pearson’s \(\chi^2\) test</a> is good for. So I set up a \(\chi^2\) test in R comparing the observed data (the frequencies of each value in the continued fraction terms for \(\pi\), \(\pi^2\), \(\pi^3\), and \(\pi^4\), as taken from OEIS) with the Gauss–Kuzmin distribution. I omitted the initial term from each continued fraction, because those are the integer parts of the powers of \(\pi\), which are both nonrandom and uninteresting for approximation purposes. I also grouped the terms of the continued fractions into six exponentially-growing buckets, by mapping each term \(t_i\) to \(\min\bigl(5,\lfloor\log_2 t_i\rfloor\bigr)\), because the \(\chi^2\) test works better when it has only a small number of well-populated frequency counts to compare, rather than \(16539\) of them, many empty or with only one representative. This meant also computing the Gauss–Kuzmin probabilities for the same buckets. These buckets still don’t have uniform probabilities but I thought it better to stick to my first choice of bucketing rather than to repeatedly adjust the parameters of the test. Even with this bucketing the \(\chi^2\) test still wasn’t strong enough to work well on the OEIS data taken separately for each individual power of \(\pi\). Instead I had to concatenate the data for the four powers I had chosen into a single test to get it to work without R complaining.</p>

<p>Because I am more fluent in Python than R, I did this as <a href="https://11011110.github.io/blog/assets/2022/pipower.py">a Python script</a> to generate a very short R script, but it would have been as easy to do the whole thing directly in R. The results: a <a href="https://en.wikipedia.org/wiki/P-value">\(p\)-value</a> somewhat greater than \(\tfrac12\) (not small). So the null hypothesis was <em>not</em> rejected and I do not have evidence that the powers of \(\pi\) have unusually big terms in their continued fractions.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108423157545534693">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/06/04/maybe-powers-pi.html"><span class="datestr">at June 04, 2022 09:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-18307925481231353">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/06/does-social-media-law-in-texas-affect.html">Does the Social Media Law in Texas affect theory bloggers?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A new law in Texas states that any social media sites that has at least 50 million subscribers a month cannot ban anyone (its more nuanced than that, but that's the drift). </p><p>(I wrote this before the Supreme courts blocked the law, which you can read about <a href="https://nypost.com/2022/05/31/scotus-blocks-texas-law-targeting-social-media-censorship/">here</a>. This is a temporary block so the issue is not settled.) </p><p>Here is an article about the law: <a href="https://www.vox.com/2022/5/12/23068017/supreme-court-first-amendment-twitter-facebook-youtube-instagram-netchoice-paxton-texas">here</a></p><p>My random thoughts</p><p>1) How can any internet law be local to Texas or to any state? I wonder the same thing about the EU's law about right-to-be-forgotten and other restrictions. </p><p>2) Does the law apply to blogs? If Scott had over 50 million readers... <i>Hold that thought. </i>Imagine if that many people cared about quantum computing, complexity theory,  the Busy Beaver function,  and Scott's political and social views. T<i>hat would be an awesome world!</i> However, back to the point- if he did have that many readers would he not be allowed to ban anyone?</p><p>3) If Lance and I had over 50 million readers... <i>Hold that thought</i>. Imagine if that many people cared about Complexity Theory, Ramsey Theory, <a href="https://blog.computationalcomplexity.org/2022/01/did-betty-white-die-in-2021why-do.html">Betty White</a> and Bill and Lance's political and social views. Would <i>that be an awesome world?</i> I leave that as an open question. However, back to the point- would they be able to block posts like: </p><p>                      Great Post. Good point about SAT. Click here for a good deal on tuxedos. </p><p>Either the poster thinks that Lance will win a Turing award and wants him to look good for the ceremony, or its a bot. </p><p>4) If Lipton and Regan's GLL blog had over 50 million readers.... <i>Hold that thought</i>. Imagine if that many people cared about Complexity theory, open-mindedness towards P=NP, catching people who cheat at chess, nice things about everyone they mention, and their political and social views. <i>That would</i> <i>be a very polite world! </i>However, back to the point- would they be able to block posts? Block people? </p><p>5) arxiv recently rejected a paper by Doren Zeilberger. This rejection was idiotic, though Doren can argue the case better than I can, so see <a href="https://sites.math.rutgers.edu/~zeilberg/Opinion183.html">here</a> for his version of events (one sign  that he can argue better than I can: he does not use any negative terms like <i>idiot.)  </i>Under the moronic Texas law, can arxiv ban Doren for life? (of course, the question arises, do they have at least 50 million subscribers?)</p><p>6) Given who is proposing the law its intent is things like <i>you can't kick Donald Trump off Twitter</i>. I  wonder if Parler or 8-chan or Truth-Social which claim to be free-speech sites, but whose origins are on the right, would block  liberals. Or block anyone? I DO NOT KNOW if they do, but I am curious. If anyone knows please post- no speculation or rumor, I only want solid information. </p><p>7) Republicans default position is to not regulate industry. It is <i>not</i> necessarily  a contradiction to support a regulation; however, they would need  a strong argument why this particular case needs regulation when other issues do not. I have not seen such an argument; however, if you have one then leave a comment. (The argument <i>they are doing</i> <i>it  to please their base </i>is not what I mean- I want a fair objective argument.) </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/06/does-social-media-law-in-texas-affect.html"><span class="datestr">at June 04, 2022 05:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-4217158232171967686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2022/06/practical-robust-and-equitable.html">Practical, Robust, and Equitable Uncertainty Estimation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p style="text-align: center;"><span lang="EN"><i>This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here: </i></span><span style="text-align: left;"><i><a href="https://arxiv.org/abs/2206.01067">https://arxiv.org/abs/2206.01067</a></i></span><span lang="EN"><i> and here is a recording of a recent talk I gave about it at the Simons Foundation:<a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/"> </a></i></span><span style="text-align: left;"><i><a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/">https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/</a><a href="http://aaronsadventures.blogspot.com/feeds/posts/This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here:   and here is a recording of me giving a talk about it: https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/"> </a>. This is cross-posted to the <a href="https://toc4fairness.org/practical-robust-and-equitable-uncertainty-estimation/">TOC4Fairness Blog</a> (and this work comes out of the Simons Collaboration on the Theory of Algorithmic Fairness)</i></span></p><p><br /></p><p>Machine Learning is really good at making point predictions --- but it sometimes makes mistakes. How should we think about which predictions we should trust? In other words, what is the right way to think about the uncertainty of particular predictions? Together with Osbert Bastani, Varun Gupta, Chris Jung, Georgy Noarov, and Ramya Ramalingam, we have some new work I'm really excited about. </p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">A natural way to quantify uncertainty is to predict a set of labels rather than a single one. Pick a degree of certainty --- say 90%. For every prediction we make, we'd like to return the smallest set of labels that is guaranteed to contain the true label 90% of the time. These are "prediction sets", and quantify uncertainty in a natural way: ideally, we will be sure about the correct label, and the prediction set will contain only a single label (the prediction we are certain about). But the larger our prediction set, the more our uncertainty, and the contents of the prediction set lets us know what exactly the model is uncertain about. </span></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN"></span></p><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://blogger.googleusercontent.com/img/a/AVvXsEiM6gcqACKJ0qGQ_We9SiWjb_Mqiyh9rgb8SllcXpgzFFf0O5wJqHKogVUlEaX-Cssihkl9uzHLnsbiBzJIAze1AQXjdZpf1PUTK6GpGIyW-j_qoUSyDnx8SpRo9i2xBJJ3ynVdXuA5vkr70nxcFNcYKdJrIhOOI_4-imNOmif4QH_v4jJI0Q"><img width="320" alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEiM6gcqACKJ0qGQ_We9SiWjb_Mqiyh9rgb8SllcXpgzFFf0O5wJqHKogVUlEaX-Cssihkl9uzHLnsbiBzJIAze1AQXjdZpf1PUTK6GpGIyW-j_qoUSyDnx8SpRo9i2xBJJ3ynVdXuA5vkr70nxcFNcYKdJrIhOOI_4-imNOmif4QH_v4jJI0Q" height="103" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">An example of prediction sets for ImageNet. This example comes from a nice recent paper by Angelopoulos, Bates, Malik, and Jordan: <a href="https://arxiv.org/abs/2009.14193">https://arxiv.org/abs/2009.14193</a> </td></tr></tbody></table><span lang="EN"><br /></span><p></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">But how can we do this? Conformal Prediction provides a particularly simple way. Here is an outline of the vanilla version of conformal prediction (there are plenty of variants): </span></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal">Step 1: Pick a (non)conformity score to measure how different a label y is from a prediction f(x). e.g. for a regression model we could choose $s(x,y) = |f(x)-y|$ --- but lots of interesting work has been done recently to develop much fancier ones. A lot of the art of conformal prediction is in finding a good score function.</p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">Step 2: Find a threshold $\tau$ such that for a new example $(x,y)$, $\Pr[s(x,y) \leq \tau] = 0.9$. An easy way to do this is using a holdout set. </span></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal">Step 3: On a new example $x$, given a point prediction $f(x)$, produce the prediction set $P(x) = \{y : s(x,y) \leq \tau\}$. </p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal">Thats it! Nice and simple. Check out <a href="https://arxiv.org/abs/2107.07511">this recent survey by Angelopolous and Bates</a> for an accessible introduction to conformal prediction. </p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">But a few things could go wrong. First, the technique of using a holdout set only works if the data is i.i.d. or more generally exchangable --- i.e. the data distribution should be permutation invariant. But maybe its coming from some changing distribution. If the distribution has changed in an expected and well behaved way, there are some fixes that let you apply the same framework, but if not you are likely in trouble. </span></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN"></span></p><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA"><img width="320" alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA" height="228" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">A joke about non-exchangable data</td></tr></tbody></table><span lang="EN"><br /></span><p></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">Second, an average over everyone might not be what you care about. If we are in a personalized medicine setting, you might care about the reliability of predictions not just overall, but for women with a family history of diabetes and egg allergies --- or whatever else you think is medically relevant about you as an individual.</span></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">This is the problem that we want to solve: How to give prediction sets that cover their label 90% of the time even if we make no assumptions at all about the data generating process, and even if we care about coverage conditional on arbitrary intersecting subsets of the data. </span></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">We want stronger guarantees in another way too. If you think about our goal, there is a way to cheat: 90% of the time, predict the (trivial) set of all labels. 10% of the time predict the empty set. This covers the real label 90% of the time, but is completely uninformative. </span></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">To avoid this "solution", we also ask that our predictions be threshold calibrated. Remember our prediction sets have the form $P_t(x) = \{y : s(x,y) \leq \tau_t\}$. Now the threshold $\tau_t$ might be different every day. But we want 90% coverage even conditional on the value of $\tau_t$. </span></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">This rules out cheating. Remarkably (I think!), for every set of groups specified ahead of time, we're able to guarantee that even if the data is generated by an adversary, that our empirical coverage converges to 90% at the statistically optimal rate. Here is what that means:</span></p> <p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><span lang="EN">Pick a threshold $\tau$ and group $G$. Consider all $n_{\tau,G}$ rounds in which the example $x$ was in $G$, and in which we predicted threshold $\tau$.<span>  </span>We promise that on this set, we cover 90% $\pm$ $1/\sqrt{n_{\tau,G}}$ of the labels. This is the best you could do even with a known distribution. </span></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal">The best thing is that the algorithm is super simple and practical. We had<a href="https://arxiv.org/abs/2101.01739"> a paper last year</a> that showed how to do much of this in theory --- but the algorithm from that paper was not easily implementable (it involved solving an exponentially large linear program with a separation oracle).  But here is our new algorithm --- it only involves doing a small amount of arithmetic for each prediction:</p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"></p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q"><img width="301" alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q" height="240" /></a></div>So we're able to implement it and run a bunch of experiments. You can read about them in detail in <a href="https://arxiv.org/abs/2206.01067">the paper</a>, but the upshot is that our new method is competitive with split conformal prediction even on "its own turf" --- i.e. when the data really is drawn i.i.d. and we only care about marginal coverage --- and really excels when the data comes from a more complicated source, or when we measure group-conditional coverage, which traditional methods tend to have much more trouble with. We run experiments on regression and classification tasks, on exchangeable data, under distribution shift, on real time series data, and on adversarial data orderings. Even when the data is i.i.d. and we only care about marginal coverage, our method has an important advantage over split conformal prediction --- since we don't need to preserve exchangability, we can use all of the data to train the underlying model, whereas split conformal prediction needs to reserve some fraction of it for a holdout set. The result is faster learning for our method, which results in smaller/more accurate prediction sets even without the complicating factors of groupwise coverage, threshold calibration or adversarial data!<p></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"><br /></p><p style="line-height: 115%; margin-bottom: 10pt;" class="MsoNormal"></p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ"><img width="320" alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ" height="103" /></a></div><div style="clear: both; text-align: center;" class="separator"><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w"><img width="320" alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w" height="112" /></a></div><br /><br /></div><br /><br /><br /><p></p></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2022/06/practical-robust-and-equitable.html"><span class="datestr">at June 03, 2022 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=2149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/practical-robust-and-equitable-uncertainty-estimation/">Practical, Robust, and Equitable Uncertainty Estimation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="has-text-align-center"><em>This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here: <a href="https://arxiv.org/abs/2206.01067">https://arxiv.org/abs/2206.01067</a>  and here is a recording of a recent talk I gave about it at the Simons Foundation: <a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/&amp;nbsp">https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/ </a></em></p>



<p>Machine Learning is really good at making point predictions — but it sometimes makes mistakes. How should we think about which predictions we should trust? In other words, what is the right way to think about the uncertainty of particular predictions? Together with Osbert Bastani, Varun Gupta, Chris Jung, Georgy Noarov, and Ramya Ramalingam, we have some new work I’m really excited about. </p>



<p>A natural way to quantify uncertainty is to predict a set of labels rather than a single one. Pick a degree of certainty — say 90%. For every prediction we make, we’d like to return the smallest set of labels that is guaranteed to contain the true label 90% of the time. These are “prediction sets”, and quantify uncertainty in a natural way: ideally, we will be sure about the correct label, and the prediction set will contain only a single label (the prediction we are certain about). But the larger our prediction set, the more our uncertainty, and the contents of the prediction set lets us know what exactly the model is uncertain about. </p>



<figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-center"><img src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/predictionset.png?resize=800%2C256&amp;ssl=1" style="width: 500px;" height="256" width="800" alt="" class="wp-image-2151" /><a href="https://www.blogger.com/blog/post/edit/25562705/4217158232171967686"></a></td></tr><tr><td class="has-text-align-center">An example of prediction sets for ImageNet. This example comes from a nice recent paper by Angelopoulos, Bates, Malik, and Jordan: <a href="https://arxiv.org/abs/2009.14193">https://arxiv.org/abs/2009.14193</a> </td></tr></tbody></table></figure>



<p>But how can we do this? Conformal Prediction provides a particularly simple way. Here is an outline of the vanilla version of conformal prediction (there are plenty of variants): </p>



<p>Step 1: Pick a (non)conformity score to measure how different a label y is from a prediction f(x). e.g. for a regression model we could choose $s(x,y) = |f(x)-y|$ — but lots of interesting work has been done recently to develop much fancier ones. A lot of the art of conformal prediction is in finding a good score function.</p>



<p>Step 2: Find a threshold $\tau$ such that for a new example <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="(x,y)" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5CPr%5Bs%28x%2Cy%29+%5Cleq+%5Ctau%5D+%3D+0.9&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\Pr[s(x,y) \leq \tau] = 0.9" class="latex" />. An easy way to do this is using a holdout set. </p>



<p>Step 3: On a new example <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, given a point prediction <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" />, produce the prediction set <img src="https://s0.wp.com/latex.php?latex=P%28x%29+%3D+%5C%7By+%3A+s%28x%2Cy%29+%5Cleq+%5Ctau%5C%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="P(x) = \{y : s(x,y) \leq \tau\}" class="latex" />. </p>



<p>Thats it! Nice and simple. Check out <a href="https://arxiv.org/abs/2107.07511">this recent survey by Angelopolous and Bates</a> for an accessible introduction to conformal prediction. </p>



<p>But a few things could go wrong. First, the technique of using a holdout set only works if the data is i.i.d. or more generally exchangable — i.e. the data distribution should be permutation invariant. But maybe its coming from some changing distribution. If the distribution has changed in an expected and well behaved way, there are some fixes that let you apply the same framework, but if not you are likely in trouble.</p>



<figure class="wp-block-table aligncenter"><table><tbody><tr><td><img src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/nonexchangable-joke.png?resize=800%2C571&amp;ssl=1" style="width: 500px;" height="571" width="800" alt="" class="wp-image-2152" /><a href="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA"></a></td></tr><tr><td>A joke about non-exchangable data</td></tr></tbody></table></figure>



<p>Second, an average over everyone might not be what you care about. If we are in a personalized medicine setting, you might care about the reliability of predictions not just overall, but for women with a family history of diabetes and egg allergies — or whatever else you think is medically relevant about you as an individual.</p>



<p>This is the problem that we want to solve: How to give prediction sets that cover their label 90% of the time even if we make no assumptions at all about the data generating process, and even if we care about coverage conditional on arbitrary intersecting subsets of the data.</p>



<p>We want stronger guarantees in another way too. If you think about our goal, there is a way to cheat: 90% of the time, predict the (trivial) set of all labels. 10% of the time predict the empty set. This covers the real label 90% of the time, but is completely uninformative.</p>



<p>To avoid this “solution”, we also ask that our predictions be threshold calibrated. Remember our prediction sets have the form <img src="https://s0.wp.com/latex.php?latex=P_t%28x%29+%3D+%5C%7By+%3A+s%28x%2Cy%29+%5Cleq+%5Ctau_t%5C%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="P_t(x) = \{y : s(x,y) \leq \tau_t\}" class="latex" />. Now the threshold <img src="https://s0.wp.com/latex.php?latex=%5Ctau_t&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\tau_t" class="latex" /> might be different every day. But we want 90% coverage even conditional on the value of <img src="https://s0.wp.com/latex.php?latex=%5Ctau_t&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\tau_t" class="latex" />.</p>



<p>This rules out cheating. Remarkably (I think!), for every set of groups specified ahead of time, we’re able to guarantee that even if the data is generated by an adversary, that our empirical coverage converges to 90% at the statistically optimal rate. Here is what that means:</p>



<p>Pick a threshold <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> and group <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="G" class="latex" />. Consider all <img src="https://s0.wp.com/latex.php?latex=n_%7B%5Ctau%2CG%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="n_{\tau,G}" class="latex" /> rounds in which the example <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> was in <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="G" class="latex" />, and in which we predicted threshold <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\tau" class="latex" />.  We promise that on this set, we cover 90% <img src="https://s0.wp.com/latex.php?latex=%5Cpm&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\pm" class="latex" /> <img src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bn_%7B%5Ctau%2CG%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="1/\sqrt{n_{\tau,G}}" class="latex" /> of the labels. This is the best you could do even with a known distribution.</p>



<p>The best thing is that the algorithm is super simple and practical. We had<a href="https://arxiv.org/abs/2101.01739"> a paper last year</a> that showed how to do much of this in theory — but the algorithm from that paper was not easily implementable (it involved solving an exponentially large linear program with a separation oracle).  But here is our new algorithm — it only involves doing a small amount of arithmetic for each prediction:</p>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q"><img src="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q" alt="" /></a></figure>



<p>So we’re able to implement it and run a bunch of experiments. You can read about them in detail in the paper, but the upshot is that our new method is competitive with split conformal prediction even on “its own turf” — i.e. when the data really is drawn i.i.d. and we only care about marginal coverage — and really excels when the data comes from a more complicated source, or when we measure group-conditional coverage, which traditional methods tend to have much more trouble with. We run experiments on regression and classification tasks, on exchangeable data, under distribution shift, on real time series data, and on adversarial data orderings. Even when the data is i.i.d. and we only care about marginal coverage, our method has an important advantage over split conformal prediction — since we don’t need to preserve exchangability, we can use all of the data to train the underlying model, whereas split conformal prediction needs to reserve some fraction of it for a holdout set. The result is faster learning for our method, which results in smaller/more accurate prediction sets even without the complicating factors of groupwise coverage, threshold calibration or adversarial data!</p>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ"><img src="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ" alt="" /></a></figure>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w"><img src="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w" alt="" /></a></figure></div>







<p class="date">
by Aaron Roth <a href="https://toc4fairness.org/practical-robust-and-equitable-uncertainty-estimation/"><span class="datestr">at June 03, 2022 12:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20122">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/">Women In Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>I like crossing the imaginary boundaries people set up between different fields—it’s very refreshing. There are lots of tools, and you don’t know which one would work—Maryam Mirzakhani.</i></p>
<p>
Shafi Goldwasser is the director of the Simons Institute for the Theory of Computing. She just ran their tenth year <a href="https://simons.berkeley.edu/workshops/simons-institute-10th-anniversary-symposium">celebration</a>. The talks are viewable now on <a href="https://www.youtube.com/watch?v=-tQ-7kR6r1M&amp;list=PLgKuh-lKre10uB8f4U8YlQssF_IJWd9sq">SimonsTV</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/sg/"><img width="240" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/sg.jpg?resize=240%2C250&amp;ssl=1" class="aligncenter size-full wp-image-20125" height="250" /></a></p>
<p>
</p><p><b> Another Conference </b></p>
<p></p><p>
Soon she will be running another conference: <a href="https://womenintheory.wordpress.com/program/">Women in Theory 2022</a> from June 7 to June 10. Below is the 2018 group photo:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/attachment/2018/" rel="attachment wp-att-20127"><img width="600" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/2018.jpg?resize=600%2C162&amp;ssl=1" class="aligncenter size-full wp-image-20127" height="162" /></a></p>
<p>
 The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the field, as well as social events and activities. The motivation for the workshop is twofold. The first goal is to deliver an invigorating educational program; the second is to bring together theory women students from different departments and foster a sense of kinship and camaraderie. </p>
<p>
Here are the speakers, along with talk titles for some of them:</p>
<ul>
<li> Laurie Weingart <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://www.cmu.edu/tepper/faculty-and-research/faculty-by-area/profiles/weingart-laurie.html"> Professor of Organizational Behavior and Theory</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/lw/" rel="attachment wp-att-20129"><img width="225" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/lw.png?resize=225%2C225&amp;ssl=1" class="aligncenter size-full wp-image-20129" height="225" /></a><p></p>
<p>
 <a href="https://www.sandbarbookstore.com/book/9781982152338">The No Club: Putting a Stop to Women’s Dead-End Work.</a> </p>
<p></p></li><li> Jennifer Chayes <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://data.berkeley.edu/people/jennifer-chayes">Associate Provost</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/jc/" rel="attachment wp-att-20131"><img width="200" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/jc.png?resize=200%2C200&amp;ssl=1" class="aligncenter size-full wp-image-20131" height="200" /></a><p></p>
<p></p></li><li> Raluca Ada Popa <br />
 <a href="https://people.eecs.berkeley.edu/~raluca/">Co-directs the RISELab</a> <p></p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/rp/" rel="attachment wp-att-20133"><img width="115" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/rp.png?resize=115%2C144&amp;ssl=1" class="aligncenter size-full wp-image-20133" height="144" /></a></p>
<p></p></li><li> Kamalika Chaudhuri <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://cseweb.ucsd.edu/~kamalika/">Research Scientist in Meta AI</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/kc/" rel="attachment wp-att-20135"><img width="259" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/kc.png?resize=259%2C194&amp;ssl=1" class="aligncenter size-full wp-image-20135" height="194" /></a><br />
 <a href="https://cseweb.ucsd.edu/~kamalika/">Challenges in Learning from Out-of-Distribution Data</a> <p></p>
<p></p></li><li> Barna Saha <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://barnasaha.net">Associate Professor with Jacobs Faculty Scholar</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/bs-3/" rel="attachment wp-att-20137"><img width="205" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/bs.png?resize=205%2C246&amp;ssl=1" class="aligncenter size-full wp-image-20137" height="246" /></a><p></p>
<p></p></li><li> Elette Boyle <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://cs.idc.ac.il/~elette/">Director FACT Research Center</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/eb/" rel="attachment wp-att-20139"><img width="188" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/eb.png?resize=188%2C268&amp;ssl=1" class="aligncenter size-full wp-image-20139" height="268" /></a><br />
 <a href="https://cs.idc.ac.il/~elette/">Zero-Knowledge Proofs on Distributed Data and Cryptographic Applications</a> <p></p>
<p></p></li><li> Gagan Aggarwal <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="http://theory.stanford.edu/~gagan/">Google research scientist.</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/ga/" rel="attachment wp-att-20141"><img width="194" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/ga.png?resize=194%2C260&amp;ssl=1" class="aligncenter size-full wp-image-20141" height="260" /></a><p></p>
<p></p></li><li> Naama Ben-David <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://sites.google.com/view/naama-ben-david/home">Postdoctoral researcher</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/nb/" rel="attachment wp-att-20143"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/nb.jpg?resize=300%2C400&amp;ssl=1" class="aligncenter size-full wp-image-20143" height="400" /></a><br />
 <a href="https://publish.illinois.edu/rising-stars/naama-ben-david/">Fast and Fair Lock-Free Locks</a> <p></p>
<p></p></li><li> Kshipra Bhawalkar <br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\blacksquare}" class="latex" /> <a href="https://cs.stanford.edu/people/kshipra/">Google Research at Mountain View</a> <br />
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/kb/" rel="attachment wp-att-20145"><img width="200" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/kb.jpg?resize=200%2C200&amp;ssl=1" class="aligncenter size-full wp-image-20145" height="200" /></a><br />
 <a href="https://research.google/people/KshipraBhawalkar/">Simple Mechanisms for Rich Advertising Auctions</a>
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Some of the talk titles have appeared while we were composing this post. Check out the Simons <a href="https://womenintheory.wordpress.com/program/">site</a> for current information. </p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/"><span class="datestr">at June 03, 2022 10:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/084">TR22-084 |  Characterizing Derandomization Through Hardness of Levin-Kolmogorov Complexity | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A central open problem in complexity theory concerns the question of whether all efficient randomized algorithms can be simulated by efficient deterministic algorithms. We consider this problem in the context of promise problems (i.e,. the $\prBPP$ v.s. $\prP$ problem) and show that for all sufficiently large constants $c$, the following are \emph{equivalent}:
\BI
\item $\prBPP=\prP$.
\item For every $\BPTIME(n^c)$ algorithm $M$, and every sufficiently large $z \in \{0,1\}^n$, there exists some $x \in \{0,1\}^n$ such that $M$ fails to decide whether $Kt(x \mid z)$ is ``very large'' ($\geq n-1$) or ``very small'' ($\leq O(\log
n)$).
\EI
where $Kt(x \mid z$) denotes the Levin-Kolmogorov complexity of $x$ conditioned on $z$.

As far as we are aware, this yields the first full \emph{characterization} of when $\prBPP = \prP$ through the hardness of some class of problems. Previous hardness assumptions used for derandomization only provide a one-sided implication.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/084"><span class="datestr">at June 03, 2022 06:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/083">TR22-083 |  Hardness of Maximum Likelihood Learning of DPPs | 

	Elena Grigorescu, 

	Brendan Juba, 

	Karl  Wimmer, 

	Ning Xie</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In these applications, the parameters of the DPP need to be fitted to match the data; typically, we seek a set of parameters that maximize the likelihood of the data. The algorithms used for this task to date either optimize over a limited family of DPPs, 
or use local improvement heuristics that do not provide theoretical guarantees of optimality.
   
It is natural to ask if there exist efficient algorithms for finding a maximum likelihood DPP model for a given data set. In seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD Thesis (2011) that the problem is NP-complete. 
The lack of a formal proof  prompted Brunel, Moitra, Rigollet and Urschel (COLT 2017) to conjecture that, 
in opposition to Kulesza's conjecture, there exists a polynomial-time algorithm for computing a maximum-likelihood DPP. They also presented some preliminary evidence supporting their conjecture.
   
In this work we prove Kulesza's conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a $\left(1-O(\frac{1}{\log^9{N}})\right)$-approximation to the maximum log-likelihood of a DPP on a ground set of $N$ elements is NP-complete. At the same time, we also obtain the first polynomial-time algorithm that achieves a nontrivial worst-case approximation 
to the optimal log-likelihood: the approximation factor is $\frac{1}{(1+o(1))\log{m}}$ unconditionally (for data sets that consist of $m$ subsets), and can be improved to $1-\frac{1+o(1)}{\log N}$ if all $N$ elements appear in a $O(1/N)$-fraction of the subsets.
   
In terms of techniques, we reduce approximating the maximum log-likelihood of DPPs on a data set to 
solving a gap instance of a ``vector coloring" problem on a  hypergraph. Such a hypergraph is built on a bounded-degree graph construction of Bogdanov, Obata and Trevisan (FOCS 2002), and is further enhanced by the strong  expanders of  Alon and Capalbo (FOCS 2007) to serve our purposes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/083"><span class="datestr">at June 02, 2022 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/06/01/phd-student-postdoc-at-max-planck-institute-ruhr-university-of-bochum-apply-by-june-30-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/06/01/phd-student-postdoc-at-max-planck-institute-ruhr-university-of-bochum-apply-by-june-30-2022/">PhD Student / Postdoc at Max Planck Institute / Ruhr University of Bochum (apply by June 30, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Giulio Malavolta and Michael Walter are looking for an outstanding PhD candidate or postdoctoral researcher. Examples of possible research areas include:</p>
<p>– Quantum information theory<br />
– Quantum and post-quantum cryptography</p>
<p>To apply for the position, please send:</p>
<p>(1) Curriculum vitae.<br />
(2) Electronic contact details of 2-3 potential references.<br />
(3) A brief cover letter.</p>
<p>Website: <a href="https://www.quantiki.org/position/open-position-mpi-sp-and-rub-quantumcryptography-phdpostdoc-100">https://www.quantiki.org/position/open-position-mpi-sp-and-rub-quantumcryptography-phdpostdoc-100</a><br />
Email: giulio.malavolta@mpi-sp.org</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/06/01/phd-student-postdoc-at-max-planck-institute-ruhr-university-of-bochum-apply-by-june-30-2022/"><span class="datestr">at June 01, 2022 08:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://bit-player.org/?p=2401">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hayes.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://bit-player.org/2022/words-for-the-wordle-weary">Words for the Wordle-Weary</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://bit-player.org" title="bit-player">bit-player</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>When the <a href="https://www.powerlanguage.co.uk/wordle/">Wordle</a> wave washed over the world some months ago, I played along like everybody else, once a day collecting my rows of gray and gold and green letters. But my main interest was not in testing my linguistic intuitions; I wanted to write a computer program to solve the puzzle. Could I create something that would play a stronger game than I do? It’s now clear the answer to that question is yes, but I can’t say whether it’s because I’m such a hotshot programmer or such a mediocre Wordler.</p>
<p>I hasten to add that my motivation in this project is not to cheat. Josh Wardle, the creator of Wordle, took all the fun out of cheating by making it way too easy. Anybody can post impressive results like this one:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2022/02/wordle-share-image1.png" height="56" width="136" alt="Image of the five green squares Wordle shows when you guess the word on the first try." border="0" class="centered" /></p>
<p class="undent">That lonely row of green squares indicates that you’ve solved the puzzle with a single brilliant guess. The Wordle app calls that performance an act of “Genius,” but if it happens more than once every few years, your friends may suspect another explanation.</p>
<p>The software I’ve written will not solve today’s puzzle for you. You can’t run the program unless you already know the answer. Thus the program won’t tell you, in advance, how to play; but it may tell you, retrospectively, how you <em>should have</em> played.</p>
<hr />
<p>The aim in Wordle is to identify a hidden five-letter word. When you make a guess, the Wordle app gives you feedback by coloring the letter tiles. A green tile indicates that you have put the right letter in the right place. The rules are not quite as simple as they seem, as I’ll explain below.Gold means the letter is present in the target word, but not where you put it. Letters absent from the target word are marked with a gray tile.</p>
<p>Figure 1.<img src="http://bit-player.org/wp-content/uploads/2022/03/example_game_crop.png" height="184" width="640" alt="Example game of Wordle, showing the initial blank grid and four guesses: CRATE, BUILT, LIMIT, and finally LIGHT, which is the target word." border="0" class="alignleft" /></p>
<p class="indent">The sequence of grids above records the progress of a game I played several weeks ago. The initial state of the puzzle is a blank grid with space for six words of five letters each. My first guess, CRATE <em>(far left)</em>, revealed that the letter T is present in the target word, but not in the fourth position; the other letters, CRAE, are absent. I then played BUILT and learned where the T goes, and that the letters I and L are also present. The third guess, LIMIT, got three letters in their correct positions, which was enough information to rule out all but one possibility. The four green tiles in row four confirm that LIGHT is the target word.</p>
<p>The official Wordle app gives you six chances to guess the Wordle-of-the-day. If you fill the grid with six wrong answers, the game ends and the app reveals the word you failed to find.</p>
<p>It’s important to bear in mind that Wordle is played in a closed universe. The word lists used throughout this essay come from the original version of the game published by Josh Wardle at <a href="https://www.powerlanguage.co.uk/">powerlanguage.co.uk</a>. In February, when the game moved to <a href="https://www.nytimes.com/games/wordle/">nytimes.com</a>, about two dozen words were shuffled around or removed. <a href="http://bit-player.org/2022/words-for-the-wordle-weary#note_1">Note 1</a> has further discussion of the word lists and the <em>Times</em>‘s revisions.The target words are drawn from a list of 2,315 possibilities, which are meant to be words familiar to anyone with a broad English vocabulary. All of these target words are also valid guess words. An additional 10,657 words are acceptable as guesses but will never appear as the Wordle-of-the-day. Many of these latter words are quite obscure, familiar only to the most serious Scrabble players. I call them the arcana.</p>
<p>Both of the word lists are downloaded to your web browser whenever you play the game, and you can examine or copy them by peeking at the JavaScript source code with your browser’s developer tools.</p>
<hr />
<p>Conceptually, a Wordle-playing program conducts a dialogue between two compu­tational agents: the Umpire and the Player. The Umpire knows the target word, and responds to submitted guess words in much the same way the Wordle app does. It marks each of the five letters in the guess as either <em>correct</em>, <em>present</em>, or <em>absent</em>, corresponding to the Wordle color codes of green, gold, and gray.</p>
<p>The Player component of the program does <em>not</em> know the target word, but it has access to the lists of common and arcane words. On each turn, the Player selects a word from one of these lists, submits it to the Umpire, and receives feedback classifying each of the letters as <em>correct</em>, <em>present</em>, or <em>absent</em>. This infor­mation can then be used to help choose the next guess word. The guessing process continues until the Player identifies the target word or exhausts its six turns.</p>
<p>Here’s a version of such a program. Type in a start word and a target word, then press <em>Go</em>. Briefly displayed in gray letters are the words the program evaluates as possible next guesses. The most promising candidate is then submitted to the Umpire and displayed with the green-gold-gray color code.</p>
<p>Program 1.
</p>
<p class="undent">Feel free to play with the buttons below the grid. The “<code>?</code>” button will pop up a brief explanation. If you’d like to peek behind the curtain, the source code for the project is available <a href="https://github.com/bit-player/thewordler">on GitHub</a>. Also, a <a href="http://bit-player.org/extras/wordler/">standalone version</a> of the program may be more convenient if you’re reading on a small screen.</p>
<p>This program is surely not the best possible Wordler, but it’s pretty good. Most puzzle instances are solved in three or four guesses. Failures to finish within six guesses are uncommon if you choose a sensible starting word. The algorithms behind Program 1 will be the main subject of this essay, but before diving into them I would like to consider some simpler approaches to playing the game.</p>
<hr />
<p>When I first set out to write a Wordler program, I chose the easiest scheme I could think of. At the start of a new game, the Player chooses a word at random from the list of 2,315 potential target words. (The arcana will not be needed in this program.) After submitting this randomly chosen word as a first guess, the Player takes the Umpire’s feedback report and uses it to sift through the list of potential targets, retaining those words that are still viable candidates and setting aside all those that conflict in some way with the feedback. The Player then selects another word at random from among the surviving candidates, and repeats the process. On each round, the list of candidates shrinks further.</p>
<p>The winnowing of the candidate list is the heart of the algorithm. Suppose the first guess word is COULD, and the Umpire’s evaluation is equivalent to this Wordle markup: <img src="http://bit-player.org/wp-content/uploads/2022/03/COULD-with-coloring.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="COULD colored gray, gold, gray, green, gray" border="0" />. You can now go through the list of candidate words and discard all those that have a letter other than L in the fourth position. You can also eliminate all words that include any instance of the letters C, U, or D.  The gold letter O rules out all words in two disjoint classes: those that <em>don’t</em> have an O anywhere in their spelling, and those that <em>do</em> have an O in the second position. After this winnowing process, only seven words remain as viable candidates.</p>
<p>One aspect of these rules often trips me up when I’m playing. Wordle is not Wheel of Fortune: The response to a guess word might reveal that the target word has an L, but it doesn’t necessarily show you <em>all</em> the Ls. If you play COULD and get the feedback displayed above, you should not assume that the green L in the fourth position is the <em>only</em> L. The target word could be ATOLL, HELLO, KNOLL, or TROLL. (When both the guess and the target words have multiple copies of the same letter, the rules get even trickier. If you want to know the gory details, see <a href="http://bit-player.org/2022/words-for-the-wordle-weary#note_2">Note 2</a>.)</p>
<p>The list-winnowing process is highly effective. Even with a randomly chosen starter word, the first guess typically eliminates more than 90 percent of the target words, leaving only about 220 viable candidates, on average. In subsequent rounds the shrinkage is not as rapid, but it’s usually enough to identify a unique solution within the allotted six guesses.</p>
<p>I wrote the random Wordler as a kind of warmup exercise, and I didn’t expect much in the way of performance. Plucking words willy-nilly from the set of all viable candidates doesn’t sound like the shrewdest strategy. However, it works surprisingly well. The graph below shows the result of running the program on each of the 2,315 target words, with the experiment repeated 10,000 times to reduce statistical noise.</p>
<p>Figure 2.Guess criterion: random<br />Guess pool: remaining target words<br />Start word: randomly chosen<img src="http://bit-player.org/wp-content/uploads/2022/02/graphWordleRandomly_1000.svg" height="" width="" alt="Bar chart showing the results of running the WordleRandomly algorithm 1,000 times on the data set of 2,315 target words." border="0" class="centered" /></p>
<p class="undent">The average number of guesses needed to find the solution is 4.11, and only 2 percent of the trials end in failure (requiring seven or more guesses).</p>
<p>Incidentally, this result is quite robust, in the sense that the outcome doesn’t depend at all on the composition of the words on the target list. If you replace the official Wordle list with 2,315 strings of random letters, the graph looks the same.</p>
<p>The surprising strength of a random player might be taken as a sign that Wordle isn’t really a very hard game. If you are guided by a single, simple rule—play any word that hasn’t already been excluded by feedback from prior guesses—you will win most games. From another point of view the news is not so cheering: If a totally mindless strategy can often solve the puzzle in four moves, you may have to work really hard to do substantially better.</p>
<hr />
<p>Still another lesson might be drawn from the success of the random player: The computer’s complete ignorance of English word patterns is not a major handicap and might even be an advantage. A human player’s judgment is biased by ingrained knowledge of differences in word frequency. If I see the partial solution BE _ _ _, I’m likely to think first of common words such as BEGIN and BENCH, rather than the rarer BELLE and BEZEL. In the Wordle list of potential targets, however, each of these words occurs with exactly the same frequency, namely 1/2315. A policy favoring common words over rare ones is not helpful in this game.</p>
<p>But a case can be made for a slightly different strategy, based on a preference not for common words but for common letters. A guess word made up of high-frequency letters should elicit more information than a guess consisting of rare letters. By definition, the high-frequency letters are more likely to be present in the target word. Even when they are absent, that fact becomes valuable know­ledge. If you make JIFFY your first guess and thereby learn that the target word does not contain a J, you eliminate 27 candidates out of 2,315. Playing EDICT and learning that the target has no E rules out 1,056 words.</p>
<p>The table below records the number of occurrences of each letter from A to Z in all the Wordle target words, broken down according to position within the word. For example, A is the first letter of 141 target words, and Z is the last letter of four words.</p>
<p>Figure 3.<img src="http://bit-player.org/wp-content/uploads/2022/02/table-of-letter-frequencies.png" height="182" width="640" alt="Table of letter frequencies by position in the data set of target words." border="0" class="centered" /></p>
<p class="undent">The same information is conveyed in the heatmap below, where lighter colors indicate more common letters.</p>
<p>Figure 4.<img src="http://bit-player.org/wp-content/uploads/2022/03/letterFreqHeatmapcrop.svg" height="" width="640" alt="Heat map representing letter frequencies as a range of colors from deep purple (lowest frequencies) through bright yellow (highest frequencies." border="0" class="alignleft" /></p>
<p class="indent">These data form the basis of another Wordle-playing algorithm. Given a list of candidate words, we compute the sum of each candidate’s letter frequencies, and select the word with the highest score. For example, the word SKUNK gets 366 points for the initial S, then 10 points for the K in the second position, and so on through the rest of the letters for an aggregate score of 366 + 10 + 165 + 182 + 113 = 836. SKUNK would win over KOALA, which has a score of 832, but lose to PIGGY (851).</p>
<p>Testing the letter-frequency algorithm against all 2,315 target words yields this distribution of results:</p>
<p>Figure 5.Guess criterion: maximize letter frequency<br />Guess pool: remaining target words<br />Start word: determined by algorithm (SLATE)<img src="http://bit-player.org/wp-content/uploads/2022/02/graphWordlePositionalFreq.svg" height="" width="" alt="Bar chart showing the results of running the Wordle algorithm based on positional letter frequencies on the data set of 2,315 target words." border="0" class="centered" /></p>
<p class="undent">The mean number of guesses is 3.83, noticeably better than the random-choice average of 4.11. The failure rate—words that aren’t guessed within six turns—is pushed down to 1.1 percent (28 out of 2,315).</p>
<p>There’s surely room for improvement in the letter-frequency algorithm. One weakness is that it treats the five positions in each word as independent variables, whereas in reality there are strong correlations between each letter and its neighbors. For example, the groups CH, SH, TH, and WH are common in English words, and Q is constantly clinging to U. Other pairs such as FH and LH are almost never seen. These attractions and repulsions between letters play a major role in human approaches to solving the Wordle puzzle (or at least they do in <em>my</em> approach). The letter-frequency program could be revised to take advantage of such knowledge, perhaps by tabulating the frequencies of bigrams (two-letter combinations). I have not attempted anything along these lines. Other ideas hijacked my attention.</p>
<hr />
<p>Like other guessing games, such as <a href="https://en.wikipedia.org/wiki/Bulls_and_Cows">Bulls and Cows</a>, <a href="http://www.cs.uni.edu/~wallingf/teaching/cs3530/resources/knuth-mastermind.pdf">Mastermind</a>, and <a href="https://en.wikipedia.org/wiki/Twenty_questions">Twenty Questions</a>, Wordle is all about acquiring the information needed to unmask a hidden answer. Thus if you’re looking for guidance on playing the game, an obvious place to turn is the body of knowledge known as information theory, formulated 75 years ago by Claude Shannon.</p>
<p>Shannon’s most important contribution (in my opinion) was to establish that information is a quantifiable, measurable substance. The unit of measure is the <em>bit</em>, defined as the amount of information needed to distinguish between two equally likely outcomes. If you flip a fair coin and learn that it came up heads, you have acquired one bit of information.</p>
<p>This scheme is easy to apply to the game of Twenty Questions, where the questions are required to have just two possible answers—yes or no. If each answer conveys one bit of information, 20 questions yield 20 bits, which is enough to distinguish one item among \(2^{20} \approx 1\) million equally likely possibilties. In general, if a collection of things has \(N\) members, then the quantity of information needed to distinguish one of its members is the base-2 logarithm of \(N\), written \(\log_2 N\).</p>
<p>In Wordle we ask no more than six questions, but answers are not just yes or no. Each guess word is a query submitted to the Umpire, who answers by coloring the letter tiles. There are more than two possible answers; in fact, there are 243 of them. As a result, a Wordle guess has the potential to yield much more than one bit of information. But there’s also the possibility of getting <em>less</em> than one bit.</p>
<p>Where does that curious number 243 come from? In the Umpire’s response to a guess, each letter is assigned one of three colors, and there are five letters in all. Hence the set of all possible responses has \(3^5 = 243\) members. Here they are, in all their poly­chrome glory:</p>
<p>Figure 6.<img src="http://bit-player.org/wp-content/uploads/2022/04/tableau_of_color_codes1.svg" height="" width="900" alt="Tableau of color codes" border="0" class="centered" /></p>
<p class="indent">These color codes represent every feedback message you could ever possibly receive when playing Wordle. (And then some! The five codes outlined in pink can never occur. <a href="http://bit-player.org/2022/words-for-the-wordle-weary#note_2">Note 2</a> explains why.) Each color pattern can be represented as a five-digit numeral in ternary (base-3) notation, with the digit 0 signifying <em>gray</em> or <em>absent</em>, 1 indicating <em>gold</em> or <em>present</em>, and 2 corresponding to <em>green</em> or <em>correct</em>. Because these are five-digit numbers, I’ve taken to calling them Zip codes. The all-gray 00000 code at the upper left appears in the Wordle grid when your guess has no letters in common with the target word. A solid-green 22222, at the bottom right, marks the successful conclusion of a game. In the middle of the grid is the all-gold 11111, which is reserved for “deranged anagrams”: Also OCEAN CANOE, REGAL GLARE, and NIGHT THING.pairs of words composed of the same letters but all in different positions, such as BOWEL and ELBOW, or BRUTE and TUBER.</p>
<p>When you play a guess word in Wordle, you can’t know in advance which of the 243 Zip codes you’ll receive as feedback, but you can know the spectrum of possibilities for that particular word. Program 2, <a href="https://github.com/bit-player/wordlespectrum.git">Source code</a> on GitHub.below, displays the spectrum graphically. When you enter a five-letter guess word, the program will pair it with each of the 2,315 target words and show how many of the pairings fall into each of the Zip-code categories.</p>
<p>Program 2.
</p>
<p>Each of the slender bars that grow from the baseline of the chart represents a single Zip-code, from all-gray at the far left, through all-gold in the middle, and on to all-green at the extreme right. The coloring of the bars interpolates between gray, gold, and green, based on the number of tiles with the corresponding color, but ignoring the position of those tiles within the word. Thus codes such as <img src="http://bit-player.org/wp-content/uploads/2022/05/01122-Zip-code.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 0.0px;" height="16" width="85" alt="01122 Zip code" border="0" /> and <img src="http://bit-player.org/wp-content/uploads/2022/05/21021-Zip-code.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 0.0px;" height="16" width="85" alt="21021 Zip code" border="0" /> get the same color in the bar chart.The height of each bar indicates (on a loga­rithmic scale) how often the guess receives the corresponding color code. If you hover the mouse pointer over a bar, the five letters in the tiles at the top of the display will be colored accordingly. (The meaning of the notations that appear in the “Statistics” box will be explained below.)</p>
<p>The pattern of short and tall bars in Program 2 is something like an atomic spectrum for Wordle queries. Each guess word generates a unique pattern. A word like MOMMY or JIFFY yields a sparse distribution, with a few very tall bars and wide empty spaces between them. The sparsest of all is QAJAQ (a word on the arcane list that seems to be a variant spelling of KAYAK): All but 18 of the 243 Zip codes are empty. Words such as SLATE, CRANE, TRACE, and RAISE, on the other hand, produce a dense array of shorter bars, like a lush carpet of grass, where most of the categories are occupied by at least one target word.</p>
<hr />
<p>Can these spectra help us solve Wordle puzzles? Indeed they can! The guiding principle is simple: Choose a guess word that has a “flat” spectrum, distributing the target words uniformly across the 243 Zip codes. The ideal is to divide the set of candidate words into 243 equal-size subsets. Then, when the Umpire’s feedback singles out one of these categories for further attention, the selected Zip code is sure to have relatively few occupants, making it easier to determine which of those occupants is the target word. In this way we maximize the expected information gain from each guess.</p>
<p>This advice to spread the target words broadly and thinly across the space of Zip codes may seem obvious and in no need of further justification. If you feel that way, read on. Those who need persuading (as I did) should consult <a href="http://bit-player.org/2022/words-for-the-wordle-weary#note_3">Note 3</a>. </p>
<p>The ideal of distributing words with perfect uniformity across the Wordle spectrum is, lamentably, unattainable. None of the 12,972 available query words achieves this goal. The spectra all have lumps and bumps and bare spots. The best we can do is choose the guess that comes closest to the ideal. But how are we to gauge this kind of closeness? What mathematical criterion should we adopt to compare those thousands of spiky spectra?</p>
<p>Information theory again waves its hand to volunteer an answer, promising to measure the number of bits of information one can expect to gain from any given Wordle spectrum. The tool for this task is an equation that appeared in Shannon’s 1948 paper “A Mathematical Theory of Communication.” I have taken some liberties with the form of the equation, adapting it to the peculiarities of the Wordle problem and also trying to make its meaning more transparent. For the mathematical details see <a href="http://bit-player.org/2022/words-for-the-wordle-weary#note_4">Note 4</a>.</p>
<p>Here’s the equation in my preferred format:</p>
<p>\[H_w = \sum_{\substack{i = 1\\n_i \ne 0}}^{242} \frac{n_i}{m} (\log_2 m - \log_2 n_i) .\]</p>
<p class="undent">\(H_w\), the quantity we are computing, is the amount of information we can expect to acquire by playing word \(w\) as our next guess. According to an <a href="http://www.eoht.info/page/Neumann-Shannon%20anecdote">oft-told tale</a>, Shannon adopted the term <em>entropy</em> at the suggestion of John von Neumann, who pointed out that “no one knows what entropy really is, so in a debate you will always have the advantage.” The letter \(H\) (which might be a Greek Eta) was introduced by Ludwig Boltzmann.Shannon named this quantity <em>entropy</em>, in analogy with the measure of disorder in physics. A Wordle spectrum has higher entropy when the target words are more broadly dispersed among the Zip codes.</p>
<p>On the righthand side of the equation we sum contributions from all the occupied Zip codes in \(w\)’s Wordle spectrum. The index \(i\) on the summation symbol \(\Sigma\) runs from 0 to 242 (which is 00000 to 22222 in ternary notation), enumerating all the Zip codes in the spectrum. The variable \(n_i\) is the number of target words assigned to Zip code \(i\), and \(m\) is the total number of target words included in the spectrum. At the start of the game, \(m = 2{,}315\), but after each guess it gets smaller.</p>
<p>Now let’s turn to the expression in parentheses. Here \(\log_2 m\) is the amount of information—the number of bits—needed to distinguish the target word among all \(m\) candidates. Likewise \(\log_2 n_i\) is the number of bits needed to pick out a single word from among the \(n_i\) words in Zip code \(i\). The difference between these quantities, \(\log_2 m - \log_2 n_i\), is the amount of information we gain if Zip code \(i\) turns out to be the correct choice—if it harbors the target word and is therefore selected by the Umpire. </p>
<p>Perhaps a numerical example will help make all this clearer. If \(m = 64\), we have \(\log_2 m = 6\): It would take 6 bits of information to single out the target among all 64 candidates. If the target is found in Zip code \(i\) and \(n_i = 4\), we still need \(\log_2 4 = 2\) bits of information to pin down its identity. Thus in going from \(m = 64\) to \(n_i = 4\) we have gained \(6 - 2 = 4\) bits of information.</p>
<p>So much for what we stand to gain if the target turns out to live in Zip code \(i\). We also have to consider the probability of that event. Intuitively, if there are more words allocated to a Zip code, the target word is more likely to be among them. The probability is simply \(n_i / m\), the fraction of all \(m\) words that land in code \(i\). Hence \(n_i / m\) and \(\log_2 m - \log_2 n_i\) act in opposition. Piling more words into Zip code \(i\) increases the probability of finding the target there, but it also increases the difficulty of isolating the target among the \(n_i\) candidates.</p>
<p>Each Zip code makes a contribution to the total expected information gain; the contribution of Zip code \(i\) is equal to the product of \(n_i / m\) and \(\log_2 m - \log_2 n_i\). Summing the contributions coming from all 243 categories yields \(H_w\), the amount of information we can expect to gain from playing word \(w\).</p>
<p>In case you find programming code more digestible than either equations or words, here is a JavaScript function for computing \(H_w\). The argument <code>spectrum</code> is an array of 243 numbers representing counts of words assigned to all the Zip codes.</p>
<pre class="language-javascript"><code>function entropy(spectrum) {
  const nzspectrum = spectrum.filter(x =&gt; x &gt; 0);
  const m = sum(spectrum);
  let H = 0;
  for (let n of spectrum) {
    H += n/m * (log2(m) - log2(n));
  }
  return H;
}
</code></pre>
<p class="undent">In Program 2, this code calculates the entropy value labeled \(H\) in the Statistics panel. The same function is invoked in Program 1 when the “Maximize entropy” algorithm is selected.</p>
<p>Before leaving this topic behind, it’s worth pausing to consider a few special cases. If \(n_i = 1\) (<em>i.e.</em>, there’s just a single word in Zip code \(i\)), then \(\log_2 n_i = 0\), and the information gain is simply \(\log_2 m\); we have acquired all the information we need to solve the puzzle. This result makes sense: If we have narrowed the choice down to a single word, it has to be the target. Now suppose  \(n_i = m\), meaning that all the candidate words have congregated in a single Zip code. Then we have \(\log_2 m - \log_2 n_i = 0\), and we gain nothing by choosing word \(w\). We had \(m\) candidates before the guess, and we still have \(m\) candidates afterward.</p>
<p>What if \(n_i = 0\)—that is, Zip code \(i\) is empty? The base-2 logarithm is defined by the equation \(2^{\lambda} = N\), where \(\lambda\) is the logarithm of \(N\). But there is no number \(\lambda\) such that \(2^{\lambda} = 0\).That’s trouble, because the logarithm of 0 is undefined, and attempting to calculate it in a computer program will raise an error signal. In the entropy equation the subscript \(n_i \ne 0\) excludes all empty Zip codes from the summation. In the JavaScript function the expression <code>spectrum.filter(x =&gt; x &gt; 0)</code> does the same thing. This exclusion does no harm because if \(n_i\) is zero, then \(n_i / m\) is also zero, meaning there’s no chance that category \(i\) holds the winner. (If a Zip code has no words at all, it can’t have the winning word.)</p>
<p>\(H_w\) attains its maximum value when all the \(n_i\) are equal. As mentioned above, there’s no word \(w\) that achieves this ideal, but we can certainly calculate how many bits such a word would produce if it did exist. In the case of a first guess, each \(n_i\) must equal \(2315 / 243 \approx 9.5\), and the total information gain is \(\log_2 2315 - \log_2 9.5 \approx 7.9\) bits. This is an upper bound for a Wordle first guess; it’s the most we could possibly get out of the process, even if we were allowed to play any arbitrary string of five letters as a guess. As we’ll see below, no real guess gains as much as six bits.</p>
<hr />
<p>These mathematical tools of information theory suggest a simple recipe for a Wordle-playing computer program. At any stage of the game the word to play is the one that elicits the most information about the target. We can estimate the information yield of each potential guess by computing its Wordle spectrum and applying the \(H_w\) equation to the resulting sequence of numbers. That’s what the JavaScript code below does.</p>
<pre class="language-javascript"><code>function maxentropy(guesswordlist, targetwordlist) {
	maxH = 0
	bestguessword = ""
	for (g in guesswordlist) {      // outer loop
		spectrum = []
		for t in targetwordlist {   // inner loop
			zipcode = score(g, t)
			spectrum(zipcode) += 1
		}
		H = entropy(spectrum)
		if (H &gt; maxH) {
			maxH = H
			bestguessword = g
		}
	}
	return bestguessword
}</code></pre>
<p class="undent">In this function the outer loop visits all the guess words; then for each of these words the inner loop sifts through all the potential target words, constructing a spectrum. When the spectrum for guess word <code>g</code> is complete, the <code>entropy</code> procedure computes the information <code>H</code> to be gained by playing word <code>w</code>. The <code>maxH</code> and <code>bestguessword</code> variables keep track of the best results seen so far, and ultimately <code>bestguessword</code> is returned as the result of the function.</p>
<p>This procedure can be applied at any stage of the Wordling process, from the first guess to the last. When choosing the first guess—the word to be entered in the blank grid at the start of the game—all 2,315 common words are equally likely potential targets. We also have 12,972 guess words to consider, drawn from both the common and arcane lists. Calculating the information gain for each such starter word reveals that the highest score goes to SOARE, at 5.886 bits. (SOARE is apparently either a variant spelling of SORREL, a reddish-brown color, or an obsolete British term for a young hawk.) ROATE, a variant of ROTE, and RAISE are not far behind.  At the bottom of the list is the notorious QAJAQ, providing just 1.892 bits of information.</p>
<p>Adopting SOARE as a starter word, we can then play complete games of Wordle with this algorithm, recording the number of guesses required to find all 2,315 target words. The result is a substantial improvement over the random and the letter-frequency methods.</p>
<p>Figure 7.Guess criterion: maximize entropy<br />Guess pool: common and arcane<br />Start word: SOARE<img src="http://bit-player.org/wp-content/uploads/2022/05/maxH_performance_soare.svg" height="" width="" alt="MaxH performance soare" border="0" class="centered" /></p>
<p>The average number of guesses per game is down to about 3.5, and almost 96 percent of all games are won in either three or four guesses. Only one target word requires six guesses, and there are no lost games, requiring seven or more guesses. As a device for Wordling, Shannon’s information theory is quite a success!</p>
<hr />
<p>Shannon’s entropy equation was explicitly designed for the function it performs here—finding the distribution with maximum entropy. But if we consider the task more broadly as looking for the most widely dispersed and most nearly uniform distribution, other approaches come to mind. For example, a statistician might suggest variance or standard deviation as an alternative to Shannon entropy. The standard deviation is defined as:</p>
<p>\[\sigma = \sqrt{\frac{\Sigma_i (n_i - \mu)^2}{N}},\]</p>
<p class="undent">where \(\mu\) is the average of the \(n_i\) values and \(N\) is the number of values. In other words, we are measuring how far the individual elements of the spectrum differ from the average value. If the target words were distributed uniformly across all the Zip codes, every \(n_i\) would be equal to \(\mu\), and the standard deviation \(\sigma\) would be zero. A large \(\sigma\) indicates that many \(n_i\) differ greatly from the mean; some Zip codes must be densely populated and others empty or nearly so. Our goal is to find the guess word whose spectrum has the smallest possible standard deviation.</p>
<p>In the Wordling program, it’s an easy matter to substitute minimum standard deviation for maximum entropy as the criterion for choosing guess words. The JavaScript code looks like this:In this case we <em>don’t</em> exclude empty Zip codes. Doing so would badly skew the results. A spectrum with all words crowded into a single Zip code would have \(\sigma = 0\), making it seem the most—rather than the least—desirable configuration.</p>
<pre class="language-javascript"><code>function stddev(spectrum) {
  const mu = sum(spectrum) / 243;
  const diffs = spectrum.map(x =&gt; x - mu);
  const variance = sum(diffs.map(x =&gt; x * x)) / 243;
  return Math.sqrt(variance);
}</code></pre>
<p> In Program 1, you can see the standard deviation algorithm in action by selecting the button labeled “Minimum std dev.” In Program 2, standard deviation values are labeled \(\sigma\) in the Statistics panel.</p>
<p>Testing the algorithm with all possible combinations of a starting word and a target word reveals that the smallest standard deviation is 22.02, and this figure is attained only by the spectrum of the word ROATE. Not far behind are RAISE, RAILE, and SOARE. At the bottom of the list, the worst choice by this criterion is IMMIX, with \(\sigma = 95.5\).</p>
<p>Using ROATE as the steady starter word, I ran a full set of complete games, surveying the standard-deviation program’s performance across all 2,315 target words. I was surprised at the outcome. Although the chart looks somewhat different—more 4s, fewer 3s—the average number of guesses came within a whisker of equalling the max-entropy result: 3.54 vs. 3.53. Of particular note, there are fewer words requiring five guesses, and a few more are solved with just two guesses.</p>
<p>Figure 8.Guess criterion: minimize standard deviation<br />Guess pool: common and arcane<br />Start word: ROATE<img src="http://bit-player.org/wp-content/uploads/2022/05/minStdDevLong_roate.svg" height="" width="" alt="Bar chart showing the percent of Wordle puzzles solved in n guesses, for n=1 through n=7+, using minimum standard deviation to choose guesses." border="0" class="centered" /></p>
<p class="indent">Why was I surprised by the strength of the standard-deviation algorithm? Well, as I said, Shannon’s \(H\) equation is a tool designed specifically for this job. Its mathematical underpinnings assert that no other rule can extract information with greater efficiency. That property seems like it ought to promise superior performance in Wordle. Standard deviation, on the other hand, is adapted to problems that take a different form. In particular, it is meant to measure dispersion in distributions with a normal or Gaussian shape. There’s no obvious reason to expect Wordle spectra to follow the normal law. Nevertheless, the \(\sigma\) rule is just as successful in choosing winners.</p>
<hr />
<p>Following up on this hint that the max-entropy algorithm is not the only Wordle wiz, I was inspired to try a rule even simpler than standard deviation. In this algorithm, which I call “max scatter,” we choose the guess word whose spectrum has the largest number of occupied Zip codes. In other words, we count the bars that sprout up in Program 2, but we ignore their height. In the Statistics panel of Program 2, the max-scatter results are designated by the letter \(\chi\) (Greek chi), which I chose by analogy with the <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function of set theory</a>. In Program 1, choose “Maximize scatter” to Wordle this way.</p>
<p>If we adopt the \(\chi\) standard, the best first guess in Wordle is TRACE, which scatters target words over 150 of the 243 Zip codes. Other strong choices are CRATE and SALET (148 codes each) and SLATE and REAST (147). The bottom of the heap is good ole QAJAQ, with 18.</p>
<p>Using TRACE as the start word and averaging over all target words, the performance of the scatter algorithm actually exceeds that of the max-entropy program. The mean number of guesses is 3.49. There are no failures requiring 7+ guesses, and only one target word requires six guesses.</p>
<p>Figure 9.Guess criterion: maximize scatter<br />Guess pool: common and arcane<br />Start word: TRACE<img src="http://bit-player.org/wp-content/uploads/2022/05/maxScatterLong_trace.svg" height="" width="" alt="Bar chart showing the percent of Wordle puzzles solved in n guesses, for n=1 through n=7+, using maximum scatter to choose guesses." border="0" class="centered" /></p>
<p class="indent">What I find most noteworthy about these results is how closely the programs are matched, as measured by the average number of guesses needed to finish a game. It really looks as if the three criteria are all equally good, and it’s a matter of indifference which one you choose. This (tentative) conclusion is supported by another series of experiments. Instead of starting every game with the word that appears best for each algorithm, I tried generating random pairs of start words and target words, and measured each program’s performance for 10,000 such pairings. Having traded good start words for randomly selected ones, it’s no surprise that performance is somewhat weaker across the board, with the average number of guesses hovering near 3.7 instead of 3.5. But all three algorithms continue to be closely aligned, with figures for average outcome within 1 percent. And in this case it’s not just the averages that line up; the three graphs look very similar, with a tall peak at four guesses per game.</p>
<p>Figure 10.<img src="http://bit-player.org/wp-content/uploads/2022/05/randompair_10000_comparison.svg" height="" width="900" alt="Three bar charts showing the percent of Wordle puzzles solved in n guesses, for n=1 through n=7+, with random pairs of start words and target words. From left to right the charts show results for max entropy, min standard deviation, and max scatter." border="0" class="centered" /></p>
<p class="indent">As I looked at these results, it occurred to me that the programs might be so nearly interchangeable for a trivial reason: because they are playing identical games most of the time. Perhaps the three criteria are similar enough that they often settle on the same sequence of guess words to solve a given puzzle. This <em>does</em> happen: There are word pairs that elicit exactly the same response from all three programs. It’s not a rare occurrence. But there are also plenty of examples like the trio of game grids in Figure 11, where each program discovered a unique pathway from FALSE to VOWEL.</p>
<p>Figure 11.<img src="http://bit-player.org/wp-content/uploads/2022/05/false-vowel-comparison.svg" height="" width="640" alt="Comparison of Wordle trajectories for the starting word FALSE and the target word VOWEL with three algorithms: max entropy, min standard deviation, max scatter." border="0" class="centered" /></p>
<p class="indent">A few further experiments show that the three programs arrive at three distinct solutions in about 35 percent of random games; in the other 65 percent, at least two of the three solutions are identical. In 20 percent of the cases all three are alike. Figure 12.<img src="http://bit-player.org/wp-content/uploads/2022/05/unique_unanimous_graph.svg" height="" width="" alt="Unique unanimous graph" border="0" class="alignright" />Figure 12 shows the relevant statistics for a sample of 5,000 games with random pairings of start and target words. (The notation \(H =\sigma =\chi\) refers to outcomes in which all programs yield the same result. In \(H \ne\sigma \ne\chi\) the three solutions are all distinct. The other three bars count individual pairwise matches.) The result for \(\sigma = \chi\) is a curiosity I don’t understand. It seems those two programs almost never agree unless \(H\) also concurs.</p>
<p>I puzzled over these observations for some time. If the algorithms discover wholly different paths through the maze of words, why are those paths so often the same length? I now have a clue to an answer, but it may not be whole story, and I would welcome alternative explanations.</p>
<hr />
<p>My mental model of what goes on inside a Wordling program runs like this: The program computes the spectrum of each word to be considered as a potential guess, then computes some function—\(H\), \(\sigma\), or \(\chi\)—that reduces the spectrum to a single number. The number estimates the expected quality or efficiency of the word if it is taken as the next guess in the game. Finally we choose the word that either maximizes or minimizes this figure of merit (the word with largest value of \(H\) or \(\chi\), or the smallest value of \(\sigma\)).</p>
<p>So far so good, but there’s an unstated assumption in that scheme: I take it for granted that evaluating the spectra will always yield a single, unique extreme value of \(H\), \(\sigma\), or \(\chi\). What happens if two words are tied for first place? One could argue, of course, that if the words earn the same score, they must be equally good choices, and we should pick one of them at random or by some arbitrary rule. Even if there are three or four or a dozen co-leaders, the same reasoning should apply. But when there are two thousand words all tied for first place, I’m not so sure.</p>
<p>Can such massive traffic jams at the finish line actually happen in a real game of Wordle? At first I didn’t even consider the possibility. When I rated all possible starting words for the Shannon max-entropy algorithm, the ranking turned out to be a total order: In the list of 12,792 words there were no ties—no pairs of words whose spectra have the same \(H\) value. Hence there’s no ambiguity about which word is best, as measured by this criterion.</p>
<p>But this analysis applies only to the opening play—the choice of a first guess, when all the common words are equally likely to be the target. In the endgame the situation is totally different. Suppose the list of 2,315 common words has been whittled down to just five viable candidates for Wordle-of-the-day. When the five target words are sorted into Zip-code categories and the entropy of these patterns is calculated (excluding empty Zip codes), there are only seven possible outcomes, as shown in Figure 13. The patterns correspond to the seven ways of partitioning the number 5 (namely 5, 4+1, 3+2, 3+1+1, 2+2+1, 2+1+1+1, and 1+1+1+1+1).</p>
<p>Figure 13.The bars in these graphs could be rearranged in various ways, but the \(H\), \(\sigma\), and \(\chi\) measures give the same result for all permutations. <img src="http://bit-player.org/wp-content/uploads/2022/05/spectra_of_five_words.svg" height="" width="" alt="Spectra of five words" border="0" class="centered" /></p>
<p class="indent">In this circumstance, the 12,972 guess words cannot all have unique values of \(H\). On the contrary, with only seven distinct values to go around, there must be thousands of tie scores in the ranking of the words. Thus the max-entropy algorithm cannot pick a unique winner; instead it assembles a large class of words that, from the program’s point of view, are all equally good choices. Which of those words ultimately becomes the next guess is more or less arbitrary. For the most part, my programs pick the one that comes first in alphabetical order.</p>
<p>The same arguments apply to the minimum-standard-deviation algorithm. As for the max-scatter function, that has numerous tied scores even when the number of candidates is large. Because the variable \(\chi\) takes on integer values in the range from 1 to 243 (and in practice never strays outside the narrower range 18 to 150), there’s no avoiding an abundance of ties.</p>
<p>The presence of all these co-leaders offers an innocent explanation of how the three algorithms might arrive at solutions that are different in detail but generally equal in quality. Although the programs pick different words, those words come from the same equivalence class, and so they yield equally good outcomes.</p>
<p>But a doubt persists. Can it really be true that hundreds or thousands of words are all equally good guesses in some Wordle positions?  Can you choose any one of those words and expect to finish the game in the same number of plays?</p>
<p>Let’s go back to Figure 11, where the maximum-entropy program follows the opening play of FALSE with DETER and then BINGO. Some digging through the entrails of the program reveals that BINGO is not the uniquely favored guess at this point in the progress of the game; VENOM, VINYL, and VIXEN are tied with BINGO at \(H = 3.122\) bits. The program chooses BINGO simply because it comes first alphabetically. As it happens, the choice is an unfortunate one. Any of the other three words would have concluded the game more quickly, in four guesses rather than five.</p>
<p>Does that mean we now have unequivocal evidence that the program is suboptimal? Not really. At the stage of the game where BINGO was chosen, there were still 10 viable possibilities for the target word. The true target might have been LIBEL, for example, in which case BINGO would have been superior to VENOM or VIXEN.</p>
<hr />
<p>Human players see Wordle as a word game. What else could it be? To solve the puzzle you scour the dusty corners of your vocabulary, searching for words that satisfy certain constraints or fit a given template. You look for patterns of vowels and consonants and other curious aspects of English orthography. </p>
<p>For the algorithmic player, on the other hand, Wordle is a numbers game. What counts is maximizing or minimizing some mathematical function, such as entropy or standard deviation. The words and letters all but disappear. </p>
<p>The link between words and numbers is the Umpire’s scoring rule, with the spectrum of Zip codes that comes out of it. Every possible combination of a guess word and a target word gets reduced to a five-digit ternary number. Instead of computing each of these numbers as the need arises, we can precompute the entire set of Zip codes and store it in a matrix. The content of the matrix is determined by the letters of the words we began with, but once all the numbers have been filled in, we can dispense with the words themselves. Operations on the matrix depend only on the numeric indices of the columns and rows. We can retrieve any Zip code by a simple table lookup, without having to think about the coloring of letter tiles. </p>
<p>In exploring this matrix, let’s set aside the arcana for the time being and work only with common words. With 2,315 possible guesses and the same number of possible targets, we have \(2{,}315^2 \approx 5.4\) million pairings of a guess and a target. Each such pairing gets a Zip code, which can be represented by a decimal integer in the range from 0 to 242. Because these small numbers fit in a single byte of computer memory, the full matrix occupies about five-and-a-half megabytes.</p>
<p>Figure 14 is a graphic representation of the matrix. Each column corresponds to a guess word, and each row to a target word. The words are arranged in alphabetical order from left to right and top to bottom. The matrix element where a column intersects a row holds the Zip code for that combination. The color scheme is the same as in Program 2. The main diagonal (upper left to lower right) is a bright green stripe because each word when played against itself gets a score of 22222, equivalent to five green tiles. The blocks of lighter green all along the diagonal mark regions where guess words and target words share the same first letter and hence have scores with at least one green tile. The largest of these blocks is for words beginning with the letter S. A few other blocks are quite tiny, reflecting the scarcity of words beginning with J, K, Q, Y, and Z. A box for X is lacking entirely; the Wordle common list has no words beginning with X.</p>
<p>Figure 14.Mouseover to magnify<img src="http://bit-player.org/wp-content/uploads/2022/03/score_matrix_HSL2.png" height="900" width="900" alt="Score matrix HSL2" border="0" class="centered withLoupe" /></p>
<p class="indent">Gazing deeply into the tweedy texture of Figure 14 reveals other curious structures, but some features are misleading. The matrix appears to be symmetric across the main diagonal: The point at (<em>a, b</em>) always has the same color as the point at  (<em>b, a</em>). But the symmetry is an artifact of the graphic presentation, where colors are assigned based only on the <em>number</em> of gray, gold, and green tiles, ignoring their positions within a word. The underlying mathematical matrix is not symmetric.</p>
<p>I first built this matrix as a mere optimization, a way to avoid continually recomputing the same Zip codes in the course of a long series of Wordle games. But I soon realized that the matrix is more than a technical speed boost. It encapsulates pretty much everything one might want to know about the entire game of Wordle.</p>
<p>Figure 15 presents a step-by-step account of how matrix methods lead to a Wordle solution. In the first stage <em>(upper left)</em> the player submits an initial guess of SLATE, which singles out column 1778 in the matrix (1778 being the position of SLATE in the alphabetized list of common words). The second stage <em>(upper right)</em> reveal’s the Umpire’s feedback, coloring the tiles as follows: <img src="http://bit-player.org/wp-content/uploads/2022/05/SLATE_colored_for_FIRST_10010.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="SLATE with gold tiles at S and T, gray elsewhere" border="0" />. To the human solver this message would mean: “Look for words that have an S (but not in front) and a T (but not in fourth position).” To the computer program the message says: “Look for rows in the matrix whose 1778th entry is equal to 10010 (base 3) or 84 (base 10). It turns out there are 24 rows meeting this condition, which are highlighted in blue. (Some lines are too closely spaced to be distinguished.) I have labeled the highlighted rows with the words they represent, but the labels are for human consumption only; the program has no need of them.</p>
<p>Figure 15.<img src="http://bit-player.org/wp-content/uploads/2022/05/matrix_solution_four_stages_900px.svg" height="" width="895" alt="Matrix solution four stages 900px" border="0" class="centered" /></p>
<p>In the third panel <em>(lower left)</em>, the program has chosen a second guess word, TROUT, which earns the feedback <img src="http://bit-player.org/wp-content/uploads/2022/05/TROUT-colored-for-FIRST-01002.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="TROUT with a gold tiles at R and a green tile for the final T" border="0" />. I should mention that this is not a word I would have considered playing at this point in the game. The O and U make sense, because the first guess eliminated A and E and left us in need of a vowel; but the presence of two Ts seems wasteful. We already know the target word has a T, and pinning down where it is seems less important than identifying the target word’s other constituent letters. I might have tried PROUD here. Yet TROUT is a brilliant move! It eliminates 7 words that start with T, 12 words that don’t have an R, 17 words that have either an O or a U or both, and 4 words that don’t end in T. In the end only one word remains in contention: FIRST.</p>
<p>All this analysis of letter positions goes on only in my mind, not in the algorithm. The computational process is simpler. The guess TROUT designates column 2118 of the matrix. Among the 24 rows identified by the first guess, SLATE, only row 743 has the Zip code value 01002 (or 29 decimal) at its intersection with column 2118. Row 743 is the row for FIRST. Because it is the only remaining viable candidate, it must be the target word, and this is confirmed when it is played as the third guess <em>(lower right)</em>. </p>
<p>There’s something disconcerting—maybe even uncanny—about a program that so featly wins a word game while ignoring the words. Indeed, you can replace all the words in Wordle with utter gibberish—with random five-letter strings like SCVMI, AHKZB, and BOZPA—and the algorithm will go on working just the same. It has to work a little harder—the average number of guesses is near 4—but a human solver would find the task almost impossible.</p>
<p>The square matrix of Figure 14 includes only the common words that serve as both guesses and targets in Wordle. Including the arcane words expands the matrix to 12,972 columns, with a little more than 30 million elements. At low resolution this wide matrix looks like this:</p>
<p>Figure 16.<img src="http://bit-player.org/wp-content/uploads/2022/04/score_matrix_HSL2_allwords_lowres.png" height="" width="900" alt="Score matrix HSL2 allwords lowres" border="0" class="centered" /></p>
<p class="undent">If you’d like to see all 30 million pixels up close and personal, <a href="http://bit-player.org/wp-content/extras/wordler/score_matrix_HSL2_allwords.tif">download</a> the 90-megabyte TIFF image.</p>
<hr />
<p>Commentators on Wordle have given much attention to the choice of a first guess: the start word, the sequence of letters you type when facing a blank grid. At this point in the game you have no clues of any kind about the composition of the target word; all you know is that it’s one of 2,315 candidates. Because this list of candidates is the same in every game, it seems  there might be one universal best starter word—an opening play that will lead to the smallest average number of guesses, when the average is taken over all the target words. Furthermore, because 2,315 isn’t a terrifyingly large number, a brute-force search for that word might be within the capacity of a less-than-super computer.</p>
<p>Start-word suggestions from players and pundits are a mixed lot. A website called <a href="https://www.polygon.com/gaming/22884031/wordle-game-tips-best-first-guess-5-letter-words">Polygon</a> offers some whimsical ideas, such as FARTS and OUIJA. GameRant also has oddball options, including JUMBO and ZAXES. <a href="https://medium.com/@tglaiel/the-mathematically-optimal-first-guess-in-wordle-cbcb03c19b0a">Tyler Glaiel</a> offers better advice, reaching deep into the arcana to come up with SOARE and ROATE. Grant Sanderson <a href="https://www.youtube.com/watch?v=v68zYyaEmEA">skillfully deduces</a> that CRANE is the best starter, then <a href="https://www.youtube.com/watch?v=fRed0Xmc2Wg">takes it all back</a>.</p>
<p>For each of the algorithms I have mentioned above (except the random one) the algorithm itself can be pressed into service to suggest a starter. For example, if we apply the max-entropy algorithm to the entire set of potential guess words, it picks out the one whose spectrum has the highest \(H\) value. As I’ve already noted, that word is SOARE, with \(H = 5.886\). Table 1 gives the 15 highest-scoring choices, based on this kind of analysis, for the max-entropy, min–standard deviation and max-scatter algorithms.</p>
<h2 style="padding-top: 10px;">First-Move Starter-Word Rankings</h2>
<p>Table 1.</p>
<div style="display: flex; padding-bottom: 10px;">
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Max Entropy</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8;">
<tr>
<td>SOARE</td>
<td>5.886</td>
</tr>
<tr>
<td>ROATE</td>
<td>5.883</td>
</tr>
<tr>
<td>RAISE</td>
<td>5.878</td>
</tr>
<tr>
<td>RAILE</td>
<td>5.866</td>
</tr>
<tr>
<td>REAST</td>
<td>5.865</td>
</tr>
<tr>
<td>SLATE</td>
<td>5.858</td>
</tr>
<tr>
<td>CRATE</td>
<td>5.835</td>
</tr>
<tr>
<td>SALET</td>
<td>5.835</td>
</tr>
<tr>
<td>IRATE</td>
<td>5.831</td>
</tr>
<tr>
<td>TRACE</td>
<td>5.831</td>
</tr>
<tr>
<td>ARISE</td>
<td>5.821</td>
</tr>
<tr>
<td>ORATE</td>
<td>5.817</td>
</tr>
<tr>
<td>STARE</td>
<td>5.807</td>
</tr>
<tr>
<td>CARTE</td>
<td>5.795</td>
</tr>
<tr>
<td>RAINE</td>
<td>5.787</td>
</tr>
</tbody>
</table>
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Min Std Dev</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8; padding-left: 6px;">
<tr>
<td>ROATE</td>
<td>22.02</td>
</tr>
<tr>
<td>RAISE</td>
<td>22.14</td>
</tr>
<tr>
<td>RAILE</td>
<td>22.22</td>
</tr>
<tr>
<td>SOARE</td>
<td>22.42</td>
</tr>
<tr>
<td>ARISE</td>
<td>22.72</td>
</tr>
<tr>
<td>IRATE</td>
<td>22.73</td>
</tr>
<tr>
<td>ORATE</td>
<td>22.76</td>
</tr>
<tr>
<td>ARIEL</td>
<td>23.05</td>
</tr>
<tr>
<td>AROSE</td>
<td>23.20</td>
</tr>
<tr>
<td>RAINE</td>
<td>23.41</td>
</tr>
<tr>
<td>ARTEL</td>
<td>23.50</td>
</tr>
<tr>
<td>TALER</td>
<td>23.55</td>
</tr>
<tr>
<td>RATEL</td>
<td>23.97</td>
</tr>
<tr>
<td>AESIR</td>
<td>23.98</td>
</tr>
<tr>
<td>ARLES</td>
<td>23.98</td>
</tr>
</tbody>
</table>
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Max Scatter</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8; padding-left: 20px;">
<tr>
<td>TRACE</td>
<td>150</td>
</tr>
<tr>
<td>CRATE</td>
<td>148</td>
</tr>
<tr>
<td>SALET</td>
<td>148</td>
</tr>
<tr>
<td>SLATE</td>
<td>147</td>
</tr>
<tr>
<td>REAST</td>
<td>147</td>
</tr>
<tr>
<td>PARSE</td>
<td>146</td>
</tr>
<tr>
<td>CARTE</td>
<td>146</td>
</tr>
<tr>
<td>CARET</td>
<td>145</td>
</tr>
<tr>
<td>PEART</td>
<td>145</td>
</tr>
<tr>
<td>CARLE</td>
<td>144</td>
</tr>
<tr>
<td>CRANE</td>
<td>142</td>
</tr>
<tr>
<td>STALE</td>
<td>142</td>
</tr>
<tr>
<td>EARST</td>
<td>142</td>
</tr>
<tr>
<td>HEART</td>
<td>141</td>
</tr>
<tr>
<td>REIST</td>
<td>141</td>
</tr>
</tbody>
</table>
</div>
<p>Perusing these lists reveals that they all differ in detail, but many of the same words appear on two or more lists, and certains patterns turn up everywhere. A disproportionate share of the words have the letters A, E, R, S, and T; and half the alphabet never appears in any of the lists.</p>
<p>Figure 17 looks at the distribution of starter quality ratings across the entire range of potential guess words, both common and arcane. The 12,972 starters have been sorted from best to worst, as measured by the max-entropy algorithm. The plot gives the number of bits of information gained by playing each of the words as the initial guess, in each case averaging over all 2,315 target words. </p>
<p>Figure 17.<img src="http://bit-player.org/wp-content/uploads/2022/05/first_round_starter_rankings_maxH.svg" height="" width="" alt="First round starter rankings maxH" border="0" class="centered" /></p>
<p>The peculiar shape of the curve tells us something important about Wordling strategies. A small subset of words in the upper left corner of the graph make exceptionally good starters. In the first round of play they elicit almost six bits of information, which is half of what’s needed to finish the game. At the other end of the curve, a slightly larger cohort of words produce really awful results as starters, plunging down to the 1.8 bits of QAJAQ. In between these extremes, the slope of the curve is gradual, and there are roughly 10,000 words that don’t differ greatly in their performance.</p>
<p>These results might be taken as the last word on first words, but I would be a cautious about that. The methodology makes an implicit “greedy” assumption: that the strongest first move will always lead to the best outcome in the full game. It’s rather like assuming that the tennis player with the best serve will always win the match. Experience suggests otherwise. Although a strong start is usually an advantage, it’s no guarantee of victory.</p>
<p>We can test the greedy assumption in a straightforward if somewhat laborious way: For each proposed starter word, we run a complete set of 2,315 full games—one for each of the target words—and we keep track of the average number of guesses needed to complete a game. Playing 2,315 games takes a few minutes for each algorithm; doing that for 12,792 starter words exceeds the limits of my patience. But I have compiled full-game results for 30 starter words, all of them drawn from near the top of the first-round rankings. </p>
<p>Table 2 gives the top-15 results for each of the algorithms. Comparing these lists with those of Table 1 reveals that first-round supremacy is not in fact a good predictor of full-game excellence. None of the words leading the pack in the first-move results remain at the head of the list when we measure the outcomes of complete games.</p>
<h2 style="padding-top: 10px;">Full-Game Starter-Word Rankings</h2>
<p>Table 2.</p>
<div style="display: flex; padding-bottom: 10px;">
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Max Entropy</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8;">
<tr>
<td>REAST</td>
<td>3.487</td>
</tr>
<tr>
<td>SALET</td>
<td>3.492</td>
</tr>
<tr>
<td>TRACE</td>
<td>3.494</td>
</tr>
<tr>
<td>SLATE</td>
<td>3.495</td>
</tr>
<tr>
<td>CRANE</td>
<td>3.495</td>
</tr>
<tr>
<td>CRATE</td>
<td>3.499</td>
</tr>
<tr>
<td>SLANE</td>
<td>3.499</td>
</tr>
<tr>
<td>CARTE</td>
<td>3.502</td>
</tr>
<tr>
<td>CARLE</td>
<td>3.503</td>
</tr>
<tr>
<td>STARE</td>
<td>3.504</td>
</tr>
<tr>
<td>CARET</td>
<td>3.505</td>
</tr>
<tr>
<td>EARST</td>
<td>3.511</td>
</tr>
<tr>
<td>SNARE</td>
<td>3.511</td>
</tr>
<tr>
<td>STALE</td>
<td>3.512</td>
</tr>
<tr>
<td>TASER</td>
<td>3.514</td>
</tr>
</tbody>
</table>
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Min Std Dev</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8; padding-left: 6px;">
<tr>
<td>TRACE</td>
<td>3.498</td>
</tr>
<tr>
<td>CRATE</td>
<td>3.503</td>
</tr>
<tr>
<td>REAST</td>
<td>3.503</td>
</tr>
<tr>
<td>SALET</td>
<td>3.508</td>
</tr>
<tr>
<td>SLATE</td>
<td>3.508</td>
</tr>
<tr>
<td>SLANE</td>
<td>3.511</td>
</tr>
<tr>
<td>CARTE</td>
<td>3.512</td>
</tr>
<tr>
<td>CARLE</td>
<td>3.516</td>
</tr>
<tr>
<td>CRANE</td>
<td>3.517</td>
</tr>
<tr>
<td>CARSE</td>
<td>3.523</td>
</tr>
<tr>
<td>STALE</td>
<td>3.524</td>
</tr>
<tr>
<td>CARET</td>
<td>3.524</td>
</tr>
<tr>
<td>STARE</td>
<td>3.525</td>
</tr>
<tr>
<td>EARST</td>
<td>3.527</td>
</tr>
<tr>
<td>SOREL</td>
<td>3.528</td>
</tr>
</tbody>
</table>
<table style="width: 180px; margin: 5px auto; font-family: sans-serif;" class="bookends">
<thead>
<tr>
<th colspan="2" style="font-size: 18px; text-align: center;">Max Scatter</th>
</tr>
</thead>
<tbody style="line-height: 16px; font-size: 16px; background-color: #f6f0e8; padding-left: 20px;">
<tr>
<td>SALET</td>
<td>3.428</td>
</tr>
<tr>
<td>REAST</td>
<td>3.434</td>
</tr>
<tr>
<td>CRANE</td>
<td>3.434</td>
</tr>
<tr>
<td>SLATE</td>
<td>3.434</td>
</tr>
<tr>
<td>CRATE</td>
<td>3.435</td>
</tr>
<tr>
<td>TRACE</td>
<td>3.435</td>
</tr>
<tr>
<td>CARLE</td>
<td>3.437</td>
</tr>
<tr>
<td>SLANE</td>
<td>3.438</td>
</tr>
<tr>
<td>CARTE</td>
<td>3.444</td>
</tr>
<tr>
<td>STALE</td>
<td>3.450</td>
</tr>
<tr>
<td>TASER</td>
<td>3.450</td>
</tr>
<tr>
<td>CARET</td>
<td>3.451</td>
</tr>
<tr>
<td>EARST</td>
<td>3.452</td>
</tr>
<tr>
<td>CARSE</td>
<td>3.453</td>
</tr>
<tr>
<td>STARE</td>
<td>3.454</td>
</tr>
</tbody>
</table>
</div>
<p class="indent">What’s the lesson here? Do I recommend that when you get up in the morning to face your daily Wordle, you always start the game with REAST or TRACE or SALET or one of the other words near the top of these lists? That’s not <em>bad</em> advice, but I’m not sure it’s the <em>best</em> advice. One problem is that each of these algorithms has its own favored list of starting words. Your own personal Wordling algorithm—whatever it may be—might respond best to some other, idiosyncratic, set of starters.</p>
<p>Moreover, my spouse, who Wordles and <a href="https://www.quordle.com/#/">Quordles</a> and <a href="https://octordle.com/">Octordles</a> and <a href="https://qntm.org/files/absurdle/absurdle.html">Absurdles</a>, reminds me gently that it’s all a game, meant to be fun, and some people may find that playing the same word day after day gets boring.</p>
<p>Figure 18 presents the various algorithms at their shiny best, each one using the starter word that brings out its best performance. For comparison I’ve also included my own Wordling record, based on 126 games I’ve played since January. I’m proud to say that I Wordle better than a random number generator.</p>
<p>Figure 18.
</p>
<hr />
<p>I turn now from the opening move to the endgame, which I find the most interesting part of Wordle—but also, often, the most frustrating. It seems reasonable to say that the endgame begins when the list of viable targets has been narrowed down to a handful, or when most of the letters in the target are known, and only one or two letters remain to be filled in.</p>
<p>Occasonally you might find yourself entering the endgame on the very first move. There’s the happy day when your first guess comes up all green—an event I have yet to experience. Or you might have a close call, such as playing the starter word EIGHT and getting this feedback: <img src="http://bit-player.org/wp-content/uploads/2022/03/EIGHT-scored-4green.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="EIGHT scored 4green" border="0" />. Having scored four green letters, it looks like you’ve got an easy win. With high hopes, you enter a second guess of FIGHT, but again you get the same four greens and one gray. So you type out LIGHT next, and then MIGHT. After two more attempts, you are left with this disappointing game board:</p>
<p>Figure 19.<img src="http://bit-player.org/wp-content/uploads/2022/03/the-IGHT-catastrophe.png" height="" width="200" alt="The IGHT catastrophe" border="0" class="centered" /></p>
<p>What seemed like a sure win has turned into a wretched loss. You have used up your six turns, but you’ve not found the Wordle-of-the-day. Indeed, there are still three candidate targets yet to be tried: SIGHT, TIGHT, and WIGHT. </p>
<p>The _ IGHT words are by no means the only troublemakers. Similar families of words match the patterns _ ATCH, _ OUGH, _ OUND, _ ASTE and _ AUNT. You can get into even deeper endgame woes when an initial guess yields three green tiles. For example, 12 words share the template S_ _ ER, 25  match _ O_ ER, and 29 are consistent with _ A_ ER. </p>
<p>These sets of words are challenging not only for the human solver but also, under some circumstances, for Wordling algorithms. In Program 1, choose the word list “Candidates only” and then try solving the puzzle for target words such as FOYER, WASTE, or VAUNT. Depending on your starter word, you are likely to see the program struggle through several guesses, and it may fail to find the answer within six tries.</p>
<p>The “Candidates only” setting requires the program to choose each guess from the set of words that have not yet been excluded from consideration as the possible target. For example, if feedback from an earlier guess has revealed that the target ends in T and has an I, then every subsequent guess must end in T and have an I. (Restricting guesses to candidates only is similar to the Wordle app’s “hard mode” setting, but a little stricter.)</p>
<p>Compelling the player to choose guesses that might be winners doesn’t seem like much of a hardship or handicap. However, trying to score a goal with every play is seldom the best policy. Other guesses, although they can’t possibly be winners, may yield more information.</p>
<p>An experienced and wordly-wise human Wordler, on seeing the feedback <img src="http://bit-player.org/wp-content/uploads/2022/03/EIGHT-scored-4green.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="EIGHT scored 4green" border="0" />, would know better than to play for an immediate win. The prudent strategy is play a word that promises to reveal the identity of the one missing letter. Here’s an example of how that works.</p>
<p>Figure 20.
</p><div style="display: flex; padding: 20px 0px;">
<img src="http://bit-player.org/wp-content/uploads/2022/05/EIGHT_FLOWN_WIGHT.png" style="padding: 0px 0px;" height="240" width="200" alt="EIGHT FLOWN WIGHT" border="0" /><img src="http://bit-player.org/wp-content/uploads/2022/05/EIGHT_FLOWN_MARSH_SIGHT.png" style="padding: 0px 0px;" height="240" width="200" alt="EIGHT FLOWN MARSH SIGHT" border="0" /><img width="200" alt="EIGHT FLOWN MARSH TIGHT" src="http://bit-player.org/wp-content/uploads/2022/05/EIGHT_FLOWN_MARSH_TIGHT.png" border="0" height="240" />
</div>
<p>At left the second-guess FLOWN detects the presence of a W, which means the target can only be WIGHT. In the middle, FLOWN reveals only absences, but the further guess MARSH finds an S, which implies the target must be SIGHT. At right, the second and third guesses have managed to eliminate F, L, W, N, M, R, and S as initial letters, and all that’s left is the T of TIGHT.</p>
<p>This virtuoso performance is not the work of some International Grand­master of Wordling. It is produced by the max-entropy algorithm, when the program is allowed a wider choice of potential guesses. The standard-deviation and max-scatter algorithms yield identical results. There is no special logic built into any of these programs for deciding when to play to win and when to hunker down and gather more information. It all comes out of the Zip code spectrum: FLOWN and MARSH are the words that maximize \(H\) and \(\chi\), and that minimize \(\sigma\). And yet, when you watch the game unfold, it looks mysterious or magical.</p>
<hr />
<p>The cautious strategy of accumulating intelligence before committing to a line of play yields better results on average, but it comes with a price. All of the best algorithms achieve their strong scores by reducing the number of games that linger for five or six rounds of guessing. However, those algorithms also reduce the number of two-guess games, an effect that has to be counted as collateral damage. Two-guess triumphs make up less than 3 percent of the games played by the Zip code–based programs. Contrast that with the letter-frequency algorithm: In most respects it is quite mediocre, but it wins more than 6 percent of its games in two guesses. And among my personal games, 7 percent are two-guess victories. (I don’t say this to brag; what it suggests is that my style of play is a tad reckless.)</p>
<p>The Zip code–based programs would have even fewer two-guess wins without a heuristic that improves performance in a small fraction of cases. Whenever the list of candidate target words has dwindled down to a length of two, there’s no point in seeking more information to distinguish between the two remaining words. Suppose that after the first guess you’re left with the words SWEAT and SWEPT as the only candidates. For your second guess you could play a word such as ADEPT, where either the A or the P would light up to indicate which candidate is the target. You would then have a guaranteed win in three rounds. But if you simply played one of the candidate words as your second guess, you would have a 50 percent chance of winning in two rounds, and otherwise would finish in three, for an average score of 2.5.</p>
<p>This heuristic is already implemented in the programs discussed above. It makes a noticeable difference. Removing it from the max-entropy program drops the number of two-guess games from 51 down to 35 (out of 2,315). </p>
<p>Can we go further with this idea? Suppose there are three candidates left as you’re preparing for the second round of guessing. The cautious, information-gathering strategy would bring consistent victory on the third guess. Playing one of the candidates leads to a game that lasts for two, three, or four turns, each with probability one-third, so the average is again three guesses. The choice appears to be neutral. In practice, playing one of the candidates brings a tiny improvement in average score—too tiny to be worth the bother. </p>
<p>Another optimization says you should always pick one of the remaining candidates for your sixth guess. Gathering additional information is pointless in this circumstance, because you’ll never have a chance to use it. However, the better algorithms almost never reach the sixth guess, so this measure has no payoff in practice.</p>
<p>Apart from minor tricks and tweaks like these, is there any prospect of building significantly better Wordling programs? I have no doubt that im­provement is possible, even though all my own attempts to get better results have failed miserably. </p>
<p>Getting to an average performance of 3.5 guesses per game seems fairly easy; getting much beyond that level may require new ideas. My impression is that existing methods work well for choosing the first guess and perhaps the second, but are less effective in closing out the endgame. When the number of candidates is small, the Zip code–based algorithms cannot identify a single best next guess; they merely divide the possibilities into a few large classes of better and worse guesses. We need finer means of discrimination. We need tiebreakers.</p>
<p>I’ll briefly mention two of my failed experiments. I thought I would try going beyond the Zip code analysis and computing for each combination of a guess word and a potential target word how much the choice would shrink the list of candidates. After all, the point of the game is to shrink that list down to a single word. But the plague of multitudinous ties afflicts this algorithm too. Besides, it’s computationally costly.</p>
<p>Another idea was to bias the ranking of the Zip code spectra, favoring codes that have more gold and green letter tiles, on the hypothesis that we learn more when a letter is present or correct. The hypothesis is disproved! Even tiny amounts of bias are detrimental.</p>
<p>My focus has been on reducing the average number of guesses, but maybe there are other goals worth pursuing. For example, can we devise an algorithm that will solve every Wordle with no more than four guesses? It’s not such a distant prospect. Already there are algorithms that exceed four guesses only in about 2 percent of the cases. </p>
<p>Perhaps progress will come from another quarter. I’ve been expecting someone to put one of the big machine-learning systems to work on Wordle. All I’ve seen so far is a <a href="https://arxiv.org/abs/2202.00557">small-scale study</a>, based on the technique of reinforcement learning, done by Benton J. Anderson and Jesse G. Meyer. Their results are not impressive, and I am led to wonder if there’s something about the problem that thwarts learning techniques just as it does other algorithmic approaches.</p>
<p>Wordle falls into the class of combinatorial word games. All you need to know is how letters go together to make a word; meaning is beside the point. Most games of this kind are highly susceptible to computational <em>force majeure</em>. A few lines of code and a big word list will find exhaustive solutions in milliseconds. For example, another New York <em>Times</em> game called Spelling Bee asks you to make words out of seven given letters, with one special letter required to appear in every word. I’m not very good at Spelling Bee, but my computer is an ace. The same code would solve the Jumble puzzles on the back pages of the newspaper. With a little more effort the program could handle Lewis Carroll’s Word Links (better known today as Don Knuth’s <a href="https://en.wikipedia.org/wiki/Word_ladder">Word Ladders</a>). And it’s a spiffy tool for cheating at Scrabble.</p>
<p>In this respect Wordle is different. One can easily write a program that plays a competent game. Even a program that chooses words at random can turn in a respectable score. But this level of proficiency is nothing like the situation with Spelling Bee or Jumble, where the program utterly annihilates all competition, leaving no shred of the game where the human player could cling to claims of supremacy. In Wordle, every now and then I beat my own program. How can that happen, in this day and age?</p>
<p>The answer might be as simple and boring as computational complexity. If I want my program to win, I’ll have to invest more CPU cycles. Or there might be a super-clever Wordle-wrangling algorithm, and I’ve just been too dumb to find it. Then again, there might be something about Wordle that sets it apart from other combinatorial word games. That would be interesting.</p>
<hr />
<h2>Notes</h2>
<h3 id="note_1">Note 1. History of the game and of the word lists.</h3>
<p>The charming story of Wordle’s creation was told last January in the New York <em>Times</em>. <a href="https://www.nytimes.com/2022/01/03/technology/wordle-word-game-creator.html">“Wordle Is a Love Story”</a> read the headline. Josh Wardle, a software developer formerly at Reddit, created the game as a gift to his partner, Palak Shah, a fan of word games. The <em>Times</em> story, written by Daniel Victor, marvelled at the noncommercial purity of the website: “There are no ads or flashing banners; no windows pop up or ask for money.” Three weeks later the <em>Times</em> bought the game, <a href="https://www.nytimes.com/2022/01/31/business/media/new-york-times-wordle.html">commenting in its own pages</a>, “The purchase . . . reflects the growing importance of games, like crosswords and Spelling Bee, in the company’s quest to increase digital subscriptions to 10 million by 2025.” So much for love stories.</p>
<p>As far as I can tell, the new owners have not fiddled with the rules of the game, but there have been a few revisions to the word lists. Here’s a summary based on changes observed between February and May.</p>
<p><br /></p>
<p class="undent">Six words were removed from the list of common words (a.k.a. target words), later added to the list of arcane words, then later still removed from that list as well, so that they are no longer valid as either a guess or a target:</p>
<p>AGORA, PUPAL, LYNCH, FIBRE, SLAVE, WENCH</p>
<p><br /></p>
<p class="undent">Twenty-two words were moved from various positions in the common words list to the end of that list (effectively delaying their appearance as Wordle-of-the-day until sometime in 2027):</p>
<p>BOBBY, ECLAT, FELLA, GAILY, HARRY, HASTY, HYDRO, </p>
<p>LIEGE, OCTAL, OMBRE, PAYER, SOOTH, UNSET, UNLIT, </p>
<p>VOMIT, FANNY, FETUS, BUTCH, STALK, FLACK, WIDOW, </p>
<p>AUGUR</p>
<p><br /></p>
<p class="undent">Two words were moved forward from near the end of the common list to a higher position where they replaced FETUS and BUTCH:</p>
<p>SHINE, GECKO</p>
<p><br /></p>
<p class="undent">Two words were removed from the arcane list and not replaced:</p>
<p>KORAN, QURAN</p>
<p><br /></p>
<p class="indent">Almost all the changes to the common list affect words that would have been played at some point in 2022 if they had been left in place. I expect further purges when the editors get around to vetting the rest of the list. </p>
<h3 id="note_2">Note 2. The Umpire’s scoring rule.</h3>
<p>When I first started playing Wordle, I had a simple notion of what the tile colors meant. If a tile was colored green, then that letter of the guess word appeared in the same position in the target word. A gold tile meant the letter would be found elsewhere in the target word. A gray tile indicated the letter was entirely absent from the target word. For my first Wordler program I wrote a procedure implementing this scheme, although its output consisted of numbers rather than colors: green = 2, gold = 1, gray = 0.</p>
<pre class="language-julia"><code>function scoreGuess(guess, target)
	score = [0, 0, 0, 0, 0]
	for i in 1:5
		if guess[i] == target[i]
			score[i] = 2
		elseif occursin(guess[i], target)
			score[i] = 1
		end
	end
	return score
end</code></pre>
<p class="indent">This rule and its computer implementation work correctly as long as we never apply them to words with repeated letters. But suppose the target word is MODEM and you play the guess MUDDY. Following the rule above, the Umpire would offer this feedback: <img src="http://bit-player.org/wp-content/uploads/2022/05/MUDDY-20210.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="MUDDY scored 2 0 2 1 0, or green gray green gold gray" border="0" />. Note the gold coloring of the second D. It’s the correct marking according to the stated rule, because the target word has a D not in the fourth position. But that D in MODEM is already “spoken for”; it is matched with the green D in the middle of MUDDY. The gold coloring of the second D could be misleading, suggesting that there’s another D in the target word.</p>
<p>The Wordle app would color the second D gray, not gold: <img src="http://bit-player.org/wp-content/uploads/2022/05/MUDDY-20200.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="MUDDY scored 2 0 2 0 0, or green gray green gray gray" border="0" />. The rule, apparently, is that each letter in the target word can be matched with only one letter in the guess word. Green matches take precedence.</p>
<p>There remains some uncertainty about which letter gets a gold score when there are multiple options. If MUDDY is the target word and ADDED is the guess word, we know that middle D will be colored green, but which of the other two Ds in ADDED gets a gold tile? I have not been able to verify how the Wordle app handles this situation, but my program assigns priority from left to right: <img src="http://bit-player.org/wp-content/uploads/2022/05/ADDED-01200.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="ADDED scored 0 1 2 0 0, or gray gold green gray gray" border="0" />.</p>
<p>This minor amendment to the rules brings a considerable cost in complexity to the code. We need an extra data structure (the array <code>tagged</code>) and an extra loop. The first loop (with index <code>i</code>) finds and marks all the green matches; the second loop (indices <code>j</code> and <code>k</code>) identifies gold tiles that have not been preempted by earlier green or gold matches. </p>
<pre class="language-julia"><code>function scoreGuess(guess, target)
  score  = [0, 0, 0, 0, 0]
  tagged = [0, 0, 0, 0, 0]
  for i in 1:5
    if guess[i] == target[i]
      score[i] = 2
      tagged[i] = 2
    end
  end
  for j in 1:5
    for k in 1:5
      if guess[j] == target[k] &amp;&amp; score[j] == 0 &amp;&amp; tagged[k] == 0
        score[j] = 1
        tagged[k] = 1
      end
    end
  end
  return score
end</code></pre>
<p>It is this element of the scoring rules that forbids Zip codes such as <img src="http://bit-player.org/wp-content/uploads/2022/05/four_green_one_gold.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="Zip code 2 2 2 2 1, colored green green green green gold" border="0" /> and its permutations in Figure 6. Under the naive rules, the guess STALL played against the target STALE would have been scored <img src="http://bit-player.org/wp-content/uploads/2022/05/STALL-22221.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="STALL scored 2 2 2 2 1, or green green green green gold" border="0" />. The refined rule renders it as <img src="http://bit-player.org/wp-content/uploads/2022/05/STALL-22220.png" style="display: inline-block; vertical-align: bottom; margin-bottom: 3.5px;" height="20" width="" alt="STALL scored 2 2 2 2 0, or green green green green gray" border="0" />.</p>
<h3 id="note_3">Note 3. The virtues of a uniform distribution.</h3>
<p>Let’s think about a game that’s simpler than Wordle, though doubtless less fun to play. You are given a deck of 64 index cards, each with a single word written on it; one of the words, known only to the Umpire, is the target. Your instructions are to “cut” the deck, dividing it into two heaps. Then the all-seeing Umpire will tell you which heap includes the target word. For the next round of play, you set aside the losing heap, and divide the winning heap into two parts; again the Umpire indicates which pile holds the winning word. The process continues until the heap approved by the Umpire consists of a single card, which must necessarily be the target. The question is: How many rounds of play will it take to reach this decisive state?</p>
<p>If you always divide the remaining stack of cards into two <em>equal</em> heaps, this question is easy to answer. On each turn, the number of cards and words remaining in contention is cut in half: from 64 to 32, then on down to 16, 8, 4, 2, 1. It takes six halvings to resolve all uncertainty about the identity of the target. This bisection algorithm is a central element of information theory, where the fundamental unit of measure, the <em>bit</em>, is defined as the amount of information needed to choose between two equally likely alternatives. Here the choices are the two heaps of equal size, which have the same probability of holding the target word. When the Umpire points to one heap or the other, that signal provides exactly one bit of information. To go all the way from 64 equally likely candidates to one identified target takes six rounds of guessing, and six bits of information.</p>
<p>Bisection is known to be an optimal strategy for many problem-solving tasks, but the source of its strength is sometimes misunderstood. What matters most is not that we split the deck into <em>two</em> parts but that we divide it into <em>equal</em> subsets. Suppose we cut a pack of 64 cards into four equal heaps rather than two. When the Umpire points to the pile that includes the target, we get twice as much information. The search space has been reduced by a factor of four, from 64 cards to 16. We still need to acquire a total of six bits to solve the problem, but because we are getting two bits per round, we can do it in three splittings rather than six. In other words, we have a tradeoff between more simple decisions and fewer complex decisions.</p>
<p>Figure 21 illustrates the nature of this tradeoff by viewing a decision process as traversing a tree from the root node (at the top) to one of 16 leaf nodes (at the bottom). For the binary tree on the left, finding your way from the root to a leaf requires making four decisions, in each case choosing one of two paths. In the quaternary tree on the right, only two decisions are needed, but there are four options at each level. Assuming a four-way choice “costs” twice as much as a two-way choice, the information content is the same in both cases: four bits.</p>
<p>Figure 21.<img src="http://bit-player.org/wp-content/uploads/2022/05/trees_binary_and_quad.svg" height="" width="" alt="Trees binary and quad" border="0" class="align-center" /></p>
<p class="indent">But what if we decide to split the deck unevenly, producing a larger and a smaller heap? For example, a one-quarter/three-quarter division would yield a pile of 16 cards on the left and 48 cards on the right. If the target happens to lie in the smaller heap, we are better off than we would be with an even split: We’ve gained two bits of information instead of one, since the search space has shrunk from 64 cards to 16. However, the probability of this outcome is only 1/4 rather than 1/2, and so our expected gain is only one bit. When the target card is in the larger pile, we acquire less than one bit of information, since the search space has fallen from 64 cards only as far as 48, and the probability of this event is 3/4. Averaging across the two possible outcomes,  the loss outweighs the gain.</p>
<p>Figure 22 shows the information budget for every possible cut point in a deck of 64 cards. The red curves labeled <em>left</em> and <em>right</em> show the number of bits obtained from each of the two piles as their size varies. (The <em>x</em> axis labels the size of the left pile; wherever the left pile has \(n\) cards, the right pile has \(64 - n\).) The overarching green curve is the sum of the left and right values. Note that the green curve has its peak in the center, where the deck has been split into two equal subsets of 32 cards each. This is the optimum strategy.</p>
<p>Figure 22.The equation for the left curve is \(y = (n/m)(\log_2 m - \log_2 n)\), where \(m = 64\) and \(n\) is the size of the left subset. For the right curve, substitute \(m - n\) for \(n\).<img src="http://bit-player.org/wp-content/uploads/2022/05/plot_of_info_from_unequal_subsets.svg" height="" width="" alt="Plot of info from unequal subsets" border="0" class="centered" /></p>
<p class="indent">I have made this game go smoothly by choosing 64 as the number of cards in the deck. Because 64 is a power of 2, you can keep dividing by 2 and never hit an odd number until you come to 1. With decks of other sizes the situation gets messy; you can’t always split the deck into two equal parts. Nevertheless, the same principles apply, with some compromises and approx­imations. Suppose we have a deck of 2,315 cards, each inscribed with one of the Wordle common words. (The deck would be about two feet high.) We repeatedly split it as close to the middle as possible, allowing an extra card in one pile or the other when necessary. Eleven bisections would be enough to distinguish one card out of \(2^{11} = 2{,}048\), which falls a little short of the goal. With 12 bisections we could find the target among \(2^{12} = 4{,}096\) cards, which is more than we need. There’s a mathematical function that interpolates between these fenceposts: the base-2 logarithm. Specifically, \(\log_2 2{,}315\) is about 11.18, which means the amount of information we need to solve a Wordle puzzle is 11.18 bits. (Note that \(2^{11.18} \approx 2315\). Of the 2,315 positions where the target card might lie within the deck, 1,783 are resolved in 11 bisections, and 532 require a 12th cut.)</p>
<h3 id="note_4">Note 4. Understanding the Shannon entropy equation.</h3>
<p>The foundational document of information theory, Claude Shannon’s 1948 paper “A Mathematical Theory of Communication,” gives the entropy equation is this form:</p>
<p>\[H = -\sum_{i} p_i \log_2 p_i .\]</p>
<p class="undent">The equation’s pedigree goes back further, to Josiah Willard Gibbs and Ludwig Boltzmann, who introduced it in a different context, the branch of physics called statistical mechanics.</p>
<p>Over the years I’ve run into this equation many times, but I do not count it among my close friends. Whenever I come upon it, I have to stop and think carefully about what the equation means and how it works. The variable \(p_i\) represents a probability. We are asked to multiply each \(p_i\) by the logarithm of \(p_i\), then sum up the products and negate the result. Why would one want to do such a thing? What does it accomplish? How do these operations reveal something about entropy and information?</p>
<p class="indent">When I look at the righthand side of the equation, I see an expression of the general form \(n \log n\), which is a familiar trope in several areas of mathematics and computer science. It generally denotes a function that grows faster than linear but slower than quadratic. For example, sorting \(n\) items might require \(n \log n\) comparison operations. (For any \(n\) greater than 2, \(n \log_2 n\) lies between \(n\) and \(n^2\).) </p>
<p>That’s <em>not</em> what’s going on here. Because \(p\) represents a probability, it must lie in the range \(0 \le p \le 1\). The logarithm of a number in this range is negative or at most zero. This gives \(p \log p\) a different complexion. It also draws attention to that weird minus sign hanging out in front of the summation symbol.</p>
<p class="indent">I believe another form of the equation is easier to understand, even though the transformation introduces more bristly mathematical notation. Let’s begin by moving the minus sign from outside to inside the summation, attaching it to the logarithmic factor:</p>
<p>\[H = \sum_{i} p_i (-\log_2 p_i) .\]</p>
<p class="undent">The rules for logarithms and exponents tell us that \(-\log x = \log x^{-1}\), and that \(x^{-1} = 1/x\). We can therefore rewrite the equation again as</p>
<p> \[H = \sum_{i} p_i \log_2 \frac{1}{p_i} .\]</p>
<p>Now the nature of the beast becomes a little clearer. Figure 23.<img src="http://bit-player.org/wp-content/uploads/2022/05/graph_of_p_log_1overp1.svg" style="margin-bottom: 0px; padding-bottom: 0px;" height="" width="275" alt="Graph of p log 1overp" border="0" class="alignright" />The factors \(p\) and \(\log (1/p)\) pull in opposite directions. If you make \(p\) larger, you make \(\log (1/p)\) smaller, and vice versa. At both extremes of the allowed range, near \(p = 0\) and \(p = 1\), the value of \(p \log (1/p)\) approaches 0. In between these limits, \(p \log (1/p)\) is always positive, and should have a maximum for some value of \(p\). Figure 23 shows that this is true. It’s no coincidence that the curve has the same form as one of those in Figure 22. (If you’re curious about the location of the peak, I’ll give you a hint: It lies where \(p\) is equal to the reciprocal of a famous number.)</p>
<p>We can now bring in some of the particulars of the Wordle problem. The probability \(p_i\) is in fact the probability that the target word is found in Zip code \(i\). Note that \(n_i / m\) has the proper behavior for a probability. It always lies between 0 and 1, and the sum of \(n_i / m\) for all \(i\) is equal to 1.This probability is equal to \(n_i / m\), where \(n_i\) is the number of words assigned to Zip code \(i\), and \(m\) is the total number of words in all the Zip codes. Making the substitutions \(p_i \rightarrow n_i / m\) and \(1 / p_i \rightarrow m / n_i\), we get the new equation</p>
<p> \[H = \sum_{i} \frac{n_i}{m} \log_2 \frac{m}{n_i} .\]</p>
<p>And now for the final transformation. The laws of logarithms state that \(\log x/y\) is equal to \(\log x - \log y\), so we can rewrite the Shannon equation as</p>
<p>\[H = \sum_{i} \frac{n_i}{m} (\log_2 m - \log_2 n_i) .\]</p>
<p class="undent">In this form the equation is easy to relate to the problem of solving a Wordle puzzle. The quantity \(\log_2 m\) is the total amount of information (measured in bits) that we need to acquire in order to identify the target word; \(\log_2 n_i\) is the amount of information we will still need to ferret out if Zip code \(i\) turns out the hold the target word. The difference of these two quantities is what we gain if the target is in Zip code \(i\). The coefficient \(n_i / m\) is the probability of this event.</p>
<p>One further emendation is needed. The logarithm function is undefined at 0, as \(\log x\) diverges toward negative infinity as \(x\) approaches 0 from above. Thus we have to exclude from the summation all terms with \(n_i = 0\). We can also fill in the limits of the index \(i\).</p>
<p>\[H = \sum_{\substack{i = 1\\n_i \ne 0}}^{242} \frac{n_i}{m} (\log_2 m - \log_2 n_i) \]</p>
<p>Shannon’s discussion of the entropy equation is oddly diffident. He introduces it by listing a few assumptions that the \(H\) function should satisfy, and then asserting as a theorem that \(H = -\Sigma p \log p\) is the only equation meeting these requirements. In an appendix to the 1948 paper he proves the theorem. But he also goes on to write, “This theorem, and the assumptions required for its proof, are in no way necessary for the present theory [<em>i.e.</em>, information theory]. It is given chiefly to lend a certain plausibility to some of our later definitions.” I’m not sure what to make of Shannon’s cavalier dismissal of his own theorem, but in the case of Wordle analysis he seems to be right. Other measures of dispersion work just as well as the entropy function.</p></div>







<p class="date">
by Brian Hayes <a href="http://bit-player.org/2022/words-for-the-wordle-weary"><span class="datestr">at June 01, 2022 07:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/06/01/ph-d-and-postdoc-positions-at-idsia-usi-supsi-apply-by-july-3-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/06/01/ph-d-and-postdoc-positions-at-idsia-usi-supsi-apply-by-july-3-2022/">Ph.D. and PostDoc positions at IDSIA, USI-SUPSI (apply by July 3, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms and Complexity Group of IDSIA Lugano (Switzerland), opens 2 Ph.D. positions and some PostDoc positions.</p>
<p>The project is on the interface of three very active areas of research in theoretical computer science and mathematics. The project proposes a new direction by combining two widely used methodologies in a new way, with an algebraic geometry view used to bring them together.</p>
<p>Website: <a href="https://people.idsia.ch/~monaldo/positions/positions.html">https://people.idsia.ch/~monaldo/positions/positions.html</a><br />
Email: monaldo.mastrolilli@idsia.ch</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/06/01/ph-d-and-postdoc-positions-at-idsia-usi-supsi-apply-by-july-3-2022/"><span class="datestr">at June 01, 2022 03:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>Irritated by repeated claims that spiral galaxies are logarithmic spirals <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108316217782130412">\(\mathbb{M}\)</a>),</span> sourced to pop science instead of expert research, I found <a href="https://doi.org/10.1093/mnras/stt1627">“Pitch angle variations in spiral galaxies” (Savchenko &amp; Reshetnikov, <em>MNRAS</em> 2013)</a> which tells a more complicated story. In logarithmic spirals, the pitch angle is constant, in Archimedean spirals it decreases with radius, and in hyperbolic spirals it increases. All three are used as models of galaxies. But real galaxies have more varied pitch angles that do not match these models.</p>
  </li>
  <li>
    <p><a href="https://mathenchant.wordpress.com/2022/05/17/good-shurik-grothendieck/">Good Shurik Grothendieck</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108321635569223835">\(\mathbb{M}\)</a>).</span> James Propp writes (another) account of Grothendieck, as a way of explaining of how a scene from <em>Good Will Hunting</em> on the nature of genius may not be as inaccurate as he once thought it was.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@Ianagol/108323524092080204">Ian Agol posts a link</a> to <a href="https://www.quantamagazine.org/how-complex-is-a-knot-new-proof-reveals-ranking-system-that-works-20220518/">a <em>Quanta</em> story</a> on <a href="https://arxiv.org/abs/2201.03626">his research showing that certain four-dimensional surfaces connecting pairs of knots can be used to partial order them</a>, asking “how well mathematicians that aren’t topologists can grok the concepts described”.</p>
  </li>
  <li>
    <p><a href="https://github.blog/changelog/2022-05-19-render-mathematical-expressions-in-markdown/">Github’s markdown rendering  now supports MathJax, finally</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108330496497692414">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=31438864">via</a>). Github uses this rendering e.g. for the home pages of github packages. For some reason their chosen syntax is a little different from Kramdown and from modern LateX. Kramdown (which is what my Jekyll-based blog uses) delimits both inline and display math with double-dollar. Modern LateX (and Mathstodon) uses backslash-paren and backslash-square-bracket. Github uses old-school-TeX single-dollar and double-dollar.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@nilesjohnson/108193220633795260">Niles Johnson posts an example of tensor products</a>. Over the integers, \((\mathbb{Z}/a)\otimes(\mathbb{Z}/b)=\mathbb{Z}/\gcd(a,b)\), maybe not what you might have expected for a product. In the ensuing thread, Jacob Siehler writes that tensor products are “one of the things where you can vacillate for years between ‘This is so simple I don’t understand what all the fuss is,’ and ‘Apparently I do not understand this at all.’ ” I agree.</p>
  </li>
  <li>
    <p><a href="http://troika.uk.com/work/troika-squaring-the-circle/">“Squaring the Circle”, Troika, 2013</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108349773535124980">\(\mathbb{M}\)</a>):</span> an artwork that, from the intended viewpoint, looks like a circle, but with a square reflection in the mirror behind it. See also <a href="https://doi.org/10.1080/10724117.2020.1714290">David Richeson’s “Squaring the circle in a mirror” in <em>Math. Horizons</em></a> about this piece.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/Morphinart/status/1487200838070284292">Ship in a Klein bottle</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/web/@Bruce/108353214826961618">via</a>).</span> Nice rendered artwork by J. Miguel Medina.</p>
  </li>
  <li>
    <p>If you’re an academic, you’re probably unhappy with Chegg <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108361284820125145">\(\mathbb{M}\)</a>).</span> I know I am. Not so much for promoting intellectual property theft, but because they profit from miseducating students by luring them into copying rather than getting the benefit of their own work, and because they greatly boost my workload by forcing me to make up new problems instead of reusing them. But did you know that <a href="https://www.chronicle.com/article/work-in-public-education-and-hate-chegg-you-might-be-an-investor">your university’s retirement fund investors may well be financial supporters of Chegg?</a></p>
  </li>
  <li>
    <p>You can make a room whose walls are entirely mirrored <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108366902122043626">\(\mathbb{M}\)</a>),</span> with the property that a light source at any point will leave some parts of the room dark. I’d previously only seen this as a mathematical construct but <a href="https://www.youtube.com/watch?v=x3VluzZTReE">in this video Steve Mould plays with a physical one</a>.</p>
  </li>
  <li>
    <p>New Wikipedia Good Article: <a href="https://en.wikipedia.org/wiki/Squaring_the_circle">Squaring the circle</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108369499495102486">\(\mathbb{M}\)</a>).</span> It mentions an approximation to \(\pi\) by Ramanujan based on truncating the continued fraction</p>

\[\pi^4=97 + \cfrac{1}{2 + \cfrac{1}{2 + \cfrac{1}{3 + \cfrac{1}{1+\cfrac{1}{16539+\cdots}}}}}\]

    <p>Does anyone understand why this continued fraction has such a big term? Equivalently, why do powers of \(\pi\) seem to have unusually good rational approximations?</p>

    <p>Relatedly, I reviewed (but did not significantly edit) another new Good Article, <a href="https://en.wikipedia.org/wiki/Lonely_runner_conjecture">Lonely runner conjecture</a>, on the unproven conjecture that for \(n\) runners moving at different linear speeds around a circular track, each runner will eventually be far from all other runners.</p>
  </li>
  <li>
    <p>When I was an undergrad, I had a job on campus as a DECsystem-20 systems programmer <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108376805535251788">\(\mathbb{M}\)</a>).</span> So when my colleague Rich Pattis announced that, in preparation for his impending retirement, he was clearing out his office and giving away, among other things, the PDP-10 front console he had rescued from the trash pile of another university, I jumped at the opportunity. Here it is, atop one of my office bookshelves. Note the 36 keys in the front row: its memory was organized into 36-bit words, not bytes.</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="PDP-10 front console" src="https://www.ics.uci.edu/~eppstein/pix/pdp10/PDP10-m.jpg" /></p>
  </li>
  <li>
    <p>In Poland, getting a full professor title requires sign-off by the president of the country, currently Andrzej Duda. As part of a pattern of holocaust denial from Duda’s party, <a href="https://web.archive.org/web/20220525204138/https://www.timeshighereducation.com/news/polish-president-stifles-genocide-researchers-professorship-bid">Duda has refused for three years to sign off on the application of Michał Bilewicz, head of the Center for Research on Prejudice at the University of Warsaw and a prominent researcher on, among other things, anti-semitism in Poland</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108380954223141014">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/05/28/weekend-reads-female-driver-stereotypes-stealth-research-ai-comes-to-fake-scientific-images/">via</a>).</p>
  </li>
  <li>
    <p>Here’s a new illustration for <a href="https://en.wikipedia.org/wiki/Mrs._Miniver%27s_problem">the Wikipedia article on Mrs. Miniver’s problem</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108387115665183435">\(\mathbb{M}\)</a>).</span> The problem asks to arrange two circles so that their intersection (yellow) has the same area as the surrounding parts of the union (blue). It involves solving a transcendental equation, so this seemed like a good time to write <a href="https://commons.wikimedia.org/wiki/File:Mrs_Miniver%27s_Problem.svg">a script to find the geometry numerically</a> before switching to a graphics editor to color it in.</p>

    <p><img src="https://11011110.github.io/blog/assets/2022/MrsMiniver.svg" alt="Three instances of Mrs. Miniver's problem" /></p>
  </li>
  <li>
    <p><a href="http://colinmorris.github.io/blog/unpopular-wiki-articles">In search of the least viewed article on Wikipedia</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108393231911048502">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=31524943">via</a>). The trick is that the “random article” button isn’t uniformly random on articles. It tags articles with permanent random numbers in the unit interval, and when pressed picks another random number and looks for the next larger tag. For seldom-viewed articles, most views are random, and the probability of getting a random view is proportional to the length of the gap between an article’s permanent random number and the next smaller one. So the tail of least-viewed articles is dominated by unlucky articles whose gap is small.</p>
  </li>
  <li>
    <p>Any baker knows: <a href="https://www.epicurious.com/expert-advice/for-easier-parchment-paper-crumple-it-article">crumple your parchment paper to make it more flexible and fit neatly into any curved pie dish</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108399624501508690">\(\mathbb{M}\)</a>).</span> But one reason this works involves deep mathematics: cut or cleanly folded paper can only fit <a href="https://en.wikipedia.org/wiki/Developable_surface">piecewise-developable surfaces</a> such as patches of planes, cylinders, or cones. By <a href="https://en.wikipedia.org/wiki/Nash%E2%80%93Kuiper_theorem">the Nash–Kuiper theorem</a>, continuous but non-smoothly-differentiable (crumpled) paper surfaces can fit any surface.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/05/31/linkage.html"><span class="datestr">at May 31, 2022 11:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/30/easy-as-abc/">Easy as ABC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<i>A modern mathematical proof is not very different from a modern machine: the simple fundamental principles are hidden and almost invisible under a mass of technical details—Hermann Weyl.</i></font></p><font color="#0044cc">
<p>
<font color="#000000"></font></p><font color="#000000">
<p>
Shinichi Mochizuki is a mathematician who is at the center of a decade old claim. He has—he says since 2012—solved a famous open problem in number theory called the <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> conjecture. This conjecture is in number theory and would revolutionize our understanding of the structure of the natural numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/sm-3/"><img width="400" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/sm-1.jpg?resize=400%2C267&amp;ssl=1" class="aligncenter wp-image-20108" height="267" /></a></p>
<p>
</p><p><b> What is the ABC? </b></p>
<p></p><p>
The <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> conjecture is a conjecture in number theory, first proposed by Joseph Oesterle and David Masser independently around 1985. It is stated in terms of three positive integers, <img src="https://s0.wp.com/latex.php?latex=%7Ba%2C+b%2C+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a, b, c}" class="latex" /> (hence the name) that are relatively prime and satisfy <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b = c}" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> denotes the product of the distinct prime factors of <img src="https://s0.wp.com/latex.php?latex=%7Babc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{abc}" class="latex" />, the conjecture essentially states that <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> is usually not much smaller than <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>
The common term for the product <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> of the distinct prime factors of a positive integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is the “radical” of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, written <img src="https://s0.wp.com/latex.php?latex=%7Brad%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{rad(n)}" class="latex" /> by Wikipedia, in Timothy Gowers’s <em>Princeton Companion</em> <a href="https://books.google.com/books?id=ZOfUsvemJDMC&amp;pg=PA681#v=onepage&amp;q&amp;f=false">article</a>, and in other sources in abc. We demur from doubling up on an established term with a different meaning and suggest calling it the “wingspan” <img src="https://s0.wp.com/latex.php?latex=%7Bw%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w(n)}" class="latex" />. Then square-free integers have the largest possible wingspan, while large prime powers minimize it. Now we can state the conjecture formally:</p>
<blockquote><p>
For every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 0}" class="latex" />, all but finitely many triples of relatively prime positive numbers giving <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+c%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b = c}" class="latex" /> have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w%28abc%29+%3E+c%5E%7B1-%5Cepsilon%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  w(abc) &gt; c^{1-\epsilon}. " class="latex" /></p>
</blockquote>
<p>
Intuitively what it says is that numbers that have large powers of different primes cannot be related by addition. As <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon \rightarrow 0}" class="latex" />, it says that the wingspan of the triple’s product <img src="https://s0.wp.com/latex.php?latex=%7Bn%3Dabc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=abc}" class="latex" /> must at least approach <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />, which is greater than the cube root of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. Note, incidentally, that if any two of <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a,b,c}" class="latex" /> share a prime factor <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> then so does the third, so we can divide out all such <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> to get <img src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%2Cc%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a',b',c'}" class="latex" /> meeting the hypothesis. </p>
<p>
The point of the conjecture is that it relates addition and multiplication. It allows making inferences about the multiplicative structure of natural numbers from additive properties and vice-versa. The formal theory of the natural numbers with respect to addition alone, called <a href="https://en.wikipedia.org/wiki/Presburger_arithmetic">Presburger arithmetic</a>, is decidable, as is the theory of multiplication alone, called <a href="https://en.wikipedia.org/wiki/Skolem_arithmetic">Skolem arithmetic</a>. The theory of both <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" />, <a href="https://en.wikipedia.org/wiki/Peano_axioms">Peano arithmetic</a>, is of course undecidable. But <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> gives a playbook for leveraging the decidable sub-systems.</p>
<p>
</p><p><b> ABC Implies What? </b></p>
<p></p><p>
The conjecture has high <em>explanatory power</em> in that many other conjectures (listed <a href="https://en.wikipedia.org/wiki/Abc_conjecture#Some_consequences">here</a>) follow from it. Among them, we note:</p>
<ol>
<li> Whereas no one has significantly simplified Andrew Wiles’s famously difficult proof strategy for Fermat’s Last Theorem (FLT), the theorem for <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \ge 6}" class="latex" /> follows quickly from a weaker analogue of the abc conjecture.
</li><li> A generalization of FLT concerning powers that are sums of powers, called the Fermat-Catalan conjecture, also follows from abc.
</li><li> If a polynomial <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x)}" class="latex" /> with integer coefficients has at least three simple zeroes, then there are only finitely many positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x)}" class="latex" /> is a perfect power (i.e., such that <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29+%3D+m%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x) = m^k}" class="latex" /> for some integers <img src="https://s0.wp.com/latex.php?latex=%7Bm%2Ck+%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m,k \geq 2}" class="latex" />).
</li></ol>
<p>
This raises the following natural question for us computational complexity theorists:</p>
<blockquote><p>
 Does the abc conjecture imply any “shocks” in complexity theory—namely, resolving basic open questions that have been open for over half a century?
</p></blockquote>
<p>
Ken and I are not aware of any. It does not seem to affect factoring, or complexity theory, or any main ingredients of our favorite P=NP problem. But it does have great impact on anything that concerns Diophantine questions. It is possible that connections may emerge at this level of detail. </p>
<p>
</p><p><b> Is The ABC Proved? </b></p>
<p></p><p>
Here is a timeline of Mochizuki <a href="https://www.scientificamerican.com/article/math-mystery-shinichi-mochizuki-and-the-impenetrable-proof/">proof</a>: </p>
<p>
Mochizuki made his work public in August 2012 without any fanfare. Soon it was picked up and the mathematical community was made aware of the claim he has proven the abc conjecture. This started the quest to determine if his proof is correct.</p>
<p>
The proof is long and complex. Workshops were held in 2015 and 2016 on it. The presentations did not lead to acceptance of Mochizuki’s ideas, and the proof remains unclear.</p>
<p>
Enter Peter Scholze and Jakob Stix—two world experts on number theory. They visited Kyoto University for five days of discussions with Mochizuki in 2018. It did not resolve the correctness of the proof but did bring into focus where the difficulties lay.</p>
<p>
They wrote a report <a href="https://ncatlab.org/nlab/files/why_abc_is_still_a_conjecture.pdf">Why abc is still a conjecture</a>. It starts:<br />
<em><br />
We, the authors of this note, came to the conclusion that there is no proof. We are going to explain where, in our opinion, the suggested proof has a problem, a problem so severe that in our opinion small modifications will not rescue the proof strategy.<br />
</em></p>
<p>
Then, Mochizuki wrote a response of his view of why their claims were wrong: <a href="https://www.kurims.kyoto-u.ac.jp/~motizuki/Cmt2018-05.pdf">Comments On The Manuscript by Scholze-Stix</a>. He said:<br />
<em><br />
It should be stated clearly that the assertion that “these are inessential to the point we are making” is completely false! I made numerous attempts to explain this during the March discussions, and it is most unfortunate that we were ultimately unable to communicate regarding this issue.<br />
</em></p>
<p>
The disagreement over the correctness remains:  Other authors have pointed to the unresolved dispute between Mochizuki and Scholze over the correctness of this work as an instance in which the peer review process of mathematical journal publication has failed in its usual function of convincing the mathematical community as a whole of the validity of a result. </p>
<p>
</p><p><b> Explaining Math </b></p>
<p></p><p>
Albert Einstein may have said:</p>
<blockquote><p>
“If you can’t explain it to a six year old, you don’t understand it yourself.”
</p></blockquote>
<p>
Some attribute this instead to Richard Feynman. But whoever said it the mathematical community generally agrees with the point. In the 1962 <a href="https://www.biblio.com/book/new-perspectives-physics-broglie-louis-translated/d/999571270">book</a> New Perspectives in Physics, by Louis De Broglie, states that Einstein, when discussing theories, said:</p>
<blockquote><p>
“<img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> ought to lend themselves to as simple a description as that even a child could understand…”
</p></blockquote>
<p>
I wonder if the requirement “explain it to a six year old” does not mean the six year old must understand it. Rather that when you explain it to them they listen politely. That is the requirement is they listen. What do you think?</p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Mochizuki still claims his <a href="https://www.nature.com/articles/nature.2012.11378">proof</a>. He violates the above rule: he cannot explain it to a six year old—not even a senior expert. It still has not yet been accepted as passing the peer review stage. See <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">this</a> for some general comments. And <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12220">this</a> for some more. It has lots of comments.</p>
<p>
Can the abc ever be resolved? I wonder if there is some way to say suppose that the abc is true. And then prove some surprising complexity consequence holds? Perhaps violate P=NP for example, or some other result. </p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/30/easy-as-abc/"><span class="datestr">at May 30, 2022 08:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-5678234482420567140">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2022/05/orna-kupfermans-interview-with-christel.html">Orna Kupferman's Interview with Christel Baier, Holger Hermanns and Joost-Pieter Katoen, CONCUR 2022 ToT Award Recipients</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i><span class="im">I am delighted to post <a href="https://www.cs.huji.ac.il/~ornak/" target="_blank">Orna Kupferman</a>'s interview with <a href="https://concur2022.mimuw.edu.pl/tot-award/" target="_blank">CONCUR 2022 Test-of-Time Award</a> recipients <a href="https://wwwtcs.inf.tu-dresden.de/~baier/" target="_blank">Christel Baier</a>, <a href="https://depend.cs.uni-saarland.de/~hermanns/" target="_blank">Holger Hermanns</a> and<a href="https://www-i2.informatik.rwth-aachen.de/~katoen/" target="_blank"> Joost-Pieter Katoen</a>. </span></i></p><p><span class="im"><i><br />Thanks to Christel, Holger and Joost-Pieter for their answers (labelled BHK in what follows) and to Orna (Q below) for conducting the interview. Enjoy and watch this space for upcoming interviews with the other award recipients!</i>  <br /></span></p><p><span class="im">Q: You receive the CONCUR Test-of-Time Award 2022 for your paper "<a href="https://link.springer.com/content/pdf/10.1007/3-540-48320-9_12.pdf" target="_blank">Approximate symbolic model checking of continuous-time Markov chains</a>," which appeared <br />at CONCUR 1998. In that article, you combine three different challenges: symbolic algorithms, real-time systems, and probabilistic systems. Could you briefly explain to our readers what the main challenge in such a combination is?<br /><br /></span>BHK: The main challenge is to provide a fixed-point characterization of time-bounded reachability probabilities: the probability to reach a given target state within a given deadline. Almost all works in the field up to 1999 treated discrete-time probabilistic models and focused on "just" reachability probabilities: what is the probability to eventually end up in a given target state? This can be characterized as a unique solution of a linear equation system. The question at stake was: how to incorporate a real-valued deadline <i>d</i>? The main insight was <br />to split the problem in staying a certain amount of time, <i>x</i> say, in the current state and using the remaining <i>d-x</i> time to reach the target from its successor state. This yields a <a href="https://en.wikipedia.org/wiki/Volterra_integral_equation" target="_blank">Volterra integral equation system</a>; indeed time-bounded reachability probabilities are unique solutions of such equation systems. In the CONCUR'99 paper we suggested to use symbolic data structures to do the numerical integration; later we found out that much more efficient techniques can be applied.<span class="im"><br /><br />Q: Could you tell us how you started your collaboration on the award-winning paper? In particular, as the paper combines three different challenges, is it the case that each of you has brought to the research different expertise?<br /><br /></span>BHK: Christel and Joost-Pieter were both in Birmingham, where a meeting of a <br />collaboration project between German and British research groups on stochastic systems and process algebra took place. There the first ideas of model checking continuous-time Markov chains arose, especially for time-bounded reachability: with stochastic process algebras there were means to model CTMCs in a compositional manner, but verification was lacking. Back in Germany, Holger suggested to include a steady-state operator, the counterpart of transient properties that can be expressed using timed reachability probabilities. We then also developed the symbolic data structure to support the verification of the entire logic.<span class="im"></span><span class="im"><br /><br />Q: Your contribution included a generalization of <a href="https://en.wikipedia.org/wiki/Binary_decision_diagram" target="_blank">BDDs (binary decision diagrams)</a> to <a href="https://www.cs.cmu.edu/~emc/papers/Contributions%20to%20Edited%20Volumes/Multi-Terminal%20Binary%20Decision%20Diagrams%20and%20Hybrid%20Decision%20Diagrams.pdf" target="_blank">MTDDs (multi-terminal decision diagrams)</a>, which allow both Boolean and real-valued variables. What do you think about the current state of symbolic algorithms, in particular the choice between SAT-based methods and methods that are based on decision diagrams?<br /><br /></span>BHK: BDD-based techniques entered probabilistic model checking in the mid 1990's for discrete-time models such as Markov chains. Our paper was one of the first, perhaps even the first, that proposed to use BDD structures for real-time stochastic processes. Nowadays, SAT, and in particular SMT-based techniques belong to the standard machinery in probabilistic model checking. SMT techniques are e.g., used in bisimulation minimization at the language level, counterexample generation, and parameter synthesis. This includes both linear as well as non-linear theories. BDD techniques are still used, mostly in combination with sparse representations, but it is fair to say that SMT is becoming more and more relevant.<span class="im"><br /><br />Q: What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved?<br /><br /></span>BHK: This depends a bit on whom you ask! Christel's recent work is about cause-effect reasoning and notions of responsibility in the verification context. This ties into the research interest of Holger who looks at the foundations of perspicuous software systems. This research is rooted in the observation that the explosion of opportunities for software-driven innovations comes with an implosion of human opportunities and capabilities to understand and control these innovations. Joost-Pieter focuses on pushing the borders of automation in weakest-precondition reasoning of probabilistic programs. This involves loop invariant synthesis, probabilistic termination proofs, the development of deductive verifiers, and so forth. Challenges are to come up with good techniques for synthesizing quantitative loop invariants, or even complete probabilistic programs.<span class="im"><br /><br />Q: What advice would you give to a young researcher who is keen to start working on topics related to symbolic algorithms, real-time systems, and probabilistic systems?<br /><br /></span>BHK: Try to keep it smart and simple.</p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2022/05/orna-kupfermans-interview-with-christel.html"><span class="datestr">at May 30, 2022 03:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1695433635483317846">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/05/discussions-i-wish-we-were-having.html">Discussions I wish we were having</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>1) Democrats think the best way to avoid school shootings (and other problems with guns) is to have regulations on Guns. They have proposed legislation. The Republicans think its a mental health issue. They have proposed legislation for this. NO THEY HAVEN"T. I would respect the <i>its a mental health</i> <i>issue</i> argument if the people saying this  respected it. They do not. See <a href="https://www.kvue.com/article/news/local/report-texas-last-access-mental-health-care/269-1283fcee-0dc4-4948-9dda-886f55c49ca3">here</a>. Idea: Politico should leak a (false) memo  by Gov Abbott where he says </p><p><i>We have a serious mental health crisis in Texas which caused the recent event. I am not just saying this to deflect from the gun issue. I have drawn up a bill to fund mental health care, providing more money, for care and for studies. I call on Republicans and Democrats to pass it ASAP.</i></p><p>I wonder- if this false memo was leaked, would he deny it and say </p><p><i>I didn't write that. I am using mental health only as a way to deflect from the gun issue. How dare they say that I am reasonable and am proposing actual solutions. </i></p><p>Or would he be forced to follow through?</p><p>2) Democrats think Biden won the election. Some Republicans think Trump won the election. One issue was Arizona. So some republicans organized a recount of Arizona. And when they found out that Biden really did win it they said, as the good Popperian scientists they are, <i>we had a falsifiable hypothesis and it</i> <i>was shown to be false, so now we acknowledge the original hypothesis was wrong. </i>NO THEY DIDN"T. They seem to point to the Arizona audit as proof that they were right, even though it proves the opposite. (Same for all the court cases they lost.)</p><p>3) At one time I read some books that challenged evolution (Darwin on Trial by Phillip Johnson was one of them). Some of them DID raise some good points about how science is done (I am NOT being sarcastic). Some of them DID raise some questions like the gap in the fossil record and Michael Behe's notion of irreducible complexity.  (In hindsight these were window dressing and not what they cared about.) MY thought at the time was <i>its good to have people</i> v<i>iew a branch of science with a different viewpoint. Perhaps the scientists at the Discovery Institute will find something interesting. </i>(The Discovery institute is a think tank and one of their interests is Int. Design.) Alas, the ID people seem to spend their time either challenging the teaching of Evolution in school OR doing really bad science. Could intelligent people who think Evolution is not correct look at it in a different way than scientists do, and do good science, or at least raise good questions,  and come up with something interesting? I used to think so. Now I am not so sure.</p><p>4) I wish the debate was <i>what to do about global warming </i>and not <i>is global warming happening? </i>Conjecture: there will come a time when environmentalists finally come around to nuclear power being part of the answer. At that point, Republicans will be against Nuclear power just because the Democrats are for it. </p><p>5) I sometimes get email discussions like the following (I will call the emailer Mel for no good reason.)</p><p>------------------------------------------------------</p><p>MEL: Dr. Gasarch, I have shown that R(5)=45.</p><p>BILL: Great! Can you email me your 2-coloring of K_{44} that has no mono K_5?</p><p>MEL: You are just being stubborn. Look at my proof!</p><p>------------------------------------------------------------</p><p>Clyde has asked me <i>what if Mel had a nonconstructive proof?</i></p><p>FINE- then MEL can tell me that. But Mel doesn't know math well enough to make that</p><p>kind of argument. Here is the discussion I wish we had</p><p>-------------------------------------------</p><p>MEL: Dr. Gasarch, I have shown that R(5)=45.</p><p>BILL: Great! Can you email me your coloring of K_{44} that has no mono K_5?</p><p>MEL: The proof is non-constructive.</p><p>BILL: Is it a probabilistic proof? If so then often the prob is not just nonzero but close to 1. Perhaps you could write a program that does the coin flipping and finds the coloring.</p><p>MEL: The proof uses the Local Lovasz Lemma so the probe is not close to 1.</p><p>BILL: Even so, that can be coded up.</p><p>MEL: Yes but... (AND THIS IS AN INTELLIGENT CONVERSATION)</p><p>----------------------------------</p><p>Maybe Mel really did prove R(5)=44, or maybe not, but the above conversation would lead to</p><p>enlightenment. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/05/discussions-i-wish-we-were-having.html"><span class="datestr">at May 30, 2022 05:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6444">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6444">An understandable failing?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I hereby precommit that this will be my last post, for a long time, around the twin themes of (1) the horribleness in the United States and the world, and (2) my desperate attempts to reason with various online commenters who hold me personally complicit in all this horribleness.  I should really focus my creativity more on <em>actually fixing</em> the world’s horribleness, than on seeking out every random social-media mudslinger who blames me for it, shouldn’t I?  Still, though, isn’t undue obsession with the latter a pretty ordinary human failing, a pretty understandable one?</p>



<p>So anyway, if you’re one of the thousands of readers who come here simply to learn more about quantum computing and computational complexity, rather than to try to provoke me into mounting a public defense of my own existence (which defense will then, ironically but inevitably, stimulate <em>even more</em> attacks that need to be defended against) … well, either scroll down to the very end of this post, or wait for the next post.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Thanks so much to all my readers who donated to <a href="https://fundtexaschoice.org/">Fund Texas Choice</a>.  As promised, I’ve personally given them a total of $4,106.28, to match the donations that came in by the deadline.  I’d encourage people to continue donating anyway, while for my part I’ll probably run some more charity matching campaigns soon.  These things are addictive, like pulling the lever of a slot machine, but where the rewards go to making the world an infinitesimal amount more consistent with your values.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Of course, now there’s a brand-new atrocity to shame my adopted state of Texas before the world.  While the Texas government will go to extraordinary lengths to protect unborn children, the world has now witnessed 19 of its<em>born</em> children consigned to gruesome deaths, as the “good guys with guns”—waited outside and prevented parents from entering the classrooms where their children were being shot.  I have nothing original to add to the global outpourings of rage and grief.  Forget about the statistical frequency of these events: I know perfectly well that the risk from car crashes and home accidents is orders-of-magnitude greater.  Think about it this way: the United States is now known to the world as “the country that can’t or won’t do anything to stop its children from semi-regularly being gunned down in classrooms,” not even measures that virtually every other comparable country on earth has successfully taken.  It’s become <em>the</em> symbol of national decline, dysfunction, and failure.  If so, then the stakes here could fairly be called existential ones—not because of its <em>direct</em> effects on child life expectancy or GDP or any other index of collective well-being that you can define and measure, but rather, because a country that lacks the will to solve this will be judged by the world, and probably accurately, as lacking the will to solve anything else.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>In return for the untold thousands of hours I’ve poured into this blog, which has never once had advertising or asked for subscriptions, my reward has been years of vilification by sneerers and trolls.  Some of the haters even compare me to Elliot Rodger and other aggrieved mass shooters.  And I mean: yes, it’s <em>true</em> that I was bullied and miserable for years.  It’s true that Elliot Rodger, Salvador Ramos (the Uvalde shooter), and most other mass shooters were also bullied and miserable for years.  But, Scott-haters, if we’re being intellectually honest about this, we might say that the similarities between the mass shooter story and the Scott Aaronson story end at a certain point not very long after that.  We might say: it’s not just that Aaronson didn’t respond by hurting anybody—rather, it’s that his response loudly <em>affirmed</em> the values of the Enlightenment, meaning like, the whole package, from individual autonomy to science and reason to the rejection of sexism and racism to everything in between.  Affirmed it in a manner that’s not secretly about popularity (demonstrably so, because it doesn’t <em>get</em> popularity), affirmed it via self-questioning methods intellectually honest enough that they’d probably <em>still</em> have converged on the right answer even in situations where it’s now obvious that almost everyone you around would’ve been converging on the <em>wrong</em> answer, like (say) Nazi Germany or the antebellum South.</p>



<p>I’ve been to the valley of darkness.  While there, I decided that the only “revenge” against the bullies that was possible or desirable was to <em>do</em> something with my life, to achieve something in science that at least some bullies might envy, while also starting a loving family and giving more than most to help strangers on the Internet and whatever good cause comes to his attention and so on.  And after 25 years of effort, some people might say I’ve sort of <em>achieved</em> the “revenge” as I’d then defined it.  And they might further say: if you could get every school shooter to redefine “revenge” as “becoming another Scott Aaronson,” that would be, you know, like, a step upwards.  An improvement.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>And let this be the final word on the matter that I ever utter in all my days, to the thousands of SneerClubbers and Twitter randos who pursue this particular line of attack against Scott Aaronson (yes, we do mean the <em>thousands</em>—which means, it both <em>feels to its recipient like</em> the entire earth yet <em>actually is</em> less than 0.01% of the earth). </p>



<p><strong>We see what <em>Scott</em> did with <em>his</em> life, when subjected for a decade to forms of psychological pressure that are infamous for causing young males to lash out violently.  What would <em>you</em> have done with <em>your</em> life?</strong></p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>A couple weeks ago, when the trolling attacks were arriving minute by minute, I toyed with the idea of permanently shutting down this blog.  <em>What’s the point?</em> I asked myself.  <em>Back in 2005, the open Internet was fun; now it’s a charred battle zone.  Why not restrict conversation to my academic colleagues and friends?  Haven’t I done enough for a public that gives me so much grief?</em>  I was dissuaded by many messages of support from loyal readers.  Thank you so much.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>If anyone needs something to cheer them up, you should really watch <a href="https://tv.apple.com/us/show/prehistoric-planet/umc.cmc.4lh4bmztauvkooqz400akxav?ign-itscg=MC_20000&amp;ign-itsct=atvp_brand_omd&amp;mttn3pid=Google%20AdWords&amp;mttnagencyid=a5e&amp;mttncc=US&amp;mttnsiteid=143238&amp;mttnsubad=OUS2019950_1-600303192626-c&amp;mttnsubkw=144248666108__J1lRfctc_&amp;mttnsubplmnt=">Prehistoric Planet</a>, narrated by an excellent, 96-year-old David Attenborough.  Maybe 35 years from now, people will believe dinosaurs looked or acted somewhat differently from these portrayals, just like they believe somewhat differently now from when I was a kid.  On the other hand, if you literally took a time machine to the Late Cretaceous and starting filming, you couldn’t get a result that <em>seemed</em> more realistic, let’s say to a documentary-watching child, than these CGI dinosaurs on their CGI planet seem.  So, in the sense of passing that child’s Turing Test, you might argue, the problem of bringing back the dinosaurs has now been solved.</p>



<p>If you … err … <em>really</em> want to be cheered up, you can follow up with <a href="https://www.pbs.org/wgbh/nova/series/dinosaur-apocalypse/">Dinosaur Apocalypse</a>, <em>also</em> narrated by Attenborough, where you can (again, as if you were there) watch the dinosaurs being drowned and burned alive in their billions when the asteroid hits.  We’d still be scurrying under rocks, were it not for that lucky event that only a monster could’ve called lucky at the time.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Several people asked me to comment on the recent <a href="https://scorpioncapital.s3.us-east-2.amazonaws.com/reports/IONQ.pdf">savage investor review</a> against the quantum computing startup IonQ.  The review amusingly mixed together every imaginable line of criticism, with every imaginable degree of reasonableness from 0% to 100%.  Like, quantum computing is impossible even in theory, <em>and</em> (in the very next sentence) other companies are much closer to realizing quantum computing than IonQ is.  And <a href="https://ionq.com/posts/may-12-2022-ionq-founders-respond">IonQ’s response</a> to the criticism, and see also <a href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/">this</a> by the indefatigable Gil Kalai.</p>



<p>Is it, err, OK if I sit this one out for now?  There’s probably, like, actually an <em>already-existing</em> machine learning model where, if you trained it on all of my previous quantum computing posts, it would know exactly what to say about this.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6444"><span class="datestr">at May 29, 2022 06:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/082">TR22-082 |  Low-Degree Polynomials Extract from Local Sources | 

	Omar Alrabiah, 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	Xin Li, 

	João Ribeiro</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue a line of work on extracting random bits from weak sources that are generated by simple processes. We focus on the model of locally samplable sources, where each bit in the source depends on a small number of (hidden) uniformly random input bits. Also known as local sources, this model was introduced by De and Watson (TOCT 2012) and Viola (SICOMP 2014), and is closely related to sources generated by AC$^0$ circuits and bounded-width branching programs. In particular, extractors for local sources also work for sources generated by these classical computational models.

Despite being introduced a decade ago, little progress has been made on improving the entropy requirement for extracting from local sources. The current best explicit extractors require entropy $n^{1/2}$, and follow via a reduction to affine extractors. To start, we prove a barrier showing that one cannot hope to improve this entropy requirement via a black-box reduction of this form. In particular, new techniques are needed.

In our main result, we seek to answer whether low-degree polynomials (over $\mathbb{F}_2$) hold potential for breaking this barrier. We answer this question in the positive, and fully characterize the power of low-degree polynomials as extractors for local sources.
More precisely, we show that a random degree $r$ polynomial is a low-error extractor for $n$-bit local sources with min-entropy $\Omega(r(n\log n)^{1/r})$, and we show that this is tight.

Our result leverages several new ingredients, which may be of independent interest. Our existential result relies on a new reduction from local sources to a more structured family, known as local non-oblivious bit-fixing sources. To show its tightness, we prove a ``local version'' of a structural result by Cohen and Tal (RANDOM 2015), which relies on a new ``low-weight'' Chevalley-Warning theorem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/082"><span class="datestr">at May 28, 2022 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20092">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/">CCC 2022 Conference</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>I have a private plane. But I fly commercial when I go to environmental conferences—Arnold Schwarzenegger</i></p>
<p>
Computational Complexity Conference <a href="https://computationalcomplexity.org">CCC</a> is about to happen:  </p>
<p><i>July 20—23, Philadelphia, PA, USA</i>  </p>
<p>It is an annual conference on the inherent difficulty of computational problems in terms of the resources they require.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/ccc/" rel="attachment wp-att-20094"><img width="600" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/ccc.png?resize=600%2C490&amp;ssl=1" class="aligncenter size-full wp-image-20094" height="490" /></a> </p>
<p>
Ryan Williams just sent out a request on behalf of CCC:  The travel allowances for CCC are available for students from US universities. The deadline for applying is June 8. Here is the <a href="https://computationalcomplexity.org/travelAllowance/travelAllowance2022.php">link</a>. </p>
<p>
</p><p><b> Accepted Papers </b></p>
<p></p><p>
Here is a list of the accepted papers with pointers to versions of the papers:</p>
<ul>
<li> Gal Beniamini. <a href="https://arxiv.org/abs/2004.14318">The Approximate Degree of Bipartite Perfect Matching </a>
</li><li> Till Tantau. <a href="https://arxiv.org/abs/2201.08895">On the Satisfaction Probability of k-CNF Formulas</a>
</li><li> Andrej Bogdanov, William Hoza, Gautam Prakriya and Edward Pyne. <a href="https://eccc.weizmann.ac.il/report/2021/143/">Hitting Sets for Regular Branching Programs</a>
</li><li> Svyatoslav Gryaznov, Pavel Pudlak and Navid Talebanfard. <a href="https://arxiv.org/abs/2201.10997">Linear Branching Programs and Directional Affine Extractors</a>
</li><li> Sandy Irani, Anand Natarajan, Chinmay Nirkhe, Sujit Rao and Henry Yuen. <a href="https://arxiv.org/abs/2111.02999">Quantum search-to-decision reductions and the state synthesis problem</a>
</li><li> Karthik C. S. and Subhash Khot. <a href="https://arxiv.org/abs/2112.03983">Almost Polynomial Factor Inapproximability for Parameterized k-Clique</a>
</li><li> Venkatesan Guruswami, Peter Manohar and Jonathan Mosheiff. <a href="https://arxiv.org/abs/2205.06738">l_p-Spread and Restricted Isometry Properties of Sparse Random Matrices</a>
</li><li> Ian Mertz and James Cook. <a href="https://eccc.weizmann.ac.il/report/2022/026/">Trading Time and Space in Catalytic Branching Programs</a>
</li><li> Harm Derksen, Visu Makam and Jeroen Zuiddam. <a href="https://staff.fnwi.uva.nl/j.zuiddam/">Subrank and Optimal Reduction of Scalar Multiplications to Generic Tensors</a>
</li><li> Guy Blanc and Dean Doron. <a href="https://eccc.weizmann.ac.il/report/2022/027/">New Near-Linear Time Decodable Codes Closer to the GV Bound</a>
</li><li> Jun-Ting Hsieh, Sidhanth Mohanty and Jeff Xu. <a href="https://arxiv.org/abs/2106.12710">Certifying solution geometry in random CSPs: counts, clusters and balance</a>
</li><li> Vikraman Arvind and Pushkar Joglekar. <a href="https://arxiv.org/abs/2202.09883">On Efficient Noncommutative Polynomial Factorization via Higman Linearization</a>
</li><li> Ivan Mihajlin and Anastasia Sofronova. <a href="https://eccc.weizmann.ac.il/report/2022/033/">A better-than-3log(n) depth lower bound for De Morgan formulas with restrictions on top gates</a>
</li><li> Dan Karliner, Roie Salama and Amnon Ta-Shma. <a href="https://eccc.weizmann.ac.il/report/2022/028/">The plane test is a local tester for Multiplicity Codes</a>
</li><li> Dmitry Sokolov. <a href="https://eccc.weizmann.ac.il/report/2021/076/">Pseudorandom Generators, Resolution and Heavy Width</a>
</li><li> Halley Goldberg, Valentine Kabanets, Zhenjian Lu and Igor C. Oliveira. <a href="https://eccc.weizmann.ac.il/report/2022/072/">Probabilistic Kolmogorov Complexity with Applications to Average-Case Complexity</a>
</li><li> Erfan Khaniki. <a href="https://eccc.weizmann.ac.il/report/2022/023/">Nisan–Wigderson generators in Proof Complexity: New lower bounds</a>
</li><li> Ryan O’Donnell and Kevin Pratt. <a href="https://arxiv.org/abs/2203.03705">High-Dimensional Expanders from Chevalley Groups</a>
</li><li> Victor Lecomte, Prasanna Ramakrishnan and Li-Yang Tan. <a href="https://arxiv.org/abs/2205.02374">The composition complexity of majority</a>
</li><li> Scott Aaronson, Devon Ingram and William Kretschmer. <a href="https://arxiv.org/abs/2111.10409">The Acrobatics of BQP</a>
</li><li> Zander Kelley and Raghu Meka. <a href="https://arxiv.org/abs/2103.14134">Random restrictions and PRGs for PTFs in Gaussian Space</a>
</li><li> Amol Aggarwal and Josh Alman. <a href="https://arxiv.org/abs/2205.06249">Optimal-Degree Polynomial Approximations for Exponentials and Gaussian Kernel Density Estimation</a>
</li><li> Lijie Chen, Jiatu Li and Tianqi Yang. <a href="https://tianqiyang.org">Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs</a>
</li><li> Pavel Hubacek. <a href="https://arxiv.org/abs/1902.10337">Snakes on Ladders: On the Complexity of the Parity Argument on Directed Cycles</a>
</li><li> Gal Arnon, Alessandro Chiesa and Eylon Yogev. <a href="https://eprint.iacr.org/2022/168">Hardness of Approximation for Stochastic Problems via Interactive Oracle Proofs</a>
</li><li> Shuichi Hirahara and Mikito Nanashima. <a href="https://simons.berkeley.edu/people/shuichi-hirahara">Finding Errorless Pessiland in Error-Prone Heuristica</a>
</li><li> Shuichi Hirahara. <a href="https://simons.berkeley.edu/people/shuichi-hirahara">Symmetry of Information from Meta-Complexity</a>
</li><li> Louis Golowich and Salil Vadhan. <a href="https://eccc.weizmann.ac.il/report/2022/024/">Pseudorandomness of Expander Random Walks for Symmetric Functions and Permutation Branching Programs</a>
</li><li> Nikhil Bansal, Makrand Sinha and Ronald de Wolf. <a href="https://arxiv.org/abs/2203.00212">Influence in Completely Bounded Block-multilinear Forms and Classical Simulation of Quantum Algorithms</a>
</li><li> Michael Saks and Rahul Santhanam. <a href="https://eccc.weizmann.ac.il/report/2022/074/">On Randomized Reductions to the Random Strings</a>
</li><li> Sarah Bordage, Mathieu Lhotel, Jade Nardi and Hugues Randriam. <a href="https://arxiv.org/abs/2011.04295">Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes</a>
</li><li> Siddharth Bhandari, Prahladh Harsha, Ramprasad Saptharishi and Srikanth Srinivasan. <a href="https://arxiv.org/abs/2205.10749">Vanishing Spaces of Random Sets and Applications to Reed-Muller Codes</a>
</li><li> Nutan Limaye, Srikanth Srinivasan and Sebastien Tavenas. <a href="https://eccc.weizmann.ac.il/report/2021/081/">On the Partial Derivative Method Applied to Lopsided Set-Multilinear Polynomials</a>
</li><li> Mika Goos, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere and Ran Tao. <a href="https://arxiv.org/abs/2202.07761">Further Collapses in TFNP</a>
</li><li> Xin Lyu. <a href="https://eccc.weizmann.ac.il/report/2022/021/">Improved Pseudorandom Generators for <img src="https://s0.wp.com/latex.php?latex=%7BAC%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{AC^0}" class="latex" /> Circuits</a>
</li><li> Yanyi Liu and Rafael Pass. <a href="https://www.cs.cornell.edu/information/news/newsitem12262/rafael-pass-and-yanyi-liu-explore-whether-one-way-functions-exist">Characterizing Derandomization Through Fine-Grained Hardness of Levin-Kolmogorov Complexity</a>
</li><li> Yanyi Liu and Rafael Pass. <a href="https://eprint.iacr.org/2021/513.pdf">On One-way Functions from NP-Complete Problems</a>
</li><li> Oliver Korten. <a href="https://eccc.weizmann.ac.il/report/2022/025/">Efficient Low-Space Simulations From the Failure of the Weak Pigeonhole Principle</a>
</li><li> Deepanshu Kush and Shubhangi Saraf. <a href="https://arxiv.org/abs/2205.00611">Improved Low-Depth Set-Multilinear Circuit Lower Bounds</a>
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Is this list with pointers helpful? It took a few minutes to form this list—had to add the pointers.</p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/"><span class="datestr">at May 27, 2022 08:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/081">TR22-081 |  Theory and Applications of Probabilistic Kolmogorov Complexity | 

	Zhenjian Lu, 

	Igor Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Diverse applications of Kolmogorov complexity to learning [CIKK16], circuit complexity [OPS19], cryptography [LP20], average-case complexity [Hir21], and proof search [Kra22] have been discovered in recent years. Since the running time of algorithms is a key resource in these fields, it is crucial in the corresponding arguments to consider time-bounded variants of Kolmogorov complexity. While fruitful interactions between time-bounded Kolmogorov complexity and different areas of theoretical computer science have been known for quite a while (e.g., [Sip83, Ko91, ABK+06, AF09], to name a few), the aforementioned results have led to a renewed interest in this topic. 

The theory of Kolmogorov complexity is well understood, but many useful results and properties of Kolmogorov complexity are not known to hold in time-bounded settings. Unfortunately, this creates technical difficulties or leads to conditional results when applying methods from time-bounded Kolmogorov complexity to algorithms and complexity theory. Perhaps even more importantly, in many cases it is desirable or even necessary to consider randomised algorithms. Since random strings have high complexity, the classical theory of time-bounded Kolmogorov complexity might be inappropriate or simply cannot be applied in such contexts.

To mitigate these issues and develop a more robust theory of time-bounded Kolmogorov complexity that survives in the important setting of randomised computations, some recent papers [Oli19, LO21, LOS21, GKLO22, LOZ22] have explored probabilistic notions of time-bounded Kolmogorov complexity, such as $\mathrm{rKt}$ complexity [Oli19], $\mathrm{rK}^t$ complexity [LOS21], and $\mathrm{pK}^t$ complexity [GKLO22]. These measures consider different ways of encoding an object via a probabilistic representation. In this survey, we provide an introduction to probabilistic time-bounded Kolmogorov complexity and its applications, highlighting many open problems and  research directions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/081"><span class="datestr">at May 27, 2022 01:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/27/being-different/">Being Different</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><i>In mathematics you don’t understand things. You just get used to them—John von Neumann.</i></p>
<p>
<a href="https://en.wikipedia.org/wiki/Harvey_Friedman">Harvey Friedman</a> is a famous mathematical logician who spent most of his career at Ohio State University. He worked not on proving new theorems as much as finding the axioms needed to prove them. Later in his career it was the axioms needed for certain <a href="https://en.wikipedia.org/wiki/Large_cardinal">large cardinal</a> theorems. These questions can be very subtle and difficult—not unlike lower bounds in complexity theory. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/hf/"><img width="194" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/hf.png?resize=194%2C259&amp;ssl=1" class="aligncenter size-full wp-image-20086" height="259" /></a><br />
Harvey was listed in the Guinness Book of World Records for being the world’s youngest professor when he taught at Stanford University at age 18 as an assistant professor of philosophy. He got his Ph.D. from Massachusetts Institute of Technology in 1967—hence the young age. </p>
<p>
</p><p><b> The Difference </b></p>
<p></p><p>
Harvey has taken a viewpoint for most of his career that is different from other logicians. Godel’s incompleteness theorems apply to most logic systems—certainly those that are strong enough for central areas of mathematics. But the majority of mathematicians believe that incompleteness does not apply to their everyday work. In short most do not think:</p>
<p>
	<i>I cannot prove P not equal to NP, so it must be incomplete.</i> 	 </p>
<p>Rather they feel that this means I am not smart enough to resolve P vs NP. Which of these is true? </p>
<p>
For example, Harvey has created numerous algebraic and geometric systems that make no explicit reference to logic but which, under a suitable coding, contain a logical system to which Godel’s incompleteness theorems apply. Furthermore, these <a href="https://www.encyclopedia.com/humanities/encyclopedias-almanacs-transcripts-and-maps/modern-logic-godel-friedman-and-reverse-mathematics">systems</a> look similar to many systems used by mathematicians in their everyday work. Harvey uses these examples to argue that incompleteness cannot be dismissed as a phenomenon that occurs only in overly general foundational frameworks contrived by logicians. He argues that it applies often to their everyday world.</p>
<p>
</p><p><b> An Example </b></p>
<p></p><p>
Harvey is also able to find numerous combinatorial statements with clear geometric meaning that are proved using large cardinals axioms and shown to require them. These <a href="https://arxiv.org/pdf/math/9811187.pdf">results</a> are famous to Harvey. Such axioms previously seemed to require statements that are not geometric. </p>
<p>
Gill Williamson can show that this can be used to connect these problems to the assumption that subset sum is solvable in polynomial time. See the paper <a href="https://arxiv.org/pdf/1907.11707.pdf">On The Difficulty Of Proving P Equals NP In ZFC</a>. This curious connection between the P vs. NP problem and the theory of large cardinals seems to suggest that either P=NP is false or otherwise not provable in ZFC. This connection is <a href="http://people.cs.uchicago.edu/~fortnow/beatcs/column81.pdf">surprising</a>.</p>
<p>
</p><p><b> Incompleteness </b></p>
<p></p><p>
Harvey does have a conjecture that shows that certain statements are not incomplete. He is interested in both sides of this coin: sometimes statements are provable and sometimes not. Concretely many mathematical theorems, such as Fermat’s Last Theorem, can be proved in very weak systems such as EFA. The <a href="https://hal.archives-ouvertes.fr/hal-00670733/document">Grand Conjecture</a> says:  Every theorem published in the Annals of Mathematics whose statement involves only finitary mathematical objects (i.e. what logicians call an arithmetical statement) can be proved in EFA. </p>
<p>
EFA is the weak fragment of Peano Arithmetic based on the usual quantifier-free axioms for <img src="https://s0.wp.com/latex.php?latex=%7B0%2C+1%2C+%2B%2C+%5Ctimes%2C+x%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0, 1, +, \times, x^y}" class="latex" />, together with the scheme of induction for all formulas whose quantifiers are bounded. While it is easy to construct artificial arithmetical statements that are true but not provable in EFA the point of the grand conjecture is that natural examples of such statements seem to be rare. </p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
 When Harvey was just starting to <a href="https://nautil.us/this-man-is-about-to-blow-up-mathematics-5857/">read</a>, at age 4 or 5, he remembers pointing to a dictionary and asking his mother what it was. It’s used to find out what words mean, she explained. A few days later, he returned to her with his verdict: The volume was completely worthless. For every word he’d looked up, the dictionary had taken him in circles: from “large” to “big” to “great” and so on, until he eventually arrived back at “large” again. She just looked at me as if I were a really strange, peculiar child, Friedman laughs. </p>
<p>
This is an <a href="https://nautil.us/this-man-is-about-to-blow-up-mathematics-5857/">insight</a> reported in an article by Jordana Cepelewicz. I cannot imagine Harvey was four or five when he told his mom this. For more stuff—when not so young—see <a href="https://www.amazon.com/Harvey-Friedmans-Research-Foundations-Mathematics/dp/0444558160?asin=0444558160&amp;revisionId=&amp;format=4&amp;depth=1">the book</a>. <a href="https://rjlipton.wpcomstaging.com/book-9/"><img width="333" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/book.jpg?resize=333%2C499&amp;ssl=1" class="aligncenter size-full wp-image-20087" height="499" /></a></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/27/being-different/"><span class="datestr">at May 27, 2022 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=22716">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/">Quantum Computers: A Brief Assessment of Progress in the Past Decade</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="article-headline-container">
<div class="header-content-container">
<p>In this post I give a brief  assessment of progress in the past decade, triggered by a recent article in Forbes Magazine that mentions my view on the matter.</p>
<h3 class="fs-headline speakable-headline font-base font-size should-redesign"><span style="color: #ff0000;">Waging War On Quantum</span> – A Forbes Article by Arthur Herman</h3>
</div>
</div>
<div class="top-contrib-block">
<div class="contribs">
<div class="contrib-container top-contrib bottom-padding">
<div class="fs-author-group-wrapper">
<div class="contrib-byline"></div>
</div>
</div>
</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Arthur_L._Herman">Arthur Herman</a> is a popular Historian and a senior fellow at the Hudson Institute. On Forbes Magazine he “comments on quantum computing and AI and American national security”, and his recent Forbes article <a href="https://www.forbes.com/sites/arthurherman/2022/05/19/waging-war-on-quantum/?sh=2044c5318896">Waging War on Quantum</a> (thanks, Alexander Vlasov) starts as follows:</p>
<blockquote><p><span style="color: #993300;"><em><strong>Quantum computing will never work. Keeping enough qubits stable long enough to do any significant calculating or processing, is a mathematical impossibility. The whole idea that one day quantum computers will discover new miracle drugs, or crack public encryption systems, is a mirage. Even worse, it’s a hoax.</strong></em></span></p>
<p><span style="color: #993300;"><em><strong>That’s been the message from so-called quantum skeptics for a decade or more, including physicists like Gil Kalai of Hebrew University and Mikhail Dyakonov of the University of Montepellier—all in spite of the fact that quantum computers have continued to grow in sophistication and qubit power. Most experts now agree it’s not a question if a large-scale quantum will emerge that can break into public encryption systems using Shor’s algorithm, but </strong></em><strong>when</strong><em><strong>.”</strong></em></span></p></blockquote>
<p><span style="color: #000000;">The first paragraph gives a reasonable description of my views, however I <strong>never</strong> referred to the whole idea of quantum computing as a hoax. </span><span style="color: #0000ff;"><span style="color: #000000;">Regarding the second paragraph, it is indeed correct that quantum computers have continued to grow in sophistication and qubit power, however <a href="https://arxiv.org/abs/1908.02499">my theory</a> (based on a computational complexity argument) is that progress for reducing the error rate will reach a wall and that recent progress merely approaches this limit. Let me elaborate a little on the development of the past decade as I see it</span>.<br />
</span></p>
<p>Before moving to my assessment I would like to note that Arthur Herman’s offers an outrageous conclusion to his article. He suggests that skepticism of quantum computers (and of the company <a href="https://ionq.com/">IonQ</a>) puts the skeptics’ countries at risk. In my opinion, the militant rhetoric of the title and of the conclusion is very inappropriate.</p>
<h2>Assessment of progress in the past decade</h2>
<p>The past quantum computing decade is characterized both by notable progress, adjustment of expectations, larger investments, much enthusiasm, and some hype. The overall picture is unclear and might be clearer 5-10 years from now.</p>
<p><span style="color: #0000ff;">The following picture (click to enlarge) describes the shift in the community view over the last decade (as I see it).</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2022/05/2010-2020d.png"><img width="724" alt="2010-2020d" src="https://gilkalai.files.wordpress.com/2022/05/2010-2020d.png?w=724&amp;h=308" class="alignnone  wp-image-22721" height="308" /></a></p>
<p>On the left you can see David DiVincenzo’s famous 7-steps road map to quantum computers. DiVincenzo put forward these steps in his 2000 paper  <a href="https://arxiv.org/abs/quant-ph/0002077">The physical implementation of quantum computation</a>,  and the above picture on the left is a graphic description of these steps in a <a href="https://www.science.org/doi/10.1126/science.1231930">2013 review paper</a> by Michel Devoret and Rob Schoelkopf.  The caption under the Figure asserts that <span style="color: #0000ff;">“Superconducting qubits are the only solid-state implementation at the third stage, and they now aim at reaching the fourth stage (green arrow). In the domain of atomic physics and quantum optics, the third stage had been previously attained by trapped ions and by Rydberg atoms. No implementation has yet reached the fourth stage, where a logical qubit can be stored, via error correction, for a time substantially longer than the decoherence time of its physical qubit components.”</span> The fourth step “logical memory with (substantial) longer lifetime than physical qubits” looked to many like a near term goal ten years ago.</p>
<p>One important development of the last ten years was to introduce building NISQ computers and achieving “quantum supremacy” (and related tasks like high “quantum volume”) as an intermediate goal towards DiVincenzo’s step four. (See the picture on the right.) Of course, there is nothing wrong with setting intermediate goals, we do it all the time and this can be very fruitful.</p>
<p>For me, from the skeptical point of view, these intermediate goals were an opportunity, allowing me to present a <a href="https://gilkalai.wordpress.com/2020/12/29/the-argument-against-quantum-computers-a-very-short-introduction/">clear computational theoretic argument</a> for why “quantum supremacy” is out of reach and to connect the problem with the theory of noise-sensitivity and noise-stability and with Fourier methods that I and my colleagues developed in the 90s.</p>
<p>Adding the intermediate goal of quantum supremacy also represented a much slower time-table than what people previously anticipated. For example, nine years ago in 2013  John Martinis <a href="https://youtu.be/7Pok08VvkeU">gave a lecture</a> at <a href="https://gilkalai.wordpress.com/2013/04/12/qstart/">QSTART</a>,  the opening conference of our HUJI quantum science center. At that time, John expected to have the ability of building distance-3 and distance-5 surface codes within a few years and the tasks of demonstrating logical gates and of logical qubits with <img src="https://s0.wp.com/latex.php?latex=10%5E%7B-15%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="10^{-15}" class="latex" /> error rates some years later. John also mentioned the ability to control 20 qubits within one month (to this Ray Laflamme commented that it is going to be a long month). All these targets are today still out of reach. It is undisputed that considerably lower noise rates are required even for achieving distance-3 surface codes and it is still not possible to have good control of 20-qubit (and perhaps even 10-qubit) quantum computation.</p>
<p>Of course, John Martinis himself was the leader of the Google efforts towards “quantum supremacy” which are now being carefully evaluated, and his vision and technology from 2013 was important for the Sycamore NISQ experiments. Let me mention that Google’s fantastic “quantum supremacy” claims were <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">largely (but not fully) refuted</a>.</p>
<p>There was a similar level of optimism from various other researchers. <a href="https://en.wikipedia.org/wiki/Robert_J._Schoelkopf#Schoelkopf.27s_law">It was expected</a> that coherence time would increase by a factor of ten every three years and there was a proposed “<a href="https://www.quantamagazine.org/does-nevens-law-describe-quantum-computings-rise-20190618/">double exponential law</a>” prediction for the classical computational power required to simulate quantum devices as time proceeds.  I personally don’t regard these specific claims as hype but rather as (at times, over-the-top) reasoned optimism, but both the reasoning and the predictions themselves should carefully be examined.</p>
<p>NISQ computers are interesting, and they allow interesting quantum physics experiments. Herman asserts that <strong><em>“quantum hybrid systems are making the qubit revolution something that’s happening now, not just a distant dream” </em></strong>and this echoes hopes of several researchers in academia and industry. My analysis asserts that NISQ computers are, from the computational complexity perspective, primitive classical computational devices with inherent chaotic behavior, and therefore, I don’t see how hybrid systems and the interface with conventional computers would turn them into useful computational devices. (They can still be useful for quantum physics experimentation.)</p>
<p>Let me repeat: slower progress than anticipated is very common, setting new intermediate goals is both common and welcome. By themselves they do not imply that the target of “<em><strong>large-scale quantum computers that can break into public encryption systems using Shor’s algorithm” </strong></em>is unrealistic, and indeed many experts in the field believe that it is a matter of time for this ultimate goal to be reached. My view is different, I try to explain my argument to other experts, and to offer experimental predictions and theoretical implications. There are good reasons to hope that the matter will be tested experimentally in the years to come, but my assessment is that the experimental picture from the past decade <strong>is not clear</strong>.</p>
<h3>IonQ, trapped ion quantum computation, and Elon Musk</h3>
<p>Herman’s article was triggered by a 183-page document written by a group called “Scorpion Capital.” The document attacks Maryland-based quantum computer company IonQ and among various concerns it also briefly mentions my and Michel Dyakonov’s positions about quantum computers.  I myself share the common view that ion-trap methods for quantum computers form a major avenue and that Chris Monroe (a co-founder of IonQ) is a major player in this direction. I don’t know much about IonQ’s specific efforts, but I would expect that large scale investment is required to put ion-trap methods to test and I personally would like to see it being tested. So I would be quite pleased to see <strong>Elon Musk</strong> deciding to buy IonQ or the Israeli trapped ion QC company of <a href="https://www.weizmann.ac.il/complex/ozeri/welcome-weizmann-trapped-ions-lab">Roee Ozeri</a> (or both) <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> and to make  trapped ion technology his quantum computing signature. Incidentally, the comment section of my 2018 <a href="https://www.quantamagazine.org/gil-kalais-argument-against-quantum-computers-20180207/">Quanta Magazine interview</a> presented an interesting exchange between Monroe and me (starting <a href="https://www.quantamagazine.org/gil-kalais-argument-against-quantum-computers-20180207/#comment-4559220424">here</a>).</p>
<h3>A few more remarks:</h3>
<p>1) Herman’s article raises several other interesting issues like when (and if) is the appropriate time to transfer to “post quantum cryptography” protocols.</p>
<p>2) There are a few researchers skeptical of quantum computers that actually conduct research and write papers (and books) about it. (There are others that regard the idea as absurd nonsense of absolutely no interest.) A notable researcher who wrote several important papers in the skeptical direction since the late 90s is Robert Alicki from the University of Gdansk.</p>
<p>3) Here is Herman’s crazy conclusion: “No one is saying the Scorpion Capital short-sellers are in Chinese pay, or that skeptics like Dyakonov and Kalai are knowingly putting their countries at risk. But waging war on the U.S. quantum industry can have serious consequences, unless quantum companies and labs show that they are not intimidated, and reassure the public that the quantum future doesn’t rest on hype but significant achievements—achievements that will make our country and our world safer, stronger, and more confident about our future as a whole.”</p>
<p>4) I changed the title to reflect the main topic of the post.</p>
<p><strong>Update 2 (June 6, 2022):</strong></p>
<h3>When (and if) is the right time to transfer to post quantum cryptography?</h3>
<p>Here is my uneducated recommendation (which is separate to the best of my ability from my overall quantum computing skepticism).  Here I take it as an assumption that the aim is to maximize communication security, and the reason for transferring to “post-quantum” cryptographic protocols is that large scale quantum computers will enable breaking most of current cryptosystems. Note that pressure to transfer earlier rather than later to new protocols and standards may reflect commercial or other interests and not towards the objective of maximizing communication security. (I take no view on these other interests.)</p>
<p>The efforts to build post quantum cryptography are intellectually interesting: I am thrilled to see Oded Regev’s LWE (learning with errors) and Ajtai-Dwork lattice-based cryptography getting used. I certainly support to invest (large amount of resources) in developing post-quantum cryptography.  The question is with transferring to new protocols and my recommendation is:</p>
<blockquote><p><em><strong>Wait in implementing new encryption standards based on post-quantum cryptography until DiVincenzo’s stage 4 is firmly established.</strong> </em></p></blockquote>
<p>(For example, until distance-5 surface codes are built.)</p>
<p>Note that moving forward from good quality quantum error correcting codes (like distance-5 surface codes) to very good quality quantum error-orrecting codes needed for quantum fault tolerance (like distance 11-surface codes), implementing logical quantum gates, and later on implementing fault-tolerance is likely to be a slow process that may take quite a few decades.</p>
<p>The crucial thing to consider is that transferring to new cryptographic methods is <strong>by itself</strong> a serious communication security risk even (in fact, especially) when it comes to classical attacks. Giving ample time to check new suggested protocols somewhat reduces this risk.</p>
<p><strong><br />
Update 1 (June 1, 2022):</strong> Very relevant to the discussion above: Yosi Avron told me about a very recent breakthrough for distance-3 surface code that is reported in the paper <a href="https://www.nature.com/articles/s41586-022-04566-8">Realizing repeated quantum error correction in a distance-three surface code</a>, by S. Krinner et al. (Here is a link to the <a href="https://arxiv.org/abs/2112.03708">arXive version</a>, the researchers are from ETH, Jülich center, Quebec and other places.) Michael Rothschild (and others) told me about this recent breakthrough by a team led by Thomas Monz from the University of Innsbruck and Markus Müller from Aachen University and Forschungszentrum Jülich in Germany for creating entangled logical qubits. The paper is <a href="https://www.nature.com/articles/s41586-020-03079-6">Entangling logical qubits with lattice surgery</a> by Erhard et al, see <a href="https://phys.org/news/2021-01-error-protected-quantum-bits-entangled.html">also here</a>.</p>
<p>Also, now that Scott Aaronson<a href="https://scottaaronson.blog/?p=6444"> outsourced this very post of mine</a> for quantum computing commentary, I will end with a quote of Scott taken from the interesting <a href="https://www.facebook.com/gil.kalai.1/posts/10160341938448792">Facebook thread</a> related to this post.  These days, Scott is busy dealing with deplorable rude remarks and attacks over the Internet pointed at him.  In my view, and this brings us back also to Herman’s piece, belligerent attacks are not appropriate in general and certainly are not constructive in academic discussions.</p>
<p>Scott commented that he chose to “sit this one out for now” regarding IonQ, and I asked him if he is still a believer in Google/Sycamore.  Here is Scott’s response (that I find rather reasonable; see further discussion between us in the thread).</p>
<div class="ecm0bbzt e5nlhep0 a8c37x1j">
<div class="kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql">
<blockquote>
<div dir="auto"><em><a href="https://www.facebook.com/scott.aaronson.5?comment_id=Y29tbWVudDoxMDE2MDM0MTkzODQ0ODc5Ml82OTc0Njc5NDQ4NjQ0MTU%3D&amp;__cft__[0]=AZUfBz2MAd0mh1X24AhBJr_wm6iSF2nHdpJEj8rQ5Opb4g7kXxrvAxd_4OpkP-8wO3xmSRYc9L3-Z955nxPad4C44wP3-3YulwpWJKfn3s2uk8uVx0Dr17qF_XCUmcCHb4M&amp;__tn__=R]-R">Scott Aaronson</a>[:] I think it [Google/Sycamore] represented a huge advance in scaling up and benchmarking NISQ devices. I think it showed the major result that the circuit fidelity scaled simply like the gate fidelity to the number of gates, and if that continues to be the case, then contrary to what you say, fault-tolerance will ultimately work. I also think that tensor network and other methods have gotten better at spoofing Sycamore’s Linear XEB score classically, nearly wiping out the quantum advantage as measured by time, though a significant quantum advantage remains as measured by floating-point ops or energy expenditure for the same LXEB score. We knew that quantum supremacy would be a moving target; recent progress underscores the need for better gate fidelities (which, fortunately, seem to be happening anyway) to stay ahead of classical computing on these benchmarks.</em></div>
<div dir="auto"></div>
</blockquote>
</div>
</div></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/"><span class="datestr">at May 26, 2022 09:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/080">TR22-080 |  QBF Merge Resolution is powerful but unnatural | 

	Meena Mahajan, 

	Gaurav Sood</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Merge Resolution proof system (M-Res) for QBFs, proposed by Beyersdorff et al. in 2019, explicitly builds partial strategies inside refutations. The original motivation for this approach was to overcome the limitations encountered in long-distance Q-Resolution proof system (LD-Q-Res), where the syntactic side-conditions, while prohibiting all unsound resolutions, also end up prohibiting some sound resolutions. However, while the advantage of M-Res over many other resolution-based QBF proof systems was already demonstrated, a comparison with LD-Q-Res itself had remained open. In this paper, we settle this question. We show that M-Res has an exponential advantage over not only LD-Q-Res, but even over LQU$^+$-Res and IRM, the most powerful among currently known resolution-based QBF proof systems. Combining this with results from Beyersdorff et al. 2020, we conclude that M-Res is incomparable with LQU-Res and LQU$^+$-Res.

Our proof method reveals two additional and curious features about M-Res: (i) MRes is not closed under restrictions, and is hence not a natural proof system, and (ii) weakening axiom clauses with existential variables provably yields an exponential advantage over M-Res without weakening. We further show that in the context of regular derivations, weakening axiom clauses with universal variables provably yields an exponential advantage over M-Res without weakening. These results suggest that MRes is better used with weakening, though whether M-Res with weakening is closed under restrictions remains open. We note that even with weakening, M-Res continues to be simulated by eFrege $+ $ $\forall$red (the simulation of ordinary M-Res was shown recently by Chew and Slivovsky).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/080"><span class="datestr">at May 26, 2022 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/05/25/tcs-talk-wednesday-june-1-inbal-talgam-cohen-technion-israel-institute-of-technology/">TCS+ talk: Wednesday, June 1 — Inbal Talgam-Cohen, Technion – Israel Institute of Technology</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p></p>



<p></p>


<p>The next TCS+ talk will take place this coming Wednesday, June 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://www.inbaltalgam.com/"><strong>Inbal Talgam-Cohen</strong></a> from the Technion – Israel Institute of Technology will speak about “<em>Algorithmic contract theory</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote">
<p>Abstract: Algorithms increasingly interact with strategic, self-interested players; their design must take player incentives into account or risk being “gamed” and failing miserably. The algorithmic game theory literature traditionally focused on “mechanisms” – algorithms that incentivize players to truthfully report the <strong>input</strong>. In this talk we shift focus from mechanisms to “contracts”, which are concerned with the algorithm’s <strong>output</strong> and players’ incentives to carry it out. The goal of this talk is to describe where we’re at in forming a new algorithmic theory of contract design.</p>
<p>I will demonstrate how algorithmic approaches – in particular the approach of <strong>beyond worst-case analysis</strong> – can shed light on a classic economic puzzle regarding simple contracts. Within the realm of <strong>incentives and learning</strong>, I will discuss how classifiers induce incentives and show a formal relation to contracts.</p>
<p>Based on joint works with Tal Alon, Magdalen Dobson, Paul Duetting, Ron Lavi, Ariel Procaccia, Tim Roughgarden, Elisheva Shamash and Jamie Tucker-Foltz.</p>
</blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/05/25/tcs-talk-wednesday-june-1-inbal-talgam-cohen-technion-israel-institute-of-technology/"><span class="datestr">at May 25, 2022 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/079">TR22-079 |  Lower Bound Methods for Sign-rank and their Limitations | 

	Hamed Hatami, 

	Pooya Hatami, 

	William Pires, 

	Ran Tao, 

	Rosie Zhao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The sign-rank of a matrix $A$ with $\pm 1$ entries is the smallest rank of a real matrix with the same sign pattern as $A$. To the best of our knowledge, there are only three known methods for proving lower bounds on the sign-rank of explicit matrices:  (i) Sign-rank is at least the VC-dimension; (ii) Forster's method, which states that sign-rank is at least the inverse of the largest possible average margin among the representations of the matrix by points and half-spaces; (iii) Sign-rank is at least a logarithmic function of the density of the largest monochromatic rectangle. 
 

We prove several results regarding the limitations of these methods. 


$\bullet$ We prove that, qualitatively,  the monochromatic rectangle density is the strongest of these three lower bounds. If it fails to provide a super-constant lower bound for the sign-rank of a  matrix, then the other two methods will fail as well.   
$\bullet$ We show that there exist $n \times n$ matrices with sign-rank $n^{\Omega(1)}$ for which none of these methods can provide a super-constant lower bound.   
$\bullet$ We show that sign-rank is at most an exponential function of the deterministic communication complexity with access to an equality oracle. We combine this result with Green and Sanders' quantitative version of  Cohen's idempotent theorem to show that for a large class of sign matrices (e.g., XOR-lifts), sign-rank is at most an exponential function of the $\gamma_2$ norm of the matrix. We conjecture that this holds for all sign matrices. 
$\bullet$ Towards answering a question of Linial,  Mendelson,   Schechtman, and  Shraibman regarding the relation between sign-rank and discrepancy, we conjecture that sign-ranks of the $\pm1$ adjacency matrices of hypercube graphs can be arbitrarily large. We prove that none of the three lower bound techniques can resolve this conjecture in the affirmative.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/079"><span class="datestr">at May 25, 2022 08:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/078">TR22-078 |  Improved local testing for multiplicity codes | 

	Dan Karliner, 

	Amnon Ta-Shma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Multiplicity codes are a generalization of Reed-Muller codes which include derivatives as well as the values of low degree polynomials, evaluated in every point in $\mathbb{F}_p^m$. 
Similarly to Reed-Muller codes, multiplicity codes have a local nature that allows for local correction and local testing. 
Recently, the authors and Salama showed that the plane test, which tests the degree of the codeword on a random plane, is a good local tester for small enough degrees.
In this work we simplify and extend the analysis of local testing for multiplicity codes, giving a more general and tighter analysis. In particular, we show that multiplicity codes $\mathrm{MRM}_{p}(m, d, s)$ over prime fields with arbitrary $d$ are locally testable by an appropriate $k$-flat test, which tests the degree of the codeword on a random $k$-dimensional affine subspace. The relationship between the degree parameter $d$ and the required dimension $k$ is shown to be nearly optimal, and improves on the degree previously known in the case of planes.

Our analysis relies on a generalization of the technique of canonincal monomials introduced by Haramaty, Shpilka and Sudan (FOCS 2011). Generalizing canonical monomials to the multiplicity case requires substantially different proofs which exploit the algebraic structure of multiplicity codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/078"><span class="datestr">at May 25, 2022 03:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/077">TR22-077 |  Explicit Lower Bounds Against $\Omega(n)$-Rounds of Sum-of-Squares | 

	Max Hopkins, 

	Ting-Chun Lin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We construct an explicit family of 3-XOR instances hard for $\Omega(n)$-levels of the Sum-of-Squares (SoS) semi-definite programming hierarchy. Not only is this the first explicit construction to beat brute force search (beyond low-order improvements (Tulsiani 2021, Pratt 2021)), combined with standard gap amplification techniques it also matches the (optimal) hardness of random instances up to imperfect completeness (Grigoriev TCS 2001, Schoenebeck FOCS 2008). 

Our result is based on a new form of small-set high dimensional expansion (SS-HDX) inspired by recent breakthroughs in locally testable and quantum LDPC codes. Adapting the recent framework of Dinur, Filmus, Harsha, and Tulsiani (ITCS 2021) for SoS lower bounds from the Ramanujan complex to this setting, we show any (bounded-degree) SS-HDX can be transformed into a highly unsatisfiable 3-XOR instance that cannot be refuted by $\Omega(n)$-levels of SoS. We then show Leverrier and Z\'emor's (Arxiv 2022) recent qLDPC construction gives the desired explicit family of bounded-degree SS-HDX. Incidentally, this gives the strongest known form of bi-directional high dimensional expansion to date.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/077"><span class="datestr">at May 25, 2022 03:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/privacy-doona/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/privacy-doona/">Privacy Doona&amp;#58; Why We Should Hide Among The Clones</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this blog post, we will discuss a recent(ish) result of Feldman, McMillan, and Talwar <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, which provides an improved and simple analysis of the so-called “amplification by shuffling” formally connecting local privacy (LDP) and shuffle privacy.<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup> Now, I’ll assume the reader is familiar with both LDP and Shuffle DP: if not, a quick-and-dirty refresher (with less quick, and less dirty references) can be found <a href="https://differentialprivacy.org/\trustmodels">here</a>, and of course there is also Albert Cheu’s excellent survey on Shuffle DP <a href="https://arxiv.org/abs/2107.11839" title="Albert Cheu. Differential Privacy in the Shuffle Model: A Survey of Separations. arXiv 2021"><strong>[Cheu21]</strong></a>.</p>

<p>I will also ignore most of the historical details, but it is worth mentioning that <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> is not the first paper on this “amplification by shuffling,” (which, for local reasons, I’ll just call a <em>privacy <a href="https://www.collinsdictionary.com/dictionary/english/doona">doona</a></em>) but rather is the culmination of a rather long line of work involving many cool ideas and papers, starting with <a href="https://arxiv.org/abs/1808.01394" title="Albert Cheu, Adam D. Smith, Jonathan Ullman, David Zeber, Maxim Zhilyaev. Distributed Differential Privacy via Shuffling. EUROCRYPT 2019"><strong>[CSUZZ19</strong></a>, <a href="https://arxiv.org/abs/1811.12469" title="Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, Abhradeep Thakurta. Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity. SODA 2019"><strong>EFMRTT19]</strong></a>: I’d refer the reader to <strong>Table 1</strong> in <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> for an overview.</p>

<p>Alright, now that the caveats are behind us, what <em>is</em> “amplification by shuffling”? In a nutshell, it is capturing the (false!) intuition that “anonymization provides privacy” (which, again, is false! Don’t do this!) and making it… less false. The idea is that while <em>anonymization does not provide in itself any meaningful privacy guarantee</em>, it can <em>amplify existing, rigorous privacy guarantee</em>. So if I start with a somewhat lousy LDP guarantee, but then all the messages sent by all users are completely anonymized, then my lousy LDP guarantee suddenly gets <em>much</em> stronger (roughly speaking, the \(\varepsilon\) parameter goes down with the square root of of the number of users involved). Which is wonderful! Let’s see what this means, quantitatively.</p>

<h3 id="the-result-of-feldman-mcmillan-and-talwar">The result of Feldman, McMillan, and Talwar.</h3>
<p>Here, we will focus on the simpler case of <em>noninteractive</em> protocols (one-shot messages from the users to the central server, no funny business with messages going back and forth); which is conceptually simpler to state and parse, still very rich and interesting, and, well, very relevant in practice (being the easiest and cheapest to deploy). If you want the results in their full glorious generality, though, they are in the paper.</p>

<p>What the main theorem of <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> is saying for this noninteractive setting can then be stated as follows: if I have an <em>\(\varepsilon_L\)-locally private</em> (LDP) protocol for a task, where all \(n\) users pass their data through the same randomizer (algorithm) \(R\) and send the resulting message \(y_i \gets R(x_i)\), then just permuting the messages \(y_1\dots,y_n\) immediately gives an \((\varepsilon,\delta)\)-<em>shuffle</em> private protocol for the same task, for any pair \((\varepsilon,\delta)\) which satisfies
<a name="eq:eps:epsL"></a>
\begin{equation}
		\varepsilon \leq \log\left( 1+ 16\frac{e^{\varepsilon_{L}}-1}{e^{\varepsilon_{L}}+1}\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{1}
\end{equation}
as long as \(n \gg e^{\varepsilon_{L}}\log(1/\delta)\). That is quite a lot to parse, though: what does this actually <em>mean</em>?</p>

<p><strong>First</strong>, the assumption that all users have the same randomizer (or at least cannot be distinguished by their randomizer) is quite natural: if they didn’t, then we wouldn’t be able to say anything in general, since the randomizer they use could just give away their identity completely. For instance, as an extreme case, the randomizer of user \(i\) could just append \(i\) to the message (it’s OK, still LDP!), and then shuffling achieves exactly nothing: we know who sent what. So OK, asking for all randomizers to be the same is not really a restriction.</p>

<p><strong>Second</strong>, each user only sends one message, and this preserves its length (we just shuffled the messages, didn’t modify them!). So if you start with an LDP protocol with amazing features XYZ (e.g., the messages are \(1\)-bit long, or users don’t share a random seed, or the randomizers run in time \(O(1)\)), then the shuffle protocol enjoys exactly the same properties. (It only enjoys naturally some <em>robustness</em>, in the sense that if \(10\%\) if the \(n\) users maliciously deviate from the protocol, they can’t really jeopardize the privacy of the remaining \(90\%\) of users.<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote" rel="footnote">2</a></sup> Which is… good.)</p>

<p><strong>Third</strong>, this is inherently approximate DP. Here we started with pure LDP (you can also extend that to approximate LDP) and ended up with approximate Shuffle DP: this is not a mistake, that’s how it is. I am not a purist (erm) myself, and that looks more than good enough to me; but if you seek pure Shuffle DP, then this result is not the droid you’re looking for.</p>

<p align="center">
  <img width="50%" alt="This is not the pure DP guarantee you are looking for." src="https://differentialprivacy.org/../images/droids-looking.png" />
</p>
<p><br /></p>

<p>Alright, <em>what</em> is this guarantee stated in <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a> giving us? Let’s interpret the expression in <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a> in two parameter regimes, focusing on \(\varepsilon\) (fixing some small \(\delta&gt;0\)). If we start with \(\varepsilon_{L} \ll 1\) for our LDP randomizers \(R\), then a first-order Taylor expansion shows that we get
\[
		\varepsilon \approx \varepsilon_{L}\cdot 8\sqrt{\frac{\log\frac{4}{\delta}}{n}}
\]
so that <em>shuffling improved our privacy parameter by a factor \(\sqrt{n}\)</em>.<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote" rel="footnote">3</a></sup> 😲 This is great! With more users, comes more privacy!</p>

<p>But that was starting with small \(\varepsilon_{L}\), that is, already pretty good privacy guarantees for our LDP “building block” \(R\). What happens if we start with “somewhat lousy” privacy guarantees, that is, \(\varepsilon_{L} \gg 1\)? Do we get anything interesting then?
Another Taylor expansion (everything is a Taylor expansion) shows us that, then,
<a name="eq:epsL:ll:one"></a>
\[
\begin{equation}
		\varepsilon \approx \log\left( 1+ 8\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{2}
\end{equation}
\]
or, put differently,
<a name="eq:epsL:gg:one"></a>
\[
\begin{equation}
		\varepsilon \approx 8e^{\varepsilon_{L}/2}\sqrt{\frac{\log\frac{4}{\delta}}{n}} \tag{3}
\end{equation}
\]
That’s a bit harder to interpret, but that seems… useful? It is: let us see how much, with a couple examples.</p>

<h4 id="learning">Learning.</h4>
<p>The first one is distribution learning, a.k.a. density estimation: you have \(n\) i.i.d. samples (one per user) from an unknown probability distribution \(\mathbf{p}\) over a discrete domain of size \(k\), and your goal is to output an estimate \(\widehat{\mathbf{p}}\) such that, with high (say, constant) probability, \(\mathbf{p}\) and \(\widehat{\mathbf{p}}\) are close in <em>total variation distance</em>:
\[
		\operatorname{TV}(\mathbf{p},\widehat{\mathbf{p}}) = \sup_{S\subseteq [k]} (\mathbf{p}(S) - \widehat{\mathbf{p}}(S) ) \leq \alpha
\]
(if total variation distance seems a bit mysterious, it’s exactly half the \(\ell_1\) distance between the probability mass functions). We know how to solve this problem in the non-private setting: \(n=\Theta\left( \frac{k}{\alpha^2} \right)\) samples are necessary and sufficient. We know how to solve this problem in the (central) DP setting: \(n=\Theta\left( \frac{k}{\alpha^2} + \frac{k}{\alpha\varepsilon} \right)\) samples are necessary and sufficient <a href="https://proceedings.neurips.cc/paper/2015/hash/2b3bf3eee2475e03885a110e9acaab61-Abstract.html" title="Ilias Diakonikolas, Moritz Hardt, Ludwig Schmidt. Differentially Private Learning of Structured Discrete Distributions. NeurIPS 2015"><strong>[DHS15]</strong></a>. We know how to solve this problem in the LDP setting: 
<a name="eq:learning:ldp"></a>
\begin{equation}
n=\Theta\left(\frac{k^2}{\alpha^2(e^\varepsilon-1)^2}+\frac{k^2}{\alpha^2e^\varepsilon}+\frac{k}{\alpha^2}\right) \tag{4}
\end{equation}
samples are necessary and sufficient <a href="http://proceedings.mlr.press/v89/acharya19a.html" title="Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication. AISTATS 2019"><strong>[ASZ19]</strong></a> (note that the first term is just \(k/(\alpha^2\varepsilon^2)\) for small \(\varepsilon\)). Now, as they say in Mulan: <em>let’s make a shuffle DP algo out of you.</em></p>

<p>If we want to achieve \((\varepsilon,\delta)\)-shuffle DP, we need to select \(\varepsilon_L\). Based on <a href="https://differentialprivacy.org/feed.xml#eq:epsL:ll:one">(2)</a> and <a href="https://differentialprivacy.org/feed.xml#eq:epsL:gg:one">(3)</a>, and ignoring pesky constants we will choose it so that
<a name="eq:choice:epsL"></a>
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,. \tag{5}
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging that back in <a href="https://differentialprivacy.org/feed.xml#eq:learning:ldp">(4)</a>, we see that the first case corresponds to the first term (small \(\varepsilon_{L}\)) and the second to the second term (\(\varepsilon_{L} \geq 1\)), and overall the condition on \(n\) for the original LDP algorithm to 
successful learn the distribution becomes
\[
	n \gtrsim 
	\frac{k^2}{\alpha^2(e^{\varepsilon_{L}}-1)^2}+\frac{k^2}{\alpha^2e^{\varepsilon_{L}}}+\frac{k}{\alpha^2}
	\approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2} 
	 \approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2}
\]
(where \(\gtrsim\) means “let’s ignore constants”). There is an \(n\) in the RHS as well, so reorganizing and handling the two terms separately the condition on \(n\) becomes
\[
	n \gtrsim \frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}+\frac{k}{\alpha^2}
\]
which… is great? We immediately get a sample complexity \(O\left(\frac{k}{\alpha^2}+\frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}\right)\) in the shuffle DP model, which (ignoring the \(\sqrt{\log(1/\delta)}\)) matches the one in the <em>central</em> DP setting!</p>

<p><strong>tl;dr:</strong> Taking an optimal LDP algorithm and just shuffling the messages <em>immediately</em> gives an optimal shuffle DP algorithm, no extra work needed.</p>

<h4 id="uniformity-testing">(Uniformity) Testing.</h4>
<p>Alright, maybe it was a fluke? Let’s look at another “basic” problem close to my heart: we don’t want to learn the probability distribution \(\mathbf{p}\), just test whether it is actually <em>the</em> uniform distribution<sup id="fnref:4"><a href="https://differentialprivacy.org/feed.xml#fn:4" class="footnote" rel="footnote">4</a></sup> \(\mathbf{u}\) on the domain \([k]={1,2,\dots,k}\). So if \(\mathbf{p} =\mathbf{u}\), you’ve got to say “yes” with probability at least \(2/3\), and if \(\operatorname{TV}(\mathbf{p},\mathbf{u})&gt;\alpha\), then you need to say “no” with probability at least \(2/3\).</p>

<p>This is also well understood in the non-private setting (\(n=\Theta(\sqrt{k}/\alpha^2)\)) <a href="https://ieeexplore.ieee.org/document/4626074" title="Liam Paninski. A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data. IEEE Trans. Inf. Theory 2008"><strong>[Paninski08]</strong></a> [see also <a href="https://ccanonne.github.io/survey-topics-dt.html}{my upcoming survey">my upcoming survey</a>], in the central DP setting (\(n=\Theta\left( \frac{\sqrt{k}}{\alpha^2} + \frac{\sqrt{k}}{\alpha\sqrt{\varepsilon}}+\frac{k^{1/3}}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{1}{\alpha\varepsilon} \right)\)) <a href="https://arxiv.org/abs/1707.05128" title="Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Differentially Private Testing of Identity and Closeness of Discrete Distributions. NeurIPS 2018"><strong>[ASZ18</strong></a>, <a href="https://arxiv.org/abs/1707.05497" title="Maryam Aliakbarpour, Ilias Diakonikolas, Ronitt Rubinfeld. Differentially Private Identity and Equivalence Testing of Discrete Distributions. ICML 2018"><strong>ADR18]</strong></a>, and in the LDP setting, where the result differs on whether the users can communicate or share a common random seed 
<a name="eq:testing:ldp:publiccoin"></a>
\begin{equation}
	n=\Theta\left( \frac{k}{\alpha^2(e^\varepsilon-1)^2} + \frac{k}{\alpha^2e^{\varepsilon/2}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{6}
\end{equation} or not
<a name="eq:testing:ldp:privatecoin"></a> 
\begin{equation}
	n=\Theta\left( \frac{k^{3/2}}{\alpha^2(e^\varepsilon-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{7}
\end{equation}
as established in a sequence of papers <a href="https://arxiv.org/abs/1812.11476" title="Inference under Information Constraints: Lower Bounds from Chi-Square Contraction. COLT 2019"><strong>[ACT19</strong></a>, <a href="https://arxiv.org/abs/1911.01452" title="Kareem Amin, Matthew Joseph, Jieming Mao. Pan-Private Uniformity Testing. COLT 2020"><strong>AJM20</strong></a>, <a href="https://arxiv.org/abs/2101.07981" title="Jayadev Acharya, Clément L. Canonne, Cody Freitag, Ziteng Sun, Himanshu Tyagi.  Inference Under Information Constraints III: Local Privacy Constraints. IEEE J. Sel. Areas Inf. Theory 2021"><strong>ACFST21</strong></a>, <a href="https://arxiv.org/abs/2007.10976" title="Jayadev Acharya, Clément L. Canonne, Yuhan Liu, Ziteng Sun, Himanshu Tyagi. Interactive Inference Under Information Constraints. IEEE Trans. Inf. Theory 2022"><strong>ACLST22</strong></a>, <a href="https://arxiv.org/abs/2108.08987" title="Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022"><strong>CL22]</strong></a>.</p>

<p>Now, say you want an (\(\varepsilon,\delta)\)-shuffle DP algorithm for uniformity testing, but don’t want to design one from scratch (though it <em>is</em> possible to do so, and some did <a href="https://arxiv.org/abs/2004.09481" title="Victor Balcer, Albert Cheu, Matthew Joseph, Jieming Mao. Connecting Robust Shuffle Privacy and Pan-Privacy. SODA 2021"><strong>[BCJM21</strong></a>, <a href="https://arxiv.org/abs/2108.08987" title="Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022"><strong>CL22</strong></a>, <a href="https://arxiv.org/abs/2112.10032" title="Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021"><strong>CY21]</strong></a>). Let’s say you want to look at the “no-common-random-seed-shared-by-users” model (a.k.a. <em>private-coin</em> setting): so you stare at the corresponding LDP communication complexity, <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:privatecoin">(7)</a>, and try to choose \(\varepsilon_L\) to start with before shuffling. This will be the same as in the learning example (i.e., <a href="https://differentialprivacy.org/feed.xml#eq:choice:epsL">(5)</a>): based on <a href="https://differentialprivacy.org/feed.xml#eq:epsL:ll:one">(2)</a> and <a href="https://differentialprivacy.org/feed.xml#eq:epsL:gg:one">(3)</a>, we will set
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,.
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging this back in <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:privatecoin">(7)</a> and quickly checking which case corresponds to each term, we then easily get that for our algorithm to correctly solve the uniformity testing problem, it suffices that the sample complexity (number of users) \(n\) satisfies
\[
	n \gtrsim \frac{k^{3/2}}{\alpha^2(e^{\varepsilon_L}-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon_L}} + \frac{\sqrt{k}}{\alpha^2}
	 \approx \frac{k^{3/2}\log(1/\delta)}{\alpha^2\varepsilon^2 n } + \frac{\sqrt{k}}{\alpha^2}
\]
which, reorganizing and solving for \(n\), means that it suffices to have 
\[
	n \gtrsim \frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon} + \frac{\sqrt{k}}{\alpha^2}\,.
\]
And, <em>voilà</em>! Even better, we also have strong evidence to suspect that this sample complexity \(O\Big(\frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)\) is tight among all private-coin algorithms.<sup id="fnref:5"><a href="https://differentialprivacy.org/feed.xml#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>Now, if you wanted to look at <em>public-coin</em> shuffle DP protocols (with a common random seed available), then you would start with an optimal public-coin LDP algorithm (and look at <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:publiccoin">(6)</a>), and setting \(\varepsilon_L\) the same way you’d get a shuffle DP algorithm with sample complexity
\[
n=O\Big(\frac{k^{2/3}\log^{1/3}(1/\delta)}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{\sqrt{k\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)
\]
which, well, is <em>also</em> strongly believed to be optimal!</p>

<p><strong>tl;dr:</strong> Here again, taking an optimal off-the-shelf LDP algorithm and just shuffling the messages <em>immediately</em> gives an optimal shuffle DP algorithm, no extra work needed.</p>

<h3 id="conclusion">Conclusion.</h3>
<p>I hope the above convinced you of how useful this privacy amplification can be: from an optimal LDP algorithm, featuring any extra appealing characteristics you like, <em>just adding an extra shuffling step as postprocessing</em> yields an (often optimal? At least good) shuffle DP algorithm, <em>with the same characteristics</em> and built-in robustness against malicious users.</p>

<p>All you need is to make sure that your starting point, the LDP algorithm satisfies a couple things: (1) all users have the same randomizer,<sup id="fnref:6"><a href="https://differentialprivacy.org/feed.xml#fn:6" class="footnote" rel="footnote">6</a></sup> and (2) it works in all regimes of \(\varepsilon\) (both high-privacy, \(\varepsilon \leq 1\), <em>and</em> low-privacy, \(\varepsilon \gg 1\)). Once you’ve got this, Bob’s your uncle! You get shuffle DP algorithms for free.</p>

<p>It is not only appealing from a theoretical point of view, by the way! The authors of the paper worked hard to make their empirical analysis compelling as well, and their code is available <a href="https://github.com/apple/ml-shuffling-amplification">on GitHub</a> 📝. But more importantly, from a practitioner’s point of view, this means it is enough to design, implement, and test <em>one</em> algorithm (the LDP one we start with) to automatically get a trusted one in the shuffle DP model as well: this reduces the risks of bugs, security failures, the amount of work spending tuning, testing…</p>

<p>So yes, whenever possible, we <em>should</em> hide among the clones!</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The title of this post is a reference to the title of <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, “Hiding Among The Clones,” and to the notion of <em>privacy blanket</em> introduced by Balle, Bell, Gascón, and Nissim <a href="https://link.springer.com/chapter/10.1007/978-3-030-26951-7_22" title="Borja Balle, James Bell, Adrià Gascón, Kobbi Nissim. The Privacy Blanket of the Shuffle Model. CRYPTO 2019"><strong>[BBGN19]</strong></a>. Intuitively, the “amplification by shuffling” paradigm can be seen as anonymizing the messages from local randomizers, whose message distribution can be mathematically decomposed as a mixture of “noise distribution not depending on the user’s input” and “distribution actually depending on their input.” As a result, each user randomly sends a message from the first or second distribution of the mixture.  But the shuffling then hides the informative messages (drawn from the second part of the mixture) among the non-informative (noise) ones: so the noise messages end up providing a “privacy blanket” in which sensitive information is safely and soundly wrapped. <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>More specifically, they can completely jeopardize the <em>utility</em> (accuracy) of the result, but in terms of privacy, all they can do is slightly reduce it: if \(10\%\) of users are malicious, the remaining \(90\%\) still get the privacy amplification of guarantee of <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a>, but with \(0.9n\) instead of \(n\). <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Of course, we started with a local privacy guarantee, and ended up with a shuffle privacy guarantee: so the two are incomparable, and one has to interpret this “amplification” in that context. <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>You can here replace uniform by any known distribution \(\mathbf{q}\) of your choosing, that doesn’t change the question (and result), but uniform is nice. <a href="https://differentialprivacy.org/feed.xml#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>As long as one is happy with approximate DP. One can achieve that in pure DP as well, but it’s a bit more complicated <a href="https://arxiv.org/abs/2112.10032" title="Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021"><strong>[CY21]</strong></a>. <a href="https://differentialprivacy.org/feed.xml#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>This is not such a big assumption usually, and there are somewhat-general ways to get to that using a logarithmic factor in the number of users. <a href="https://differentialprivacy.org/feed.xml#fnref:6" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Clément Canonne <a href="https://differentialprivacy.org/privacy-doona/"><span class="datestr">at May 24, 2022 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/">IDEAL Workshop on High-Dimensional Geometry and Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 27, 2022 Northwestern University Mudd 3514 https://www.ideal.northwestern.edu/events/high-dimensional-analysis/ Speakers Ainesh Bakshi (Carnegie Mellon University), Arnold Filtser (Bar Ilan University), Weiyun Ma (Stanford University), Assaf Naor (Princeton University), Erik Waingarten (Stanford University) Logistics Dates: Friday, May 27 Location: Northwestern University Rooms: Mudd 3514 Streaming: Zoom Registration Link on website- https://www.ideal.northwestern.edu/events/high-dimensional-analysis/ PRELIMINARY Schedule All times are in … <a href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on High-Dimensional Geometry and Analysis</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/"><span class="datestr">at May 23, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
