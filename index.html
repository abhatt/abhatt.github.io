<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="http://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 11, 2021 11:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4680987113771618016">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/i-have-book-out-on-muffins-you-prob.html">I have a book out on muffins (you prob already know that)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i>Lance</i>: How come you haven't blogged on your muffin book? You've blogged about two books by Harry Lewis (see <a href="https://blog.computationalcomplexity.org/2021/10/how-have-computers-changed-society.html">here</a> and <a href="https://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html">here</a>) one book by the lesswrong community (see <a href="https://blog.computationalcomplexity.org/2021/09/review-of-blog-book-based-on-less-wrong.html">here</a>), and you even did a mashup of a post by two different Scott A's (see <a href="https://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html">here</a>),  but not on your own work.</p><p><i>Bill</i>: I thought I did a post on my muffin book.</p><p><i>Lance</i>: No. You have blogged <i>about </i>the muffin problem, and sometimes you <i>mention</i> either the book or the problem in passing, but you haven't had a post that says</p><p><i><b>HEY, I wrote a book!</b></i></p><p>And this is all the more strange since you asked me to have the book on our blog page. </p><p><i>Bill: </i>(Searches blog with keyword muffin and finds no ref to muffin book). Well pierce my ears and call be drafty! I have not posted on the muffin book! Do you recall my thoughts on when to tell people you are working on a book?</p><p><i>Lance</i>: No</p><p><i>Bill</i>:  I had a college roommate who was an aspiring science fiction writer who told me there are two kinds of people: Those who talk about writing a book, and those who write a book. I have adapted this to:</p><p><i><b>Do not tell people you are writing a book until you are picking out the cover art.</b></i></p><p><i>Lance: </i>I posted about my book when I hadn't even decided on <a href="https://blog.computationalcomplexity.org/2012/06/name-my-book.html">the title</a>. But your cover art is picked out (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>).  And, by the way, its very nice, though it makes me hungry. So I think you can begin talking about the book.</p><p><i>Bill</i>: Indeed! I will!</p><p>------------------------------------------------------------------------------------</p><p><b>Hey I have a book! (See <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a> to buy it on amazon.) </b></p><p><b>Title: Mathematical Muffin Morsels: Nobody Wants a Small Piece</b></p><p><b>by Gasarch, Metz, Prinz, Smolyak</b></p><p><b>(The other authors were undergraduates when we wrote the book. Prinz and Smolyak are now grad students in CS, Metz is in Finance.) </b></p><p><b>Origin: </b></p><p><a href="https://en.wikipedia.org/wiki/Martin_Gardner">Martin Gardner</a> wrote a Mathematics Recreational column for Scientific American for many years, starting in 1956 and ending in the early 1980s. For many STEM people of my generation (Using my <a href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html">fake birthday</a> of Oct 1, 1960, I am 62 years old) Martin Gardner's columns were both an inspiration and an early exposure to mathematics. His columns also made the line between Mathematical Recreation and so-called serious mathematics thin or nonexistent. (See <a href="http://www.cs.umd.edu/~gasarch/bookrev/FRED/gardner.pdf">here</a> for a review of Martin Gardner in the 21st century, a book about the kind of math Gardner wrote of. The book makes a mockery of the distinction between recreational and serious mathematics.) He passed away in 2010 at the age of 95.</p><p>There is a gathering in his honor that is hold roughly every 2 years, called <a href="https://www.gathering4gardner.org/">Gathering For Gardner</a>. (It was cancelled in Spring 2020 and Spring 2021 because of COVID- though its in Atlanta where the CDC is, so they could have had it as an experiment and told the CDC the results). You have to be invited to goto it. I got an invite for 2016 from my contact at World Scientific who published my previous book, <i>Problems with a Point: Exploring Math and Computer Science co-authored with Clyde Kruskal </i> (I had two blogs on it, <a href="https://blog.computationalcomplexity.org/2019/02/problems-with-point-exploring-math-and.html">here</a> and <a href="https://blog.computationalcomplexity.org/2019/04/problems-with-point-not-plug-just-some.html">here</a>, and you can buy it on amazon <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279974">here</a>.) I did three posts on G4G-2016 (<a href="https://blog.computationalcomplexity.org/2016/04/some-short-bits-from-gathering-for.html">here</a>, <a href="https://blog.computationalcomplexity.org/2016/05/some-more-bits-from-gathering-for.html">here</a>, and <a href="https://blog.computationalcomplexity.org/search?q=Gathering">here</a>).</p><p>Aside from seeing some great talks that I understood and liked, I also picked up a pamphlet titled:</p><p><b>The Julia Robinson Math Festival</b></p><p><b>A Sample of Mathematical Puzzles</b></p><p><b>Compiled By Nancy Blackman</b></p><p>One of the problems, credited to Alan Frank, was</p><p>How can you divide and distribute 5 muffins for 3 students so that everyone gets 5/3 and the smallest piece is as big as possible?</p><p>They had some other values for muffins and students as well. </p><p>I solved the (5,3) problem and the other ones as well. That was fun. </p><p>When I got home I began looking at the problem for m muffins and s students. I let f(m,s) be the biggest smallest piece possible for giving out m muffins to s students. I proved a general theorem, called the <i>Floor-Ceiling theorem</i>, that always gives an upper bound, FC(m,s) on f(m,s). I worked out formulas for </p><p>f(m,1) (trivial), </p><p>f(m,2) (trivial), </p><p>f(m,3) (its always FC(m,3),</p><p> f(m,4) (its always FC(m,4)).</p><p>While working on f(m,5) I found that  f(m,5) was always FC(m,5) EXCEPT for m=11. So what's up with f(11,5)?  </p><p>By the Floor Ceiling theorem f(11,5) \le 11/25. We (at that point several ugrads and HS students had joined the project)  were unable to find a protocol that would show f(11,5)\ge 11/25. Personally I thought there WAS such an protocol but perhaps it was more complicated than the ones we had found (We were finding them by hand using some easy linear algebra.) Perhaps a computer program was needed. We did find a protocol for f(11,5)\ge 13/30, which surely was not optimal. </p><p>While on an Amtrak I began working out the following train of thought: The protocol for f(11,5)\le 11/25 MUST have </p><p>(1) every muffin cut into two pieces,</p><p>(2) 3 students get 4 pieces, </p><p>(3) 2 students get 5 pieces. </p><p>While working on getting a protocol for f(11,5)\le 11/25 with these properties I found that... <i>there could be no such protocol</i>! Then by reworking what I did I found that f(11,5)\le 13/30. So it was done! and we had a new technique, which we call <i>The Half Method. </i>To see the full proof see my slides <a href="http://www.cs.umd.edu/~gasarch/MUFFINS/muffintalkGen.pdf">here</a></p><p>The story above is typical: We get f(m,k) for all 1\le k\le SOMETHING, we get stuck, and then we find ANOTHER technique to show upper bounds (which in this case are limits on how well we can do). This happened about 8 times depending on how you count.  After a while we realized that this could not just be an article, this was a book! World Scienfiic agreed to publish it, and its out now.</p><p>Misc Notes</p><p>1) I got a conference paper out of it, in the Fun with Algorithms Conference, with some of the co-authors on the book, and some other people. <a href="https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=8806">here is the conf paper</a>.</p><p>2) Early on we realized that f(m,s) = (m/s)f(s,m) so we only had to look at the m&gt;s case.</p><p>3) The fact that f(m,s) exists and is rational is not obvious, but is true. In fact, f(m,s) can be found by a mixed-int program. </p><p>4) Late on in the process I found that there was a by-invite-only math newsgroup that had discussed the problem, and in fact was where Alan Frank first posted it. I obtained their materials and found that they had already shown f(m,s)=(m/s)f(s,m) and also that the answer is always rational and exists. Aside from that our results did not overlap.</p><p>5) Even later in the process Scott Huddleston emailed me (out of the blue) that he had a program that solved the muffin problem quickly. I was skeptical at first, but he did indeed have a whole new way to look at the problem and his code was very fast (I had Jacob Prinz, one of the co-authors on the book, recode it). Later Richard Chatwin (see <a href="https://arxiv.org/abs/1907.08726">here</a>) seems to have proven that Scott's method always works. The approach of Scott and Richard is where to go if you want to do serious further research on Muffins. My book is where you want to go if you want to learn some easy and fun math (a HS student could read it). </p><p>6) I co-authored a column with Scott H, Erik Metz, Jacob Prinz on Muffins, featuring his technique, in Lane's complexity column, <a href="http://www.cs.umd.edu/~gasarch/papers/sigmuffins.pdf">here</a>.</p><p>7) I had an REU student, Stephanie Warman, write a muffin package based on the book.</p><p>8) I gave a talk an invited talk on The Muffin Problem  at a Joint AMS-MAA meeting. </p><p>9) I gave a talk at Gathering for Gardner 2018 on The Muffin Problem. </p><p>10) I often give talks on it to groups of High School students.</p><p>11) When I teach Discrete Math Honors I talk about it and assign problems on it- it really is part of the course. As such its a good way to reinforce the pigeon hole principle. </p><p>12) I contacted Alan Frank about my work. We arranged to meet at an MIT combinatorics seminar where I was to give a talk on muffins. He brought 11 muffins, with 1 cut (1/2,1/2), 2 cut (14/30,16/30),</p><p>and 8 cut (13/30,17/30) so that the 11 of us could each get 11/5 with smallest piece 13/30. </p><p>13) Coda: </p><p>Why did I keep working on this problem?  I kept working on it because I kept hitting barriers and (with co-authors) breaking them with new techniques that were interesting.  If early on a barrier was not breakable then I would have stopped. If (say) Floor-ceiling solved everything than I might have gotten a paper out of  this, but surely not a book.</p><p>Lesson for all of us: look around you! Its not clear what is going to inspire a project!</p><p>Lasting effect: I am reluctant to throw out old math magazines and pamphlets since you never know when one will lead to a book.</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/i-have-book-out-on-muffins-you-prob.html"><span class="datestr">at October 11, 2021 01:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5868">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5868">Gaussian BosonSampling, higher-order correlations, and spoofing: An update</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In my <a href="https://www.scottaaronson.com/blog/?p=5859">last post</a>, I wrote (among other things) about an ongoing scientific debate between the group of Chaoyang Lu at USTC in China, which over the past year has been doing experiments that seek to demonstrate quantum supremacy via Gaussian BosonSampling; and the group of Sergio Boixo at Google, which had a <a href="https://arxiv.org/abs/2109.11525">recent paper</a> on a polynomial-time classical algorithm to sample approximately from the same distributions.  I reported the facts as I understood them at the time.  Since then, though, a long call with the Google team gave me a new and different understanding, and I feel duty-bound to share that here.</p>



<p>A week ago, I considered it obvious that if, using a classical spoofer, you could beat the USTC experiment on a metric like total variation distance from the ideal distribution, then you would’ve completely destroyed USTC’s claim of quantum supremacy.  The reason I believed *that*, in turn, is a proposition that I hadn’t given a name but needs one, so let me call it <strong>Hypothesis H</strong>:</p>



<blockquote class="wp-block-quote"><p>The only way a classical algorithm to spoof BosonSampling can possibly do well in total variation distance, is by correctly reproducing the high-order correlations (correlations among the occupation numbers of large numbers of modes) — because that’s where the complexity of BosonSampling lies (if it lies anywhere).</p></blockquote>



<p>Hypothesis H had important downstream consequences.  Google’s algorithm, by the Google team’s own admission, does not reproduce the high-order correlations.  Furthermore, importantly, because of limitations on both samples and classical computation time, Google’s paper calculates the total variation distance from the ideal distribution only on the marginal distribution on roughly 14 out of 144 modes.  On that marginal distribution, Google’s algorithm does do better than the experiment in total variation distance.  Google presents a claimed extrapolation to the full 144 modes, but eyeballing the graphs, it was far from clear to me what would happen: like, maybe the spoofing algorithm would continue to win, but maybe the experiment would turn around and win; who knows?</p>



<p>Chaoyang, meanwhile, made a clear prediction that the experiment would turn around and win, because of</p>



<ol><li>the experiment’s success in reproducing the high-order correlations,</li><li>the admitted failure of Google’s algorithm in reproducing the high-order correlations, and</li><li>the seeming impossibility of doing well on BosonSampling *without* reproducing the high-order correlations (Hypothesis H).</li></ol>



<p>Given everything my experience told me about the central importance of high-order correlations for BosonSampling, I was inclined to agree with Chaoyang.</p>



<p>Now for the kicker: it seems that Hypothesis H is false.  A classical spoofer could beat a BosonSampling experiment on total variation distance from the ideal distribution, without even bothering to reproduce the high-order correlations correctly.</p>



<p>This is true because of a combination of two facts about the existing noisy BosonSampling experiments.  The first fact is that the contribution from the order-k correlations falls off like 1/exp(k).  The second fact is that, due to calibration errors and the like, the experiments already show significant deviations from the ideal distribution on the order-1 and order-2 correlations.</p>



<p>Put these facts together and what do you find?  Well, suppose your classical spoofing algorithm takes care to get the low-order contributions to the distribution exactly right.  Just for that reason alone, it could already win over a noisy BosonSampling experiment, as judged by benchmarks like total variation distance from the ideal distribution, or for that matter linear cross-entropy.  Yes, the experiment will beat the classical simulation on the higher-order correlations.  But because those higher-order correlations are exponentially attenuated anyway, they won’t be enough to make up the difference.  The experiment’s lack of perfection on the low-order correlations will swamp everything else.</p>



<p>Granted, I still don’t know for sure that this <em>is</em> what happens — that depends on whether I believe Sergio or Chaoyang about the extrapolation of the variation distance to the full 144 modes (my own eyeballs having failed to render a verdict!).  But I now see that it’s logically possible, maybe even plausible.</p>



<p>So, let’s imagine for the sake of argument that Google’s simulation wins on variation distance, even though the experiment wins on the high-order correlations.  In that case, what would be our verdict: would USTC have achieved quantum supremacy via BosonSampling, or not?</p>



<p>It’s clear what each side could say.</p>



<p>Google could say: by a metric that Scott Aaronson, the coinventor of BosonSampling, thought was perfectly adequate as late as last week — namely, total variation distance from the ideal distribution — we won.  We achieved lower variation distance than USTC’s experiment, and we did it using a fast classical algorithm.  End of discussion.  No moving the goalposts after the fact.</p>



<p>Google could even add: BosonSampling is a <em>sampling</em> task; it’s right there in the name!  The only purpose of any benchmark — whether Linear XEB or high-order correlation — is to give evidence about whether you are or aren’t sampling from a distribution close to the ideal one.  But that means that, if you accept that we <em>are</em> doing the latter better than the experiment, then there’s nothing more to argue about.</p>



<p>USTC could respond: even if Scott Aaronson <em>is</em> the coinventor of BosonSampling, he’s extremely far from an infallible oracle.  In the case at hand, his lack of appreciation for the sources of error in realistic experiments caused him to fixate inappropriately on variation distance as the success criterion.  If you want to see the quantum advantage in our system, you have to deliberately subtract off the low-order correlations and look at the high-order correlations.</p>



<p>USTC could add: from the very beginning, the whole point of quantum supremacy experiments was to demonstrate a clear speedup on <em>some</em> benchmark — we never particularly cared which one!  That horse is out of the barn as soon as we’re talking about quantum supremacy at all — something the Google group, which itself reported the first quantum supremacy experiment in Fall 2019, again for a completely artificial benchmark — knows as well as anyone else.  (The Google team even has experience with adjusting benchmarks: when, for example, <a href="https://arxiv.org/abs/2103.03074">Pan and Zhang</a> pointed out that Linear XEB as originally specified is pretty easy to spoof for random 2D circuits, the most cogent rejoinder was: OK, fine then, then add an extra check that the returned samples are sufficiently different from one another, which kills Pan and Zhang’s spoofing strategy.)  In that case, then, why isn’t a benchmark tailored to the high-order correlations as good as variation distance or linear cross-entropy or any other benchmark?</p>



<p>Both positions are reasonable and have merit — though I confess to somewhat greater sympathy for the one that appeals to my doofosity rather than my supposed infallibility!</p>



<p>OK, but suppose, again for the sake of argument, that we accepted the second position, and we said that USTC gets to declare quantum supremacy as long as its experiment does better than any known classical simulation at reproducing the high-order correlations.  We’d still face the question: does the USTC experiment, in fact, do better on that metric?  It would be awkward if, having won the right to change the rules in its favor, USTC still lost even under the new rules.</p>



<p>Sergio tells me that USTC directly reported experimental data only for up to order-7 correlations, and at least individually, the order-7 correlations are easy to reproduce on a laptop (although <em>sampling</em> in a way that reproduces the order-7 correlations might still be hard—a point that Chaoyang confirms, and where further research would be great).  OK, but USTC also reported that their experiment seems to reproduce up to order-19 correlations.  And order-19 correlations, the Google team agrees, are hard to sample consistently with on a classical computer by any currently known algorithm.</p>



<p>So then, why don’t we have direct data for the order-19 correlations?  The trouble is simply that it would’ve taken USTC an astronomical amount of computation time.  So instead, they relied on a statistical extrapolation from the observed strength of the lower-order correlations — there we go again with the extrapolations!  Of course, if we’re going to let Google rest its case on an extrapolation, then maybe it’s only sporting to let USTC do the same.</p>



<p>You might wonder: why didn’t we have to worry about any of this stuff with the <em>other</em> path to quantum supremacy, the one via random circuit sampling with superconducting qubits?  The reason is that, with random circuit sampling, all the correlations except the highest-order ones are completely trivial — or, to say it another way, the reduced state of any small number of output qubits is exponentially close to the maximally mixed state.  This is a real difference between BosonSampling and random circuit sampling—and even 5-6 years ago, we knew that this represented an advantage for random circuit sampling, although I now have a deeper appreciation for just how great of an advantage it is.  For it means that, with random circuit sampling, it’s easier to place a “sword in the stone”: to say, for example, <em>here</em> is the Linear XEB score achieved by the trivial classical algorithm that outputs random bits, and lo, our experiment achieves a higher score, and lo, we challenge anyone to invent a fast classical spoofing method that achieves a similarly high score.</p>



<p>With BosonSampling, by contrast, we have various metrics with which to judge performance, but so far, for none of those metrics do we have a plausible hypothesis that says “<em>here’s</em> the best that any polynomial-time classical algorithm can possibly hope to do, and it’s completely plausible even for a noisy current or planned BosonSampling experiment to do better than that.”</p>



<p>In the end, then, I come back to the exact same three goals I would’ve recommended a week ago for the future of quantum supremacy experiments, but with all of them now even more acutely important than before:</p>



<ol><li>Experimentally, to increase the fidelity of the devices (with BosonSampling, for example, to observe a larger contribution from the high-order correlations) — a much more urgent goal, from the standpoint of evading classical spoofing algorithms, than further increasing the dimensionality of the Hilbert space.</li><li>Theoretically, to design better ways to verify the results of sampling-based quantum supremacy experiments classically — ideally, even ways that could be applied via polynomial-time tests.</li><li>For Gaussian BosonSampling in particular, to get a better understanding of the plausible limits of classical spoofing algorithms, and exactly how good a noisy device needs to be before it exceeds those limits.</li></ol>



<p>Thanks so much to Sergio Boixo and Ben Villalonga for the conversation, and to Chaoyang Lu and Jelmer Renema for comments on this post.  Needless to say, any remaining errors are my own.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5868"><span class="datestr">at October 10, 2021 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03624">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03624">On the Complexity of Inductively Learning Guarded Rules</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Andrei Draghici, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gottlob:Georg.html">Georg Gottlob</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lanzinger:Matthias.html">Matthias Lanzinger</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03624">PDF</a><br /><b>Abstract: </b>We investigate the computational complexity of mining guarded clauses from
clausal datasets through the framework of inductive logic programming (ILP). We
show that learning guarded clauses is NP-complete and thus one step below the
$\sigma^P_2$-complete task of learning Horn clauses on the polynomial
hierarchy. Motivated by practical applications on large datasets we identify a
natural tractable fragment of the problem. Finally, we also generalise all of
our results to $k$-guarded clauses for constant $k$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03624"><span class="datestr">at October 10, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03620">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03620">Hyperparameter Tuning with Renyi Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papernot:Nicolas.html">Nicolas Papernot</a>, Thomas Steinke <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03620">PDF</a><br /><b>Abstract: </b>For many differentially private algorithms, such as the prominent noisy
stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy
leakage of a single training run is well understood. However, few studies have
reasoned about the privacy leakage resulting from the multiple training runs
needed to fine tune the value of the training algorithm's hyperparameters. In
this work, we first illustrate how simply setting hyperparameters based on
non-private training runs can leak private information. Motivated by this
observation, we then provide privacy guarantees for hyperparameter search
procedures within the framework of Renyi Differential Privacy. Our results
improve and extend the work of Liu and Talwar (STOC 2019). Our analysis
supports our previous observation that tuning hyperparameters does indeed leak
private information, but we prove that, under certain assumptions, this leakage
is modest, as long as each candidate training run needed to select
hyperparameters is itself differentially private.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03620"><span class="datestr">at October 10, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03460">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03460">Finding popular branchings in vertex-weighted digraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Kei Natsui, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takazawa:Kenjiro.html">Kenjiro Takazawa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03460">PDF</a><br /><b>Abstract: </b>Popular matchings have been intensively studied recently as a relaxed concept
of stable matchings. By applying the concept of popular matchings to branchings
in directed graphs, Kavitha et al.\ (2020) introduced popular branchings. In a
directed graph $G=(V_G,E_G)$, each vertex has preferences over its incoming
edges. For branchings $B_1$ and $B_2$ in $G$, a vertex $v\in V_G$ prefers $B_1$
to $B_2$ if $v$ prefers its incoming edge of $B_1$ to that of $B_2$, where
having an arbitrary incoming edge is preferred to having none, and $B_1$ is
more popular than $B_2$ if the number of vertices that prefer $B_1$ is greater
than the number of vertices that prefer $B_2$. A branching $B$ is called a
popular branching if there is no branching more popular than $B$. Kavitha et
al.\ (2020) proposed an algorithm for finding a popular branching when the
preferences of each vertex are given by a strict partial order. The validity of
this algorithm is proved by utilizing classical theorems on the duality of
weighted arborescences. In this paper, we generalize popular branchings to
weighted popular branchings in vertex-weighted directed graphs in the same
manner as weighted popular matchings by Mestre (2014). We give an algorithm for
finding a weighted popular branching, which extends the algorithm of Kavitha et
al., when the preferences of each vertex are given by a total preorder and the
weights satisfy certain conditions. Our algorithm includes elaborated
procedures resulting from the vertex-weights, and its validity is proved by
extending the argument of the duality of weighted arborescences.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03460"><span class="datestr">at October 10, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03279">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03279">Polynomial Turing Kernels for Clique with an Optimal Number of Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fluschnik:Till.html">Till Fluschnik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heeger:Klaus.html">Klaus Heeger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hermelin:Danny.html">Danny Hermelin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03279">PDF</a><br /><b>Abstract: </b>A polynomial Turing kernel for some parameterized problem $P$ is a
polynomial-time algorithm that solves $P$ using queries to an oracle of $P$
whose sizes are upper-bounded by some polynomial in the parameter. Here the
term "polynomial" refers to the bound on the query sizes, as the running time
of any kernel is required to be polynomial. One of the most important open
goals in parameterized complexity is to understand the applicability and
limitations of polynomial Turing Kernels. As any fixed-parameter tractable
problem admits a Turing kernel of some size, the focus has mostly being on
determining which problems admit such kernels whose query sizes can be indeed
bounded by some polynomial.
</p>
<p>In this paper we take a different approach, and instead focus on the number
of queries that a Turing kernel uses, assuming it is restricted to using only
polynomial sized queries. Our study focuses on one the main problems studied in
parameterized complexity, the Clique problem: Given a graph $G$ and an integer
$k$, determine whether there are $k$ pairwise adjacent vertices in $G$. We show
that Clique parameterized by several structural parameters exhibits the
following phenomena:
</p>
<p>- It admits polynomial Turing kernels which use a sublinear number of
queries, namely $O(n/\log^c n)$ queries where $n$ is the total size of the
graph and $c$ is any constant. This holds even for a very restrictive type of
Turing kernels which we call OR-kernels.
</p>
<p>- It does not admit polynomial Turing kernels which use $O(n^{1-\epsilon})$
queries, unless NP$\subseteq$coNP/poly.
</p>
<p>For proving the second item above, we develop a new framework for bounding
the number of queries needed by polynomial Turing kernels. This framework is
inspired by the standard lower bounds framework for Karp kernels, and while it
is quite similar, it still requires some novel ideas to allow its extension to
the Turing setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03279"><span class="datestr">at October 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03195">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03195">Coresets for Decision Trees of Signals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jubran:Ibrahim.html">Ibrahim Jubran</a>, Ernesto Evgeniy Sanches Shayda, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Newman:Ilan.html">Ilan Newman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Dan.html">Dan Feldman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03195">PDF</a><br /><b>Abstract: </b>A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix
(2D-signal) into $k\geq 1$ block matrices (axis-parallel rectangles, leaves)
where each rectangle is assigned a real label. Its regression or classification
loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared
differences over every label in $D$ and its assigned label by $t$. Given an
error parameter $\varepsilon\in(0,1)$, a $(k,\varepsilon)$-coreset $C$ of $D$
is a small summarization that provably approximates this loss to \emph{every}
such tree, up to a multiplicative factor of $1\pm\varepsilon$. In particular,
the optimal $k$-tree of $C$ is a $(1+\varepsilon)$-approximation to the optimal
$k$-tree of $D$.
</p>
<p>We provide the first algorithm that outputs such a $(k,\varepsilon)$-coreset
for \emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial
in $k\log(N)/\varepsilon$, and its construction takes $O(Nk)$ time. This is by
forging a link between decision trees from machine learning -- to partition
trees in computational geometry.
</p>
<p>Experimental results on \texttt{sklearn} and \texttt{lightGBM} show that
applying our coresets on real-world data-sets boosts the computation time of
random forests and their parameter tuning by up to x$10$, while keeping similar
accuracy. Full open source code is provided.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03195"><span class="datestr">at October 10, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03152">Optimal (Euclidean) Metric Compression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Tal.html">Tal Wagner</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03152">PDF</a><br /><b>Abstract: </b>We study the problem of representing all distances between $n$ points in
$\mathbb R^d$, with arbitrarily small distortion, using as few bits as
possible. We give asymptotically tight bounds for this problem, for Euclidean
metrics, for $\ell_1$ (a.k.a.~Manhattan) metrics, and for general metrics.
</p>
<p>Our bounds for Euclidean metrics mark the first improvement over compression
schemes based on discretizing the classical dimensionality reduction theorem of
Johnson and Lindenstrauss (Contemp.~Math.~1984). Since it is known that no
better dimension reduction is possible, our results establish that Euclidean
metric compression is possible beyond dimension reduction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03152"><span class="datestr">at October 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03136">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03136">The Gromov-Hausdorff distance between ultrametric spaces: its structure and computation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=eacute=moli:Facundo.html">Facundo Mémoli</a>, Zane Smith, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wan:Zhengchao.html">Zhengchao Wan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03136">PDF</a><br /><b>Abstract: </b>The Gromov-Hausdorff distance ($d_\mathrm{GH}$) provides a natural way of
quantifying the dissimilarity between two given metric spaces. It is known that
computing $d_\mathrm{GH}$ between two finite metric spaces is NP-hard, even in
the case of finite ultrametric spaces which are highly structured metric spaces
in the sense that they satisfy the so-called \emph{strong triangle inequality}.
Ultrametric spaces naturally arise in many applications such as hierarchical
clustering, phylogenetics, genomics, and even linguistics. By exploiting the
special structures of ultrametric spaces, (1) we identify a one parameter
family $\{d_\mathrm{GH}^{(p)}\}_{p\in[1,\infty]}$ of distances defined in a
flavor similar to the Gromov-Hausdorff distance on the collection of finite
ultrametric spaces, and in particular $d_\mathrm{GH}^{(1)} =d_\mathrm{GH}$. The
extreme case when $p=\infty$, which we also denote by $u_\mathrm{GH}$, turns
out to be an ultrametric on the collection of ultrametric spaces. Whereas for
all $p\in[1,\infty)$, $d_\mathrm{GH}^{(p)}$ yields NP-hard problems, we prove
that surprisingly $u_\mathrm{GH}$ can be computed in polynomial time. The proof
is based on a structural theorem for $u_\mathrm{GH}$ established in this paper;
(2) inspired by the structural theorem for $u_\mathrm{GH}$, and by carefully
leveraging properties of ultrametric spaces, we also establish a structural
theorem for $d_\mathrm{GH}$ when restricted to ultrametric spaces. This
structural theorem allows us to identify special families of ultrametric spaces
on which $d_\mathrm{GH}$ is computationally tractable. These families are
determined by properties related to the doubling constant of metric space.
Based on these families, we devise a fixed-parameter tractable (FPT) algorithm
for computing the exact value of $d_\mathrm{GH}$ between ultrametric spaces. We
believe ours is the first such algorithm to be identified.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03136"><span class="datestr">at October 10, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03122">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03122">Faster algorithm for Unique $(k,2)$-CSP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zamir:Or.html">Or Zamir</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03122">PDF</a><br /><b>Abstract: </b>In a $(k,2)$-Constraint Satisfaction Problem we are given a set of arbitrary
constraints on pairs of $k$-ary variables, and are asked to find an assignment
of values to these variables such that all constraints are satisfied. The
$(k,2)$-CSP problem generalizes problems like $k$-coloring and
$k$-list-coloring. In the Unique $(k,2)$-CSP problem, we add the assumption
that the input set of constraints has at most one satisfying assignment.
</p>
<p>Beigel and Eppstein gave an algorithm for $(k,2)$-CSP running in time
$O\left(\left(0.4518k\right)^n\right)$ for $k&gt;3$ and $O\left(1.356^n\right)$
for $k=3$, where $n$ is the number of variables. Feder and Motwani improved
upon the Beigel-Eppstein algorithm for $k\geq 11$. Hertli, Hurbain, Millius,
Moser, Scheder and Szedl{\'a}k improved these bounds for Unique $(k,2)$-CSP for
every $k\geq 5$.
</p>
<p>We improve the result of Hertli et al. and obtain better bounds for
Unique~$(k,2)$-CSP for~$k\geq 5$. In particular, we improve the running time of
Unique~$(5,2)$-CSP from~$O\left(2.254^n\right)$ to~$O\left(2.232^n\right)$ and
Unique~$(6,2)$-CSP from~$O\left(2.652^n\right)$ to~$O\left(2.641^n\right)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03122"><span class="datestr">at October 10, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.03070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.03070">Robust Algorithms for GMM Estimation: A Finite Sample Viewpoint</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohatgi:Dhruv.html">Dhruv Rohatgi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Syrgkanis:Vasilis.html">Vasilis Syrgkanis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.03070">PDF</a><br /><b>Abstract: </b>For many inference problems in statistics and econometrics, the unknown
parameter is identified by a set of moment conditions. A generic method of
solving moment conditions is the Generalized Method of Moments (GMM). However,
classical GMM estimation is potentially very sensitive to outliers. Robustified
GMM estimators have been developed in the past, but suffer from several
drawbacks: computational intractability, poor dimension-dependence, and no
quantitative recovery guarantees in the presence of a constant fraction of
outliers. In this work, we develop the first computationally efficient GMM
estimator (under intuitive assumptions) that can tolerate a constant $\epsilon$
fraction of adversarially corrupted samples, and that has an $\ell_2$ recovery
guarantee of $O(\sqrt{\epsilon})$. To achieve this, we draw upon and extend a
recent line of work on algorithmic robust statistics for related but simpler
problems such as mean estimation, linear regression and stochastic
optimization. As two examples of the generality of our algorithm, we show how
our estimation algorithm and assumptions apply to instrumental variables linear
and logistic regression. Moreover, we experimentally validate that our
estimator outperforms classical IV regression and two-stage Huber regression on
synthetic and semi-synthetic datasets with corruption.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.03070"><span class="datestr">at October 10, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1343">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/">Soliciting information about Women in TCS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CATCS is compiling a list of <em>Women in Theoretical Computer Science (WinTCS)</em>, hoping to facilitate engagement and opportunities for Women in TCS in the future. </p>



<p>We cordially invite women (broadly defined as anyone who self-identifies as a woman) as well as gender non-conforming TCS researchers to participate the following simple <a href="https://docs.google.com/forms/d/e/1FAIpQLSc2LcI0mtUvyKgl34OqbxDVpu0zbYs0fiLmU_5jr2qHybCfMQ/viewform?usp=sf_link" target="_blank" rel="noreferrer noopener">google-form survey</a> to provide your information. </p>



<p>Please submit your information before <strong>Oct. 31, 2021</strong>. After that, information can still be provided through the CATCS website, and will be added to the list monthly. <br />**Please feel free to share this solicitation broadly within your networks.**</p>



<p>If you have any questions regarding this survey, please feel free to contact us at <a target="_blank" rel="noreferrer noopener">goldner@bu.edu</a> or <a target="_blank" rel="noreferrer noopener">yusuwang@ucsd.edu</a>.  </p>



<p>– Kira Goldner and Yusu Wang (on behalf of the SIGACT CATCS)</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/"><span class="datestr">at October 08, 2021 09:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5384653288710765756">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/c-is-for-cookie-and-thats-good-enough.html">C++ is for Cookie and That's Good Enough for Me</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Potbelly, a local sandwich chain, made me an offer I couldn't refuse: change my password and earn a free (and quite tasty) oatmeal chocolate chip cookie. A free cookie is a great motivator, and checking that this wasn't some clever phishing attack, changed my password and got my cookie. Not sure why Potbelly wanted me to change my password but happy to take their cookie.</p><p>Potbelly likely didn't make this offer to everyone so what if you want a cookie?</p><p></p><ol style="text-align: left;"><li>Use an app to get a cookie delivered.</li><li>Visit a specialty cookie store.</li><li>Go to your local supermarket and pick up a package of <a href="https://www.marianos.com/p/chips-ahoy-original-chocolate-chip-cookies-family-size/0004400003338">Chip's Ahoy</a>.</li><li>Buy some pre-made <a href="https://www.marianos.com/p/pillsbury-ready-to-bake-chocolate-chip-cookie-dough/0001800081778">cookie dough</a> and put it in the oven.</li><li>Buy some <a href="https://www.marianos.com/p/betty-crocker-chocolate-chip-cookie-mix/0001600030650">cookie mix</a>, add ingredients and bake.</li><li>Find a <a href="https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57">cookie recipe</a>, buy the ingredients and get cooking</li><li>Get fresh ingredients direct from a farm stand</li><li>Grow and gather your own ingredients, ala <a href="https://shop.scholastic.com/teachers-ecommerce/teacher/books/pancakes-pancakes-9780545653619.html">Pancakes Pancakes</a></li></ol><div>In machine learning we seem to be heading into a similar set of choices</div><div><ol style="text-align: left;"><li>Not even realize you are using machine learning, such as recommendations on Netflix or Facebook.</li><li>Using ML implicitly, like talking to Alexa</li><li>Using pre-trained ML through an app, like Google Translate</li><li>Using pre-trained ML through an API</li><li>Using a model like GPT-3 with an appropriate prompt</li><li>Use an easily trained model like <a href="https://aws.amazon.com/fraud-detector/">Amazon Fraud Detector</a></li><li>An integrated machine learning environment like <a href="https://aws.amazon.com/sagemaker/">Sagemaker</a></li><li>Use pre-built ML tools like TensorFlow or PyTorch</li><li>Code up your own ML algorithms in C++</li><li>Build your own hardware and software</li></ol><div>and probably missing a few options.</div><div><br /></div><div>When you want cookies or learning, do you buy it prepackaged or do you roll your own? And when people offer it to you for free, how wary should you be?</div></div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/c-is-for-cookie-and-thats-good-enough.html"><span class="datestr">at October 08, 2021 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/08/arv-on-abelian-cayley-graphs/">ARV on Abelian Cayley Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Continuing from <a href="https://lucatrevisan.wordpress.com/2021/10/07/buser-inequalities-in-graphs/">the previous post</a>, we are going to prove the following result: let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> be a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Cayley graph of an Abelian group, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> be the normalized edge expansion of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BARV%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{ARV(G)}" class="latex" /> be the value of the ARV semidefinite programming relaxation of sparsest cut on <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> (we will define it below), and <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G)}" class="latex" /> be the second smallest normalized Laplacian eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Then we have <a name="main"></a></p>
<p><a name="main"></a></p><a name="main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_2+%28G%29+%5Cleq+O%28d%29+%5Ccdot+%28ARV+%28G%29%29%5E2+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_2 (G) \leq O(d) \cdot (ARV (G))^2 \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="main"></a><a name="main"></a> which, together with the fact that <img src="https://s0.wp.com/latex.php?latex=%7BARV%28G%29+%5Cleq+2+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{ARV(G) \leq 2 \phi(G)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29+%5Cleq+%5Csqrt%7B2+%5Clambda_2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G) \leq \sqrt{2 \lambda_2}}" class="latex" />, implies the Buser inequality <a name="buser"></a></p>
<p><a name="buser"></a></p><a name="buser">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_2+%28G%29+%5Cleq+O%28d%29+%5Ccdot+%5Cphi%5E2+%28G%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_2 (G) \leq O(d) \cdot \phi^2 (G) \ \ \ \ \ (2)" class="latex" /></p>
</a><p><a name="buser"></a><a name="buser"></a> and the approximation bound <a name="arvapx"></a></p>
<p><a name="arvapx"></a></p><a name="arvapx">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cphi%28G%29+%5Cleq+O%28%5Csqrt+d%29+%5Ccdot+ARV%28G%29+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \phi(G) \leq O(\sqrt d) \cdot ARV(G) \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="arvapx"></a><a name="arvapx"></a> The proof of <a href="https://lucatrevisan.wordpress.com/feed/#main">(1)</a>, due to Shayan Oveis Gharan and myself, is very similar to the proof by Bauer et al. of <a href="https://lucatrevisan.wordpress.com/feed/#buser">(2)</a>.</p>
<p><span id="more-4564"></span></p>
<p><b>1. Ideas </b></p>
<p>For a positive integer parameter <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, call <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> the multigraph whose adjacency matrix is <img src="https://s0.wp.com/latex.php?latex=%7BA%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> is the adjacency matrix of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. So <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d^t}" class="latex" />-regular graph, and each edge in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> corresponds to a length-<img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> walk in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Our proof boils down to showing <a name="rw"></a></p>
<p><a name="rw"></a></p><a name="rw">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++ARV%28G%5Et%29+%5Cleq+O%28%5Csqrt%7Bdt%7D%29+%5Ccdot+ARV%28G%29+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   ARV(G^t) \leq O(\sqrt{dt}) \cdot ARV(G) \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="rw"></a><a name="rw"></a> which gives <a href="https://lucatrevisan.wordpress.com/feed/#main">(1)</a> after we note that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%5Et%29+%3D+1+-+%281+-+%5Clambda_2%28G%29%29%5Et+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G^t) = 1 - (1 - \lambda_2(G))^t " class="latex" /></p>
<p> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ARV%28G%5Et%29+%5Cgeq+%5Clambda_2+%28G%5Et%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  ARV(G^t) \geq \lambda_2 (G^t) " class="latex" /></p>
<p> and we combine the above inequalities with <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3A%3D+1%2F%5Clambda_2+%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t := 1/\lambda_2 (G)}" class="latex" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5COmega%281%29+%5Cleq+1+-+%281+-+%5Clambda_2%28G%29%29%5Et+%3D+%5Clambda_2+%28G%5Et%29+%5Cleq+ARV%28G%5Et%29+%5Cleq+O%28%5Csqrt%7Bdt%7D%29+%5Ccdot+ARV%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Omega(1) \leq 1 - (1 - \lambda_2(G))^t = \lambda_2 (G^t) \leq ARV(G^t) \leq O(\sqrt{dt}) \cdot ARV(G) " class="latex" /></p>
<p> The reader will see that our argument could also prove (roughly as done by Bauer et al.) that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%5Et%29+%5Cleq+O%28%5Csqrt+d%29+%5Ccdot+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G^t) \leq O(\sqrt d) \cdot \phi(G)}" class="latex" />, simply by reasoning about distributions of cuts instead of reasoning about ARV solutions, which would give <a href="https://lucatrevisan.wordpress.com/feed/#buser">(2)</a> more directly. By reasoning about ARV solutions, however, we are also able to establish <a href="https://lucatrevisan.wordpress.com/feed/#arvapx">(3)</a>, which we think is independently interesting.</p>
<p>It remains to prove <a href="https://lucatrevisan.wordpress.com/feed/#rw">(4)</a>. I will provide a completely self-contained proof, including the definition of Cayley graphs and of the ARV relaxation, but, first, here is a summary for the reader already familiar with this material: we take an optimal solution of ARV for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, and bound how well it does for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" />. We need to understand how much bigger is the fraction of edges cut by the solution in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> compared to the fraction of edges cut in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />; a random edge in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is obtained by randomly sampling <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> generators and adding them together, which is roughly like sampling <img src="https://s0.wp.com/latex.php?latex=%7Bt%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t/d}" class="latex" /> times each generator, each time with a random sign. Because of cancellations, we expect that the sum of these <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> random generators can be obtained by summing roughly <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bt%2Fd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{t/d}}" class="latex" /> copies of each of the <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> generators, or <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Btd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{td}}" class="latex" /> generators in total. So a random edge of <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> corresponds roughly to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Btd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{td}}" class="latex" /> edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, and this is by how much, at most, the fraction of cut edges can grow.</p>
<p><b>2. Definitions </b></p>
<p>Now we present more details and definition. Recall that if <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> is a group, for which we use additive notation, and <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> is a set or multiset of group elements, then the Cayley graph <img src="https://s0.wp.com/latex.php?latex=%7BCay%28%5CGamma%2CS%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Cay(\Gamma,S)}" class="latex" /> is the graph that has a vertex for every group element and an edge <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cx%2Bs%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x,x+s)}" class="latex" /> for every group element <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in \Gamma}" class="latex" /> and every element <img src="https://s0.wp.com/latex.php?latex=%7Bs%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s\in S}" class="latex" />. We restrict ourselves to undirected graphs, so we will always assume that if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> is an element of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7B-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-s}" class="latex" /> is also an element of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> (with the same multiplicity, if <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> if a multiset). Note that the resulting undirected graph is <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S|}" class="latex" />-regular.</p>
<p>Several families of graphs can be seen to be Cayley graphs, including cycles, cliques, balanced complete bipartite graphs, hypercubes, toruses, and so on. All the above examples are actually Cayley graphs of <em>Abelian</em> groups. Several interesting families of graphs, for example several families of expanders, are Cayley graphs of non-Abelian groups, but the result of this post will apply only to Abelian groups.</p>
<p>To define the ARV relaxation, let us take it slowly and start from the definition of the sparsest cut problem. The edge expansion problem <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> is closely related to the <em>sparsest cut</em> problem, which can be defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csigma%28G%29+%3A%3D+%5Cmin_%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5EV%7D+%5C+%5C+%5Cfrac%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sigma(G) := \min_{x \in \{0,1\}^V} \ \ \frac{x^T L_G x}{x^T L_K x } " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7BL_G+%3D+I+-+A%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_G = I - A/d}" class="latex" /> is the normalized Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BL_K+%3D+I+-+J%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_K = I - J/n}" class="latex" /> is the normalized Laplacian of the clique on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> vertices. We wrote it this way to emphasize the similarity with the computation of the second smallest normalized Laplacian eigenvalue, which can be written as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%3D+%5Cmin_%7Bx+%5Cin+R%5EV+%3A+%5C+x+%5Cperp+%7B%5Cbf+1%7D%7D+%5C+%5C+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ETx+%7D+%3D+%5Cmin_%7Bx%5Cin+R%5EV%7D+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) = \min_{x \in R^V : \ x \perp {\bf 1}} \ \ \frac {x^T L_G x}{x^Tx } = \min_{x\in R^V} \frac {x^T L_G x}{x^T L_K x} " class="latex" /></p>
<p> where the second equality is perhaps not obvious but is easily proved (all the solutions <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cperp+%7B%5Cbf+1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\perp {\bf 1}}" class="latex" /> have the same cost function on both sides, and the last expression is shift invariant, so there is no loss in optimizing over all <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5EV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb R}^V}" class="latex" /> or just in the space <img src="https://s0.wp.com/latex.php?latex=%7B%5Cperp+%7B%5Cbf+1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\perp {\bf 1}}" class="latex" />). We see that the computation of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is just a relaxation of the sparsest cut problem, and so we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2%28G%29+%5Cleq+%5Csigma%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2(G) \leq \sigma(G) " class="latex" /></p>
<p> We can write the sparsest cut problem in a less algebraic version as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csigma%28G%29+%3D+%5Cmin_%7BS%5Csubseteq+V%7D+%5C+%5C+%5Cfrac%7B+cut%28S%29%7D%7B%5Cfrac+dn+%5Ccdot+%7CV%7C+%5Ccdot+%7CV-S%7C+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sigma(G) = \min_{S\subseteq V} \ \ \frac{ cut(S)}{\frac dn \cdot |V| \cdot |V-S| } " class="latex" /></p>
<p> and recall that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%3D+%5Cmin_%7BS%5Csubseteq+V%3A+%7CS%7C+%5Cleq+%5Cfrac+%7B%7CV%7C%7D+2%7D+%5C+%5C+%5Cfrac%7B+cut%28S%29%7D%7Bd+%7CS%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) = \min_{S\subseteq V: |S| \leq \frac {|V|} 2} \ \ \frac{ cut(S)}{d |S|} " class="latex" /></p>
<p> The cost function for <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> does not change if we switch <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BV-S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V-S}" class="latex" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> could be defined equivalently as an optimization problem over subsets <img src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+V%3A+%7CS%7C+%5Cleq+%7CV%7C%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S\subseteq V: |S| \leq |V|/2}" class="latex" />, and at this point <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> are the same problem except for an extra factor of <img src="https://s0.wp.com/latex.php?latex=%7B%7CV-S%7C%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|V-S|/n}" class="latex" /> in the denominator of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" />. Such a factor is always between <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, so we have:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%5Cleq+%5Csigma%28G%29%5Cleq+2+%5Cphi%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) \leq \sigma(G)\leq 2 \phi(G) " class="latex" /></p>
<p> (Note that all these definitions have given us a particularly convoluted proof that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29+%5Cleq+2+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G) \leq 2 \phi(G)}" class="latex" />.)</p>
<p>Yet another way to characterize <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%3D+%5Cmin_%7Bx%5Cin+R%5EV%7D+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x%7D+%3D+%5Cmin_%7BX+%5Csucceq+%7B%5Cbf+0%7D%7D+%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) = \min_{x\in R^V} \frac {x^T L_G x}{x^T L_K x} = \min_{X \succeq {\bf 0}} \frac{L_G \bullet X}{L_K\bullet X} " class="latex" /></p>
<p> Where <img src="https://s0.wp.com/latex.php?latex=%7BA%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A\bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" /> is the Frobenius inner product between matrices. This is also something that is not obvious but that it is not difficult to prove, the main point being that if we write a PSD matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_ix_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_i x_ix_i^T}" class="latex" />, then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+%3D+%5Cfrac%7B%5Csum_i+x_i%5ET+L_G+x_i%7D%7B%5Csum_i+x_i%5ET+L_Kx_i%7D+%5Cgeq+%5Cmin_i+%5Cfrac%7B+x_i%5ET+L_G+x_i%7D%7Bx_i%5ET+L_Kx_i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{L_G \bullet X}{L_K\bullet X} = \frac{\sum_i x_i^T L_G x_i}{\sum_i x_i^T L_Kx_i} \geq \min_i \frac{ x_i^T L_G x_i}{x_i^T L_Kx_i} " class="latex" /></p>
<p> and so there is no loss in passing from an optimization over all PSD matrices versus all rank-1 PSD matrices.</p>
<p>We can rewrite <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2+%5Cmin_%7BX+%5Csucceq+%7B%5Cbf+0%7D%7D+%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2 \min_{X \succeq {\bf 0}} \frac{L_G \bullet X}{L_K\bullet X} }" class="latex" /> in terms of the Cholesky decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bllr%7D+%5Cmin+%26+%5Cfrac+%7B%5Csum_%7B%28u%2Cv%29%5Cin+E%7D+%7C%7C+x_u+-+x_v%7C%7C%5E2+%7D%7B%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D+%5C%5C+%5Cmbox%7Bs.t%7D%5C%5C+%26+x_v+%5Cin+%7B%5Cmathbb+R%7D%5Em+%26+%5C+%5C+%5Cforall+v%5Cin+V%5C%5C+%26+m%5Cgeq+1+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{llr} \min &amp; \frac {\sum_{(u,v)\in E} || x_u - x_v||^2 }{\frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 } \\ \mbox{s.t}\\ &amp; x_v \in {\mathbb R}^m &amp; \ \ \forall v\in V\\ &amp; m\geq 1 \end{array} " class="latex" /></p>
<p> where the correspondence between PSD matrices <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and vectors <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> is that we have <img src="https://s0.wp.com/latex.php?latex=%7BX_%7Bu%2Cv%7D+%3D+%5Clangle+x_u%2Cx_v+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_{u,v} = \langle x_u,x_v \rangle}" class="latex" /> (that is, <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> is the Cholesky decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is the Gram matrix of the <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" />). An integral solution of the sparsest cut problem corresponds to choosing rank <img src="https://s0.wp.com/latex.php?latex=%7Bm%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m=1}" class="latex" /> and solutions in which each 1-dimensional <img src="https://s0.wp.com/latex.php?latex=%7Bx_v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_v}" class="latex" /> is either 1 or 0, corresponding on whether <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> is in the set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> or not. The ARV relaxation is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bllr%7D+%5Cmin+%26+%5Cfrac+%7B%5Csum_%7B%28u%2Cv%29%5Cin+E%7D+%7C%7C+x_u+-+x_v%7C%7C%5E2+%7D%7B%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D+%5C%5C+%5Cmbox%7Bs.t%7D%5C%5C+%26+%7C%7Cx_u+-+x_v%7C%7C%5E2+%5Cleq+%7C%7Cx_u+-+x_z%7C%7C%5E2+%2B+%7C%7Cx_z+-+x_v%7C%7C%5E2+%26%5C+%5C+%5Cforall+u%2Cv%2Cz+%5Cin+V%5C%5C+%26+x_v+%5Cin+%7B%5Cmathbb+R%7D%5Em+%26+%5Cforall+v%5Cin+V%5C%5C+%26+m%5Cgeq+1+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{llr} \min &amp; \frac {\sum_{(u,v)\in E} || x_u - x_v||^2 }{\frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 } \\ \mbox{s.t}\\ &amp; ||x_u - x_v||^2 \leq ||x_u - x_z||^2 + ||x_z - x_v||^2 &amp;\ \ \forall u,v,z \in V\\ &amp; x_v \in {\mathbb R}^m &amp; \forall v\in V\\ &amp; m\geq 1 \end{array} " class="latex" /></p>
<p> which is a relaxation of sparsest cut because the “triangle inequality” constraints that we introduced are satisfied by 1-dimensional 0/1 solutions. Thus we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%5Cleq+ARV%28G%29+%5Cleq+%5Cphi%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) \leq ARV(G) \leq \phi(G) " class="latex" /></p>
<p><b>3. The Argument </b></p>
<p>Let us take any solution for ARV, and let us symmetrize it so that the symmetrized solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_u+-+x_%7Bu+%2B+s%7D+%7C%7C%5E2+%3D+%7C%7C+x_v+-+x_%7Bv%2Bs%7D+%7C%7C%5E2+%5C+%5C+%5Cforall+s%5Cin+S+%5C+%5Cforall+u%2Cv%5Cin+V+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || x_u - x_{u + s} ||^2 = || x_v - x_{v+s} ||^2 \ \ \forall s\in S \ \forall u,v\in V " class="latex" /></p>
<p> That is, make sure that the contribution of each edge to the numerator of the cost function depends only on the generator that defines the edge, and not on the pair of endpoints.</p>
<p>It is easier to see that this symmetrization is possible if we view our solution as a PSD matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />. In this case, for every group element <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" />, we can let <img src="https://s0.wp.com/latex.php?latex=%7BX_g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_g}" class="latex" /> be the solution (with the same cost) obtained by permuting rows and columns according to the mapping <img src="https://s0.wp.com/latex.php?latex=%7Bv+%5Crightarrow+g%2B+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v \rightarrow g+ v}" class="latex" />; then we can consider the solution <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1n+%5Csum_%7Bg%5Cin+V%7D+X_g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1n \sum_{g\in V} X_g}" class="latex" />, which satisfies the required symmetry condition.</p>
<p>Because of this condition, the cost function applied to <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+%7B+%5Cfrac+n2+%5Ccdot+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2%7D%7B+%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac { \frac n2 \cdot \sum_{s\in S} || x_s - x_0 ||^2}{ \frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 }" class="latex" /></p>
<p> and the cost function applied to <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+%7B+%5Cfrac+n2+%5Ccdot+%5Csum_%7Bs_1%2C%5Cldots%2Cs_t%5Cin+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_t%7D+-+x_0+%7C%7C%5E2%7D%7B+%5Cfrac+%7Bd%5Et%7D+n+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac { \frac n2 \cdot \sum_{s_1,\ldots,s_t\in S^t} || x_{s_1 + \cdots + s_t} - x_0 ||^2}{ \frac {d^t} n \sum_{u &lt; v} ||x_u - x_v ||^2 }" class="latex" /></p>
<p> meaning that our goal is now simply to prove</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1%7Bd%5Et%7D+%5Csum_%7B%28s_1%2C%5Cldots%2Cs_t%29%5Cin+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+%5Cleq+O%28%5Csqrt+%7Bdt%7D%29+%5Ccdot+%5Cfrac+1d+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac 1{d^t} \sum_{(s_1,\ldots,s_t)\in S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 \leq O(\sqrt {dt}) \cdot \frac 1d \sum_{s\in S} || x_s - x_0 ||^2 " class="latex" /></p>
<p> or, if we take a probabilistic view</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_%7B%28s_1%2C%5Cldots%2Cs_t%29+%5Csim+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+%5Cleq+O%28%5Csqrt+%7Bdt%7D%29+%5Ccdot+%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+%5C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_{(s_1,\ldots,s_t) \sim S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 \leq O(\sqrt {dt}) \cdot \mathop{\mathbb E}_{s\sim S} || x_s - x_0 ||^2 \ " class="latex" /></p>
<p> If we let <img src="https://s0.wp.com/latex.php?latex=%7Bc_s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s}" class="latex" /> be the number of times that generator <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> appears in the sum <img src="https://s0.wp.com/latex.php?latex=%7Bs_1+%2B+%5Cldots+%2B+s_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_1 + \ldots + s_t}" class="latex" />, counting cancellations (so that, if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> appears 4 times and <img src="https://s0.wp.com/latex.php?latex=%7B-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-s}" class="latex" /> appears 6 times we let <img src="https://s0.wp.com/latex.php?latex=%7Bc_s+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s = 0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7B-s%7D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_{-s} = 2}" class="latex" />) we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++s_1+%2B+%5Cldots+%2B+s_+t+%3D+%5Csum_%7Bs%5Cin+S%7D+c_s+%5Ccdot+s+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  s_1 + \ldots + s_ t = \sum_{s\in S} c_s \cdot s " class="latex" /></p>
<p> where multiplying an integer by a generator means adding the generator to itself that many times. Using the triangle inequality and the symmetrization we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_%7Bs_1+%2B+%5Cldots+%2B+s_t%7D+-+x_0+%7C%7C%5E2+%5Cleq+%5Csum_%7Bs%5Cin+S%7D+c_s+%5Ccdot+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || x_{s_1 + \ldots + s_t} - x_0 ||^2 \leq \sum_{s\in S} c_s \cdot || x_s - x_0 ||^2 " class="latex" /></p>
<p> The next observation is that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+c_s+%5Cleq+O%28%5Csqrt+%7Bt%2Fd%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} c_s \leq O(\sqrt {t/d}) " class="latex" /></p>
<p> where the expectation is over the choice of <img src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_1,\ldots,s_t}" class="latex" />. This is because <img src="https://s0.wp.com/latex.php?latex=%7Bc_s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s}" class="latex" /> can be seen as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmin+%5C%7B+0+%2C+X_1+%2B+%5Cldots+X_t+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\min \{ 0 , X_1 + \ldots X_t \}}" class="latex" />, where we define <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = +1}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%7Bs_i+%3D+s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_i = s}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = -1}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%7Bs_i+%3D+-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_i = -s}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = 0}" class="latex" /> otherwise. We have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+c_s+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%5Cmin+%5C%7B+0+%2C+X_1+%2B+%5Cldots+X_t+%5C%7D+%5Cleq+%5Cmathop%7B%5Cmathbb+E%7D+%7C+X_1+%2B+%5Cldots+%2B+X_t+%7C+%5Cleq+%5Csqrt+%7B%5Cmathop%7B%5Cmathbb+E%7D+%28X_1+%2B+%5Cldots+%2B+X_t%29%5E2%7D+%3D+%5Csqrt%7B2t%2Fd%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} c_s = \mathop{\mathbb E} \min \{ 0 , X_1 + \ldots X_t \} \leq \mathop{\mathbb E} | X_1 + \ldots + X_t | \leq \sqrt {\mathop{\mathbb E} (X_1 + \ldots + X_t)^2} = \sqrt{2t/d} " class="latex" /></p>
<p> Combining everything, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_%7B%28s_1%2C%5Cldots%2Cs_t%29+%5Csim+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_{(s_1,\ldots,s_t) \sim S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bs%5Cin+S%7D+%5Cmathop%7B%5Cmathbb+E%7D+c_s+%7C%7C+x_s+-+x_0+%7C%7C%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \leq \sum_{s\in S} \mathop{\mathbb E} c_s || x_s - x_0 ||^2" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%5Cleft%28+%5Csqrt+%7B%5Cfrac+td%7D+%5Cright%29+%5Ccdot+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \leq O\left( \sqrt {\frac td} \right) \cdot \sum_{s\in S} || x_s - x_0 ||^2" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+O%28%5Csqrt%7Bdt%7D%29+%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = O(\sqrt{dt}) \mathop{\mathbb E}_{s\sim S} || x_s - x_0 ||^2 " class="latex" /></p>
<p>So every solution of ARV for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is also a solution for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> with a cost that increases at most by a <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7Bdt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt {dt})}" class="latex" /> factor, which proves <a href="https://lucatrevisan.wordpress.com/feed/#rw">(4)</a> as promised.</p>
<p><b>4. A Couple of Interesting Questions </b></p>
<p>We showed that ARV has integrality gap at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Abelian Cayley graph, but we did not demonstrate a rounding algorithm able to achieve an <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> approximation ratio.</p>
<p>If we follow the argument, starting from an ARV solution of cost <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" />, choosing <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/d\epsilon^2}" class="latex" /> we see that <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> has an ARV solution (the same as before) of cost at most, say, <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" />, and so <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%5Et%29+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G^t) \leq 1/2}" class="latex" />, implying that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29+%5Cleq+O%281%2Ft%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G) \leq O(1/t)}" class="latex" /> and so Fiedler’s algorithm according to a Laplacian eigenvalue finds a cut of sparsity at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7B1%2Ft%7D%29+%3D+O%28%5Csqrt+d+%5Ccdot+%5Cepsilon+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt{1/t}) = O(\sqrt d \cdot \epsilon )}" class="latex" />.</p>
<p>We can also see that if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is the matrix corresponding to an ARV solution of value <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, then one of the eigenvectors <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> must be a test vector of Rayleigh quotient at most <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" /> for the Laplacian <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> is order of <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/d\epsilon^2}" class="latex" />. However it is not clear how to get, from such a vector, a test vector for the Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> of Rayleigh quotient at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7B1%2Ft%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt {1/t} )}" class="latex" /> for the Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, though one such vector should be <img src="https://s0.wp.com/latex.php?latex=%7BA%5Ek+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^k x}" class="latex" /> for a properly chosen <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> in the range <img src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2C+t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1,\ldots, t}" class="latex" />.</p>
<p>If this actually work, then the following, or something like that, would be a rounding algorithm: given a PSD solution <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> of ARV of cost <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" />, consider PSD matrices of the form <img src="https://s0.wp.com/latex.php?latex=%7BA%5Et+X+A%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^t X A^t}" class="latex" />, which do not necessarily satisfy the triangle inequalities any more, for <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%2C%5Cldots%2C+1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t = 1,\ldots, 1/d\epsilon^2}" class="latex" />, and try to round them using a random hyperplane. Would this make sense for other classes of graphs?</p>
<p>It is plausible that sparsest cut in Abelian Cayley graphs has actually a constant-factor approximation in polynomial or quasi-polynomial time, and maybe either that ARV achieves constant-factor approximation.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/08/arv-on-abelian-cayley-graphs/"><span class="datestr">at October 08, 2021 02:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19194">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/">Physics Nobel Prize for 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>If I could explain it to the average person, I wouldn’t have been worth the Nobel Prize— Richard Feynman</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/gp/" rel="attachment wp-att-19196"><img width="170" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/gp.png?resize=170%2C170&amp;ssl=1" class="aligncenter wp-image-19196" height="170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite from front-page <a href="https://www.nobelprize.org/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Giorgio Parisi has just been awarded the 2021 Physics Nobel Prize for his work on the disorder in systems of all kinds. At the same time Syukuro Manabe and Klaus Hasselmann won for their joint work on the physical modeling of the Earth’s climate and reliably predicting global warming.</p>
<p>
Today Ken and I look at some aspects of this year’s <a href="https://www.nobelprize.org/prizes/physics/2021/summary/">prizes</a>.</p>
<p>
Small issue: I do believe in human-caused global warming, but shouldn’t the citation read: <i>for reliably predicting the future temperature of the planet</i>? Currently it reads a bit like <i>for reliably predicting the <b>rise</b> in the global stock market</i>, as if the outcome not the model were primary. </p>
<p>
Oh well. Every year there are Nobel prizes awarded for things that have at least some computational aspect. This year’s prize is certainly for something related to computation. One aspect that we are hoping for is Aspect—Alain Aspect. Ken was among <a href="https://www.insidescience.org/news/nine-nobel-prize-predictions-2021">those</a> <a href="https://www.france24.com/en/live-news/20211005-invisibility-cloak-and-quantum-physics-tipped-for-nobel-prize">tipping</a> him, John Clauser, and Anton Zeilinger for this year’s prize—for their work demonstrating quantum entanglement and non-classical experimental outcomes. This work spans substantial areas of quantum computing.</p>
<p>
</p><p></p><h2> General Comments </h2><p></p>
<p></p><p>
The Nobel committee’s technical <a href="https://www.nobelprize.org/uploads/2021/10/sciback_fy_en_21.pdf">review</a> of the prizewinning trio’s accomplishments ends with an expansive comment: </p>
<blockquote><p><b> </b> <em> Clearly this year’s Laureates have made groundbreaking contributions to our understanding of complex physical systems in their broadest sense, from the microscopic to the global. They show that without a proper accounting of disorder, noise and variability, determinism is just an illusion. Indeed, the work recognized here reflects in part the comment ascribed to Richard Feynman (Nobel Laureate 1965), that he “Believed in the primacy of doubt, not as a blemish on our ability to know, but as the essence of knowing.”</em></p><em>
</em><p><em>
Recognizing the work of this troika reflects the importance of understanding that no single prediction of anything can be taken as inviolable truth, and that without soberly probing the origins of variability we cannot understand the behavior of any system. Therefore, only after having considered these origins do we understand that global warming is real and attributable to our own activities, that a vast array of the phenomena we observe in nature emerge from an underlying disorder, and that embracing the noise and uncertainty is an essential step on the road towards predictability. </em>
</p></blockquote>
<p></p><p>
Ken says about the global-warming part of the last sentence that all you need to do is ask a wine grower, of which there are many in the Niagara region. The latitudes at which certain grape strains thrive have been <a href="https://www.economist.com/graphic-detail/2019/11/22/climate-change-is-forcing-winemakers-to-move-further-from-the-equator">shifting</a> <a href="https://www.economist.com/europe/2021/07/15/climate-change-is-affecting-wine-flavours">recently</a> and <a href="https://www.winemag.com/2020/02/03/wine-climate-change/">steadily</a> <a href="https://www.nytimes.com/interactive/2019/10/14/dining/drinks/climate-change-wine.html">north</a>.</p>
<p>
For some general comments by Parisi, see his <a href="https://www.nobelprize.org/prizes/physics/2021/parisi/interview/">interview</a>. He says lots of neat stuff including:</p>
<blockquote><p><b> </b> <em> Well, things that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> Well, my mentor Nicola Cabibbo was usually saying that we should work on a problem only if working on the problem is fun. So, I mean, fun is not very clear what it means, but it’s something that we find deeply interesting, and that we strongly believe that it is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> I mean you won’t find fun in <i>unclear</i> because one gets a new idea of something unexpected and so on. So I tried to work on something that was interesting and which I believed that had some capacity to add something. </em>
</p></blockquote>
<p>
</p><p></p><h2> His Trick </h2><p></p>
<p></p><p>
The Nobel citation says: “One of the many theoretical tools Professor Parisi has used to establish his theory is the so-called ‘replica trick’—a mathematical method which takes a disordered system, replicates it multiple times, and compares how different replicas of the system behave. You can do this, for instance, by compressing marbles in a box, which will form a different configuration each time you make the compression. Over many repetitions, Parisi knew, telling patterns might emerge.” They point to a <a href="https://arxiv.org/abs/1409.2722">paper</a> from talks by Parisi in 2013 also involving Flaviano Morone, Francesco Caltagirone, and Elizabeth Harrison. </p>
<p>
The trick has a Wikipedia <a href="https://en.wikipedia.org/wiki/Replica_trick">page</a>, which says that its crux “is that while the disorder averaging is done assuming <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> to be an integer, to recover the disorder-averaged logarithm one must send <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> continuously to zero. This apparent contradiction at the heart of the replica trick has never been formally resolved, however in all cases where the replica method can be compared with other exact solutions, the methods lead to the same results.” </p>
<p>
The mathematical identity underlying the replica trick is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+Z+%3D+%5Clim_%7Bn%5Crightarrow+0%7D%5Cfrac%7BZ%5En+-+1%7D%7Bn%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \ln Z = \lim_{n\rightarrow 0}\frac{Z^n - 1}{n}. " class="latex" /></p>
<p>The <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z}" class="latex" /> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition function</a> of a physical system or some related thermodynamical measure. The power <img src="https://s0.wp.com/latex.php?latex=%7BZ%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z^n}" class="latex" /> represents <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent copies of the system—if <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is an integer, that is. But instead we treat <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> as a real number going to zero. The formal justification is best seen via a Taylor series expansion: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7BZ%5En+-+1%7D%7Bn%7D+%26%3D%26+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B-1+%2B+e%5E%7Bn+%5Cln+Z%7D%7D%7Bn%7D%5C%5C+%26%3D%26+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B-1+%2B+1+%2B+n+%5Cln+Z+%2B+%5Cfrac%7B1%7D%7B2%21%7D+%28n+%5Cln+Z%29%5E2+%2B+%5Cfrac%7B1%7D%7B3%21%7D+%28n+%5Cln+Z%29%5E3+%2B+%5Cdots%7D%7Bn%7D%5C%5C+%26%3D%26+%5Cln+Z+%2B+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cfrac%7B1%7D%7B2%21%7D+%28n+%5Cln+Z%29%5E2+%2B+%5Cfrac%7B1%7D%7B3%21%7D+%28n+%5Cln+Z%29%5E3+%2B+%5Cdots%7D%7Bn%7D%5C%5C+%26%3D%26+%5Cln+Z.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  \lim_{n \rightarrow 0} \frac{Z^n - 1}{n} &amp;=&amp; \lim_{n \rightarrow 0} \frac{-1 + e^{n \ln Z}}{n}\\ &amp;=&amp; \lim_{n \rightarrow 0} \frac{-1 + 1 + n \ln Z + \frac{1}{2!} (n \ln Z)^2 + \frac{1}{3!} (n \ln Z)^3 + \dots}{n}\\ &amp;=&amp; \ln Z + \lim_{n \rightarrow 0} \frac{\frac{1}{2!} (n \ln Z)^2 + \frac{1}{3!} (n \ln Z)^3 + \dots}{n}\\ &amp;=&amp; \ln Z. \end{array} " class="latex" /></p>
<p>
But on the whole, this strikes me as a silly idea. Suppose that you have a nasty function <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%2C2%2C3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=1,2,3,\dots}" class="latex" />. How do you try to understand the behavior of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" />? Several ideas come to mind: </p>
<ol>
<li>
Try to see how <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> grows as <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \rightarrow \infty}" class="latex" />? <p></p>
</li><li>
Try to get an approximate formula for <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> grows? <p></p>
</li><li>
Try to understand <img src="https://s0.wp.com/latex.php?latex=%7Bf%280%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(0)}" class="latex" />?
</li></ol>
<p>Wait, this is nuts. How can the value <img src="https://s0.wp.com/latex.php?latex=%7Bf%280%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(0)}" class="latex" /> help us understand the limit of 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%281%29%2Cf%282%29%2C%5Cdots%2Cf%281000000%29%2C%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(1),f(2),\dots,f(1000000),\dots " class="latex" /></p>
<p>Indeed.  Another form is that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Cpartial+Z%5En%7D%7B%5Cpartial+n%7D+%3D+%5Cfrac%7B%5Cpartial+e%5E%7Bn%5Cln+Z%7D%7D%7B%5Cpartial+n%7D+%3D+%28%5Cln+Z%29Z%5En+%5Cto+%5Cln+Z+%5Ctext+%7B+as+%7D+n+%5Cto+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\partial Z^n}{\partial n} = \frac{\partial e^{n\ln Z}}{\partial n} = (\ln Z)Z^n \to \ln Z \text { as } n \to 0. " class="latex" /></p>
<p>
The trick here is that instead of letting <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> go to infinity they set <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> to zero. This is crazy. But it is so crazy that it yields a ton of insight into the behavior for large <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. I wish I could understand this better. Perhaps it could help us with our problems like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" />? </p>
<p>
Indeed, one of Parisi’s main applications is to <a href="https://en.wikipedia.org/wiki/Spin_glass">spin glass</a> models, which are <a href="https://www.quora.com/What-is-the-relationship-between-the-Ising-model-and-spin-glasses">related</a> to the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> and likewise have associated problems that are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-hard. This <a href="https://arxiv.org/abs/1810.05907">applies</a> even to a spin-glass model for which Parisi showed that exact solutions are computable.</p>
<p>
</p><p></p><h2> Manabe and Hasselmann </h2><p></p>
<p></p><p>
The prize for Manabe is well summed by the title to this Italian news <a href="https://www.greenandblue.it/2021/10/06/news/syukuro_suki_manabe_lo_scienziato_che_ha_messo_il_clima_nei_computer-321057100/?ref=drla-1">article</a>, which translates as, “The scientist who put the climate into computers.” The article’s subhead makes a point that deserves further reflection: “Without his models it would have been impossible to do experiments on the climate.” Stated positively and picking up “reliable” from the Nobel citation, the point is:</p>
<blockquote><p><b> </b> <em> Having reliable climate models has made it possible to do experiments on the climate. </em>
</p></blockquote>
<p></p><p>
Now we cannot actually <em>do</em> experiments <em>on</em> the climate—short of setting off a <a href="https://en.wikipedia.org/wiki/Nuclear_winter">nuclear winter</a> or erecting a <a href="https://en.wikipedia.org/wiki/Dyson_sphere">Dyson sphere</a>, maybe. We can observe changes caused by <a href="https://en.wikipedia.org/wiki/El_Nino">El Niño</a> events and shifts in the jet stream, for instance, and try to build the framework of a controlled experiment around them. But that’s all. </p>
<p>
So what is meant is that the models are robust enough, and have proven themselves on predictions at smaller or broader scales, that we can confidently regard computational experiments with them as indicative of real-world outcomes. The article mentions being able to tell what would happen if we made mountains disappear or shuffled the continents around. But in line with what we said in the intro, the logic sounds circular or self-confirming unless its reasonableness is explained more. The Washington Post <a href="https://www.washingtonpost.com/science/2021/10/05/nobel-prize-physics/">article</a> on the prizes stops short of saying that Manabe’s modeling of the effect of atmospheric carbon dioxide is a truly confirmed prediction, but it does say that his 50 years of modeling choices have had an almost 1.000 batting average.</p>
<p>
Hasselmann is hailed for supplying a different piece that promotes confidence and accounting of causality. This is to demonstrate consistent effects of short-term local weather events on longer-term global climate. To those like us not versed in the background, this might again sound circular: didn’t the global configuration cause the local event? One particular effect Hasselmann traced is of weather events on ocean currents. The chain from atmosphere to storm to ocean sub-surface is non-circular. </p>
<p>
Perhaps all this can be put as pithily as John Wheeler’s non-circular explanation of general relativity: “Matter tells space-time how to curve, and curved space-time tells matter how to move.” But until then, we feel a need to hallow a more fundamental story of how computational modeling works and why it is effective.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
One day we will see a more computationally-oriented Nobel Prize, but how soon? Until then, best to all working on theory. You are Laureates in our eyes.</p>
<p>
[added equation with derivatives to “Trick” section]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/"><span class="datestr">at October 08, 2021 04:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/07/research-team-leader-at-ideas-ncbr-ltd-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/07/research-team-leader-at-ideas-ncbr-ltd-apply-by-december-31-2021/">Research Team Leader at IDEAS NCBR Ltd.  (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Place of work: Warsaw, Poland</p>
<p>The newly established IDEAS NCBR institute is looking for a position of Research Team Leader dealing with formal modeling and proving the security of cryptographic protocols used in blockchain technology. The research will be carried out in cooperation with the cryptography and blockchain laboratory headed by prof. Stefan Dziembowski at the University of Warsaw.</p>
<p>Website: <a href="https://ideas-ncbr.pl/en/research-team-leader/">https://ideas-ncbr.pl/en/research-team-leader/</a><br />
Email: jobs@ideas-ncbr.pl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/07/research-team-leader-at-ideas-ncbr-ltd-apply-by-december-31-2021/"><span class="datestr">at October 07, 2021 01:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4562">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/07/buser-inequalities-in-graphs/">Buser Inequalities in Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>As life is tentatively returning to normal, I would like to once again post technical material here. Before returning to online optimization, I would like to start with something from 2015 that we never wrote up properly, that has to do with <em>graph curvature</em> and with <em>Buser inequalities in graphs</em>.</p>
<p><span id="more-4562"></span></p>
<p>The starting point is the Cheeger inequalities on graphs.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7BG%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G= (V,E)}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular graph, and <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> is its adjacency matrix, then we define the <em>normalized Laplacian matrix</em> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7BL%3A+%3D+I+-+A%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L: = I - A/d}" class="latex" />, and we call <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> the second smallest (counting multiplicities) eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. It is important in the spectral theory of graphs that this eigenvalue has a variational characterization as the solution of the following optimization problem:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%3D+%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5EV+%3A+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle%3D0%7D+%5C+%5Cfrac+%7B+x%5ET+L+x%7D%7B%7C%7Cx%7C%7C%5E2%7D+%3D+%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5EV+%3A+%5Clangle+x%2C%7B%5Cbf+1%7D+%5Crangle%3D0%7D+%5C+%5Cfrac+%7B+%5Csum_%7B%28u%2Cv%29+%5Cin+E%7D+%28x_u+-+x_v%29%5E2%7D%7Bd%5Csum_v+x_v%5E2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 = \min_{x \in {\mathbb R}^V : \langle x, {\bf 1} \rangle=0} \ \frac { x^T L x}{||x||^2} = \min_{x \in {\mathbb R}^V : \langle x,{\bf 1} \rangle=0} \ \frac { \sum_{(u,v) \in E} (x_u - x_v)^2}{d\sum_v x_v^2 } " class="latex" /></p>
<p>The <em>normalized edge expansion</em> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%3D+%5Cmin_%7BS+%3A+%7CS%7C+%5Cleq+%7CV%7C%2F2%7D+%5C+%5Cfrac%7B+cut%28S%29+%7D%7Bd%7CS%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) = \min_{S : |S| \leq |V|/2} \ \frac{ cut(S) }{d|S|} " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bcut%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{cut(S)}" class="latex" /> denotes the number of edges with one endpoint in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> and one endpoint outside <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />. We have talked about these quantities several times, so we will just jump to the fact that the following <em>Cheeger inequalities</em> hold:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+%7B%5Clambda_2+%7D2+%5Cleq+%5Cphi%28G%29+%5Cleq+%5Csqrt+%7B2+%5Clambda_2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac {\lambda_2 }2 \leq \phi(G) \leq \sqrt {2 \lambda_2} " class="latex" /></p>
<p>Those inequalities are called Cheeger inequality because the upper bound is the discrete analog of a result of Cheeger concerning Riemann manifolds. There were previous posts about this <a href="https://lucatrevisan.wordpress.com/2013/03/20/the-cheeger-inequality-in-manifolds/">here</a> and <a href="https://lucatrevisan.wordpress.com/2013/03/21/proof-of-the-cheeger-inequality-in-manifolds/">here</a>, and for our current purposes it will be enough to recall that, roughly speaking, a Riemann manifold defines a space on which we can define real-valued and complex-valued functions, and such functions can be integrated and differentiated. Subsets <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> of a Riemann manifold have, if they are measurable, a well defined volume, and their boundary <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpartial+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\partial S}" class="latex" /> a well defined lower-dimensional volume; it possible to define a Cheeger constant of a manifold in a way that is syntactically analogous to the definition of edge expansion:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%28M%29+%3D+%5Cmin_%7BS+%3A+vol%28S%29+%5Cleq+vol%28M%29%2F2%7D+%5C+%5C+%5C+%5Cfrac%7Bvol%28%5Cpartial+S%29%7D%7Bvol%28S%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi (M) = \min_{S : vol(S) \leq vol(M)/2} \ \ \ \frac{vol(\partial S)}{vol(S)} " class="latex" /></p>
<p>It is also possible to define a Laplacian operator <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> on smooth functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+M+%5Crightarrow+%7B%5Cmathbb+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: M \rightarrow {\mathbb C}}" class="latex" />, and if <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is the second smallest eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> we have the characterization</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%3D+%5Cmin_%7Bf+%3A+M+%5Crightarrow+%7B%5Cmathbb+R%7D+%5C+%5C+%7B%5Crm+smooth%7D%2C+%5Cint_M+f+%3D+0%7D+%5C+%5C+%5Cfrac%7B+%5Clangle+Lf+%2C+f%5Crangle%7D%7B%5Clangle+f%2Cf%5Crangle%7D+%3D+%5Cmin_%7Bf+%3A+M+%5Crightarrow+%7B%5Cmathbb+R%7D+%5C+%5C+%7B%5Crm+smooth%7D%2C+%5Cint_M+f+%3D+0%7D+%5C+%5C+%5Cfrac%7B+%5Cint_M+%7C%7C%5Cnabla+f%7C%7C%5E2%7D%7B%5Cint_M+f%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 = \min_{f : M \rightarrow {\mathbb R} \ \ {\rm smooth}, \int_M f = 0} \ \ \frac{ \langle Lf , f\rangle}{\langle f,f\rangle} = \min_{f : M \rightarrow {\mathbb R} \ \ {\rm smooth}, \int_M f = 0} \ \ \frac{ \int_M ||\nabla f||^2}{\int_M f^2}" class="latex" /></p>
<p> and Cheeger proved</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28M%29+%5Cleq+2+%5Csqrt%7B%5Clambda_2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(M) \leq 2 \sqrt{\lambda_2 } " class="latex" /></p>
<p> which is syntactically almost the same inequality and, actually, has a syntactically very similar proof (the extra <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt 2}" class="latex" /> factor comes from the fact that things are normalized slightly differently).</p>
<p>The “dictionary” between finite graphs and compact manifolds is that vertices correspond to points, degree correspond to dimensionality, adjacent vertices <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w}" class="latex" /> correspond to “infinitesimally close” points along one dimension, and the set of edges in the cut <img src="https://s0.wp.com/latex.php?latex=%7B%28S%2CV-S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(S,V-S)}" class="latex" /> correspond to the boundary <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpartial+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\partial S}" class="latex" /> of a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, “volume” corresponds to number of edges (both when we think of volume of the boundary and volume of a set), vectors <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%7B%5Cmathbb+R%7D%5EV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in {\mathbb R}^V}" class="latex" /> correspond to smooth functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+M+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: M \rightarrow {\mathbb R}}" class="latex" />, and the collection of values of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> at the neighbors of a vertex <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%3A+%28u%2Cv%29+%5Cin+E+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v : (u,v) \in E \}}" class="latex" /> corresponds to the gradient <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f%28u%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\nabla f(u)}" class="latex" /> of a function at point <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" />.</p>
<p>Having made this long premise, the point of this post is that the inequality</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%5Cgeq+%5Cfrac+%7B%5Clambda_2%7D2+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) \geq \frac {\lambda_2}2 \ , " class="latex" /></p>
<p> which is the easy direction to show in the graph case, does not hold for manifolds.</p>
<p>The easy proof for graphs is that if <img src="https://s0.wp.com/latex.php?latex=%7B%28S%2CV-S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(S,V-S)}" class="latex" /> is the cut that realizes the minimum edge expansion, we can take <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+%7B%5Cbf+1%7D_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = {\bf 1}_S}" class="latex" />, or rather the projection of the above vector on the space orthogonal to <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\bf 1}}" class="latex" />, and a two-line calculation gives the inequality.</p>
<p>If, however, <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> is a subset of points of the manifold that realizes the Cheeger constant, we cannot define <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(\cdot)}" class="latex" /> to be the indicator of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, because the function would not be smooth and its gradient would be undefined.</p>
<p>We could think of rescuing the proof by taking a smoothed version of the indicator of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, something that goes to 1 on one side and to 0 on the other side of the cut, as a function of the distance from the boundary. The “quadratic form” of such a function, however, would depend not just on the area of the boundary of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, but also on how quickly the volume of the subset of the manifold at distance <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" /> from the boundary of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> grows as a function of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" />.</p>
<p>If the Ricci curvature of the manifold is negative, this volume grows very quickly, and the quadratic form of the smoothed function is very bad. The graph analog of this would be a graph made of two expanders joined by a bridge edge, and the problem is that the analogy between graphs and manifolds breaks down because “infinitesimal distance” in the manifold corresponds to any <img src="https://s0.wp.com/latex.php?latex=%7Bo%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{o(\log n)}" class="latex" /> distance on the graph, and although the bridge edge is a sparse cut in the graph, it is crossed by a lot of paths of length <img src="https://s0.wp.com/latex.php?latex=%7Bo%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{o(\log n)}" class="latex" />, or at least this is the best intuition that I have been able to build about what breaks down here in the analogy between graphs and manifolds.</p>
<p>If the Ricci curvature of the manifold is non-negative and the dimension is bounded, there is however an inequality in manifolds that goes in the direction of the “easy graph Cheeger inequality,” and it has the form</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%5Cleq+10+%5Cphi%5E2+%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 \leq 10 \phi^2 (M) " class="latex" /></p>
<p> This is the <em>Buser inequality</em> for manifolds of non-negative Ricci curvature, and note how strong it is: it says that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28M%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(M)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Clambda_2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{\lambda_2}}" class="latex" /> approximate each other up to the constant factor <img src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt%7B10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\sqrt{10}}" class="latex" />.</p>
<p>If the Ricci curvature <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" /> is negative, and <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is the dimension of the manifold, then one has the inequality</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%5Cleq+2+%5Csqrt%7B%7CR%7C+%5Ccdot+%28n-1%29%7D+%5Ccdot+%5Cphi%28M%29+%2B+10+%5Cphi%5E2+%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 \leq 2 \sqrt{|R| \cdot (n-1)} \cdot \phi(M) + 10 \phi^2 (M) " class="latex" /></p>
<p>Given the importance that curvature has in the study of manifolds, there has been considerable interest in defining a notion of curvature for graphs. For example curvature relates to how quickly balls around a point grow with the diameter the ball, a concept that is of great importance in graphs as well; curvature is a locally defined concept that has a global consequences, and in graph property testing one is precisely interested in understanding global properties based on local ones; and a Buser-type inequality in graphs would be very interesting because it would provide a class of graphs for which Fiedler’s algorithm provides constant-factor approximation for sparsest cut.</p>
<p>There have been multiple attempts at defining notions of curvature for graphs, the main ones being Olivier curvature and Bakry-Emery curvature. Each captures some but not all of the useful properties of Ricci curvature in manifolds. My (admittedly, poorly informed) intuition is that curvature in manifold is defined “locally” but, as we mentioned above, “local” in graphs can have multiple meanings depending on the distance scale, and it is difficult to come up with a clean and usable definition that talks about multiple distance scales.</p>
<p>Specifically to the point of the Buser inequality, <a href="https://arxiv.org/pdf/1306.2561.pdf">Bauer et al</a> and <a href="https://arxiv.org/pdf/1501.00516.pdf">Klartag et al</a> prove Buser inequalities for graphs with respect to the Bakry-Emery definition. Because Cayley graphs of Abelian groups happen to have curvature 0 according to this definition, their work gives the following result:</p>
<blockquote><p><b>Theorem 1 (Buser inequality for Cayley graphs of Abelian groups)</b> <em> If <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G=(V,E)}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Cayley graph of an Abelian group, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is the second smallest normalized Laplacian eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> is the normalized edge expansion of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, we have </em></p>
<p><em></em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%5Cleq+O%28d%29+%5Ccdot+%5Cphi%5E2+%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 \leq O(d) \cdot \phi^2 (G) " class="latex" /></p>
</em><p><em></em><em> </em></p></blockquote>
<p>(Klartag et al. state the result as above; Bauer et al. state it only for certain groups, but I think that their proof applies to all Abelian groups)</p>
<p>In particular, the above statement implies that if we have a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Abelian Cayley graph then <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7B%5Clambda_2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt {\lambda_2}}" class="latex" /> provides an approximation of the sparsest cut up to a multiplicative error <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> and that Fiedler’s algorithm is an <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> approximate algorithm for sparsest cut in Abelian Cayley graphs.</p>
<p>At some point in 2015 (or maybe 2014, during the Simons program on spectral graph theory?), I read the paper of Bauer at al. and wondered about a few questions. First of all, is there a way to prove the above theorem without using any notion of curvature?</p>
<p>Secondly, the theorem implies that Fiedler’s algorithm has a good approximation, but it does so in a very unusual way: we have <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> that is a continuous relaxation of the sparsest cut problem, and we have Fiedler’s algorithm that, as analyzed by Cheeger’s inequality, provides a somewhat bad rounding, and finally the Buser inequality is telling us that the relaxation has very poor integrality gap on all Abelian Cayley graphs, and so even though the rounding seems bad it is actually good compared to the integral optimum. There should be an underlying reason for all this, meaning some other relaxation that has an actual integrality gap for sparsest cut that is at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" />.</p>
<p>I mentioned these questions to Shayan Oveis Gharan, and we were able to prove that if <img src="https://s0.wp.com/latex.php?latex=%7BARV%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{ARV(G)}" class="latex" /> is the value of the Arora-Rao-Vazirani (or Goemans-Linial) semidefinite programming relaxation of sparsest cut, then, for all <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Cayley graphs, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%5Cleq+O%28d%29+%5Ccdot+%28ARV%28G%29%29%5E2+%5Cleq+O%28d%29+%5Ccdot+%5Cphi%5E2+%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 \leq O(d) \cdot (ARV(G))^2 \leq O(d) \cdot \phi^2 (G) " class="latex" /></p>
<p> which proves the above theorem and, together with the Cheeger inequality <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2+%5Cgeq+%5Cphi%5E2%28G%29%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2 \geq \phi^2(G)/2}" class="latex" />, also shows</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%28G%29+%5Cleq+O%28%5Csqrt+d%29+%5Ccdot+ARV%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi (G) \leq O(\sqrt d) \cdot ARV(G) " class="latex" /></p>
<p> that is, the ARV relaxation has integrality gap at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> on Abelian Cayley graphs. I will discuss the proof (which is a sort of “sum-of-squares version” of the proof of Bauer et al.) in the next post.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/07/buser-inequalities-in-graphs/"><span class="datestr">at October 07, 2021 10:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/07/faculty-positions-at-all-levels-at-university-at-buffalo-apply-by-november-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/07/faculty-positions-at-all-levels-at-university-at-buffalo-apply-by-november-30-2021/">Faculty positions at all levels at University at Buffalo (apply by November 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We have 6 tenure/tenure track position out of which one is focused on theory. We are looking for folks in both algorithms and complexity. (The Nov 30 deadline is a suggestion, we will be considering applications a bit after that as well.)</p>
<p>Website: <a href="https://www.ubjobs.buffalo.edu/postings/30481">https://www.ubjobs.buffalo.edu/postings/30481</a><br />
Email: atri@buffalo.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/07/faculty-positions-at-all-levels-at-university-at-buffalo-apply-by-november-30-2021/"><span class="datestr">at October 07, 2021 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/06/postdoc-at-durham-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/06/postdoc-at-durham-university-apply-by-december-1-2021/">Postdoc at Durham University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I’m looking for a postdoctoral researcher from Jan 22 to work on an EPSRC-funded project on password-hashing algorithms and idealized models of computation at Durham Uni. Applicants with backgrounds in Cryptography, Algorithms and Complexity are very welcome. Durham is one of the top universities in the UK, &amp; the CS dept. hosts one of the strongest theory groups across the ACiD and NESTiD groups.</p>
<p>Website: <a href="https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/V034065/1">https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/V034065/1</a><br />
Email: pooya.farshim@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/06/postdoc-at-durham-university-apply-by-december-1-2021/"><span class="datestr">at October 06, 2021 04:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1576">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/10/news-for-september-2021/">News for September 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A nice array of papers for this month, ranging from distribution testing, dense graph property testing, sublinear graph algorithms, and various mixtures of them. Let us now sample our spread! <em>(Adding another semistreaming paper who achieved similar results to another posted paper. -Ed)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>A Lower Bound on the Complexity of Testing Grained Distributions</strong> by Oded Goldreich and Dana Ron (<a href="https://eccc.weizmann.ac.il/report/2021/129/">ECCC</a>). A discrete distribution is called \(m\)-grained if all probabilities are integer multiples of \(1/m\). This paper studies the complexity of testing this property of distributions. For simplicity, consider the property of being \(n/2\)-grained, where the support size is \(n\). The classic lower bound for testing uniformity shows that \(\Omega(\sqrt{n})\) samples are required to distinguish the uniform distribution from a distribution uniform on \(n/2\) elements. Thus, we get a lower bound of \(\Omega(\sqrt{n})\) for testing \(n/2\)-grainedness (if I am permitted to use that word). This paper proves a lower bound of \(\Omega(n^c)\), for all constant \(c &lt; 1\). It is conjectured that the lower bound is actually \(\Omega(n/\log n)\), which would match the upper bound (for any label-invariant property).</p>



<p><strong>Testing Distributions of Huge Objects</strong> by Oded Goldreich and Dana Ron (<a href="https://eccc.weizmann.ac.il/report/2021/133/">ECCC</a>). This paper introduced a new model that marries distribution testing with property testing on strings. The “object” of interest is a distribution \(\mathcal{D}\) over strings of length \(n\). We wish to test if \(\mathcal{D}\) possesses some property. The tester can get a random string \(x\) from the distribution, and can query any desired index of \(x\). The distance between distributions is defined using the earthmover distance (where we use the Hamming distance between strings). This model is called the DoHO (Distributions of Huge Objects) model. There are many questions posed and connections drawn to classical property testing and distribution testing. What I find interesting is a compelling application: the distribution \(\mathcal{D}\) may represent noisy or perturbed versions of a single object. The DoHO model gives a natural generalization of standard property testing to noisy objects. This paper considers problems such as testing if \(\mathcal{D}\) is: a random perturbation of a string, or a random cyclic shift, or a random isomorphism of a graph. </p>



<p><strong>Sublinear Time and Space Algorithms for Correlation Clustering via Sparse-Dense Decompositions</strong> by Sepehr Assadi and Chen Wang (<a href="https://arxiv.org/pdf/2109.14528.pdf">arXiv</a>). Correlation clustering is a classic problem where edges in a graph are labeled ‘+’ or ‘-‘, denoting whether these edges should be uncut or cut. The aim is to cluster the graph minimizing the total “disagreements” (cut ‘+’ edges or uncut ‘-‘ edges). This paper gives an \(O(1)\)-approximation algorithm that runs in \(O(n\log^2n)\) time; this is the first sublinear time approximation algorithm for this problem. Correlation clustering has seen results for the property testing/sublinear algorithms community, first by <a href="https://arxiv.org/abs/1312.5105">Bonchi, Garcia Soriano, and Kutzkov</a>. But previous results were essentially on the dense graph model, giving \(O(\epsilon n^2)\) error assuming adjacency matrix input. This paper considers access to the adjacency list of ‘+’ edges. Interestingly (from a technical standpoint), the key tool is a new <em>sparse-dense</em> decomposition. Such decompositions emerged from the seminal work of <a href="https://arxiv.org/abs/1807.08886">Assadi-Chen-Khanna</a> for sublinear \((\Delta+1)\)-colorings, and it is great to see applications beyond coloring.</p>



<p><strong>Sublinear-Time Computation in the Presence of Online Erasures</strong> by Iden Kalemaj, Sofya Raskhodnikova, and Nithin Varma (<a href="https://arxiv.org/abs/2109.08745">arXiv</a>). Can property testing be done when portions of the input are hidden? This question was first raised by <a href="https://arxiv.org/abs/1607.05786">Dixit-Raskhodnikova-Thakurta-Varma</a>, who gave a model of <em>erasure-resilient testing.</em> There is an adversary who hides (erases) part of the input function; queries to those parts just yield a dummy symbol. This paper defines an online version of this model. There is an erasure parameter \(t\). On each query by the property tester, the adversary can erase \(t\) values of the function. Consider the property of classic linearity of functions \(f:\{0,1\}^d \rightarrow \{0,1\}\). The BLR tester queries triples of pairs \((x,y, x \oplus y)\). Observe how this tester is easily defeated by our adversary, by erasing the value \(f(x\oplus y)\). One of the main results of this paper is a \(O(1/\varepsilon)\)-query tester for linearity, that works for any constant erasure parameter \(t\). Note that this matches the bound for the standard setting. There are a number of results for other classic properties, such as monotonicity (sortedness) and Lipschitz.</p>



<p>S<strong>ublinear Time Eigenvalue Approximation via Random Sampling</strong> by Rajarshi Bhattacharjee, Cameron Musco, and Archan Ray (<a href="https://arxiv.org/abs/2109.07647">arXiv</a>). Consider the problem of estimating all the eigenvalues of a real, symmetric \(n \times n\) matrix \(M\) with bounded entries, in sublinear time. The main result shows that the eigenvalues of a uniform random \(O(\epsilon^{-4}\log n)\) principal submatrix can be used to approximate all eigenvalues of \(M\) up to additive error \(\epsilon n\). One can think of this as a sort of concentration inequality for eigenvalues. This result follows (and builds upon) work of <a href="https://arxiv.org/abs/2005.06441">Bakshi-Chepurko-Jayaram</a> on property testing semidefiniteness. The key idea is that eigenvectors corresponding to large eigenvalues have small infinity norm: intuitively, since all entries in \(M\) are bounded, such an eigenvector must have its mass spread out among many coordinates. Hence, we can get information about it by randomly sampling a few coordinates. The paper also shows that approach of taking principal submatrices requires taking \(\Omega(\epsilon^{-2})\) columns/rows.</p>



<p><strong>Deterministic Graph Coloring in the Streaming Model</strong> by Sepehr Assadi, Andrew Chen, and Glenn Sun (arXiv). This is technically not a sublinear algorithms paper<em> (well ok, streaming is sublinear, but we tend not to cover the streaming literature. Maybe we should? – Ed.)</em> But, I think that the connections are of interest to our community of readers. The main tool of sublinear \((\Delta+1)\)-coloring algorithm of <a href="https://arxiv.org/abs/1807.08886">Assadi-Chen-Khanna</a> is the palette sparsification lemma (\(\Delta\) is the maximum degree). This lemma shows that vertices can randomly shorten their ”palette” of colors, after which all colorings from these palettes lead to very few monochromatic edges. This is an immensely powerful tool, since one can get immediately sublinear complexity algorithms in many models: adjacency list, streaming, distributed. Is the randomness necessary? Note that these algorithms run in \(\Omega(n)\) time/space, so it is conceivable that deterministic sublinear algorithms exists. This paper shows that randomization is necessary in the semi-streaming model (space \(O(n poly(\log n))\)). Indeed, there exist no deterministic semi-streaming algorithms that can achieve even \(\exp(\Delta^{o(1)})\) colorings.</p>



<p><strong>Adversarially Robust Coloring for Graph Stream</strong> by Amit Chakrabarti, Prantar Ghosh, and Manuel Stoeckl (<a href="https://arxiv.org/abs/2109.11130">arXiv</a>). This paper studies the same problem as the above, but presents the results in a different way. In a randomized algorithm, we normally think of an adversary that fixes a (hard) input, and the algorithm then makes its random decisions. An adaptive adversary is one that changes the input (stream) based on the decisions of an algorithm. In this definition, a robust algorithm is one that can give correct answers, even for adversarially generated output. A deterministic algorithm is automatically robust. This paper show that there do not exist semi-streaming algorithms that can achieve \((\Delta+1)\)-colorings. The quantitative lower bound is weaker (\(\Omega(\Delta^2)\) colors), but it is against a stronger adversary.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/2021/10/news-for-september-2021/"><span class="datestr">at October 06, 2021 04:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=573">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/10/05/tcs-talk-wednesday-october-13-nutan-limaye-it-university-of-copenhagen/">TCS+ talk: Wednesday, October 13 — Nutan Limaye, IT University of Copenhagen</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place next Wednesday, October 13th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC: <a href="https://www.timeanddate.com/worldclock/converter.html?iso=20211013T170000&amp;p1=tz_pt&amp;p2=tz_et&amp;p3=tz_cest&amp;p4=1440">check your time zone!</a>). <a href="https://www.cse.iitb.ac.in/~nutan/"><strong>Nutan Limaye</strong></a> from IT University of Copenhagen will speak about “<em>Superpolynomial Lower Bounds Against Low-Depth Algebraic Circuits</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Every multivariate polynomial P(X) can be written as a sum of monomials, i.e. a sum of products of variables and field constants. In general, the size of such an expression is the number of monomials that have a non-zero coefficient in P.</p>
<p>What happens if we add another layer of complexity, and consider sums of products of sums (of variables and field constants) expressions? Now, it becomes unclear how to prove that a given polynomial P(X) does not have small expressions. In this result, we solve exactly this problem.</p>
<p>More precisely, we prove that certain explicit polynomials have no polynomial-sized “Sigma-Pi-Sigma” (sums of products of sums) representations. We can also show similar results for Sigma-Pi-Sigma-Pi, Sigma-Pi-Sigma-Pi-Sigma and so on for all “constant-depth” expressions.</p>
<p>The talk is based on a joint work of Nutan Limaye, Srikanth Srinivasan, and Sébastien Tavenas.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/10/05/tcs-talk-wednesday-october-13-nutan-limaye-it-university-of-copenhagen/"><span class="datestr">at October 05, 2021 09:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/05/hasselmann-manabe-and-parisi-win-2021-physics-nobel-prize/">Hasselmann, Manabe and Parisi win 2021 Physics Nobel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img src="https://lucatrevisan.files.wordpress.com/2021/10/itscomingrome-lorenza-parisi-1.jpeg?w=584" alt="itscomingrome-lorenza-parisi" class="alignnone size-full wp-image-4558" /></p>
<p>Today the Italian academic community, along with lots of other people, was delighted to hear that <a href="https://en.wikipedia.org/wiki/Giorgio_Parisi">Giorgio Parisi</a> is one of the three recipients of the 2021 Nobel Prize for Physics.</p>
<p>Parisi has been a giant in the area of understanding “complex” and “disordered” systems. Perhaps, his most influential contribution has been his “replica method” for the analysis of the Sherrington-Kirkpatrick model. His ideas have led to several breakthroughs in statistical physics by Parisi and his collaborators, and they have also found applications in computer science: to tight analyses on a number of questions about combinatorial optimization on random graphs, to results on random constraint satisfaction problems (including the famous connection with random k-SAT analyzed by Mezard, Parisi and Zecchina) and random error correcting codes, and to understanding the solution landscape in optimization problems arising from machine learning. Furthermore these ideas have also led to the development and analysis of algorithms.</p>
<p>The news was particularly well received at Bocconi, where most of the faculty of the future CS department has done work that involved the replica method. (Not to be left out, even I <a href="https://arxiv.org/abs/2008.05648">have recently used replica methods</a>.)</p>
<p>Mezard and Montanari have written a <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">book-length treatment</a> on the interplay between ideas from statistical physics, algorithms, optimization, information theory and coding theory that arise from this tradition. Readers of <em>in theory</em> looking for a shorter exposition aimed at theoretical computer scientists will enjoy these <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">notes posted by Boaz Barak</a>, or <a href="https://windowsontheory.org/2018/03/14/statistical-physics-dictionary/">this even shorter post by Boaz</a>.</p>
<p>In this post, I will try to give a sense to the reader of what the replica method for the Sherrington-Kirkpatrick model looks like when applied to the average-case analysis of optimization problems, stripped of all the physics. Of course, without the physics, nothing makes any sense, and the interested reader should look at Boaz’s posts (and to references that he provides) for an introduction to the context. I did not have time to check too carefully what I wrote, so be aware that several details could be wrong.</p>
<p>What is the typical value of the max cut in a <img src="https://s0.wp.com/latex.php?latex=%7BG_%7Bn%2C%5Cfrac+12%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_{n,\frac 12}}" class="latex" /> random graph with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> vertices?</p>
<p>Working out an upper bound using union bounds and Chernoff bound, and a lower bound by thinking about a greedy algorithm, we can quickly convince ourselves that the answer is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+%7Bn%5E2%7D%7B4%7D+%2B+%5CTheta%28n%5E%7B1.5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac {n^2}{4} + \Theta(n^{1.5})}" class="latex" />. Great, but <em>what is the constant in front of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1.5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{1.5}}" class="latex" />?</em> This question is answered by the <em>Parisi formula</em>, though this fact was not rigorously established by Parisi. (Guerra proved that the formula gives an upper bound, Talagrand proved that it gives a tight bound.)</p>
<p>Some manipulations can reduce the question above to the following question: suppose that I pick a random <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n\times n}" class="latex" /> symmetric matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />, say with zero diagonal, and such that (up to the symmetry requirement) the entries are mutually independent and each entry is equally likely to be <img src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+1}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" />, or perhaps each entry is distributed according to a standard normal distribution (the two versions can be proved to be equivalent), what is the typical value of</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax+_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5C+%5C+%5Cfrac+1%7Bn%5E%7B1.5%7D%7D+x%5ET+M+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \max _{x \in \{+1,1\}^n } \ \ \frac 1{n^{1.5}} x^T M x " class="latex" /></p>
<p> up to <img src="https://s0.wp.com/latex.php?latex=%7Bo_n%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{o_n(1)}" class="latex" /> additive terms,?</p>
<p>As a first step, we could replace the maximum with a “soft-max,” and note that, for every choice of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta&gt;0}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax+_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5C+%5C+x%5ET+M+x+%5Cleq+%5Cfrac+1+%5Cbeta+%5Clog+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \max _{x \in \{+1,1\}^n } \ \ x^T M x \leq \frac 1 \beta \log \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} " class="latex" /></p>
<p> The above upper bound gets tighter and tighter for larger <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta}" class="latex" />, so if we were able to estimate</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Clog+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \log \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} " class="latex" /></p>
<p> for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta}" class="latex" /> (where the expectation is over the randomness of <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />) then we would be in good shape.</p>
<p>We could definitely use convexity and write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Cmax+_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5C+%5C+x%5ET+M+x+%5Cleq+%5Cfrac+1+%5Cbeta+%5Cmathop%7B%5Cmathbb+E%7D+%5Clog+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+%5Cleq+%5Cfrac+1+%5Cbeta+%5Clog+%5Cmathop%7B%5Cmathbb+E%7D+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \max _{x \in \{+1,1\}^n } \ \ x^T M x \leq \frac 1 \beta \mathop{\mathbb E} \log \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} \leq \frac 1 \beta \log \mathop{\mathbb E} \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} " class="latex" /></p>
<p> and then use linearity of expectation and independence of the entries of <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> to get to</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cfrac+1+%5Cbeta+%5Clog+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5Cprod_%7B1%5Cleq+i+%3C+j%5Cleq+n%7D+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B2%5Cbeta+M_%7Bi%2Cj%7D+x_i+x_j+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \leq \frac 1 \beta \log \sum_{x \in \{+1,1\}^n } \prod_{1\leq i &lt; j\leq n} \mathop{\mathbb E} e^{2\beta M_{i,j} x_i x_j } " class="latex" /></p>
<p> Now things simplify quite a bit, because for all <img src="https://s0.wp.com/latex.php?latex=%7Bi%3Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i&lt;j}" class="latex" /> the expression <img src="https://s0.wp.com/latex.php?latex=%7BM_%7Bi%2Cj%7D+x_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_{i,j} x_i x_j}" class="latex" />, in the Rademacher setting, is equally likely to be <img src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+1}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" />, so that, for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta = o(1)}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B2%5Cbeta+M_%7Bi%2Cj%7D+x_i+x_j+%7D+%3D+cosh+%282%5Cbeta%29+%5Cleq+1+%2B+O%28%5Cbeta%5E2%29+%5Cleq+e%5E%7BO%28%5Cbeta%5E2%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} e^{2\beta M_{i,j} x_i x_j } = cosh (2\beta) \leq 1 + O(\beta^2) \leq e^{O(\beta^2)} " class="latex" /></p>
<p> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5Cprod_%7B1%5Cleq+i+%3C+j%5Cleq+n%7D+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B2%5Cbeta+M_%7Bi%2Cj%7D+x_i+x_j+%7D+%5Cleq+2%5En+%5Ccdot+e%5E%7BO%28%5Cbeta%5E2+n%5E2%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{x \in \{+1,1\}^n } \prod_{1\leq i &lt; j\leq n} \mathop{\mathbb E} e^{2\beta M_{i,j} x_i x_j } \leq 2^n \cdot e^{O(\beta^2 n^2)} " class="latex" /></p>
<p> so that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1+%5Cbeta+%5Clog+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+%5Cprod_%7B1%5Cleq+i+%3C+j%5Cleq+n%7D+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B2%5Cbeta+M_%7Bi%2Cj%7D+x_i+x_j+%7D+%5Cleq+%5Cfrac+%7BO%28n%29%7D%7B%5Cbeta%7D+%2B+O%28%5Cbeta+n%5E2%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac 1 \beta \log \sum_{x \in \{+1,1\}^n } \prod_{1\leq i &lt; j\leq n} \mathop{\mathbb E} e^{2\beta M_{i,j} x_i x_j } \leq \frac {O(n)}{\beta} + O(\beta n^2) " class="latex" /></p>
<p> which, choosing <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+1%2F%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta = 1/\sqrt n}" class="latex" />, gives an <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B1.5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(n^{1.5})}" class="latex" /> upper bound which is in the right ballpark. Note that this is exactly the same calculations coming out of a Chernoff bound and union bound. If we optimize the choice of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\beta}" class="latex" /> we unfortunately do not get the right constant in front of <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1.5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{1.5}}" class="latex" />.</p>
<p>So, if we call</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3A%3D+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  F := \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} " class="latex" /></p>
<p> we see that we lose too much if we do the step</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Clog+F+%5Cleq+%5Clog+%5Cmathop%7B%5Cmathbb+E%7D+F+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \log F \leq \log \mathop{\mathbb E} F " class="latex" /></p>
<p> But what else can we do to get rid of the logarithm and to reduce to an expression in which we take expectations of products of independent quantities (if we are not able to exploit the assumption that <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> has mutually independent entries, we will not be able to make progress)?</p>
<p>The idea is that if <img src="https://s0.wp.com/latex.php?latex=%7Bk%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k&gt;0}" class="latex" /> is a small enough quantity (something much smaller than <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Clog+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\log F}" class="latex" />), then <img src="https://s0.wp.com/latex.php?latex=%7BF%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F^k}" class="latex" /> is close to 1 and we have the approximation</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+F%5Ek+%5Capprox+F%5Ek-1+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \log F^k \approx F^k-1 " class="latex" /></p>
<p> and we obviously have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+F%5Ek+%3D+k+%5Clog+F+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \log F^k = k \log F " class="latex" /></p>
<p> so we can use the approximation</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+F+%5Capprox+%5Cfrac+1k+%28F%5Ek+-+1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \log F \approx \frac 1k (F^k - 1) " class="latex" /></p>
<p> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Clog+F+%5Capprox+%5Cfrac+1k+%28%5Cmathop%7B%5Cmathbb+E%7D+F%5Ek+-+1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \log F \approx \frac 1k (\mathop{\mathbb E} F^k - 1) " class="latex" /></p>
<p> Let’s forget for a moment that we want <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> to be a very small parameter. If <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> was an integer, we would have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+F%5Ek+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft%28+%5Csum_%7Bx+%5Cin+%5C%7B%2B1%2C1%5C%7D%5En+%7D+e%5E%7B%5Cbeta+x%5ET+Mx%7D+%5Cright%29%5Ek+%3D+%5Csum_%7Bx%5E%7B%281%29%7D%2C%5Cldots+x%5E%7B%28k%29%7D+%5Cin+%5C%7B%2B1%2C-1%5C%7D%5En%7D+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B%5Cbeta+%5Ccdot+%28+x%5E%7B%281%29+T%7D+M+x%5E%7B%281%29%7D+%2B+%5Ccdots+%2B+x%5E%7B%28k%29T%7D+M+x%5E%7B%28k%29%7D%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} F^k = \mathop{\mathbb E} \left( \sum_{x \in \{+1,1\}^n } e^{\beta x^T Mx} \right)^k = \sum_{x^{(1)},\ldots x^{(k)} \in \{+1,-1\}^n} \mathop{\mathbb E} e^{\beta \cdot ( x^{(1) T} M x^{(1)} + \cdots + x^{(k)T} M x^{(k)}) } " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bx%5E%7B%281%29%7D%2C%5Cldots+x%5E%7B%28k%29%7D+%5Cin+%5C%7B%2B1%2C-1%5C%7D%5En%7D+%5C+%5C+%5Cprod_%7Bi%3C+j%7D+%5C+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7B2%5Cbeta+M_%7Bi%2Cj%7D+%5Ccdot+%28+x%5E%7B%281%29%7D_i+x%5E%7B%281%29%7D_j+%2B+%5Ccdots+%2B+x%5E%7B%28k%29%7D_i+x%5E%7B%28k%29%7D_j+%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = \sum_{x^{(1)},\ldots x^{(k)} \in \{+1,-1\}^n} \ \ \prod_{i&lt; j} \ \mathop{\mathbb E} e^{2\beta M_{i,j} \cdot ( x^{(1)}_i x^{(1)}_j + \cdots + x^{(k)}_i x^{(k)}_j )} " class="latex" /></p>
<p> Note that the above expression involves choices of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />-tuples of feasible solutions of our maximization problem. These are the “replicas” in “replica method.”</p>
<p>The above expression does not look too bad, and note how we were fully able to use the independence assumption and “simplify” the expression. Unfortunately, it is actually still very bad. In this case it is preferable to assume the <img src="https://s0.wp.com/latex.php?latex=%7BM_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_{i,j}}" class="latex" /> to be Gaussian, write the expectation as an integral, do a change of variable and some tricks so that we reduce to computing the maximum of a certain function, let’s call it <img src="https://s0.wp.com/latex.php?latex=%7BG%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G(z)}" class="latex" />, where the input <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k \times k}" class="latex" /> matrix, and then we have to guess what is an input of maximum value for this function. If we are lucky, the maximum is equivalent by a <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> in which all entries are identical, the <em>replica symmetric solution</em>. In the Sherrington-Kirkpatrick model we don’t have such luck, and the next guess is that the optimal <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> is a block-diagonal matrix, or a <em>replica symmetry-breaking solution</em>. For large <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />, and large number of blocks, we can approximate the choice of such matrices by writing down a system of differential equations, the <em>Parisi equations</em>, and we are going to assume that such equations do indeed describe an optimal <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> and so a solution to the integral, and so they give as a computation of <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cmathop%7B%5Cmathbb+E%7D+F%5Ek+-+1%29%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\mathop{\mathbb E} F^k - 1)/k}" class="latex" />.</p>
<p>After all this, we get an expression for <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cmathop%7B%5Cmathbb+E%7D+F%5Ek+-+1%29%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\mathop{\mathbb E} F^k - 1)/k}" class="latex" /> for every sufficiently large integer <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />, as a function of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> up to lower order errors. What next? Remember how we wanted <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> to be a tiny real number and not a sufficiently large integer? Well, we take the expression, we forget about the error terms, and we set <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D0%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k=0\ldots}" class="latex" /></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/05/hasselmann-manabe-and-parisi-win-2021-physics-nobel-prize/"><span class="datestr">at October 05, 2021 08:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/05/tenure-track-faculty-positions-at-portland-state-university-apply-by-november-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/05/tenure-track-faculty-positions-at-portland-state-university-apply-by-november-1-2021/">tenure-track faculty positions at Portland State University (apply by November 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science department at Portland State University has multiple Faculty openings this year. Theory is a top interest area. One position is part of a cluster hire in computational science, which also welcomes theory folks if their research intersects with big data analysis, scalable ML, scientific computing etc.</p>
<p>Website: <a href="https://www.pdx.edu/computer-science/open-faculty-positions">https://www.pdx.edu/computer-science/open-faculty-positions</a><br />
Email: cssearch@pdx.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/05/tenure-track-faculty-positions-at-portland-state-university-apply-by-november-1-2021/"><span class="datestr">at October 05, 2021 08:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5859">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5859">The Physics Nobel, Gaussian BosonSampling, and Dorian Abbot</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>1. Huge congratulations to the <a href="https://www.nobelprize.org/prizes/physics/2021/summary/">winners</a> of this year’s Nobel Prize in Physics: <a href="https://en.wikipedia.org/wiki/Syukuro_Manabe">Syukuro Manabe</a> and <a href="https://en.wikipedia.org/wiki/Klaus_Hasselmann">Klaus Hasselmann</a> for climate modelling, and separately, <a href="https://en.wikipedia.org/wiki/Giorgio_Parisi">Giorgio Parisi</a> for statistical physics.  While I don’t know the others, I had the great honor to get to know Parisi three years ago, when he was chair of the committee that awarded me the <a href="https://www.scottaaronson.com/blog/?p=3955">Tomassoni-Chisesi Prize in Physics</a>, and when I visited Parisi’s department at Sapienza University of Rome to give the prize lecture and collect the award.  I remember Parisi’s kindness, a lot of good food, and a lot of discussion of the interplay between theoretical computer science and physics.  Note that, while much of Parisi’s work is beyond my competence to comment on, in computer science he’s very well-known for applying statistical physics methods to the analysis of <a href="https://arxiv.org/abs/cs/0212002">survey propagation</a>—an algorithm that revolutionized the study of random 3SAT when it was introduced two decades ago.</p>



<hr class="wp-block-separator" />



<p>2. Two weeks ago, a group at Google <a href="https://arxiv.org/abs/2109.11525">put out a paper</a> with a new efficient classical algorithm to simulate the recent Gaussian BosonSampling experiments from USTC in China.  They argued that this algorithm called into question USTC’s claim of BosonSampling-based quantum supremacy.  Since then, I’ve been in contact with Sergio Boixo from Google, Chaoyang Lu from USTC, and Jelmer Renema, a Dutch BosonSampling expert and friend of the blog, to try to get to the bottom of this.  Very briefly, the situation seems to be that Google’s new algorithm outperforms the USTC experiment on one particular metric: namely, <em>total variation distance from the ideal marginal distribution, if (crucially) you look at only a subset of the optical modes</em>, <em>say 14 modes out of 144 total</em>.  Meanwhile, though, if you look at the k<sup>th</sup>-order correlations for large values of k, then the USTC experiment continues to win.  With the experiment, the correlations fall off exponentially with k but still have a meaningful, detectable signal even for (say) k=19, whereas with Google’s spoofing algorithm, you choose the k that you want to spoof (say, 2 or 3), and then the correlations become nonsense for larger k.</p>



<p>Now, given that you were only ever <em>supposed</em> to see a quantum advantage from BosonSampling if you looked at the k<sup>th</sup>-order correlations for large values of k, and given that we already knew, from the work of Leonid Gurvits, that <em>very</em> small marginals in BosonSampling experiments would be easy to reproduce on a classical computer, my inclination is to say that USTC’s claim of BosonSampling-based quantum supremacy still stands.  On the other hand, it’s true that, with BosonSampling especially, more so than with qubit-based random circuit sampling, we currently lack an adequate theoretical understanding of what the <em>target</em> should be.  That is, which numerical metric should an experiment aim to maximize, and how well does it have to score on that metric before it’s plausibly outperforming any fast classical algorithm?  One thing I feel confident about is that, whichever metric is chosen—Linear Cross-Entropy or whatever else—it needs to capture the k<sup>th</sup>-order correlations for large values of k.  No metric that’s insensitive to those correlations is good enough.</p>



<hr class="wp-block-separator" />



<p>3. Like many others, I was outraged and depressed that MIT <a href="https://bariweiss.substack.com/p/mit-abandons-its-mission-and-me">uninvited Dorian Abbot</a> (see also <a href="https://www.newsweek.com/mit-cancels-geophysicists-lecture-after-activists-outrage-over-his-views-diversity-1635371">here</a>), a geophysicist at the University of Chicago, who was slated to give the Carlson Lecture in the Department of Earth, Atmospheric, and Planetary Sciences about the atmospheres of extrasolar planets.  The reason for the cancellation was that, totally unrelatedly to his scheduled lecture, Abbot had <a href="https://www.newsweek.com/diversity-problem-campus-opinion-1618419">argued in <em>Newsweek</em></a> and elsewhere that Diversity, Equity, and Inclusion initiatives should aim for equality for opportunity rather than equality of outcomes, a Twitter-mob decided to go after him in retaliation, and they succeeded.  It should go without saying that it’s perfectly reasonable to <strong>disagree</strong> with Abbot’s stance, to <strong>counterargue</strong>—if those very concepts haven’t gone the way of floppy disks.  It should also go without saying that the MIT EAPS department chair is <em>free</em> to bow to social-media pressure, as he did, rather than standing on principle … just like I’m <em>free</em> to criticize him for it.  To my mind, though, cancelling a scientific talk because of the speaker’s centrist (!) political views completely, 100% validates the right’s narrative about academia, that it’s become a fanatically intolerant echo chamber.  To my fellow progressive academics, I beseech thee in the bowels of Bertrand Russell: <em>why would you commit such an unforced error?</em></p>



<p>Yes, one can <em>imagine</em> views (e.g., open Nazism) so hateful that they might justify the cancellation of unrelated scientific lectures by people who hold those views, as many physicists after WWII refused to speak to Werner Heisenberg.  But it seems obvious to me—as it would’ve been obvious to everyone else not long ago—that no matter where a reasonable person draws the line, Abbot’s views as he expressed them in <em>Newsweek</em> don’t come within a hundred miles of it.  To be more explicit still: if Abbot’s views justify deplatforming him as a planetary scientist, then <strong>all my quantum computing and theoretical computer science lectures deserve to be cancelled too</strong>, for the many attempts I’ve made on this blog over the past 16 years to share my honest thoughts and life experiences, to write like a vulnerable human being rather than like a university press office.  While I’m sure some sneerers gleefully embrace that implication, I ask everyone else to consider how deeply they believe in the idea of academic freedom at all—keeping in mind that such a commitment <em>only ever gets tested</em> when there’s a chance someone might denounce you for it.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Princeton’s James Madison Program has <a href="https://jmp.princeton.edu/events/climate-and-potential-life-other-planets">volunteered</a> to host Abbot’s Zoom talk in place of MIT.  The talk is entitled “Climate and the Potential for Life on Other Planets.”  Like probably hundreds of others who heard about this only because of the attempted cancellation, I plan to attend!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Bonus Update:</span></strong> <a href="https://www.youtube.com/watch?v=934eLObPLmM">Here’s a neat YouTube video</a> put together by the ACM about me as well as David Silver of AlphaGo and AlphaZero, on the occasion of our ACM Prizes in Computing.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5859"><span class="datestr">at October 05, 2021 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=123">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2021/10/04/focs-2021-best-paper-awards/">FOCS 2021 Best Paper Awards</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>On behalf of the FOCS 2021 PC, I am delighted to announce the Best Paper Awards.</p>



<p><strong>Best paper: </strong></p>



<p>Nutan Limaye, Srikanth Srinivasan and Sébastien Tavenas. <em>Superpolynomial Lower Bounds Against Low-Depth Algebraic Circuits</em></p>



<p></p>



<p>This paper makes a fundamental advance by proving super-polynomial lower bounds against algebraic circuits of arbitrary constant depth. </p>



<p>Paper: <a href="https://eccc.weizmann.ac.il/report/2021/081/" rel="nofollow">https://eccc.weizmann.ac.il/report/2021/081/</a></p>



<p></p>



<p><strong>Machtey Award for Best Student Paper:</strong></p>



<p>Xiao Mao. <em>Breaking the Cubic Barrier for (Unweighted) Tree Edit Distance</em><a href="https://twitter.com/NisheethVishnoi"><br /></a><br />This paper shows how, unlike the weighted case, the unweighted tree edit distance problem has a sub-cubic time algorithm. </p>



<p>Paper: <a href="https://t.co/G8yw1EZwuR?amp=1" target="_blank" rel="noreferrer noopener">https://arxiv.org/abs/2106.02026</a></p>



<p></p>



<p>Congratulations to the winners and see you all in Denver from Feb 7-10, 2022!<a href="https://twitter.com/NisheethVishnoi/status/1445099200274960390/photo/1"></a></p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2021/10/04/focs-2021-best-paper-awards/"><span class="datestr">at October 04, 2021 06:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/04/five-tenure-track-faculty-positions-in-cs-at-the-university-of-virginia-at-university-of-virginia-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/04/five-tenure-track-faculty-positions-in-cs-at-the-university-of-virginia-at-university-of-virginia-apply-by-december-1-2021/">Five Tenure-Track Faculty Positions in CS at the University of Virginia at University of Virginia  (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Computer Science Department at the University of Virginia seeks 5 tenured or tenure-track faculty at all ranks, with Theory as one of our core targeted areas. Review of applications begins 12/1/2021.</p>
<p>Website: <a href="https://uva.wd1.myworkdayjobs.com/en-US/UVAJobs/job/Charlottesville-VA/Open-Rank-Faculty-Position-in-Computer-Science_R0028993">https://uva.wd1.myworkdayjobs.com/en-US/UVAJobs/job/Charlottesville-VA/Open-Rank-Faculty-Position-in-Computer-Science_R0028993</a><br />
Email: selbaum@virginia.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/04/five-tenure-track-faculty-positions-in-cs-at-the-university-of-virginia-at-university-of-virginia-apply-by-december-1-2021/"><span class="datestr">at October 04, 2021 06:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-10-04-crusader-agreement-with-dollars-slash-leq-1-slash-3$-error-is-impossible-for-$n-slash-leq-3f$-if-the-adversary-can-simulate/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-10-04-crusader-agreement-with-dollars-slash-leq-1-slash-3$-error-is-impossible-for-$n-slash-leq-3f$-if-the-adversary-can-simulate/">Crusader Agreement with $\leq 1/3$ Error is Impossible for $n\leq 3f$ if the Adversary can Simulate</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The classic FLM lower bound says that in Synchrony, Byzantine Agreement is impossible when $n \leq 3f$. We discussed this important bound in a previous post. In this post we strengthen the FLM lower bound in two important ways: Maybe randomization allows circumventing the FLM lower bound? No! Even allowing...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-10-04-crusader-agreement-with-dollars-slash-leq-1-slash-3$-error-is-impossible-for-$n-slash-leq-3f$-if-the-adversary-can-simulate/"><span class="datestr">at October 04, 2021 02:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-989853369577969981">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/how-have-computers-changed-society.html">How have computers changed society? Harry Lewis (with co-authors) have a book out on that.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> (Disclosure - Harry Lewis was my PhD advisor.)</p><p><br /></p><p>It seems like just a few weeks ago I I blogged about a book of Harry Lewis's that was recently available (see <a href="https://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html">here</a>).  And now I am blogging about another one. Writing two books in two years seems hard! I can only think of one other computer scientist who has done that recently (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics-ebook/dp/B08BJ4G2Z1/ref=sr_1_2?dchild=1&amp;keywords=gasarch&amp;qid=1626492618&amp;sr=8-2">here</a> and <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279729/ref=sr_1_2?dchild=1&amp;keywords=gasarch+point&amp;qid=1626492655&amp;sr=8-2">here</a>).</p><p><br /></p><p>In 2008 Abelson, Ledeen, and Lewis wrote </p><p><i>Blown to Bits: Your Life, Liberty, and Happiness after the Digital Explosion</i></p><p>which I reviewed in SIGACT news, see <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/bitsbook.pdf">here</a></p><p><br /></p><p>Both computers and society have changed since 2008. Hence an update was needed. </p><p>In 2021 Adelson, Ledeen, Lewis, and Seltzer wrote a second edition.</p><p><br /></p><p>Should you buy the new version if you bought the old version? </p><p>1) Not my problem- I got them both for free since I reviewed them. </p><p>2) Not your problem- The second edition is available free-on-line <a href="https://www.bitsbook.com/thebook/">here</a>. Is that a link to some dark corner of the dark web? No, its the formal webpage about the book. So the book is available free-on-line legally, if you care (and even if you don't care). </p><p>3) If you like paper, the book is on amazon. (If you don't like paper, the book is still on amazon). </p><p><br /></p><p>I reviewed it in SIGACT news. A non-paywalled link: <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/b2bits2.pdf">here</a> (is that link legal? I have no idea.) </p><p>In this post I'll just mention two things that changed since the last book</p><p>1) Shared Music and pirating were an issue back in 2008.  It does not seem to be anymore since there is now a variety of services that seem to make pirating not worth it: itunes, streaming services, and some bands give it away for free and ask you to pay what its worth. Movies are still struggling with this issue. </p><p>2) AI systems that reinforce existing bias is a new problem.</p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/how-have-computers-changed-society.html"><span class="datestr">at October 04, 2021 04:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/02/generating-fibbinary-numbers">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/02/generating-fibbinary-numbers.html">Generating fibbinary numbers, three ways</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I just added to Wikipedia two articles on the <a href="https://en.wikipedia.org/wiki/Jordan%E2%80%93P%C3%B3lya_number">Jordan–Pólya numbers</a> and <a href="https://en.wikipedia.org/wiki/Fibbinary_number">fibbinary numbers</a>, two integer sequences used in <a href="https://11011110.github.io/blog/2021/09/24/which-integer-sequences.html">my recent paper on Egyptian fractions</a>. Jordan–Pólya numbers are the products of factorials, while the fibbinary numbers are the ones with binary representations having no two consecutive 1’s. The OEIS page on the fibbinary numbers, <a href="https://oeis.org/A003714">A003714</a>, lists many ways of generating this sequence algorithmically, of which most are boring or slow (generate all binary numbers and test which ones belong to the sequence; you can test if a variable <code class="language-plaintext highlighter-rouge">x</code> is fibbinary by checking that <code class="language-plaintext highlighter-rouge">x&amp;(x&gt;&gt;1)</code> is zero). I thought it might be interesting to highlight two of those methods that are a little more clever and generate these numbers in small numbers of operations.</p>

<p>Some functional languages, and in part Python even though it’s mostly not functional, have a notion of a stream, a potentially infinite sequence of values generated by a coroutine. In Python, you can program these using <a href="https://www.python.org/dev/peps/pep-0255/">simple generators</a> and the <code class="language-plaintext highlighter-rouge">yield</code> keyword. I wrote here long ago about <a href="https://11011110.github.io/blog/2011/10/02/generating-permutations-with.html">methods for using generators recursively</a>: a generator can call itself, manipulate the resulting sequence of values, and pass them on to its output. It’s actually a very old idea, used for instance to generate <a href="https://en.wikipedia.org/wiki/Regular_number">regular numbers</a> by Dijkstra <a href="http://web.cecs.pdx.edu/~black/AdvancedProgramming/Lectures/Smalltalk%20II/Dijkstra%20on%20Hamming%27s%20Problem.pdf">in his 1976 book <em>A Discipline of Programming</em></a>. Reinhard Zumkeller used the same idea to generate the fibbinary numbers in Haskell, based on the observation that the sequence of positive fibbinary numbers can be generated, starting from the number 1, by two operations, doubling smaller values or replacing a smaller value \(x\) with \(4x+1\). Here is is, translated into Python:</p>

<figure class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">heapq</span> <span class="kn">import</span> <span class="n">merge</span>

<span class="k">def</span> <span class="nf">affine</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">x</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="n">b</span>

<span class="k">def</span> <span class="nf">fibbinary</span><span class="p">():</span>
    <span class="k">yield</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">merge</span><span class="p">(</span><span class="n">affine</span><span class="p">(</span><span class="n">fibbinary</span><span class="p">(),</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">affine</span><span class="p">(</span><span class="n">fibbinary</span><span class="p">(),</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
        <span class="k">yield</span> <span class="n">x</span></code></pre></figure>

<p>It’s elegant, but has a couple of minor flaws. First, it omits the number \(0\), and while it can be modified to include \(0\), the modifications make the code messier. But second, it takes more than a constant amount of time per element to generate each sequence element. A fibbinary number \(x\) has to be generated from a sequence of smaller elements by repeated doubling and quadrupling, and that takes \(\log x\) steps per element. Even if we assume those steps to take constant time each, generating the first \(n\) elements in this way takes time \(\Theta(n\log n)\). It’s better than the \(\Theta(n^{\log_\varphi 2})\approx n^{1.44}\) that you would get from generate-and-test, but still not as good as we might hope for. One way to fix this would be to memoize the generator, so that the recursive calls look at a stored copy of the sequence generated by the outer call rather than generating the same sequence redundantly, but this again makes the code messier and also takes more storage than necessary.</p>

<p>Instead, Jörg Arndt observed that you can generate each fibbinary number directly from the previous one by a process closely resembling binary addition. Adding one to a binary number sets the first available bit from zero to one, and zeros out all the smaller bits; here, a bit is available if it is already zero. Finding the next fibbinary number does the same thing, but with a different definition of availability: a bit is available if both it and the next larger bit are zero. We can find the available bit using binary addition on a modified word that fills in bits whose neighbor is nonzero. Using this idea, we can generate the fibbinary numbers in a constant number of bitwise binary word-level operations per number. Here it is again in Python, translated from Arndt’s C++ and simplified based on <a href="https://mathstodon.xyz/@efroach76/107037399683569338">a comment by efroach76</a>:</p>

<figure class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">fibbinary</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">x</span>
        <span class="n">y</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">y</span></code></pre></figure>

<p>It’s even possible to use the same idea to generate the fibbinary numbers in a constant amortized number of bit-level operations per number, although this ends up being a little less efficient in practice because high-level languages end up translating all these bit operations into word operations anyway.</p>

<figure class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">fibbinary</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">x</span>
        <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">x</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y</span> <span class="o">|</span> <span class="p">(</span><span class="n">y</span><span class="o">&lt;&lt;</span><span class="mi">1</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">&amp;=~</span> <span class="n">y</span>
            <span class="n">y</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">|=</span> <span class="n">y</span></code></pre></figure>

<p>The inner loop ends immediately at fibbinary numbers whose successor is odd (at positions given by the ones of the <a href="https://en.wikipedia.org/wiki/Fibonacci_word">Fibonacci word</a>), whose fraction of the total is \(1-1/\varphi\approx 0.382\), where \(\varphi\) is the golden ratio. It ends in two steps for the remaining values when their next bit is odd, in the same proportion, and so on. So the average number of steps for the inner loop adds in a geometric series to \(O(1)\).</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107034017632258123">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/02/generating-fibbinary-numbers.html"><span class="datestr">at October 02, 2021 01:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/02/postdoc-at-irif-paris-france-apply-by-november-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/02/postdoc-at-irif-paris-france-apply-by-november-1-2021/">postdoc at IRIF, Paris, France (apply by November 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IRIF, Paris, France, is seeking excellent candidates for postdoctoral positions in all areas of the Foundations of Computer Science.</p>
<p>IRIF is a joint laboratory of the CNRS (French National Center for Scientific Research) and Université de Paris; see <a href="https://www.irif.fr/en/informations/presentation">https://www.irif.fr/en/informations/presentation</a> .</p>
<p>Knowledge of French is not required; applications can be sent either in French or in English.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc">https://www.irif.fr/postes/postdoc</a><br />
Email: postdoc-advice@irif.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/02/postdoc-at-irif-paris-france-apply-by-november-1-2021/"><span class="datestr">at October 02, 2021 07:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/142">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/142">TR21-142 |  Mixing of 3-term progressions in Quasirandom Groups  | 

	Amey Bhangale, 

	Prahladh Harsha, 

	Sourya Roy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this note, we show the mixing of three-term progressions $(x, xg, xg^2)$ in every finite quasirandom group, fully answering a question of Gowers. More precisely, we show that for any $D$-quasirandom group $G$ and any three sets $A_1, A_2, A_3 \subset G$, we have
\[ \left|\Pr_{x,y\sim G}\left[ x \in A_1, xy \in A_2, xy^2 \in A_3\right] - \prod_{i=1}^3 \Pr_{x\sim G}\left[x \in A_i\right] \right| \leq \left(\frac{2}{\sqrt{D}}\right)^{\frac14}.\] 
Prior to this, Tao answered this question when the underlying quasirandom group is $\mathrm{SL}_{d}(\mathbb{F}_q)$. Subsequently, Peluse extended the result to all nonabelian finite simple groups. In this work, we show that a slight modification of Peluse's argument is sufficient to fully resolve Gower's quasirandom conjecture for 3-term progressions. Surprisingly, unlike the proofs of Tao and Peluse, our proof is elementary and only uses basic facts from nonabelian Fourier analysis.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/142"><span class="datestr">at October 01, 2021 04:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blog.simons.berkeley.edu/?p=595">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/simons.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.simons.berkeley.edu/2021/09/theory-at-the-institute-and-beyond-september-2021/">Theory at the Institute and Beyond, September 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>by <a href="https://simons.berkeley.edu/people/prasad-raghavendra">Prasad Raghavendra</a> (Simons Institute)</p>



<p><img src="https://simons.berkeley.edu/sites/default/files/news/_prasad_and_algebraic_circuit_complexity_v4.png" style="margin: 5px; float: right; width: 328px; height: 219px;" alt="" />Being in one of the talks in the Simons Institute auditorium, witnessing live and lively interaction with the speaker, feels like the closest thing to normal since the start of the pandemic. There is a sense of tangible joy among the participants just to be sharing the same physical space, let alone the fantastic environs of the Institute. The renewed energy is all there to witness in the programs this semester on <a href="https://simons.berkeley.edu/programs/si2021"><u>Computational Complexity of Statistical Inference</u></a> (CCSI) and <a href="https://simons.berkeley.edu/programs/gmos2021"><u>Geometric Methods in Optimization and Sampling</u></a> (GMOS), both of which are now in full swing. Although masking is maintained, it doesn’t seem to change the quintessential Simons program experience even a little bit. I am referring, of course, to the constant feeling of missing out on all the incredibly interesting activities going on, much of which one is unable to fit into their schedule.</p>



<p>At least some of the palpable energy can be attributed to over 40 postdocs and research fellows who have arrived at the Institute this semester, many of whom will stay on for a year or two. This extraordinary group of young researchers covers the whole gamut of topics, ranging from cryptography, quantum computing, and fairness to machine learning, data structures, algorithms, and complexity theory. Each of these postdocs and fellows gave a 10-minute presentation at the “Meet the Fellows’’ welcome event that the Institute held on September 8 and 9. Check out their <a href="https://simons.berkeley.edu/events/meet-fellows-welcome-event-wednesday-schedule"><u>talks</u></a> for glimpses of the cutting edge in all these subfields of theory.</p>



<p><strong>An advance in algebraic circuit complexity</strong><br />
	This time around, there is some good news from the front lines on circuit complexity, one of the most challenging arenas within theoretical computer science.</p>



<p>An algebraic circuit consists of gates, each of which carries out either addition or multiplication over some field, say real numbers. The depth of the circuit is the length of the longest path from the output to one of its inputs. Naturally, an algebraic circuit computes a polynomial over its inputs.</p>



<p>In the world of Boolean circuits with AND/OR/NOT gates, lower bounds against constant depth circuits, aka AC0 circuit lower bounds, have been known since the 1980s and are one of the most influential results in complexity theory. For general algebraic circuits over a large field (say reals), even superpolynomial lower bounds for depth three circuits had remained elusive. In a <a href="https://eccc.weizmann.ac.il/report/2021/081/">remarkable paper</a>, Nutan Limaye, Srikanth Srinivasan, and Sébastien Tavenas have obtained the first superpolynomial lower bounds against general algebraic circuits of all constant depths over fields of characteristic zero (say reals). Furthermore, the lower-bound result is shown for a simple polynomial known as “iterated matrix multiplication” whose input consists of \(d\) matrices \(X_1,\ldots,X_d\) of dimension \(n \times n\), and the goal is to compute a fixed entry of their product \(X_1 \cdot X_2 \cdots X_d\). The same work also obtains a depth hierarchy theorem for algebraic circuits showing that for every depth <em>D</em>, there is an explicit polynomial that can be computed by a depth <em>D</em> circuit of size <em>s</em>, but requires circuits of size superpolynomial in <em>s</em> if the depth is <em>D</em>-1.</p>



<p><strong>Remarkable work of Matthew Brennan</strong><br /> The theory community suffered a terrible loss this year with the tragic and untimely passing of one of our rising stars, Matthew Brennan. While still a graduate student at MIT, Matthew almost single-handedly pushed forward an ambitious research program at the intersection of computational complexity and statistics. Here we will try to give a glimpse of Matthew’s extensive body of research.</p>



<span id="more-595"></span>



<p></p>



<p>To set the context, traditionally the field of computational complexity has been concerned with characterizing how much memory or computational power is needed, while statistics sheds light on how much data are needed. The interplay between computational and statistical resources and their underlying trade-offs is only recently coming into focus.</p>



<p>Many statistical estimation tasks exhibit information-computation gaps wherein the accuracy obtained by efficient algorithms is strictly worse than the estimate that one could in principle extract from the data if there were no computational limitations. In other problems, there is an apparent trade-off between the number of samples needed for an estimation task and the computational power of the estimation algorithm.</p>



<p>Identifying and rigorously establishing the presence of these information-computation trade-offs is a fundamental challenge that has drawn much interest in theoretical computer science and statistics. To rigorously establish an information-computation gap, one would need to prove lower bounds against efficient algorithms for statistical estimation problems.</p>



<p>Here on, there are two routes that one could take. First, one can prove limitations for specific classes of algorithms like MCMC, spectral methods, semidefinite programming, statistical queries, and so on. This approach has been highly successful, yielding precise lower bounds in many models and numerous relations discovered between the models.</p>



<p>Alternatively, one can take the grander route: assume the intractability of a well-studied computational problem and use <em>reductions</em> to establish hardness of the statistical estimation problem at hand. In a seminal work, Berthet and Rigollet showed a reduction from the planted clique problem to the sparse principal component analysis (sparse PCA), thereby establishing the presence of an information-computation gap for a version of sparse PCA.</p>



<p>In its most canonical setup, the input to the sparse PCA problem consists of samples from a \(d\)-dimensional Gaussian with covariance \(\Sigma = I_d + \theta vv^T\), where \(v\) is a \(k\)-sparse unit vector and \(\theta\) denotes the signal strength. A simple hypothesis testing variant of the problem asks to distinguish \(n\) samples drawn from this spiked covariance matrix and \(n\) samples drawn from the standard Gaussian distribution.</p>



<p>Notice that statistical estimation problems like sparse PCA are most interesting when their inputs are drawn from some natural distribution like the Gaussian distribution. Therefore, a reduction from a hard problem <em>A</em> to a statistical estimation problem <em>B</em> will not only have to map instances of <em>A</em> to instances of <em>B</em>, but also have to produce the right distribution over instances of <em>B</em>. For example, ideally a reduction from planted clique to sparse PCA must map instances of planted clique (random graphs with cliques) to random vectors drawn from an appropriate Gaussian measure. How could a reduction ever do this, one might ask.</p>



<p>In fact, this is why one might decide not to take the grand route of using reductions to understand the complexity of statistical problems. While reductions have been exceptionally successful in establishing tight hardness results for <em>NP</em>-complete optimization problems, building a similar web of reductions for statistical problems like sparse PCA seemed hopeless to me. Not for Matthew, who courageously took this challenge head-on and succeeded!</p>



<p>Despite efforts following the seminal work of Berthet and Rigollet, showing a tight hardness result for sparse PCA in its most canonical Gaussian setup seemed out of reach. In a series of works, Matthew and coauthors developed a set of simple reduction tools that when put together led to a tight characterization of the complexity of the sparse PCA problem in its most canonical setup, over the entire range of parameters!</p>



<p>More recently, Matthew and his advisor, Guy Bresler, proposed a generalized planted clique conjecture referred to as “secret leakage planted clique” that can serve as a starting point for a web of reductions encompassing a variety of problems such as tensor PCA, robust sparse mean estimation, and semirandom community recovery. Not only have these works developed a set of useful reduction techniques for statistical problems, but most importantly, they have demonstrated that reductions can indeed be harnessed toward tight hardness results for statistical problems. Matthew was a recipient of the best student paper award at the Conference on Learning Theory (COLT) on two separate occasions for this line of work.</p>



<p>Matthew’s work is one of the central themes of this semester’s program on Computational Complexity of Statistical Inference. His work has laid the foundations of a theory of reductions for statistical problems, something for us to build on over the coming years. While his ideas are ever present this semester, his presence at the program is sorely missed.</p></div>







<p class="date">
by Preeti Aroon <a href="https://blog.simons.berkeley.edu/2021/09/theory-at-the-institute-and-beyond-september-2021/"><span class="datestr">at October 01, 2021 03:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/09/30/phd-student-at-university-of-bergen-norway-apply-by-november-14-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/09/30/phd-student-at-university-of-bergen-norway-apply-by-november-14-2021/">PhD Student at University of Bergen (Norway) (apply by November 14, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>3-year PhD student position in the field of Symbolic Algorithms (extension to 4 years if teaching). Salary: 491,200 NOK per year (approx 48,000 EUR per year) before taxes. Applicants should have a MSc. degree (or obtain a MSc. degree before 31/01/2022) and a strong background in some relevant subfield of TCS, such as algorithms, graph theory, combinatorics, automata theory, type theory, etc.</p>
<p>Website: <a href="https://www.jobbnorge.no/en/available-jobs/job/212570/phd-research-fellow-in-informatics-symbolic-algorithms">https://www.jobbnorge.no/en/available-jobs/job/212570/phd-research-fellow-in-informatics-symbolic-algorithms</a><br />
Email: mateus.oliveira@uib.no</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/09/30/phd-student-at-university-of-bergen-norway-apply-by-november-14-2021/"><span class="datestr">at September 30, 2021 09:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/09/30/postdoc-at-university-of-bergen-norway-apply-by-november-14-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/09/30/postdoc-at-university-of-bergen-norway-apply-by-november-14-2021/">Postdoc at University of Bergen (Norway) (apply by November 14, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>3-year postdoc position in the field of Symbolic Algorithms. Salary: 535,500 NOK per year (approx. 52,500 EUR per year) before taxes. Applicants should have a Ph.D. degree (or obtain a Ph.D. degree before 31/01/2022) and a strong publication record in some relevant subfield of TCS, such as algorithms, graph theory, combinatorics, automata theory, type theory, etc.</p>
<p>Website: <a href="https://www.jobbnorge.no/en/available-jobs/job/212553/postdoctoral-research-fellow-position-within-informatics-symbolic-algorithms">https://www.jobbnorge.no/en/available-jobs/job/212553/postdoctoral-research-fellow-position-within-informatics-symbolic-algorithms</a><br />
Email: mateus.oliveira@uib.no</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/09/30/postdoc-at-university-of-bergen-norway-apply-by-november-14-2021/"><span class="datestr">at September 30, 2021 09:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8844632344201187395">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/09/being-chair.html">Being the Chair</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>If you have Netflix and interested in the academic world, I recommend <a href="https://www.netflix.com/title/81206259">The Chair</a>, a six-episode dramatic series starring Sandra Oh as a new English department chair at a "lower tier ivy league university". The series takes many artistic liberties and compresses much in a short time period but gets much about academics right such as the tension between faculty and the administration with the chair caught in the middle, the need to create majors that attract students, faculty past their prime teaching the same courses in the same way for decades, faculty who get themselves in a hole and keep digging, alumni donors controlling academic decisions, pressure to build a diverse faculty, faculty feeling under appreciated and getting outside offers, and a wonderful exposition of how the field has changed over the past thirty years given to someone who had dropped out before finishing their PhD to take on a different career.</p><p>When I served as department chair at Georgia Tech, I dealt with most if not all of these issues above, though not at the same time. I had some challenges that today's English department doesn't face: how to handle enrollments that more than doubled while barely able to hire more faculty than were departing, not that I would trade in a second for the existential crisis that English departments are going through. </p><p>When I left Georgia Tech after seven years, I had outlasted every other current chair in the Colleges of Computing, Science and Engineering. Not sure what this says about me or about Georgia Tech.</p><p>Being chair is the most challenging job in academia. The faculty technically report to you but you aren't their boss in any traditional sense--they came to academia because of the freedom to work on what they want and they won't give it up. It's virtually impossible to fire anyone with tenure. The joke goes that a chair needs two umbrellas, one to block stuff coming from the administration going to the faculty and the other to block the stuff from the faculty from going to the administration. Since I left it has gotten much uglier in the University System of Georgia which has no mask or vaccine mandates and glad I'm not the chair to deal with that.</p><p>This all sounds like I'm discouraging of becoming a department chair and it certainly isn't a job for anyone but it can be a very rewarding job. You can help shape the future of the department by the faculty you hire and the vision you set and create an environment that helps your faculty and students succeed. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/09/being-chair.html"><span class="datestr">at September 30, 2021 02:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/09/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/09/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/09/15/pearson-sues-former-partner-chegg-copyright-infringement">Textbook company Pearson sues Chegg for copyright infringement, for selling solutions to textbook homework problems</a> (<a href="https://mathstodon.xyz/@11011110/106945803113477254">\(\mathbb{M}\)</a>). On the one hand, for-profit cheater-enablers like Chegg are a cancer on higher education. On the other, the solution to a problem is generally a concept, not a text, and should not be something that can be locked up under copyright. So I don’t know who to root for?</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/09/13/future-academic-conference"><em>Inside Higher Ed</em> on the future of academic conferences</a> (<a href="https://mathstodon.xyz/@11011110/106951348197294218">\(\mathbb{M}\)</a>). A significant fraction of academics surveyed said that they still felt unsafe going to physical conferences, and with the carbon footprint and reduced expenses of virtual but greater interactivity of physical meetings, some mix of both seems likely going forward. However, trying to mix both in one conference (especially for conferences with many parallel small talks or panels) seems difficult and expensive.</p>
  </li>
  <li>
    <p><a href="https://www.kennethmoreland.com/color-advice/">Color map advice for scientific visualization</a> (<a href="https://mathstodon.xyz/@11011110/106955479284414863">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=28579720">via</a>).</p>
  </li>
  <li>
    <p>Three sets of talk slides from recent talks (<a href="https://mathstodon.xyz/@11011110/106962519567502209">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-WG-21-slides.pdf">The graphs of stably matchable pairs, <em>WG 2021</em></a></p>
      </li>
      <li>
        <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-WADS-21-slides.pdf">A stronger lower bound on parametric minimum spanning trees, <em>WADS 2021</em></a></p>
      </li>
      <li>
        <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-GD-21-slides.pdf">Limitations on realistic hyperbolic graph drawing, <em>GD 2021</em></a></p>
      </li>
    </ul>

    <p>Unfortunately I don’t have links to recordings of the talks.</p>
  </li>
  <li>
    <p>I thought for sure I had posted before about <a href="https://www.thisiscolossal.com/2017/09/bold-new-geometric-cake-designs-by-dinara-kasko/">Dinara Kasko’s 3d-printed geometric food designs</a> (<a href="https://mathstodon.xyz/@11011110/106968564827718278">\(\mathbb{M}\)</a>, <a href="https://culturainquieta.com/es/arte/diseno/item/12643-la-reposteria-matematica-de-la-arquitecta-dinara-kasko.html">see also</a>, <a href="https://dinarakasko.com/">home page</a>), but grep tells me that if I ever did, it wasn’t with her name.</p>
  </li>
  <li>
    <p>The <a href="https://zbmath.org/">zbMATH</a> mathematics review database was broken for a day (<a href="https://mathstodon.xyz/@11011110/106971763813682368">\(\mathbb{M}\)</a>), unable to show the old scans of printed reviews, but fortunately it got better.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=-p7C5FrgAzU">Twelve threads</a> (<a href="https://mathstodon.xyz/@11011110/106982013693223792">\(\mathbb{M}\)</a>). Vi Hart’s latest video mixes up discussions of the nature of social media, the philosophy of mathematical creativity, an exploration of symmetry, and an investigation of the spot patterns of 8-sided dice (which turn out not to all be the same) and how to visualize them.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@mathcination/106905943747555880">Visualization of Poncelet’s porism</a>, calculated with difficulty by mathcination from Cayley’s criteria. It would have been easier to calculate a regular hendecagram and then perturb it by a projective transformation, but that wasn’t the point.</p>
  </li>
  <li>
    <p>I recently saw a link to <a href="https://www.w3.org/TR/MathML3/chapter1.html">Chapter 1 of the MathML 3.0 spec</a> (<a href="https://mathstodon.xyz/@11011110/107002528612296323">\(\mathbb{M}\)</a>), using as an example the quadratic formula in both layout markup and content markup. Its totally unwieldy non-human-readable expansion obscures the fact that the MathML authors didn’t even get the math right: their content markup silently replaces the plus-minus sign, by which the correct formula represents both solutions, with an addition operation, giving only one of the two. Time to re-link my old <a href="https://11011110.github.io/blog/2015/08/04/mathml-considered-harmful.html">anti-MathML rant</a>?</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/09/hacer-origami-sculptures/">Giant origami animals in Midtown Manhattan</a> (<a href="https://mathstodon.xyz/@11011110/107007549470942566">\(\mathbb{M}\)</a>). But now I want to know if they were really each fabricated from a single uncut square sheet of steel. And if so, how did they get the corners so crisp?</p>
  </li>
  <li>
    <p><a href="https://www.getty.edu/publications/virtuallibrary/pdf/9780892363353.pdf"><em>The Topkapı Scroll – Geometry and Ornament in Islamic Architecture</em>, by Harvard professor Gülru Necipoğlu</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107017227413204703">\(\mathbb{M}\)</a>).</span> One of many beautiful art books free for download from the <a href="https://www.getty.edu/publications/virtuallibrary/">Getty Virtual Library</a>.</p>
  </li>
  <li>
    <p><a href="http://www.formulas.it/formulog/wp-content/uploads/2014/12/sierpinski-aplimat.pdf">Sierpiński triangles in stone, on medieval floors in Rome</a> (<a href="https://mathstodon.xyz/@11011110/107021612310283993">\(\mathbb{M}\)</a>), Elisa Conversano and Laura Tedeschini Lalli. See also Kim Williams, “<a href="https://doi.org/10.1007%2Fbf03024339">The pavements of the Cosmati</a>”. The <a href="https://commons.wikimedia.org/wiki/Category:Sierpi%C5%84ski_triangles_in_Cosmatesque_pavements">collection of images of these on Wikimedia commons</a> is a little sad — Conversano and Tedeschini Lalli, and Williams, have a lot more.</p>
  </li>
  <li>
    <p>Every invertible function computable both ways in polynomial time has polynomial-size reversible logic circuits, using extra “dummy” values that are zero on both input and output (<a href="https://mathstodon.xyz/@11011110/107024956502298623">\(\mathbb{M}\)</a>): See Jacopini, Mentrasti, &amp; Sontacchi, “<a href="https://doi.org/10.1137/0403020">Reversible turing machines and polynomial time reversibly computable functions</a>”, SIAM J. Disc. Math. 1990.</p>

    <p><em>Theorem:</em> No circuit of reversible gates of arity less than \(n\), without dummy values, can compute \(n\)-bit 2’s-complement negation.</p>

    <p><em>Proof:</em> Each gate performs an even permutation on the \(2^n\) inputs, but negation is an odd permutation. \(\Box\)</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/09/30/linkage.html"><span class="datestr">at September 30, 2021 11:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-09-30-distributed-consensus-made-simple-for-real-this-time/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-09-30-distributed-consensus-made-simple-for-real-this-time/">Distributed consensus made simple (for real this time!)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Multi-Paxos is the de facto solution for deciding a log of commands to execute on a replicated state machine, yet it’s famously difficult to understand, motivating the switch to ‘simpler’ consensus protocols such as Raft. The conventional wisdom is that the best way to use Paxos (aka Synod, or single-shot...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-09-30-distributed-consensus-made-simple-for-real-this-time/"><span class="datestr">at September 30, 2021 07:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/">Baby Steps</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>You don’t have to see the whole staircase, just take the first step—Martin Luther King, Jr.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/viazovska/" rel="attachment wp-att-19168"><img width="195" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/09/Viazovska.jpg?resize=195%2C150&amp;ssl=1" class="alignright wp-image-19168" height="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Still from her IHES <a href="https://www.youtube.com/watch?v=xALXm2XHDWc">lecture</a></font></td>
</tr>
</tbody>
</table>
<p>
Maryna Viazovska was the first person to prove an exact bound on sphere packing in a dimension higher than 3. She <a href="https://arxiv.org/pdf/1603.04246.pdf">achieved</a> this for dimension 8 in 2016 by making an improvement of <b>0.00001</b> over one previous <a href="https://annals.math.princeton.edu/wp-content/uploads/annals-v157-n2-p09.pdf">paper</a> by Henry Cohn and Noam Elkies. This also led to the <a href="https://arxiv.org/pdf/1603.06518.pdf">solution</a> in dimension 24, joint with Cohn and Abhinav Kumar, Stephen Miller, and Danylo Radchenko.</p>
<p>
Today we talk about partial progress—baby steps—and its relation to solving conjectures.<br />
<span id="more-19166"></span></p>
<p>
This post is a continuation of our recent <a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/">thoughts</a> about how those who claim to have solved a major problem almost always claim to have solved the whole thing in one large leap. We’ve thought about declaring a rule that any claim on a major open problem, at least about complexity lower bounds, must first be a <em>partial result</em>. We will give concrete suggestions in that direction at the end. But on reflection, we’ll curb the dogmatics a little.</p>
<p>
In her 2016 <em>Quanta</em> <a href="https://www.quantamagazine.org/sphere-packing-solved-in-higher-dimensions-20160330">article</a> on Viazovska’s breakthrough, Erica Klarreich quotes Peter Sarnak:</p>
<blockquote><p><b> </b> <em> “It’s stunningly simple, as all great things are. You just start reading the paper and you know this is correct.” </em>
</p></blockquote>
<p>
We hasten to add that the paper’s correctness is evident amid the context that others had established over the previous two decades, going back at least to Thomas Hales’s voluminous proof of Johannes Kepler’s conjecture for dimension 3. To quote Klarreich:</p>
<blockquote><p><b> </b> <em> Researchers have known for more than a decade what the missing ingredient in the proof [of optimality for dimensions 8 and 24] should be—an “auxiliary” function that can calculate the largest allowable sphere density—but they couldn’t find the right function. … [F]inding the right modular form allowed Viazovska to prove [the case of 8] in a mere 23 pages. </em>
</p></blockquote>
<p>
Thus Viazovska brought in a new tool to the problem, <em>modular forms</em>, which had already proved their merit in resolving the Fermat conjecture. But she still needed to choose the right modular form among many candidates. Klarreich quotes her as saying it is difficult even just to explain how she knew which one to use. This brings us back to the challenge of explaining—or debunking—the flash of insight that claimers claim to have. At least in this case, per Sarnak’s quote, the proof was in the pudding.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/cohnexcerpt/" rel="attachment wp-att-19169"><img width="460" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/09/CohnExcerpt.jpg?resize=460%2C174&amp;ssl=1" class="aligncenter size-full wp-image-19169" height="174" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">From Cohn’s 2017 <i>AMS Notices</i> <a href="https://www.ams.org/publications/journals/notices/201702/rnoti-p102.pdf">article</a></font>
</td>
</tr>
</tbody></table>
<p>
To complete the mixing of our original message, the examples we chose happen to involve improvements by tiny amounts. We’ve even understated Viazovska’s above: reckoned against a later <a href="https://annals.math.princeton.edu/2009/170-3/p01">paper</a> by Cohn and Elkies, her improvement was </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0.000000000000000000000000000001.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  0.000000000000000000000000000001. " class="latex" /></p>
<p>
The recent <a href="https://rjlipton.wpcomstaging.com/2020/10/26/a-vast-and-tiny-breakthrough/">post</a> where we discussed the phrase “the proof is in the pudding” involves a number with six more <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" />s than that. These are <b>not</b> what we mean by “baby steps.” But let us tell our next story, which also involves Cohn.</p>
<p>
</p><p></p><h2> Steps Toward Matrix Multiplication </h2><p></p>
<p></p><p>
We have <a href="https://rjlipton.wpcomstaging.com/2011/11/29/a-breakthrough-on-matrix-product/">covered</a> <a href="https://rjlipton.wpcomstaging.com/2011/12/03/the-meaning-of-omega/">other</a> <a href="https://rjlipton.wpcomstaging.com/2012/02/01/a-brief-history-of-matrix-product/">work</a> on the exponent <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> of matrix product. This means the infimum of all <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e}" class="latex" /> such that any two <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \times n}" class="latex" /> matrices can be multiplied in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Ee%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(n^e)}" class="latex" /> unit operations. In 1990, Don Coppersmith and Shmuel Winograd <a href="https://www.sciencedirect.com/science/article/pii/S0747717108800132">brought</a> <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> down below <img src="https://s0.wp.com/latex.php?latex=%7B2.375477%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.375477}" class="latex" />. The current best bound <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3C+2.37286%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega &lt; 2.37286}" class="latex" /> is from a SODA 2021 <a href="https://arxiv.org/abs/2010.05846">paper</a> by Josh Alman and Virginia Williams; its abstract highlights the improvement by <img src="https://s0.wp.com/latex.php?latex=%7B0.00001%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0.00001}" class="latex" /> from the previous best.</p>
<p>
In 2005, however, Cohn wrote a <a href="https://arxiv.org/pdf/math/0511460.pdf">paper</a> with Robert Kleinberg, Balazs Szegedy, and Christopher Umans on ideas for taking <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> all the way down to <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> in one jump. This diverges from both the reading of “baby step” as “tiny improvement” and our idea meaning limited-but-definite partial progress. Their strategy for the full jump has come in and out of clouds since then. But their paper has motivated the subsequent partial progress in several ways. We have <a href="https://rjlipton.wpcomstaging.com/2010/03/27/fast-matrix-products-and-other-amazing-results/">mentioned</a> this paper <a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/">several</a> <a href="https://rjlipton.wpcomstaging.com/2012/06/22/cricket-400-and-complexity-theory/">times</a>, notably <a href="https://rjlipton.wpcomstaging.com/2018/08/30/limits-on-matrix-multiplication/">here</a>. </p>
<p>
The objective need not be <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> versus <img src="https://s0.wp.com/latex.php?latex=%7B2.372...%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.372...}" class="latex" />, however. There is a notable milepost just above <img src="https://s0.wp.com/latex.php?latex=%7B2.30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.30}" class="latex" />. It comes from a STOC 2015 <a href="https://arxiv.org/abs/1411.5414">paper</a> by Andris Ambainis, Yuval Flimus, and François Le Gall. It shows that a wide class of techniques of the kind used since 1990 cannot achieve better than <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3C+2.3078%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega &lt; 2.3078}" class="latex" />. </p>
<p>
Other recent <a href="https://drops.dagstuhl.de/opus/volltexte/2018/8360/pdf/LIPIcs-ITCS-2018-25.pdf">work</a> by Alman and Williams shows both a dimension along which the road down to <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> may still be open but other limits in other directions. Even their new <img src="https://s0.wp.com/latex.php?latex=%7B0.00001%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0.00001}" class="latex" /> improvement has new ideas that may be exploited more generally. This is a feature of partial progress that contrasts with what we said <a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/">recently</a> about not learning much from whole-jump attempts: partial progress always shows a higher learning curve.</p>
<p>
</p><p></p><h2> Some Partial Progress Objectives </h2><p></p>
<p></p><p>
Cohn also has a page of informal <a href="http://math.mit.edu/~cohn/Thoughts/">thoughts</a> on how to perform research, including <a href="https://math.mit.edu/~cohn/Thoughts/advice.html">advice</a> for those who claim to have solved some open problem. We want to add this advice:</p>
<blockquote><p><b> </b> <em> Make sure ahead of time that your advance really covers the intermediate ground it jumps through. Even better, walk back your claim a little so that it proves something only a step or so beyond previous knowledge. </em>
</p></blockquote>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/steps/" rel="attachment wp-att-19171"><img width="270" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/09/steps.png?resize=270%2C186&amp;ssl=1" class="aligncenter size-full wp-image-19171" height="186" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.kimaverycoaching.com/baby-steps/">source</a> (not <a href="https://www.warehouse3b.top/ProductDetail.aspx?iid=151594314&amp;pr=30.99">this</a>)</font>
</td>
</tr>
</tbody></table>
<p>
Here are a few examples of such things to prove in complexity theory:</p>
<p></p><p></p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P \neq NP}}" class="latex" />: The claim is that SAT requires exponential time. <p></p>
</li><li>
<b>A Baby Step</b>: Prove SAT cannot be done in linear time. Or that it cannot be done in <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^2}" class="latex" /> time.
</li></ul>
<p><br /></p>
<ul>
<li>
<b>Circuit Lower Bounds</b>: The claim is that some concrete problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> requires super-polynomial size general circuits. <p></p>
</li><li>
<b>A Baby Step</b>: Prove that some concrete problem cannot be done in a linear size boolean circuit–or one of size <img src="https://s0.wp.com/latex.php?latex=%7B6n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6n}" class="latex" />.
</li></ul>
<p><br /></p>
<ul>
<li>
<b>Factoring</b>: The claim is that factoring a general number <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> requires super-polynomial time in the bit-length of <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" />. <p></p>
</li><li>
<b>A Baby Step</b>: Prove that factoring cannot be done in linear time. Or not in quadratic time. See <a href="https://math.mit.edu/~cohn/Thoughts/factoring.html">this</a>.
</li></ul>
<p><br /></p>
<ul>
<li>
<b>Graph Isomorphism</b>: The claim is that there is polynomial-time algorithm to tell whether two graphs are isomorphic. <p></p>
</li><li>
<b>A Baby Step</b>: Prove the case where the graphs have degree a fixed constant.
</li></ul>
<p>
This is an example of a baby step that is already done. Note: it was not an easy step. It is a famous result of Eugene Luks—see his 1982 <a href="https://www.sciencedirect.com/science/article/pii/0022000082900095">paper</a>. </p>
<p></p><p><br />
We could go deeper into known computational complexity issues to find more. The idea applies to problems from mathematics in general. Here is one:</p>
<p></p><p></p>
<ul>
<li>
<b>Riemann Hypothesis”</b> The <a href="https://en.wikipedia.org/wiki/Riemann_hypothesis">claim</a> is of course that 	<p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7Bn%5Es%7D+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^s} = 0 " class="latex" /></p>
<p>only happens for negative even integers and for <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> with real part <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" />. The latter are the zeroes in the <i>critical strip</i> where the real part is between <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />. The sum is convergent for real part <img src="https://s0.wp.com/latex.php?latex=%7B%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&gt; 1}" class="latex" /> and its analytic continuation for other <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> is understood.</p>
</li><li>
<b>A Baby Step</b>: Prove that it is impossible for just two zeroes to be off the half line. Or that only a finite number can be off the half line.
</li></ul>
<p></p><p><br />
We invite readers to add more favorite math examples in comments, or others from complexity theory.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are some additional first steps? </p>
<p>
Have we also—unintendedly but effectively—made a case for the value of tiny improvements? At least when they are conceptual improvements?</p>
<p>
Here is the answer to the puzzle in the previous <a href="https://rjlipton.wpcomstaging.com/2021/09/26/a-possible-ramsey-insight-into-p-versus-np/">post</a>: Barbara wins. She forces the sum of every row to be <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" />. Alan makes an entry <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />, she then makes an entry in the same row <img src="https://s0.wp.com/latex.php?latex=%7B-x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-x}" class="latex" />. This works since 1986 is even. This makes the matrix singular, since the matrix times the all <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> vector is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" />.</p>
<p></p><p><br />
[inserted “in dimension 8” at top, added note on “analytic continuation” for Riemann, other tweaks]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/"><span class="datestr">at September 30, 2021 06:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/09/29/postdoc-at-cwi-apply-by-october-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/09/29/postdoc-at-cwi-apply-by-october-31-2021/">Postdoc at CWI (apply by October 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A 2-year postdoctoral position is available at CWI in the area of discrete and continuous optimization under the supervision of Daniel Dadush. Candidates with interests in theoretical aspects of integer programming &amp; linear programming, as well as discrepancy theory should apply. No teaching requirement. The starting date is flexible.</p>
<p>Website: <a href="https://www.cwi.nl/jobs/vacancies/894611">https://www.cwi.nl/jobs/vacancies/894611</a><br />
Email: dadush@cwi.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/09/29/postdoc-at-cwi-apply-by-october-31-2021/"><span class="datestr">at September 29, 2021 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-09-29-the-round-complexity-of-reliable-broadcast/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-09-29-the-round-complexity-of-reliable-broadcast/">The round complexity of Reliable Broadcast</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Reliable Broadcast is an important building block of many Asynchronous protocols. There is a broadcaster that has some input value, $v$, and a non-faulty party that terminates needs to output a value. Reliable Broadcast is defined via two properties: Validity: If the broadcaster is non-faulty then eventually all non-faulty parties...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-09-29-the-round-complexity-of-reliable-broadcast/"><span class="datestr">at September 29, 2021 10:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
