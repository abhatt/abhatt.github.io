<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 21, 2021 08:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=562">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/05/20/tcs-talk-wednesday-may-26-kira-goldner-columbia-university/">TCS+ talk: Wednesday, May 26 — Kira Goldner, Columbia University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://www.kiragoldner.com/"><strong>Kira Goldner</strong></a> from Columbia University will speak about “<em>An Overview of Using Mechanism Design for Social Good</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In order to accurately predict an algorithm’s outcome and quality when it interacts with participants who have a stake in the outcome, we must design it to be robust to strategic manipulation. This is the subject of algorithmic mechanism design, which borrows ideas from game theory and economics to design robust algorithms. In this talk, I will show how results from the theoretical foundations of algorithmic mechanism design can be used to solve problems of societal concern.</p>
<p>I will overview recent work in this area in many different applications — housing, labor markets, carbon license allocations, health insurance markets, and more — as well as discuss open problems and directions ripe for tools from both mechanism design and general TCS.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/05/20/tcs-talk-wednesday-may-26-kira-goldner-columbia-university/"><span class="datestr">at May 20, 2021 09:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5273315380945865876">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/emerging-from-pandemic.html">Emerging from the Pandemic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The City of Chicago yesterday agreed with the latest CDC guidelines that those of us fully vaccinated no longer have to wear masks in most settings. Lollapalooza, the big Chicago music festival, will be held in full capacity this summer. There are still some restrictions but barring any surprise setbacks should be mostly gone by the Fourth of July.</p><p>It's not appropriate to call the pandemic over. Too many countries are suffering and lack good access to vaccines or strong health care. But in most of the US vaccinations are readily available and it really feels like we are putting the pandemic in the past.</p><p>Variants that beat the vaccines could emerge. Vaccines could become ineffective over time. Too many still need to be vaccinated and too many don't want to do so. Typically problems start small and quickly get big by exponential growth but likely with enough warning if we need boosters or extra precautions. And all those who cut in line to get shots early are now my canaries for the effects wearing off.</p><p>What will this post-pandemic world look like? Continued virtual meetings from people who don't want to spend the 10-15 minutes to get across campus or to come to campus at all. A bigger move to casual dress, even in the corporate world. No more snow days. Changes in ways we won't expect. </p><p>We all have our different attitudes but I'm ready to move on. Time to start the "after times".</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/emerging-from-pandemic.html"><span class="datestr">at May 20, 2021 02:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21532">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/">To cheer you up in difficult times 25: some mathematical news! (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h2>Topology</h2>



<p></p>


<h3>Quasi-polynomial algorithms for telling if a knot is trivial</h3>


<p>Marc Lackenby announced a quasi-polynomial time algorithm to decide whether a given knot is the unknot! This is a big breakthrough. This question is known to be both in NP and in coNP. See <a href="https://gilkalai.wordpress.com/2012/04/10/greg-kuperberg-it-is-in-np-to-tell-if-a-knot-is-knotted-under-grh/">this post</a>, and updates there in the <a href="https://gilkalai.wordpress.com/2012/04/10/greg-kuperberg-it-is-in-np-to-tell-if-a-knot-is-knotted-under-grh/#comment-43442">comment section</a>.</p>



<h4>Topology seminar, UC Davis, February 2021 and Oxford, March 2021 and <strong>today, May 20!</strong> <strong>16:00 CET</strong>, in the Copenhagen-Jerusalem combinatorics seminar (see details at the end of the post).</h4>



<p><a href="http://people.maths.ox.ac.uk/lackenby/quasipolynomial-talk-oxford.pdf">Unknot recognition in quasi-polynomial time</a> (The link is to the slides)</p>


<h3>NP hardness for related questions regarding knots</h3>


<p>This is a talk by Martin Tancer given at the  Copenhagen-Jerusalem  combinatorics seminar.</p>



<p>Title:  <a href="https://arxiv.org/abs/1810.03502">The unbearable hardness of unknotting</a>  (link to the 2018 arXive paper)</p>



<p>Abstract: </p>



<p>During the talk, I will sketch a proof that deciding if a diagram of the unknot can be untangled using at most k Riedemeister moves (where k is part of the input) is NP-hard. (This is not the same as the unknot recognition but it reveals some difficulties.) Similar ideas can be also used for proving that several other similar invariants are NP-hard to recognize on links.</p>



<p>Joint work with A. de Mesmay, Y. Rieck and E. Sedgwick.</p>



<h3>An approach toward exotic differentiable 4-dim spheres</h3>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Show one of these 21 links is smoothly slice and collect your Fields medal. <a href="https://twitter.com/hashtag/RBG?src=hash&amp;ref_src=twsrc%5Etfw">#RBG</a> <a href="https://t.co/Uem6CcXp7h">https://t.co/Uem6CcXp7h</a> <a href="https://t.co/eIPYVRktR0">pic.twitter.com/eIPYVRktR0</a></p>— Ian Agol (@agolian) <a href="https://twitter.com/agolian/status/1360269132822835201?ref_src=twsrc%5Etfw">February 12, 2021</a></blockquote></div>
</div></figure>



<p></p>



<h3>Monumental work on Arnold’s conjecture</h3>



<p><a href="https://arxiv.org/abs/2103.01507">Arnold Conjecture and Morava K-theory</a> by Mohammed Abouzaid, Andrew J. Blumberg</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">This is a big deal:<a href="https://t.co/CDGhu0GRZ3">https://t.co/CDGhu0GRZ3</a></p>— Justin Curry (@currying) <a href="https://twitter.com/currying/status/1367090561849696261?ref_src=twsrc%5Etfw">March 3, 2021</a></blockquote></div>
</div></figure>



<p>(I heard it from a retweet by Alex Kontorovich.)</p>



<p>Lower bound for the number of fixed points  of a map is a central theme in topology (and other areas) with many great results. The Lefshetz fixed point theorem tells you that a certain signed sum of the fixed points is bounded from below . For symplectic manifold Arnold’s conjecture asserts that the number of fixed point is bounded below from the sun of Betti numbers. This have led and is connected to tremendous mathematics.</p>



<blockquote class="wp-block-quote"><p><em>Abstract (part): We prove that the rank of the cohomology of a closed symplectic manifold with coefficients in a field of characteristic p is smaller than the number of periodic orbits of any non-degenerate Hamiltonian flow.</em></p></blockquote>



<h2>Graph theory</h2>



<h3><a href="https://arxiv.org/abs/2009.05495">Large spanning graphs with odd degrees</a></h3>



<p>Asaf Ferber and Michael Krivelevich solved an old conjecture in Graph theory. (In a 1994 paper by Yair Caro the problem is already referred to as part of the folklore of graph theory.)</p>



<p><strong>Abstract</strong>: We prove that every graph G on n vertices with no isolated vertices contains an induced subgraph of size at least n/10000 with all degrees odd. This solves an old and well-known conjecture in graph theory.</p>



<p>Here is <a href="https://www.quantamagazine.org/mathematicians-answer-old-question-about-odd-graphs-20210519/">an article about it in Quanta Magazine</a>. And here is a <a href="https://www.quantamagazine.org/new-proof-reveals-that-graphs-with-no-pentagons-are-fundamentally-different-20210426/">Quanta Magazine article</a> on the solution of the Erdos-Hajnal conjecture for pentagon-free graphs (that I mentioned in <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">this post</a>).</p>



<p></p>



<h3><a href="https://arxiv.org/abs/2011.10939">The Betti numbers of the independence complex of a ternary graph</a></h3>



<p>A ternary graph (aka Trinity graph)  has no induced cycles of length divisible by three. Chudnovsky, Scott, Seymour and Spirkl <a href="https://arxiv.org/abs/1810.00065">proved</a> that for any ternary graph <em>G</em>, the number of independent sets with even cardinality and the independent sets with odd cardinality differ by at most 1. <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wu%2C+H">Hehui Wu</a> and <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhang%2C+W">Wentao Zhang</a> <a href="https://arxiv.org/abs/2101.07131">proved the stronger conjecture</a> that for ternary graphs, the sum of Betti numbers of the independent complex is at most one. (Both conjectures were proposed by Roy Meshulam and me.) Congratulations to Hehui and Wentao and all other people mentioned in this post. </p>



<p>For related advances see <a href="https://arxiv.org/search/?searchtype=author&amp;query=Engstrom%2C+A">Alexander Engstrom</a>, <a href="https://arxiv.org/abs/2009.11077">On the topological Kalai-Meshulam conjecture</a>;  <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kim%2C+J">Jinha Kim,</a> <a href="https://arxiv.org/abs/2101.07131">The homotopy type of the independence complex of graphs with no induced cycle of length divisible by three </a> <br /><br />Here is <a href="https://web.math.princeton.edu/~pds/papers/chibounded/paper.pdf">a very nice survey by Paul Seymour and Alex Scott on chi-boundedness</a>. Here is an <a href="https://gilkalai.wordpress.com/2014/12/19/when-a-few-colors-suffice/">earlier post on Trinity graphs and chi-boundedness.</a></p>



<p></p>



<p> </p>



<h2>A breakthrough in additive combinatorics<br /><br /></h2>



<figure class="wp-block-image is-resized"><img width="96" alt="THP" src="https://gilkalai.files.wordpress.com/2021/05/thp.jpg" class="wp-image-21752" height="140" /><strong>Huy Tuan Pham</strong></figure>



<p><a href="http://www.its.caltech.edu/~dconlon/">David Conlon</a>, <a href="https://stanford.edu/~jacobfox/">Jacob Fox</a> and <a href="https://web.stanford.edu/~huypham/">Huy Tuan Pham</a> used a new unified approach to solve a variety of open problems around subset sums including the open problems of Burr and Erdős on Ramsey complete sequences (Erdős offered $350 for their solutions) and a problem of Alon and Erdős on estimating the minimum number <img src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="f(n)" class="latex" /> of colors needed to color the positive integers less than <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" /> cannot be written as a sum of elements of the same color. Conlon, Fox and Pham  proved that <img src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="f(n)" class="latex" /> is within a constant factor of <em>n<sup>4/3</sup>φ(n)<sup>-1</sup>(log n)<sup>-1/3</sup>(loglog n)<sup>-2/3</sup></em>, where <em>φ(n)</em> is the Euler totient function. The paper is: </p>



<h3>The paper is: <a href="https://arxiv.org/abs/2104.14766">Subset sums, completeness and colorings</a>,</h3>



<h2><br />P-adic local Langlands</h2>



<h3><a href="https://www.galoisrepresentations.com/2021/03/09/test-your-intuition-p-adic-local-langlands-edition/">Test Your Intuition: p-adic local Langlands edition</a>!</h3>



<p>This post over Persiflage will test your intuition about 2-dimensional crystalline deformation rings. Since I couldn’t understand even the first sentence, I decided to ask my FB and HUJI friend to explain it to me, so I may try to write about it later. Actually, a better idea is that  Persiflage will try to give a vastly non-technical  explanation from scratch on p-adic local Langlands conjecture, and where it stands and why it matters. (To be clear, I am sure it is a great staff! And perhaps it is not possible to explain it to a wide audience. But it worth a try.)<br /><br />Meanwhile it reminded me of the following story to which I apparently was a witness.</p>



<p>At the time of my birth, the medical approach was that mother and child needed to stay at the hospital for at least four nights and even for an entire whole week. After two nights at the hospital, my mother decided  that she was ready to go home and got into a lengthy discussion with her doctor about it. At some point the exhausted doctor told her “Madam, I studied seven years in medical school, and I know what is good for you.” My mother was not impressed and replied: “Had I studied seven years in medical school, I would  have been a medical doctor as well. And now, let me go home.” At the end, my mother (and myself) left the hospital on the third day. Years later this became the medical standard. (My own approach is <em>to follow </em>medical doctor’s instructions.)</p>



<h2>Mathematics over the media</h2>



<h3><a href="http://www.ramanujanmachine.com/">Ramanujan machine</a>: machine learning for producing mathematical conjectures</h3>



<p>This is a cool and very interesting direction by very very young Israeli mathematicians  that was reported all over the media. (See, <a href="https://www.nature.com/articles/s41586-021-03229-4">here for a Nature article</a>.)</p>



<p>(There are various groups all over the world attempting to use machine learning to produce mathematical conjectures.)</p>



<h3>A new approach for solving quadratic equations</h3>



<p>I heard it (I think it was a year ago) from a Facebook post written by the prime minister of Singapore! And then I saw it in other places all over the media (<a href="https://www.nytimes.com/2020/02/05/science/quadratic-equations-algebra.html">here on the NYT</a>). I like it, and, in fact, I like the many things <a href="https://www.math.cmu.edu/~ploh/cmu.shtml">Po-Shen Loh</a> (who is my grand academic nephew) does in the service of mathematics.</p>



<p></p>



<p>***************</p>



<p><strong>Copenhagen-Jerusalem Combinatorics Seminar</strong><br /><br />It is my pleasure to invite you to the following talk:<br />**************************************************************************<br /><a>Thursday, May 20</a> Time: <a>16:00-18:00 CET, Jerusalem +1<br /></a> Location: Zoom: <a href="https://ucph-ku.zoom.us/j/69204766431" target="_blank" rel="noreferrer noopener"></a><a href="https://ucph-ku.zoom.us/j/69204766431" target="_blank" rel="noreferrer noopener">https://ucph-ku.zoom.us/j/69204766431</a></p>



<p>************************************************************************** </p>



<p>Marc Lackenby:  Unknot recognition in quasi-polynomial time<br />**************************************************************************<br /><br />Abstract: </p>



<p>I will outline a new algorithm for unknot recognition that runs in quasi-polynomial time. The input is a diagram of a knot with n crossings, and the running time is 2^{O((log n)^3)}. The algorithm uses a wide variety of tools from 3-manifold theory, including normal surfaces, hierarchies and Heegaard splittings. In my talk, I will explain this background theory, as well as explain how it fits into the algorithm.</p>



<pre class="wp-block-preformatted"></pre>



<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/"><span class="datestr">at May 20, 2021 11:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.09217">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.09217">Approximation Algorithms For The Euclidean Dispersion Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Pawan_K=.html">Pawan K. Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Gautam_K=.html">Gautam K. Das</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09217">PDF</a><br /><b>Abstract: </b>In this article, we consider the Euclidean dispersion problems. Let
$P=\{p_{1}, p_{2}, \ldots, p_{n}\}$ be a set of $n$ points in $\mathbb{R}^2$.
For each point $p \in P$ and $S \subseteq P$, we define $cost_{\gamma}(p,S)$ as
the sum of Euclidean distance from $p$ to the nearest $\gamma $ point in $S
\setminus \{p\}$. We define $cost_{\gamma}(S)=\min_{p \in
S}\{cost_{\gamma}(p,S)\}$ for $S \subseteq P$. In the $\gamma$-dispersion
problem, a set $P$ of $n$ points in $\mathbb{R}^2$ and a positive integer $k
\in [\gamma+1,n]$ are given. The objective is to find a subset $S\subseteq P$
of size $k$ such that $cost_{\gamma}(S)$ is maximized. We consider both
$2$-dispersion and $1$-dispersion problem in $\mathbb{R}^2$. Along with these,
we also consider $2$-dispersion problem when points are placed on a line. In
this paper, we propose a simple polynomial time $(2\sqrt 3 + \epsilon )$-factor
approximation algorithm for the $2$-dispersion problem, for any $\epsilon &gt; 0$,
which is an improvement over the best known approximation factor $4\sqrt3$
[Amano, K. and Nakano, S. I., An approximation algorithm for the $2$-dispersion
problem, IEICE Transactions on Information and Systems, Vol. 103(3), pp.
506-508, 2020]. Next, we develop a common framework for designing an
approximation algorithm for the Euclidean dispersion problem. With this common
framework, we improve the approximation factor to $2\sqrt 3$ for the
$2$-dispersion problem in $\mathbb{R}^2$. Using the same framework, we propose
a polynomial time algorithm, which returns an optimal solution for the
$2$-dispersion problem when points are placed on a line. Moreover, to show the
effectiveness of the framework, we also propose a $2$-factor approximation
algorithm for the $1$-dispersion problem in $\mathbb{R}^2$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.09217"><span class="datestr">at May 20, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.09145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.09145">Modeling Precomputation In Games Played Under Computational Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Thomas Orton <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09145">PDF</a><br /><b>Abstract: </b>Understanding the properties of games played under computational constraints
remains challenging. For example, how do we expect rational (but
computationally bounded) players to play games with a prohibitively large
number of states, such as chess? This paper presents a novel model for the
precomputation (preparing moves in advance) aspect of computationally
constrained games. A fundamental trade-off is shown between randomness of play,
and susceptibility to precomputation, suggesting that randomization is
necessary in games with computational constraints. We present efficient
algorithms for computing how susceptible a strategy is to precomputation, and
computing an $\epsilon$-Nash equilibrium of our model. Numerical experiments
measuring the trade-off between randomness and precomputation are provided for
Stockfish (a well-known chess playing algorithm).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.09145"><span class="datestr">at May 20, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.09047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.09047">Obstructing Classification via Projection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Pantea Haghighatkhah, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meulemans:Wouter.html">Wouter Meulemans</a>, Bettina Speckman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Urhausen:J=eacute=r=ocirc=me.html">Jérôme Urhausen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verbeek:Kevin.html">Kevin Verbeek</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09047">PDF</a><br /><b>Abstract: </b>Machine learning and data mining techniques are effective tools to classify
large amounts of data. But they tend to preserve any inherent bias in the data,
for example, with regards to gender or race. Removing such bias from data or
the learned representations is quite challenging. In this paper we study a
geometric problem which models a possible approach for bias removal. Our input
is a set of points P in Euclidean space R^d and each point is labeled with k
binary-valued properties. A priori we assume that it is "easy" to classify the
data according to each property. Our goal is to obstruct the classification
according to one property by a suitable projection to a lower-dimensional
Euclidean space R^m (m &lt; d), while classification according to all other
properties remains easy.
</p>
<p>What it means for classification to be easy depends on the classification
model used. We first consider classification by linear separability as employed
by support vector machines. We use Kirchberger's Theorem to show that, under
certain conditions, a simple projection to R^(d-1) suffices to eliminate the
linear separability of one of the properties whilst maintaining the linear
separability of the other properties. We also study the problem of maximizing
the linear "inseparability" of the chosen property. Second, we consider more
complex forms of separability and prove a connection between the number of
projections required to obstruct classification and the Helly-type properties
of such separabilities.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.09047"><span class="datestr">at May 20, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.08980">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.08980">Degrees and Gaps: Tight Complexity Results of General Factor Problems Parameterized by Treewidth and Cutwidth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marx:D=aacute=niel.html">Dániel Marx</a>, Govind S. Sankar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schepper:Philipp.html">Philipp Schepper</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08980">PDF</a><br /><b>Abstract: </b>For the General Factor problem we are given an undirected graph $G$ and for
each vertex $v\in V(G)$ a finite set $B_v$ of non-negative integers. The task
is to decide if there is a subset $S\subseteq E(G)$ such that $deg_S(v)\in B_v$
for all vertices $v$ of $G$. The maxgap of a finite integer set $B$ is the
largest $d\ge 0$ such that there is an $a\ge 0$ with $[a,a+d+1]\cap
B=\{a,a+d+1\}$. Cornu\'ejols (1988) showed that if the maxgap of all sets $B_v$
is at most 1, then the decision version of General Factor is poly-time
solvable. Dudycz and Paluch (2018) extended this result for the minimization
and maximization versions. Using convolution techniques from van Rooij (2020),
we improve upon the previous algorithm by Arulselvan et al. (2018) and present
an algorithm counting the number of solutions of a certain size in time
$O^*((M+1)^k)$, given a tree decomposition of width $k$, where $M=\max_v \max
B_v$.
</p>
<p>We prove that this algorithm is essentially optimal for all cases that are
not polynomial time solvable for the decision, minimization or maximization
versions. We prove that such improvements are not possible even for $B$-Factor,
which is General Factor on graphs where all sets $B_v$ agree with the fixed set
$B$. We show that for every fixed $B$ where the problem is NP-hard, our new
algorithm cannot be significantly improved: assuming the Strong Exponential
Time Hypothesis (SETH), no algorithm can solve $B$-Factor in time $O^*((\max
B+1-\epsilon)^k)$ for any $\epsilon&gt;0$. We extend this bound to the counting
version of $B$-Factor for arbitrary, non-trivial sets $B$, assuming #SETH.
</p>
<p>We also investigate the parameterization of the problem by cutwidth. Unlike
for treewidth, a larger set $B$ does not make the problem harder: Given a
linear layout of width $k$ we give a $O^*(2^k)$ algorithm for any $B$ and
provide a matching lower bound that this is optimal for the NP-hard cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.08980"><span class="datestr">at May 20, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.08974">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.08974">Pseudo-Hadamard matrices of the first generation and an algorithm for producing them</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharipov:Ruslan.html">Ruslan Sharipov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08974">PDF</a><br /><b>Abstract: </b>Hadamard matrices in $\{0,1\}$ presentation are square $m\times m$ matrices
whose entries are zeros and ones and whose rows considered as vectors in $\Bbb
R^m$ produce the Gram matrix of a special form with respect to the standard
scalar product in $\Bbb R^m$. The concept of Hadamard matrices is extended in
the present paper. As a result pseudo-Hadamard matrices of the first generation
are defined and investigated. An algorithm for generating these pseudo-Hadamard
matrices is designed and is used for testing some conjectures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.08974"><span class="datestr">at May 20, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.08967">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.08967">Speed Scaling On Parallel Servers with MapReduce Type Precedence Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaze:Rahul.html">Rahul Vaze</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nair:Jayakrishnan.html">Jayakrishnan Nair</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08967">PDF</a><br /><b>Abstract: </b>A multiple server setting is considered, where each server has tunable speed,
and increasing the speed incurs an energy cost. Jobs arrive to a single queue,
and each job has two types of sub-tasks, map and reduce, and a {\bf precedence}
constraint among them: any reduce task of a job can only be processed once all
the map tasks of the job have been completed. In addition to the scheduling
problem, i.e., which task to execute on which server, with tunable speed, an
additional decision variable is the choice of speed for each server, so as to
minimize a linear combination of the sum of the flow times of jobs/tasks and
the total energy cost. The precedence constraints present new challenges for
the speed scaling problem with multiple servers, namely that the number of
tasks that can be executed at any time may be small but the total number of
outstanding tasks might be quite large. We present simple speed scaling
algorithms that are shown to have competitive ratios, that depend on the power
cost function, and/or the ratio of the size of the largest task and the
shortest reduce task, but not on the number of jobs, or the number of servers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.08967"><span class="datestr">at May 20, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.08917">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.08917">Can We Break Symmetry with o(m) Communication?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pai:Shreyas.html">Shreyas Pai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pandurangan:Gopal.html">Gopal Pandurangan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pemmaraju:Sriram_V=.html">Sriram V. Pemmaraju</a>, Peter Robinson <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08917">PDF</a><br /><b>Abstract: </b>We study the communication cost (or message complexity) of fundamental
distributed symmetry breaking problems, namely, coloring and MIS. While
significant progress has been made in understanding and improving the running
time of such problems, much less is known about the message complexity of these
problems. In fact, all known algorithms need at least $\Omega(m)$ communication
for these problems, where $m$ is the number of edges in the graph. We address
the following question in this paper: can we solve problems such as coloring
and MIS using sublinear, i.e., $o(m)$ communication, and if so under what
conditions? [See full abstract in pdf]
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.08917"><span class="datestr">at May 20, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.08763">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.08763">Online bin packing of squares and cubes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Leah.html">Leah Epstein</a>, Loay Mualem <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08763">PDF</a><br /><b>Abstract: </b>In the d-dimensional online bin packing problem, d-dimensional cubes of
positive sizes no larger than 1 are presented one by one to be assigned to
positions in d-dimensional unit cube bins. In this work, we provide improved
upper bounds on the asymptotic competitive ratio for square and cube bin
packing problems, where our bounds do not exceed 2.0885 and 2.5735 for square
and cube packing, respectively. To achieve these results, we adapt and improve
a previously designed harmonic-type algorithm, and apply a different method for
defining weight functions. We detect deficiencies in the state-of-the-art
results by providing counter-examples to the current best algorithms and the
analysis, where the claimed bounds were 2.1187 for square packing and 2.6161
for cube packing.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.08763"><span class="datestr">at May 20, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1883">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/">Entropy Estimation via Two Chains: Streamlining the Proof of the Sunflower Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://en.wikipedia.org/wiki/Sunflower_(mathematics)">sunflower lemma</a> describes an interesting combinatorial property of set families: any large family of small sets must contain a large <em>sunflower</em>—a sub-family consisting of sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_r" class="latex" /> with a shared <em>core</em> <img src="https://s0.wp.com/latex.php?latex=S%3A%3DA_1%5Ccap%5Ccdots%5Ccap+A_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S:=A_1\cap\cdots\cap A_r" class="latex" /> and disjoint <em>petals</em> <img src="https://s0.wp.com/latex.php?latex=A_1%5Cbackslash+S%2C%5Cldots%2CA_r%5Cbackslash+S.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1\backslash S,\ldots,A_r\backslash S." class="latex" /> (See the <a href="https://theorydish.blog/feed/#fig-sunflower">figure</a> below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential <a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">paper</a> by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see <a href="https://dl.acm.org/doi/abs/10.1145/3357713.3384234">STOC version</a> of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science).</p>



<div class="wp-block-image" id="fig-sunflower"><figure class="aligncenter size-full is-resized"><img width="474" alt="" src="https://theorydish.files.wordpress.com/2021/04/sunflower_example-1.png" class="wp-image-2195" height="207" />Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower.</figure></div>



<p>The lemma was first proved by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=111692">Erdős-Rado</a> in 1960, who gave the quantitative bound that among <img src="https://s0.wp.com/latex.php?latex=%28r-1%29%5Ekk%21%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(r-1)^kk!+1" class="latex" /> distinct sets each of size at most <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> one can find <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r" class="latex" /> sets forming a sunflower. Erdos-Rado conjectured that the <img src="https://s0.wp.com/latex.php?latex=k%21%3Dk%5E%7Bk%281+-+o%281%29%29%7D+%3D+k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k!=k^{k(1 - o(1))} = k^{\Omega(k)}" class="latex" /> in the bound could be significantly improved to <img src="https://s0.wp.com/latex.php?latex=O%281%29%5Ek.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(1)^k." class="latex" /> For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the <img src="https://s0.wp.com/latex.php?latex=k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^{\Omega(k)}" class="latex" /> dependence on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> even for <img src="https://s0.wp.com/latex.php?latex=r+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r = 3" class="latex" /> despite much research effort. In 2019, a breakthrough work by <a href="https://arxiv.org/abs/1908.08483">Alweiss-Lovett-Wu-Zhang</a> improved the bound significantly to <img src="https://s0.wp.com/latex.php?latex=%28Cr%5E3%5Clog+k%5Clog%5Clog_2+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr^3\log k\log\log_2 k)^k" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> being at least <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="3" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="C" class="latex" /> being an absolute constant. The breakthrough led to many new results including improved monotone circuit lower bounds by <a href="https://arxiv.org/abs/2012.03883">Cavalar-Kumar-Rossman</a>.</p>



<p>Since the breakthrough of Alweiss-Lovett-Wu-Zhang, researchers have been refining the bound and simplifying the proof. The current best bound is <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log k)^k" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k \ge 2" class="latex" /> proved by Bell-Chueluecha-Warnke via a minor but powerful twist in the proof of an earlier <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+%28rk%29%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log (rk))^k" class="latex" /> <a href="https://arxiv.org/abs/1909.04774">bound</a> by Anup Rao:</p>



<p id="thm-1"><strong>Theorem 1</strong> (Sunflower Lemma <a href="https://arxiv.org/abs/2009.09327">[BCW’20]</a>).<em> There exists a constant <img src="https://s0.wp.com/latex.php?latex=C%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="C&gt;0" class="latex" /> such that the following holds for all positive integers <img src="https://s0.wp.com/latex.php?latex=k%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k\ge 2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=r.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r." class="latex" /> Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be a family of at least <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log k)^k" class="latex" /> distinct sets each of size at most <img src="https://s0.wp.com/latex.php?latex=k.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k." class="latex" /> Then <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> contains <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r" class="latex" /> sets that form a sunflower.</em></p>



<p>The aim of this blog post is to present a streamlined proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a>. The proof is largely based on a <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a> by Terence Tao where he presented an elegant proof of Rao’s result using <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. However, Tao’s proof included a trick of <em>passing to a conditional copy twice</em>, which Tao described as <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/\#comment-579138">“somewhat magical”</a>. <strong>We show here that the trick is not necessary for the proof, and avoiding the trick gives a simpler proof with a slightly better constant in the bound.</strong></p>



<p>We start by defining <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread families, a notion key to all recent proofs of the sunflower lemma. We use <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> as a shorthand for <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2CN%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{1,\ldots,N\}." class="latex" /> We use boldface symbols to denote random variables, and non-boldface ones to denote deterministic quantities.</p>



<p><strong>Definition 1</strong> (Spread family). <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be a family of finite sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N" class="latex" /> that are not necessarily distinct. For <img src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R &gt; 1" class="latex" />, we say <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread if for all <img src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+%5Ccup_%7Bn%3D1%7D%5ENA_n%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S\subseteq \cup_{n=1}^NA_n," class="latex" /></em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CPr%5BS%5Csubseteq+A_%7B%5Cmathbf+n%7D%5D+%5Cle+R%5E%7B-%7CS%7C%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \Pr[S\subseteq A_{\mathbf n}] \le R^{-|S|},\end{aligned}" class="latex" /></em></p>



<p><em>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> is chosen uniformly at random from <img src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]." class="latex" /></em></p>



<p>The main technical part of recent improvements in the sunflower lemma happens in the proof of the following refinement lemma. We use the base-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> logarithm throughout.</p>



<p id="lm-2"><strong>Lemma 2</strong> (Refinement). <em>Let <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> be a finite set. <em>For <img src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R &gt; 1" class="latex" /></em>, let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread family of sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N%5Csubseteq+X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N\subseteq X." class="latex" /></em> <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> be a size-<img src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta|X|" class="latex" /> subset of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> chosen uniformly at random for <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin%281%2FR%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta\in(1/R,1]" class="latex" /> assuming <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta|X|" class="latex" /> is an integer. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> be a uniform random number in <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> independent of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W." class="latex" /> There is a random number <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> (being not independent from <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+n%2C%5Cmathbf+W%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(\mathbf n,\mathbf W)" class="latex" /> in general) such that <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccup%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_{\mathbf n'}\subseteq A_{\mathbf n}\cup\mathbf W" class="latex" /> almost surely and </em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C%5Cle%5Cfrac%7B4%7D%7B%5Clog%28R%5Cdelta%29%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%7D%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}\mathbb E|A_{\mathbf n'}\backslash \mathbf W|\le\frac{4}{\log(R\delta)}\mathbb E|A_{\mathbf n}|.\end{aligned}" class="latex" /></em></p>



<p>The proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> using Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> can be found in Rao’s <a href="https://arxiv.org/abs/1909.04774">paper</a> and Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, so we omit it here and focus on proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>. Rao and Tao both proved a slightly weaker version of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> but this weakness can be overcome using a minor twist observed by <a href="https://arxiv.org/abs/2009.09327">Bell-Chueluecha-Warnke</a>. They also used slightly different forms of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> but the differences are non-essential.</p>



<p>It is easy to see that in Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> one can convert <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> to a deterministic function of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> without violating any property of the original <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> guaranteed by the lemma, because after conditioning on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> one can fix any additional randomness in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|A_{\mathbf n'}\backslash \mathbf W|" class="latex" /> is minimized. This more deterministic version of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> can be somewhat more convenient for proving Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> and it was used in Rao’s proof, but allowing additional randomness in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> enabled Tao to construct <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> explicitly and obtain a simpler proof of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>.</p>



<p>We follow Tao’s idea of proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using Shannon entropy, but we present the proof in a more streamlined fashion with a slightly sharper constant in the bound (Tao proved a version of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> where the constant 4 was replaced by 5). Specifically, we present the proof in a way resembling a basic technique in combinatorics called <em><a href="https://en.wikipedia.org/wiki/Double_counting_(proof_technique)">counting in two ways</a></em>: one can show that two quantities are equal by showing that they both count the number of elements in the same set. Here, we estimate the entropy of the same collection of random variables in two different ways, and prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> by comparing the two estimates. The way we obtain the two estimates relies crucially on the chain rule of conditional entropy:</p>



<p class="has-text-align-center" id="eq-1"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29+%3D+%5Csum_%7Bi%3D1%7D%5Em%5Cmathbb+H%28%5Cmathbf+a_i%7C%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_%7Bi-1%7D%29.+%26%26++%281%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb H(\mathbf a_1,\ldots,\mathbf a_m) = \sum_{i=1}^m\mathbb H(\mathbf a_i|\mathbf a_1,\ldots,\mathbf a_{i-1}). &amp;&amp;  (1) \end{aligned}" class="latex" /></p>



<p>Equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> holds for arbitrary random variables <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf a_1,\ldots,\mathbf a_m" class="latex" /> taking values in a discret set. We say that equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> computes the entropy of <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(\mathbf a_1,\ldots,\mathbf a_m)" class="latex" /> via the chain </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbf+a_1%5Crightarrow+%5Ccdots+%5Crightarrow+%5Cmathbf+a_m.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}\mathbf a_1\rightarrow \cdots \rightarrow \mathbf a_m.\end{aligned}" class="latex" /></p>



<p>To prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>, we obtain two entropy estimates for the same collection of random variables by applying <a href="https://theorydish.blog/feed/#eq-1">(1)</a> to two different chains.</p>



<p>We need the following useful lemmas about Shannon entropy. We omit their proofs here as they can be found in Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, where many basic properties of Shannon entropy are also discussed. (We also highly recommend Tao’s other blog posts about <a href="https://terrytao.wordpress.com/2017/03/01/special-cases-of-shannon-entropy/">Shannon entropy</a> and the <a href="https://terrytao.wordpress.com/2009/08/05/mosers-entropy-compression-argument/">entropy compression argument</a>.)</p>



<p id="lm-3"><strong>Lemma 3</strong> (Subsets of small sets have small conditional entropy). <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%2C%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A,\mathbf B" class="latex" /> be finite random sets. Assume <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A\subseteq \mathbf B" class="latex" /> almost surely. Then <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%7C%5Cmathbf+B%29%5Cle+%5Cmathbb+E%7C%5Cmathbf+B%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf A|\mathbf B)\le \mathbb E|\mathbf B|." class="latex" /></em></p>



<p id="lm-4"><strong>Lemma 4</strong> (Information-theoretic interpretation of spread). <em>Let <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread family of finite sets. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> be chosen uniformly at random from <img src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]." class="latex" /> Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf S" class="latex" /> be a random set satisfying <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S%5Csubseteq+A_%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf S\subseteq A_{\mathbf n}" class="latex" /> almost surely. Then</em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+n%7C%5Cmathbf+S%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+n%29+-+%28%5Clog+R%29+%5Cmathbb+E%7C%5Cmathbf+S%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb H(\mathbf n|\mathbf S) \le \mathbb H(\mathbf n) - (\log R) \mathbb E|\mathbf S|.\end{aligned}" class="latex" /></em></p>



<p id="lm-5"><strong>Lemma 5</strong> (Information-theoretic properties of uniformly random subsets of fixed size). <em>Let <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> be a finite set. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> be a size-<img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta |X|" class="latex" /> subset of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> chosen uniformly at random for <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin+%280%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta\in (0,1]" class="latex" /> assuming <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta |X|" class="latex" /> is an integer. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A" class="latex" /> be a random subset of <img src="https://s0.wp.com/latex.php?latex=X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X." class="latex" /> </em>The following inequalities hold:</p>



<ol><li><em>(absorption) <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%5Ccup+%5Cmathbf+W%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+%2B+1+%2B+%281+%2B+%5Clog%281%2F%5Cdelta%29%29%5Cmathbb+E%7C%5Cmathbf+A%7C%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf A\cup \mathbf W) \le \mathbb H(\mathbf W) + 1 + (1 + \log(1/\delta))\mathbb E|\mathbf A|;" class="latex" /></em></li><li><em>(spread) if <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A\subseteq \mathbf W" class="latex" /> almost surely, then <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+W%7C%5Cmathbf+A%29%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+-+%5Clog%281%2F%5Cdelta%29%5Cmathbb+E%7C%5Cmathbf+A%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf W|\mathbf A)\le \mathbb H(\mathbf W) - \log(1/\delta)\mathbb E|\mathbf A|." class="latex" /></em></li></ol>



<p><em>Proof of Lemma </em><a href="https://theorydish.blog/feed/#lm-2">2</a>. If there exists <img src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n\in[N]" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=A_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_n" class="latex" /> is empty, we can simply choose <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27%3Dn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'=n" class="latex" /> deterministically. We assume henceforth that <img src="https://s0.wp.com/latex.php?latex=A_n%5Cneq+%5Cemptyset&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_n\neq \emptyset" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n\in[N]." class="latex" /></p>



<p>Following Tao’s proof, we construct <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}'" class="latex" /> by creating a conditionally independent copy <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%27+%3D+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\cup {\mathbf W}' = {A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" /> In other words, <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> have the same conditional distribution given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" />, and they are also conditionally independent given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" /> This construction guarantees that <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" /> almost surely, which implies that <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_{\mathbf n'}\backslash \mathbf W\subseteq A_{\mathbf n}\cap A_{\mathbf n'}" class="latex" /> almost surely.</p>



<p>It remains to prove that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}'" class="latex" /> constructed as above satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|." class="latex" /> We achieve this by estimating <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" /> using the chain rule <a href="https://theorydish.blog/feed/#eq-1">(1)</a> via two different chains, one for a lower bound and the other for an upper bound. The lower bound is obtained via the following chain:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbf n},{\mathbf W}\rightarrow {\mathbf n}',{\mathbf W}'. \end{aligned}" class="latex" /></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following way:</p>



<p class="has-text-align-center" id="eq-2"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29.+%26%26+%282%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') = {\mathbb H}({\mathbf n},{\mathbf W}) + {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}). &amp;&amp; (2)\end{aligned}" class="latex" /></p>



<p>By the independence of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf W}" class="latex" />,</p>



<p class="has-text-align-center" id="eq-3"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29.+%26%26+%283%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W}) = {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}). &amp;&amp; (3)\end{aligned}" class="latex" /></p>



<p>By the conditional independence of <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" /> and their identical conditional distribution,</p>



<p class="has-text-align-center" id="eq-4"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Cmathbb+H%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%5Cge+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%26+%284%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \mathbb H({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}) \\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W},{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ \ge {} &amp; {\mathbb H}({\mathbf n}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|,&amp; (4) \end{aligned}" class="latex" /></p>



<p>where the last inequality is by Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 1. Plugging <a href="https://theorydish.blog/feed/#eq-3">(3)</a>, <a href="https://theorydish.blog/feed/#eq-4">(4)</a> into <a href="https://theorydish.blog/feed/#eq-2">(2)</a>, we get the following lower bound:</p>



<p class="has-text-align-center" id="eq-5"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cge+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%285%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \ge {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|. &amp; (5)\end{aligned}" class="latex" /></p>



<p>Now we establish an upper bound for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" /> via a different chain:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%5Crightarrow+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%5Crightarrow+%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+W%7D%27.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbf n}\rightarrow {A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\rightarrow {\mathbf n}'\rightarrow {\mathbf W}\rightarrow {\mathbf W}'.\end{aligned}" class="latex" /></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following manner:</p>



<p class="has-text-align-center" id="eq-6"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+W%7D%27%29%5C%5C%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29.%26%286%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')\\ = {} &amp; {\mathbb H}({\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W},{\mathbf W}')\\= {} &amp; {\mathbb H}({\mathbf n})\\&amp; + {\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\\&amp; + {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\&amp; + {\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}')\\&amp; + {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}).&amp;(6)\end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}," class="latex" /></p>



<p class="has-text-align-center" id="eq-7"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5Cle%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26%26+%287%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\le{\mathbb E}|{A_{{\mathbf n}}}|. &amp;&amp; (7) \end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-4">4</a> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}'}}," class="latex" /></p>



<p class="has-text-align-center" id="eq-8"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+-+%28%5Clog+R%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%288%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}'|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}') - (\log R){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (8) \end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 2 and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Csubseteq+%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}\subseteq {\mathbf W}," class="latex" /></p>



<p class="has-text-align-center" id="eq-9"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29+%5C%5C++%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%28%5Clog+%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%289%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}') \\  \le {} &amp; {\mathbb H}({\mathbf W}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}) \\ \le {} &amp; {\mathbb H}({\mathbf W}) - (\log (1/\delta)){\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}|. &amp; (9)\end{aligned}" class="latex" /></p>



<p>Since <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D%27+%3D+%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5Cbackslash%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf W}' = ({A_{{\mathbf n}}}\cup{\mathbf W})\backslash({A_{{\mathbf n}'}}\backslash{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\backslash{\mathbf W}'\subseteq {A_{{\mathbf n}'}}," class="latex" /> by Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a>,</p>



<p class="has-text-align-center" id="eq-10"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26%2810%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp; {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}) \\ \le {} &amp; {\mathbb H}({A_{{\mathbf n}'}}\backslash{\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W})\\ \le {} &amp; {\mathbb E}|{A_{{\mathbf n}'}}|. &amp;(10)\end{aligned}" class="latex" /></p>



<p>Plugging <a href="https://theorydish.blog/feed/#eq-7">(7)</a>, <a href="https://theorydish.blog/feed/#eq-8">(8)</a>, <a href="https://theorydish.blog/feed/#eq-9">(9)</a>, <a href="https://theorydish.blog/feed/#eq-10">(10)</a> into <a href="https://theorydish.blog/feed/#eq-6">(6)</a> and simplifying using</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%2C%5Cquad+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%5C%5C%26%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+-+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n}') = {\mathbb H}({\mathbf n}),\quad {\mathbb E}|{A_{{\mathbf n}'}}| = {\mathbb E}|{A_{{\mathbf n}}}|,\\&amp;{\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}| = {\mathbb E}|{A_{{\mathbf n}'}}| - {\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|,\end{aligned}" class="latex" /></p>



<p>we get the following upper bound:</p>



<p class="has-text-align-center" id="eq-11"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cle+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+%2B+%282+-+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%5C%5C+%26+-+%28%5Clog+%28R%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%2811%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \le {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) + (2 - \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}| \\ &amp; - (\log (R\delta)){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (11)\end{aligned}" class="latex" /></p>



<p>Comparing <a href="https://theorydish.blog/feed/#eq-5">(5)</a> and <a href="https://theorydish.blog/feed/#eq-11">(11)</a> proves the desired inequality <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|.\square" class="latex" /></p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Moses Charikar, Omer Reingold, and Li-Yang Tan for valuable feedback and inspiring discussions.</p></div>







<p class="date">
by Lunjia Hu <a href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/"><span class="datestr">at May 19, 2021 11:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/">Tenure-track assistant professorship in algorithms and complexity theory at Lund University (apply by June 8, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CS department at Lund University invites applications for a tenure-track assistant professorship in algorithms and complexity theory. The application deadline is June 8, 2021. See <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a><br />
Email: jakob.nordstrom@cs.lth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/"><span class="datestr">at May 19, 2021 10:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8124">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/">Workshop on Local Algorithms (Guest post by Ronitt Rubinfeld)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The fifth <a href="http://www.local-algorithms.com/" target="_blank" rel="noreferrer noopener">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform" target="_blank" rel="noreferrer noopener">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters" target="_blank" rel="noreferrer noopener">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule" target="_blank" rel="noreferrer noopener">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter</li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?" target="_blank" rel="noreferrer noopener">the website</a>;</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/"><span class="datestr">at May 19, 2021 05:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/">Submit your papers to the 62nd FOCS!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://easychair.org/conferences/?conf=focs2021">submission server</a> for the 62nd FOCS is open! </p>



<p>Please read the <a href="https://focs2021.cs.colorado.edu/cfp/">Call for Papers</a> carefully before you submit. </p>



<p>Below are some important points:</p>



<ul><li>The title/abstract registration deadline is <strong>5 pm EDT</strong> on <strong>May 31st, 2021</strong>. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after this deadline.</li><li>The full paper submission deadline is <strong>5 pm EDT</strong> on <strong>June 3, 2021</strong>.</li><li>The conference is expected to take place <strong>physically</strong> in Boulder, Colorado <strong>Feb 7-10, 2022</strong>.</li><li>Papers that broaden the reach of the theory of computing, make foundational connections to other areas, or raise important problems and demonstrate that they can benefit from theoretical investigation and analysis are especially encouraged.</li><li>Reviewers will be asked to evaluate submissions <strong>both on conceptual and technical merits</strong>. So, authors are encouraged to emphasize both conceptual and technical novelty in the first few pages of the paper. </li><li>FOCS is an IEEE conference and as such, we will review papers in alignment with the IEEE ethics <a href="https://www.ieee.org/about/corporate/governance/p7-8.html">guidelines</a>. Authors are encouraged to reflect on these guidelines in shaping their work, dissemination, and submission.</li></ul>



<p></p>



<p>The PC is eagerly looking forward to your submissions!</p>



<p> </p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/"><span class="datestr">at May 19, 2021 02:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18714">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/">Hilbert Tenth On Rationals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>We must know. We will know—David Hilbert</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/mp/" rel="attachment wp-att-18716"><img width="150" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/mp-150x150.jpeg?resize=150%2C150&amp;ssl=1" class="alignright wp-image-18716" height="150" /></a></p>
<p>
Mihai Prunescu is at the Institute of Mathematics of the Romanian Academy. He works in logic and complexity theory. We recently discussed his <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">work</a> on Hilbert’s Tenth.</p>
<p>
Today we thought we would do a follow up discussion of a recent paper of his on the famous Hilbert’s Tenth problem. </p>
<p>
“Hilbert’s Tenth” is actually a suite of problems: is it decidable whether a finite set of equations of a certain kind have a common solution over a certain domain?  When the domain is the integers and the equations are polynomials set to zero we have the original form, which was shown to be undecidable by Yuriy Matiyasevich, completing work of Julia Robinson, Martin Davis, and Hilary Putnam.  But over the reals, the problem for polynomial equations is decidable, and Hilbert himself had shown this over the complex numbers.  The rational numbers are the key unsolved case, which I posted about <a href="https://rjlipton.wpcomstaging.com/2010/08/07/hilberts-tenth-over-the-rationals/">eleven</a> and <a href="https://rjlipton.wpcomstaging.com/2011/07/19/hilberts-10-5th-problem/">ten</a> years ago and <a href="https://rjlipton.wpcomstaging.com/2019/06/19/diophantine-equations/">again</a> more <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">recently</a>.</p>
<p>
The last post, two months ago, was about attempts to get leverage on the rationals by extending the equations to allow exponentiation.  Mihai added several helpful comments to that post, and with his blessing, this is trying to restart the discussion as busy pandemic-impacted university terms for Ken and some others we know draw to a close.</p>
<p>
</p><h2> Subtleties With Exponents </h2><p></p>
<p>
He has helped us understand a few subtleties in correspondence since then.  They concern both that rational numbers and exponentials are involved.  The focal point confounds our expectation that the square <img src="https://s0.wp.com/latex.php?latex=%7Br%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r^2}" class="latex" /> of any nonzero rational number <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> should be positive.  Here are two examples of issues:</p>
<ul>
<li> If <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = x^{1/2}}" class="latex" /> and we put <img src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r^2 = x^{1}}" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = -1}" class="latex" /> gives a negative value on the right-hand side.
</li><li>If <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = x^{1/4}}" class="latex" /> and we put <img src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r^2 = x^{1/2}}" class="latex" /> then the negative square root of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is a possible value.
</li></ul>
<p>
If we have two equations <img src="https://s0.wp.com/latex.php?latex=%7BE_1%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E_1(\vec{x})=0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BE_2%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E_2(\vec{x})=0}" class="latex" /> and we want to say that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bx%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\vec{x}}" class="latex" /> solves at least one of them, we can introduce the single equation</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5Ccdot+E_2%28%5Cvec%7Bx%7D%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_1(\vec{x})\cdot E_2(\vec{x}) = 0 " class="latex" /></p>
<p>
This is not affected by the subtleties.  But now suppose we want to do AND instead of OR.  The natural idea is to make the equation</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5E2+%2B+E_2%28%5Cvec%7Bx%7D%29%5E2+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_1(\vec{x})^2 + E_2(\vec{x})^2 = 0 " class="latex" /></p>
<p>
However, suppose we have the equation <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+%3D+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{1/4} = y}" class="latex" />.  This is the same as <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+-+y+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{1/4} - y = 0}" class="latex" />.  If we square it, we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B1%2F2%7D+%2B+y%5E2+-+2x%5E%7B1%2F4%7Dy+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x^{1/2} + y^2 - 2x^{1/4}y = 0 " class="latex" /></p>
<p>
The abstract worry is that this could allow solutions where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{1/2}}" class="latex" /> is chosen <i>negative</i>, though not intended as the square of <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{1/4}}" class="latex" /> (whether positive or negative).  Mihai gave us a simple example of two exponential equations where unintended solutions occur after squaring and adding them.  Consider</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5Ev+%3D+0+%5Cwedge+w%5Ex+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  u^v = 0 \wedge w^x = 0. " class="latex" /></p>
<p>
versus</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5E%7B2v%7D+%2B+w%5E%7B2x%7D+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  u^{2v} + w^{2x} = 0. " class="latex" /></p>
<p>
The former set is solved only by <img src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+w+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u = w = 0}" class="latex" /> with the exponents nonzero.  The latter, however, allows solutions illustrating both issues above:</p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+1%2C%5C%3B+w+%3D+-1%2C%5C%3B+x+%3D+1%2F2%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u = v = 1,\; w = -1,\; x = 1/2;}" class="latex" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+w+%3D+1%2C%5C%3B+x+%3D+1%2F4%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u = v = w = 1,\; x = 1/4;}" class="latex" />
</li></ul>
<p>
where in the latter, the negative square root of <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> is taken.  What this means is that with rationals and/or exponentials we must watch our manipulations more carefully.  </p>
<p>
</p><h2> The Theorem </h2><p></p>
<p>
The following <a href="https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/exponential-diophantine-problem-for-mathbb-q/48F3FBED93B3138F34D39A52DB560BC0">theorem</a> is essentially due to Mihai: </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BP%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x_1,\dots,x_n)}" class="latex" /> be an integer polynomial. Then it is undecidable to determine whether there are <img src="https://s0.wp.com/latex.php?latex=%7Ba_1%2C%5Cdots%2Ca_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a_1,\dots,a_n}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb Q}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb_1%2C%5Cdots%2Cb_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b_1,\dots,b_n}" class="latex" /> also in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb Q}}" class="latex" /> so that </em></p><em>
<ol>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7BP%28a_1%2C%5Cdots%2Ca_n%29%3D0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(a_1,\dots,a_n)=0}" class="latex" /> and <p></p>
</li><li>
For each <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%2C%5Cdots%2Cn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k=1,\dots,n}" class="latex" /> 	<p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_k+%3D+2%5E%7Ba_k%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  b_k = 2^{a_k}. " class="latex" /></p>
</li></ol>
</em><p><em></em>
</p></blockquote>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb Q}}" class="latex" /> as usual is the rationals.  The full question whether the above theorem can be proved without any exponential functions <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^x}" class="latex" /> remains open, after much attention by many researchers. </p>
<p>
</p><p></p><h2> Some History </h2><p></p>
<p></p><p>
The reason we say “essentially” due to Prunescu must be explained. He recently published an article showing that <i>The Exponential Diophantine Problem For <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb Q}}" class="latex" /></i> is undecidable: It is in the <i>Journal of Symbolic Logic</i> (JSL), Volume 85, Issue 2. </p>
<p>
His clever proof showed that the solutions <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x,y)}" class="latex" /> over the rationals of 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5Ey+%3D+y%5Ex+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x^y = y^x " class="latex" /></p>
<p>lie in a one-dimensional space. This space is parameterized by the integers and so it can be used to define the integers. This immediately proves the undecidability by reduction to the classic Hilbert’s Tenth over the integers.</p>
<p>
Ken and I then wrote a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">post</a> on our blog GLL explaining his JSL result. Mihai kindly added a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/#comment-116365">comment</a> on the post that basically stated the above theorem with <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^y}" class="latex" />. </p>
<p>
I was initially puzzled since the above theorem seems to be stronger than his JSL theorem. His published result uses <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y}" class="latex" /> a binary exponential function and the new theorem needs only the unary function <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^y}" class="latex" />. Now it must be said that the cases could be incomparable—because the extra freedom of allowing any rational base <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y}" class="latex" /> could promote a different kind of analysis that makes the existence of such solutions decidable.  The equivalence logic coming back from undecidable cases over the integers might not apply in the above reduction.  But the undecidability when the base is fixed to be <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> still strikes me as capturing the general import.</p>
<p>
What’s more, the theorem fixing <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^x}" class="latex" /> as the base has a much simpler proof.  It requires only a lemma that extends the famous Euclid Theorem: The value <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{2}}" class="latex" /> is not rational.</p>
<p></p><h2> Proof of the Lemma </h2><p></p>
<p>
The proof rests on the famous result of Matiyasevich on Hilbert’s Tenth and the following lemma: </p>
<blockquote><p><b>Lemma 2</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \ge 0}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \in {\mathbb Q}}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ex+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^x \in {\mathbb Q}}" class="latex" />. Then <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is an integer. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  Clearly we can assume that <img src="https://s0.wp.com/latex.php?latex=%7Bx%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x&gt;0}" class="latex" />. We will prove the lemma in two steps. </p>
<ol>
<li>
The value <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^x}" class="latex" /> is an integer. <p></p>
</li><li>
The value <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is an integer.
</li></ol>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = r/s}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Br%2C+s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r, s}" class="latex" /> integers that are co-prime and <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r \ge 1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s \ge 1}" class="latex" />. </p>
<p>
<i>Step (1):</i> Now let 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Ex+%3D+%5Cfrac%7Bu%7D%7Bv%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^x = \frac{u}{v} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> are co-prime integers. We can assume that <img src="https://s0.wp.com/latex.php?latex=%7B%7Cv%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|v| \ge 2}" class="latex" />; otherwise, step (1) is true. Now 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+%5Cfrac%7Bu%5Es%7D%7Bv%5Es%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^r = \frac{u^s}{v^s}. " class="latex" /></p>
<p>But <img src="https://s0.wp.com/latex.php?latex=%7Bu%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u^s}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v^s}" class="latex" /> are co-prime since <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> are co-prime. But this implies that <img src="https://s0.wp.com/latex.php?latex=%7B2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^r}" class="latex" /> is not an integer, since <img src="https://s0.wp.com/latex.php?latex=%7B%7Cv%5Es%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|v^s| \ge 2}" class="latex" />. Note this uses that <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r \ge 1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s \ge 1}" class="latex" />. This contradiction proves step (1).</p>
<p>
<i>Step (2):</i> By step (1) we thus have that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%2Fs%7D+%3D+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^{r/s} = y " class="latex" /></p>
<p>for some integer <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" />. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+y%5Es.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^r = y^s. " class="latex" /></p>
<p>Thus <img src="https://s0.wp.com/latex.php?latex=%7By%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y^s}" class="latex" /> is an integer and so by unique factorization it follows that <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" /> must be an integer power of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+2%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y = 2^k}" class="latex" />. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%7D+%3D+2%5E%7Bsk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^{r} = 2^{sk}. " class="latex" /></p>
<p>This implies that <img src="https://s0.wp.com/latex.php?latex=%7Br%3Dsk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r=sk}" class="latex" /> and so that <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = r/s=k}" class="latex" /> is an integer. This proves step (2). <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Box" class="latex" /></p>
<p></p><h2> Open Problems </h2><p></p>
<p>
We have shared the topic also with Joël Ouaknine and thank him for his comments as well. Addressing our readers, do you have any further comments? What do you think about the rationals? </p>
<p>
Ken related an idea while corresponding with Mihai: Suppose we introduce a special squaring function <img src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Sq}" class="latex" /> with the properties that</p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7BSq%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Sq(r)}" class="latex" /> is always positive for nonzero <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" />.<p></p>
</li><li>
In particular, <img src="https://s0.wp.com/latex.php?latex=%7BSq%28x%5E%7B1%2F4%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Sq(x^{1/4})}" class="latex" /> gives only the positive square root of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />.<p></p>
</li><li>
Terms with <img src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Sq}" class="latex" /> may not appear in exponents, nor be bases of them.  They may only be added and multiplied.
</li></ul>
<p>
The motivation is that now</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Sq%28E_1%28%5Cvec%7Bx%7D%29+%2B+Sq%28E_2%28%5Cvec%7Bx%7D%29%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Sq(E_1(\vec{x}) + Sq(E_2(\vec{x})) = 0 " class="latex" /></p>
<p>
does give the AND of the two equations.  What then becomes of the above undecidability proofs, and does the AND property have other applications?</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/"><span class="datestr">at May 19, 2021 12:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1517">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1517">Announcing WOLA’21 (Workshop on Local Algorithms)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The fifth <a href="http://www.local-algorithms.com">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter </li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?">the website</a>; to stay up to date with the latest announcements concerning WOLA, <a href="https://lists.csail.mit.edu/mailman/listinfo/wola">join our mailing list</a>!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1517"><span class="datestr">at May 19, 2021 12:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-6555947.post-8966314939306062304">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/suresh.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html">Transitions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I've been at the U of Utah and Salt Lake City for 14 years (14.5 really). It was my first academic job and the longest time I've spent anywhere (throughout my whole life). So it's a little hard to accept that I'm moving to my next adventure. </p><p>It's a two-part adventure, because why make one move when you can make two. </p><p>Firstly, as of today, I'm going to working with Alondra Nelson at the White House Office of Science and Technology Policy, advising on matters relating to fairness and bias in tech systems. This is a scary and exciting new position, and I hope to help to nudge things along just a bit further in the direction of tech that can help more than it harms, especially for those who've been left behind in our rush to an algorithmically controlled future. </p><p>Secondly, I'm moving to Brown University to join the CS department there as well as their Data Science Initiative. Together with Seny Kamara and others, I'm going to start a new center on Computing for the People, to help think through what it means to do computer science that truly responds to the needs of people, instead of hiding behind a neutrality that merely gives more power to those already in power. </p><p>Lots of changes, and because of the pandemic, all this will happen in slow machine, but it's a whirlwind of emotions (and new clothes - apparently tech conference T-shirts don't work in formal settings - WHO KNEW!!!). </p><p><br /></p><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=aZaRzVm3CmE:2HobITiPYuo:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=aZaRzVm3CmE:2HobITiPYuo:63t7Ie-LG7Y"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=63t7Ie-LG7Y" border="0" /></a>
</div><img width="1" alt="" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/aZaRzVm3CmE" height="1" /></div>







<p class="date">
by Suresh Venkatasubramanian (noreply@blogger.com) <a href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html"><span class="datestr">at May 17, 2021 02:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7172198197646487541">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html">Why do countries and companies invest their own money (or is it?) in Quantum Computing (Non-Rhetorical)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> There have been some recent blog posts by Scott (see <a href="https://www.scottaaronson.com/blog/?p=5387">here</a>) and Lance (see <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">here</a>)  about the hype for SHORT TERM APPLICATIONS of Quantum Computing, which they both object to. </p><p>I have a question that has been touched on but I want to get it more out there.</p><p>PREAMBLE TO QUESTION:  The following scenarios, while distasteful, do make sense: </p><p>a) A researcher on their grants exaggerates or even out-right lies about the applications of their work. </p><p>b) A journalist in their articles exaggerates or even out-right lies about the applications of the science they are covering.</p><p>c) A company exaggerates or even out-right lies about the applications of their project to a venture capitalist or other kind of investor. </p><p>QUESTION: </p><p>Why does a company or country invest THEIR OWN MONEY into Quantum Computing which is unlikely to have a short term profit or benefit? Presumably they hire honest scientists to tell them the limits of the applications in the short term. </p><p>ANSWERS I HAVE GOTTEN FROM ASKING THIS</p><p>1) QC might be able to do something cool and profitable, like factoring, or simulating physics quantum experiments or something else, in the short term. Quantum Crypto is already happening, and that's a close cousin of Quantum Computing. </p><p>2) QC might be able to do something cool and profitable (like in (1)) in the long term, and both companies and countries think they will be around for a long time. (For a list of America's 10 oldest companies see <a href="https://www.businessnewsdaily.com/8122-oldest-companies-in-america.html">here</a>.) </p><p>3) The company or country is in this for the long term, not for a practical project, but because they realize that doing GOOD SCIENCE is of a general benefit (this might make more sense for a country than a company). And funding Quantum Computing is great for science. </p><p>4) Bait and Switch: The company claims they are doing Quantum to attract very smart people to work with them, and then have those smart people do something else.</p><p>5) (this is a variant of 1) While Quantum Computing may not have any short term applications, there will be classical applications INSPIRED by it (this has already happened, see <a href="https://www.scottaaronson.com/blog/?p=3880">here</a>).</p><p>6) Some of these companies make money by applying for GRANTS to do QC, so its NOT their money. In fact, they are using QC to  GET money.</p><p>7) For a country its not the leaders money, its that Taxpayer's money- though that still leaves the question of why spend Taxpayer money on this and not on something else.</p><p>8) For a company its not their money- its Venture Capitalists  and others (though for a big company like Google I would think it IS their money). </p><p>9) The scientists advising the company or country are giving them bad (or at least self-serving) advice so that those scientists can profit- and do good science. So this is a variant of (3) but without the company or country knowing it. </p><p>10) In some countries and companies group-think sets in, so if the leader (who perhaps is not a scientist) thinks intuitively that QC is good, the scientists who work for them, who know better, choose to not speak up, or else they would lose their jobs...or worse. </p><p>11) For countries this could be like going to the moon: Country A wants to beat Country B to the moon for bragging rights. Scientists get to do good research even if they don't care about bragging rights. </p><p>12) (similar to 11 but for a company) If a company does great work on QC then it is good publicity for that company. </p><p>13) Some variant of the <a href="https://en.wikipedia.org/wiki/Greater_fool_theory">greater fool theory</a>. If so, will there be a bubble? A bail-out? </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html"><span class="datestr">at May 16, 2021 05:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5490">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5490">What I told my kids</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa.  And that’s true.</p>



<p>But it’s also not so complicated that there are no clearly identifiable good guys or bad guys.  It’s just that they cut across the sides.</p>



<p>The good guys are anyone, on either side, whose ideal end state is two countries, Israel and Palestine, living side by side in peace.</p>



<p>The bad guys are anyone, on either side, whose ideal end state is the other side being, if not outright exterminated, then expelled from its current main population centers (ones where it’s been for several generations or more) and forcibly resettled someplace far away.</p>



<p>(And those whose ideal end state is everyone living together with no border — possibly as part of the general abolition of nation-states?   They’re not bad guys; they can plead insanity.  [<strong>Update:</strong> <a href="https://www.scottaaronson.com/blog/?p=5490#comment-1889918">See here</a> for clarifications!])</p>



<p>Hamas are bad guys.  They fire rockets indiscriminately at population centers, hoping to kill as many civilians as they can.  (Unfortunately for them and fortunately for Israel, they’re not great at that, and also they’re aiming at a target that’s world-historically good at defending itself.)</p>



<p>The IDF, whatever else you say about it, sends evacuation warnings to civilians before it strikes the missile centers that are embedded where they live.  Even if Hamas could aim its missiles, the idea of it extending the same courtesy to Israeli civilians is black comedy.</p>



<p>Netanyahu is not as bad as Hamas, because he has the power to kill millions of Palestinians and yet kills only hundreds … whereas if Hamas had the power to kill all Jews, it told the world in its charter that it would immediately do so, and it’s acted consistently with its word.</p>



<p>(An aside: I’m convinced that Hamas has the most top-heavy management structure of any organization in the world.  Every day, Israel takes out another dozen of its <em>most senior, highest-level</em> commanders, apparently leaving hundreds more.  How many senior commanders do they have?  Do they have even a single junior commander?)</p>



<p>Anyway, not being as bad as Hamas is an extremely low bar, and Netanyahu is a thoroughly bad guy.  He’s corrupt and power-mad.  Like Trump, he winks at his side’s monstrous extremists without taking moral responsibility for them.  And if it were ever possible to believe that he wanted two countries as the ideal end state, it hasn’t been possible to believe that for at least a decade.</p>



<p>Netanyahu and Hamas are allies, not enemies.  Both now blatantly, obviously rely on the other to stay in power, to demonstrate their worldview and thereby beat their internal adversaries.</p>



<p>Whenever you see anyone opine about this conflict, on Facebook or Twitter or in an op-ed or anywhere else, keep your focus relentlessly on the question of what that person <em>wants</em>, of what they’d do if they had unlimited power.  If they’re a Zionist who talks about how “there’s no such place as Palestine,” how it’s a newly invented political construct: OK then, does that mean they’d relocate the 5 million self-described Palestinians to Jordan?  Or where?  If, on the other side, someone keeps talking about the “Zionist occupation,” always leaving it strategically unspecified whether they mean just the West Bank and parts of East Jerusalem or also Tel Aviv and Haifa, if they talk about the Nakba (catastrophe) of Israel’s creation in 1947 … OK then, what’s to be done with the 7 million Jews now living there?  Should they go back to the European countries that murdered their families, or the Arab countries that expelled them?  Should the US take them all?  Out with it!</p>



<p>Don’t let them dodge the question.  Don’t let them change the subject to something they’d much rather talk about, like the details of the other side’s latest outrage.  Those details always seem so important, and yet everyone’s stance on every specific outrage is like 80% predictable if you know their desired end state.  So just keep asking directly about their desired end state.</p>



<p>If, like me, you favor two countries living in peace, then you need never fear anyone asking you the same thing.  You can then shout your desired end state from the rooftops, leaving unsettled only the admittedly-difficult “engineering problem” of how to get there.  Crucially, whatever their disagreements or rivalries, everyone trying to solve the same engineering problem is in a certain sense part of the same team.  At least, there’s rarely any reason to kill someone trying to solve the same problem that you are.</p>



<p>“What is this person’s ideal end state?”  Just keep asking that and there’s a limit to how wrong you can ever be about this.  You can still make factual mistakes, but it’s then almost impossible to make a moral mistake.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5490"><span class="datestr">at May 16, 2021 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.amstat.org/asa/News/Greek-Statistician-Found-Liable-for-Slander-Continues-to-Face-Persecution.aspx">Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou</a> (<a href="https://mathstodon.xyz/@11011110/106164322094769198">\(\mathbb{M}\)</a>) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/arxivabs">That other microblogging site has a bot specifically devoted to replacing links to pdf versions of arxiv preprints by links to the abstracts of the same preprint</a> (<a href="https://mathstodon.xyz/@11011110/106169351037107488">\(\mathbb{M}\)</a>). Is there something like that for mastodon? If not there should be.</p>
  </li>
  <li>
    <p>For a centrally symmetric star-shaped set in the plane, each line through the center cuts its perimeter into two equal-length curves. But these are not the only shapes with this property: 18th-century Jesuit polymath <a href="https://en.wikipedia.org/wiki/Roger_Joseph_Boscovich">Roger Boscovich</a> observed that a heart-like shape formed by three semicircles has the same property (<a href="https://mathstodon.xyz/@11011110/106175759357772585">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.2307/2974900">via</a>).</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/boscovich.svg" alt="Boscovich's cardioid, with its perimeter bisected by a line through its cusp" /></p>
  </li>
  <li>
    <p><a href="https://picturethismaths.wordpress.com/2020/10/31/three-correlations-and-a-samosa/">Three correlations and a samosa</a> (<a href="https://mathstodon.xyz/@11011110/106180193506681489">\(\mathbb{M}\)</a>, <a href="https://picturethismaths.wordpress.com/2021/02/03/three-correlations-and-other-statistical-models/">see also</a>, <a href="https://picturethismaths.wordpress.com/2021/03/18/three-correlations-and-an-algebraic-classification/">see also</a>). \(3\times 3\) symmetric matrices with unit diagonals form a three-dimensional linear space, in which the samosa is a curvy 3d convex set representing the positive definite matrices. Taking sections of it allows you to infer the possible correlations between two variables, given each of their correlations with a third.</p>
  </li>
  <li>
    <p>I couldn’t resist picking up a copy of <em>The Architecture of Trees</em> (<a href="https://mathstodon.xyz/@11011110/106184861337043968">\(\mathbb{M}\)</a>), a coffee-table book centered on pen-and-ink illustrations of the summer and winter forms of over 200 types of tree, on a recent visit to the Mendocino Coast Botanical Gardens (beautiful this time of year with many flowers in bloom). Some reviews: <a href="https://www.startribune.com/new-book-is-tree-tome-like-few-others-part-science-part-art-marvel/507788702/">1</a>, <a href="https://www.thedailybeast.com/the-architecture-of-trees-travel-with-the-book-that-captures-the-beauty-of-trees">2</a>, <a href="http://spacing.ca/national/2020/08/04/book-review-the-architecture-of-trees/ https://dirt.asla.org/2019/07/10/the-architecture-of-trees-2/">3</a>.</p>
  </li>
  <li>
    <p><a href="http://www.xl-muse.com/html/en/index.php?ac=article&amp;at=read&amp;did=239">Dujiangyan Zhongshuge</a> (<a href="https://mathstodon.xyz/@11011110/106192492039422969">\(\mathbb{M}\)</a>, <a href="https://www.thisiscolossal.com/2021/05/x-living-dujiangyan-zhongshuge/">via</a>), bookstore in Chengdu with mirrored floors and ceilings creating the feeling of an infinite Escher palace of books.</p>
  </li>
  <li>
    <p><a href="http://keenan.is/illustrating/">Illustrating geometry</a> (<a href="https://mathstodon.xyz/@11011110/106204129125568328">\(\mathbb{M}\)</a>). An apparently-defunct blog from 2016–2017 with several interesting posts about technical illustrations in mathematics.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/391885/440">Which \(n\times n\times n\) grids have Hamiltonian cycles that turn at every step?</a> (<a href="https://mathstodon.xyz/@11011110/106218352172585271">\(\mathbb{M}\)</a>) After I linked to this, a later answer pointed to the recent book <em>Bicycle or Unicycle？A Collection of Intriguing Mathematical Puzzles</em>, by Stan Wagon and Daniel Velleman, which has solutions for all even \(n\) on pp. 89–96. A simple parity argument shows that it’s impossible on odd grids, but the same book conjectures that these have Hamiltonian paths except in the case \(n=3\).</p>
  </li>
  <li>
    <p><a href="https://wg2021.mimuw.edu.pl/accepted-papers/">Accepted papers to the International Workshop
on Graph-Theoretic Concepts in Computer Science</a> (<a href="https://mathstodon.xyz/@11011110/106220947095567586">\(\mathbb{M}\)</a>). My paper “<a href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html">The Graphs of Stably Matchable Pairs</a>” is one of them. The conference will be online June 23–25. Unlike many conferences, WG prepares the proceedings after the conference, to allow authors to incorporate feedback from presentations. Details of how to participate don’t seem to be online but I’m sure they’ll be made available soon.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/05/13/why-you-shouldnt-be-too-pessimistic/">Why you shouldn’t be too pessimistic</a> (<a href="https://mathstodon.xyz/@11011110/106230884405427138">\(\mathbb{M}\)</a>). Igor Pak on the nature of mathematical conjectures.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/careers/2021/05/two-surnames-no-hyphen-claiming-my-identity-latin-american-scientist">Two surnames, no hyphen: Claiming my identity as a Latin American scientist</a> (<a href="https://mathstodon.xyz/@11011110/106241178695539953">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/05/15">via</a>). Johana Goyes Vallejos in <em>Science</em>, on respect for diversity in naming styles in academia.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/15/linkage.html"><span class="datestr">at May 15, 2021 05:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html">Congratulations, Dr. Havvaei!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, <a href="https://www.ics.uci.edu/~ehavvaei/">Elham Havvaei</a> (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</p>

<p>Haleh is Iranian, but came to UC Irvine via Florida in 2016, after working for a master’s degree with Narsingh Deo at the University of Central Florida. I’ve written here already about the research she’s done with me: <a href="https://11011110.github.io/blog/2018/10/07/recognizing-sparse-leaf.html">reconstructing unknown trees from graphs connecting close-together leaves</a> (from IPEC 2018 and Algorithmica 2020), <a href="https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone.html">simplifying graphs whose vertices and edges represent the milestones, tasks, and ordering constraints of a project</a> (SWAT 2020), and <a href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html">classifying problems of finding large subgraphs with one property in graphs with another property</a> (not yet published).</p>

<p>Her next step will be taking a position at Twitter in San Francisco, as a data scientist.</p>

<p>Congratulations, Haleh!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106236739159044402">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html"><span class="datestr">at May 14, 2021 06:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/070">TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/070"><span class="datestr">at May 13, 2021 05:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/">Matrix—The Meeting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img width="192" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" class="alignright wp-image-18739" height="128" /></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{20-60}" class="latex" /> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img width="384" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" class="alignright wp-image-18742" height="216" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br />
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p></p>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p></p><h2> The Question </h2><p></p>
<p></p><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Ax = b " class="latex" /></p>
<p>Alice could go off and return the <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> that solves the system. Or she could say there is no such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. Or she could say there are many such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> I have. <p></p>
</li><li>
I plan to see what the norm of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is. <p></p>
</li><li>
Or, I plan to see what <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_1}" class="latex" /> is. <p></p>
</li><li>
Or, I could be just happy to know that there is some <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. <p></p>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p></p><h2> One View </h2><p></p>
<p></p><p>
What does it mean to solve the equation <img src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Ax=b}" class="latex" />, for an invertible matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" />? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br />
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br />
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/"><span class="datestr">at May 13, 2021 12:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1929264998273205739">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html">Cryptocurrency, Blockchains and NFTs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p></p><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br /></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html"><span class="datestr">at May 13, 2021 11:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html">The constructive solid geometry of piecewise-linear functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/minmax.svg" alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" /></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg" alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" /></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html"><span class="datestr">at May 12, 2021 05:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/069">TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/069"><span class="datestr">at May 12, 2021 08:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/glm_saga/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/glm_saga/">Debuggable Deep Networks: Sparse Linear Models (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/intro.png" alt="Overview" /></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png" alt="Examples of feature visualization" /></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png" alt="Examples of word cloud visualization" /></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png" alt="Feature visualization dump" /></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas width="400" id="ablation_dense_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img src="https://gradientscience.org/feed.xml" id="reg" />
        <div id="reg_slider"></div>
        <div class="quarterblock"> </div>
        <div style="text-align: center;" class="quarterblock">Accuracy: <span id="reg_accuracy"></span>%</div>
        <div style="text-align: center;" class="quarterblock">Non-zero: <span id="reg_sparsity"></span>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas width="400" id="ablation_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
    </div>
    <div id="zoom">
        <img src="https://gradientscience.org/feed.xml" id="feature_big" />
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>







<p class="date">
<a href="https://gradientscience.org/glm_saga/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/debugging/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/debugging/">Debuggable Deep Networks: Usage and Evaluation (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png" height="350" /></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png" alt="Wordcloud visualization of feature used in unbiased BERT" /></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" value="" id="toxic_">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="christianity" id="toxic_christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="African" id="toxic_African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="Catholic" id="toxic_Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"></span>
        <canvas width="400" id="toxic" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png" height="350" /></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"></div>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>

</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas width="400" id="sst_canvas" height="200"></canvas>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png" alt="ImageNet counterfactuals" /></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/problematic.png" alt="Problematic features" /></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png" height="350" /></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"></div>
  <div style="float: none;" class="blocktxt" id="mislabels"> </div>
  <div style="clear: both;" id="misimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"></span><br /><span style="color: red;" class="choices_info_text"><b></b></span><br /><span style="color: green;" class="choices_info_text"><b></b></span><br /><hr /><h3 style="text-align: center;"><h3><div style="text-align: center; font-weight: 300; margin: 0.75em auto;" class="sp_txt"></div><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png" /><hr style="margin: 0.75em auto;" /><div style="text-align: center; font-weight: 200;" class="sp_txt"><span></span></div><hr style="margin: 0.3em auto;" /><h3 style="text-align: center;"><h3><div style="text-align: center;" class="sp_txt"><span style="font-weight: 200;"></span></div><br /><span></span></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"></span><span class="choices_info_text"></span><br /><span class="choices_info_text"></span><div class="mis_txt blocktxt thirdblock"><br /><span class="widgetheading"></span></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;" /></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div></div></h3></h3></div>







<p class="date">
<a href="https://gradientscience.org/debugging/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">STOC Test of time award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br /><br />The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br /><br />Please nominate the papers you think have most influenced our field!</li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/"><span class="datestr">at May 11, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br />
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/"><span class="datestr">at May 10, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5486">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br /></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br /></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5486"><span class="datestr">at May 10, 2021 05:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8609684815037895352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html">Trump, Facebook, and ComplexityBlog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html"><span class="datestr">at May 10, 2021 12:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">Arc-triangle tilings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg" alt="Tiling by ordinary triangles" /></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg" alt="Tiling by arc-triangles with two curved sides" /></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg" alt="Tiling by arc-triangles with three curved sides" /></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pinwheels.svg" alt="Tiling by arc-triangles with two curved sides, without reflection" /></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html"><span class="datestr">at May 09, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1512">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1512">News for April 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper.<em> (We may have missed some; if so, please let us know in the comments!)</em></p>



<p><strong>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</strong>, by Sepehr Assadi and Vishvajeet N (<a href="https://arxiv.org/abs/2104.04908">arXiv</a>). This paper establishes space vs. pass trade-offs lower bounds for streaming algorithms, for a variety of graph tasks: that is, of the sort “any \(m\)-pass-streaming algorithm for task \(\mathcal{T}\) must use memory at least \(f(m)\).” The tasks considered include graph property estimation (size of the maximum matching, of the max cut, of the  weight of the MST) and property testing for sparse graphs (connectivity, bipartiteness, and cycle-freeness). The authors obtained exponentially improved lower bounds for those, via reductions to a relatively standard problem, (noisy) gap cycle counting, for which they establish their main lower bound. As a key component of their proof, they prove a general direct product result (XOR lemma) for the streaming setting, showing that the advantage for solving the XOR of \(\ell\) copies of a streaming predicate \(f\) decreases exponentially with \(\ell\). </p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1512"><span class="datestr">at May 08, 2021 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/068">TR21-068 |  Quantum Proofs of Proximity | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Subhayan Roy Moulik, 

	Justin Thaler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the systematic study of QMA algorithms in the setting of property testing, to which we refer as QMA proofs of proximity (QMAPs). These are quantum query algorithms that receive explicit access to a sublinear-size untrusted proof and are required to accept inputs having a property $\Pi$ and reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a minuscule portion of their input.

Our algorithmic results include a general-purpose theorem that enables quantum speedups for testing an expressive class of properties, namely, those that are succinctly decomposable. Furthermore, we show quantum speedups for properties that lie outside of this family, such as graph bipartitneness.

We also investigate the complexity landscape of this model, showing that QMAPs can be exponentially stronger than both classical proofs of proximity and quantum testers. To this end, we extend the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to prove quantum property testing lower bounds via reductions from communication complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory of Computing, 2016).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/068"><span class="datestr">at May 08, 2021 11:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/067">TR21-067 |  Variety Evasive Subspace Families | 

	Zeyu Guo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of constructing explicit variety evasive subspace families. Given a family $\mathcal{F}$ of subvarieties of a projective or affine space, a collection $\mathcal{H}$ of projective or affine $k$-subspaces is $(\mathcal{F},\epsilon)$-evasive if for every $\mathcal{V}\in\mathcal{F}$, all but at most $\epsilon$-fraction of $W\in\mathcal{H}$ intersect every irreducible component of $\mathcal{V}$ with (at most) the expected dimension. The problem of constructing such an explicit subspace family generalizes both deterministic black-box polynomial identity testing (PIT) and the problem of constructing explicit (weak) lossless rank condensers. 

Using Chow forms, we construct explicit $k$-subspace families of polynomial size that are evasive for all varieties of bounded degree in a projective or affine $n$-space. As one application, we obtain a complete derandomization of Noether's normalization lemma for varieties of bounded degree in a projective or affine $n$-space. In another application, we obtain a simple polynomial-time black-box PIT algorithm for depth-4 arithmetic circuits with bounded top fan-in and bottom fan-in that are not in the Sylvester-Gallai configuration, improving and simplifying a result of Gupta (ECCC TR 14-130).

As a complement of our explicit construction, we prove a lower bound for the size of $k$-subspace families that are evasive for degree-$d$ varieties in a projective $n$-space. When $n-k=\Omega(n)$, the lower bound is superpolynomial unless $d$ is bounded. The proof uses a dimension-counting argument on Chow varieties that parametrize projective subvarieties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/067"><span class="datestr">at May 08, 2021 06:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html">Congratulations, Dr. Matias!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://pmatias.com/">Pedro Ascensao Ferreira Matias</a>, one of the students working with Mike Goodrich in the UC Irvine <a href="https://www.ics.uci.edu/~theory/">Center for Algorithms and Theory of Computation</a>, passed his Ph.D. defense today.</p>

<p>Pedro is Portuguese, and came to UCI after a bachelor’s degree from the University of Coimbra in Portugal and a master’s degree from Chalmers University of Technology in Sweden.</p>

<p>The general topic of Pedro’s research is “exact learning”, the inference of structured information from queries or other smaller pieces of data. I’ve written here before about my work with Matias on <a href="https://11011110.github.io/blog/2019/02/21/mutual-nearest-neighbors.html">nearest-neighbor chains</a> and on <a href="https://11011110.github.io/blog/2019/08/17/footprints-in-snow.html">tracking paths in planar graphs</a>, the problem of placing sensors on a small subset of vertices so that, by detecting the order in which a path reaches each sensor, you can uniquely determine the whole path. His dissertation combines the tracking paths work with a second paper on tracking paths (“How to Catch Marathon Cheaters: New Approximation Algorithms for Tracking Paths”, <a href="https://arxiv.org/abs/2104.12337">arXiv:2104.12337</a>, to appear at WADS 2021), and a paper on reconstructing periodic and near-periodic strings from sublinear numbers of queries (“Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors and Jumbled-Index Queries in String Reconstruction”, <a href="https://arxiv.org/abs/2007.08787">arXiv:2007.08787</a>, in SPIRE 2020). He also has recent papers on reconstructing trees in SPAA 2020 and ESA 2020.</p>

<p>After finishing his doctorate, Pedro’s next position will be working for Facebook.</p>

<p>Congratulations, Pedro!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106196168129163033">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html"><span class="datestr">at May 07, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">Faculty Full Professor at University of Bamberg, Germany (apply by June 18, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems.</p>
<p>Website: <a href="https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/">https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/</a><br />
Email: michael.mendler@uni-bamberg.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/"><span class="datestr">at May 07, 2021 02:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1659">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/">Fair Clustering with Probabilistic Group Membership</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br /><br />Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h \in {\cal H}" class="latex" /> to designate its group membership and we were to consider the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="l_h" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="u_h" class="latex" /> are the lower and upper pre-set proportionality bounds for color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C_i" class="latex" /> denotes the points in cluster <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="i" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C^h_i" class="latex" /> denotes the subset of those points with color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br /></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" class="wp-image-1735" height="151" />Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br /></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" class="wp-image-1737" height="210" />Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-center, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-median, and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p></p></div>







<p class="date">
by seyed2357 <a href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/"><span class="datestr">at May 07, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
