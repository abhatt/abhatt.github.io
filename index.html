<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="/favicon.ico">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="https://blog.xrds.acm.org/tag/theory/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<br>
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://nanoexplanations.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nanoexplanations.wordpress.com" title="Nanoexplanations">Aaron Sterling</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Aleksander Madry</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://andysresearch.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://andysresearch.blogspot.com/" title="Andy's Math/CS page">Andy's Math/CS page</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="http://feedworld.net/toc/scripts/stackexchange.py/main" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="http status 503">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange Community Blog</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://teachingintrotocs.blogspot.com/feeds/posts/default/-/TCS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://teachingintrotocs.blogspot.com/search/label/TCS" title="A CS Professor's blog">Claire Mathieu</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default?alt=rss" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" class="message" title="internal server error">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="http://blog.oddhead.com/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.oddhead.com" title="Oddhead Blog">David Pennock</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://example.com/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://jsaia.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://jsaia.wordpress.com" title="Machinations">Jared Saia</a>
<br>
<a class="feedlink" href="https://ontopo.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ontopo.wordpress.com" title="on Topology">John Moeller</a>
<br>
<a class="feedlink" href="https://jonkatz.wordpress.com/category/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://jonkatz.wordpress.com" title="TCS – Random bits">Jonathan Katz</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="internal server error">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Manoj Prabhakaran</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastian Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons institute blog</a>
<br>
<a class="feedlink" href="https://speedupblogger.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://speedupblogger.wordpress.com" title="Speedup in Computational Complexity">Speedup in Computational Complexity</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/WebdiariosDeMotocicleta" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://infoweekly.blogspot.com/" title="WebDiarios de Motocicleta">WebDiarios de Motocicleta</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CG">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://kdphd.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://kdphd.blogspot.com/" title="kd-PhD">kd-phd</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">my slice of pizza</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">tcs math</a>
<br>
</div>

<p>
Maintained by <A href="http://randomwalker.info/">Arnab Bhattacharyya &amp; Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at December 17, 2018 06:12 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=941">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2013/12/theory-behind-big-data/">Theory Behind Big Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p style="text-align: justify;">As a PhD student who does research on theory and algorithms for massive data analysis, I am interested in exploring current and future challenges in this area, which I’d like to share it here. There are two major points of view when we talk about big data problems:</p>
<p style="text-align: justify;">One is more focused on industry and business aspects of big data, and includes many IT companies who work on analytics. These companies believe that the potential of big data lies in its ability to solve business problems and provide new business opportunities. To get the most from big data investments, they focus on questions which companies would like to answer. They view big data not as a technological problem but as a business solution, and their main goals are to visualize, explore, discover and predict.</p>
<p style="text-align: justify;"><span id="more-941"></span>On the more theoretic side, researchers are interested in the theory behind big data, and its use in designing efficient algorithms. As a personal experience, I have been to different career fairs with companies that work in the area of big data. I expected we will have many common things to talk about, but when I described my work and the problems which are interesting to us, I realized that the way we are looking at this problem in academia is different from what the industry is looking for.</p>
<p style="text-align: justify;">While there will always be a gap between theory in academia and applications in industry, I feel that since the origin of the “big data” problem is real world applications and challenges this gap should be less pronounces than in other theory fields in Computer Science. The main question that arises is that what it means when we say “theory for big data”? How it is different from “classic” theoretical computer science?</p>
<p style="text-align: justify;">There seem to be different perspectives among theoreticians regarding this question. Some researchers consider big data as “bad news” for algorithm design, since this leads to intelligent and sophisticated algorithms being replaced by less clever algorithms that can be applied efficiently to massive data sets (as pointed out by Prabhakar Raghavan in <a href="http://theory.stanford.edu/stoc2013/">STOC13</a>).</p>
<p style="text-align: justify;">On the other side, many researchers have a more positive view, and think of big data as a great opportunity to rethink classic techniques for algorithm design and underlying theoretical foundations.</p>
<p style="text-align: justify;"><a href="http://mrtz.org/">Moritz Hardt</a> has an interesting <a href="http://mrtz.org/blog/what-should-a-theory-of-big-data-do/">blog</a> post discussing this point. He argues that the starting point is to explore the properties that large data sets exhibit and how they might affect algorithm design.</p>
<p style="text-align: justify;">There is an ongoing effort in the community to make the most of the big data opportunity. As part of the program called <a href="http://simons.berkeley.edu/programs/bigdata2013">“The Theoretical Foundation of Big Data”</a>, which is held at Simons Institute for Theoretical Computer Science this year with many visiting scientists working in the area of massive data, there are several workshops and lots of interesting talks on different related topics. Olivia has covered briefly one of the recent ones, titled, <a href="http://simons.berkeley.edu/workshops/bigdata2013-3">“Unifying Theory and Experiment for Large-Scale Networks”</a>, <a href="http://xrds.acm.org/blog/2013/11/laying-the-foundation-for-a-common-ground/">here</a>.</p>
<p style="text-align: justify;">In my future posts, I will try to discuss some interesting problems which may be considered as the core of theoretic research on big data. I will try to show why studying the theory behind big data is important, and assess how much of this study has been effective and helpful to the main goal – making data processing faster.</p>
<p>The post <a href="https://blog.xrds.acm.org/2013/12/theory-behind-big-data/" rel="nofollow">Theory Behind Big Data</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Samira Daruki <a href="https://blog.xrds.acm.org/2013/12/theory-behind-big-data/"><span class="datestr">at December 19, 2013 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=898">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2013/11/ultra-efficient-via-sublinearity-2/">Ultra-Efficient via Sublinearity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For a long time in the area of design and analysis of algorithms, when we have said that an algorithm is efficient we meant that it runs in time polynomial in the input size n and finding a linear time algorithm have been considered as the most efficient way to solve a problem. It’s been because of this assumption that we need at least to consider and read all the input to solve the problem. This way it seems that we cannot do much better! But nowadays the data sets are growing fast in various areas and applications in a way that it hardly fits in storage and in this case even linear time is prohibitive. To work with this massive amount of data, the traditional notions of an efficient algorithm is not sufficient anymore and we need to design more efficient algorithms and data structures. This encourages researchers to ask whether it is possible to solve the problems using just sublinear amount of resources? what does that mean exactly when we say ‘sublinear resources’?<br />
We can think of sublinear algorithms in the area of big data in three different categories:</p>
<p><span id="more-898"></span>Sublinear space algorithms: Here we are more focused on algorithms for processing data streams which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). These algorithms have limited memory available to them (much less than the input size which is assumed to be sublinear and is typically polylogarithmic) and also limited processing time per item. Based on this settings, the algorithm produces an approximate answer using a summary of the data stream in memory. [<a href="http://en.wikipedia.org/wiki/Streaming_algorithm" target="_blank" title="1">1</a>]</p>
<p>Sublinear communication algorithms: The scenario in this category is a bit different: data is distributed among multiple machines and the goal is to compute some function on the union of data sets. Apparently to do these distributed computations, we let the machines to have communications among each other and of course, the goal is to do this with the least amount of communications.</p>
<p>Sublinear time algorithms: Here we are more looking for algorithms that do not even need to read whole input to answer the query on that. Since these algorithms must provide an answer without reading the entire input, they are typically heavily depend on randomization and provide approximate answer. In other words, we can look at the sublinear time algorithms as sort of randomized approximation algorithms. Still there are problems for which deterministic exact sublinear time algorithms are known. But typical algorithms that are exact and yet run in sub-linear time use parallel processing or alternatively have guaranteed assumptions on the input structure (as the logarithmic time binary search and many tree maintenance algorithms do)- However, the specific term sublinear time algorithm is usually reserved to algorithms that run over classical serial machine models and are not allowed prior assumptions on the input.</p>
<p>In the scope of sublinear time algorithms, there are two main categories of interest: The algorithms which need to compute an approximate value and the ones which require to make an approximation decision and are called as <a href="http://en.wikipedia.org/wiki/Property_testing" target="_blank" title="">“property testing”</a>. Informally speaking, in property testing the goal is to design efficient algorithms to decide whether a given mathematical object has a certain property or is `far’ from having this property, i.e. is significantly different from any object that has the property. To make this decision, algorithm can perform local queries on the object, but the final decision consider a global view of the object and decision task should be performed by querying the object in as few places as possible.<br />
More precisely, for a fixed property P and any object O, if object O has property P the algorithm should accept with probability at least 2/3, otherwise if the object O is ε-far from having property P, then the algorithm should reject with probability at least 2/3. Here one-sided error is much more desired, means the accepting probability be 1 instead of 2/3. In such cases, the algorithm has a non-zero error probability only on inputs that are far from having the property and never reject inputs that have the property. This necessitate the algorithm in the case of rejecting some input to provide (small) evidence to show that the input does not have the property.<br />
To determine what exactly means to be ε-far from having property P, we need to define the distance measure based on the problem. Then we can interpret it as the Hamming distance between object O and any other object O’ having the property P is at least ε|O|. For example, if the property is in graphs to test whether they have k-clique(a clique of size k), then being ε-far from this property means that more than ε -fraction of edges should be added to the graph so that it have a clique of size k.</p>
<p>While each algorithm has features that are speci�fic to the property it tests, there are several common algorithmic and analysis techniques for property testing[<a href="http://www.wisdom.weizmann.ac.il/~oded/PDF/dana-tech.pdf" target="_blank" title="2">2</a>]. Probably the most popular one is applying the idea of <a href="http://en.wikipedia.org/wiki/Szemer%C3%A9di_regularity_lemma" target="_blank" title="Szemer�edi's Regularity Lemma">Szemer�edi’s Regularity Lemma</a>, which is very important tool and central key to the analysis of testing graph properties in the dense-graphs model.</p>
<p>Property testing initially was defined by Rubinfeld and Sudan[<a href="http://dl.acm.org/citation.cfm?id=586678" target="_blank" title="3">3</a>] for testing algebraic properties of functions and was discussed in the context of Program Testing and Probabilistically Checkable Proofs(<a href="http://en.wikipedia.org/wiki/Probabilistically_checkable_proof" target="_blank" title="PCP">PCP</a>). In program checking, the idea is to test that the program satisfies certain properties before checking whether it computes a specified function.<br />
Later, Goldreich, Goldwasser and Ron [<a href="http://groups.csail.mit.edu/cis/pubs/shafi/1998-jacm.pdf" target="_blank" title="4">4</a>] initiated the study of testing properties of graphs and presented some general results on the relations between testing and learning. In recent years there has been a growing body of work dealing with properties of functions, distributions and combinatorial objects such as graphs, strings, sets of points and many algorithms have been developed with complexity that is sub-linear or even independent of size of the object. But still the research in this area is new and there are much left to understand and explore.<br />
If you are interested to follow the research trends in the area of Sublinear time algorithms and property testing, there is a great blog- [<a href="http://ptreview.sublinear.info/" target="_blank" title="PTReview">PTReview</a>]- which discusses and report about the latest news, research develops and papers on the property testing and sublinear time algorithms. There are also a bunch of available surveys by researchers working in the area of sublinear time algorithms.</p>
<p>Maybe the interesting point about property testing algorithms is that while they are decision algorithms, in several cases they can be transformed to optimization problems which actually constructs the approximation solutions and this is the key link between property testing and “classical” approximation. Now maybe the main question to ask is that when is it valuable to think about property testers? Is it just restricted to certain problems and just the cases which we are dealing with huge amount of data?</p>
<p>To wrap up, we can summarize the setting of interests for applying property testing algorithms as follows:[<a href="http://www.wisdom.weizmann.ac.il/~oded/PDF/dana-tech.pdf" target="_blank" title="2">2</a>]<br />
– The object is huge and expensive to be fully scanned. We need to make just a approximate decision.<br />
– The object is not very large, but the property we are looking at is NP-hard. This includes many problems in Graph theory, for example coloring.<br />
– The Object is not large and the decision problem has a polynomial-time algorithm. But still we desire to have a more efficient algorithm even by sacrificing some part of accuracy.<br />
– Similar to last case, object is not large and the decision problem has a polynomial-time algorithm, but the final decision must be exact. In this case, the property testing is useful since we can first run it on the data and if it passes the test as accepted, then we run the exact algorithm. This will help us to save time when the input is far from having the property.</p>
<p>As a take-home message, It seems that for every researcher who wants to start working on the area of algorithm design and theoretical foundations of large data analysis, it’s a must to have a good flavor of algorithmic and analysis techniques used for sublinear time algorithms.</p>
<p>The post <a href="https://blog.xrds.acm.org/2013/11/ultra-efficient-via-sublinearity-2/" rel="nofollow">Ultra-Efficient via Sublinearity</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Samira Daruki <a href="https://blog.xrds.acm.org/2013/11/ultra-efficient-via-sublinearity-2/"><span class="datestr">at November 26, 2013 12:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=888">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2013/11/laying-the-foundation-for-a-common-ground/">Laying the Foundation for a Common Ground</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This week, the <a href="http://simons.berkeley.edu/" target="_blank" title="Simons Institute">Simons Institute</a> hosted a workshop entitled <em><a href="http://simons.berkeley.edu/workshops/bigdata2013-3" target="_blank" title="Unifying Theory and Experiment for Large-Scale Networks">Unifying Theory and Experiment for Large-Scale Networks</a></em>. The goal of the workshop is to bring together researchers involved in various large networks problems to discuss both the theoretical models and the empirical process for testing and validating them. Even further, the “unifying” in the title suggests a forum on where the ends of the spectrum may meet.</p>
<p><span id="more-888"></span>Many questions came up around the very issue of finding unifying ground. How much should we invest in the theory if we have the empirical results? To what degree do we tune our models to mirror real-world data?</p>
<p>The structure of the workshop was well-suited to such a discussion. The four-day series was divided into sessions, each of which consisted of four 30-minute talks and then an open, unrecorded panel discussion. The full schedule, with abstracts and video, can be found <a href="http://simons.berkeley.edu/workshops/schedule/77" target="_blank" title="here">here</a>. There was a good representation of theory/experiment across and within the sessions. While there was some clear polarity, there were at least some important insights as to the reasons for the distance between theory and practice in large-scale network problems. Below are some of the major themes that came up.</p>
<ul>
<ul>
<li><em>Do we have the model right?<br />
</em>Theory that can be used in practice depends on the theory assuming the right model. But what is the “right” model? In general, there seems to be little agreement on this. Firstly, models that can be nicely abstracted may not accurately model network data that occurs in nature. How relevant is a stochastic assumption, or i.i.d. sampling in practice? A poignant example was given by <a href="http://www.cs.princeton.edu/~csesha/" target="_blank" title="C. Seshadhri">C. Seshedhri</a> in his talk on finding triangles in graphs in an online streaming model <a href="http://arxiv.org/abs/1302.6220" target="_blank">[1]</a>. After his conclusions, he mentioned a question posed by the experimental community at his lab: edges may occur more than once in real online streaming networks, so how can these results be applied to multigraphs? No matter how promising the results, if they do not fit a scenario, they cannot be implemented in practice. And, for clarification, the results have been extended to multigraphs by considering the induced simple graph <a href="http://arxiv.org/abs/1310.7665" target="_blank">[2]</a> (but these are still not used in production).Second, many problems seem to be lacking the theory because statistical models change very quickly. The variety of models, metrics, and parameters may even distract from the development of good practical algorithms with theoretical backing. As a result, many theoretical researchers will essentially pick the model most attractive to them and develop the theory there, whether or not there is a practical need.</li>
<li><em>Empirical results beat theoretical bounds.<br />
</em>Even problems <em>with </em>backing theory are generally not applied. Most algorithms presented at conferences are not available off-the-shelf. Getting these algorithms ready for prime time will almost certainly require many iterations of improvement and calibrating, and a team with a very good understanding of both the theory and practice.Why isn’t the theory enough? One reason may be the focus on scalability among the theory community. It can be generally observed that memory capacity is growing more quickly than problem sizes, so it is not necessarily useful in practice for algorithms to be highly scalable. Somewhat along these lines also is the question of what needs to be achieved. Complexity and accuracy analyses presented in algorithms papers are typically worst-case bounds. However, it can often be the case that data we see in real-world scenarios behave better, even much better, than the worst case. It may be unnecessary to implement robust algorithms when simpler heuristics perform better in practice. <a href="http://vigna.dsi.unimi.it/" target="_blank" title="Vigna">Vigna</a> delivered a provocative view early in the workshop on why approximate set representations used to analyze large networks perform better than their theoretical guarantees.</li>
<li><em>How can we improve if we don’t know how we’re doing?<br />
</em>This was the tagline of <a href="http://www.princeton.edu/~mjs3/" target="_blank" title="Salganik's">Salganik’s</a> talk on modeling epidemic networks in Rwanda. Without methods for validation, it is impossible to bridge the gap between theory and practice. Surprisingly, though, some classes of algorithms have achieved success in spite of this. In particular, local partitioning algorithms have demonstrated both theoretical and practical success. I’ll discuss one example next.</li>
</ul>
</ul>
<h3>Computing overlapping clusters with local computations</h3>
<p><a href="http://research.google.com/pubs/mirrokni.html" target="_blank" title="Vahab Mirrokni">Vahab Mirrokni</a> closed the session on clustering with a presentation on his <a href="https://www.cs.purdue.edu/homes/dgleich/publications/Andersen%202012%20-%20overlapping.pdf" target="_blank" title="joint work">joint work</a> with Reid Andersen and David Gleich on clustering for distributed computing.</p>
<p>The goal of a global clustering is to partition the nodes of a graph into distinct subsets such that there is little communication between the clusters (few crossing edges) and no single cluster is too large. This has clear applications to distributed computing, where large datasets are relatively equally partitioned onto a group of processors, with minimal communication required between processors. In an overlapping clustering, partitions are not required to be disjoint, and more of the graph is stored than is required.</p>
<p>The goal of an overlapping clustering can be formulated in terms of random walks. Consider the walk on the graph v1, v2, …, vt. Let T be a mapping from the vertex set to a set of clusters, and say and the corresponding sequence of active clusters containing the vertex at each step is C1 = T(v1), C2 = T(v2), …, Ct = T(vt). Then the goal of an overlapping clustering is to divide the graph into clusters which minimizes the number of times the active cluster must be changed during a random walk.</p>
<p>In this work, overlapping clusters are found first through <em>local</em> partitions. Candidate clusters are generated from local clusters computed from each vertex, using the PageRank procedure of <a href="http://www.cs.cmu.edu/afs/cs/user/glmiller/public/Scientific-Computing/F-11/RelatedWork/local_partitioning_full.pdf" target="_blank" title="[3]">[3]</a>, for example. Overlapping clusters with upper bounded volume are then computed by combining the local clusters in a certain way which minimizes random walk cluster crossing. The result is a set of clusters each with a bounded volume and for which communication, modeled through the random walk probability diffusion, is kept minimal.</p>
<h3>Local algorithms in practice</h3>
<p>Local clustering is an example where the best performing algorithms have the backing of theoretical guarantees. For finding small clusters, for example, local algorithms are the best option in terms of performance and quality. Global clustering, however, is a different story, and is generally too hard to move beyond empirical guarantees. The overlapping clusters algorithm is a nice example of applying a theoretically sound procedure to a problem which generally does not have the theoretical backing.</p>
<p>However, even still, the best global clustering algorithms are those based on good empirical performance. So do we need the theory? Will there always be an impenetrable gap between theory and practice? The workshop set the stage for some necessary discussion, but the general consensus seemed to be that the goals of these two ends are irreconcilably different. Of course, theorists need the motivation of practice, and practitioners need the inspiration of theory, but we may be some time away from theoretical results being applied to large-scale problems.</p>
<p>As a footnote, <a href="http://simons.berkeley.edu/people/ravi-kannan" target="_blank" title="Ravi Kanaan">Ravi Kanaan</a> is giving an upcoming talk at the Simons Institute on whether theory is necessary in clustering, which might offer some insight.</p>
<p>The post <a href="https://blog.xrds.acm.org/2013/11/laying-the-foundation-for-a-common-ground/" rel="nofollow">Laying the Foundation for a Common Ground</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Olivia <a href="https://blog.xrds.acm.org/2013/11/laying-the-foundation-for-a-common-ground/"><span class="datestr">at November 23, 2013 05:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=3323">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2017/12/flex-lexical-analysis/">Flex, Regular Expressions, and Lexical Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<h2> What Makes a Programming Language?</h2>
<p>There is an alphabet, words, grammar, statements, semantics, and various ways to organize the previous in order to create a computer program in a programming language. Flex helps developers create a tool called a lexical analyzer which identifies the words of a program during the compilation/interpretation process.</p>
<p><span id="more-3323"></span></p>
<p>A compiler takes a text file and parses it by character trying to match patterns at each of the aforementioned levels. The initial parse is the lexical analysis, this pass ensures that there are no lexical errors, which are characters that don’t contribute to meaningful words.</p>
<p>Meaningful words for a programming language are described by a regular language. An implementation for describing a regular language is regular expressions. An implementation for parsing text while looking for matches to regular expressions is a flex lexical analyzer.</p>
<p>Essentially, programming a flex lexer means defining various regular expressions which detail all of the possible words and characters that are meaningful in a correct program for your language.</p>
<h2>BooleanLogicLanguage Example Language</h2>
<p>To illustrate flex, BooleanLogicLanguage is an example language which a flex-based lexer will lexically analyze. For no reason in particular, the purpose of this language is to evaluate Boolean logic expressions.</p>
<div style="width: 310px;" id="attachment_3325" class="wp-caption alignnone"><a href="http://xrds.acm.org/blog/wp-content/uploads/2017/11/scan0010.jpg" rel="attachment wp-att-3325"><img src="http://xrds.acm.org/blog/wp-content/uploads/2017/11/scan0010-300x142.jpg" alt="An example of a program made in BooleanLogicLanguage" height="142" class="size-medium wp-image-3325" width="300" /></a><p class="wp-caption-text">An example of a program made in BooleanLogicLanguage</p></div>
<p>This diagram is an example of what a correct program would look like in this BooleanLogicLanguage. The lexical analyzer should be able to parse this without errors. Note that in this language, ‘INTEGER(‘ and ‘INTEGER)’ are the ‘words’ used to separate pieces of code — as opposed to ‘(‘, ‘)’, ‘{‘, or ‘}’. When creating your own language, you are free to do whatever you want, but following convention, to some degree, just ensures that the tool you are creating is easy to use.</p>
<p>NOTE: A programming language is a tool for using a computer. A GUI is a tool for using a computer. Siri or Cortana are tools for using a computer, etc.</p>
<div style="width: 220px;" id="attachment_3324" class="wp-caption alignnone"><a href="http://xrds.acm.org/blog/wp-content/uploads/2017/11/scan0008.jpg" rel="attachment wp-att-3324"><img src="http://xrds.acm.org/blog/wp-content/uploads/2017/11/scan0008-210x300.jpg" alt="Regular expressions describing the lexical structure of BooleanLogicLanguage" height="300" class="size-medium wp-image-3324" width="210" /></a><p class="wp-caption-text">Regular expressions describing the lexical structure of BooleanLogicLanguage</p></div>
<p>This diagram is a sketch of the regular expressions which will be used in the flex program in order to describe meaningful words. The phrases on the left hand side are token names. Token names are not necessarily important during lexical analysis, but they are of the utmost importance when performing the syntactic analysis (which comes after a lexical analysis during compilation/interpretation — not covered in this post).</p>
<p>The most difficult part of this process is defining tokens and figuring out what sort of regular expression should describe them. For this example, I decided to make a language that would evaluate Boolean logic. Then I started writing potential programs in this language, and once I wrote enough that looked interesting and useful, I defined tokens and regular expressions to ensure those particular programs would be correct. I made code up that I liked and then fit tokens and regex to make them work.</p>
<p><strong>A Quick Note on Regular Expressions (Regex)</strong></p>
<p>[…] denotes a single thing, whose identity is dictated by what’s inside the brackets.</p>
<p>A single character is a single thing.</p>
<p>* denotes zero or more of the single thing before it.</p>
<p>? denotes one or zero of the single thing before it.</p>
<p>(…) is just a grouping, usually used with * or ?.</p>
<p>The difference between […] and (…) is that the square brackets represents one of what is inside of it and the parentheses are a grouping. For example [abc] and (abc): ‘a’, ‘b’, ‘c’ all match the former, and ‘abc’ matches the latter.</p>
<p>– denotes a range and has specific applications that are very useful: A-Z, a-z, 0-9.</p>
<h2>Flex</h2>
<p>Flex is like a C program, except it has a  further defined layout.</p>
<p>%{</p>
<p>C code declarations</p>
<p>%}</p>
<p>definitions of regular expressions</p>
<p>%%</p>
<p>regular expression1   code to execute on match (such as a macro)</p>
<p>regular expression2  other code to execute on match (such as a function)</p>
<p>%%</p>
<p>C code functions</p>
<p>main function</p>
<p>The power of Flex comes from being able to define regular expressions (between ‘%}’ and ‘%%’) and then attach them to code (between ‘%%’ and ‘%%’). The additional areas for C code are both handy and what gives the lexer its true functionality (doing something when a regex is matched).</p>
<p>NOTE: flex essentially turns this flex file (extension ‘.l’) into a C program which is then compiled like you would compile any C program. The result is an object-file/program which you execute on/with a text file containing programming code. And in this case, the output of the program is to a text file (Tokens.txt) and also to stdout (terminal).</p>
<div style="background: #ffffff; overflow: auto; width: auto; border: solid gray; border-width: .1em .1em .1em .8em; padding: .2em .6em;">
<table width="578" style="height: 1437px;">
<tbody>
<tr>
<td>
<pre style="margin: 0; line-height: 125%;"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88</pre>
</td>
<td>
<pre style="margin: 0; line-height: 125%;">%{
<span style="font-weight: bold;">int</span> numErrors = 0;
<span style="font-weight: bold;">char</span>* arg;
<span style="font-weight: bold;">typedef</span> <span style="font-weight: bold;">struct</span> node {
	<span style="font-weight: bold;">char</span>* lexeme;
	<span style="font-weight: bold;">char</span>* token;
	<span style="font-weight: bold;">struct</span> node* next;
} <span style="font-weight: bold;">node_t</span>;
<span style="font-weight: bold;">node_t</span> head;
<span style="font-weight: bold;">node_t</span>* current = &amp;head;
<span style="font-weight: bold;">int</span> yywrap(<span style="font-weight: bold;">void</span>);
<span style="font-weight: bold;">void</span> store(<span style="font-weight: bold;">char</span>* lexeme);
<span style="font-weight: bold;">void</span> error(<span style="font-weight: bold;">void</span>);
<span style="font-weight: bold;">void</span> printStored(<span style="font-weight: bold;">void</span>);
%}
whitespace		(<span>\</span>t|<span style="font-style: italic;">" "</span>|<span>\</span>r)
newline			<span>\</span>n
real 			-?[0-9]+<span>\</span>.[0-9]+
integer			-?[0-9]+
string			<span>\</span><span style="font-style: italic;">".*</span><span style="font-weight: bold; font-style: italic;">\"</span>
boolean_op_binary	(<span style="font-style: italic;">" AND "</span>|<span style="font-style: italic;">" OR "</span>|<span style="font-style: italic;">" XOR "</span>)
boolean_op_unary	<span style="font-style: italic;">"NOT "</span>
boolean_literal		(<span style="font-style: italic;">"True"</span>|<span style="font-style: italic;">"False"</span>)
identifier		[A-Z]+
separator_open		({real}|{integer})<span>\</span>(
separator_close		<span>\</span>)({real}|{integer})
assignment		=
IO			<span style="font-style: italic;">"print "</span>
terminal		;
%%
{whitespace}				{ECHO;}
{newline}				{ECHO;}
{real}					{ECHO;}
{integer}				{ECHO;}
{string}				{ECHO;}
{boolean_op_binary}			{ECHO; arg = <span style="font-style: italic;">"BOOL_OP_BINARY"</span>; store(yytext);}
{boolean_op_unary}			{ECHO; arg = <span style="font-style: italic;">"BOOL_OP_UNARY"</span>; store(yytext);}
{boolean_literal}			{ECHO; arg = <span style="font-style: italic;">"BOOL_LITERAL"</span>; store(yytext);}
{identifier}				{ECHO; arg = <span style="font-style: italic;">"IDENTIFIER"</span>; store(yytext);}
{separator_open}			{ECHO; arg = <span style="font-style: italic;">"OPEN"</span>; store(yytext);}
{separator_close}			{ECHO; arg = <span style="font-style: italic;">"CLOSE"</span>; store(yytext);}
{assignment}				{ECHO; arg = <span style="font-style: italic;">"ASSIGN"</span>; store(yytext);}
{IO}					{ECHO; arg = <span style="font-style: italic;">"IO"</span>; store(yytext);}
{terminal}				{ECHO; arg = <span style="font-style: italic;">"TERMINAL"</span>; store(yytext);}
.					{ECHO; numErrors++; error();}
%%
<span style="font-weight: bold;">int</span> yywrap(<span style="font-weight: bold;">void</span>) {
	<span style="font-weight: bold;">return</span> 1;
}
<span style="font-weight: bold;">void</span> store(<span style="font-weight: bold;">char</span>* lexeme) {
	current-&gt;lexeme = malloc(<span style="font-weight: bold;">sizeof</span>(strlen(lexeme)+1));
	strcpy(current-&gt;lexeme,lexeme);
	current-&gt;token = malloc(<span style="font-weight: bold;">sizeof</span>(strlen(arg)+1));
	strcpy(current-&gt;token,arg);
	<span style="font-weight: bold;">node_t</span>* temp;
	temp = malloc(<span style="font-weight: bold;">sizeof</span>(<span style="font-weight: bold;">node_t</span>));
	current-&gt;next = temp;
	current = current-&gt;next;
}
<span style="font-weight: bold;">void</span> error(<span style="font-weight: bold;">void</span>) {
	printf(<span style="font-style: italic;">"[e]"</span>);
}
<span style="font-weight: bold;">void</span> printStored(<span style="font-weight: bold;">void</span>) {
	<span style="font-weight: bold;">node_t</span>* c = &amp;head;
	<span style="font-weight: bold;">FILE</span>* f = fopen(<span style="font-style: italic;">"Tokens.txt"</span>,<span style="font-style: italic;">"w"</span>);

	<span style="font-weight: bold;">while</span> (c-&gt;next) {
		fprintf(f,<span style="font-style: italic;">"%s</span><span style="font-weight: bold; font-style: italic;">\t</span><span style="font-style: italic;">%s</span><span style="font-weight: bold; font-style: italic;">\n</span><span style="font-style: italic;">"</span>,c-&gt;lexeme,c-&gt;token);
		c = c-&gt;next;
	}
	fclose(f);
	printf(<span style="font-style: italic;">"Tokens.txt written.</span><span style="font-weight: bold; font-style: italic;">\n</span><span style="font-style: italic;">"</span>);
}
<span style="font-weight: bold;">int</span> main(<span style="font-weight: bold;">int</span> argc, <span style="font-weight: bold;">char</span> *argv[]) {
	<span style="font-style: italic;">// ensures number of command line arguments</span>
	<span style="font-weight: bold;">if</span> (argc != 2) {
		printf(<span style="font-style: italic;">"Please enter one filename as an argument.</span><span style="font-weight: bold; font-style: italic;">\n</span><span style="font-style: italic;">"</span>);
		<span style="font-weight: bold;">return</span> -1;
	}
	<span style="font-style: italic;">// opens the file with name of second argument</span>
	yyin = fopen(argv[1],<span style="font-style: italic;">"r"</span>);
	yylex();
	<span style="font-style: italic;">// close file</span>
	fclose(yyin);
	printf(<span style="font-style: italic;">"</span><span style="font-weight: bold; font-style: italic;">\n</span><span style="font-style: italic;">LexicalErrors %d</span><span style="font-weight: bold; font-style: italic;">\n</span><span style="font-style: italic;">"</span>,numErrors);
	printStored();
	<span style="font-weight: bold;">return</span> 0;
}</pre>
</td>
</tr>
</tbody>
</table>
</div>
<p>The above code is a flex file which parses the BooleanLogicLanguage.</p>
<div style="background: #ffffff; overflow: auto; width: auto; border: solid gray; border-width: .1em .1em .1em .8em; padding: .2em .6em;">
<table>
<tbody>
<tr>
<td>
<pre style="margin: 0; line-height: 125%;">1
2
3
4
5
6
7
8
9</pre>
</td>
<td>
<pre style="margin: 0; line-height: 125%;">CC=gcc
CFLAGS=
LexerFile=lexer

lexer: lex.yy.c
	<span style="font-weight: bold;">$(</span>CC<span style="font-weight: bold;">)</span> <span style="font-weight: bold;">$(</span>CCFLAGS<span style="font-weight: bold;">)</span> -o lexer lex.yy.c

lex.yy.c: <span style="font-weight: bold;">$(</span>LexerFile<span style="font-weight: bold;">)</span>.l
	flex <span style="font-weight: bold;">$(</span>LexerFile<span style="font-weight: bold;">)</span>.l</pre>
</td>
</tr>
</tbody>
</table>
</div>
<p>The above code is a makefile, which when run in the same directory as the flex file, will create the ‘lexer’ program. This was tested on an Ubuntu 16.04 operating system. The GNU C Compiler (gcc) is required in addition to flex.</p>
<div style="background: #ffffff; overflow: auto; width: auto; border: solid gray; border-width: .1em .1em .1em .8em; padding: .2em .6em;">
<table>
<tbody>
<tr>
<td>
<pre style="margin: 0; line-height: 125%;">1
2
3
4
5</pre>
</td>
<td>
<pre style="margin: 0; line-height: 125%;">P = True;
R = False;

Q = 1(NOT P)1 XOR 2(P AND 3(NOT R)3)2;
print Q;
</pre>
</td>
</tr>
</tbody>
</table>
</div>
<p>The above text should be parsed without errors by our lexer. And the lexer should output a Tokens.txt matching each lexeme to the token describing its regex.</p>
<div style="background: #ffffff; overflow: auto; width: auto; border: solid gray; border-width: .1em .1em .1em .8em; padding: .2em .6em;">
<table>
<tbody>
<tr>
<td>
<pre style="margin: 0; line-height: 125%;"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre>
</td>
<td>
<pre style="margin: 0; line-height: 125%;">P	IDENTIFIER
=	ASSIGN
True	BOOL_LITERAL
;	TERMINAL
R	IDENTIFIER
=	ASSIGN
False	BOOL_LITERAL
;	TERMINAL
Q	IDENTIFIER
=	ASSIGN
1(	OPEN
NOT 	BOOL_OP_UNARY
P	IDENTIFIER
)1	CLOSE
 XOR 	BOOL_OP_BINARY
2(	OPEN
P	IDENTIFIER
 AND 	BOOL_OP_BINARY
3(	OPEN
NOT 	BOOL_OP_UNARY
R	IDENTIFIER
)3	CLOSE
)2	CLOSE
;	TERMINAL
print 	IO
Q	IDENTIFIER
;	TERMINAL
</pre>
</td>
</tr>
</tbody>
</table>
</div>
<p>Here is the Token.txt for that initial errorless program.</p>
<p></p><div style="width: 310px;" id="attachment_3356" class="wp-caption alignnone"><a href="http://xrds.acm.org/blog/wp-content/uploads/2017/12/Screen-Shot-2017-12-10-at-5.19.44-PM.png" rel="attachment wp-att-3356"><img src="http://xrds.acm.org/blog/wp-content/uploads/2017/12/Screen-Shot-2017-12-10-at-5.19.44-PM-300x97.png" alt="Lexical errors are designated with a '[e]' for this lexer." height="97" class="size-medium wp-image-3356" width="300" /></a><p class="wp-caption-text">Lexical errors are designated with a ‘[e]’ for this lexer, and it comes jsut after the erroneous character.</p></div>The above is a screenshot of an example of parsing a program with lexical errors.<p></p>
<p><strong>What is Next?</strong></p>
<p>Classically, once you can identify words, including what type of word (token), you can then create a grammar. You could think of a token as ‘noun’ or ‘verb’ or ‘adjective’, if comparing a programming language to a natural language. The step after lexical analysis (checking for correctness of words) is syntactic analysis (checking for correctness of grammar). Making a comparison to natural languages again, an English grammar could be PHRASE: article noun verb (The dog ran, A bird flies, etc). In BooleanLogicLanguage there could be STATEMENT: IDENTIFIER ASSIGN BOOL_LITERAL (Q = True, A = False, etc). But each of those are an example of just one type of phrase or statement, you can have multiple definitions for a production in the grammar for your language.</p>
<p>Non-Classically? I don’t know. Natural Language Processing (NLP) is a very active area, but I’m not aware of anyone using those techniques for parsing textual programming languages.</p>
<p>The post <a href="https://blog.xrds.acm.org/2017/12/flex-lexical-analysis/" rel="nofollow">Flex, Regular Expressions, and Lexical Analysis</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Alexander DeForge <a href="https://blog.xrds.acm.org/2017/12/flex-lexical-analysis/"><span class="datestr">at December 10, 2017 11:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2014/07/how-connected-are-quantum-graphs/">How Connected are Quantum Graphs?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Some students in my department this quarter hosted a reading group on quantum computing. Quantum computing is becoming more and more relevant and the topic attracted the participation of a diverse group of researchers. The best way to handle the scope of the topic and diversity of the participants was to invite volunteer speakers to give talks on the quantum analog of their own research area — “Quantum Circuit Lower Bounds,” “Quantum Game Theory,” and “QPSPACE” were among some of the topics. Naturally, I saw this as a great opportunity to understand more about quantum spectral graph theory. In this post I will outline some basic definitions and properties of quantum graphs, and as a follow up to my <a href="http://xrds.acm.org/blog/2014/05/the-geometric-origins-of-spectral-graph-theory/">previous post</a> on the connections between spectral geometry and graph theory, discuss isospectral properties of discrete and quantum graphs.<span id="more-1347"></span></p>
<p>Let’s begin by setting the scene. Let <em>X</em> be a vector space equipped with an inner product (also known as a <em>Hilbert space</em>) and let <em>T: X → X</em> be a self-adjoint bounded linear operator on the vector space with domain <em>D(T)</em>. (If these terms are foreign to you, don’t worry too much. It is enough to accept that the Laplacians we’ll be talking about come with things called eigenvalues — keep reading!) The <em>point spectrum</em> of <em>T</em> is</p>
<p style="text-align: center;"><em>σ<sub>p</sub>(T) = { λ | ∃ some u in D(T) with Tu = λu }</em></p>
<p>We call these <em>λ</em> the <em>eigenvalues</em> of <em>T</em>, and they will be our main players.</p>
<h1><b> Graphs and Laplace operators </b></h1>
<p>Let’s begin with something comfy — discrete graphs. That is, a collection of vertices, <em>V</em>, and edges, <em>E</em>, connecting the vertices. In the discrete graph world, when we talk about functions on the graph we mean a function over the vertices. Most often, this function will assign a real number to each vertex in the graph, something like this: <em>φ: V → <strong>R.  </strong></em>The <em>discrete Laplace operator</em> (or simply the Laplacian when talking about graphs specifically) can be described by its action on an arbitrary such φ:</p>
<p style="text-align: center;"><em>(Lφ)(v) = ∑<sub>u~v</sub> (φ(u) – φ(v)) = (D – A)φ(v)</em></p>
<p>(Take a look at my <a href="http://xrds.acm.org/blog/2014/05/the-geometric-origins-of-spectral-graph-theory/">previous post</a> if this looks like nonsense.)</p>
<p>An important question in <a href="http://en.wikipedia.org/wiki/Spectral_geometry">spectral geometry</a> is</p>
<blockquote><p>To what degree does the spectrum of the Laplace operator determine the underlying space?</p></blockquote>
<p>That is, what can the eigenvalues of the graph Laplacian tell us about the graph itself? In this post, I’ll explore this question and specifically how quantum graph spectra behave in somewhat unexpected ways.</p>
<h1><b> Graph connectivity </b></h1>
<p>Again, before tackling quantum graphs, lets become a little more familiar with the discrete setting. If the graph has <em>n</em> vertices, the size of the discrete Laplacian will be <em>n x n</em>. The discrete Laplacian is positive semidefinite, and so is has exactly <em>n</em> non-negative real eigenvalues <em>0 = λ<sub>0</sub> ≤ λ<sub>1</sub> ≤ … ≤  λ<sub>n-1</sub></em>. The second smallest eigenvalue, <em>λ<sub>1</sub></em> is known as the <em>algebraic connectivity</em> of the graph, because it provides certain bounds on how “connected” the graph is. That is, the larger <em>λ<sub>1</sub></em> is, the more connected the graph. Namely, by adding an edge between two vertices in the graph, <em>λ<sub>1 </sub></em>becomes greater or stays the same, but never decreases.</p>
<p>This seems simple enough, but already things get wacky with quantum graphs.</p>
<p>A quantum graph is a simple graph (a set of vertices and set of undirected edges with no self-loops or multiedges) now with lengths associated to each edge. A first difference between a quantum graph and a typical graph is that things are no longer discrete. Now when we think of function on the graph, we think of a function that operates on the vertices and <em>all points along the edges</em>. The last piece that defines a quantum graph is a self-adjoint differential operator, which acts much like the Laplacian on discrete graphs. Now when we speak about the spectrum, we are referring to eigenvalues of this operator.</p>
<p>So what can be said about the connectivity about a quantum graph? Well, let’s think about adding an edge between two existing vertices. In the discrete case, adding an edge essentially made the two endpoints “closer” in graph terms. In the quantum case, however, we are now adding to the total <em>length</em> of the graph, since each edge has an associated length and adds all points along the edge. So, when the edge is long enough, we are actually decreasing the connectivity of the graph! That is, by adding this edge we may be shrinking <em>λ<sub>1</sub></em>. The exact statement is given by Kurasov et al. in <a href="http://iopscience.iop.org/1751-8121/46/27/275309">this paper</a>, and includes the specific conditions on the edge for which the connectivity shrinks.</p>
<p>So is there a way to change the connectivity without adding length to the graph? The same paper tells us there is! Namely, if we change the graph by picking two vertices and joining them together, but keeping the set of edges the same, we can increase connectivity! Their proof is some clever manipulation of the Rayleigh quotient to show that <em>λ<sub>1</sub></em> for the newly obtained graph is at least as big as <em>λ<sub>1</sub></em> in the original.</p>
<p>One fun consequence of this is the fact that the most highly connected quantum graph is the flower graph with <em>n</em> loops attached to one vertex. Tell that to your sweetheart next time you bring them daisies.</p>
<p><img src="http://www.walkingrandomly.com/images/SAGE/sage_flower.png" alt="" height="209" class="alignnone aligncenter" width="228" /></p>
<p style="text-align: right;"><em>                       The flower graph with n loops has the largest spectral gap among all graphs formed by a given set of edges.</em></p>
<h1><b> Hearing the shape of a quantum graph </b></h1>
<p>To end, we’ll have just a bit of fun. Last time we asked whether or not, given a set of eigenvalues, it is possible to determine the shape of the graph. This is related to the question of hearing the shape of a drum given its fundamental tone.</p>
<p>We said generally that it is possible to have non-isomorphic graphs share the same spectrum (isospectral). So, unfortunately, it is not possible to hear the shape of a graph (or a drum, for that matter). But how bad are our chances for discrete vs. quantum graphs? Well, for discrete graphs, our chances are pretty bad. It has been shown that there are families of non-isomorphic isospectral graphs of size that grow exponentially in the number of edges. That’s a lot of graphs that sound the same! The quantum case? Well, according to Ralf Rueckriemen, who did his <a href="http://arxiv.org/abs/1110.3626" title="thesis">thesis</a> on quantum graph spectra, we at least know that any family of isospectral quantum graphs is finite. So if you really want a better shot at hearing the shape of a graph, go quantum!</p>
<p>The post <a href="https://blog.xrds.acm.org/2014/07/how-connected-are-quantum-graphs/" rel="nofollow">How Connected are Quantum Graphs?</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Olivia <a href="https://blog.xrds.acm.org/2014/07/how-connected-are-quantum-graphs/"><span class="datestr">at July 04, 2014 01:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1251">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2014/05/the-geometric-origins-of-spectral-graph-theory/">The Geometric Origins of Spectral Graph Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Spectral graph theory is the study of the intimate relationship of <a href="http://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvalues</a> to different properties of graphs. Most typically the eigenvalues are associated to eigenfunctions of matrices associated to graphs known as Laplacians. Let $latex G = (V, E)$ be a weighted or unweighted undirected graph (there are simple extensions to directed graphs for many problems). Let $latex A$ be the adjacency matrix and let $latex D$ be the diagonal degree matrix, that is, $latex (D)_{ii} = d_i$, the degree of vertex $latex i$. Then a common definition for the Laplacian is $latex L = D-A$.</p>
<p>As I develop in my research in spectral graph theory, I am consistently amazed by the truth that many results in spectral graph theory can be seen as discrete analogues to results in spectral geometry. I am not accustomed to thinking of graphs as geometric objects, but in fact graph Laplacian matrices are nicely related to Laplace operators on Riemannian manifolds. In this post, I’d like to discuss a few of these relationships.<span id="more-1251"></span></p>
<h2>Discretizing a membrane</h2>
<p>Let’s consider a simple example. Imagine a membrane in the $latex xy$ plane with vertical displacement given by $latex z(x, y)$. Then the movement of the membrane can be described by the wave equation:</p>
<p align="center">$latex \Delta z = \frac{1}{c^2}\cdot\frac{\partial^2 z}{\partial t^2}, \ \ \ \ \ \ \ (1)$</p>
<p>where $latex t$ represents time, $latex c$ is the speed of the wave, and $latex \Delta$ is the Laplace operator acting on the function $latex z$. If we assume the membrane moves like a spring, Hooke’s law gives us</p>
<p align="center">$latex \frac{\partial^2 z}{\partial t^2} = -kz,$</p>
<p>where $latex k$ is a constant representing the stiffness of the spring. Then, plugging<br />
this in to (1), our wave equation simplifies to</p>
<p align="center">$latex -\Delta z = \frac{k}{c^2}z. \ \ \ \ \ \ \ (2)$</p>
<p>In other words, $latex z$ is an eigenfunction of the Laplace operator $latex \Delta$. We’ll return to this in a bit, but for now let’s do a little magic on the left side of the equation.</p>
<p>The Laplace operator returns the sum of second partial derivatives, so we can expand the above as</p>
<p align="center">$latex \frac{\partial^2 z}{\partial x^2}+\frac{\partial^2 z}{\partial y^2} = \frac{1}{c^2}\cdot\frac{\partial^2 z}{\partial t^2}.$</p>
<p>Suppose we want to approximate the membrane by a discrete grid of size $latex w$.</p>
<p style="text-align: center;"><a href="http://xrds.acm.org/blog/wp-content/uploads/2014/05/Ograph.png"><img src="http://xrds.acm.org/blog/wp-content/uploads/2014/05/Ograph-300x161.png" alt="Ograph" height="161" class="alignnone size-medium wp-image-1272" width="300" /></a></p>
<p>That is, we would like to approximate the continuous Laplacian on the left side of the equation. First, we have that</p>
<p align="center">$latex \frac{\partial z(x,y)}{\partial x} \approx \frac{z(x,y) – z(x-w,y)}{w},$</p>
<p align="center">$latex \frac{\partial z(x,y)}{\partial y} \approx \frac{z(x,y) – z(x,y-w)}{w}$</p>
<p>so we can approximate the second partial derivative with respect to $latex x$ by</p>
<p align="center">$latex \frac{\partial^2 z(x,y)}{\partial x^2} \approx \frac{\frac{\partial z(x+w,y)}{\partial x} – \frac{\partial z(x,y)}{\partial x}}{w} = \frac{z(x+w,y) + z(x-w,y) – 2z(x,y)}{w^2},$</p>
<p>and similarly for $latex y$. Then the left side of (2) becomes:</p>
<p align="center">$latex -\Delta z \approx \frac{4z(x,y) – z(x+w,y) – z(x-w,y) – z(x,y+w) – z(x, y-w)}{w^2}.$</p>
<p>(This is the finite difference method.) Now recall, this was derived by approximating the membrane by a discrete mesh. Let’s visualize a few points on the grid:</p>
<p><a href="http://xrds.acm.org/blog/wp-content/uploads/2014/05/discretelaplacian_grid.png"><img src="http://xrds.acm.org/blog/wp-content/uploads/2014/05/discretelaplacian_grid-300x220.png" alt="discretelaplacian_grid" height="220" class="size-medium wp-image-1266 aligncenter" width="300" /></a></p>
<p>If we instead consider these points as nodes and edges in a graph, and use the graph Laplacian $latex L = D-A$ as our operator. Then we can see that</p>
<p align="center">$latex Lz(x,y) = 4z(x,y) – z(x+w,y) – z(x-w,y) – z(x,y+w) – z(x, y-w).$</p>
<p>But this is exactly the (unnormalized) approximated discrete Laplacian above! Remarkable! Now we’re beginning to see that this whole graph “Laplacian” nomenclature has some substance after all, it really is a discrete analogue of the Laplace operator!</p>
<h2>Manifolds or graphs?</h2>
<p>Beyond their operations, there are elegant parallels between the Riemannian manifold setting and the discrete graph setting for Laplacians. For example, let $latex U \subset \mathbb{R}^d$ be an open, non-empty set and let $latex L^2(U)$ denote the space of square integrable functions on $latex U$. Then the domain of the Laplace operator $latex D(\Delta)$ is the space of smooth, compactly supported functions on $latex U$ and dense in $latex L^2(U)$. Then a first property is that the Laplace operator is symmetric on $latex D(\Delta)$. Similarly, the Laplacian $latex L$ is a symmetric matrix. Another parallel is the range of the spectra of $latex \Delta$ and $latex L$. On $latex \mathbb{R}^n$, the spectrum of $latex \Delta$ is $latex [0,\infty)$, and similarly $latex L$ has all non-negative real eigenvalues (that is, it is always positive semidefinite).</p>
<p>According to spectral graph theorist Fan Chung, it is possible to treat the continuous and discrete cases by a universal approach:</p>
<blockquote><p>The general setting is as follows:</p>
<ol>
<li>an underlying space $latex M$ with a finite measure $latex \mu$;<i><br />
</i></li>
<li>a well-defined Laplace operator $latex \mathcal{L}$ on functions on $latex M$ [a matrix or a graph] so that $latex \mathcal{L}$ is a self-adjoint operator in $latex L^2(M,\mu)$ with a discrete spectrum;</li>
<li>if $latex M$ has a boundary then the boundary condition should be chosen so that<br />
it does not disrupt self-adjointness of $latex \mathcal{L}$;</li>
<li>a distance function $latex dist(x,y)$ on $latex M$ so that $latex |\nabla dist| \leq 1$ for an appropriate notion of gradient.</li>
</ol>
<p style="text-align: right;"><span style="font-style: normal;">(Chung, <em>Spectral Graph Theory, 48.</em>)</span></p>
<p style="text-align: right;">
</p></blockquote>
<h2 style="text-align: left;">Boundary conditions and hearing the shape of a graph</h2>
<p>The spectrum of the continuous Laplace operator gained due recognition with the famous question posed by Mark Kac: <em>can you hear the shape of a drum?</em> The question essentially asked whether drums can be isospectral, or share eigenvalues.</p>
<p>Specifically, let’s model a drum as a membrane stretched and clamped over a boundary, represented by some domain $latex D$ in the plane. Let $latex \lambda_i$ be the <a href="http://en.wikipedia.org/wiki/Dirichlet_eigenvalue"><em>Dirichlet eigenvalues</em></a>, defined by</p>
<p align="center">$latex -\Delta u = \lambda u,$</p>
<p>with the constraint that $latex u = 0$ on the boundary of $latex D$ (consider the membrane from equation (2) with a boundary, for instance). Then these Dirichlet eigenvalues are precisely the fundamental tone and harmonics the drum can produce. The question, then, is: given the set of Dirichlet eigenvalues, can we infer the shape of the drum? That is, do there exist distinct isospectral domains in the plane?</p>
<p>We can describe a similar problem in the discrete case. Let $latex G$ be a graph and $latex S$ and induced subgraph with non-empty vertex boundary (the set of vertices not in $latex S$ but adjacent to vertices in $latex S$). Then we say a function $latex f: V \rightarrow \mathbb{R}$ satisfies the Dirichlet boundary condition when $latex f(v) = 0$ for every vertex $latex v$ in the vertex boundary of $latex S$. Then, for some function $latex f$ satisfying the Dirichlet boundary condition, the Dirichlet eigenvalues of $latex G$ with respect to $latex S$ are the $latex \lambda_i$ satisfying</p>
<p align="center">$latex \mathcal{L}f(v) = \lambda f(v)$</p>
<p>for every $latex v$ in $latex S$. Note here that $latex \mathcal{L}$ is the <em>normalized Laplacian</em> given by $latex \mathcal{L} = D^{-1/2}LD^{-1/2}$. Then an analagous question is: given the set of Dirichlet eigenvalues, can we infer the shape of $latex S$?</p>
<p style="text-align: center;"><a href="http://xrds.acm.org/blog/wp-content/uploads/2014/05/Ograph3.png"><img src="http://xrds.acm.org/blog/wp-content/uploads/2014/05/Ograph3-300x147.png" alt="Ograph3" height="147" class="alignnone size-medium wp-image-1276" width="300" /></a></p>
<p style="text-align: left;">The answer to both questions turns out to be yes. The first construction of isospectral drums in two dimensions was given by <a href="http://www.ams.org/journals/bull/1992-27-01/S0273-0979-1992-00289-6/">Gordon, Webb, and Wolper</a>t in 1992, and toward the end of the 2000 ought’s, <a href="http://iopscience.iop.org/1751-8121/42/17/175202">Ram Band, Ori Parzanchevski, and Gilad Ben-Shach</a> gave a construction of isospectral drums and graphs (a follow up is <a href="http://link.springer.com/article/10.1007/s12220-009-9115-6">here</a>).</p>
<h2 style="text-align: left;">The not-so-scary continuous cousin</h2>
<p>So, as it turns out the “Laplacian” name of our star player in spectral graph theory is not so arbitrary, and there are many parallels between the continuous Laplace operator and the discrete graph Laplacian. As I continue to enrich my understanding of the connections between the two cases, I can only hope that the power of the Laplace operator will help me gain intuition about the power of the graph Laplacian. To conclude, I leave the reader with a few words from Chung’s book:</p>
<blockquote><p>For almost every known result in spectral geometry, a corresponding [question] can be asked: Can the results be translated to graph theory?</p>
<p style="text-align: right;">Chung, Spectral Graph Theory, 54.</p>
<p style="text-align: right;">
</p><p style="text-align: right;">
</p></blockquote>
<p style="text-align: left;"><em>A special thanks to Kyle Mooney for the images.</em></p>
<p>The post <a href="https://blog.xrds.acm.org/2014/05/the-geometric-origins-of-spectral-graph-theory/" rel="nofollow">The Geometric Origins of Spectral Graph Theory</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Olivia <a href="https://blog.xrds.acm.org/2014/05/the-geometric-origins-of-spectral-graph-theory/"><span class="datestr">at May 11, 2014 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1215">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2014/04/software-packages-for-theoreticians-by-theoreticians/">Software Packages for Theoreticians by Theoreticians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Brown University’s<a href="http://icerm.brown.edu/home/index.php" title="ICERM"> ICERM</a> recently hosted a workshop titled <a href="http://icerm.brown.edu/sp-s14-w3/">“Electrical Flows, Graph Laplacians, and Algorithms,”</a> where top researchers convened to present and discuss their recent progress in spectral graph theory and algorithms. <a href="http://math.mit.edu/~rpeng/">Richard Peng</a> opened up the workshop with an overview talk on efficient solvers for linear systems with graph Laplacians as the coefficient matrix. He presented a thorough history of the topic and set the stage for the variety of technical talks on fast algorithms for graph sparsification, spectral clustering, computing max flow, as well as a variety of other local and approximation algorithms.</p>
<p>His talk (as well as many of the rest) are archived and <a href="http://icerm.brown.edu/video_archive/">available</a> thanks to ICERM. I will focus on one highlight – a point that resonated with the conclusion of Richard Peng’s talk – a call for more software implementing these new, fast algorithms. In this light, I’d like to briefly discuss some of the software packages out there for spectral graph theory and the analysis of large graphs being developed by theoreticians active in the area.<span id="more-1215"></span></p>
<h2>Trilinos</h2>
<p><a href="http://www.trilinos.org">Trilinos</a> is a project out of Sandia National Labs for developing robust parallel algorithms and implementing them with general purpose software. The focus of the project is enabling technologies for large-scale scientific problems as to encourage further research in the field of parallel, robust, large-scale algorithms. In recognition of existing software for numerical computation, the developers at Trilinos make use of established packages such as <a href="http://www.netlib.org/lapack/">LAPACK</a> (for solving systems of simultaneous linear equations), and provides interfaces for <a href="http://www.cs.sandia.gov/CRF/aztec1.html">Aztec</a> (a parallel solver for sparse linear systems), <a href="http://crd-legacy.lbl.gov/~xiaoye/SuperLU/">SuperLU</a> (high performance LU factorization for solving linear systems), Mumps, and Umfpack among others. Currently, Trilinos provides robust parallel numerical algorithms for automatic differentiation, partitioning, preconditioning, and solving linear and nonlinear systems to name a few. The beauty of having a team of algorithmists behind the project is the emphasis on enabling further algorithmic research by building tools for developing tools.</p>
<h2>Zoltan</h2>
<p><a href="http://www.sandia.gov/~egboman/">Erik Boman</a> has contributed important work in the area of the preconditioners for linear systems and the support theory for preconditioners. <a href="http://www.cs.sandia.gov/Zoltan/Zoltan.html">Zoltan</a>, one of his projects, is a toolkit also from Sandia National Labs comprised of combinatorial algorithms for parallel or unstructured applications. It uses dynamic load balancing and partitioning algorithms for parallelizing the computation of applications whose work loads change over the course of the computation. To deal with the problem of <i>dynamic </i>partitioning, a suite of partitioning algorithms is included in the Zoltan toolkit. In particular, it makes use of geometric algorithms (group together objects that are physically close), graph algorithms (minimize a cut dividing groups of objects), and hypergraph algorithms (minimize communication costs between groups of objects) for load balancing partitioning. Another important function Zoltan provides is for graph coloring and graph ordering, which in turn can be used for parallel preconditioners and linear solvers.</p>
<h2>Sangria</h2>
<p><a href="http://www.cs.cmu.edu/~sangria/sangria.html">Sangria</a> is another project focused on developing and implementing parallel geometric and numerical algorithms. Housed at Carnegie Mellon, the software uses parallel algorithms for simulating complex flows with dynamic interfaces that achieves good accuracy.</p>
<h2>MatlabBGL</h2>
<p><a href="https://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/">MatlabBGL</a> is a Matlab package written by <a href="https://www.cs.purdue.edu/homes/dgleich/">David Gleich</a>, designed to work with sparse graphs on hundreds of thousands of nodes. The library includes common graph algorithms such as computing shortest paths (Dijkstra, Bellman-Ford, Floyd-Warshall), finding an minimum spanning tree (Kruskal, Prim), depth-first search, breadth-first search, and max flow – all optimized for efficiency on large graphs. One of the most useful features of MatlabBGL is the visitor feature for monitoring an algorithm, implemented from the <a href="http://www.boost.org/doc/libs/1_55_0/libs/graph/doc/">Boost Graph Library</a>. Visitors output all the steps taken by the algorithm, and dissecting the output is useful for optimization. A nice illustration of this feature with Dijkstra’s algorithm in the documentation tells us how a graph is explored, vertex by vertex.</p>
<h2>SVR Meshing</h2>
<p><a href="http://www.cs.cmu.edu/~bhudson/">Beno<span style="font-family: Ubuntu, 'Times New Roman';">î</span>t Hudson’s</a> PhD work was on sparse mesh refinement, and <a href="http://sparse-meshing.com/svr/0.2.1/">SVR</a> (for Sparse Voronoi Refinement) is the implementation of his algorithm for Delaunay refinement. SVR is a provably fast algorithm for producing small meshes, which is useful for when remeshing occurs during simulation due to domain change or refinement.</p>
<h2>CMG + SpA</h2>
<p><a href="http://www.cs.cmu.edu/~jkoutis/SpectralAlgorithms.htm">SpA</a> is a Matlab program for computing the effective resistances in an electrical network and is an implementation of the <a href="http://arxiv.org/abs/0803.0929">Spielman-Srivastrava</a>. As this requires solving linear systems, it uses <a href="http://www.cs.cmu.edu/~jkoutis/cmg.html">Combinatorial Multigrid (CMG)</a>, a Matlab-based solver for linear systems in symmetric diagonally-dominant matrices written by <a href="http://www.cs.cmu.edu/~jkoutis/">Yiannis Koutis</a>.</p>
<p>This is by no means a comprehensive list, I encourage you include more useful software in the comments.</p>
<p><a href="https://www.flickr.com/photos/ezu/" title="here"> </a></p>
<p>The post <a href="https://blog.xrds.acm.org/2014/04/software-packages-for-theoreticians-by-theoreticians/" rel="nofollow">Software Packages for Theoreticians by Theoreticians</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Olivia <a href="https://blog.xrds.acm.org/2014/04/software-packages-for-theoreticians-by-theoreticians/"><span class="datestr">at April 28, 2014 04:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1198">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2014/04/big-data-communication-and-lower-bounds/">Big Data, Communication and Lower Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p style="text-align: left;">As the size of the available data increases, massive data sets cannot be stored in their entirety in memory of a single machine. Furthermore due to the small amount of memory and computation power available on a single node, one needs to distribute the data and computation among multiple machines when processing tasks.</p>
<p style="text-align: left;">However, transferring the big data is very expensive. In fact, it is more expensive than the computations on the datasets. Thus, in the distributed model, the amount of communication plays an important role in the total cost of an algorithm and the aim is to minimize the amount of communication among processors (CPUs). This is one of the main motivations to study the theory of Communication Complexity, which originates from Big Data processing.<span id="more-1198"></span></p>
<p style="text-align: left;">
Communication Complexity (CC) has a rich theory behind it and exhibits a beautiful mathematical structure which can be explored by using various mathematical tools.<br />
In fact, Communication Complexity can be applied to many different problems from theory of computation to other related fields, making this area a fundamental key in our understanding of computation.</p>
<p style="text-align: left;">
CC studies the amount of communication bits that the participants of a communication system need to exchange in order to perform certain tasks. A very simple model for exploring this type of questions was proposed by Yao et. al. in 1979[<a href="http://dl.acm.org/citation.cfm?id=804414">1</a>]. In their model there are two parties, denoted as Alice and Bob and their goal is to compute some specific function f(x, y), which x is input for Alice and y is input for Bob. The results proven in this model can be generalized to more complicated scenarios as well.</p>
<p style="text-align: left;">
Although, at first glance it seems that the field of communication complexity is mostly related to problems in which explicit communication is involved, such as distributed computing, the fact is that its applications are much broader, some of which communication does not even appear in the problem. Examples of such problems are: designing Boolean Circuits, Networks and Data Structures, in particular with regards to computing the lower bounds on the related cost in these type of problems.</p>
<div style="width: 650px;" id="attachment_1222" class="wp-caption alignnone"><a href="http://xrds.acm.org/blog/wp-content/uploads/2014/04/8097457581_2651abd9a5_z.jpg"><img src="http://xrds.acm.org/blog/wp-content/uploads/2014/04/8097457581_2651abd9a5_z.jpg" alt="Image licensed in CC 2.0 by Jonny Goldstein" height="431" class="size-full wp-image-1222" width="640" /></a><p class="wp-caption-text">Image licensed in <a href="https://www.flickr.com/photos/jonnygoldstein/8097457581/in/photostream/" target="_blank">CC 2.0</a> by <a href="https://www.flickr.com/photos/jonnygoldstein/" target="_blank">Jonny Goldstein</a></p></div>
<p style="text-align: left;">It might be surprising and odd that CC can be applied to problems in which communication is not involved. Thus, here I discuss about a few basic problems which communication complexity plays a key role:</p>
<p style="text-align: left;"><strong>Distributed Learning via CC:<br />
</strong><br />
Let’s consider a framework where the data is distributed between different locations and parties (each having an arbitrary partition of an overall dataset) and our main goal is to learn a low error hypothesis with respect to the overall distribution of data, using as small amount of communication and as few rounds of communication, as possible, i.e. in distributed learning we are looking for applicable techniques for achieving communication-efficient learning. Different problems such as classification, optimization and differential privacy have been discussed in this setting in some recent work [<a href="http://www.cc.gatech.edu/~ninamf/papers/distributed_learning.pdf">2</a>, <a href="http://arxiv.org/abs/1204.3523">3</a>, <a href="http://arxiv.org/abs/1202.6078">4</a>].</p>
<p style="text-align: left;"><strong>Data Outsourcing and Streaming Interactive Proofs via CC:<br />
</strong><br />
When the dataset is fairly large, the data owner cannot retain all the data and so the storage and computation needs to be outsourced to some service provider. In such situations, data owners wants to rest assured that the computations performed by service provider are correct and complete. We can model this scenario by a verification protocol over data stream, in which there is a resource-limited verifier V and more powerful prover P. The verifier starts a conversation with the prover which does the computations and solves the problem. Then, the prover sends a proof to show the validity of his answer and convince the verifier to accept its results. The streaming data models the incremental pass over the data by the verifier as it sends the data to the cloud. In this setting, verifier just requires tracking logarithmic amount of the data, but instead this requires the communication of information among the players. Here, the goal is to design an interactive proof system [<a href="http://en.wikipedia.org/wiki/Interactive_proof_system">5</a>] with logarithmic communication to verify each query, i.e. after seeing the input and the proof, the verifier should be able to verify the proof of a correct statement with high probability, and reject every possible proof which is presented for a wrong statement. Note that here we consider a more powerful verifier by allowing probabilistic verification. This way the problem of verification in cloud computing for massive data streams links to the communication complexity theory and Arthur-Merlin games [<a href="http://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">6</a>]. There has been a series of works on streaming interactive proofs for different problems, which can be found in [<a href="http://people.seas.harvard.edu/~jthaler/thesis.pdf">7</a>].</p>
<p style="text-align: left;"><strong>Data Structure Lower Bound via CC:<br />
</strong><br />
Here the golden key is to discover the link between communication complexity and data structure and then use this connection to prove lower bounds for data structures supporting certain type of queries. For example, consider we want to design an efficient data structure for answering the queries of type “is i in S?”. To evaluate the quality of the implementation, there are two measures: (1) space which is the total number of memory cells which is used; and, (2) time which is the number of accesses (reads or writes) to the memory needed to accomplish a task and answer a query. This data structure problem can be viewed as a communication complexity problem by setting two parties: One party (Alice) gets as an input a set S and the other party (Bob) gets as an input an element i. The goal is to check whether i is in S. It can be shown that any implementation for the data structure problem can be reduced to a protocol in communication complexity problem in which complexity is related to the complexity of the data structure and as a result, bound for the communication complexity implies the time-space trade-off for the corresponding data structure.</p>
<p style="text-align: left;">
A simple scenario to show this connection is as follows: suppose there is a cell-probe algorithm [<a href="http://stackoverflow.com/questions/15728520/cell-probe-model">8</a>] for a problem which uses a data structure with space s and t queries. This results in a communication protocol for the same problem with communication t (w + log s) in the following way: when the processor asks for the contents of a memory cell, this can be done by Alice sending a message of log s bits, indicating the index of the desired cell and Bob answers with w bits to describe the content of the cell and this scenario will be done in t rounds of communication. A nice study of communication complexity techniques for computing data structure lower bounds can be found in [<a href="http://erikdemaine.org/theses/mpatrascu.pdf">9</a>].</p>
<p style="text-align: left;"><strong>Property Testing Lower Bound via CC:<br />
</strong><br />
Property testing was discussed in a previous [<a href="http://xrds.acm.org/blog/2013/11/ultra-efficient-via-sublinearity-2/">post</a>] as a type of sublinear algorithms. To recap, in here our goal is to formalize the question “what can be determined about a large object when we have limited access to it?”. Studies show that there is strong connection between testing and communication complexity [<a href="http://web.mit.edu/matulef/www/papers/PTviaCC-camera.pdf">10</a>]. The biggest similarity is that both involve parties (tester and communication players) with unbounded computational power and restricted access to their input.</p>
<p style="text-align: left;">
In [<a href="http://web.mit.edu/matulef/www/papers/PTviaCC-camera.pdf">10</a>] they consider the case where the large object is the Boolean function f on n input bits and the goal is to decide whether this function has the property P. A variety of techniques and algorithms have been developed for testing Boolean functions, but what distinguishes this work is that they propose techniques for reducing property testing to communication complexity and use this connection for proving lower bounds in certain types of testing problems.</p>
<p style="text-align: left;">
The main idea behind the reduction from testing to communication complexity problem is to set up a communication game as follows: Alice has a function f and Bob has a function g as inputs and they want to check if the joint function h, which is some combination of functions f and g, has a particular property P or is \epsilon-far from all the functions which have the property P. In this setting, now the link is that the number of required queries for testing whether function h has this property will be related to the number of bits which Alice and Bob need to communicate to do this task.</p>
<p style="text-align: left;">As you can see from what we discussed above, the cases in which communication is not explicitly used, communication complexity is used for proving lower bounds. The communication complexity framework has been well-studied and there are several basic problems which are known to require a large amount of communication. Then, the hardness of these and related problems has been used to obtain lower bounds in many areas such as streaming algorithms, circuit complexity, data structures, proof complexity and property testing. The basic idea used here is as follows: in some specific problem that we would like to bound, instead of starting from “scratch” by studying the structure of the problem, we try to find a connection between that and a hard communication problem in which probably the communication complexity is well known . If we can ﬁnd such a connection, then we can reduce the work involved for proving new bounds, or give simpler proofs of known bounds.</p>
<p>Now maybe the big question here is that why we care about computing lower bounds and what is important about it?<br />
Observe the main difference between upper bounds and lower bounds [<a href="http://cs.au.dk/~larsen/papers/dissertation.pdf">11</a>]: Upper bounds show the existence of an efficient solution, while lower bounds must say something about all possible solutions even those which no one has thought of yet. So it’s not surprising that proving some non-trivial lower bound is significantly harder than obtaining some non-trivial upper bound. The natural goal when proving lower bounds is of course to show that the upper bounds we know for some problem are optimal, i.e. there cannot exist a faster data structure than the one we already have.</p>
<p>Now think of big data: after decades of research, we arrived at efficient solutions for most of the well-known problems in the field, i.e., the upper bounds. However, since we are dealing with massive data sets, even a small improvement in the performance of any key algorithms or data structure, would have a huge impact.<br />
Thus researchers strive to improve the known solutions. But when does it end? Can we always improve the solutions we have? Or is there some limit to how efficiently a data structure problem can be solved? This is exactly the question addressed by lower bounds. Lower bounds are mathematical functions putting a limit on the performance of algorithms and data structures [<a href="http://cs.au.dk/~larsen/papers/dissertation.pdf">11</a>].</p>
<p>As the concluding remark, it seems that theory of communication complexity and techniques for proving lower bounds serve as two important tools for improving our power to design efficient algorithms and data structures for massive data.</p>
<p>The post <a href="https://blog.xrds.acm.org/2014/04/big-data-communication-and-lower-bounds/" rel="nofollow">Big Data, Communication and Lower Bounds</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Samira Daruki <a href="https://blog.xrds.acm.org/2014/04/big-data-communication-and-lower-bounds/"><span class="datestr">at April 28, 2014 04:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1065">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2014/01/the-thesis/">The Thesis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>All PhD candidates around the world know about the thesis. You always knew about the thesis. It marks the beginning of the end for your career as a PhD and if you actually do it, you can have that  cool “Dr.” title that you always wanted in your business card. What is the problem then? Why it seems so frustrating when you are sitting down to do it? The following is based on a true story, actually my story. How I managed to write it down and track my progress.<span id="more-1065"></span></p>
<p><strong>Problem Definition</strong></p>
<p>A typical PhD follows a simple process: read, think, propose, publish, and the <i>thesis</i>. It is straightforward and one can imagine that if you are already there with the rest of the stuff, the write up would be rather easy. But it is not.</p>
<p>The problem lies, mostly in that writing the thesis is a lengthy and lonely act. You have to do it, nobody will come to your aid, except maybe from your advisor.</p>
<p>In my case, I faced the following problem; for quite some time, I could not motivate myself to write it down. I began writing and half page later, I always stopped. I tried everything, but nothing seemed to motivate me. My advisor got uncomfortable and we began talking about a method to track my progress that would motivate me.</p>
<p><strong>The Idea</strong></p>
<p>Then I saw it, Georgios Gousios’s Thesis-o-meter (see link below). This was a couple of scripts that posted every day the progress of the PhD in each chapter. I decided to do it myself, introducing some alterations that would work better for me.</p>
<p>First, I had to find a tangible way to measure the progress. I thought that was easy, the number of pages. The number of pages of a document is nice, if you want to measure the size of the text, but surely it cannot act as a day-to-day key performance indicator (KPI). And why is that? Because simply if you bootstrap your thesis in LaTeX and you put all the standard chapters, bibliography, etc you will find yourself with at least 15 pages. So, that day I would have an enormous progress. The next day, I would write only text. I think one or two pages. The other day text and I would put on some charts. This will count as three of four pages. Better huh? This is the problem.</p>
<p>If you are a person like me, you could add one or two figures, and say “Ok, I am good for today, I added two pages!”. This is a nice excuse if you want to procrastinate. I needed something that would present the naked truth. That would make me sit there and make some serious progress.</p>
<p>So, number of pages was out of the question, but I thought that we can actually use it. The number of pages will be the end goal with a minimum and a maximum. In Greece, a PhD usually has 150 to 200 pages length (in my discipline of course, computer science). So, I thought, this is the goal: a large block of text around those limits.</p>
<p>Then I thought that my metric should be the number of words in the text instead of the number of pages. Since, I wrote my thesis in LaTeX, I just count the words for each file with standard UNIX tools, for example with the command <code>wc -l myfile.tex</code>. So, the algorithm has the following steps:</p>
<ul>
<li>The goal is set to 150-200 pages in total</li>
<li>Each day,
<ul>
<li>Count the words for all files</li>
<li>Count the pages of the actual thesis file, for example the output PDF</li>
<li>Find the word contribution for that day just by subtracting from the previous’s day word count</li>
<li>Find an average of words per number of pages</li>
<li>Finally, provide an estimation for the completion of the thesis</li>
</ul>
</li>
</ul>
<p><strong>Experience Report</strong></p>
<p>I implemented this in Python and shell script. The process worked, each day a report was generated and sent to my advisor, but the best thing was that each day, I saw the estimation trimmed down a little. This is the last report I produced:</p>
<pre>10c10
     1899 build/2-meta-programming.tex
13c13
     1164 build/3-requirements.tex
60,61c60,61
&lt;    13931 build/thesis.bib
    14058 build/thesis.bib
&gt;    55747 total

---- Progress ----
Worked for 167 day(s) ... 
Last submission: 20121025
Word Count (last version): 55747
Page Count (last version): 179
Avg Words per Page (last version): 311
Last submission effort: 142

---- Estimations ----
Page Count Range (final version): (min, max) = (150, 200)
Word Count Range (final version): (min, max) = (46650, 62200)
Avg Effort (Words per Day): 184
Estimated Completion in: (min, max) = (-50, 35) days, (-2.50, 1.75) months
Estimated Completion Date: (best, worst) = (2012-08-11, 2012-12-16)</pre>
<p>The average words per page was 311 and I wrote almost 184 words each day.</p>
<p><strong>Epilogue</strong></p>
<p>I wrote my thesis, but I have not submitted it (at least now, but I hope to soon), for a number of practical reasons. Still, the process succeeded, I found my KPIs and they actually led me to finishing up the work. This is a fact and now I have to find another motivation-driven method to do the rest of the required stuff. C’est la vie.</p>
<p><strong>Related Links and Availability</strong><br />
I plan to release an open source version of my thesis-o-meter in <a href="https://github.com/bkarak">my Github profile</a> soon. I also found various alternative thesis-o-meters:</p>
<ul>
<li>Salvatore Scellato, <a href="http://www.cl.cam.ac.uk/~ss824/thesisometer.html">http://www.cl.cam.ac.uk/~ss824/thesisometer.html</a></li>
<li>Georgios Gousios, <a href="http://www.gousios.gr/sw/tom.html">http://www.gousios.gr/sw/tom.html</a></li>
<li>Justin Boyan, <a href="http://www.cs.cmu.edu/~jab/tom/">http://www.cs.cmu.edu/~jab/tom/</a></li>
</ul>
<p>The post <a href="https://blog.xrds.acm.org/2014/01/the-thesis/" rel="nofollow">The Thesis</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Vassilios Karakoidas <a href="https://blog.xrds.acm.org/2014/01/the-thesis/"><span class="datestr">at January 31, 2014 10:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://xrds.acm.org/blog/?p=1020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/xrds.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.xrds.acm.org/2013/12/the-evolution-of-local-graph-partitioning/">The Evolution of Local Graph Partitioning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.xrds.acm.org" title="Theory – XRDS">ACM Crossroads student magazine</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>As a follow-up to my <a href="http://xrds.acm.org/blog/2013/11/laying-the-foundation-for-a-common-ground/">previous post</a> on the discussion of where theory and experimentation meet in the study of large-scale networks, I would like to discuss in more detail one of the empirically best-performing algorithms which also has a sound theoretical background: spectral partitioning. In this post I will examine the history of the problem, outline some key results, and present some future ideas for the problem.<span id="more-1020"></span></p>
<h2>Finding local partitions</h2>
<p>The goal of a local partitioning algorithm is to identify a community in a massive network. In this context, a community can be loosely defined as a collection of well-connected vertices who are are also reasonably well-separated from the rest of the network. The quality of a community given by a subset of vertices $latex S\subseteq V$ can be determined by a number of measures. One common measure is a ratio of edge connections from $latex S$ to the rest of the network divided by the size of $latex S$, known as the <em>conductance</em>. Specifically, let $latex \partial(S)$ be the number of edges with one endpoint in $latex S$ and the other not in $latex S$, and let the <em>volume</em> of $latex S$, $latex \textmd{vol}(S) = \sum_{v\in S}deg(v)$ be the sum of the degrees of vertices in $latex S$. Then the conductance of $latex S$ is $latex \phi(S) := \partial(S)/\textmd{vol}(S)$. The goal of a local partitioning algorithm is, given a vertex $latex v\in V$, to identify a subset $latex S$ near $latex v$ with small conductance.</p>
<p>Local algorithms may be used iteratively to find global balanced cuts or a clustering of the entire network. However, the problem is also of independent interest in itself. The ability to identify local communities has important implications in social networks, for instance, when we are only interested in a set of entities with particular features, rather than how these features vary over the entire network. A clear example is advertising. A particular product will be of highest interest to a particular constituent, and advertisers are probably unconcerned with the rest of the market.</p>
<p>In any case, running time determines how these can be applied to massive networks. In general, local algorithms will have running times in terms of the size of the output – the local cluster – rather than the entire graph. The trend in the last decade or so has been in developing local partitioning algorithms which run in time nearly linear in the size of the output.</p>
<h2>Previous work on the problem</h2>
<p>Results for partitioning algorithms can be traced back to the Cheeger inequalities given by Alon and Milman in ’85. Recall that the <em>edge expansion</em> of a set, related to the conductance, is defined by $latex h_G(S) = \partial(S)/|S|$, where now we are simply concerned with the number of vertices in $latex S$, $latex |S|$. The edge expansion of a graph, $latex h(G)$, is the minimum edge expansion over all subsets. The Cheeger inequalities give a way to relate $latex h(G)$ to the second smallest eigenvalue of a normalized adjacency matrix. In her <a href="http://www.math.ucsd.edu/~fan/research/revised.html">book on spectral graph theory</a>, Chung gives an analog of the Cheeger inequalities for the conductance of a set as defined above, this time using the second smallest eigenvalue of the normalized Laplacian. The Cheeger inequalities prove that, in nearly linear time, an $latex O(1/\sqrt{\phi(G)})$-approximation to the conductance of the graph can be computed.</p>
<p>Following the Cheeger inequalities, a local partitioning algorithm was studied by Spielman and Teng beginning with their <a href="http://www.cs.yale.edu/homes/spielman/precon/precon.html">STOC result of 2004</a>. Their result was improved in 2006 by <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4031383&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4031383">Andersen, Chung, and Lang</a>, later in 2009 by <a href="http://dl.acm.org/citation.cfm?id=1536449">Andersen and Peres</a>, and in 2012 by <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6375296&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6375296">Gharan and Trevisan</a>.</p>
<p>The intuition behind these algorithms has to do with mixing rates. Let us revisit our notion of a community – a collection of well-connected vertices which are relatively separate from the rest of the graph. This can also be understood in term of random walks. Namely, if we start a random walk within a high-quality community, we can expect with reasonably high probability to remain within the community after a certain number of steps.</p>
<p>Theoretical results for local partitioning algorithms are typically formulated as follows. Given a set $latex S\subseteq V$, if a starting vertex $latex v$ is sampled from $latex S$, then with certain probability a set $latex T$ can be found with small conductance, in terms of $latex \phi(S)$, in time proportional to the size of $latex T$. The <em>work/volume</em> ratio is the work required for a single run of the algorithm divided by the volume of the output set. Spielman and Teng find such a set by examining threshold sets of the probability distribution of a $latex t$-step random walk from the vertex $latex v$ sampled with probability proportional to degree in $latex S$. The set output by their algorithm achieves conductance $latex O(\phi(S)^{1/2}\log^{3/2}n)$ with work/volume ratio $latex O(\phi(S)^{-2} polylog(n))$. This is improved by Andersen, Chung, and Lang using distributions of PageRank vectors to analyze random walks with a probability of being “reset” to the starting vertex at each step. Their algorithm improves the conductance of the output set to $latex O(\phi(S)^{1/2}\log^{1/2}n)$ with work/volume ratio $latex O(\phi(S)^{-1} polylog(n))$.</p>
<p>Andersen and Peres manage to improve the work/volume ratio of the output set. Their methods simulate a volume biased evolving set process which is a Markov chain with states that are subsets of vertices and transition rules that grow or shrink the current set. Specifically, start with a vertex $latex v$ and produce a sequence of sets $latex S_1, S_2, \ldots, S_{\tau}$ with the property that at least one set $latex S_t$ is such that $latex \partial(S_t)/\textmd{vol}(S_t) \leq O(\sqrt{\log(\textmd{vol}(S_{\tau}))/\tau})$. Then if the process constructs sets of volume at most $latex \gamma$ for all sets up to some time $latex T$, they achieve a set of volume at most $latex \gamma$ and conductance $latex O(\sqrt{\log\gamma/T})$.</p>
<h2>Current best results</h2>
<p>As mentioned, the results of Andersen and Peres have recently been improved by Gharan and Trevisan in their 2012 FOCS paper. Their main tool is again threshold sets of random walks, with an improved lower bound on the probability that a lazy random walk is entirely contained in a subset $latex S$ after $latex t$ steps. This is an improvement on the work of Spielman and Teng. In their algorithm, they also use an evolving set process, and beat the bounds of Andersen and Peres by performing copies of the evolving set process in parallel. They show that, for a starting vertex $latex v$, a target conductance $latex \phi\in(0,1)$, a target volume $latex \gamma$, and $latex 0 &lt; \epsilon &lt; 1$, their algorithm outputs a set $latex S$ with conductance $latex O(\sqrt{\phi/\epsilon})$ that performs with work/volume ratio $latex O(\gamma^{\epsilon}\phi^{-1/2}\log^2 n)$. The running time is slightly super linear in the size of the optimum.</p>
<p>All the results are summarized in the following table.</p>
<p><a href="http://xrds.acm.org/blog/wp-content/uploads/2013/12/localtable1.png"><img src="http://xrds.acm.org/blog/wp-content/uploads/2013/12/localtable1-300x81.png" alt="localtable" height="81" class="alignnone size-medium wp-image-1029" width="300" /></a></p>
<h2>Where do we go from here?</h2>
<p>This is a hard problem, but a rewarding one. Gharan and Trevisan design a local variant of the Cheeger inequalities that give a sublinear time algorithm with an approximation guarantee that does not depend on the size of the graph. This opens many doors for algorithms on massive networks. There is already a body of work that applies local partitioning algorithms. For instance, in <a href="http://bioinformatics.oxfordjournals.org/content/25/12/i253.short">identifying local alignments in protein networks</a>. However, as I also mentioned in my previous post, there is often some calibration involved in moving these algorithms to production. I think local partitioning algorithms are primed for application, due to their promising theoretical bounds and the simplicity of their implementations (simulating random walks) and can be close to mainstream application.</p>
<p>The post <a href="https://blog.xrds.acm.org/2013/12/the-evolution-of-local-graph-partitioning/" rel="nofollow">The Evolution of Local Graph Partitioning</a> appeared first on <a href="https://blog.xrds.acm.org" rel="nofollow">XRDS</a>.</p></div>







<p class="date">
by Olivia <a href="https://blog.xrds.acm.org/2013/12/the-evolution-of-local-graph-partitioning/"><span class="datestr">at December 27, 2013 02:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6204">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/">Statistical Physics: an introduction in two parts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Statistical physics has deep connections to many computational problems, including statistical inference, counting and sampling, and optimization. Perhaps especially compelling are the field’s insights and intuitions regarding “average-case complexity” and information-computation gaps. These are topics for which the traditional theoretical CS approaches seem <a href="http://www.cse.cuhk.edu.hk/~andrejb/pubs/average-now.pdf">ill-suited</a>, while on the other hand statistical physics has supplied a rich (albeit not always mathematically rigorous) theory.</p>
<p>Statistical physics is the first topic in the <a href="https://www.boazbarak.org/fall18seminar/">seminar course</a> I am co-teaching with Boaz this fall, and one of our primary goals is to explore this theory. This blog post is a re-working of a lecture I gave in class this past Friday. It is meant to serve as an introduction to statistical physics, and is composed of two parts: in the first part, I introduce the basic concepts from statistical physics in a hands-on manner, by demonstrating a phase transition for the Ising model on the complete graph. In the second part, I introduce random k-SAT and the satisfiability conjecture, and give some moment-method based proofs of bounds on the satisfiability threshold.</p>
<p><span style="color: #999999;"><em>Update on September 16, 3:48pm: the first version of this post contained an incorrect plot of the energy density of the Ising model on the complete graph, which I have amended below.</em></span></p>
<h2>I. From particle interactions to macroscopic behaviors</h2>
<p>In statistical physics, the goal is to understand how materials behave on a macroscopic scale based on a simple model of particle-particle interactions.</p>
<p>For example, consider a block of iron. In a block of iron, we have many iron particles, and each has a net <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}" class="latex" title="\{\pm 1\}" />-polarization or “spin” which is induced by the quantum spins of its unpaired electrons. On the microscopic scale, nearby iron atoms “want” to have the same spin. From what I was able to gather on <a href="https://en.wikipedia.org/wiki/Ferromagnetism#Explanation" title="Explanation of Ferromagnetism">Wikipedia</a>, this is because the unpaired electrons in the distinct iron atoms repel each other, and if two nearby iron atoms have the same spins, then this allows them to be in a physical configuration where the atoms are further apart in space, which results in a <strong>lower energy state</strong> (because of the repulsion between electrons).</p>
<p>When most of the particles in a block of iron have correlated spins, then on a macroscopic scale we observe this correlation as the phenomenon of magnetism (or ferromagnetism if we want to be technically correct).</p>
<p>In the 1890’s, Pierre Curie showed that if you heat up a block of iron (introducing energy into the system), it eventually loses its magnetization. In fact, magnetization exhibits a <strong>phase transition</strong>: there is a critical temperature, <img src="https://s0.wp.com/latex.php?latex=T_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_c" class="latex" title="T_c" />, below which a block of iron will act as a magnet, and above which it will suddenly lose its magnetism. This is called the “Curie temperature”. This phase transition is in contrast to the alternative, in which the iron would gradually lose its magnetization as it is heated.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/trans.png?w=600" alt="trans" class="alignnone size-full wp-image-6217" /></p>
<h3>The Ising Model</h3>
<p>We’ll now set up a simple model of the microscopic particle-particle interactions, and see how the global phenomenon of the magnetization phase transition emerges. This is called the <em>Ising model</em>, and it is one of the more canonical models in statistical physics.</p>
<p>Suppse that we have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> iron atoms, and that their interactions are described by the (for simplicity unweighted) graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> with adjacency matrix <img src="https://s0.wp.com/latex.php?latex=A_G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_G" class="latex" title="A_G" />. For example, we may think of the atoms as being arranged in a 3D cubic lattice, and then <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> would be the 3D cubic lattice graph. We give each atom a label in <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D+%3D+%5C%7B1%2C%5Cldots%2Cn%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[n] = \{1,\ldots,n\}" class="latex" title="[n] = \{1,\ldots,n\}" />, and we associate with each atom <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n]" class="latex" title="i \in [n]" /> a spin <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i \in \{\pm 1\}" class="latex" title="x_i \in \{\pm 1\}" />.</p>
<p>For each choice of spins or <em>state</em> <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> we associate the <em>total energy</em></p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+-+x%5E%5Ctop+A_G+x%3D+%5Csum_%7B%28i%2Cj%29+%5Cin+G%7D+-+x_i+x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = - x^\top A_G x= \sum_{(i,j) \in G} - x_i x_j" class="latex" title="E(x) = - x^\top A_G x= \sum_{(i,j) \in G} - x_i x_j" />.</p>
<p>If two interacting particles have the same spin, then they are in a “lower energy” configuration, and then they contribute <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-1" class="latex" title="-1" /> to the total energy. If two neighboring particles have opposite spins, then they are in a “higher energy” configuration, and they contribute <img src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+1" class="latex" title="+1" /> to the total energy.</p>
<p>We also introduce a <em>temperature</em> parameter <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. At each <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />, we want to describe what a “typical” configuration for our block of iron looks like. When <img src="https://s0.wp.com/latex.php?latex=T%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T=0" class="latex" title="T=0" />, there is no kinetic energy in the system, so we expect the system to be in the lowest-energy state, i.e. all atoms have the same spin. As the temperature increases, the kinetic energy also increases, and we will begin to see more anomalies.</p>
<p>In statistical physics, the “description” takes the form of a probability distribution over states <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />. To this end we define the <em>Boltzmann distribution</em>, with density function <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}(x)" class="latex" title="P_{\beta}(x)" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29+%5Cpropto+%5Cexp%28-%5Cbeta+E%28x%29%29%2C+%5Cqquad+%5Ctext%7Bwhere+%7D+%5Cbeta+%3D+%5Cfrac%7B1%7D%7BT%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}(x) \propto \exp(-\beta E(x)), \qquad \text{where } \beta = \frac{1}{T}." class="latex" title="P_{\beta}(x) \propto \exp(-\beta E(x)), \qquad \text{where } \beta = \frac{1}{T}." /></p>
<p>As <img src="https://s0.wp.com/latex.php?latex=T%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T\to 0" class="latex" title="T\to 0" />, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta\to\infty" class="latex" title="\beta\to\infty" />, <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}(x)" class="latex" title="P_{\beta}(x)" /> becomes supported entirely on the <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> that minimize <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x)" class="latex" title="E(x)" />; we call these the <em>ground states</em> (for connected <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> these are exactly <img src="https://s0.wp.com/latex.php?latex=x+%3D+%5Cpm+%5Cvec%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x = \pm \vec{1}" class="latex" title="x = \pm \vec{1}" />). On the other hand as <img src="https://s0.wp.com/latex.php?latex=T+%5Cto+%5Cinfty%2C+%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T \to \infty, \beta \to 0" class="latex" title="T \to \infty, \beta \to 0" />, all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> are weighted equally according to <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}" class="latex" title="P_{\beta}" />.</p>
<p>Above we have defined the Boltzmann distribution to be proportional to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+E%28x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(-\beta E(x))" class="latex" title="\exp(-\beta E(x))" />. To spell it out,</p>
<p><img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D%28x%29+%3D+%5Cfrac%7B1%7D%7BZ%28%5Cbeta%29%7D+%5Cexp%28-%5Cbeta+E%28x%29%29%2C+%5Cqquad+%5Ctext%7Bwhere+%7D+Z%28%5Cbeta%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+%5Cexp%28-%5Cbeta+E%28x%29%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}(x) = \frac{1}{Z(\beta)} \exp(-\beta E(x)), \qquad \text{where } Z(\beta) = \sum_{x \in \{\pm 1\}^n} \exp(-\beta E(x))." class="latex" title="P_{\beta}(x) = \frac{1}{Z(\beta)} \exp(-\beta E(x)), \qquad \text{where } Z(\beta) = \sum_{x \in \{\pm 1\}^n} \exp(-\beta E(x))." /></p>
<p>The normalizing quantity <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta)" class="latex" title="Z(\beta)" /> is referred to as the <em>partition function</em>, and is interesting in its own right. For example, from <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta)" class="latex" title="Z(\beta)" /> we can compute the <em>free energy</em> <img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(\beta)" class="latex" title="F(\beta)" /> of the system, as well as the <em>internal energy</em> <img src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(\beta)" class="latex" title="U(\beta)" /> and the <em>entropy</em> <img src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S(\beta)" class="latex" title="S(\beta)" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29+%3D+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln+Z%28%5Cbeta%29%2C+%5Cqquad+%5Cqquad+U%28%5Cbeta%29+%3D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cbeta%7D+%28%5Cbeta+F%28%5Cbeta%29%29%2C+%5Cqquad+%5Cqquad+S%28%5Cbeta%29+%3D+%5Cbeta%5E2+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cbeta%7D+F%28%5Cbeta%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(\beta) = -\frac{1}{\beta} \ln Z(\beta), \qquad \qquad U(\beta) = \frac{\partial}{\partial \beta} (\beta F(\beta)), \qquad \qquad S(\beta) = \beta^2 \frac{\partial}{\partial \beta} F(\beta)." class="latex" title="F(\beta) = -\frac{1}{\beta} \ln Z(\beta), \qquad \qquad U(\beta) = \frac{\partial}{\partial \beta} (\beta F(\beta)), \qquad \qquad S(\beta) = \beta^2 \frac{\partial}{\partial \beta} F(\beta)." /></p>
<p>Using some straightforward calculus, we can then see that <img src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S(\beta)" class="latex" title="S(\beta)" /> is the Shannon entropy,</p>
<p><img src="https://s0.wp.com/latex.php?latex=S%28%5Cbeta%29+%3D+-+%5Csum_%7Bx%7D+P_%7B%5Cbeta%7D%28x%29%5Cln+%28P_%7B%5Cbeta%7D%28x%29%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S(\beta) = - \sum_{x} P_{\beta}(x)\ln (P_{\beta}(x))," class="latex" title="S(\beta) = - \sum_{x} P_{\beta}(x)\ln (P_{\beta}(x))," /></p>
<p>that <img src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(\beta)" class="latex" title="U(\beta)" /> is the average energy in the system,</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28%5Cbeta%29+%3D+%5Csum_%7Bx%7D+P_%7B%5Cbeta%7D%28x%29+%5Ccdot+E%28x%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(\beta) = \sum_{x} P_{\beta}(x) \cdot E(x)," class="latex" title="U(\beta) = \sum_{x} P_{\beta}(x) \cdot E(x)," /></p>
<p>and that the free energy is the difference of the internal energy and the product of the temperature <img src="https://s0.wp.com/latex.php?latex=T+%3D+%5Cfrac%7B1%7D%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T = \frac{1}{\beta}" class="latex" title="T = \frac{1}{\beta}" /> and the entropy,</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29+%3D+U%28%5Cbeta%29+-+T+%5Ccdot+S%28%5Cbeta%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(\beta) = U(\beta) - T \cdot S(\beta)," class="latex" title="F(\beta) = U(\beta) - T \cdot S(\beta)," /></p>
<p>just like the <a href="https://en.wikipedia.org/wiki/Gibbs_free_energy">classical thermodynamic definitons</a>!</p>
<h3>Why these functions?</h3>
<p>The free energy, internal energy, and entropy encode information about the typical behavior of the system at temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />. We can get some intuition by considering the extremes, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to \infty" class="latex" title="\beta \to \infty" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to 0" class="latex" title="\beta \to 0" />.</p>
<p>In cold systems with <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to \infty" class="latex" title="\beta \to \infty" />, if we let <img src="https://s0.wp.com/latex.php?latex=E_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E_0" class="latex" title="E_0" /> be the energy of the ground state, <img src="https://s0.wp.com/latex.php?latex=N_0+%3D+%7C%5C%7Bx+%3A+E%28x%29+%3D+E_0%5C%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_0 = |\{x : E(x) = E_0\}|" class="latex" title="N_0 = |\{x : E(x) = E_0\}|" /> be the number of ground state configurations, and <img src="https://s0.wp.com/latex.php?latex=%5CDelta_E+%3D+%5Cmin_%7Bx+%3A+E%28x%29+%5Ctextgreater+E_0%7D+E%28x%29+-+E_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_E = \min_{x : E(x) \textgreater E_0} E(x) - E_0" class="latex" title="\Delta_E = \min_{x : E(x) \textgreater E_0} E(x) - E_0" /> be the energy gap, then</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%3D+N_0+%5Cexp%28-%5Cbeta+E_0%29%5Ccdot%5Cleft%281+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta) = N_0 \exp(-\beta E_0)\cdot\left(1 + O\left(\exp(-\beta \Delta_E)\right)\right)," class="latex" title="Z(\beta) = N_0 \exp(-\beta E_0)\cdot\left(1 + O\left(\exp(-\beta \Delta_E)\right)\right)," /></p>
<p>where the <img src="https://s0.wp.com/latex.php?latex=O%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\cdot)" class="latex" title="O(\cdot)" /> notation hides factors that do not depend on <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />. From this it isn’t hard to work out that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+F%28%5Cbeta%29+%26%3D+E_0+-%5Cfrac%7B1%7D%7B%5Cbeta%7D+%5Cln%28N_0%29+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%2C%5C%5C+E%28%5Cbeta%29+%26%3D+E_0+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29%5C%5C+S%28%5Cbeta%29+%26%3D+%5Cln+N_0+%2B+O%5Cleft%28%5Cexp%28-%5Cbeta+%5CDelta_E%29%5Cright%29.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} F(\beta) &amp;= E_0 -\frac{1}{\beta} \ln(N_0) + O\left(\exp(-\beta \Delta_E)\right),\\ E(\beta) &amp;= E_0 + O\left(\exp(-\beta \Delta_E)\right)\\ S(\beta) &amp;= \ln N_0 + O\left(\exp(-\beta \Delta_E)\right). \end{aligned}" class="latex" title="\begin{aligned} F(\beta) &amp;= E_0 -\frac{1}{\beta} \ln(N_0) + O\left(\exp(-\beta \Delta_E)\right),\\ E(\beta) &amp;= E_0 + O\left(\exp(-\beta \Delta_E)\right)\\ S(\beta) &amp;= \ln N_0 + O\left(\exp(-\beta \Delta_E)\right). \end{aligned}" /></p>
<p>We can see that the behavior of the system is dominated by the few ground states. As <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to \infty" class="latex" title="\beta \to \infty" />, all of the free energy can be attributed to the internal energy term.</p>
<p>On the other hand, as <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \to 0" class="latex" title="\beta \to 0" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+F%28%5Cbeta%29+%26%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5C%7B%5Cpm+1%5C%7D%5En%7D+E%28x%29+-+%5Cfrac%7Bn%7D%7B%5Cbeta%7D+%2B+O%28%5Cbeta%29%2C%5C%5C+U%28%5Cbeta%29+%26%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5C%7B%5Cpm+1%5C%7D%5En%7D+E%28x%29+%2B+O%28%5Cbeta%29%2C%5C%5C+S%28%5Cbeta%29+%26%3D+n+%2B+O%28%5Cbeta%29%2C+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} F(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) - \frac{n}{\beta} + O(\beta),\\ U(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) + O(\beta),\\ S(\beta) &amp;= n + O(\beta), \end{aligned}" class="latex" title="\begin{aligned} F(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) - \frac{n}{\beta} + O(\beta),\\ U(\beta) &amp;= \mathbb{E}_{x\sim\{\pm 1\}^n} E(x) + O(\beta),\\ S(\beta) &amp;= n + O(\beta), \end{aligned}" /></p>
<p>and the behavior of the system is chaotic, with the free energy dominated by the entropy term.</p>
<h3>Phase transition at the critical temperature.</h3>
<p>We say that the system undergoes a <em>phase transition</em> at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta_c" class="latex" title="\beta_c" /> if the <em>energy density</em> <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn%5Cto%5Cinfty%7D%5Cfrac%7B1%7D%7Bn%7DF%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n\to\infty}\frac{1}{n}F(\beta)" class="latex" title="f(\beta) = \lim_{n\to\infty}\frac{1}{n}F(\beta)" /> is not analytic at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cbeta_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta = \beta_c" class="latex" title="\beta = \beta_c" />. Often, this comes from a shift in the relative contributions of the internal energy and entropy terms to <img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(\beta)" class="latex" title="F(\beta)" />. Phase transitions are often associated as well with a <strong>qualitative</strong> change in system behavior.</p>
<p>For example, we’ll now show that for the Ising model with <img src="https://s0.wp.com/latex.php?latex=G+%3D+K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G = K_n" class="latex" title="G = K_n" /> the complete graph with self loops, the system has a phase transition at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_c+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta_c = \frac{1}{2}" class="latex" title="\beta_c = \frac{1}{2}" /> (the self-loops don’t make much physical sense, but are convenient to work with). Furthermore, we’ll show that this phase transition corresponds to a qualitative change in the system, i.e. the loss of magnetism.</p>
<p>Define the <em>magnetization</em> of the system with spins <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> to be <img src="https://s0.wp.com/latex.php?latex=m%28x%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m(x) = \frac{1}{n}\sum_i x_i" class="latex" title="m(x) = \frac{1}{n}\sum_i x_i" />. If <img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+m%28x%29+%5Cnot%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{n \to \infty} m(x) \not\to 0" class="latex" title="\lim_{n \to \infty} m(x) \not\to 0" />, then we say the system is <em>magnetized</em>.</p>
<p>In the complete graph, normalized so that the total interaction of each particle is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />, there is a direct relationship between the energy and the magnetization:</p>
<p><img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%2C+j%7D+-+x_i+x_j+%3D+-+n%5Ccdot+m%28x%29%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = \frac{1}{n}\sum_{i, j} - x_i x_j = - n\cdot m(x)^2." class="latex" title="E(x) = \frac{1}{n}\sum_{i, j} - x_i x_j = - n\cdot m(x)^2." /></p>
<h3>Computing the energy density.</h3>
<p>The magnetization takes values <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bk%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{k}{n}" class="latex" title="\frac{k}{n}" /> for <img src="https://s0.wp.com/latex.php?latex=k%5Cin%5C%7B-n%2C%5Cldots%2C+n%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k\in\{-n,\ldots, n\}" class="latex" title="k\in\{-n,\ldots, n\}" />. So, letting <img src="https://s0.wp.com/latex.php?latex=N_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_k" class="latex" title="N_k" /> be the number of states with magnetization <img src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k/n" class="latex" title="k/n" />, we have that</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%3D+%5Csum_%7Bx%7D+%5Cexp%28-%5Cbeta%28-+n%5Ccdot+m%28x%29%5E2%29%29+%3D+%5Csum_%7Bk+%3D+-n%7D%5E%7Bn%7D+N_k+%5Ccdot+%5Cexp%5Cleft%28%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta) = \sum_{x} \exp(-\beta(- n\cdot m(x)^2)) = \sum_{k = -n}^{n} N_k \cdot \exp\left(\beta n \left(\frac{k}{n}\right)^2\right)." class="latex" title="Z(\beta) = \sum_{x} \exp(-\beta(- n\cdot m(x)^2)) = \sum_{k = -n}^{n} N_k \cdot \exp\left(\beta n \left(\frac{k}{n}\right)^2\right)." /></p>
<p>Now, <img src="https://s0.wp.com/latex.php?latex=N_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_k" class="latex" title="N_k" /> is just the number of strings with Hamming weight <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%28n%2Bk%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}(n+k)" class="latex" title="\frac{1}{2}(n+k)" />, so <img src="https://s0.wp.com/latex.php?latex=N_k+%3D+%5Cbinom%7Bn%7D%7B%28n%2Bk%29%2F2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_k = \binom{n}{(n+k)/2}" class="latex" title="N_k = \binom{n}{(n+k)/2}" />. By Stirling’s approximation <img src="https://s0.wp.com/latex.php?latex=N_k+%5Capprox+%5Cexp%28n+%5Ccdot+H%28%5Cfrac%7B1%2Bk%2Fn%7D%7B2%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_k \approx \exp(n \cdot H(\frac{1+k/n}{2}))" class="latex" title="N_k \approx \exp(n \cdot H(\frac{1+k/n}{2}))" />, where <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is the entropy function, so up to lower-order terms</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29+%5Capprox+%5Csum_%7B%5Csubstack%7Bk%5Cin+%5B%5Cpm+n%5D%5C%5C+k%2Bn+%5Cequiv_2+0%7D%7D+%5Cexp%5Cleft%28+%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+H%5Cleft%28%5Cfrac%7B1+%2B+%5Cfrac%7Bk%7D%7Bn%7D%7D%7B2%7D%5Cright%29+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta) \approx \sum_{\substack{k\in [\pm n]\\ k+n \equiv_2 0}} \exp\left( \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1 + \frac{k}{n}}{2}\right) n\right)." class="latex" title="Z(\beta) \approx \sum_{\substack{k\in [\pm n]\\ k+n \equiv_2 0}} \exp\left( \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1 + \frac{k}{n}}{2}\right) n\right)." /></p>
<p>Now we apply the following simplification: for <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \mathbb{R}^n" class="latex" title="x \in \mathbb{R}^n" />, <img src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5C%7C_%7B%5Cinfty%7D+%5Cle+%5C%7Cx%5C%7C_1+%5Cle+n+%5C%7Cx%5C%7C_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\|x\|_{\infty} \le \|x\|_1 \le n \|x\|_{\infty}" class="latex" title="\|x\|_{\infty} \le \|x\|_1 \le n \|x\|_{\infty}" />, and then <img src="https://s0.wp.com/latex.php?latex=%5Clog%28%5C%7Cx%5C%7C_1%29+%3D+%5Clog+%5C%7Cx%5C%7C_%7B%5Cinfty%7D+%2B+O%28%5Clog+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log(\|x\|_1) = \log \|x\|_{\infty} + O(\log n)" class="latex" title="\log(\|x\|_1) = \log \|x\|_{\infty} + O(\log n)" />. Treating our summands as the entries of the vector <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />, from this we have,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clog%28Z%28%5Cbeta%29%29+%3D+%5Cmax_%7Bk+%5Cin+%5C%7B-n%2C%5Cldots%2C+n%5C%7D%7D+%5Cbeta+n+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+H%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cleft%281%2B%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5Ccdot+n%5Cright%29+%2B+O%5Cleft%28%5Clog+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log(Z(\beta)) = \max_{k \in \{-n,\ldots, n\}} \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1}{2}\left(1+\frac{k}{n}\right)\cdot n\right) + O\left(\log n\right)." class="latex" title="\log(Z(\beta)) = \max_{k \in \{-n,\ldots, n\}} \beta n \left(\frac{k}{n}\right)^2 + H\left(\frac{1}{2}\left(1+\frac{k}{n}\right)\cdot n\right) + O\left(\log n\right)." /></p>
<p>By definition of the energy density,</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5Cleft%28-+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%29%5Cright%29+%3D+-%5Clim_%7Bn%5Cto%5Cinfty%7D+%5Cmax_%7Bk+%5Cin+%5C%7B-n%2C%5Cldots%2C+n%5C%7D%7D+%5Cleft%28%5Cfrac%7Bk%7D%7Bn%7D%5Cright%29%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%281%2B%5Cfrac%7Bk%7D%7Bn%7D%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = \lim_{n \to \infty} \left(- \frac{1}{\beta n} \log Z(\beta)\right) = -\lim_{n\to\infty} \max_{k \in \{-n,\ldots, n\}} \left(\frac{k}{n}\right)^2 + \frac{1}{\beta}H\left(\frac{1}{2}(1+\frac{k}{n})\right)," class="latex" title="f(\beta) = \lim_{n \to \infty} \left(- \frac{1}{\beta n} \log Z(\beta)\right) = -\lim_{n\to\infty} \max_{k \in \{-n,\ldots, n\}} \left(\frac{k}{n}\right)^2 + \frac{1}{\beta}H\left(\frac{1}{2}(1+\frac{k}{n})\right)," /></p>
<p>and since <img src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \to \infty" class="latex" title="n \to \infty" /> independently of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />, we also have</p>
<p><img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+-%5Cleft%28%5Cmax_%7B%5Cdelta+%5Cin+%5B-1%2C1%5D%7D+%5Cdelta%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B2%7D%5Cright%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta) = -\left(\max_{\delta \in [-1,1]} \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right)," class="latex" title="f(\beta) = -\left(\max_{\delta \in [-1,1]} \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right)," /></p>
<p>because the error from rounding <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta \in [-1,1]" class="latex" title="\delta \in [-1,1]" /> to the nearest factor of <img src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2/n" class="latex" title="2/n" /> is <img src="https://s0.wp.com/latex.php?latex=O%281%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1/n)" class="latex" title="O(1/n)" />.</p>
<p>We can see that the first term in the expression for <img src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\beta)" class="latex" title="f(\beta)" /> corresponds to the square of the magnetization (and therefore the energy); the more magnetized the system is, the larger the contribution from the first term. The second term corresponds to the entropy, or the number of configurations in the support; the larger the support, the larger the contribution of the second term. As <img src="https://s0.wp.com/latex.php?latex=T+%3D+%5Cfrac%7B1%7D%7B%5Cbeta%7D%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T = \frac{1}{\beta}\to \infty" class="latex" title="T = \frac{1}{\beta}\to \infty" />, the contribution of the entropy term overwhelms the contribution of the energy term; this is consistent with our physical intuition.</p>
<h3>A phase transition.</h3>
<p>We’ll now demonstrate that there is indeed a phase transition in <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta" class="latex" title="\beta" />. To do so, we solve for this maximum. Taking the derivative with respect to <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta" class="latex" title="\delta" />, we have that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cdelta%7D+%5Cleft%28+%5Cdelta%5E2+%2B+%5Cfrac%7B1%7D%7B%5Cbeta%7DH%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B2%7D%5Cright%29%5Cright%29+%3D+2%5Cdelta+%2B+%5Cfrac%7B1%7D%7B2%5Cbeta%7D+%5Cln+%5Cleft%28%5Cfrac%7B1-%5Cdelta%7D%7B1%2B%5Cdelta%7D%5Cright%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\partial}{\partial \delta} \left( \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right) = 2\delta + \frac{1}{2\beta} \ln \left(\frac{1-\delta}{1+\delta}\right)," class="latex" title="\frac{\partial}{\partial \delta} \left( \delta^2 + \frac{1}{\beta}H\left(\frac{1+\delta}{2}\right)\right) = 2\delta + \frac{1}{2\beta} \ln \left(\frac{1-\delta}{1+\delta}\right)," /></p>
<p>so the derivative is <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> whenever <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B4%5Cbeta%7D+%5Cln%5Cleft%28%5Cfrac%7B1%2B%5Cdelta%7D%7B1-%5Cdelta%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = \frac{1}{4\beta} \ln\left(\frac{1+\delta}{1-\delta}\right)" class="latex" title="\delta = \frac{1}{4\beta} \ln\left(\frac{1+\delta}{1-\delta}\right)" />. From this, we can check the maxima. When <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Ctextgreater+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \textgreater \frac{1}{2}" class="latex" title="\beta \textgreater \frac{1}{2}" />, there are two maxima equidistant from the origin, corresponding to negatively or positively-magnetized states. When <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Ctextless+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \textless \frac{1}{2}" class="latex" title="\beta \textless \frac{1}{2}" />, the maximizer is <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />, corresponding to an unmagnetized state.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/exponent.png?w=600" alt="exponent" class="alignnone size-full wp-image-6210" /></p>
<p>Given the maximizers, we now have the energy density. When we plot the energy density, the phase transition at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta = \frac{1}{2}" class="latex" title="\beta = \frac{1}{2}" /> is subtle (an earlier version of this post contained a mistaken plot):</p>

<a href="https://windowsontheory.org/energy-density-2/"><img src="https://windowsontheory.files.wordpress.com/2018/09/energy-density1.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>
<a href="https://windowsontheory.org/energy-density-zoom-3/"><img src="https://windowsontheory.files.wordpress.com/2018/09/energy-density-zoom2.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>

<p>But when we plot the derivative, we can see that it is not smooth at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta = \frac{1}{2}" class="latex" title="\beta = \frac{1}{2}" />:</p>

<a href="https://windowsontheory.org/deriv/"><img src="https://windowsontheory.files.wordpress.com/2018/09/deriv.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>
<a href="https://windowsontheory.org/deriv-zoom/"><img src="https://windowsontheory.files.wordpress.com/2018/09/deriv-zoom.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>

<p>And with some calculus it is possible to show that the second derivative is indeed not continuous at <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta = \frac{1}{2}" class="latex" title="\beta = \frac{1}{2}" />.</p>
<p>Qualitatively, it is convincing that this phase transition in the energy density is related to a transition in the magnetization (because the maximizing <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta" class="latex" title="\delta" /> corresponds to the typical magnetization). One can make this formal by performing a similar calculation to show that the internal energy undergoes a phase transition, which in this case is proportional to the expected squared magnetization, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7BP_%5Cbeta%7D%5B-n+%5Ccdot+m%28x%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}_{P_\beta}[-n \cdot m(x)^2]" class="latex" title="\mathbb{E}_{P_\beta}[-n \cdot m(x)^2]" />.</p>
<h3>Beyond the complete graph</h3>
<p>The Ising model on the complete graph <img src="https://s0.wp.com/latex.php?latex=K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K_n" class="latex" title="K_n" /> (also called the Curie-Weiss model) is perhaps not a very convincing model for a physical block of iron; we expect that locality should govern the strength of the interactions. But because the energy and the magnetization are related so simply, it is easy to solve.</p>
<p>Solutions are also known for the 1D and 2D grids; solving it on higher-dimensional lattices, as well as in many other interesting settings, remains open. Interestingly, the <a href="https://en.wikipedia.org/wiki/Conformal_bootstrap">conformal bootstrap method</a> that <a href="https://windowsontheory.org/2018/08/06/physics-envy/">Boaz mentioned</a> has been <a href="https://arxiv.org/abs/1203.6064">used</a> towards solving the Ising model on higher-dimensional grids.</p>
<h2>II. Constraint Satisfaction Problems</h2>
<p>For those familiar with constraint satisfaction problems (CSPs), it may have already been clear that the Ising model is a CSP. The spins <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> are Boolean variables, and the energy function <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x)" class="latex" title="E(x)" /> is an objective function corresponding to the EQUALITY CSP on <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> (a pretty boring CSP, when taken without negations). The Boltzmann distribution gives a probability distribution over assignments to the variables <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />, and the temperature <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> determines the objective value of a typical <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \sim P_{\beta}" class="latex" title="x \sim P_{\beta}" />.</p>
<p>We can similarly define the energy, Boltzmann distribution, and free energy/entropy for any CSP (and even to continuous domains such as <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \mathbb{R}^n" class="latex" title="x \in \mathbb{R}^n" />). Especially popular with statistical physicists are:</p>
<ul>
<li>CSPs (such as the Ising model) over grids and other lattices.</li>
<li>Gaussian spin glasses: CSPs over <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" /> or <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \mathbb{R}^n" class="latex" title="x \in \mathbb{R}^n" /> where the energy function is proportional to <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x%5E%7B%5Cotimes+k%7D%2C+J%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle x^{\otimes k}, J\rangle" class="latex" title="\langle x^{\otimes k}, J\rangle" />, where <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J" class="latex" title="J" /> is an order-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> symmetric tensor with entries sampled i.i.d. from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(0,1)" class="latex" title="\mathcal{N}(0,1)" />. The Ising model on a graph with random Gaussian weights is an example for <img src="https://s0.wp.com/latex.php?latex=k+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 2" class="latex" title="k = 2" />.</li>
<li>Random instances of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT: Of the <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bn%7D%7Bk%7D+%5Ccdot+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\binom{n}{k} \cdot 2^k" class="latex" title="\binom{n}{k} \cdot 2^k" /> possible clauses (with negations) on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> variables, <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> clauses <img src="https://s0.wp.com/latex.php?latex=C_1%2C%5Cldots%2CC_m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_1,\ldots,C_m" class="latex" title="C_1,\ldots,C_m" /> are sampled uniformly at random, and the energy function is the number of satisfied clauses, <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%7C%5C%7B+i+%5Cin+%5Bm%5D+%3A+C_i%28x%29+%3D+1%5C%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = |\{ i \in [m] : C_i(x) = 1\}|" class="latex" title="E(x) = |\{ i \in [m] : C_i(x) = 1\}|" />.</li>
<li>Random instances of other Boolean and larger-alphabet CSPs.</li>
</ul>
<p>In some cases, these CSPs are reasonable models for physical systems; in<br />
other cases, they are primarily of theoretical interest.</p>
<h3>Algorithmic questions</h3>
<p>As theoretical computer scientists, we are used to seeing CSPs in the contex of optimization. In statistical physics, the goal is to understand the qualitative behavior of the system as described by the Boltzmann distribution. They ask algorithmic questions such as:</p>
<ul>
<li>Can we estimate the free energy <img src="https://s0.wp.com/latex.php?latex=F%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(\beta)" class="latex" title="F(\beta)" />? The partition function <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\beta)" class="latex" title="Z(\beta)" />? And, relatedly,</li>
<li>Can we sample from <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}" class="latex" title="P_{\beta}" />?</li>
</ul>
<p>But these tasks are not so different from optimization. For example, if our system is an instance of 3SAT, when <img src="https://s0.wp.com/latex.php?latex=T%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T=0" class="latex" title="T=0" />, the Boltzmann distribution is the uniform distribution over maximally satisfying assignments, and so estimating <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cinfty%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(\infty)" class="latex" title="Z(\infty)" /> is equivalent to deciding the SAT formula, and sampling from <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\infty}" class="latex" title="P_{\infty}" /> is equivalent to solving the SAT formula. As <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> increases, sampling from <img src="https://s0.wp.com/latex.php?latex=P_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{\beta}" class="latex" title="P_{\beta}" /> corresponds to sampling an approximate solution.</p>
<p>Clearly in the worst case, these tasks are NP-Hard (and even #P-hard). But even for random instances these algorithmic questions are interesting.</p>
<h3>Phase transitions in satisfiability</h3>
<p>In random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT, the system is controlled not only by the temperature <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> but also by the <em>clause density</em> <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cfrac%7Bm%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha = \frac{m}{n}" class="latex" title="\alpha = \frac{m}{n}" />. For the remainder of the post, we will focus on the zero-temperature regime <img src="https://s0.wp.com/latex.php?latex=T+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T = 0" class="latex" title="T = 0" />, and we will see that <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT exhibits phase transitions in <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> as well.</p>
<p>The most natural “physical” trait to track in a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT formula is whether or not it is <em>satisfiable</em>. When <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha = 0" class="latex" title="\alpha = 0" />, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT instances are clearly satisfiable, because they have no constraints. Similarly when <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+n%5Ek+%5Ccdot+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha = n^k \cdot 2^k" class="latex" title="\alpha = n^k \cdot 2^k" />, random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT instances cannot be satisfiable, because for any set of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> variables they will contain all <img src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k" class="latex" title="2^k" /> possible <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT constraints (clearly unsatisfiable). It is natural to ask: is there a satisfiability phase transition in <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" />?</p>
<p>For <img src="https://s0.wp.com/latex.php?latex=k+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 2" class="latex" title="k = 2" />, one can show that the answer is yes. For <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \ge 3" class="latex" title="k \ge 3" />, numerical evidence strongly points to this being the case; further, the following theorem of Friedgut gives a partial answer:</p>
<p><strong>Theorem:</strong><br />
For there exists a function <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c(n)" class="latex" title="\alpha_c(n)" /> such that for any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon \textgreater 0" class="latex" title="\epsilon \textgreater 0" />, if <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" /> is a random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT formula on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> variables with <img src="https://s0.wp.com/latex.php?latex=%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha n" class="latex" title="\alpha n" /> clauses, then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5CPr%5B%5Cphi+%5Ctext%7B+is+satisfiable%7D%5D+%3D+%5Cbegin%7Bcases%7D+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextless+%281-%5Cepsilon%29+%5Calpha_c%28n%29%5C%5C+0+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextgreater+%281%2B%5Cepsilon%29+%5Calpha_c%28n%29+%5Cend%7Bcases%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless (1-\epsilon) \alpha_c(n)\\ 0 &amp; \text{ if } \alpha \textgreater (1+\epsilon) \alpha_c(n) \end{cases}." class="latex" title="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless (1-\epsilon) \alpha_c(n)\\ 0 &amp; \text{ if } \alpha \textgreater (1+\epsilon) \alpha_c(n) \end{cases}." /></p>
<p> </p>
<p>However, this theorem allows for the possibility that the threshold depends on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />. From a statistical physics standpoint, this would be ridiculous, as it suggests that the behavior of the system depends on the number of particles that participate in it. We state the commonly held stronger conjecture:</p>
<p><strong>Conjecture:</strong><br />
for all <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \ge 3" class="latex" title="k \ge 3" />, there exists a constant <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" /> depending only on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> such that if <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" /> is a random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT instance in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> variables and <img src="https://s0.wp.com/latex.php?latex=%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha n" class="latex" title="\alpha n" /> clauses, then</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D+%5CPr%5B%5Cphi+%5Ctext%7B+is+satisfiable%7D%5D+%3D+%5Cbegin%7Bcases%7D+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextless+%5Calpha_c+%5C%5C+1+%26+%5Ctext%7B+if+%7D+%5Calpha+%5Ctextgreater+%5Calpha_c+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless \alpha_c \\ 1 &amp; \text{ if } \alpha \textgreater \alpha_c \end{cases}" class="latex" title="\lim_{n \to \infty} \Pr[\phi \text{ is satisfiable}] = \begin{cases} 1 &amp; \text{ if } \alpha \textless \alpha_c \\ 1 &amp; \text{ if } \alpha \textgreater \alpha_c \end{cases}" /></p>
<p> </p>
<p>In 2015, Jian Ding, Allan Sly, and Nike Sun established the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT conjecture all <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> larger than some fixed constant <img src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k_0" class="latex" title="k_0" />, and we will be hearing about the proof from Nike later on in the course.</p>
<h3>Bounds on <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" /> via the method of moments</h3>
<p>Let us move to a simpler model of random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT formulas, which is a bit easier to work with and is a close approximation to our original model. Instead of sampling <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT clauses without replacement, we will sample them with replacement and also allow variables to appear multiple times in the same clause (so each literal is chosen uniformly at random). The independence of the clauses makes computations in this model simpler.</p>
<p>We’ll prove the following bounds. The upper bound is a fairly straightforward computation, and the lower bound is given by an elegant argument due to Achlioptas and Peres.</p>
<p><strong>Theorem:</strong><br />
For every <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \ge 3" class="latex" title="k \ge 3" />, <img src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Cln+2+-+O%28k%29+%5Cle+%5Calpha_c+%5Cle+2%5Ek+%5Cln+2+-+O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k \ln 2 - O(k) \le \alpha_c \le 2^k \ln 2 - O(1)" class="latex" title="2^k \ln 2 - O(k) \le \alpha_c \le 2^k \ln 2 - O(1)" />.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" /> be a random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT formula with <img src="https://s0.wp.com/latex.php?latex=m+%3D+%5Calpha+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m = \alpha n" class="latex" title="m = \alpha n" /> clauses, <img src="https://s0.wp.com/latex.php?latex=C_1%2C%5Cldots%2CC_m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_1,\ldots,C_m" class="latex" title="C_1,\ldots,C_m" />. For an assignment <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" />, let <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(x)" class="latex" title="\phi(x)" /> be the <img src="https://s0.wp.com/latex.php?latex=0%2F1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0/1" class="latex" title="0/1" /> indicator that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" />. Finally, let <img src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D+%3D+%5Csum_x+%5Cphi%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{\alpha} = \sum_x \phi(x)" class="latex" title="Z_{\alpha} = \sum_x \phi(x)" /> be the number of satisfying assignments of <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" />.</p>
<h4>Upper bound via the first moment method.</h4>
<p>We have by Markov’s inequality that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5BZ_%7B%5Calpha%7D+%5Cge+1%5D+%5Cle+%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D+%3D+%5Csum_%7Bx%7D+%5CPr%5B%5Cphi%28x%29%5D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[Z_{\alpha} \ge 1] \le \mathbb{E}[Z_{\alpha}] = \sum_{x} \Pr[\phi(x)]." class="latex" title="\Pr[Z_{\alpha} \ge 1] \le \mathbb{E}[Z_{\alpha}] = \sum_{x} \Pr[\phi(x)]." /></p>
<p>Fix an assignment <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />. Then by the independence of the clauses,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Cphi%28x%29%5D+%3D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5CPr%5BC_j%28x%29+%3D+1%5D+%3D+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\phi(x)] = \prod_{j \in [m]} \Pr[C_j(x) = 1] = \left(1 - \frac{1}{2^k}\right)^m," class="latex" title="\Pr[\phi(x)] = \prod_{j \in [m]} \Pr[C_j(x) = 1] = \left(1 - \frac{1}{2^k}\right)^m," /></p>
<p>since each clause has <img src="https://s0.wp.com/latex.php?latex=2%5Ek+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k -1" class="latex" title="2^k -1" /> satisfying assignments. Summing over all<br />
<img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D+%3D+2%5En+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E%7B%5Calpha+n%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z_{\alpha}] = 2^n \left(1 - \frac{1}{2^k}\right)^{\alpha n}." class="latex" title="\mathbb{E}[Z_{\alpha}] = 2^n \left(1 - \frac{1}{2^k}\right)^{\alpha n}." /></p>
<p>We can see that if <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+2%5Ek+%5Cln+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textgreater 2^k \ln 2" class="latex" title="\alpha \textgreater 2^k \ln 2" />, this quantity will go to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />. So we have:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha_c+%5Cle+2%5Ek+%5Cln+2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c \le 2^k \ln 2." class="latex" title="\alpha_c \le 2^k \ln 2." /></p>
<h4>Lower bound via the second moment method.</h4>
<p>To lower bound <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" />, we can use the <em>second moment method</em>. We’ll<br />
calculate the second moment of <img src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{\alpha}" class="latex" title="Z_{\alpha}" />. An easy use of Cauchy-Schwarz (for non-negative <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX%5D+%5Cle+%5Csqrt%7B%5Cmathbb%7BE%7D%5BX%5E2%5D%5Cmathbb%7BE%7D%5BI%5BX+%5Ctextgreater+0%5D%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[X] \le \sqrt{\mathbb{E}[X^2]\mathbb{E}[I[X \textgreater 0]]}" class="latex" title="\mathbb{E}[X] \le \sqrt{\mathbb{E}[X^2]\mathbb{E}[I[X \textgreater 0]]}" />) implies that if there is a constant <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c" class="latex" title="c" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D+Z_%7B%5Calpha%7D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7DZ_%7B%5Calpha%7D%5E2%7D+%5Cge+c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{n \to \infty}\frac{\left(\mathbb{E} Z_{\alpha}\right)^2}{\mathbb{E}Z_{\alpha}^2} \ge c" class="latex" title="\lim_{n \to \infty}\frac{\left(\mathbb{E} Z_{\alpha}\right)^2}{\mathbb{E}Z_{\alpha}^2} \ge c" />,</p>
<p>then with at least constant probability <img src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D+%5Cge+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{\alpha} \ge 1" class="latex" title="Z_{\alpha} \ge 1" />, and then Friedgut’s theorem implies that <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextless+%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textless \alpha_c" class="latex" title="\alpha \textless \alpha_c" />. From above we have an expression for the numerator, so we now set out to bound the second moment of <img src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{\alpha}" class="latex" title="Z_{\alpha}" />. We have that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Z_%7B%5Calpha%7D%5E2+%3D+%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28%5Csum_%7Bx%7DI%5B%5Cphi%28x%29%5D%5Cright%29%5E2%5Cright%5D+%3D+%5Csum_%7Bx%2Cy%7D+%5CPr%5Cleft%5B%5Cphi%28x%29%5Cwedge+%5Cphi%28y%29%5Cright%5D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E} Z_{\alpha}^2 = \mathbb{E}\left[\left(\sum_{x}I[\phi(x)]\right)^2\right] = \sum_{x,y} \Pr\left[\phi(x)\wedge \phi(y)\right]," class="latex" title="\mathbb{E} Z_{\alpha}^2 = \mathbb{E}\left[\left(\sum_{x}I[\phi(x)]\right)^2\right] = \sum_{x,y} \Pr\left[\phi(x)\wedge \phi(y)\right]," /></p>
<p>and by the independence of the clauses,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5Cleft%5B%5Cphi%28x%29%5Cwedge+%5Cphi%28y%29%5Cright%5D+%3D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5CPr%5BC_j%28x%29+%3D+1+%5Cwedge+C_j%28y%29+%3D+1%5D+%3D+%5Cleft%28%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr\left[\phi(x)\wedge \phi(y)\right] = \prod_{j \in [m]} \Pr[C_j(x) = 1 \wedge C_j(y) = 1] = \left(\Pr[C(x) = 1 \wedge C(y) = 1]\right)^m," class="latex" title="\Pr\left[\phi(x)\wedge \phi(y)\right] = \prod_{j \in [m]} \Pr[C_j(x) = 1 \wedge C_j(y) = 1] = \left(\Pr[C(x) = 1 \wedge C(y) = 1]\right)^m," /></p>
<p>for a single random <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT clause <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />. But, for <img src="https://s0.wp.com/latex.php?latex=x%2Cy+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y \in \{\pm 1\}^n" class="latex" title="x,y \in \{\pm 1\}^n" />, the events <img src="https://s0.wp.com/latex.php?latex=C%28x%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(x) = 1" class="latex" title="C(x) = 1" /> and <img src="https://s0.wp.com/latex.php?latex=C%28y%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(y)=1" class="latex" title="C(y)=1" /> are not independent. This is easier to see if we apply inclusion-exclusion,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D+%3D+1+-+%5CPr%5BC%28x%29+%3D+0%5D+-+%5CPr%5BC%28y%29+%3D+0%5D+%2B+%5CPr%5BC%28x%29+%3D+C%28y%29+%3D+0%5D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - \Pr[C(x) = 0] - \Pr[C(y) = 0] + \Pr[C(x) = C(y) = 0]," class="latex" title="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - \Pr[C(x) = 0] - \Pr[C(y) = 0] + \Pr[C(x) = C(y) = 0]," /></p>
<p>The event that both <img src="https://s0.wp.com/latex.php?latex=C%28x%29+%3D+C%28y%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(x) = C(y) = 0" class="latex" title="C(x) = C(y) = 0" /> when <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> is a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT clause occurs only when <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> agree exactly on the assignments to the variables of <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />, since otherwise at least one of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> or <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> must be satisfied (because each literal in <img src="https://s0.wp.com/latex.php?latex=C%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(x)" class="latex" title="C(x)" /> is negated for <img src="https://s0.wp.com/latex.php?latex=C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(y)" class="latex" title="C(y)" />). Thus, this probability depends on the Hamming distance between <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/agreement.png?w=300" alt="agreement" class="alignnone wp-image-6205" width="300" /></p>
<p>For <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+%281-%5Cdelta%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}\|x-y\|_1 = (1-\delta)n" class="latex" title="\frac{1}{2}\|x-y\|_1 = (1-\delta)n" />, the probability that <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />‘s variables are all in the subset on which <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> agree is <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta^k" class="latex" title="\delta^k" /> (up to lower order terms). So, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+C%28y%29+%3D+0%5D+%3D+%5Cdelta%5Ek+%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[C(x) = C(y) = 0] = \delta^k \cdot \frac{1}{2^k}," class="latex" title="\Pr[C(x) = C(y) = 0] = \delta^k \cdot \frac{1}{2^k}," /></p>
<p>and then for <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+n+-+%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}\|x-y\|_1 = n - \ell" class="latex" title="\frac{1}{2}\|x-y\|_1 = n - \ell" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5BC%28x%29+%3D+1+%5Cwedge+C%28y%29+%3D+1%5D+%3D+1+-+2%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - 2\cdot \frac{1}{2^k} + \left(\frac{\ell}{2n}\right)^k." class="latex" title="\Pr[C(x) = 1 \wedge C(y) = 1] = 1 - 2\cdot \frac{1}{2^k} + \left(\frac{\ell}{2n}\right)^k." /></p>
<p>Because there are <img src="https://s0.wp.com/latex.php?latex=2%5En+%5Ccdot+%5Cbinom%7Bn%7D%7B%5Cell%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n \cdot \binom{n}{\ell}" class="latex" title="2^n \cdot \binom{n}{\ell}" /> pairs <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> at distance <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+n+-+%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}\|x-y\|_1 = n - \ell" class="latex" title="\frac{1}{2}\|x-y\|_1 = n - \ell" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5BZ_%7B%5Calpha%7D%5E2%5Cright%5D+%3D+%5Csum_%7B%5Cell+%3D+0%7D%5E%7Bn%7D+2%5En+%5Ccdot+%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Ccdot+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29%5Em%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}\left[Z_{\alpha}^2\right] = \sum_{\ell = 0}^{n} 2^n \cdot \binom{n}{\ell} \cdot \left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right)^m," class="latex" title="\mathbb{E}\left[Z_{\alpha}^2\right] = \sum_{\ell = 0}^{n} 2^n \cdot \binom{n}{\ell} \cdot \left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right)^m," /></p>
<p>and using the Stirling bound <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Cle+O%28%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%29%5Cexp%28n+%5Ccdot+H%28%5Cell%2Fn%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\binom{n}{\ell} \le O(\frac{1}{\sqrt{n}})\exp(n \cdot H(\ell/n))" class="latex" title="\binom{n}{\ell} \le O(\frac{1}{\sqrt{n}})\exp(n \cdot H(\ell/n))" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%3D+%5Csum_%7B%5Cell+%3D+0%7D%5E%7Bn%7DO%28%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%29+%5Cexp%5Cleft%28+%5Cleft%28H%5Cleft%28%5Cfrac%7B%5Cell%7D%7Bn%7D%5Cright%29+%2B+%5Cln+2%5Cright%29%5Ccdot+n+%2B+%5Cln%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29+%5Ccdot+%5Calpha+n%5Cright%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="= \sum_{\ell = 0}^{n}O(\frac{1}{\sqrt{n}}) \exp\left( \left(H\left(\frac{\ell}{n}\right) + \ln 2\right)\cdot n + \ln\left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right) \cdot \alpha n\right)." class="latex" title="= \sum_{\ell = 0}^{n}O(\frac{1}{\sqrt{n}}) \exp\left( \left(H\left(\frac{\ell}{n}\right) + \ln 2\right)\cdot n + \ln\left(1 - \frac{1}{2^{k-1}} + \left(\frac{\ell}{2n}\right)^k\right) \cdot \alpha n\right)." /></p>
<p>Using Laplace’s method, we can show that this sum will be dominated by the <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\sqrt{n})" class="latex" title="O(\sqrt{n})" /> terms around the maximizing summand; so defining <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cdelta%29+%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha%5Cln+%281+-+2%5E%7B-k%2B1%7D+%2B+%5Cdelta%5Ek+2%5E%7B-k%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi(\delta) = H(\delta) + \ln 2 + \alpha\ln (1 - 2^{-k+1} + \delta^k 2^{-k}))" class="latex" title="\psi(\delta) = H(\delta) + \ln 2 + \alpha\ln (1 - 2^{-k+1} + \delta^k 2^{-k}))" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5E2%5D+%3D+O%281%29+%5Ccdot+%5Cexp%28+n+%5Ccdot+%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi%28%5Cdelta%29%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z_{\alpha}^2] = O(1) \cdot \exp( n \cdot \max_{\delta \in [0,1]} \psi(\delta))." class="latex" title="\mathbb{E}[Z_{\alpha}^2] = O(1) \cdot \exp( n \cdot \max_{\delta \in [0,1]} \psi(\delta))." /></p>
<p>If we want that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cle+c+%5Ccdot+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z^2] \le c \cdot \mathbb{E}[Z]^2" class="latex" title="\mathbb{E}[Z^2] \le c \cdot \mathbb{E}[Z]^2" />, then we require that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi%28%5Cdelta%29+%5Cle+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Cmathbb%7BE%7D%5BZ%5D%5E2%29+%3D+2%5Cln+2+%2B+2%5Calpha+%5Cln+%281-2%5E%7B-k%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\max_{\delta \in [0,1]} \psi(\delta) \le \frac{1}{n}\ln(\mathbb{E}[Z]^2) = 2\ln 2 + 2\alpha \ln (1-2^{-k})." class="latex" title="\max_{\delta \in [0,1]} \psi(\delta) \le \frac{1}{n}\ln(\mathbb{E}[Z]^2) = 2\ln 2 + 2\alpha \ln (1-2^{-k})." /></p>
<p>However, we can see (using calculus) that this inequality does not hold whenever <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textgreater 0" class="latex" title="\alpha \textgreater 0" />. In the plot below one can also see this, though the values are very close when <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> is close to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />:</p>
<p> </p>

<a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/sat-second-moment-3/"><img src="https://windowsontheory.files.wordpress.com/2018/09/sat-second-moment2.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>
<a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/sat-second-moment-zoom-2/"><img src="https://windowsontheory.files.wordpress.com/2018/09/sat-second-moment-zoom1.png?w=150&amp;h=100" alt="" height="100" class="attachment-thumbnail size-thumbnail" width="150" /></a>

<p>So the naive second moment method fails to establish any lower bound on <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" />. The problem here is that the second moment is dominated by the large correlations of close strings; whenever <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textgreater 0" class="latex" title="\alpha \textgreater 0" />, the sum is dominated by pairs of strings <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> which are closer than <img src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n/2" class="latex" title="n/2" /> in Hamming distance, which is atypical. For strings at Hamming distance <img src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n/2" class="latex" title="n/2" />, what is relevant is <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cfrac%7B1%7D%7B2%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi(\frac{1}{2})" class="latex" title="\psi(\frac{1}{2})" />, which is always</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+H%28%5Cfrac%7B1%7D%7B2%7D%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5E%7Bk-1%7D%7D+%2B+%5Cfrac%7B1%7D%7B2%5E%7B2k%7D%7D%5Cright%29+%3D+2%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E%7B2%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi(\frac{1}{2}) = H(\frac{1}{2}) + \ln 2 + \alpha \ln \left(1 - \frac{1}{2^{k-1}} + \frac{1}{2^{2k}}\right) = 2\ln 2 + \alpha \ln \left(1 - \frac{1}{2^k}\right)^{2}," class="latex" title="\psi(\frac{1}{2}) = H(\frac{1}{2}) + \ln 2 + \alpha \ln \left(1 - \frac{1}{2^{k-1}} + \frac{1}{2^{2k}}\right) = 2\ln 2 + \alpha \ln \left(1 - \frac{1}{2^k}\right)^{2}," /></p>
<p>which is equal to <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D%5Cln%5Cmathbb%7BE%7D%5BZ_%7B%5Calpha%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{n}\ln\mathbb{E}[Z_{\alpha}]^2" class="latex" title="\frac{1}{n}\ln\mathbb{E}[Z_{\alpha}]^2" />. At distance <img src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n/2" class="latex" title="n/2" />, the value of <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" /> on pairs <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> is essentially uncorrelated (one can think of drawing a uniformly random <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> given <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />), so such pairs are representative of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z]^2" class="latex" title="\mathbb{E}[Z]^2" />.</p>
<h4>A more delicate approach.</h4>
<p>To get a good bound, we have to perform a fancier second moment method calculation, due to Achlioptas and Peres. We will re-weight the terms so that more typical pairs, at distance <img src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n/2" class="latex" title="n/2" />, are dominant. Rather than computing <img src="https://s0.wp.com/latex.php?latex=Z_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z_{\alpha}" class="latex" title="Z_{\alpha}" />, we compute <img src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_{\alpha}" class="latex" title="X_{\alpha}" />, where</p>
<p><img src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D+%3D+%5Csum_%7Bx%7D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+w_j%28x%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_{\alpha} = \sum_{x} \prod_{j \in [m]} w_j(x)," class="latex" title="X_{\alpha} = \sum_{x} \prod_{j \in [m]} w_j(x)," /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j(x) = 0" class="latex" title="w_j(x) = 0" /> if <img src="https://s0.wp.com/latex.php?latex=C_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_j(x) = 0" class="latex" title="C_j(x) = 0" />, and <img src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+%5Ceta%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j(x) = \eta^t" class="latex" title="w_j(x) = \eta^t" /> if <img src="https://s0.wp.com/latex.php?latex=C_j%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_j(x)" class="latex" title="C_j(x)" /> is satisfied by <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> variables. Since <img src="https://s0.wp.com/latex.php?latex=X_%7B%5Calpha%7D+%5Ctextgreater+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_{\alpha} \textgreater 0" class="latex" title="X_{\alpha} \textgreater 0" /> only if <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" /> has satisfying assignments, the goal is to still bound <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5E2%5D+%5Cle+c%5Ccdot+%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[X_{\alpha}^2] \le c\cdot \mathbb{E}[X_{\alpha}]^2" class="latex" title="\mathbb{E}[X_{\alpha}^2] \le c\cdot \mathbb{E}[X_{\alpha}]^2" />. Again using the independence of the clauses, we have that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5D+%3D+2%5En+%5Ccdot+%5Cmathbb%7BE%7D%5Bw_j%28x%29%5D%5Em+%3D+2%5En+%5Cleft%282%5E%7B-k%7D%5Csum_%7Bt+%3D+1%7D%5Ek+%5Cbinom%7Bk%7D%7Bt%7D+%5Ceta%5Et+%5Cright%29%5Em+%3D+2%5En+%5Cleft%28%5Cleft%28%5Cfrac%7B1%2B%5Ceta%7D%7B2%7D%5Cright%29%5Ek-2%5E%7B-k%7D%5Cright%29%5Em.%5Clabel%7Beq%3Aexpect%7D%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \mathbb{E}[X_{\alpha}] = 2^n \cdot \mathbb{E}[w_j(x)]^m = 2^n \left(2^{-k}\sum_{t = 1}^k \binom{k}{t} \eta^t \right)^m = 2^n \left(\left(\frac{1+\eta}{2}\right)^k-2^{-k}\right)^m.\label{eq:expect}\end{aligned}" class="latex" title="\begin{aligned} \mathbb{E}[X_{\alpha}] = 2^n \cdot \mathbb{E}[w_j(x)]^m = 2^n \left(2^{-k}\sum_{t = 1}^k \binom{k}{t} \eta^t \right)^m = 2^n \left(\left(\frac{1+\eta}{2}\right)^k-2^{-k}\right)^m.\label{eq:expect}\end{aligned}" /></p>
<p>And calculating again the second moment,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX_%7B%5Calpha%7D%5E2%5D+%3D+%5Csum_%7Bx%2Cy%7D+%5Cprod_%7Bj+%5Cin+%5Bm%5D%7D+%5Cmathbb%7BE%7D%5Bw_j%28x%29%5Ccdot+w_j%28y%29%5D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[X_{\alpha}^2] = \sum_{x,y} \prod_{j \in [m]} \mathbb{E}[w_j(x)\cdot w_j(y)]." class="latex" title="\mathbb{E}[X_{\alpha}^2] = \sum_{x,y} \prod_{j \in [m]} \mathbb{E}[w_j(x)\cdot w_j(y)]." /></p>
<p>For a fixed clause <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />, we can partition the variables in its scope into 4 sets, according to whether <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> agree on the variable, and whether the variable does or does not satisfy the literals of the clause. Suppose that <img src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j" class="latex" title="w_j" /> has <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> variables on which <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> agree, and that of these <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> are satisfying for <img src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j" class="latex" title="w_j" /> and <img src="https://s0.wp.com/latex.php?latex=a+-+t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a - t" class="latex" title="a - t" /> are not.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/agreement-numbers.png?w=300" alt="agreement-numbers" class="alignnone wp-image-6206" width="300" /></p>
<p>Then, <img src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j" class="latex" title="w_j" /> has <img src="https://s0.wp.com/latex.php?latex=k-a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k-a" class="latex" title="k-a" /> variables on which <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> disagree, and any variable which does not satisfy <img src="https://s0.wp.com/latex.php?latex=w_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j" class="latex" title="w_j" /> for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> must satisfy it for <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />. For a <img src="https://s0.wp.com/latex.php?latex=w_j%28x%29+w_j%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j(x) w_j(y)" class="latex" title="w_j(x) w_j(y)" /> to be nonzero, either there must be at least one literal in the variables on which <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> agree which agrees with <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" />, or otherwise there must be at least one literal in the variables on which <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> disagree which agrees with each string. Therefore, if <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> agree on a <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta" class="latex" title="\delta" />-fraction of variables,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb%7BE%7D%5Bw_j%28x%29+%5Ccdot+w_j%28y%29%5D+%26%3D+%5Csum_%7Ba+%3D+1%7D%5Ek+%5Csum_%7Bt%3D1%7D%5Ea+%5Ceta%5E%7B2t+%2B+k+-+a%7D+%5Ccdot+%5CPr%5Bw_j+%5Ctext%7B+has+%7D+a+%5Ctext%7B+variables+in+%7D+x+%5Ccap+y%2C%5C+t+%5Cin+x+%5Ccap+y+%5Ctext%7B+satisfy+%7Dw%2C+%5C+w_j%28x%29%2Cw_j%28y%29+%5Ctextgreater+0%5D%5C%5C+%26%3D+%5Cleft%28%5Csum_%7Ba+%3D+0%7D%5Ek%5Csum_%7Bt%3D0%7D%5Ea+%5Ceta%5E%7Bk+-+a+%2B+2t%7D+%5Ccdot+%5Cbinom%7Bk%7D%7Ba%7D%5Cdelta%5E%7Ba%7D%5Cleft%281+-+%5Cdelta%5Cright%29%5E%7Bk-a%7D+2%5E%7B-a%7D+%5Cbinom%7Ba%7D%7Bt%7D%5Cright%29+-+%5Cleft%28%5Csum_%7Ba%3D0%7D%5Ek+%5Ceta%5E%7Bk-a%7D+%5Cbinom%7Bk%7D%7Ba%7D+%5Cdelta%5Ea%281-%5Cdelta%29%5E%7Bk-a%7D+2%5E%7B-k%2B1%7D%5Cright%29+%2B+%5Cdelta%5Ek2%5E%7B-k%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \mathbb{E}[w_j(x) \cdot w_j(y)] &amp;= \sum_{a = 1}^k \sum_{t=1}^a \eta^{2t + k - a} \cdot \Pr[w_j \text{ has } a \text{ variables in } x \cap y,\ t \in x \cap y \text{ satisfy }w, \ w_j(x),w_j(y) \textgreater 0]\\ &amp;= \left(\sum_{a = 0}^k\sum_{t=0}^a \eta^{k - a + 2t} \cdot \binom{k}{a}\delta^{a}\left(1 - \delta\right)^{k-a} 2^{-a} \binom{a}{t}\right) - \left(\sum_{a=0}^k \eta^{k-a} \binom{k}{a} \delta^a(1-\delta)^{k-a} 2^{-k+1}\right) + \delta^k2^{-k},\end{aligned}" class="latex" title="\begin{aligned} \mathbb{E}[w_j(x) \cdot w_j(y)] &amp;= \sum_{a = 1}^k \sum_{t=1}^a \eta^{2t + k - a} \cdot \Pr[w_j \text{ has } a \text{ variables in } x \cap y,\ t \in x \cap y \text{ satisfy }w, \ w_j(x),w_j(y) \textgreater 0]\\ &amp;= \left(\sum_{a = 0}^k\sum_{t=0}^a \eta^{k - a + 2t} \cdot \binom{k}{a}\delta^{a}\left(1 - \delta\right)^{k-a} 2^{-a} \binom{a}{t}\right) - \left(\sum_{a=0}^k \eta^{k-a} \binom{k}{a} \delta^a(1-\delta)^{k-a} 2^{-k+1}\right) + \delta^k2^{-k},\end{aligned}" /></p>
<p>where the first sum ignores the cases when <img src="https://s0.wp.com/latex.php?latex=w_j%28x%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j(x) = 0" class="latex" title="w_j(x) = 0" /> or <img src="https://s0.wp.com/latex.php?latex=w_j%28y%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_j(y) = 0" class="latex" title="w_j(y) = 0" />, the second sum subtracts for the cases when <img src="https://s0.wp.com/latex.php?latex=t%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=0" class="latex" title="t=0" /> the contribution of the terms where all the literals agree with either <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> or <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />, and the final term accounts for the fact that the term <img src="https://s0.wp.com/latex.php?latex=a+%3D+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a = k" class="latex" title="a = k" /> is subtracted<br />
twice. Simplifying,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bw_j%28x%29+%5Ccdot+w_j%28y%29%5D+%3D+%5Cleft%28%5Cfrac%7B%281%2B%5Ceta%5E2%29%7D%7B2%7D%5Cdelta+%2B+%5Ceta%281-%5Cdelta%29%5Cright%29%5Ek+-+2%5E%7B1-k%7D+%5Cleft%28%5Ceta%281-%5Cdelta%29+%2B+%5Cdelta%5Cright%29%5Ek+%2B+%5Cleft%28%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cright%29%5Ek.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[w_j(x) \cdot w_j(y)] = \left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k." class="latex" title="\mathbb{E}[w_j(x) \cdot w_j(y)] = \left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k." /></p>
<p>Define<br />
<img src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%28%5Cdelta%29+%3D%5Cleft%28%5Cfrac%7B%281%2B%5Ceta%5E2%29%7D%7B2%7D%5Cdelta+%2B+%5Ceta%281-%5Cdelta%29%5Cright%29%5Ek+-+2%5E%7B1-k%7D+%5Cleft%28%5Ceta%281-%5Cdelta%29+%2B+%5Cdelta%5Cright%29%5Ek+%2B+%5Cleft%28%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cright%29%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q_{\eta}(\delta) =\left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k" class="latex" title="q_{\eta}(\delta) =\left(\frac{(1+\eta^2)}{2}\delta + \eta(1-\delta)\right)^k - 2^{1-k} \left(\eta(1-\delta) + \delta\right)^k + \left(\frac{\delta}{2}\right)^k" />.<br />
So we have (using Laplace’s method again)</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BX%5E2%5D+%3D+2%5En+%5Csum_%7B%5Cell+%3D+0%7D%5En+%5Cbinom%7Bn%7D%7B%5Cell%7D+q_%5Ceta%5Cleft%28%5Cfrac%7B%5Cell%7D%7Bn%7D%5Cright%29%5E%7B%5Calpha+n%7D+%5Capprox+O%281%29+%5Ccdot+%5Cexp%28%5Cmax_%7B%5Cdelta+%5Cin+%5B0%2C1%5D%7D+%5Cpsi_%5Ceta+%28%5Cdelta%29+%5Ccdot+n%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[X^2] = 2^n \sum_{\ell = 0}^n \binom{n}{\ell} q_\eta\left(\frac{\ell}{n}\right)^{\alpha n} \approx O(1) \cdot \exp(\max_{\delta \in [0,1]} \psi_\eta (\delta) \cdot n)," class="latex" title="\mathbb{E}[X^2] = 2^n \sum_{\ell = 0}^n \binom{n}{\ell} q_\eta\left(\frac{\ell}{n}\right)^{\alpha n} \approx O(1) \cdot \exp(\max_{\delta \in [0,1]} \psi_\eta (\delta) \cdot n)," /></p>
<p>For<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Ceta%28%5Cdelta%29+%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+q_%7B%5Ceta%7D%28%5Cdelta%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_\eta(\delta) = H(\delta) + \ln 2 + \alpha \ln q_{\eta}(\delta)." class="latex" title="\psi_\eta(\delta) = H(\delta) + \ln 2 + \alpha \ln q_{\eta}(\delta)." /></p>
<p>When <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = \frac{1}{2}" class="latex" title="\delta = \frac{1}{2}" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cright%29+%3D+%5Cleft%28%5Cleft%28%5Cfrac%7B1%2B%5Ceta%7D%7B2%7D%5Cright%29%5Ek+-+%5Cfrac%7B1%7D%7B2%5Ek%7D%5Cright%29%5E2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q_{\eta}\left(\frac{1}{2}\right) = \left(\left(\frac{1+\eta}{2}\right)^k - \frac{1}{2^k}\right)^2," class="latex" title="q_{\eta}\left(\frac{1}{2}\right) = \left(\left(\frac{1+\eta}{2}\right)^k - \frac{1}{2^k}\right)^2," /></p>
<p>which is equal to the log of the square of the expectation, so again for the second moment method to succeed, <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = \frac{1}{2}" class="latex" title="\delta = \frac{1}{2}" /> must be the global maximum.</p>
<p>Guided by this consideration, we can set <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\eta" class="latex" title="\eta" /> so that the derivative <img src="https://s0.wp.com/latex.php?latex=q_%7B%5Ceta%7D%27%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q_{\eta}'(\frac{1}{2}) = 0" class="latex" title="q_{\eta}'(\frac{1}{2}) = 0" />, so that <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_\eta" class="latex" title="\psi_\eta" /> achieves a local maximum at <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = \frac{1}{2}" class="latex" title="\delta = \frac{1}{2}" />. The choice for this (after doing some calculus) turns out to be the positive, real solution to the equation <img src="https://s0.wp.com/latex.php?latex=%281%2B%5Ceta%29%5E%7Bk-1%7D%281-%5Ceta%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1+\eta)^{k-1}(1-\eta) = 1" class="latex" title="(1+\eta)^{k-1}(1-\eta) = 1" />. With this choice, one can show that the global maximum is indeed achieved at <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = \frac{1}{2}" class="latex" title="\delta = \frac{1}{2}" /> as long as <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextless+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textless 2^k \ln 2 - O(k)" class="latex" title="\alpha \textless 2^k \ln 2 - O(k)" />. Below, we plot <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_{\eta}" class="latex" title="\psi_{\eta}" /> with this optimal choice of <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\eta" class="latex" title="\eta" /> for several values of <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> at <img src="https://s0.wp.com/latex.php?latex=k+%3D+5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 5" class="latex" title="k = 5" />:</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/fancy-second-moment.png?w=600" alt="fancy-second-moment" class="alignnone size-full wp-image-6211" /></p>
<p>So we have bounded <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c+%5Cge+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c \ge 2^k \ln 2 - O(k)" class="latex" title="\alpha_c \ge 2^k \ln 2 - O(k)" />.</p>
<h3>Another phase transition</h3>
<p>What is the correct answer for <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" />? We now have it in a window of size <img src="https://s0.wp.com/latex.php?latex=O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(k)" class="latex" title="O(k)" />. Experimental data and heuristic predictions indicate that it is closer to <img src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Cln+2+-+O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k \ln 2 - O(1)" class="latex" title="2^k \ln 2 - O(1)" /> (and in fact for large <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />, Ding, Sly and Sun showed that <img src="https://s0.wp.com/latex.php?latex=%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_c" class="latex" title="\alpha_c" /> is a specific constant in the interval <img src="https://s0.wp.com/latex.php?latex=%5B2%5Ek+%5Cln+2+-2%2C+2%5Ek+%5Cln+2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[2^k \ln 2 -2, 2^k \ln 2]" class="latex" title="[2^k \ln 2 -2, 2^k \ln 2]" />). So why can’t we push the second moment method further?</p>
<p>It turns out that there is a good reason for this, having to do with another phase transition. In fact, we know that <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT has not only satisfiable and unsatisfiable phases, but also <em>clustering</em> and <em>condensation</em> phases:</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/ksat-phases.png?w=600" alt="ksat-phases" class="alignnone size-full wp-image-6212" /></p>
<p>In the clustering phase, there are exponentially many clusters of solutions, each containing exponentially many solutions, and each at hamming distance <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega(n)" class="latex" title="\Omega(n)" /> from each other. In the condensation phase, there are even fewer clusters, but solutions still exist. We can see evidence of this already in the way that the second moment method failed us. When <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Ctextgreater+2%5Ek+%5Cln+2+-+O%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \textgreater 2^k \ln 2 - O(k)" class="latex" title="\alpha \textgreater 2^k \ln 2 - O(k)" />, we see that the global maximum of the exponent is actually attained close to <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = 1" class="latex" title="\delta = 1" />. This is because solutions with large overlap come to dominate the set of satisfying assignments.</p>
<p>One can also establish the existence of clusters using the method of moments. The trick is to compute the probability that two solutions, <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> with overlap <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta n" class="latex" title="\delta n" />, are both satisfying for <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi" class="latex" title="\phi" />. In fact, we have already done this. From above,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5Cleft%5B%5Cphi%28x%29+%5Cwedge+%5Cphi%28y%29+%7E%5Cmid%7E+%5Cfrac%7B1%7D%7B2%7D%5C%7Cx+-+y%5C%7C_1+%3D+%281-%5Cdelta%29n%5Cright%5D+%3D+%5Cleft%281+-+2%5Ccdot+%5Cfrac%7B1%7D%7B2%5Ek%7D+%2B+2%5E%7B-k%7D%5Cdelta%5Ek%5Cright%29%5E%7B%5Calpha+n%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr\left[\phi(x) \wedge \phi(y) ~\mid~ \frac{1}{2}\|x - y\|_1 = (1-\delta)n\right] = \left(1 - 2\cdot \frac{1}{2^k} + 2^{-k}\delta^k\right)^{\alpha n}." class="latex" title="\Pr\left[\phi(x) \wedge \phi(y) ~\mid~ \frac{1}{2}\|x - y\|_1 = (1-\delta)n\right] = \left(1 - 2\cdot \frac{1}{2^k} + 2^{-k}\delta^k\right)^{\alpha n}." /></p>
<p>Now, by a union bound, an upper bound on the probability that there exists a pair of solutions <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y" class="latex" title="x,y" /> with overlap <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta n" class="latex" title="\delta n" /> for any <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta \in [\delta_1,\delta_2]" class="latex" title="\delta \in [\delta_1,\delta_2]" /> is at most</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Cexists+x%2Cy+%5C+s.t.+%5C+%5Cphi%28x%29+%5Cwedge+%5Cphi%28y%29%2C+%5C+%5Cfrac%7B1%7D%7B2%7D%5C%7Cx-y%5C%7C_1+%3D+%281-%5Cdelta%29n+%5Ctext%7B+for+%7D+%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D%5D+%5Cle+%5Csum_%7B%5Cell+%3D+%5Cdelta_1+n%7D%5E%7B%5Cdelta_2+n%7D+%5Cbinom%7Bn%7D%7B%5Cell%7D+%5Cleft%281+-+2%5E%7B1-k%7D+%2B+%5Cleft%28%5Cfrac%7B%5Cell%7D%7B2n%7D%5Cright%29%5Ek%5Cright%29%5E%7B%5Calpha+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[\exists x,y \ s.t. \ \phi(x) \wedge \phi(y), \ \frac{1}{2}\|x-y\|_1 = (1-\delta)n \text{ for } \delta \in [\delta_1,\delta_2]] \le \sum_{\ell = \delta_1 n}^{\delta_2 n} \binom{n}{\ell} \left(1 - 2^{1-k} + \left(\frac{\ell}{2n}\right)^k\right)^{\alpha n}" class="latex" title="\Pr[\exists x,y \ s.t. \ \phi(x) \wedge \phi(y), \ \frac{1}{2}\|x-y\|_1 = (1-\delta)n \text{ for } \delta \in [\delta_1,\delta_2]] \le \sum_{\ell = \delta_1 n}^{\delta_2 n} \binom{n}{\ell} \left(1 - 2^{1-k} + \left(\frac{\ell}{2n}\right)^k\right)^{\alpha n}" /><br />
<img src="https://s0.wp.com/latex.php?latex=%5Cle+%5Cint_%7B%5Cdelta_1%7D%5E%7B%5Cdelta_2%7D+%5Cexp%5Cleft%28+%5Cleft%28H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha+%5Cln+%5Cleft%281-2%5E%7B1-k%7D+%2B+2%5E%7B-k%7D%5Cdelta%5Ek%5Cright%29%5Cright%29n%5Cright%29+d%5Cdelta%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\le \int_{\delta_1}^{\delta_2} \exp\left( \left(H(\delta) + \ln 2 + \alpha \ln \left(1-2^{1-k} + 2^{-k}\delta^k\right)\right)n\right) d\delta," class="latex" title="\le \int_{\delta_1}^{\delta_2} \exp\left( \left(H(\delta) + \ln 2 + \alpha \ln \left(1-2^{1-k} + 2^{-k}\delta^k\right)\right)n\right) d\delta," /></p>
<p>and if the function <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%28%5Cdelta%29+%3A%3D+H%28%5Cdelta%29+%2B+%5Cln+2+%2B+%5Calpha%5Cln+%281+%2B+2%5E%7B-k%7D%5Cdelta%5Ek+-+2%5E%7B1-k%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta(\delta) := H(\delta) + \ln 2 + \alpha\ln (1 + 2^{-k}\delta^k - 2^{1-k})" class="latex" title="\beta(\delta) := H(\delta) + \ln 2 + \alpha\ln (1 + 2^{-k}\delta^k - 2^{1-k})" /> is such that <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%28%5Cdelta%29+%5Ctextless+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta(\delta) \textless 0" class="latex" title="\beta(\delta) \textless 0" /> for all <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cin+%5B%5Cdelta_1%2C%5Cdelta_2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta \in [\delta_1,\delta_2]" class="latex" title="\delta \in [\delta_1,\delta_2]" />, then we conclude that the probability that there is a pair of satisfying assignments at distance between <img src="https://s0.wp.com/latex.php?latex=%281-%5Cdelta_2%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1-\delta_2)n" class="latex" title="(1-\delta_2)n" /> and <img src="https://s0.wp.com/latex.php?latex=%281-%5Cdelta_1%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1-\delta_1)n" class="latex" title="(1-\delta_1)n" /> is <img src="https://s0.wp.com/latex.php?latex=o%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="o(1)" class="latex" title="o(1)" />.</p>
<p>Achlioptas and Ricci-Tersenghi showed that for <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \ge 4" class="latex" title="k \ge 4" />, <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%281-%5Ceta%292%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha = (1-\eta)2^k" class="latex" title="\alpha = (1-\eta)2^k" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> a fixed constant, the above function is less than <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />. Rather than doing the tedious calculus, we can verify by plotting for <img src="https://s0.wp.com/latex.php?latex=k+%3D+8%2C+%5Calpha+%3D+169&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 8, \alpha = 169" class="latex" title="k = 8, \alpha = 169" /> (with <img src="https://s0.wp.com/latex.php?latex=169+%5Ctextless+%5Calpha_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="169 \textless \alpha_c" class="latex" title="169 \textless \alpha_c" />):</p>
<p> </p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/09/clusters.png?w=600" alt="clusters" class="alignnone size-full wp-image-6207" /></p>
<p>They also use the second moment method to show the clusters are non-empty, that there are exponentially many of them, each containing exponentially many solutions. This gives a proof of the existence of a clustering regime.</p>
<h3>Solution geometry and algorithms</h3>
<p>This study of the space of solutions is referred to as <em>solution geometry</em>, and understanding solution geometry turns out to be essential in proving better bounds on the critical density.</p>
<p>Solution geometry is also intimately related to the success of local search algorithms such as belief propagation, and to heuristics for predicting phase transitions such as replica symmetry breaking.</p>
<p>These topics and more to follow, in the coming weeks.</p>
<h2>Resources</h2>
<p>In preparing this blog post/lecture I leaned heavily on Chapters 2 and 10 of Marc Mézard and Andrea Montanari’s <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">“Information, Physics, and Computation”</a>, on Chapters 12,13,&amp;14 of Cris Moore and Stephan Mertens’ <a href="http://nature-of-computation.org/">“The nature of Computation,”</a> and on Dimitris Achlioptas and Federico Ricci-Tersenghi’s manuscript <a href="https://arxiv.org/abs/cs/0611052">“On the Solution-Space Geometry of Random CSPs”</a>. I also consulted Wikipedia for some physics basics.</p></div>







<p class="date">
by tselilschramm <a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/"><span class="datestr">at September 15, 2018 09:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/12/03/MityaNN2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/12/03/MityaNN2/">The search for biologically plausible neural computation&amp;#58; A similarity-based approach</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>This is the second post in a series reviewing recent progress in designing artificial neural networks (NNs) that resemble natural NNs not just superficially, but on a deeper, algorithmic level. In addition to serving as models of natural NNs, such networks can serve as general-purpose machine learning algorithms. Respecting biological constraints, viewed  naively as a handicap in developing competitive general-purpose machine learning algorithms, can instead facilitate the development of artificial NNs by restricting the search space of possible algorithms.</p>

<p>In <a href="http://www.offconvex.org/2016/11/03/MityaNN1/">the previous post</a>, we focused on the constraints that must be met for an unsupervised algorithm to be biologically plausible. For an algorithm to be implementable as a NN, it must be formulated in the online setting. In the corresponding NN, synaptic weight updates must be local, i.e. depend on the activity of only two neurons that the synapse connects. Then, we demonstrated that deriving NNs for dimensionality reduction in a conventional way - by minimizing the reconstruction error - results in multi-neuron networks with biologically implausible non-local learning rules.</p>

<p>In this post, we propose a different objective function which we term similarity matching. From this objective function, we derive an online algorithm implementable by a NN with local learning rules. Then, we introduce other similarity-based algorithms which include more biological features such as different neuron classes and nonlinear activation functions. Finally, we review similarity-matching algorithms with state-of-the-art performance.</p>

<h2 id="similarity-matching-objective-function">Similarity-matching objective function</h2>

<p>We start by stating an objective function that will be used to derive NNs for linear dimensionality reduction. Let ${\bf x}_t\in \mathbb{R}^n$, $t = 1,\ldots T$, be a set of data points (inputs) and ${\bf y}_t\in \mathbb{R}^k$, $t = 1,\ldots T$, ($k &lt; n$) be their learned representation (outputs). The similarity of a pair of inputs, ${\bf x}$<sub><small>$t$</small></sub> and ${\bf x}$<sub><small>$t’$</small></sub>, can be defined as their dot-product, ${\bf x}$<sub><small>$t$</small></sub>${}^\top {\bf x}$<sub><small>$t’$</small></sub>. Analogously, the similarity of a pair of outputs is ${\bf y}$<sub><small>$t$</small></sub>${}^\top {\bf y}$<sub><small>$t’$</small></sub>. Similarity matching, as its name suggests, learns a representation where the similarity between each pair of outputs matches that of the corresponding inputs:</p>



<p>This offline objective function, <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">previously employed for multidimensional scaling</a>, is optimized by the projections of inputs onto the principal subspace of their covariance, i.e. performing PCA up to an orthogonal rotation. Moreover, <a href="http://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum">(2.1) has no local minima other than the principal subspace solution</a>.</p>

<p>The similarity-matching objective (2.1) may seem like a strange choice for deriving an online algorithm implementable by a NN.  In the online setting, inputs are streamed to the algorithm sequentially and each output must be computed before the next input arrives. Yet, in (2.1), pairs of inputs and outputs from different time points interact with each other. In addition, whereas ${\bf x}_t$ and ${\bf y}_t$ could be interpreted as inputs and outputs to a network, unlike in <a href="http://www.offconvex.org/2016/11/03/MityaNN1/">the reconstruction approach (1.4)</a>, synaptic weights do not appear explicitly in (2.1).</p>

<h2 id="variable-substitution-trick">Variable substitution trick</h2>

<p>Both of the above concerns can be resolved by a <a href="https://www.researchgate.net/publication/315570715_Why_Do_Similarity_Matching_Objectives_Lead_to_HebbianAnti-Hebbian_Networks">simple math trick akin to completing the square</a>. We first focus on the cross-term in (2.1), which we call similarity alignment. By re-ordering the variables and introducing a new variable, ${\bf W} \in  \mathbb{R}^{k\times n}$, we obtain:</p>





<p>To prove the second identity, find optimal ${\bf W}$ by taking a derivative of the expression on the right with respect to ${\bf W}$ and setting it to zero, and then substitute the optimal ${\bf W}$ back into the expression. Similarly, for the quartic ${\bf y}_t$ term in (2.1):</p>





<p>By substituting (2.2) and (2.3) into (2.1) we get:</p>



<p>where</p>



<p>In the resulting objective function, (2.4),(2.5), optimal outputs at different time steps can be computed independently, making the problem amenable to an online algorithm. The price paid for this simplification is the appearance of the minimax optimization problem in variables, ${\bf W}$ and ${\bf M}$. Minimization over ${\bf W}$ aligns output channels with the greatest variance directions of the input and maximization over ${\bf M}$ diversifies the output channels. The competition between the two in a gradient descent/ascent algorithm results in the principal subspace projection which is <a href="https://www.researchgate.net/publication/315570715_Why_Do_Similarity_Matching_Objectives_Lead_to_HebbianAnti-Hebbian_Networks">the only stable fixed point of the corresponding dynamics</a>.</p>

<h2 id="online-algorithm-and-neural-network">Online algorithm and neural network</h2>

<p>Now, we are ready to derive an algorithm for optimizing  (2.1) online. First, by  minimizing (2.5) with respect to ${\bf y}_t$ while keeping ${\bf W}$ and ${\bf M}$ fixed we get the dynamics for the output variables :</p>



<p>To find ${\bf y}_t$ after the presentation of the corresponding input, ${\bf x}_t$, (2.6) is iterated until convergence.</p>

<p>After the convergence of ${\bf y}_t$ we update ${\bf W}$ and ${\bf M}$ by gradient descent of (2.2) and gradient ascent of (2.3) respectively:</p>



<p>Algorithm (2.6),(2.7), first derived <a href="https://www.researchgate.net/publication/273003026_A_HebbianAnti-Hebbian_Neural_Network_for_Linear_Subspace_Learning_A_Derivation_from_Multidimensional_Scaling_of_Streaming_Data">here</a>, can be naturally implemented by a biologically plausible NN, Figure 1. Here, activity (firing rate) of the upstream neurons corresponds to input variables. Output variables are computed by the dynamics of activity (2.6) in a single layer of neurons. Variables ${\bf W}$ and ${\bf M}$ are represented by the weights of synapses in feedforward and lateral connections respectively. The learning rules (2.7) are local, i.e. the weight update, $\Delta W_{ij}$, for the synapse between $j^{\rm th}$ input neuron and $i^{\rm th}$ output neuron depends only on the activities, $x_j$, of $j^{\rm th}$ input neuron and, $y_i$, of $i^{\rm th}$ output neuron, and the synaptic weight. In neuroscience, learning rules (2.7) for ${\bf W}$ and ${\bf M}$ are called Hebbian and anti-Hebbian respectively.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=18_7ApBztw00VwUn-Y81kOC4s8XTuXVVN" alt="" /></p>

<p><em>Figure 1: A Hebbian/Anti-Hebbian network derived from similarity matching.</em></p>

<p>To summarize, starting with the similarity-matching objective, we derived a Hebbian/anti-Hebbian NN for dimensionality reduction. The minimax objective  can be viewed as a zero-sum game played by the weights of feedforward and lateral connections. This demonstrates that synapses with local updates can still collectively work together to optimize a global objective. A similar, although not identical, NN was proposed by Foldiak heuristically. The advantage of our normative approach is that the offline solution is known. Although no proof of convergence exists in the online setting, algorithm (2.6),(2.7) performs well in practice.</p>

<h2 id="other-similarity-based-objectives-and-linear-networks">Other similarity-based objectives and linear networks</h2>

<p>We used the same framework to derive NNs for other computational tasks and incorporating more biological features. As the algorithm (2.6),(2.7) and the NN in Figure 1 were derived from the similarity-matching objective (2.1), they project data onto the principal subspace but do not necessarily recover principal components <em>per se</em>. To derive PCA algorithms we modified the objective function (2.1), <a href="https://arxiv.org/abs/1511.09468">here</a> and <a href="https://arxiv.org/abs/1810.06966">here</a>, to encourage orthogonality of ${\bf W}$. Such algorithms are implemented by NNs of the same architecture as in Figure 1 but with slightly different learning rules.</p>

<p>Although the similarity-matching NN in Figure 1 relies on biologically plausible local learning rules, it lacks biological realism in several other ways. For example, computing output requires recurrent activity that must settle faster than the time scale of the input variation, which is unlikely in biology. To respect this biological constraint, <a href="https://arxiv.org/abs/1810.06966">we modified</a> the dimensionality reduction algorithm to avoid recurrency.</p>

<p>Another non-biological feature of the NN in Figure 1 is that the output neurons compete with each other by communicating via lateral connections. In biology, such interactions are not direct but mediated by interneurons. To reflect these observations, we modified the objective function by introducing a whitening constraint:</p>



<p>where ${\bf I}$<sub><small>$k$</small></sub> is the $k$-by-$k$ identity matrix. Then, by representing the whitening constraint using Lagrange relaxation, <a href="http://papers.nips.cc/paper/5885-a-normative-theory-of-adaptive-dimensionality-reduction-in-neural-networks">we derived NNs</a> where interneurons appear naturally - their activity is modeled by the Lagrange multipliers, ${\bf z} _t^\top {\bf z} _{t’}$ (Figure 2):</p>



<p>Notice how (2.9) contains the ${\bf y}$-${\bf z}$ similarity-alignment term similar to (2.2). We can now derive learning rules for the ${\bf y}$-${\bf z}$ connections using the variable substitution trick, leading to the network in Figure 2. For details of this and other NN derivations, see <a href="http://papers.nips.cc/paper/5885-a-normative-theory-of-adaptive-dimensionality-reduction-in-neural-networks">here</a>.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1tYmjxDN2SUZY-8--uV_qGTb_gEIg0O8C" alt="" /></p>

<p><em>Figure 2: A biologically-plausible NN for whitening inputs, derived from a constrained similarity-alignment cost function.</em></p>

<h2 id="nonnegative-similarity-matching-objective-and-a-nonlinear-network">Nonnegative similarity-matching objective and a nonlinear network</h2>

<p>So far, we considered similarity-based NNs with linear neurons. However, biological neurons are not linear and many interesting computations require nonlinearity. A resolution to this discrepancy was suggested by the observation that the output of biological neurons is nonnegative (firing rate cannot be below zero). Hence, we modified the optimization problem by requiring that the output of the similarity-matching cost function (2.1) is nonnegative:</p>



<p>Solutions of the optimization problem (2.10) are very different from PCA: They can <a href="https://arxiv.org/abs/1503.00680">cluster well-segregated data and extract sparse features from data</a>. Understanding the nature of these solutions will be the topic of the next post. For now, we note that (2.10) can be solved by the same online algorithm as  (2.1) except that the output variables are projected onto the nonnegative domain. Such algorithm maps onto the same network as Figure 1 but with rectifying neurons (ReLUs), Figure 3A.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1QahRt7mzirInGnmuSJh-4R4b88q9gyRq" alt="" /></p>

<p><em>Figure 3: A) A nonlinear Hebbian/Anti-Hebbian network derived from nonnegative similarity matching. B) Stacked network for NICA. NSM - nonnegative similarity-matching.</em></p>

<p>Another problem solved by similarity-based networks is <a href="https://www.researchgate.net/publication/317300210_Blind_Nonnegative_Source_Separation_Using_Biological_Neural_Networks">the nonnegative independent component analysis</a> (NICA) which can be used for blind source separation. The problem is to recover independent and nonnegative sources from  observing only their linear mixture. <a href="https://www.researchgate.net/publication/3342750_Conditions_for_nonnegative_independent_component_analysis">Plumbley showed</a> that NICA can be solved in two steps, Figure 4. First, whiten the data to obtain an orthogonal rotation of the sources. Second, find an orthogonal rotation of the whitened sources that yields a nonnegative output, Figure 4.  The first step can be implemented by the whitening network in Figure 2. The second step can be implemented by the nonnegative similarity-matching network, Figure 3A, because an orthogonal rotation does not affect dot-product similarities. Therefore, <a href="https://www.researchgate.net/publication/317300210_Blind_Nonnegative_Source_Separation_Using_Biological_Neural_Networks">NICA is solved by stacking the whitening and the nonnegative similarity-matching networks</a>, Figure 3B.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=16uJTV1QQpwcZcueXLCqNJieKzyWk0ZYd" alt="" /></p>

<p><em>Figure 4: Illustration of the nonnegative independent component analysis algorithm. Two source channels (left) are linearly transformed to a two-dimensional mixture, which are the inputs to the algorithm (middle). Whitening (right) yields an orthogonal rotation of the sources. Sources are then recovered by solving the nonnegative similarity-matching problem. Green and red plus signs track two source vectors across mixing and whitening stages.</em></p>

<h2 id="similarity-based-algorithms-as-general-purpose-tools">Similarity-based algorithms as general-purpose tools</h2>

<p>While the derivation of the similarity-matching algorithm was motivated by constraints imposed by biology, the resulting algorithm performs well on large-scale data. <a href="https://arxiv.org/abs/1808.02083">A recent paper</a> introduced an efficient modification of the similarity-matching algorithm and demonstrated its competitiveness with the state-of-the-art principal subspace projection algorithms in both processing speed and convergence rate. A package with implementations of these algorithms is <a href="https://github.com/flatironinstitute/online_psp">here</a> and <a href="https://github.com/flatironinstitute/online_psp_matlab">here</a>.</p>

<p>In this blog post, we introduced linear and non-linear similarity-matching NNs that can serve as models of biological NNs and as general-purpose machine-learning tools. In the next post, we will discuss the nature of the solutions to nonnegative similarity-based networks.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/12/03/MityaNN2/"><span class="datestr">at December 03, 2018 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/11/07/optimization-beyond-landscape/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/11/07/optimization-beyond-landscape/">Understanding optimization in deep learning by analyzing trajectories of gradient descent</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>Neural network optimization is fundamentally non-convex, and yet simple gradient-based algorithms seem to consistently solve such problems.
This phenomenon is one of the central pillars of deep learning, and forms a mystery many of us theorists are trying to unravel. 
In this post I’ll survey some recent attempts to tackle this problem, finishing off with a discussion on my <a href="https://arxiv.org/pdf/1810.02281.pdf">new paper with Sanjeev Arora, Noah Golowich and Wei Hu</a>, which for the case of gradient descent over deep linear neural networks, provides a guarantee for convergence to global minimum at a linear rate.</p>

<h2 id="landscape-approach-and-its-limitations">Landscape Approach and Its Limitations</h2>

<p>Many papers on optimization in deep learning implicitly assume that a rigorous understanding will follow from establishing geometric properties of the loss <em>landscape</em>, and in particular, of <em>critical points</em> (points where the gradient vanishes).
For example, through an analogy with the spherical spin-glass model from condensed matter physics, <a href="http://proceedings.mlr.press/v38/choromanska15.pdf">Choromanska et al. 2015</a> argued for what has become a colloquial conjecture in deep learning:</p>

<blockquote>
  <p><strong>Landscape Conjecture:</strong>
In neural network optimization problems, suboptimal critical points are very likely to have negative eigenvalues to their Hessian. 
In other words, there are almost <em>no poor local minima</em>, and nearly all <em>saddle points are strict</em>.</p>
</blockquote>

<p>Strong forms of this conjecture were proven for loss landscapes of various simple problems involving <strong>shallow</strong> (two layer) models, e.g. <a href="https://papers.nips.cc/paper/6271-global-optimality-of-local-search-for-low-rank-matrix-recovery.pdf">matrix sensing</a>, <a href="https://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf">matrix completion</a>, <a href="http://proceedings.mlr.press/v40/Ge15.pdf">orthogonal tensor decomposition</a>, <a href="https://arxiv.org/pdf/1602.06664.pdf">phase retrieval</a>, and <a href="http://proceedings.mlr.press/v80/du18a/du18a.pdf">neural networks with quadratic activation</a>.
There was also work on establishing convergence of gradient descent to global minimum when the Landscape Conjecture holds, as described in the excellent posts on this blog by <a href="http://www.offconvex.org/2016/03/22/saddlepoints/">Rong Ge</a>, <a href="http://www.offconvex.org/2016/03/24/saddles-again/">Ben Recht</a> and <a href="http://www.offconvex.org/2017/07/19/saddle-efficiency/">Chi Jin and Michael Jordan</a>. 
They describe how gradient descent can arrive at a second order local minimum (critical point whose Hessian is positive semidefinite) by escaping all strict saddle points, and how this process is efficient given that perturbations are added to the algorithm. 
Note that under the Landscape Conjecture, i.e. when there are no poor local minima and non-strict saddles, second order local minima are also global minima.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/optimization-beyond-landscape-points.png" alt="Local minima and saddle points" width="100%" />
</p>

<p>However, it has become clear that the landscape approach (and the Landscape Conjecture) cannot be applied as is to <strong>deep</strong> (three or more layer) networks, for several reasons.
First, deep networks typically induce non-strict saddles (e.g. at the point where all weights are zero, see <a href="https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf">Kawaguchi 2016</a>).
Second, a landscape perspective largely ignores algorithmic aspects that empirically are known to greatly affect convergence with deep networks — for example the <a href="http://proceedings.mlr.press/v28/sutskever13.html">type of initialization</a>, or <a href="http://proceedings.mlr.press/v37/ioffe15.pdf">batch normalization</a>.
Finally, as I argued in my <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">previous blog post</a>, based upon <a href="http://proceedings.mlr.press/v80/arora18a/arora18a.pdf">work with Sanjeev Arora and Elad Hazan</a>, adding (redundant) linear layers to a classic linear model can sometimes accelerate gradient-based optimization, without any gain in expressiveness, and despite introducing non-convexity to a formerly convex problem. 
Any landscape analysis that relies on properties of critical points alone will have difficulty explaining this phenomenon, as through such lens, nothing is easier to optimize than a convex objective with a single critical point which is the global minimum.</p>

<h2 id="a-way-out">A Way Out?</h2>

<p>The limitations of the landscape approach for analyzing optimization in deep learning suggest that it may be abstracting away too many important details.
Perhaps a more relevant question than “is the landscape graceful?” is “what is the behavior of specific optimizer <strong>trajectories</strong> emanating from specific initializations?”.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/optimization-beyond-landscape-trajectories.png" alt="Different trajectories lead to qualitatively different results" width="66%" />
</p>

<p>While the trajectory-based approach is seemingly much more burdensome than landscape analyses, it is already leading to notable progress.
Several recent papers (e.g. <a href="http://proceedings.mlr.press/v70/brutzkus17a/brutzkus17a.pdf">Brutzkus and Globerson 2017</a>; <a href="https://papers.nips.cc/paper/6662-convergence-analysis-of-two-layer-neural-networks-with-relu-activation.pdf">Li and Yuan 2017</a>; <a href="http://proceedings.mlr.press/v70/zhong17a/zhong17a.pdf">Zhong et al. 2017</a>; <a href="http://proceedings.mlr.press/v70/tian17a/tian17a.pdf">Tian 2017</a>; <a href="https://openreview.net/pdf?id=rJ33wwxRb">Brutzkus et al. 2018</a>; <a href="http://proceedings.mlr.press/v75/li18a/li18a.pdf">Li et al. 2018</a>; <a href="https://arxiv.org/pdf/1806.00900.pdf">Du et al. 2018</a>; <a href="http://romaincouillet.hebfree.org/docs/conf/nips_GDD.pdf">Liao et al. 2018</a>) have adopted this strategy, successfully analyzing different types of shallow models.
Moreover, trajectory-based analyses are beginning to set foot beyond the realm of the landscape approach — for the case of linear neural networks, they have successfully established convergence of gradient descent to global minimum under <strong>arbitrary depth</strong>.</p>

<h2 id="trajectory-based-analyses-for-deep-linear-neural-networks">Trajectory-Based Analyses for Deep Linear Neural Networks</h2>

<p>Linear neural networks are fully-connected neural networks with linear (no) activation.
Specifically, a depth $N$ linear network with input dimension $d_0$, output dimension $d_N$, and hidden dimensions $d_1,d_2,\ldots,d_{N-1}$, is a linear mapping from $\mathbb{R}^{d_0}$ to $\mathbb{R}^{d_N}$ parameterized by $x \mapsto W_N W_{N-1} \cdots W_1 x$, where $W_j \in \mathbb{R}^{d_j \times d_{j-1}}$ is regarded as the weight matrix of layer $j$.
Though trivial from a representational perspective, linear neural networks are, somewhat surprisingly, complex in terms of optimization — they lead to non-convex training problems with multiple minima and saddle points.
Being viewed as a theoretical surrogate for optimization in deep learning, the application of gradient-based algorithms to linear neural networks is receiving significant attention these days.</p>

<p>To my knowledge, <a href="https://arxiv.org/pdf/1312.6120.pdf">Saxe et al. 2014</a> were the first to carry out a trajectory-based analysis for deep (three or more layer) linear networks, treating gradient flow (gradient descent with infinitesimally small learning rate) minimizing $\ell_2$ loss over whitened data.
Though a very significant contribution, this analysis did not formally establish convergence to global minimum, nor treat the aspect of computational complexity (number of iterations required to converge).
The recent work of <a href="http://proceedings.mlr.press/v80/bartlett18a.html">Bartlett et al. 2018</a> makes progress towards addressing these gaps, by applying a trajectory-based analysis to gradient descent for the special case of linear residual networks, i.e. linear networks with uniform width across all layers ($d_0=d_1=\cdots=d_N$) and identity initialization ($W_j=I$, $\forall j$).
Considering different data-label distributions (which boil down to what they refer to as “targets”), Bartlett et al. demonstrate cases where gradient descent provably converges to global minimum at a linear rate — loss is less than $\epsilon&gt;0$ from optimum after $\mathcal{O}(\log\frac{1}{\epsilon})$ iterations — as well as situations where it fails to converge.</p>

<p>In a <a href="https://arxiv.org/pdf/1810.02281.pdf">new paper with Sanjeev Arora, Noah Golowich and Wei Hu</a>, we take an additional step forward in virtue of the trajectory-based approach.
Specifically, we analyze trajectories of gradient descent for any linear neural network that does not include “bottleneck layers”, i.e. whose hidden dimensions are no smaller than the minimum between the input and output dimensions ($d_j \geq \min\{d_0,d_N\}$, $\forall j$), and prove convergence to global minimum, at a linear rate, provided that initialization meets the following two conditions:
<em>(i)</em> <em>approximate balancedness</em> — $W_{j+1}^\top W_{j+1} \approx W_j W_j^\top$, $\forall j$;
and <em>(ii)</em> <em>deficiency margin</em> — initial loss is smaller than the loss of any rank deficient solution.
We show that both conditions are necessary, in the sense that violating any one of them may lead to a trajectory that fails to converge.
Approximate balancedness at initialization is trivially met in the special case of linear residual networks, and also holds for the customary setting of initialization via small random perturbations centered at zero.
The latter also leads to deficiency margin with positive probability.
For the case $d_N=1$, i.e. scalar regression, we provide a random initialization scheme under which both conditions are met, and thus convergence to global minimum at linear rate takes place, with constant probability.</p>

<p>Key to our analysis is the observation that if weights are initialized to be approximately balanced, they will remain that way throughout the iterations of gradient descent.
In other words, trajectories taken by the optimizer adhere to a special characterization:</p>

<blockquote>
  <p><strong>Trajectory Characterization:</strong> <br /></p>

  <p>,</p>
</blockquote>

<p>which means that throughout the entire timeline, all layers have (approximately) the same set of singular values, and the left singular vectors of each layer (approximately) coincide with the right singular vectors of the layer that follows.
We show that this regularity implies steady progress for gradient descent, thereby demonstrating that even in cases where the loss landscape is complex as a whole (includes many non-strict saddle points), it may be particularly well-behaved around the specific trajectories taken by the optimizer.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Tackling the question of optimization in deep learning through the landscape approach, i.e. by analyzing the geometry of the objective independently of the algorithm used for training, is conceptually appealing.
However this strategy suffers from inherent limitations, predominantly as it requires the entire objective to be graceful, which seems to be too strict of a demand.
The alternative approach of taking into account the optimizer and its initialization, and focusing on the landscape only along the resulting trajectories, is gaining more and more traction.
While landscape analyses have thus far been limited to shallow (two layer) models only, the trajectory-based approach has recently treated arbitrarily deep models, proving convergence of gradient descent to global minimum at a linear rate.
Much work however remains to be done, as this success covered only linear neural networks.
I expect the trajectory-based approach to be key in developing our formal understanding of gradient-based optimization for deep non-linear networks as well.</p>

<p><a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/11/07/optimization-beyond-landscape/"><span class="datestr">at November 07, 2018 12:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/09/18/alacarte/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/09/18/alacarte/">Simple and efficient semantic embeddings for rare words, n-grams, and language features</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>Distributional methods for capturing meaning, such as word embeddings, often require observing many examples of words in context. But most humans can infer a reasonable meaning from very few or even a single occurrence. For instance,  if we read “Porgies live in shallow temperate marine waters,” we have a good idea that a <em>porgy</em> is a fish. Since language corpora often have a long tail of “rare words,” it is an interesting problem to imbue NLP algorithms with this capability. This is especially important for n-grams (i.e., ordered n-tuples of words, like “ice cream”), many of which occur rarely in the corpus.</p>

<p>Here we describe a simple but principled approach called <em>à la carte</em> embeddings, described in our <a href="http://aclweb.org/anthology/P18-1002">ACL’18 paper</a> with Yingyu Liang, Tengyu Ma, and Brandon Stewart. It also easily extends to learning embeddings of arbitrary language features such as word-senses and $n$-grams. The paper also combines these with our recent <a href="http://www.offconvex.org/2018/06/25/textembeddings/">deep-learning-free text embeddings</a> to get simple deep-learning free text embeddings with even better performance on downstream classification tasks, quite competitive with deep learning approaches.</p>

<h2 id="inducing-word-embedding-from-their-contexts-a-surprising-linear-relationship">Inducing word embedding from their contexts: a surprising linear relationship</h2>

<p>Suppose a single occurrence of a word $w$ is surrounded by a sequence $c$ of words. What is a reasonable guess for the word embedding $v_w$  of $w$? For convenience, we will let $u_w^c$ denote the  average of the word embeddings of words in $c$. Anybody who knows the word2vec method may reasonably guess the following.</p>

<blockquote>
  <p><strong>Guess 1:</strong> Up to scaling, $u_w^c$ is a good estimate for $v_w$.</p>
</blockquote>

<p>Unfortunately, this totally fails. Even taking thousands of occurrences of $w$, the average of such estimates stays far from the ground truth embedding $v_w$. The following discovery should therefore be surprising (read below for a theoretical justification):</p>

<blockquote>
  <p><strong>Theorem 1</strong> (From <a href="https://transacl.org/ojs/index.php/tacl/article/view/1346">this TACL’18 paper</a>): There is a single matrix $A$ (depending only upon the text corpus) such that $A u_w^c$ is a good estimate for $v_w$.</p>
</blockquote>

<p>Note that the best such $A$ can be found via linear regression by minimizing the average $|Au_w^c -v_w|^2$ over occurrences of frequent words $w$, for which we already have word embeddings.</p>

<p>Once such an $A$ has been learnt from frequent words, the induction of embeddings for new words works very well. As we receive more and more occurrences of $w$ the average of $Au_w^c$ over all sentences containing $w$ has cosine similarity $&gt;0.9$ with the true word embedding $v_w$ (this holds for GloVe as well as word2vec).</p>

<p>Thus the learnt $A$ gives a way to induce embeddings for new words from a few or even a single occurrence. We call this the   <em>à la carte</em> embedding of $w$,  because we don’t need to pay the <em>prix fixe</em> of re-running GloVe or word2vec on the entire corpus each time a new word is needed.</p>

<h3 id="testing-embeddings-for-rare-words">Testing embeddings for rare words</h3>
<p>Using Stanford’s <a href="https://nlp.stanford.edu/~lmthang/morphoNLM/">Rare Words</a> dataset we created the 
<a href="http://nlp.cs.princeton.edu/CRW/"><em>Contextual Rare Words</em></a> dataset where, along with word pairs and human-rated scores, we also provide contexts (i.e., few usages) for the rare words.</p>

<p>We compare the performance of our method with alternatives such as <a href="http://www.offconvex.org/2018/06/17/textembeddings/">top singular component removal and frequency down-weighting</a> and find that <em>à la carte</em> embedding consistently outperforms other methods and requires far fewer contexts to match their best performance.
Below we plot the increase in Spearman correlation with human ratings as the tested algorithms are given more samples of the words in context. We see that given only 8 occurences of the word, the <em>a la carte</em> method outperforms  other baselines that’re given 128 occurences.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/ALCcrwplot.svg" width="60%" />
</p>

<p>Now we turn to the task mentioned in the opening para of this post. <a href="http://aclweb.org/anthology/D17-1030">Herbelot and Baroni</a> constructed a “nonce” dataset consisting of single-word concepts and their Wikipedia definitions, to test algorithms that “simulate the process by which a competent speaker encounters a new word in known contexts.” They tested various methods, including a modified version of word2vec.
As we show in the table below, <em>à la carte</em> embedding outperforms all their methods in terms of the average rank of the target vector’s similarity with the constructed vector. The true word embedding is among the closest 165 or so word vectors to our embedding. 
(Note that the vocabulary size exceeds 200K, so this is considered a strong performance.)</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/ALCnonce.svg" width="50%" />
</p>

<h2 id="a-theory-of-induced-embeddings-for-general-features">A theory of induced embeddings for general features</h2>

<p>Why should the matrix $A$ mentioned above exist in the first place? 
Sanjeev, Yingyu, and Tengyu’s <a href="https://transacl.org/ojs/index.php/tacl/article/view/1346">TACL’18</a> paper together with Yuanzhi Li and Andrej Risteski gives a justification via a latent-variable model of corpus generation that is a modification of their earlier model described in <a href="https://transacl.org/ojs/index.php/tacl/article/view/742">TACL’16</a> (see also this <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">blog post</a>) The basic idea is to consider a random walk over an ellipsoid instead of the unit square. 
Under this modification of the rand-walk model, whose approximate MLE objective is similar to that of GloVe, their first theorem shows the following:</p>



<p>where the expectation is taken over possible contexts $c$.</p>

<p>This result also explains the linear algebraic structure of the embeddings of polysemous words (words having multiple possible meanings, such as <em>tie</em>) discussed in an earlier <a href="http://www.offconvex.org/2016/07/10/embeddingspolysemy/">post</a>.
Assuming for simplicity that $tie$ only has two meanings (<em>clothing</em> and <em>game</em>), it is easy to see that its word embedding is a linear transformation of the sum of the average context vectors of its two senses:</p>



<p>The above also shows that we can get a reasonable estimate for the vector of the sense <em>clothing</em>, and, by extension many other features of interest, by setting $v_\textrm{clothing}=A\mathbb{E}v_\textrm{clothing}^\textrm{avg}$.
Note that this linear method also subsumes other context representations, such as removing the <a href="http://www.offconvex.org/2018/06/17/textembeddings/">top singular component or down-weighting frequent directions</a>.</p>

<h3 id="n-gram-embeddings">$n$-gram embeddings</h3>

<p>While the theory suggests existence of a linear transform between word embeddings and their context embeddings, one could also use this linear transform to induce embeddings for other kinds of linguistic features in context.
We test this hypothesis by inducing embeddings for $n$-grams by using contexts from a large text corpus and word embeddings trained on the same corpus.
A qualitative evaluation of the $n$-gram embeddings is done by finding the closest words to it in terms of cosine similarity between the embeddings.
As evident from the below figure, <em>à la carte</em> bigram embeddings capture the meaning of the phrase better than some other compositional and learned bigram embeddings.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/ALCngram_quality.png" width="65%" />
</p>

<h3 id="sentence-embeddings">Sentence embeddings</h3>
<p>We also use these $n$-gram embeddings to construct sentence embeddings, similarly to <a href="http://www.offconvex.org/2018/06/25/textembeddings/">DisC embeddings</a>, to evaluate on classification tasks.
A sentence is embedded as the concatenation of sums of embeddings for $n$-gram in the sentence for use in downstream classification tasks.
Using this simple approach we can match the performance of other linear and LSTM representations, even obtaining state-of-the-art results on some of them. Note that Logeswaran and Lee is a contemporary paper that uses deep nets.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/ALCngram_clf.svg" width="80%" />
</p>

<h2 id="discussion">Discussion</h2>

<p>Our <em>à la carte</em> method is simple, almost elementary, and yet gives results competitive with many other feature embedding methods and also beats them in many cases.
Can one do zero-shot learning of word embeddings, i.e. inducing embeddings for a words/features without any context?
Character level methods such as <a href="https://fasttext.cc/">fastText</a> can do this and it is a good problem to incorporate character level information into the <em>à la carte</em> approach (the few things we tried didn’t work so far).</p>

<p>The <em>à la carte</em> code is <a href="https://github.com/NLPrinceton/ALaCarte">available here</a>, allowing you to re-create the results described.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/09/18/alacarte/"><span class="datestr">at September 18, 2018 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/07/27/approximating-recurrent/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/07/27/approximating-recurrent/">When Recurrent Models Don't Need to be Recurrent</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>In the last few years, deep learning practitioners have proposed a litany of
different sequence models.  Although recurrent neural networks were once the
tool of choice, now models like the autoregressive
<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">Wavenet</a> or the
<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a>
are replacing RNNs on a diverse set of tasks. In this post, we explore the
trade-offs between recurrent and feed-forward models. Feed-forward models can
offer improvements in training stability and speed, while recurrent models are
strictly more expressive. Intriguingly, this added expressivity does not seem to
boost the performance of recurrent models.  Several groups have shown
feed-forward networks can match the results of the best recurrent models on
benchmark sequence tasks. This phenomenon raises an interesting question for
theoretical investigation:</p>

<blockquote>
  <p>When and why can feed-forward networks replace recurrent neural networks
without a loss in performance?</p>
</blockquote>

<p>We discuss several proposed answers to this question and highlight our
<a href="https://arxiv.org/abs/1805.10369">recent work</a> that offers an explanation in
terms of a fundamental stability property.</p>

<h1 id="a-tale-of-two-sequence-models">A Tale of Two Sequence Models</h1>
<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>
<p>The many variants of recurrent models all have a similar form. The model
maintains a state $h_t$ that summarizes the past sequence of inputs. At each
time step $t$, the state is updated according to the equation
[
    h_{t+1} = \phi(h_t, x_t),
]
where $x_t$ is the input at time $t$, $\phi$ is a differentiable map, and $h_0$
is an initial state. In a vanilla recurrent neural network, the model is
parameterized by matrices $W$ and $U$, and the state is updated according to
[
    h_{t+1} = \tanh(Wh_t + Ux_t).
]
In practice, the <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Long Short-Term Memory
(LSTM)</a> network is
more frequently used. In either case, to make predictions, the state is passed
to a function $f$, and the model predicts $y_t = f(h_t)$. Since the state $h_t$
is a function of all of the past inputs $x_0, \dots, x_t$, the prediction $y_t$
depends on the entire history $x_0, \dots, x_t$ as well.</p>

<p>A recurrent model can also be represented graphically.</p>
<p style="text-align: center;">
    <img src="http://www.offconvex.org/assets/approx_recurrent/recurrent_net.png" height="250px" width="500px" />
</p>

<p>Recurrent models are fit to data using backpropagation. However, backpropagating
gradients from time step $T$ to time step $0$ often requires infeasibly large
amounts of memory, so essentially every implementation of a recurrent model
<em>truncates</em> the model and only backpropagates gradient $k$ times steps.</p>
<figure>
    <p style="text-align: center;">
        <img src="http://www.offconvex.org/assets/approx_recurrent/truncated_backprop.png" />
    </p>
    
    <small>
        Source: <a href="https://r2rt.com/styles-of-truncated-backpropagation.html"> 
        https://r2rt.com/styles-of-truncated-backpropagation.html </a>
    </small>
    
</figure>

<p>In this setup, the predictions of the recurrent model still depend on the entire
history $x_0, \dots, x_T$. However, it’s not clear how this training procedure
affects the model’s ability to learn long-term patterns, particularly those that
require more than $k$ steps.</p>

<h2 id="autoregressive-feed-forward-models">Autoregressive, Feed-Forward Models</h2>
<p>Instead of making predictions from a state that depends on the entire history,
an autoregressive model directly predicts $y_t$ using only the $k$ most recent
inputs, $x_{t-k+1}, \dots, x_{t}$. This corresponds to a strong <em>conditional
independence</em> assumption. In particular, a feed-forward model assumes the target
only depends on the $k$ most recent inputs. Google’s
<a href="https://arxiv.org/abs/1609.03499">WaveNet</a> nicely illustrates this general
principle.</p>

<figure>
    <p style="text-align: center;">
        <img src="https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif" />
    </p>
    
    <small>
        Source: <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/"> 
        https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>
    </small>
    
</figure>

<p>In contrast to an RNN, the limited context of a feed-forward model means that it
cannot capture patterns that extend more than $k$ steps. However, using
techniques like dilated-convolutions, one can make $k$ quite large.</p>

<h1 id="why-care-about-feed-forward-models">Why Care About Feed-Forward Models?</h1>
<p>At the outset, recurrent models appear to be a strictly more flexible and
expressive model class than feed-forward models. After all, feed-forward
networks make a strong conditional independence assumption that recurrent models
don’t make. Even if feed-forward models are less expressive, there are still
several reasons one might prefer a feed-forward network.</p>
<ul>
  <li><strong>Parallelization</strong>: Convolutional feed-forward models are easier to <a href="https://arxiv.org/abs/1705.03122">parallelize
at training time</a>. 
There’s no hidden state to update and maintain, and
therefore no sequential dependencies between outputs. This allows very
efficient implementations of training on modern hardware.</li>
  <li><strong>Trainability</strong>: Training deep convolutional neural networks is the
bread-and-butter of deep learning. Whereas recurrent models are often more
finicky and difficult to <a href="https://arxiv.org/abs/1211.5063">optimize</a>,
significant effort has gone into designing architectures and software to
efficiently and reliably train deep feed-forward networks.</li>
  <li><strong>Inference Speed</strong>: In some cases, feed-forward models can be significantly
more light-weight and perform <a href="https://arxiv.org/abs/1211.5063">inference faster than similar recurrent
systems</a>. In other cases,
particularly for long sequences, autoregressive inference is a large
bottleneck and requires <a href="https://arxiv.org/abs/1702.07825">significant engineering
work</a> or <a href="https://arxiv.org/abs/1711.10433">significant
cleverness</a> to overcome.</li>
</ul>

<h1 id="feed-forward-models-can-outperform-recurrent-models">Feed-Forward Models Can Outperform Recurrent Models</h1>
<p>Although it appears trainability and parallelization for feed-forward models
comes at the price of reduced accuracy, there have been several recent examples
showing that feed-forward networks can actually achieve the same accuracies as
their recurrent counterparts on benchmark tasks.</p>

<ul>
  <li>
    <p><strong>Language Modeling.</strong>
In language modeling, the goal is to predict the next word in a document given
all of the previous words. Feed-forward models make predictions using only the
$k$ most recent words, whereas recurrent models can potentially use the entire
document.  The <a href="https://arxiv.org/abs/1612.08083">Gated-Convolutional Language
Model</a> is a feed-forward autoregressive models
that is competitive with <a href="https://arxiv.org/abs/1602.02410">large LSTM baseline
models</a>. Despite using a truncation length of
$k=25$, the model outperforms a large LSTM on the
<a href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">Wikitext-103</a>
benchmark, which is designed to reward models that capture long-term
dependencies. On the <a href="http://www.statmt.org/lm-benchmark/">Billion Word
Benchmark</a>, the model is slightly worse
than the largest LSTM, but is faster to train and uses fewer resources.</p>
  </li>
  <li>
    <p><strong>Machine Translation.</strong>
The goal in machine translation is to map sequences of English words to
sequences of, say, French words. Feed-forward models make translations using
only $k$ words of the sentence, whereas recurrent models can leverage the entire
sentence.  Within the deep learning world, variants of the LSTM-based <a href="https://arxiv.org/abs/1409.0473">Sequence
to Sequence with Attention</a> model, particularly
<a href="https://arxiv.org/abs/1609.08144">Google Neural Machine Translation</a>, were
superseded first by a fully <a href="https://arxiv.org/abs/1705.03122">convolutional sequence to
sequence</a> model and then by the
<a href="https://arxiv.org/abs/1706.03762">Transformer</a>.<sup id="fnref:1"><a href="http://www.offconvex.org/feed.xml#fn:1" class="footnote">1</a></sup></p>
  </li>
</ul>
<figure>
    <p style="text-align: center;">
        <img src="https://raw.githubusercontent.com/facebookresearch/fairseq/master/fairseq.gif" />
    </p>
    
    <small>
        Source: <a href="https://github.com/facebookresearch/fairseq/blob/master/fairseq.gif"> 
        https://github.com/facebookresearch/fairseq/blob/master/fairseq.gif </a>
    </small>
    
</figure>

<ul>
  <li>
    <p><strong>Speech Synthesis.</strong>
In speech synthesis, one seeks to generate a realistic human speech signal.
Feed-forward models are limited to the past $k$ samples, whereas recurrent
models can use the entire history. Upon publication, the feed-forward,
autoregressive <a href="https://arxiv.org/abs/1609.03499">WaveNet</a> was a substantial
improvement over LSTM-RNN parametric models.</p>
  </li>
  <li>
    <p><strong>Everthing Else.</strong> 
Recently <a href="https://arxiv.org/abs/1803.01271">Bai et al.</a> proposed a generic
feed-forward model leveraging dilated convolutions and showed it outperforms
recurrent baselines on tasks ranging from synthetic copying tasks to music
generation.</p>
  </li>
</ul>

<h1 id="how-can-feed-forward-models-outperform-recurrent-ones">How Can Feed-Forward Models Outperform Recurrent Ones?</h1>
<p>In the examples above, feed-forward networks achieve results on par with or
better than recurrent networks. This is perplexing since recurrent models
seem to be more powerful a priori. One explanation for this phenomenon is
given by <a href="https://arxiv.org/abs/1612.08083">Dauphin et al.</a>:</p>

<blockquote>
  <p>The unlimited context offered by recurrent models is not strictly necessary
for language modeling.</p>
</blockquote>

<p>In other words, it’s possible you don’t need a large amount of context to do
well on the prediction task on average. <a href="https://arxiv.org/abs/1612.02526">Recent theoretical
work</a> offers some evidence in favor of this view.</p>

<p>Another explanation is given by <a href="https://arxiv.org/abs/1803.01271">Bai et al.</a>:</p>
<blockquote>
  <p>The “infinite memory” advantage of RNNs is largely absent in practice.</p>
</blockquote>

<p>As Bai et al. report, even in experiments explicitly requiring long-term
context, RNN variants were unable to learn long sequences. On the Billion Word
Benchmark, an <a href="https://arxiv.org/abs/1703.10724">intriguing Google Technical
Report</a> suggests an LSTM $n$-gram model with
$n=13$ words of memory is as good as an LSTM with arbitrary context.</p>

<p>This evidence leads us to conjecture: <strong>Recurrent models <em>trained in practice</em>
are effectively feed-forward.</strong> This could happen either because truncated
backpropagation time cannot learn patterns significantly longer than $k$ steps,
or, more provocatively, because models <em>trainable by gradient descent</em> cannot
have long-term memory.</p>

<p>In <a href="https://arxiv.org/abs/1805.10369">our recent paper</a>, we study the gap
between recurrent and feed-forward models trained using gradient descent. We
show if the recurrent model is <em>stable</em> (meaning the gradients can not explode),
then the model can be well-approximated by a feed-forward network for the
purposes of both <em>inference and training.</em> In other words, we show feed-forward
and stable recurrent models trained by gradient descent are <em>equivalent</em> in the
sense of making identical predictions at test-time. Of course, not all models
trained in practice are stable. We also give empirical evidence the stability
condition can be imposed on certain recurrent models without loss in
performance.</p>

<h1 id="conclusion">Conclusion</h1>
<p>Despite some initial attempts, there is still much to do to understand
why feed-forward models are competitive with recurrent ones and
shed light onto the trade-offs between sequence models. How much memory is
really needed to perform well on common sequence benchmarks? What are the
expressivity trade-offs between truncated RNNs (which can be considered
feed-forward) and the convolutional models that are in popular use? Why can
feed-forward networks perform as well as unstable RNNs in practice?</p>

<p>Answering these questions is a step towards building a theory that can both
explain the strengths and limitations of our current methods and give guidance
about how to choose between different classes of models in concrete settings.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The Transformer isn’t strictly a feed-forward model in the style described above (since it doesn’t make the $k$ step conditional independence assumption), but is not really a recurrent model because it doesn’t maintain a hidden state. <a href="http://www.offconvex.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/07/27/approximating-recurrent/"><span class="datestr">at July 27, 2018 08:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/06/25/textembeddings/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/06/25/textembeddings/">Deep-learning-free Text and Sentence Embedding, Part 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>This post continues <a href="http://www.offconvex.org/2018/06/17/textembeddings/">Sanjeev’s post</a> and describes further attempts to construct elementary and interpretable text embeddings. 
The previous post described the <a href="https://openreview.net/pdf?id=SyK00v5xx">the SIF embedding</a>, which uses a simple weighted combination of word embeddings combined with some mild “denoising” based upon singular vectors, yet outperforms many deep learning based methods, including <a href="https://arxiv.org/pdf/1506.06726.pdf">Skipthought</a>, on certain downstream NLP tasks such as sentence semantic similarity and entailment. 
See also this <a href="http://nlp.town/blog/sentence-similarity/">independent study by Yves Peirsman</a>.</p>

<p>However, SIF embeddings embeddings ignore word order (similar to classic <em>Bag of Words</em> models in NLP), which leads to unexciting performance on many other downstream classification tasks. 
(Even the denoising via SVD, which is crucial in similarity tasks, can sometimes reduces performance on other tasks.) 
Can we design a text embedding with the simplicity and transparency of SIF while also incorporating word order information?
Our <a href="https://openreview.net/pdf?id=B1e5ef-C-">ICLR’18 paper</a> with Kiran Vodrahalli does this, and achieves strong empirical performance and also some surprising theoretical guarantees stemming from the <a href="https://en.wikipedia.org/wiki/Compressed_sensing">theory of compressed sensing</a>. 
It is competitive with all pre-2018 LSTM-based methods on standard tasks. 
Even better, it is much faster to compute, since it uses pretrained (GloVe) word vectors and simple linear algebra.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/unsupervised_pipeline.png" alt="Pipeline" width="50%" />
</p>

<h2 id="incorporating-local-word-order-n-gram-embeddings">Incorporating local word order: $n$-gram embeddings</h2>

<p><em>Bigrams</em> are ordered word-pairs that appear in the sentence, and $n$-grams are ordered $n$-tuples. 
A document with $k$ words has $k-1$ bigrams and $k-n+1$ $n$-grams. 
The <em>Bag of n-gram (BonG) representation</em> of a document refers to a long vector whose each entry is indexed by all possible $n$-grams, and contains the number of times the corresponding $n$-gram appears in the document. 
Linear classifiers trained on BonG representations are a <a href="https://www.aclweb.org/anthology/P12-2018">surprisingly strong baseline for document classification tasks</a>.
While $n$-grams don’t directly encode long-range dependencies in text, one hopes that a fair bit of such information is implicitly present.</p>

<p>A trivial idea for incorporating $n$-grams into SIF embeddings would be to treat $n$-grams like words, and compute word embeddings for them using either GloVe and word2vec. 
This runs into the difficulty that the number of distinct $n$-grams in the corpus gets very large even for $n=2$ (let alone $n=3$), making it almost impossible to solve word2vec or GloVe. 
Thus one gravitates towards a more <em>compositional</em> approach.</p>

<blockquote>
  <p><strong>Compositional $n$-gram embedding:</strong> Represent $n$-gram $g=(w_1,\dots,w_n)$ as the element-wise product $v_g=v_{w_1}\odot\cdots\odot v_{w_n}$ of the embeddings of its constituent words.</p>
</blockquote>

<p>Note that due to the element-wise multiplication we actually represent unordered $n$-gram information, not ordered $n$-grams (the performance for order-preserving methods is about the same).
Now we are ready to define our <em>Distributed Co-occurrence (DisC) embeddings</em>.</p>

<blockquote>
  <p>The <strong>DisC embedding</strong> of a piece of text is just a concatenation for $(v_1, v_2, \ldots)$ where $v_n$ is the sum of the $n$-gram embeddings of all $n$-grams in the document (for $n=1$ this is just the sum of word embeddings).</p>
</blockquote>

<p>Note that DisC embeddings leverage classic Bag-of-n-Gram information as well as the power of word embeddings. 
For instance,  the sentences <em>“Loved this movie!”</em> and <em>“I enjoyed the film.”</em> share no $n$-gram information for any $n$, but  their DisC embeddings are fairly similar. 
Thus if the first example comes with a label, it gives the learner some idea of how to classify the second. 
This can be useful especially in settings with few labeled examples; e.g. DisC outperform BonG on the Stanford Sentiment Treebank (SST) task, which has only 6,000 labeled examples. 
DisC embeddings also beat SIF and a standard LSTM-based method, Skipthoughts. 
On the much larger IMDB testbed, BonG still reigns at top (although DisC is not too far behind).</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/clfperf_sst_imdb.png" alt="Performance on SST and IMDB" width="70%" />
</div>

<p>Skip-thoughts does match or beat our DisC embeddings on some other classification tasks, but that’s still not too shabby an outcome for such a simple method.  (By contrast, LSTM methods can take days or weeks of training, and are quite slow to evaluate at test time on a new piece of text.)</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/sentenceembedtable.jpg" alt="Performance on various classification tasks" width="70%" />
</div>

<h2 id="some-theoretical-analysis-via-compressed-sensing">Some theoretical analysis via compressed sensing</h2>

<p>A linear SIF-like embedding represents a document with Bag-of-Words vector $x$ as 

where $v_w$ is the embedding of word $w$ and $\alpha_w$ is a scaling term. 
In other words, it represents document $x$ as $A x$ where $A$ is the matrix with as many columns as the number of words in the language, and the column corresponding to word $w$ is $\alpha_w A$. 
Note that $x$ has many zero coordinates corresponding to words that don’t occur in the document; in other words it’s a <em>sparse</em> vector.</p>

<p>The starting point of our DisC work was the realization that perhaps the reason SIF-like embeddings work reasonably well is that they <em>preserve</em> the Bag-of-words information, in the sense that it may be possible to <em>easily recover</em> $x$ from $A$. 
This is not an outlandish conjecture at all, because <a href="https://en.wikipedia.org/wiki/Compressed_sensing"><em>compressed sensing</em></a> does exactly this when $x$ is suitably sparse and matrix $A$ has some nice properties such as RIP or incoherence. 
A classic example is when $A$ is a random matrix, which in our case corresponds to using random vectors as word embeddings.
Thus one could try to use random word embeddings instead of GloVe vectors in the construction and see what happens! 
Indeed, we find that so long as we raise the dimension of the word embeddings, then text embeddings using random vectors do indeed converge to the performance of BonG representations.</p>

<p>This is a surprising result, as compressed sensing does not imply this per se, since the ability to reconstruct the BoW vector from its compressed version doesn’t directly imply that the compressed version gives the same performance as BoW on linear classification tasks. 
However, a result of <a href="https://pdfs.semanticscholar.org/627c/14fe9097d459b8fd47e8a901694198be9d5d.pdf">Calderbank, Jafarpour, &amp; Schapire</a> shows that the compressed sensing condition that implies optimal recovery also implies good performance on linear classification under compression. Intuitively, this happens because of two facts.</p>

<p> 
</p>

<p>Furthermore, by extending these ideas to the $n$-gram case, we show that our DisC embeddings computed using random word vectors, which can be seen as a linear compression of the BonG representation, can do as well as the original BonG representation on linear classification tasks. To do this we prove that the “sensing” matrix $A$ corresponding to DisC embeddings satisfy the  <em>Restricted Isometry Property (RIP)</em> introduced in the seminal paper of <a href="https://statweb.stanford.edu/~candes/papers/DecodingLP.pdf">Candes &amp; Tao</a>. The theorem relies upon <a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/A%20Mathematical%20Introduction%20to%20Compressive%20Sensing.pdf">compressed sensing results for bounded orthonormal systems</a> and says that then the performance of DisC embeddings on linear classification tasks approaches that of BonG vectors as we increase the dimension. 
Please see our paper for details of the proof.</p>

<p>It is worth noting that our idea of composing objects (words) represented by random vectors to embed structures ($n$-grams/documents) is closely related to ideas in neuroscience and neural coding proposed by  <a href="http://www2.fiit.stuba.sk/~kvasnicka/CognitiveScience/6.prednaska/plate.ieee95.pdf">Tony Plate</a> and <a href="http://www.rctn.org/vs265/kanerva09-hyperdimensional.pdf">Pentti Kanerva</a>.
They also were interested in how these objects and structures could be recovered from the representations;
we take the further step of relating the recoverability to performance on a downstream linear classification task.
Text classification over compressed BonG vectors has been proposed before by <a href="https://papers.nips.cc/paper/4932-compressive-feature-learning.pdf">Paskov, West, Mitchell, &amp; Hastie</a>, albeit with a more complicated compression that does not achieve a low-dimensional representation (dimension &gt;100,000) due to the use of classical lossless algorithms rather than linear projection.
Our work ties together these ideas of composition and compression into a simple text representation method with provable guarantees.</p>

<h2 id="a-surprising-lower-bound-on-the-power-of-lstm-based-text-representations">A surprising lower bound on the power of LSTM-based text representations</h2>

<p>The above result also leads to a new theorem about deep learning: <em>text embeddings computed using low-memory LSTMs can do at least as well as BonG representations on downstream classification tasks</em>.
At first glance this result may seem uninteresting: surely it’s no surprise that the field’s latest and greatest method is at least as powerful as its oldest? 
But in practice, most papers on LSTM-based text embeddings make it a point to compare to performance of BonG baseline, and <em>often are unable to improve upon that baseline</em>! 
Thus empirically this new theorem had not been clear at all! (One reason could be that our theory requires the random embeddings to be somewhat higher dimensional than the LSTM work had considered.)</p>

<p>The new theorem follows from considering an LSTM that uses random vectors as word embeddings and computes the DisC embedding in one pass over the text. (For details see our appendix.)</p>

<p>We empirically tested the effect of dimensionality by measuring performance of DisC on IMDb sentiment classification.
As our theory predicts, the accuracy of DisC using random word embeddings converges to that of BonGs as dimensionality increases. (In the figure below “Rademacher vectors” are those with entries drawn randomly from $\pm1$.) Interestingly we also find that DisC using pretrained word embeddings like GloVe reaches BonG performance at much smaller dimensions, an unsurprising but important point that we will discuss next.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imdbperf_uni_bi.png" width="60%" />
</div>

<h2 id="unexplained-mystery-higher-performance-of-pretrained-word-embeddings">Unexplained mystery: higher performance of pretrained word embeddings</h2>

<p>While compressed sensing theory is a good starting point for understanding the power of linear text embeddings, it leaves some mysteries. 
Using pre-trained embeddings (such as GloVe) in DisC gives higher performance than random embeddings, both in recovering the BonG information out of the text embedding, as well as in downstream tasks. However, pre-trained embeddings do not satisfy some of the nice properties assumed in compressed sensing theory such as RIP or incoherence, since those properties forbid pairs of words having similar  embeddings.</p>

<p>Even though the matrix of embeddings does not satisfy these classical compressed sensing properties, we find that using Basis Pursuit, a sparse recovery approach related to LASSO with provable guarantees for RIP matrices, we can recover Bag-of-Words information better using GloVe-based text embeddings than from embeddings using random word vectors (measuring success via the $F_1$-score of the recovered words — higher is better).</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/recovery.png" width="60%" />
</div>

<p>Note that random embeddings are better than pretrained embeddings at recovering words from random word salad (the right-hand image).
This suggests that pretrained embeddings are specialized — thanks to their training on a text corpus — to do well only on real text rather than a random collection of words.
It would be nice to give a mathematical explanation for this phenomenon. 
We suspect that this should be possible using a result of <a href="http://www.pnas.org/content/pnas/102/27/9446.full.pdf">Donoho &amp; Tanner</a>, which we use to show that words in a document can be recovered from the sum of word vectors if and only if there is a hyperplane containing the vectors for words in the document with the vectors for all other words on one side of it.
Since co-occurring words will have similar embeddings, that should make it easier to find such a hyperplane separating words in a document from the rest of the words and hence would ensure good recovery.</p>

<p>However, even if this could be made more rigorous, it would only imply sparse recovery, not good performance on classification tasks.
Perhaps assuming a generative model for text, like the RandWalk model discussed in an <a href="https://www.offconvex.org/2016/02/14/word-embeddings-2/">earlier post</a>, could help move this theory forward.</p>

<h2 id="discussion">Discussion</h2>

<p>Could we improve the performance of such simple embeddings even further? 
One promising idea is to define better $n$-gram embeddings than the simple compositional embeddings defined in DisC. 
An independent <a href="https://arxiv.org/abs/1703.02507">NAACL’18 paper</a> of Pagliardini, Gupta, &amp; Jaggi proposes a text embedding similar to DisC in which unigram and bigram embeddings are trained specifically to be added together to form sentence embeddings, also achieving good results, albeit not as good as DisC. 
(Of course, their training time is higher than ours.) 
In our upcoming <a href="https://arxiv.org/abs/1805.05388">ACL’18 paper</a> with Yingyu Liang, Tengyu Ma, &amp; Brandon Stewart we give a very simple and efficient method to induce embeddings for $n$-grams as well as other rare linguistic features that improves upon DisC and beats skipthought on several other benchmarks. 
This will be described in a future blog post.</p>

<p>Sample code for constructing and evaluating DisC embeddings is <a href="https://github.com/NLPrinceton/text_embedding">available</a>, as well as <a href="https://github.com/NLPrinceton/sparse_recovery">solvers</a> for recreating the sparse recovery results for word embeddings.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/06/25/textembeddings/"><span class="datestr">at June 25, 2018 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/06/17/textembeddings/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/06/17/textembeddings/">Deep-learning-free Text and Sentence Embedding, Part 1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>Word embeddings (see my old <a href="http://www.offconvex.org/2015/12/12/word-embeddings-1/">post1</a>  and 
<a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">post2</a>) capture the idea that one can express “meaning” of words using a vector, so that the cosine of the angle between the vectors captures semantic similarity. (“Cosine similarity” property.) Sentence embeddings and text embeddings try to achieve  something similar: use a fixed-dimensional vector to represent a small piece of text, say a sentence or a small paragraph. The performance of such embeddings can be tested via the Sentence Textual Similarity (STS) datasets (see the <a href="http://ixa2.si.ehu.es/stswiki/index.php/Main_Page">wiki page</a>), which contain sentence pairs humanly-labeled with similarity ratings.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/textembeddingvectorslide.jpg" alt="What are text embeddings." width="30%" />
</p>

<p>A general hope behind computing text embeddings is that they can be learnt using a large <em>unlabeled</em> text corpus (similar to word embeddings) and then allow good performance on downstream classification tasks with few <em>labeled</em> examples. Thus the overall pipeline could look like this:</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/textembeddingpipeline.jpg" alt="How are text embeddings used in downstream classification task." width="80%" />
</p>

<p>Computing such representations is a form of <a href="http://www.offconvex.org/2017/06/26/unsupervised1/">representation learning as well as unsupervised learning</a>. This post will be an introduction to <strong>extremely simple</strong> ways of computing sentence embeddings, which on many standard tasks, beat many state-of-the-art  deep learning methods. This post is based upon <a href="https://openreview.net/pdf?id=SyK00v5xx">my ICLR’17 paper on SIF embeddings</a> with Yingyu Liang and Tengyu Ma.</p>

<h2 id="existing-methods">Existing methods</h2>

<p><a href="https://dl.acm.org/citation.cfm?id=2133826">Topic modeling</a> is a classic technique for unsupervised learning on text and it also yields a vector representation for a paragraph (or longer document), specifically, the vector of “topics” occuring in this document and their relative proportions. Unfortunately, topic modeling is not accurate at producing good representations at the sentence or short paragraph level, and furthermore there appears to be no variant of topic modeling that leads to the good cosine similarity property that we desire.</p>

<p><em>Recurrent neural net</em> is the default  deep learning technique  to train a <a href="https://www.tensorflow.org/tutorials/recurrent">language model</a>. It scans the text from left to right, maintaining a fixed-dimensional vector-representation of the text it has seen so far. It’s goal is to use this representation to predict the next word at each time step, and the training objective is to maximise log-likelihood of the data (or similar). Thus for example, a well-trained model when given a text fragment <em>“I went to the cafe and ordered a ….”</em>   would assign high probability to <em>“coffee”, “croissant”</em> etc. and low probability to  <em>“puppy”</em>. Myriad variations of such language models exist, many using biLSTMs which have some long-term memory and can scan the text forward and backwards. Lately biLSTMs have been replaced by convolutional architectures with attention mechanism; see for instance <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">this paper</a>.</p>

<p>One obtains a text representation by peeking at the internal representation (i.e., node activations) at the top layer of this deep model. After all, when the model is scanning through text, its ability to predict  the next word must imply that this internal representation implicitly captures a gist of all it has seen, reflecting rules of grammar, common-sense etc. (e.g., that you don’t order a puppy at a cafe). Some notable modern efforts along such lines are <a href="https://arxiv.org/abs/1506.01057">Hierarchichal Neural Autoencoder of Li et al.</a> as well as <a href="https://arxiv.org/abs/1502.06922">Palangi et al</a>, and  <a href="https://arxiv.org/abs/1506.06726"><em>Skipthought</em> of Kiros et al.</a>.</p>

<p>As with all deep learning models, one wishes for interpretability: what information exactly did the machine choose to put into the text embedding? Besides <a href="https://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf">the usual reasons for seeking interpretability</a>, in an NLP context it may help us leverage  additional external resources such as <a href="https://wordnet.princeton.edu/">WordNet</a> in the task. Other motivations include
 transfer learning/domain adaptation (to solve classification tasks for a  small text corpus, leverage text embeddings trained on a large unrelated corpus).</p>

<h2 id="surprising-power-of-simple-linear-representations">Surprising power of simple linear representations</h2>

<p>In practice, many NLP applications rely on a simple sentence embedding: the average of the embeddings of the words in it. This makes some intuitive sense, because recall that the <a href="https://arxiv.org/pdf/1310.4546.pdf">Word2Vec paper</a> uses  the following expression (in the their simpler CBOW word embedding)</p>



<p>which suggests that the sense of a sequence of words is captured via simple average of word vectors.</p>

<p>While this simple average has only fair performance in capturing sentence similarity via cosine similarity, it can be quite powerful in downstream classification tasks (after passing through a single layer neural net)   as shown in a 
 surprising paper of <a href="https://arxiv.org/abs/1511.08198">Wieting et al. ICLR’16</a>.</p>

<h2 id="better-linear-representation-sif-embeddings">Better linear representation: SIF embeddings</h2>

<p>My <a href="https://openreview.net/pdf?id=SyK00v5xx">ICLR’17 paper</a> with Yingyu Liang and Tengyu Ma improved such simple averaging  using our <strong>SIF</strong> embeddings. They’re motivated by the empirical observation that word embeddings have various pecularities stemming from the training method, which tries to capture word cooccurence probabilities using vector inner product, and words sometimes occur out of context in documents. These anomalies cause the average of word vectors to have nontrivial components along semantically meaningless directions. SIF embeddings try to combat this in two ways, which I describe intuitively first, followed by more theoretical justification.</p>

<p><strong>Idea 1: Nonuniform weighting of words.</strong>
Conventional wisdom in information retrieval holds that “frequent words carry less signal.” Usually this is captured via <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF weighting</a>, which assigns weightings to words inversely proportional to their frequency. We introduce a new variant we call <em>Smoothed Inverse Frequency</em> (SIF) weighting, 
 which assigns to word $w$ a weighting $\alpha_w = a/(a+ p_w)$ where $p_w$ is the frequency of $w$ in the corpus and $a$ is a hyperparameter. Thus the embedding of a piece of text is $\sum_w \alpha_w v_w$ where the sum is over words in it. 
 (Aside: word frequencies can be estimated from any sufficiently large corpus; we find embedding quality to be not too dependent upon this.)</p>

<p>On a related note, we found that  folklore understanding of word2vec, viz., expression (1), is <em>false.</em>  A dig into the code reveals a resampling trick that is tantamount to a weighted average quite similar to our SIF weighting. (See Section 3.1 in our paper for a discussion.)</p>

<p><strong>Idea 2: Remove component from top singular direction.</strong>
  The next idea is to modify the above weighted average by removing the component in a special direction, corresponding to the   top   singular direction set of weighted embeddings of a smallish sample of sentences from the domain (if doing domain adaptation, component is computed using sentences of the target domain). The paper notes that the direction corresponding to the top singular vector tends to contain information related to grammar and stop words, and removing the component in this subspace really cleans up the text embedding’s ability to express meaning.</p>

<h2 id="theoretical-justification">Theoretical justification</h2>
<p>A notable part of our paper is to give a theoretical justification for this weighting using a generative model for text similar to one used in our <a href="http://aclweb.org/anthology/Q16-1028">word embedding paper in TACL’16</a> as described in <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">my old post</a>.
  That model tries to give the causative relationship between word meanings and their cooccurence probabilities.  It  thinks of corpus generation as a dynamic process, where the $t$-th word is produced at step $t$. The model says that the process is driven by the random walk of a <em>discourse</em> vector $c_t \in \Re^d$. It is a unit vector whose direction in space represents <em>what is being talked about.</em>
 Each word has a  (time-invariant) latent vector $v_w \in \Re^d$ that captures its correlations with the discourse vector. We model this bias with a loglinear word production model:</p>



<p>The discourse vector does a slow geometric random walk over the unit sphere in $\Re^d$. Thus $c_{t+1}$ is obtained by a small random displacement from $c_t$. Since expression (2) places much higher probability on words that are clustered around $c_t$, and  $c_t$ moves slowly. If the discourse vector moves slowly, then we can assume a single discourse vector gave rise to the entire sentence or short paragraph. Thus given a sentence, a plausible vector representation of its “meaning” is a  <em>max a posteriori</em> (MAP) estimate of the discourse vector that generated it.</p>

<p>Such models have been empirically studied for a while, but our paper gave a theoretical analysis, and showed that various subcases imply standard word embedding methods such as word2vec and GloVe. For example, it shows that MAP estimate of the discourse vector is the simple average of the embeddings of the preceding $k$ words – in other words,  the average word vector!</p>

<p>This  model is clearly simplistic and our ICLR’17 paper suggests two correction terms, intended to account for words occuring out of context, and to allow some  common words  (<em>“the”, “and”, “but”</em> etc.) appear often regardless of the discourse. We first introduce an  additive term $\alpha p(w)$ in the log-linear model, where $p(w)$ is the unigram probability (in the entire corpus) of word and $\alpha$ is a scalar. This allows words to occur even if their vectors have very low inner products with $c_s$. 
  Secondly, we introduce a common discourse vector $c_0\in \Re^d$ which serves as a correction term for the most frequent discourse that is often related to syntax. It boosts the co-occurrence probability of words that have a high component along $c_0$.(One could make other correction terms, which are left to future work.) To put it another way, words that need to appear a lot out of context can do so by having a component along $c_0$, and the size of this component controls its probability of appearance out of context.</p>

<p>Concretely, given the discourse vector $c_s$ that produces sentence $s$, the probability of a word $w$ is emitted in the sentence $s$  is modeled as follows, where $\tilde{c}_{s}  = \beta c_0 + (1-\beta) c_s, c_0 \perp c_s$,
  $\alpha$ and $\beta$ are scalar hyperparameters:</p>



<p>where</p>



<p>is the normalizing constant (the partition function). We see that the model allows a word $w$ unrelated to the discourse $c_s$ to be emitted for two reasons: a) by chance from the term $\alpha p(w)$; b) if $w$ is correlated with the common direction $c_0$.</p>

<p>The paper shows that the MAP estimate of the $c_s$ vector corresponds to the SIF embeddings described earlier, where the top singular vector used in their construction is an estimate of the $c_0$ vector in the model.</p>

<h2 id="empirical-performance">Empirical performance</h2>

<p>The performance of this embedding scheme appears in the figure below. Note that Wieting et al. had already shown that their method (which is semi-supervised, relying upon a large unannotated corpus and a small annotated corpus) beats many LSTM-based methods. So this table only compares to their work; see the papers for comparison with more past work.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/textembedexperiments.jpg" alt="Performance of our embedding on downstream classification tasks" width="80%" />
</p>

<p>For other performance results please see the paper.</p>

<h2 id="next-post">Next post</h2>

<p>In the next post, I will sketch improvements to the above embedding in two of our new papers. Special guest appearance: Compressed Sensing (aka Sparse Recovery).</p>

<p>The SIF embedding package is available <a href="https://github.com/PrincetonML/SIF">from our github page</a></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/06/17/textembeddings/"><span class="datestr">at June 17, 2018 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/03/12/bigan/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/03/12/bigan/">Limitations of Encoder-Decoder GAN architectures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>This is yet another post about <a href="http://www.offconvex.org/2017/03/15/GANs/">Generative Adversarial Nets (GANs)</a>, and based upon our new <a href="https://openreview.net/forum?id=BJehNfW0-">ICLR’18 paper</a> with Yi Zhang.  A quick recap of the story so far. GANs are an unsupervised method in deep learning to learn interesting distributions (e.g., images of human faces), and also have a plethora of uses for image-to-image mappings in computer vision. Standard GANs training is motivated using this task of distribution learning, and is designed with the idea that given large enough deep nets and enough training examples, as well as accurate optimization, GANs will learn the full distribution.</p>

<p><a href="http://www.offconvex.org/2017/03/30/GANs2/">Sanjeev’s previous post</a> concerned <a href="https://arxiv.org/abs/1703.00573">his co-authored ICML’17 paper</a> which called this intuition into question when the deep nets have finite capacity. It shows that the training objective has near-equilibria where the discriminator is fooled —i.e., training objective is good—but the generator’s distributions has very small support, i.e. shows <em>mode collapse.</em>  This is a failure of the model, and raises the question whether such bad equilibria are found in real-life training. A <a href="http://www.offconvex.org/2017/07/07/GANs3/">second post</a> showed empirical evidence that they do, using the birthday-paradox test.</p>

<p>The current post concerns our <a href="https://arxiv.org/abs/1711.02651">new result</a> (part of our upcoming <a href="https://openreview.net/forum?id=BJehNfW0-">ICLR paper</a>) which shows that bad equilibria exist also in more recent GAN architectures based on simultaneously learning an <em>encoder</em> and <em>decoder</em>. This should be surprising because many researchers believe that encoder-decoder architectures fix many issues with GANs, including mode collapse.</p>

<p>As we will see, encoder-decoder GANs seem very powerful. In particular, the proof of the previously mentioned <a href="http://www.offconvex.org/2017/03/30/GANs2/">negative result</a> utterly breaks down for this architecture. But, we then discovered a cute argument that shows encoder-decoder GANs can have poor solutions, featuring not only mode collapse but also encoders that map images to nonsense (more precisely Gaussian noise). This is the worst possible failure of the model one could imagine.</p>

<h2 id="encoder-decoder-architectures">Encoder-decoder architectures</h2>

<p>Encoders and decoders have long been around in machine learning in various forms – especially deep learning. Speaking loosely, underlying all of them are two basic assumptions: <br />
(1) Some form of the so-called <a href="https://mitpress.mit.edu/sites/default/files/titles/content/9780262033589_sch_0001.pdf"><em>manifold assumption</em></a> which asserts that high-dimensional data such as real-life images lie (roughly) on a low-dimensional manifold. (“Manifold” should be interpreted rather informally – sometimes this intuition applies only very approximately sometimes it’s meant in a “distributional” sense, etc.)  <br />
(2) The low-dimensional structure is “meaningful”: if we think of an image $x$ as a high-dimensional vector and its “code” $z$ as its coordinates on the low-dimensional manifold, the code $z$ is thought of as a “high-level” descriptor of the image.</p>

<p>With the above two points in mind, an <em>encoder</em> maps the image to its code, and a <em>decoder</em> computes the reverse map. (We also discussed encoders and decoders in <a href="http://www.offconvex.org/2017/06/27/unsupervised1/">our earlier post on representation learning</a> in a more general setup.)</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/BIGAN_manifold2.jpg" alt="Manifold structure" width="80%" />
</p>

<h2 id="encoder-decoder-gans">Encoder-Decoder GANs</h2>
<p>These were introduced by <a href="https://arxiv.org/abs/1606.00704">Dumoulin et al.(ALI)</a> and <a href="https://arxiv.org/abs/1605.09782">Donahue et al.(BiGAN)</a>. They involve two competitors: Player 1 involves a discriminator net $D$ that is given an input of the form (image, code) and it outputs a number in the interval $[0,1]$, which denotes its “satisfaction level” with this input. Player 2 trains a decoder net $G$ (also called <em>generator</em> in the GANs setting) and an encoder net $E$.</p>

<p>Recall that in the standard GAN, discriminator tries to distinguish real images from images generated by the generator $G$. Here 
discriminator’s input is an image and its code. Specifically, Player 1 is trying to train its net to distinguish between the following two settings, and Player 2 is trying to make sure the two settings look indistinguishable to Player 1’s net.</p>

<p>
</p>

<p>(Here it is assumed that a random code is a vector with i.i.d gaussian coordinates, though one could consider other distributions.)</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/BIGAN_2settings_v2.jpg" alt="Two settings which discriminator net has to distinguish between" width="80%" />
</p>
<p>The hoped-for equilibrium obviously is one where generator and encoder are inverses of each other: $E(G(z)) \approx z$ and $G(E(x)) \approx x$, and the joint distributions $(z,G(z))$ and $(E(x), x)$ roughly match.
The underlying intuition is that if this happens, Player 1 must’ve produced a “meaningful” representation $E(x)$ for the images – and this should improve the quality of the generator as well. 
Indeed, <a href="https://arxiv.org/abs/1606.00704">Dumoulin et al.(ALI)</a> provide some small-scale empirical examples on mixtures of Gaussians for which encoder-decoder architectures seem to ameliorate the problem of mode collapse.</p>

<p>The above papers prove that when the encoder/decoder/discriminator have infinite capacity, the desired solution is indeed an equilibrium. However, we’ll see that things are very different when capacities are finite.</p>

<h2 id="finite-capacity-discriminators-are-weak">Finite-capacity discriminators are weak</h2>

<p>Say a generator/encoder pair $(G,E)$ $\epsilon$-<em>fools</em> a decoder $D$ if</p>



<p>In other words, $D$ has roughly similar output in Settings 1 and 2.</p>

<p>Our theorem applies when the distribution consists of realistic images, as explained later. We show the following:</p>

<blockquote>
  <p>(Informal theorem) If the discriminator $D$ has capacity (i.e. number of parameters) at most $p$, then there is an encoder $E$ of capacity $\ll p$ and  generator $G$ of slightly larger capacity than $p$ such that $(G, E)$ can $\epsilon$-fool every such $D$. Furthermore, the generator exhibits mode collapse: its distribution is essentially supported on a bit more than $p$ images, and the encoder $E$ just outputs white noise (i.e. does not extract any “meaningful” representation) given an image.</p>
</blockquote>

<p>(Note that such a $(G, E)$ represents an $\epsilon$-approximate equilibrium, in the sense that player 1 cannot gain more than $\epsilon$ in the distinguishing probability by switching its discriminator. )</p>

<p>It is important that the encoder’s capacity is much less than $p$, and thus the theorem allows a discriminator that is able to simulate $E$ if it needed, and in particular verify for a random seed $z$ that $E(G(z)) \approx z$. The theorem says that even the ability to conduct such a verification cannot give it power to force encoder to produce meaningful codes. This is a counterintuitive aspect of the result. The main difficulty in the proof (which stumped us for a bit) was how to exhibit such an equilibrium where $E$ is a small net.</p>

<p>This is ensured by a simple assumption. We assume the image distribution is mildly “noised”: say, every 100th pixel is replaced by Gaussian noise. To a human, such an image would of course be indistinguishable from a real image. (NB: Our proof could be carried out via some other assumptions to the effect that images have an innate stochastic/noise component that is efficiently extractable by a small neural network. But let’s keep things clean.) When noise $\eta$ is thus added to an image $x$, we denote the resulting image as $x \odot \eta$.</p>

<p>Now the encoder will be rather trivial: given the noised image $x \odot \eta$, output $\eta$. Clearly, such an encoder does not in any sense capture “meaning” in the image. It is also implementable by a tiny single-layer net, as required by the theorem.</p>

<h3 id="construction-of-generator">Construction of generator</h3>

<p>As usual in the GAN literature, we will assume the discriminator is $L$-<a href="https://www.encyclopediaofmath.org/index.php/Lipschitz_constant">Lipschitz</a>. This can be a loose upperbound, since only $\log L$ enters quantitatively in the proof.</p>

<p>The generator $G(z)$ in the theorem statement memorizes a hash function that partitions the set of all seeds/codes $z$ into $m$ equal-sized blocks; it also memorizes a “pool” of $m := p \log^2(pL)/ \epsilon^2$ unnoised images $\tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_m$. When presented with a random seed $z$, the generator computes the block of the partition that $z$ lies in, and then produces the image $\tilde{x}_i \odot z$, where $i$ is the block $z$ belongs to. (See the Figure below.)</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/BIGAN_construction_2.jpg" alt="The bad generator construction" width="50%" />
</p>

<p>Now we have to prove that such a memorizing generator exists that $\epsilon$-fools all discriminators of capacity $p$. This is shown by the <a href="https://en.wikipedia.org/wiki/Probabilistic_method">probabilistic method</a>: we describe a distribution over generators $G$ that works “in expectation”, and subsequently use concentration bounds to prove there exists at least one generator that does the job.</p>

<p>The distribution on $G$’s is straightforward: we select the pool of (unnoised) images 
$\tilde{x}_1, \tilde{x}_2, .., \tilde{x}_m$ at random. Why is this distribution for $G$ sensible? Notice the following simple fact:</p>



<p>In other words, the “expected” encoder correctly matches the expectation of $D(x, E(x))$, so that the discriminator is fooled “in expectation”.
This of course is not enough: we need some kind of concentration argument to show a particular $G$ works against <em>all possible discriminators</em>, which will ultimately use the fact that the discriminator $D$ has a small capacity and small Lipschitz constant. (Think covering number arguments in learning theory.)</p>

<p>Towards that, another useful observation: if $q$ is the uniform distribution over sets $T= {z_1, z_2,\dots, z_m}$, s.t. each $z_i$ is independently sampled from the conditional distribution inside the $i$-th block of the partition of the noise space, by the law of total expectation one can see that 

The right hand side is an average of terms, each of which is a bounded function of mutually independent random variables – so, by e.g. McDiarmid’s inequality it concentrates around it’s expectation, which by (3) is exactly $E_{z} D(G(z), z)$.</p>

<p>To finish the argument off, we use the fact that due to Lipschitzness and the bound on the number of parameters, the “effective” number of distinct discriminators is small, so we can union bound over them. (Formally, this translates to an epsilon-net + union bound argument. This also gives rise to the value of $m$ used in the construction.)</p>

<h2 id="takeaway">Takeaway</h2>

<p>The result should be interpreted as saying that possibly the theoretical foundations of GANs need more work. The current way of thinking about them as distribution learners may not be the right way to formalize them. Furthermore, one has to take care about transfering notions invented for distribution learning, such as encoders and decoders, over into the GANs setting. Finally there is an empirical question whether any of the <a href="https://deephunt.in/the-gan-zoo-79597dc8c347">myriad GANS variations</a> can avoid mode collapse.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/03/12/bigan/"><span class="datestr">at March 12, 2018 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/03/02/acceleration-overparameterization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/03/02/acceleration-overparameterization/">Can increasing depth serve to accelerate optimization?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>“How does depth help?” is a fundamental question in the theory of deep learning. Conventional wisdom, backed by theoretical studies (e.g. <a href="http://proceedings.mlr.press/v49/eldan16.pdf">Eldan &amp; Shamir 2016</a>; <a href="http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf">Raghu et al. 2017</a>; <a href="http://proceedings.mlr.press/v65/lee17a/lee17a.pdf">Lee et al. 2017</a>; <a href="http://proceedings.mlr.press/v49/cohen16.pdf">Cohen et al. 2016</a>; <a href="http://proceedings.mlr.press/v65/daniely17a/daniely17a.pdf">Daniely 2017</a>; <a href="https://openreview.net/pdf?id=B1J_rgWRW">Arora et al. 2018</a>), holds that adding layers increases expressive power. But often this expressive gain comes at a price –optimization is harder for deeper networks (viz., <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing/exploding gradients</a>). Recent works on “landscape characterization” implicitly adopt this worldview (e.g. <a href="https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf">Kawaguchi 2016</a>; <a href="https://openreview.net/pdf?id=ryxB0Rtxx">Hardt &amp; Ma 2017</a>; <a href="http://proceedings.mlr.press/v38/choromanska15.pdf">Choromanska et al. 2015</a>; <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf">Haeffele &amp; Vidal 2017</a>; <a href="https://arxiv.org/pdf/1605.08361.pdf">Soudry &amp; Carmon 2016</a>; <a href="https://arxiv.org/pdf/1712.08968.pdf">Safran &amp; Shamir 2017</a>). They prove theorems about local minima and/or saddle points in the objective of a deep network, while implicitly assuming that the ideal landscape would be convex (single global minimum, no other critical point). My <a href="https://arxiv.org/pdf/1802.06509.pdf">new paper</a>  with Sanjeev Arora and Elad Hazan makes the counterintuitive suggestion that sometimes, increasing depth can <em>accelerate</em> optimization.</p>

<p>Our work can also be seen as one more piece of evidence for a nascent belief that <em>overparameterization</em> of deep nets may be a good thing. By contrast, classical statistics discourages training a model with more parameters than necessary <a href="https://www.rasch.org/rmt/rmt222b.htm">as this can lead to overfitting</a>.</p>

<h2 id="ell_p-regression">$\ell_p$ Regression</h2>

<p>Let’s begin by considering a very simple learning problem - scalar linear regression with $\ell_p$ loss (our theory and experiments will apply to $p&gt;2$):</p>



<p>$S$ here stands for a training set, consisting of pairs $(\mathbf{x},y)$ where $\mathbf{x}$ is a vector representing an instance and $y$ is a (numeric) scalar standing for its label; $\mathbf{w}$ is the parameter vector we wish to learn.  Let’s convert the linear model to an extremely simple “depth-2 network”, by replacing the vector $\mathbf{w}$ with a vector $\mathbf{w_1}$ times a scalar $\omega_2$. Clearly, this is an overparameterization that does not change expressiveness, but yields the (non-convex) objective:</p>



<p>We show in the paper, that if one applies gradient descent over $\mathbf{w_1}$ and $\omega_2$, with small learning rate and near-zero initialization (as customary in deep learning), the induced dynamics on the overall (<em>end-to-end</em>) model $\mathbf{w}=\mathbf{w_1}\omega_2$ can be written as follows:</p>



<p>where $\rho^{(t)}$ and $\mu^{(t,\tau)}$ are appropriately defined (time-dependent) coefficients.
Thus the seemingly benign addition of a single multiplicative scalar turned plain gradient descent into a scheme that somehow has a memory of past gradients —the key feature of <a href="https://distill.pub/2017/momentum/">momentum</a> methods— as well as a time-varying learning rate. While theoretical analysis of the precise benefit of momentum methods is never easy, a simple experiment with $p=4$, on <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>’s <a href="https://archive.ics.uci.edu/ml/datasets/gas+sensor+array+drift+dataset">“Gas Sensor Array Drift at Different Concentrations” dataset</a>, shows the following effect:</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/acc_oprm_L4_exp.png" alt="L4 regression experiment" width="40%" />
</p>

<p>Not only did the overparameterization accelerate gradient descent, but it has done so more than two well-known, explicitly designed acceleration methods – <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> and <a href="https://arxiv.org/pdf/1212.5701.pdf">AdaDelta</a> (the former did not really provide a speedup in this experiment). We observed similar speedups in other settings as well.</p>

<p>What is happening here? Can non-convex objectives corresponding to deep networks be easier to optimize than convex ones?
Is this phenomenon common or is it limited to toy problems as above?
We take a first crack at addressing these questions…</p>

<h2 id="overparameterization-decoupling-optimization-from-expressiveness">Overparameterization: Decoupling Optimization from Expressiveness</h2>

<p>A general study of the effect of depth on optimization entails an inherent difficulty - deeper networks may seem to converge faster due to their superior expressiveness.
In other words, if optimization of a deep network progresses more rapidly than that of a shallow one, it may not be obvious whether this is a result of a true acceleration phenomenon, or simply a byproduct of the fact that the shallow model cannot reach the same loss as the deep one.
We resolve this conundrum by focusing on models whose representational capacity is oblivious to depth - <em>linear neural networks</em>, the subject of many recent studies.
With linear networks, adding layers does not alter expressiveness; it manifests itself only in the replacement of a matrix parameter by a product of matrices - an overparameterization.
Accordingly, if this leads to accelerated convergence, one can be certain that it is not an outcome of any phenomenon other than favorable properties of depth for optimization.</p>

<h2 id="implicit-dynamics-of-depth">Implicit Dynamics of Depth</h2>

<p>Suppose we are interested in learning a linear model parameterized by a matrix $W$, through minimization of some training loss $L(W)$.
Instead of working directly with $W$, we replace it by a depth $N$ linear neural network, i.e. we overparameterize it as $W=W_{N}W_{N-1}\cdots{W_1}$, with $W_j$ being weight matrices of individual layers.
In the paper we show that if one applies gradient descent over $W_{1}\ldots{W}_N$, with small learning rate $\eta$, and with the condition:</p>



<p>satisfied at optimization commencement (note that this approximately holds with standard near-zero initialization), the dynamics induced on the overall end-to-end mapping $W$ can be written as follows:</p>



<p>We validate empirically that this analytically derived update rule (over classic linear model) indeed complies with deep network optimization, and take a series of steps to theoretically interpret it.
We find that the transformation applied to the gradient $\nabla{L}(W)$ (multiplication from the left by $[WW^\top]^\frac{j-1}{N}$, and from the right by $[W^\top{W}]^\frac{N-j}{N}$, followed by summation over $j$) is a particular preconditioning scheme, that promotes movement along directions already taken by optimization.
More concretely, the preconditioning can be seen as a combination of two elements:</p>
<ul>
  <li>an adaptive learning rate that increases step sizes away from initialization; and</li>
  <li>a “momentum-like” operation that stretches the gradient along the azimuth taken so far.</li>
</ul>

<p>An important point to make is that the update rule above, referred to hereafter as the <em>end-to-end update rule</em>, does not depend on widths of hidden layers in the linear neural network, only on its depth ($N$).
This implies that from an optimization perspective, overparameterizing using wide or narrow networks has the same effect - it is only the number of layers that matters.
Therefore, acceleration by depth need not be computationally demanding - a fact we clearly observe in our experiments (previous figure for example shows acceleration by orders of magnitude at the price of a single extra scalar parameter).</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/acc_oprm_update_rule.png" alt="End-to-end update rule" width="60%" />
</p>

<h2 id="beyond-regularization">Beyond Regularization</h2>

<p>The end-to-end update rule defines an optimization scheme whose steps are a function of the gradient $\nabla{L}(W)$ and the parameter $W$.
As opposed to many acceleration methods (e.g. <a href="https://distill.pub/2017/momentum/">momentum</a> or <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a>) that explicitly maintain auxiliary variables, this scheme is memoryless, and by definition born from gradient descent over something (overparameterized objective).
It is therefore natural to ask if we can represent the end-to-end update rule as gradient descent over some regularization of the loss $L(W)$, i.e. over some function of $W$.
We prove, somewhat surprisingly, that the answer is almost always negative - as long as the loss $L(W)$ does not have a critical point at $W=0$, the end-to-end update rule, i.e. the effect of overparameterization, cannot be attained via <em>any</em> regularizer.</p>

<h2 id="acceleration">Acceleration</h2>

<p>So far, we analyzed the effect of depth (in the form of overparameterization) on optimization by presenting an equivalent preconditioning scheme and discussing some of its properties.
We have not, however, provided any theoretical evidence in support of acceleration (faster convergence) resulting from this scheme.
Full characterization of the scenarios in which there is a speedup goes beyond the scope of our paper.
Nonetheless, we do analyze a simple $\ell_p$ regression problem, and find that whether or not increasing depth accelerates depends on the choice of $p$:
for $p=2$ (square loss) adding layers does not lead to a speedup (in accordance with previous findings by <a href="https://arxiv.org/pdf/1312.6120.pdf">Saxe et al. 2014</a>);
for $p&gt;2$ it can, and this may be attributed to the preconditioning scheme’s ability to handle large plateaus in the objective landscape.
A number of experiments, with $p$ equal to 2 and 4, and depths ranging between 1 (classic linear model) and 8, support this conclusion.</p>

<h2 id="non-linear-experiment">Non-Linear Experiment</h2>

<p>As a final test, we evaluated the effect of overparameterization on optimization in a non-idealized (yet simple) deep learning setting - the <a href="https://github.com/tensorflow/models/tree/master/tutorials/image/mnist">convolutional network tutorial for MNIST built into TensorFlow</a>.
We introduced overparameterization by simply placing two matrices in succession instead of the matrix in each dense layer.
With an addition of roughly 15% in number of parameters, optimization accelerated by orders of magnitude:</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/acc_oprm_cnn_exp.png" alt="TensorFlow MNIST CNN experiment" width="40%" />
</p>

<p>We note that similar experiments on other convolutional networks also gave rise to a speedup, but not nearly as prominent as the above.
Empirical characterization of conditions under which overparameterization accelerates optimization in non-linear settings is potentially an interesting direction for future research.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Our work provides insight into benefits of depth in the form of overparameterization, from the perspective of optimization.
Many open questions and problems remain.
For example, is it possible to rigorously analyze the acceleration effect of the end-to-end update rule (analogously to, say, <a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf">Nesterov 1983</a> or <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Duchi et al. 2011</a>)?
Treatment of non-linear deep networks is of course also of interest, as well as more extensive empirical evaluation.</p>

<p><a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/03/02/acceleration-overparameterization/"><span class="datestr">at March 02, 2018 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2018/02/17/generalization2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2018/02/17/generalization2/">Proving generalization of deep nets via compression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>This post is about <a href="https://arxiv.org/abs/1802.05296">my new paper with Rong Ge, Behnam Neyshabur, and Yi Zhang</a> which offers some new perspective into the generalization mystery for deep nets discussed in 
<a href="http://www.offconvex.org/2017/12/08/generalization1/">my earlier post</a>. The new paper introduces an elementary compression-based framework for proving generalization bounds. It shows that deep nets are highly noise stable, and consequently, compressible. The framework also gives easy proofs (sketched below) of some papers that appeared in the past year.</p>

<p>Recall that the <strong>basic theorem</strong> of generalization theory says something like this: if training set had $m$ samples then the  <em>generalization error</em> —defined as the difference between error on training data and  test data (aka held out data)— is of the order of $\sqrt{N/m}$. Here  $N$ is the number of <em>effective parameters</em> (or <em>complexity measure</em>) of the net; it is at most the actual number of trainable parameters but could be much less. (For ease of exposition this post will ignore nuisance factors like $\log N$ etc. which also appear in the these calculations.) The mystery is that networks with millions of parameters have low generalization error even when $m =50K$ (as in CIFAR10 dataset), which suggests that the number of true parameters is actually much less than $50K$. The papers  <a href="https://arxiv.org/abs/1706.08498">Bartlett et al. NIPS’17</a> and <a href="https://openreview.net/forum?id=Skz_WfbCZ">Neyshabur et al. ICLR’18</a>
try to quantify the complexity measure using very interesting ideas like Pac-Bayes and Margin (which influenced our paper). But ultimately the quantitative estimates are fairly vacuous  —orders of magnitude <em>more</em> than the number of <em>actual parameters.</em>  By contrast our new estimates are several orders of magnitude better, and on the verge of being interesting. See the following bar graph on a log scale. (All bounds are listed  ignoring “nuisance factors.” Number of trainable parameters is included only to indicate scale.)</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/acompare.png" alt="comparison of bounds from various recent papers" width="75%" />
</p>

<h2 id="the-compression-approach">The Compression Approach</h2>

<p>The compression approach takes a deep net $C$ with $N$ trainable parameters and tries to compress it to another one $\hat{C}$ that has (a) much fewer parameters $\hat{N}$ than $C$ and (b) has roughly the same training error as $C$.</p>

<p>Then the above basic theorem guarantees that so long as the number of training samples exceeds $\hat{N}$, then $\hat{C}$ <em>does</em> generalize well (even if $C$ doesn’t).  An extension of this approach says that the same conclusions holds if we let the compression algorithm to depend upon an arbitrarily long <em>random string</em> provided this string is fixed in advance of seeing the training data. We call this <em>compression with respect to fixed string</em> and rely upon it.</p>

<p>Note that the above approach proves good generalization of the compressed $\hat{C}$, not the original $C$. (I suspect the ideas may extend to proving good generalization of the original $C$; the hurdles seem technical rather than inherent.) Something similar was true of earlier approaches using PAC-Bayes bounds, which also  prove the generalization of some  net related to $C$, not of $C$ itself. (Hence the tongue-in-cheek title of the classic reference <a href="http://www.cs.cmu.edu/~jcl/papers/nn_bound/not_bound.pdf">Langford-Caruana2002</a>.)</p>

<p>Of course, in practice  deep nets are well-known to be compressible using a slew of ideas—by factors of 10x to 100x; see <a href="https://arxiv.org/abs/1710.09282">the recent survey</a>. However, usually such compression involves <em>retraining</em> the compressed net. Our paper doesn’t consider retraining the net (since it involves reasoning about the loss landscape) but followup work should look at this.</p>

<h2 id="flat-minima-and-noise-stability">Flat minima and Noise Stability</h2>

<p>Modern generalization results can be seen as proceeding via some formalization of a <em>flat minimum</em> of the loss landscape. This was suggested in 1990s as the source of good generalization <a href="http://www.bioinf.jku.at/publications/older/3304.pdf">Hochreiter and Schmidhuber 1995</a>. Recent empirical work of <a href="https://arxiv.org/abs/1609.04836">Keskar et al 2016</a> on modern deep architectures finds that flatness does correlate with better generalization, though the issue is complicated, as discussed in an upcoming post by Behnam Neyshabur.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/aflatminima.png" alt="Flat vs sharp minima" width="65%" />
</p>

<p>Here’s the intuition why a flat minimum should generalize better, as originally articulated by <a href="http://www.cs.toronto.edu/~fritz/absps/colt93.pdf">Hinton and Camp 1993</a>. Crudely speaking, suppose a flat minimum is one that occupies “volume” $\tau$ in the landscape. (The flatter the minimum, the higher $\tau$ is.)  Then the number of <em>distinct</em> flat minima in the landscape is at most $S =\text{total volume}/\tau$. Thus one can number the flat minima from $1$ to $S$, implying that a flat minimum can be represented using $\log S$ bits.  The above-mentioned <em>basic theorem</em> implies that flat minima generalize if the number of training samples $m$ exceeds $\log S$.</p>

<p>PAC-Bayes approaches try to formalize the above intuition by defining a flat minimum as follows: it is a net $C$ such that adding appropriately-scaled gaussian noise to all its trainable parameters does not greatly affect the training error. This allows quantifying the “volume” above in terms of probability/measure (see 
<a href="http://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf">my lecture notes</a> or <a href="https://arxiv.org/abs/1703.11008">Dziugaite-Roy</a>) and yields some explicit estimates on sample complexity.  However, obtaining good quantitative estimates from this calculation has proved difficut, as seen in the bar graph earlier.</p>

<p>We formalize “flat minimum” using noise stability of a slightly different form. Roughly speaking, it says that if we inject appropriately scaled gaussian noise at the output of some layer, then this noise gets attenuated as it propagates up to higher layers. (Here “top” direction refers to the output of the net.)  This is obviously related to notions like dropout, though it arises also in nets that are not trained with dropout. The following figure illustrates how noise injected at a certain layer of VGG19 (trained on CIFAR10) affects the higher layer. The y-axis denote the magnitude of the noise ($\ell_2$ norm) as a multiple of the vector being computed at the layer, and shows how a single noise vector quickly attenuates as it propagates up the layers.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/attenuate.jpg" alt="How noises attenuates as it travels up the layers of VGG." width="65%" />
</p>
<p>Clearly, computation of the trained net is highly resistant to noise. (This has obvious implications for biological neural nets…) 
Note that the training involved no explicit injection of noise (eg dropout). Of course, stochastic gradient descent <em>implicitly</em> adds noise to the gradient, and it would be nice to investigate more rigorously if the noise stability arises from this or from some other source.</p>

<h2 id="noise-stability-and-compressibility-of-single-layer">Noise stability and compressibility of single layer</h2>

<p>To understand why noise-stable nets are compressible, let’s first understand noise stability for a single layer in the net, where we ignore the nonlinearity. Then this layer is just a linear transformation, i.e., matrix $M$.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/alinear.png" alt="matrix M describing a single layer" width="40%" />
</p>

<p>What does it mean that this matrix’s output is stable to noise? Suppose the vector at the previous layer is a unit vector $x$. This is the output of the lower layers on an actual sample, so $x$ can be thought of as the “signal” for the current layer.  The matrix converts $x$ into $Mx$. If we inject a noise vector $\eta$ of unit norm at $x$ then the output must become $M(x +\eta)$. We say $M$ is noise stable for input $x$ if such noising affects the output very little, which implies the norm of $Mx$ is much higher than that of $M \eta$. 
The former is at most $\sigma_{max}(M)$, the largest singular value of $M$. The latter is approximately 
$(\sum_i \sigma_i(M)^2)^{1/2}/\sqrt{h}$ where $\sigma_i(M)$ is the $i$th singular value of $M$ and $h$ is dimension of $Mx$. The reason is that gaussian noise divides itself evenly across all directions, with variance in each direction $1/h$. 
We conclude that:
</p>

<p>which implies that the matrix has an uneven distribution of singular values. Ratio of left side and right side is called the <a href="https://nickhar.wordpress.com/2012/02/29/lecture-15-low-rank-approximation-of-matrices/"><em>stable rank</em></a> and is at most the linear algebraic rank. Furthermore, the above analysis suggests that the “signal” $x$ is <em>correlated</em> with the singular directions corresponding to the higher singular values, which is at the root of the noise stability.</p>

<p>Our experiments on VGG and GoogleNet reveal that the higher layers of deep nets—where most of the net’s parameters reside—do indeed exhibit a highly uneven distribution of singular values, and that the signal aligns more with the higher singular directions. The figure below describes layer 10 in VGG19 trained on CIFAR10.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/aspectrumlayer10.png" alt="distribution of singular values of matrix at layer 10 of VGG19" width="45%" />
</p>

<h2 id="compressing-multilayer-net">Compressing multilayer net</h2>

<p>The above analysis of noise stability in terms of singular values cannot hold across multiple layers of a deep net, because the mapping becomes nonlinear, thus lacking a notion of singular values.  Noise stability is therefore formalized using the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a> of this mapping, which is the matrix describing how the output reacts to tiny perturbations of the input. Noise stability says that this nonlinear mapping passes signal (i.e., the vector from previous layers) much more strongly than it does a noise vector.</p>

<p>Our compression algorithm applies a randomized transformation to the matrix of each layer (aside: note the use of randomness, which fits in our “compressing with fixed string” framework) that relies on the low stable rank condition at each layer. This compression introduces error in the layer’s output, but the vector describing this error is “gaussian-like” due to the use of randomness in the compression. Thus this error gets attenuated by higher layers.</p>

<p>Details can be found in the paper. All noise stability properties formalized there are later checked in the experiments section.</p>

<h2 id="simpler-proofs-of-existing-generalization-bounds">Simpler proofs of existing generalization bounds</h2>

<p>In the paper we also use our compression framework to give elementary (say, 1-page) proofs of the previous generalization bounds from the past year. For example, the paper of <a href="https://openreview.net/forum?id=Skz_WfbCZ">Neyshabur et al.</a> shows the following is an upper bound on the generalization error where $A_i$ is the matrix describing the $i$th layer.</p>

<p style="text-align: center;">
<img src="http://www.offconvex.org/assets/saddle_eff/aexpression1.png" alt="Expression for effective number of parameters in Neyshabur et al" width="50%" />
</p>

<p>Comparing to the <em>basic theorem</em>, we realize the numerator corresponds to the number of effective parameters. The second part of the expression is the sum of stable ranks of the layer matrices, and is a natural measure of complexity. The first part is product of spectral norms (= top singular value) of the layer matrices, which happens to be an upper bound on the Lipschitz constant of the entire network. (Lipschitz constant of a mapping $f$ in this context is a constant $L$ such that $f(x) \leq L c\dot |x|$.) 
The reason this is the Lipschitz constant is that if an input $x$ is presented at the bottom of the net, then each successive layer can multiply its norm by at most the top singular value, and the ReLU nonlinearity can only decrease norm since its only action is to zero out some entries.</p>

<p>Having decoded the above expression, it is clear how to interpret it as an analysis of a (deterministic) compression of the net. Compress each layer by zero-ing out (in the <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">SVD</a>) singular values less than some threshold $t|A|$, which we hope turns it into a low rank matrix. (Recall that a matrix with rank $r$ can be expressed using $2nr$ parameters.)   A simple computation shows that the number of remaining singular values is at most the stable rank divided by $t^2$.  How do we set $t$? The truncation introduces error in the layer’s computation, which gets propagated through the higher layers and magnified at most by the Lipschitz constant. We want to make this propagated error small, which can be done by making $t$ inversely proportional to the Lipschitz constant.  This leads to the above bound on the number of effective parameters.</p>

<p>This proof sketch also clarifies how our work improves upon the older works: they are also (implicitly) compressing the deep net, but their analysis of how much compression is possible is much more pessimistic because they assume the network transmits noise at peak efficiency given by the Lipschitz constant.</p>

<h2 id="extending-the-ideas-to-convolutional-nets">Extending the ideas to convolutional nets</h2>

<p>Convolutional nets could not be dealt with cleanly in the earlier papers. I must admit that handling convolution stumped us as too for a while. A layer in a convolutional net applies the same filter to all patches in that layer. This <em>weight sharing</em> means that the full layer matrix already has a fairly compact representation, and it seems challenging to compress this further. However, in nets like VGG and GoogleNet, the higher layers use rather large filter matrices (i.e., they use a large number of channels), and one could hope to compress these individual filter matrices.</p>

<p>Let’s discuss the two naive ideas. The first is to compress the filter independently in different patches. This unfortunately is not a compression at all, since  each  copy of the filter then comes with its own parameters. The second idea is to do a single compression of the filter and use the compressed copy in each patch. This messes up the error analysis because the errors introduced due to compression in the different copies are now correlated, whereas the analysis requires them to be more like gaussian.</p>

<p>The idea we end up using is to compress the filters using $k$-wise independence (an idea from <a href="https://en.wikipedia.org/wiki/K-independent_hashing">theory of hashing schemes</a>), where $k$ is roughly logarithmic in the number of training samples.</p>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>While generalization theory can seem merely academic at times —since in practice held-out data establishes generalizaton— I hope you see from the above account that understanding generalization can give some interesting insights into what is going on in deep net training. Insights about noise stability of trained deep nets have obvious interest for study of biological neural nets. (See also the classic <a href="http://fab.cba.mit.edu/classes/862.16/notes/computation/vonNeumann-1956.pdf">von Neumann ideas</a> on noise resilient computation.)</p>

<p>At the same time, I suspect that compressibility is only one part of the generalization mystery, and that we are still missing some big idea. I don’t see how to use the above ideas to demonstrate that the effective number of parameters in VGG19 is as low as $50k$, as seems to be the case. I suspect doing so will force us to understand the structure of the data (in this case, real-life images) which the above analysis mostly ignores. The only property of data used is that the deep net aligns itself better with data than with noise.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2018/02/17/generalization2/"><span class="datestr">at February 17, 2018 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2017/12/08/generalization1/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2017/12/08/generalization1/">Generalization Theory and Deep Nets, An introduction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>Deep learning holds many mysteries for theory, as we have discussed on this blog. Lately many ML theorists have become interested in the generalization mystery: why do trained deep nets perform well on previously unseen data, even though they have way more free parameters than the number of datapoints (the classic “overfitting” regime)? Zhang et al.’s  paper  <a href="https://arxiv.org/abs/1611.03530">Understanding Deep Learning requires Rethinking Generalization</a> played some role in bringing attention to this challenge. Their  main experimental finding is that if you take a classic convnet architecture, say <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">Alexnet</a>, and train it on  images with random labels, then you can still achieve very high accuracy on the training data. (Furthermore, usual regularization strategies, which are believed to promote better generalization, do not help much.) Needless to say, the trained net is subsequently unable to predict the (random) labels of still-unseen images, which means it doesn’t generalize. The paper notes that the ability to fit a classifier to data with random labels is also a traditional measure in machine learning called Rademacher complexity (which we will discuss shortly) and thus Rademacher complexity gives no meaningful bounds on sample complexity.  I found this paper entertainingly written and recommend reading it, despite having given away the punchline. Congratulations to the authors for winning best paper at ICLR 2017.</p>

<p>But I would be remiss if I didn’t report that at the <a href="https://simons.berkeley.edu/programs/machinelearning2017">Simons Institute Semester on theoretical ML in spring 2017</a> generalization theory experts expressed unhappiness about this paper, and especially its title. They felt that similar issues had been extensively studied  in context of simpler models such as kernel SVMs (which, to be fair, is clearly mentioned in the paper). It is trivial to design SVM architectures with high Rademacher complexity which nevertheless train and generalize well on real-life data. Furthermore, theory was developed to explain this generalization behavior (and also for related models like boosting). On a related note, several earlier papers of Behnam Neyshabur and coauthors (see <a href="https://arxiv.org/abs/1605.07154">this paper</a> and for a full account, <a href="https://arxiv.org/abs/1703.11008">Behnam’s thesis</a>)
had made points fairly similar to Zhang et al. pertaining to deep nets.</p>

<p>But regardless of such complaints, we should be happy about the attention brought by Zhang et al.’s paper to a core theory challenge. Indeed, the passionate discussants at the Simons semester themselves banded up in  subgroups to address this challenge: these resulted in papers by <a href="https://arxiv.org/abs/1703.11008">Dzigaite and Roy</a>, then <a href="https://arxiv.org/abs/1706.08498">Bartlett, Foster, and Telgarsky</a> and finally <a href="https://arxiv.org/abs/1707.09564">Neyshabur, Bhojapalli, MacAallester, Srebro</a>. (The latter two were presented at NIPS’17 this week.)</p>

<p>Before surveying these results let me start by suggesting that some of the controversy over the title of Zhang et al.’s paper stems from some basic confusion about whether or not current generalization theory is prescriptive or merely descriptive. These confusions  arise from the standard treatment of generalization theory in courses and textbooks, as I discovered while teaching the recent developments in <a href="http://www.cs.princeton.edu/courses/archive/fall17/cos597A/">my graduate seminar</a>.</p>

<h3 id="prescriptive-versus-descriptive-theory">Prescriptive versus descriptive theory</h3>

<p>To illustrate the difference, consider a patient who says to his doctor: “Doctor, I wake up often at night and am tired all day.”</p>

<blockquote>
  <p>Doctor 1 (without any physical examination): “Oh, you have sleep disorder.”</p>
</blockquote>

<p>I call such a diagnosis <em>descriptive</em>, since it only attaches a label to the patient’s problem, without giving any insight into how to solve the problem. Contrast with:</p>

<blockquote>
  <p>Doctor 2 (after careful physical examination): “A growth in your sinus is causing sleep apnea. Removing it will resolve your problems.”</p>
</blockquote>

<p>Such a diagnosis is <em>prescriptive.</em></p>

<h2 id="generalization-theory-descriptive-or-prescriptive">Generalization theory: descriptive or prescriptive?</h2>

<p>Generalization theory notions such as VC dimension, Rademacher complexity, and PAC-Bayes bound, consist of attaching a <em>descriptive label</em> to the basic phenomenon of lack of generalization. They are hard to compute for today’s complicated ML models, let alone to use as a guide in designing learning systems.</p>

<p>Recall what it means for a hypothesis/classifier $h$ to not generalize. Assume the training data consists of a sample $S = {(x_1, y_1), (x_2, y_2),\ldots, (x_m, y_m)}$ of $m$ examples from some distribution ${\mathcal D}$. A <em>loss function</em> $\ell$ describes how well hypothesis $h$ classifies a datapoint: the loss $\ell(h, (x, y))$ is high if the hypothesis didn’t come close to producing the label $y$ on $x$ and low if it came close.  (To give an example,  the <em>regression</em> loss is $(h(x) -y)^2$.) Now let us denote by $\Delta_S(h)$ the average loss on samplepoints in $S$, and by $\Delta_{\mathcal D}(h)$ the expected loss on samples from distribution ${\mathcal D}$. 
Training  <em>generalizes</em> if the hypothesis $h$ that minimises $\Delta_S(h)$ for a random sample $S$ also achieves very similarly low loss $\Delta_{\mathcal D}(h)$ on the full distribution. When this fails to happen, we have:</p>

<blockquote>
  <p><strong>Lack of generalization:</strong>  $\Delta_S(h) \ll \Delta_{\mathcal D}(h) \qquad (1). $</p>
</blockquote>

<p>In practice, lack of generalization is detected by taking a second sample
 (“held out set”) $S_2$ of size $m$ from ${\mathcal D}$. By concentration bounds expected loss of $h$ on this second sample closely approximates $\Delta_{\mathcal D}(h)$, allowing us to conclude</p>



<h3 id="generalization-theory-descriptive-parts">Generalization Theory: Descriptive Parts</h3>

<p>Let’s discuss <strong>Rademacher complexity,</strong> which I will simplify a bit for this discussion. (See also  <a href="http://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf">scribe notes of my lecture</a>.) For convenience assume in this discussion that labels and loss are $0,1$, and 
 assume that the badly generalizing $h$ predicts perfectly on the training sample $S$ and is completely wrong on the heldout set $S_2$, meaning</p>

<p>$\Delta_S(h) - \Delta_{S_2}(h) \approx - 1 \qquad (3)$</p>

<p>Rademacher complexity concerns the following thought experiment. Take a single sample of size $2m$   from $\mathcal{D}$, split it into two and call the first half $S$ and the second $S_2$. <em>Flip</em> the labels of points in $S_2$. Now try to find a classifier $C$ that best describes this new sample, meaning one that minimizes $\Delta_S(h)  + 1- \Delta_{S_2}(h)$. This expression follows since flipping the label of a point turns good classification into bad and vice versa, and thus the loss function for $S_2$ is $1$ minus the old loss. We say the class of classifiers has high Rademacher complexity if with high probability this quantity is small, say close to $0$.</p>

<p>But a glance  at (3) shows that it implies high Rademacher complexity: $S, S_2$ were random samples of size $m$  from $\mathcal{D}$, so their combined size is $2m$, and when generalization failed we succeeded in finding a hypothesis $h$ for which $\Delta_S(h)  + 1- \Delta_{S_2}(h)$ is very small.</p>

<p>In other words, returning to our medical analogy, the doctor only had to hear “Generalization didn’t happen” to pipe up with: “Rademacher complexity is high.” This is why I call this result descriptive.</p>

<p>The <strong>VC dimension</strong> bound is similarly descriptive.  VC dimension is defined to be at least $k +1$ if there exists a set of size $k$ such that the following is true. If we look at all possible classifiers in the class, and the sequence of labels each gives to the $k$ datapoints in the sample, then we can find all possible $2^{k}$ sequences of $0$’s and $1$’s.</p>

<p>If generalization does not happen as in (2) or (3) then this turns out to imply that VC dimension is at least around $\epsilon m$ for some $\epsilon &gt;0$. The reason is that the $2m$ data points were split randomly into $S, S_2$, and there are $2^{2m}$ such splittings. When the generalization error is $\Omega(1)$ this can be shown to imply that we can achieve  $2^{\Omega(m)}$ labelings of the $2m$ datapoints using all possible classifiers. Now the classic Sauer’s lemma (see any lecture notes on this topic, such as <a href="https://www.cs.princeton.edu/courses/archive/spring14/cos511/scribe_notes/0220.pdf">Schapire’s</a>) can be used to show that
 VC dimension is at least $\epsilon m/\log m$ for some constant $\epsilon&gt;0$.</p>

<p>Thus again, the doctor only has to hear “Generalization didn’t happen with sample size $m$” to pipe up with: “VC dimension is higher than $\Omega(m/log m)$.”</p>

<p>One can similarly show that PAC-Bayes bounds are also descriptive, as you can see in <a href="http://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf">scribe notes from my lecture</a>.</p>

<blockquote>
  <p>Why do students get confused and think that such tools of generalization theory gives some powerful technique to guide design of machine learning algorithms?</p>
</blockquote>

<p>Answer: Probably because  standard presentation in lecture notes and textbooks seems to pretend that we are computationally-omnipotent beings who can <em>compute</em> VC dimension and Rademacher complexity and thus arrive at meaningful bounds on sample sizes needed for training to generalize. While this may have been possible in the old days with simple classifiers, today we have
complicated classifiers with millions of variables, which furthermore are products of  nonconvex optimization techniques like backpropagation. 
 The only way to actually lowerbound Rademacher complexity of such complicated learning architectures is to try training a classifier, and detect lack of generalization via a held-out set. Every practitioner in the world already does this (without realizing it), and  kudos to Zhang et al. for highlighting that theory currently offers nothing better.</p>

<h2 id="toward-a-prescriptive-generalization-theory-the-new-papers">Toward a prescriptive generalization theory: the new papers</h2>

<p>In our medical analogy we saw that the doctor needs to at least do a physical examination to have a prescriptive diagnosis. The authors of the new papers intuitively grasp this point, and try to identify properties of real-life deep nets that may lead to better generalization. Such an analysis (related to “margin”) was done for simple 2-layer networks couple decades ago, and the challenge is to find analogs for multilayer networks. Both Bartlett et al. and Neyshabur et al. hone in on <a href="https://nickhar.wordpress.com/2012/02/29/lecture-15-low-rank-approximation-of-matrices/"><em>stable rank</em></a> of the weight matrices of the layers of the deep net. These can be seen as an instance of a “flat minimum” which has been discussed in <a href="http://www.bioinf.jku.at/publications/older/3304.pdf">neural nets literature</a> for many years. I will present my take on these results as well as some improvements in a future post. Note that these methods do not as yet give any nontrivial bounds on the number of datapoints needed for training the nets in question.</p>

<p><a href="https://arxiv.org/abs/1703.11008">Dziugaite and Roy</a> take a slightly different tack. They start with McAllester’s 1999 PAC-Bayes bound, which says that if the algorithm’s prior distribution on the hypotheses is $P$ then for every posterior distributions $Q$ (which could depend on the data) on the hypotheses the generalization error of the average classifier picked according to $Q$ is upper bounded as follows where $D()$ denotes <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>:</p>

<div style="text-align: center;">
 <img src="http://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/pacbayes.png" style="width: 600px;" />
</div>

<p>This allows upperbounds on generalization error (specifically, upperbounds on number of samples that guarantee such an upperbound) by proceeding as in  <a href="http://www.cs.cmu.edu/~jcl/papers/nn_bound/not_bound.pdf">Langford and Caruana’s old paper</a> where $P$ is a uniform gaussian, and $Q$ is a noised version of the trained deep net (whose generalization we are trying to explain). Specifically, if $w_{ij}$ is the weight of edge ${i, j}$ in the trained net, then $Q$ consists of adding a gaussian noise $\eta_{ij}$ to weight $w_{ij}$. Thus a random classifier according to $Q$ is nothing but a noised version of the trained net. Now we arrive at the crucial idea: Use nonconvex optimization to find a choice for the variance of $\eta_{ij}$ that balances two competing criteria: (a) the average classifier drawn from $Q$ has training error not much more than the original trained net (again, this is a quantification of the “flatness” of the minimum found by the optimization) and (b) the right hand side of the above expression is as small as possible.  Assuming (a) and (b) can be suitably bounded, it follows that the average classifier from Q works reasonably well on unseen data.  (Note that this method only proves generalization of a noised version of the trained classifier.)</p>

<p>Applying this method on  simple fully-connected neural nets trained on MNIST dataset, they can prove that the method achieves error $17$ percent error on MNIST (whereas the <em>actual</em> error is much lower at 2-3 percent). Hence the title of their paper, which promises <em>nonvacuous generalization bounds.</em> What I find most interesting about this result is that it uses the power of nonconvex optimization (harnessed above to find a suitable noised distribution $Q$) to cast light on one of the metaquestions about nonconvex optimization, namely, why does deep learning not overfit!</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
<a href="http://offconvex.github.io/2017/12/08/generalization1/"><span class="datestr">at December 08, 2017 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2017/01/03/discrepancy-constructive-rw">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2017/01/03/discrepancy-constructive-rw/">Discrepancy: a constructive proof via random walks in the hypercube</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>
In this post, I'll give two constructive/algorithmic discrepancy upper bounds. The first, by Beck and Fiala, applies to sparse set systems. The second, by Lovett and Meka, improves on the Beck-Fiala result and also matches the guarantees of Spencer's theorem.
</p><p>

</p><h3 class="tex">Discrepancy Minimization</h3> Recall that, given a system of subsets of $[n]$, $\cS = S_1,\ldots,S_m \subseteq [n]$, the discrepancy of a coloring $x \in \{\pm 1\}^n$ on $\cS$ is defined to be \[ \disc(x,\cS) = \max_{S_j \in \cS} \left|\sum_{i \in S_j} x_i\right|. \] In the previous post, we proved Spencer's theorem, which says that for any $\cS$, $\min_{x}\disc(x,\cS) \le O(\sqrt{n\log\frac{m}{n}})$.
<p>
The natural associated algorithmic task is <em>discrepancy minimization</em>---given $\cS$, we want to compute \[ x^* = \argmin_{x \in \{\pm 1\}^n} \disc(x,\cS). \] Spencer's theorem guarantees that some $x$ achieving $\disc(x,\cS) \le O(\sqrt{n\log\frac{m}{n}})$ always exists, but the proof does not provide a natural algorithm for finding the discrepancy minimizer $x^*$. Actually, finding the minimizer $x^*$ is NP-hard.
</p><blockquote><b>Theorem 1 (Charikar-Newman-Nikolov)</b> <em> Given a set system $\cS$ with $O(n)$ sets, it is NP-hard to distinguish whether $\disc(\cS) = 0$ or $\disc(\cS) = \Omega(\sqrt{n})$. </em></blockquote>

<p>


</p><p>
Still, it turns out that Spencer's theorem <em>can</em> be made algorithmic---there are efficient algorithms for computing a coloring with discrepancy $O(\sqrt{n})$. The first such algorithm was given by Bansal in 2010, and it was based on semidefinite programming. Later, in 2012, Lovett and Meka gave a simplified and slightly more general version of Bansal's result. The Lovett-Meka algorithm uses some ideas from Bansal's algorithm, but it does not rely on SDPs, using instead only linear algebra and properties of random vectors.
</p><p>
I think it is more natural to see the Lovett-Meka result after seeing the simpler result of Beck and Fiala for the special case when $\cS$ is sparse, and so I will give a brief account of that algorithm first.
</p><p>
</p><h3 class="tex">Sparse set systems and Beck-Fiala</h3>
<p>
Suppose that we have a set system $\cS$ which is <em>sparse</em>, so that every item is in at most $t$ sets. In this case, we can get the following specialized bound:
</p><blockquote><b>Theorem 2 (Beck-Fiala)</b> <em> If $\cS = S_1,\ldots,S_m$ is a set system with $S_j \subseteq [n] ~ \forall j \in [m]$, and each $i\in[n]$ is only included it at most $t$ sets of $\cS$, then there is an algorithm that computes a coloring $x \in \{\pm 1\}^n$ with \[ 	\disc(x,\cS) \le 2t - 1. \] </em></blockquote>

<p>

 Beck and Fiala also conjectured that one could obtain a bound of $\disc(\cS) \le O(\sqrt{t})$ for this setting---the Beck-Fiala conjecture is a major open problem in discrepancy theory. <em>Proof:</em>  The proof is algorithmic---we'll start with the fractional coloring $x_0 = \vec{0}$, and update $x$ iteratively until we reach an integral point in $\{\pm 1\}^n$, arguing that we cannot do too much damage along the way.
</p><p>
 The algorithm is as follows. At step $k$ of the algorithm, say we have the fractional coloring $x_k \in [-1,1]^n$. We keep track of the “live” items, or items for which $|x_i| &lt; 1$. We also keep track of the “dangerous” sets: a set is called dangerous if it contain more than $t$ live items.
</p><blockquote><b>Claim 1</b> <em> 	At step $k$, if there are $n_k$ live items, then there can be at most $n_k - 1$ dangerous sets. </em></blockquote>

<p>

 This is true because each dangerous set has at least $t+1$ live items, but the maximum degree of each item is $t$, and so if we restrict the incidence matrix $A$ to the rows corresponding to dangerous sets, there are at most $n_k\cdot t$ nonzero entries, and therefore there can be at most $\lfloor\frac{t\cdot n_k}{t+1}\rfloor \le n_k-1$ dangerous sets.
</p><p>
 So, if we let $A_k$ be the restriction of the incidence matrix to live columns and dangerous rows in the $k$th step, $A_k$ is not full rank, so there must always exist some vector $y_k \in \R^{n_k}$ which is orthogonal to all rows of $A_k$, and furthermore we can find $y_k$ efficiently.
</p><p>
 Let $z_k$ be the natural extension of $y_k$ to the space of non-live items (so that $z_k(i) = y_k(i)$ if $i$ is live and $0$ otherwise). We perform the update \[ 	x_{k+1} = x_k + \alpha \cdot z_k, \] where $\alpha \in \R_+$ is chosen to be the largest number so that $x_{k+1} \in [-1,1]^n$. In other words, we start with $\alpha = 0$, and grow $\alpha$ until at least one of the entries of $x_{k+1}$ hits $1$ or $-1$. Thus the number of live items decreases by at least one, and the discrepancy of every dangerous set is $0$.
</p><p>
 Now we only have to argue that once a set $S_j$ is no longer dangerous, its discrepancy can never grow larger than $2t-1$. If $S_j$ stopped being dangerous at step $k'$, $S_j$ had at most $t$ live items in $x_{k'}$ and $\iprod{x_{k'},a_j} = 0$. In the worst case each live $i \in S_j$ can go from $x_{k'}(i) = 1-\epsilon_i$ to $x(i) = -1$, so a bound of $2t$ on the final discrepancy of $S_j$ is easy. To get $2t-1$, we just notice that because the total discrepancy of $S_j$ was $0$ at step $k'$, the sum over the live $i \in S_j$ must be integral, and for live $|x_{k'}(i)| &lt; 1$, so this gives us a lower bound of $\left|\sum \epsilon_i\right| \ge 1$. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">Constructive Spencer via guided random walks</h3>
<p>
As mentioned above, the first algorithmic proof of Spencer's result was given by Bansal in 2010. The proof was a little bit similar to the Beck-Fiala algorithm, in that it starts with the fractional coloring $x_0 = 0$, and makes updates to $x$ iteratively until hitting some integral coloring, bounding the error incurred along the way. The extreme point of departure is the manner in which the iterative updates to $x$ are chosen. Instead of choosing some arbitrary direction orthogonal to the dangerous sets, Bansal's algorithm uses a semidefinite program to take a random step---the semidefinite program makes sure that this random walk will make progress without violating discrepancy constraints too much. This makes the proof non-constructive, since to argue the feasibility of the SDP, Bansal relied on Spencer's result.
</p><p>
In 2012, Lovett and Meka simplified Bansal's approach. They removed the semidefinite programming step, returning to linear algebraic arguments reminiscent of the Beck-Fiala proof. Instead of using the SDP to guide the random walk, they argue that so long as there are not too many integral vertices, there is a high-dimensional subspace of $\R^n$ in which the random walk can proceed without violating the discrepancy constraints by too much, and then they take a random step in this subspace. This gives a truly constructive proof of Spencer's result.
</p><p>
Ignoring variations in the constants chosen, the main theorem of the paper is the following:
</p><blockquote><b>Theorem 3</b> <em> Suppose that for $\lambda \in \R^m$ with $\lambda \ge 0$, \begin{equation} 	\sum_j \exp\left(-\frac{\lambda_j^2}{32}\right) \le \frac{n}{16}.\label{cond} \end{equation} Then for any starting point $x \in [-1,1]^n$, there exists a partial coloring $x' \in [-1,1]^n$ with at least $n/2$ entries of $x$ having magnitude $1$ and $|\langle x - x', a_j\rangle | \le \lambda_j \sqrt{|S_j|}$ for all $j \in [m]$, and an algorithm that finds such an $x'$ with probability at least $1/10$. </em></blockquote>

<p>


</p><p>
First let's see that this implies Spencer's result. We'll apply the theorem recursively, like Spencer does, $T = O(\log n)$ times. We'll start with the coloring $x_0 = 0$. At the $t$th iteration of the algorithm, say we have $n_t \le n/2^{t-1}$ items uncolored. For all of $j \in [m]$, we will set $\lambda^{(t)}_j = \sqrt{32\log{\frac{m}{n_t}} + \log 16}$ (it's easy to check that this satisfies the condition of the theorem). Then we use the algorithm to find $x_t = x'$. Letting $a_j$ be the 0/1 indicator vector for $S_j$, by the triangle inequality and the guarantees of the theorem, \begin{align*} \disc(x_T, S_j) ~=~ |\langle x_T, a_j\rangle| ~\le~ \sum_{t=0}^T |\langle x_{t} - x_{t+1}, a_j \rangle| ~&amp;\le ~\sum_{t=0} \lambda_j^{(t)}\sqrt{|S_j^{(t)}|}, \end{align*} And since there are at most $n/2^{t-1}$ active items in $S_j$ at timestep $t$, \begin{align*} &amp;\le \sum_{t=0}\sqrt{n_t\log\frac{m}{n_t}} ~\le~ O(\sqrt{n\log m/n}), \end{align*} where the last inequality follows because the sequence $\frac{n_t\log(m/n_t)}{n\log(m/n)}$ decays at least as fast as $2^{-t}\log 2^t$. So, this recovers Spencer's result.
</p><p>

</p><blockquote><b>Remark 1 (Sparse set systems)</b> <em> The algorithms of Bansal and of Lovett and Meka can be generalized to give an upper bound of $O(\sqrt{t}\log n)$ for $t$-sparse set systems. If the $\lambda_j$'s are set to $\lambda_j = c \cdot \sqrt{\frac{t}{|S_j|}}$ for some constant $c$, then by Markov's inequality and by the sparsity of $\cS$ there are at most $2^{-k}n$ sets with $|S_j| \in [2^ktn,2^{k+1}tn]$, and so \[ 	\sum_{j}\exp\left(-\frac{\lambda_j^2}{32}\right) 	\le \sum_{k=0}^{\infty} \frac{n}{2^k}\cdot \exp\left( \frac{-c^2}{2^{k+1}\cdot 32}\right), \] which meets condition (\ref{cond}) of the theorem if $c$ is chosen properly, so the conclusion follows. </em></blockquote>
<p>
Now, we will prove the theorem.
</p><p>
<b>Main idea:</b> Just as Beck and Fiala do, we'll start with some point $x_0$, and update $x$ iteratively, fixing $x(i)$ the moment that $|x(i)| = 1$. We will differ in our updates---we redefine a set to be dangerous when we come close to violating the constraint $|\iprod{x_t, a_j}| \ge \lambda_j\sqrt{|S_j|}$. So unlike Beck-Fiala, by default we start with no dangerous sets, and we add sets to the dangerous list when they become too imbalanced.
</p><p>
Just like Beck-Fiala, we will only make updates orthogonal to the dangerous sets. Our updates will take the form of a random walk in the non-dangerous subspace. The trick will be to argue that by our condition (\ref{cond}) and by properties of Gaussian random walks, with reasonable probability the rank of the dangerous subspace does not become too large as long as there are still many live items to color in.

</p><p>
<em>Proof:</em>  The algorithm is as follows: Set the step size $\gamma = 1/100n^2$, and the safety margin $\delta = \gamma \cdot 10\log n$. Initialize the set of non-live items $D_v = \emptyset $ (notationally this is more convenient than keeping track of live items), and initialize the set of dangerous constraints $D_S$. Initialize the starting coloring $x_0 = x$ and the starting subspace $V_0 = \R^n$.

</p><ol> <li> For $k = 1,\ldots, K= 8/\gamma^2$:

<ol> 	<li> Sample the random vector $g_k$ by sampling $g \sim \cN(0, \Id)$ and projecting $g$ into the subspace $V_k$. 	</li><li> Take a random step by setting $x_k = x_{k-1} + \gamma \cdot g_k$. 	</li><li> For any $i \in [n]$ such that $|x_k(i)| \ge 1 - \delta$, add $i$ to $D_v$. 	</li><li> For any $j \in [m]$ such that $|\langle x_k - x_0, a_j\rangle| \ge \lambda_i\sqrt{|S_j|} - \delta$, add $S_j$ to $D_S$. 	</li><li> Set $V_{k+1}$ to be the subspace orthogonal to all $e_i$ for $i \in D_v$ and orthogonal to all $a_i$ for $S_i \in D_S$.
</li></ol>

 </li><li> If $|x_K(i)| \ge 1-\delta$, set $x'(i) = \sgn(x_K(i))$. Otherwise, set $x'(i) = x_K(i)$.
</li></ol>


<p>
Each of these steps can be done in polynomial time. Now, for proving correctness, there are several concerns: can we always assume $x_k \in [-1,1]^n$ and $|\iprod{x_k,a_j}|\le \lambda_j\sqrt{|S_j|}$, or does step (b) ever make us jump out of the box? Does the rounding in step 5 change the discrepancy of sets by too much? Will the algorithm ever get stuck in a place where we can't make progress (i.e. $V_k = \emptyset$) before coloring at least $n/2$ items?
</p><p>
The first two concerns are easy to take care of, so here are informal arguments. Since the Gaussian steps have small magnitude $\gamma$, and since we have a reasonable safety margin $\delta$ away from violating any constraint, the probability that we ever violate hard constraints in step (b) is polynomially small. The small safety margin also ensures that with high probability, the rounding we perform in step (c) cannot change the discrepancy of any set by more than $n\delta = O(1/\log n)$ over the course of the entire algorithm.
</p><p>
It remains to argue that with probability at least $1/10$, we won't get stuck before we will color at least $n/2$ items. We'll use a couple of (relatively standard) properties of Gaussian projections:
</p><blockquote><b>Claim 2</b> <em><a name="f1"></a> 	If $u\in \R^n$ and $g\in \R^n$ is a vector with i.i.d. entries $g_i \sim \cN(0,\sigma^2)$, then $\iprod{g,u}\sim \cN(0,\sigma^2\|u\|_2^2)$. </em></blockquote>

<p>


</p><p>

</p><blockquote><b>Claim 3</b> <em><a name="f2"></a> 	If $g\in \R^n$ is a vector with i.i.d. entries $g_i \sim \cN(0,\sigma^2)$, and $g'$ is the orthogonal projection of $g$ into a subspace $S \subseteq \R^n$, then $\E[\|g'\|_2^2] = \sigma^2\cdot \dim(S)$. </em></blockquote>

<p>


</p><p>

</p><blockquote><b>Claim 4</b> <em><a name="f3"></a> 	If $u\in \R^n$ and $g\in \R^n$ is a vector with i.i.d. entries $g_i\sim\cN(0,\sigma^2)$, and $g'$ is the orthogonal projection of $g$ into a subspace $S \subseteq \R^n$, then $\iprod{g',u}\sim \cN(0,\alpha\|u\|_2^2)$ where $\alpha \le \sigma^2$. </em></blockquote>

<p>

 The proof of the first claim follows from the additive property of Gaussians. The second and third claims can be proven using the first claim, by considering an orthogonal projection matrix into the subspace $S$.
</p><p>
Now, we are equipped to prove the rest of the theorem. We first relate the progress of the algorithm, as measured by the variance of the Gaussian steps, to the dimension of $V_k$. By the independence of the gaussians $g_k$, have that \begin{align*} \E[\|x_{K} - x_0\|_2^2] ~=~ \E\left[\left\|\gamma \cdot \sum_{k=1}^K g_k\right\|_2^2\right] &amp;= \gamma^2 \cdot \sum_{k=1}^K \E\left[\left\|g_k\right\|_2^2\right]\\ &amp;= \gamma^2 \sum_{k=1}^K \E[\dim(V_k)] \qquad \\ &amp;\ge~ \gamma^2 K \cdot \E[\dim(V_K)], \end{align*} where the second line follows from Claim <a href="http://learningwitherrors.org/f2">2</a> and the last line is because the dimension of $V_k$ decreases with $k$. Since $\dim(V_K) \ge n - |D_v| - |D_S|$, \begin{align*} &amp;\ge 8 \E[n - |D_v| - |D_S|]. \end{align*} On the other hand, $\E[\|x_{K} - x_0\|_2^2] \le 2n$, because we stop moving in the direction of items once the coordinate magnitude is close to $1$. Thus, \begin{align} 2n &amp;\ge 8 (n - \E[|D_S|] - \E[|D_v|],\nonumber\\ \E[|D_v|] &amp;\ge \frac{3}{4}n - \E[|D_S|],\label{dsbd} \end{align}
</p><p>
Now, all that remains for us to do is argue that the expected number of dangerous sets is not too large---if you like, we are arguing that we don't arrive at $V_k = \emptyset$ before coloring in enough vertices. Recall a set $S_j$ is in $D_S$ only if for some $k$, $|\langle x_0 - x_k, a_j\rangle| \ge \lambda_i\sqrt{|S_j|} - \delta$. Since we only move orthogonal to $a_j$ for $S_j \in D_S$, it suffices to count the number of $S_j$ which are dangerous at the final step when $k = K$.
</p><p>
 Let $J \subseteq [m]$ be the set of $j \in [m]$ for which $\lambda_j\sqrt{|S_j|} \ge 2\delta$. By Claim <a href="http://learningwitherrors.org/atom.xml#f2">3</a>, $x_0 - x_K$ is a Gaussian vector supported on $\R^n$ with expected square norm at most $K \cdot \gamma^2 \le 8$, and $a_j$ is a vector of norm $\sqrt{S_j}$. Now, although $x_0-x_K$ is not exactly the orthogonal projection of a Gaussian vector into a subspace of $\R^n$, we can more or less apply Claim <a href="http://learningwitherrors.org/atom.xml#f3">4</a> to<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> If we want to be rigorous, we should break up $x_0 - x_K$ into the independent Gaussian increments $x_k - x_{k+1}$ and apply Claim <a href="http://learningwitherrors.org/atom.xml#f3">4</a> to each of them, then look at their sum. </span> conclude that $\langle x_0-x_K, a_j\rangle$ is distributed as a Gaussian with variance at most $8|S_j|$. Therefore for sets $j \in J$, \[ \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \lambda_j\sqrt{|S_j|} - \delta\right] \le \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \frac{1}{2}\lambda_j\sqrt{|S_j|}\right] \le 2\exp\left(-\frac{\lambda_j^2}{32}\right). \] By condition (\ref{cond}) of the theorem, there are at most $n/16$ sets in $[m]\setminus J$, or sets with $\lambda_j\sqrt{|S_j|} &lt; 2\delta$, since for each such set $\exp(-\lambda_j^2/32) \ge n^{O(1/n^4)} \approx 1$. So the expected number of sets for which $|\langle x_0 - x_K, a_j\rangle| \ge \lambda_j\sqrt{|S_j|} -\delta $ is at most \begin{align*} \E[|D_S|] \le \frac{n}{16} + \sum_{j\in J} \Pr\left[|\langle x_0 - x_K, a_j\rangle| \ge \frac{1}{2}\lambda_j\sqrt{|S_j|} \right] \le \frac{n}{16} + 2\cdot \sum_{j \in J} \exp\left(-\frac{\lambda_i^2}{32}\right) \le \frac{3}{16}n,  \end{align*} where for the last inequality we apply condition (\ref{cond}) of the theorem.
</p><p>
Now, plugging back into (\ref{dsbd}), \begin{align*} \E[|D_v|] \ge \left(\frac{3}{4} - \frac{3}{16}\right)n = \frac{9}{16}n. \end{align*} Let $p$ be the probability that $|D_v|$ has fewer than $n/2$ colored items. Since there can be at most $n$ colored items, \[ \frac{9}{16}n \le \E[|D_v|] &lt; (1-p) \cdot n + p\cdot \frac{n}{2} = \left(1-\frac{p}{2}\right)n. \] From this we have that $p &lt; 7/8$, and so by the union bound the algorithm succeeds with probability at least $1/8 - o(1)$. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">More sources</h3>
The <a href="https://arxiv.org/abs/1203.5747">paper</a> of Lovett and Meka, as well as the previously mentioned <a href="http://www.win.tue.nl/~nikhil/pubs/author%20-nikhil-2.pdf">chapter</a> of Nikhil Bansal are good resources.
The original algorithmic result can be found in <a href="https://arxiv.org/abs/1002.2259">this</a> paper of Bansal.
<p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Tselil Schramm <a href="http://learningwitherrors.org/2017/01/03/discrepancy-constructive-rw/"><span class="datestr">at January 03, 2017 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/12/26/discrepancy-spencer-six">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/12/26/discrepancy-spencer-six/">Discrepancy: definitions and Spencer's six standard deviations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>  This is a first in a series of (probably at least 3) blog posts about discrepancy minimization. There are already many expositions of this topic, and it is not clear that the world needs yet another, but here it is :). In this post, I'll introduce the basic definitions, give one simple upper bound, one simple lower bound, and then prove Spencer's famous (and elegant) “six standard deviations” theorem. I won't focus on mathematical context, but at the end I will give pointers to some resources on the topic.



<p>
</p><h3 class="tex">Discrepancy</h3>
<p>
Suppose we have $n$ items, and a set of subsets of $[n]$, $\mathcal{S} = S_1,\ldots,S_m$ with $S_j \subseteq [n]$ for all $j \in [m]$. We want to assign each item $i \in [n]$ a sign or “color” $x_i \in \{\pm 1\}$, with the goal that the coloring of each set is as balanced as possible---formally, this is called minimizing the <em>discrepancy</em>. So for a coloring $x \in \{\pm 1\}^n$, the discrepancy or imbalance of $S \subseteq [n]$ is given by \[ \disc(x, S) = \left|\sum_{i \in S} x_i \right|, \] We define the discrepancy of $\cS$ to be the discrepancy of the worst set in $\cS$ under the most balanced coloring, \[ \disc(\cS) = \min_{x \in \{\pm 1\}^n} \max_{S_j \in \cS}~ \disc(x,S_j). \]
</p><p>
It can also be convenient to think of this as a matrix problem. Consider the incidence matrix $A$ of $\cS$, the $m \times n$ matrix with \[ A_{ji} = \begin{cases} 1 &amp; i \in S_j\\ 		 0 &amp; \text{otherwise}. 		 \end{cases} \] Then letting $a_j$ be the 0/1 indicator vector for set $S_j$, or the $j$th row of $A$, the discrepancy of $x \in \{\pm 1\}^n$ on $S_j$ is equal to $|\iprod{a_j,x}|$, and the discrepancy of $\cS$ is equivalent to \[ \disc(\cS) = \min_{x \in \{\pm 1\}^n} \|Ax\|_{\infty}. \]
</p><p>
</p><h3 class="tex">Some cursory upper and lower bounds</h3> A priori, it might not be clear what $\disc(\cS)$ should be. Some set systems have discrepancy zero---consider for example a “planted” set system, in which the rows of the incidence matrix $A$ are chosen to be orthogonal to some coloring $x\in \{\pm 1\}^n$. On the other hand, naively we could have set systems with discrepancy as large as $n$.
<p>
</p><h3 class="tex">Uniformly Random Coloring</h3> It is not very hard to see that $n$ is an egregious upper bound (when $m = |\cS|$ is not too large). Let's consider $x \in \{\pm\}^n$ chosen uniformly at random. For each $S_j \in \cS$, by Hoeffding's inequality, we have that \[ \Pr\left[\left|\sum_{i \in S_j} x_i \right| \ge t \sqrt{n} \right] \le 2\exp\left(-\frac{t^2}{2}\right). \] So, if we set $t = 2\sqrt{\log m})$, we can beat the union bound over the sets: \[ \Pr[\|Ax\|_{\infty} \ge t\sqrt{n}] \le \sum_{j \in [m]} \Pr\left[\left|\sum_{i \in S_j} x_i \right| \ge t\sqrt{n} \right] \le m \cdot 2\exp(-4 \log m) = O\left(\frac{1}{m^3}\right). \] So for any set system $\cS$, the random coloring gives the improved bound, \begin{equation} \disc(\cS) \le O(\sqrt{n\log m}).\label{randomub} \end{equation}
<p>
</p><h3 class="tex">A Lower Bound</h3> We'll see that the upper bound we got from the random coloring is almost tight. Our lower bound will come from the set system defined by the Hadamard matrix $H$---for us, it will be enough to know that the Hadamard matrix is a symmetric matrix with entries in $\{\pm 1\}$, the first column (and therefore row) of $H$ is the all-1's vector $\vec{1}$, and the columns are mutually orthogonal so that $HH^{\top} = n\cdot \Id$.
<p>
The basic idea for the lower bound is that, because $H$ has large eigenvalues, it cannot map any Boolean vector into a ball of small radius, giving us large discrepancy. But $H$ has negative entries, so in the process of translating it into a valid incidence matrix with entries in $\{0,1\}$ we have to make sure we didn't introduce a small eigenvalue in the direction of some $x \in \{\pm 1\}^n$.
</p><p>
The proof is only a couple of lines. Let $J$ be the all-$1$s matrix. We can define the set system $\cS$ that has an incidence matrix $A = \frac{1}{2}(J+H)$---this is valid, because $A$ is a $0/1$ matrix. Now, for any $x \in \{\pm 1\}^n$, we have that \begin{align*} n \cdot \|Ax\|_{\infty}^2 ~\ge~\|Ax\|_2^2 ~=~ x^{\top}A^{\top} A x ~=~\frac{1}{4} x^{\top} \left(H^{\top} H + J^{\top} J + J^{\top} H + H^{\top} J\right)x. \end{align*} To simplify, we can observe that $H^{\top}H = n\cdot \Id$, that $J^{\top} J = n\cdot J$. Also, because the first row/column of $H$ is equal to $\vec{1}$, and because the rows of $H$ are orthogonal, $H\vec{1} = n \cdot e_1$, and so $JH^{\top} = n \cdot \vec{1}e_1^{\top}$. So, \begin{align*} x^{\top} A^{\top}Ax ~=~ \frac{n}{4} \cdot x^{\top}\left( \Id + J + e_1 \vec{1}^{\top} + \vec{1}e_1^{\top}\right) x ~=~ \frac{n}{4}\left(n + \iprod{x,\vec{1}}^2 + 2\cdot x_1 \cdot \iprod{x,\vec{1}}\right) \end{align*} Because $A$'s first row is $\vec{1}$ we have that $|\iprod{x,\vec{1}}| &gt; \sqrt{n} \implies \|Ax\|_{\infty} \ge \sqrt{n}$. And otherwise if $|\iprod{x,\vec{1}}| &lt; \sqrt{n}$, plugging in to the above we have that \begin{align*} n\cdot \|Ax\|_{\infty}^2 ~\ge~ x^{\top} A^{\top} A x ~&gt;~ \frac{n}{4}\left(n - 2\sqrt{n}\right) \quad \implies \quad \|Ax\|_{\infty} \ge O(\sqrt{n}). \end{align*} So for the Hadamard set system $\cS$, \begin{equation} \disc(\cS) \ge O(\sqrt{n}).\label{lb} \end{equation}
</p><p>
</p><h3 class="tex">Spencer's Theorem</h3>
<p>
It turns out that in fact the lower bound from (\ref{lb}) is tight up to constant factors, and we can get rid of the logarithmic factor from the upper bound (\ref{randomub}). In the 1980's, Spencer proved the following result:
</p><blockquote><b>Theorem 1</b> <em> For any set system $\cS$ on $[n]$ with $|\cS| = m$, \[ 	\disc(\cS) \le 6\sqrt{n\log\frac{m}{n}} \] </em></blockquote>

<p>

 Maybe improving on the logarithmic factor in (\ref{randomub}) does not seem like a big deal, but a priori it is not obvious that one should be able to get a bound better than the random assignment. Also, Spencer's proof is extremely elegant (though nonconstructive). We'll prove it below, but we'll assume that $m = n$, and we'll be sloppy with constants (so we'll get an upper bound of $O(\sqrt{n})$ instead of $6\sqrt{n}$).
</p><p>

</p><p>
<b>The gist.</b> The proof is by induction---given $\cS$ and $[n]$, Spencer shows that there exists some <em>partial coloring</em> $y \in \{-1,0,1\}^n$ with at least a constant fraction of the elements colored in and low discrepancy, so that $\|y\|_1 \ge c\cdot n$ and $\|Ay\|_{\infty} \le C\sqrt{n}$ for constants $c,C$. Then this fact is applied inductively for $\log n$ steps, until all of the items are colored, and the total discrepancy is $\sum_{t=0}^{\log n} C\cdot \sqrt{c^t\cdot n} = O(\sqrt{n})$, since $c &lt; 1$.
</p><p>
The reason such a partial coloring $y$ must exist is because the map $Ax$ is not spread out enough---by the pigeonhole principle, one can show that there are at least $2^{\Omega(n)}$ distinct points $x \in \{\pm 1\}^n$ that get mapped to a ball of radius $O(\sqrt{n})$. Since there are so many points, there must exist $x_1,x_2$ in this ball that have large Hamming distance, so that their difference has many nonzero entries, and so the partial coloring is given by $y = \frac{1}{2}(x_1-x_2)$.
</p><p>
Now for the more formal proof. We will prove the following lemma, which we will then apply inductively:
</p><blockquote><b>Lemma 2</b> <em> Let $m,n \in \N$ with $n \le m$, and let $A$ be an $m \times n$ matrix with $0/1$ entries. Then there exist universal constants $c_1,c_2$ such that there always exists a vector $y \in \{-1,0,1\}^n$ so that $\|y\|_1 \ge c_1 \cdot n$ and \[ \|Ay\|_{\infty} \le c_2\cdot \sqrt{n\log\frac{m}{n}}. \] </em></blockquote>

<p>


</p><p>
<em>Proof:</em>  Let $\cB_{\infty}(r,p)$ denote the ball of radius $r$ around $p$, where distance is measured in $\ell_\infty$. We'll show that there must exist some point $q \in \R^m$ such that \[ 	\Pr_{x\sim\{\pm 1\}^n}[ Ax \in \cB_{\infty}(r,q)] \ge 2^{-cn}, \] for some constant $c$. Our strategy will be to use the pigeonhole principle. We'll identify a set $B \subset \R^m$ with $|B| \le 2^{\epsilon n}$, so that for uniformly chosen $x \in \{\pm 1\}^n$, there exists a point in $B$ close to $Ax$ with probability at least $\frac{1}{2}$; because $|B| &lt; 2^{\epsilon n}$, there must be some $q \in B$ which is near at least $2^{(1-\epsilon)n}$. To find such a $B$, which contains few points but is close to $Ax$ with constant probability over uniform $x \in \{\pm 1\}^n$, we'll choose $B$ to be a discretization of the set of points in $\R^m$ that do not have too many entries of large magnitude. The way that we define “large magnitude” will partially depend on the standard deviation of entries of $Ax$, and partly on wanting to keep $B$ relatively small.
</p><p>
 Define the function $f:\{\pm 1\}^n \to \Z^m$ so that \[ 	f(x) = \left\lceil \frac{1}{\sqrt {2n\log \frac{m}{n}}} Ax\right\rceil. \] In words, $f$ maps $x$ to the integral point closest to $Ax/\sqrt{n\log \frac{m}{n}}$.
</p><p>
We next identify some small subset $B \subset \Z^m$ which contains a large fraction of the range of $f$, which will imply that there must exist some $q \in \sqrt{n\log\frac{m}{n}}\cdot B$ which is close to $Ax$ for many $x \in \{\pm 1\}^n$. Define $B \subset \Z^m$ to be the set for which at most a $\kappa_t = 2^{t+2} (m/n)^{-t^2}$-fraction of coordinates are larger than $t$, \[ 	B = \left\{(b_1,\ldots,b_m) \in \Z^m ~|~ |\{b_i ~s.t.~ |b_i| \ge t\}| \le \kappa_t \cdot m\right\}. \] For uniformly chosen $x$, by Hoeffding's inequality, \[ 	\Pr\left[ |\iprod{x,a_j}| \ge t \sqrt{2n\log\frac{m}{n}} \right]\le 2\left(m/n\right)^{-t^2} \] And so the expected number of $j \in [m]$ for which $|\iprod{x,a_j}| \ge t\sqrt{n\log\frac{m}{n}}$ is at most \[ 	\E\left[\sum_{j\in[m]} \Ind\left(\left|\iprod{x,a_j}\right| \ge t\sqrt{2n\log\frac{m}{n}}\right) \right] \le m \cdot 2\left(\frac{m}{n}\right)^{-t^2}, \] And by Markov's inequality \begin{align} 	\Pr\left[\sum_{j\in[m]} \Ind\left(\left|\iprod{x,a_j}\right| \ge t\sqrt{2n\log\frac{m}{n}}\right) \ge 2^{t+1} m \cdot \left(\frac{m}{n}\right)^{-t^2}\right] 	&amp;\le \frac{1}{2^{t+1}}.\label{eq:cond} \end{align} Recall that $\kappa_t = 2^{t+1} \left(\frac{m}{n}\right)^{-t^2}$. Thus, for any $x$, the probability that $f(x) = \lceil (n\log\frac{m}{n})^{-1/2} \cdot Ax\rceil \not\in B$ is the sum over (\ref{eq:cond}) for all $t \le \sqrt{n}$, and so by a union bound, \begin{align*} 	\Pr[f(x) \not \in B] 	~=~ \Pr\left[\exists t ~s.t.~ \sum_{j \in [m]} \Ind\left(|\iprod{x,a_j}| \ge t\sqrt{n\log\frac{m}{n}}\right) \ge \kappa_t \cdot m\right] 	~\le~ \sum_{t=1}^{\sqrt{n}} \frac{1}{2^{t+1}} 	~\le~ \frac{1}{2}. \end{align*}
</p><p>
 At the same time, the size of $B$ can be bounded with some meticulous but uncomplicated counting arguments---we won't reproduce them at full resolution here, but the basic idea is that if we consider a point in $B$, it should have at most $\alpha_t \le \kappa_t$ entries of value $\pm t$. So for any valid sequence $\alpha = \alpha_1,\ldots,\alpha_n \le \kappa_1,\ldots,\kappa_n$, we have at most \[ 	\prod_{t=1}^{n}2^{\alpha_t m} \cdot \binom{\left(1 - \sum_{s &lt; t}\alpha_s\right)\cdot m}{\alpha_t \cdot m} \] points, and then summing over all valid $\alpha$, \begin{align*} 	|B| 	&amp;\le \sum_{\alpha} 	\prod_{t=1}^{n}2^{\alpha_t m} \cdot \binom{\left(1 - \sum_{s &lt; t}\alpha_s\right)\cdot m}{\alpha_t \cdot m}. \end{align*} After applying a number of rearrangements and approximations, by our choice of $\kappa_t$'s one can conclude that \[ 	|B| \le 2^{cn}, \] for some constant $c &lt; 1$.
</p><p>

</p><p>
 Since $|B| \le 2^{cn}$ but $f(x) \in B$ for at least $2^{n-1}$ points, it follows that there must exist some $q \in B$ such that $f$ maps at least $2^{n(1-c) -1}$ of the $x \in \{\pm 1\}^n$ to $q$. If $f(x) = p/\sqrt{2n\log \frac{m}{n}}$, then by definition $x \in \cB(\sqrt{2n\log \frac{m}{n}}, p)$. So we have that \[ 	\Pr\left[Ax \in \cB_{\infty}\left(\sqrt{2n\log\frac{m}{n}}, \sqrt{2n\log\frac{m}{n}}\cdot q\right)\right] \ge 2^{-cn}. \]
</p><p>
The proof is now complete if we observe that any subset $C \subset \{\pm 1\}^n$ with $|C| \ge 2^{cn}$ must have two points at hamming distance at least $\Omega(n)$. This is by a theorem of Kleitman, but it is not hard to see. The idea is that, if we choose a single point $p \in C$, the number of points around it of Hamming distance at most $2\epsilon n$ is \[ \sum_{k=1}^{cn} \binom{n}{k} \le 2^{H(\epsilon)n}, \] where $H(\cdot)$ is the binary entropy function (this is a standard upper bound for a partial sum of binary coefficients). So if $|C| \ge 2^{ H(\epsilon)\cdot n}$, it must contain at least two points of Hamming distance at least $2\epsilon n$.
</p><p>
Since there are at least $2^{(1-c)n}$ points in $\{\pm 1\}^n$ so that $Ax \in \cB_{\infty}(\sqrt{n},q)$, there must be two points $x_1,x_2$ such that $\|x_1-x_2\|_1 \ge 2H^{-1}(1-c)\cdot n$ and $\|Ax_1 - Ax_2\|_{\infty} \le \|Ax_1 - q\|_{\infty} + \|q - Ax_2\|_{\infty} \le 2\sqrt{n}$. Setting $y = \frac{1}{2}(x_1-x_2)$, $c_1 = H^{-1}(1-c)$ and $c_2 = \sqrt{2}$, the conclusion holds. $$\tag*{$\blacksquare$}$$
</p><p>
Now, we are ready to prove Spencer's Theorem.
</p><p>
<em>Proof:</em>  We will apply our lemma recursively, coloring in a $c_1$-fraction of the remaining items at a time. After each partial coloring, we update $A$ by removing columns corresponding to colored items, so that at the $t$th step, there are at most $c_1^t\cdot n$ columns in $A$. There can be at most $\log n/\log c_1$ rounds of partial coloring, and at the $t$th round we can incur a discrepancy of at most $c_2\sqrt{c_1^t n \log\frac{m}{c_1^t n}}$ in each set. Thus the total discrepancy is at most \begin{align*} 	\disc(\cS) 	&amp;\le \sum_{t=0}^{O(\log n)}c_2\sqrt{c_1^t n \log\frac{m}{c_1^t n}}\\ 	&amp;\le c_2\sqrt{n}\cdot \sum_{t=0}^{\infty}c_1^t\left(\log\frac{m}{n} + t\log \frac{1}{c_1} \right)^{1/2} 	\end{align*}
And since $(x+y)^{1/2} \le x^{1/2} + y^{1/2}$ for $x,y \ge 0$,
\begin{align*}
&amp;\le O\left(\sqrt{n\log\frac{m}{n}}\right)\cdot \sum_{t=0}^{\infty}c_1^{t/2} 	+ O\left(\sqrt{n}\right)\sum_{t=0}^{\infty} c_1^{t/2}\cdot t^{1/2}\\ 	&amp;\le O\left(\sqrt{n\log\frac{m}{n}}\right), \end{align*} and the conclusion follows. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h2 class="tex">More sources</h2> There are many very good expositions of discrepancy results.
For this post I heavily relied on:

<ul> <li> Joel Spencer's 1985 <a href="http://www.ams.org/journals/tran/1985-289-02/S0002-9947-1985-0784009-0/">paper</a> with the six standard deviations result.
	</li><li> Nikhil Bansal's <a href="http://link.springer.com/chapter/10.1007/978-3-319-04696-9_6">book chapter</a> about algorithmic discrepancy.
	    Full text available <a href="http://www.win.tue.nl/~nikhil/pubs/author%20-nikhil-2.pdf">here</a> at the time of writing.
</li></ul>

 These references contain pointers to other good resources, especially books, which give a more detailed account of the mathematical/historical context of discrepancy minimization.
<p>
Also, see the proof in Alon and Spencer's “The Probabilistic Method” which is based on entropy and is really clean.
</p><p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Tselil Schramm <a href="http://learningwitherrors.org/2016/12/26/discrepancy-spencer-six/"><span class="datestr">at December 26, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/08/24/uniform-direct-product">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/08/24/uniform-direct-product/">Constructive Hardness Amplification via Uniform Direct Product</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>
<em>This post was motivated by trying to understand the recent paper “Learning Algorithms from Natural Proofs”, by Carmosino-Impagliazzo-Kabanets-Kolokolova [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>]. They crucially use the fact that several results in hardness amplification can be made constructive. In this post, we will look at the Uniform Direct Product Theorem of Impagliazzo-Jaiswal-Kabanets-Wigderson [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>]. We will state the original theorem and algorithm of [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], then we will present a simpler analysis for a (weaker) non-uniform version of their algorithm, which contains some of the main ideas. </em>
</p><p>
For a given function $f: \{0, 1\}^n \to \{0, 1\}^\ell$, say a circuit $C$ “$\eps$-computes $f$” if $C$ computes $f$ correctly on at least $\eps$-fraction of inputs. That is, $\Pr_x[C(x) = f(x)] \geq \epsilon$. We are interested in the following kind of direct product theorem (informally): “If function $f$ cannot be $\eps$-computed by any small circuit $C$, then the direct-product $f^{\ox k}(x_1, x_2, \dots x_k) := (f(x_1), f(x_2), \dots, f(x_k))$ cannot be computed better than roughly $\eps^k$ by any similarly small circuit.” <sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a>  If this seems trivial, consider the $k=2$ case. We want to show that if $\Pr_x[C(x) = f(x)] \leq \epsilon$ for all small circuits $C$, then $\Pr_{x, y}[C'(x, y) = (f(x), f(y))] \lesssim \eps^2$ for all similarly small circuits $C'$. This is clearly true if the circuit $C'$ operates independently on its inputs, but not as clear otherwise (eg, the correctness of $C'$-s two outputs could be highly correlated). Indeed, proofs of the direct-product theorem take advantage of this correlation.  </span>
</p><p>
This is usually proved<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span id="footnote2" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> See the last section for good references to prior proofs. </span> in contrapositive, by showing: If there exists a circuit $C'$ that $\eps^k$-computes $f^{\ox k}$, then there exists a similarly-sized circuit $C$ that $\eps$-computes $f$. The very interesting part is, this amplification can be made fully constructive, by a simple algorithm.

</p><blockquote><b>Theorem 1 ([<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], and Theorem 4.1 [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>])</b> <em> <a name="thmuniformDP"></a> Let $k \in \N, \eps &gt; 0$. There is a (uniform) PPT algorithm $\A$ with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^{\ox k}$ for some function $f:\{0,1\}^n \to \{0, 1\}^\ell$. </li><li> <b>Output:</b> With probability $\Omega(\eps)$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</li></ul>

 for $\delta = O(\log(1/\eps)/k)$. In particular, $(1-\delta) = \eps^{O(1/k)}$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


</p><p>
Note that we can only hope to construct the good circuit with probability $\Omega(\eps)$, since unique decoding is impossible: the circuit $C'$ may $\eps$-compute up to $(1/\eps)$ different functions $f$ (agreeing with a different function on each $\eps$-fraction of its inputs).
</p><p>
</p><h2 class="tex">1. Uniform Version </h2>
<p>
The algorithm for Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> is: </p><div class="framed"> $\mathcal{A}(C')$:
<p>
Input: A circuit $C'$ that $\eps$-computes the direct-product $f^{\ox k}$.

</p><ol> <li> Pick $k$ iid random inputs $x_i \in \{0, 1\}^n$, let $\vec b = (x_1, \dots, x_k)$, and evaluate $C'(\vec b)$. </li><li> Pick a random subset $A \subset \{x_1, \dots, x_k\}$ of size $k/2$. Record $v := C'(\vec b)|_A$ as the answers of $C'$ on the inputs in $A$. </li><li> Output the circuit $C_{A, v}$ defined below (with the values $v$ on the subset $A$ hardcoded).
</li></ol>

 </div>
 <p></p><p>
$C_{A, v}$ is defined as the randomized circuit:
 </p><div class="framed"> $C_{A, v}(x)$:
<p>
On input $x \in \{0, 1\}^n$, check if $x \in A$, in which case output $v|_x$ (the hardcoded value of $x$ according to $v$). Otherwise, repeat the following $T = O(\log(1/\delta)/\epsilon)$ times.

</p><ol> <li> Sample $(k/2 - 1)$ additional iid random strings $\{y_j\}$, each $y_j \in \{0, 1\}^n$, and let $\vec b := (x, A, \{y_j\})$ be the tuple of $k$ strings. </li><li> Evaluate $C'(\pi(\vec b))$ for a random permutation $\pi$ of the $k$ inputs. </li><li> If the answers of $C'$ restricted to $A$ agree with the hardcoded values $v$, then output $C'(\pi(\vec b))|_x$, (the answer $C'$ gave for $x$), and stop.
</li></ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p></p><p>
<b>Intuition:</b> Suppose the values $v$ returned when the Algorithm queries $C'(b)$ are actually correct. That is, $v|_x = f(x)$ for all $x \in A$. Then, the circuit $C_{A, v}$ evaluates $C'$ on input $\vec b = (b_1, \dots b_k)$, and it knows the correct value of $f(b_i)$ is on half of these coordinates. So, $C_{A, v}(x)$ tries to estimate whether a random point $C'(\vec b)$ is correct or not, based on if it agrees on the known subset of coordinates. The idea is that a value of $C'(\vec b)$ that is wrong on many coordinates is unlikely to pass this test. (See [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] for the full proof).
</p><p>
Now, in the remainder of this note, we will develop and prove a simpler (weaker) version.
</p><p>
</p><h2 class="tex">2. Symmetrizing </h2> The direct-product as defined above has a permutation symmetry: \[ f^{\ox k}(\pi(x_1, \dots x_k)) = \pi(f^{\ox k}(x_1, \dots x_k)) \] for any permutation $\pi$.
<p>
The algorithm of Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> strongly takes advantage of this symmetry (indeed, the algorithm would not work as promised if we omitted the random permutations).<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span id="footnote3" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a> Consider a $C'(x_1, \dots x_k)$ that is correct if $x_1$ lies in some $\eps$-density set, and random otherwise. Without the random permutations, $C_{A, v}(x)$ will always evaluate $C'(x, \dots)$, and produce no output for $(1-\epsilon)$-fraction of inputs $x$. </span> To simplify presentation, it helps to define the direct-product $f^k$ as a function over $k$-<b>multisets</b> of inputs, instead of over $k$-tuples of inputs. Following [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], for the remainder of this note, we will work in the setting of $k$-multisets, and denote the $k$-multiset direct product as $f^k$. That is, $f^k$ takes as input an (unordered) $k$-multiset $B = \{x_1, x_2, \dots, x_k\}$, and returns the $k$-tuple \[f^k(\{x_1, x_2, \dots, x_k\}) := (f(x_1), f(x_2), \dots, f(x_k))\]
</p><p>
We consider the probability measure induced by the uniform measure over tuples. That is, “pick a random $k$-multiset of $U$” means to generate a multiset by picking $k$ iid random elements from the universe $U$, and forming the (unordered) multiset containing them.<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote4">4</a></sup><span id="footnote4" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote4" name="footnote4">4.</a> So for example, for $k=3$ the multiset $\{a, a, a\}$ has lower probability of being drawn than $\{a, a, b\}$ for $a \neq b$. </span>
</p><p>
The notion of $\eps$-computing remains the same:<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote5">5</a></sup><span id="footnote5" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote5" name="footnote5">5.</a> For our purposes, having a randomized circuit that $\eps$-computes $f^{\ox k}$ is essentially equivalent to having a randomized circuit that $\eps$-computes $f^k$. The proofs will extend to randomized circuits, where we say $C$ $\eps$-computes $f$ if $\Pr_{C, x}[C(x) = f(x)] \geq \eps$, taken over randomness of $C$ as well as $x$. </span> A circuit $C'(B)$ $\epsilon$-computes $f^k$ if \[\Pr_{B \sim \text{random $k$-multiset}}[C'(B) = f^k(B)] \geq \epsilon\] Note that $C'$ is allowed to give different answers for the same element in a multiset, e.g. if $C'(\{a, a, a\}) = (y_1, y_2, y_3)$, the $y_i$s may all be distinct -- we don't take advantage of this symmetry.
</p><p>
</p><h2 class="tex">3. Oracle Version </h2> Here we present and prove a simpler version of the algorithm, in the case when we also have access to an oracle for $f$. (This can be seen as a non-uniform version).
<blockquote><b>Theorem 2</b> <em> <a name="thmoracle"></a> Let $k \in \N, \eps &gt; 0$, and $f:\{0,1\}^n \to \{0, 1\}^\ell$. There is a PPT algorithm $\A^f$ with oracle access to $f$, with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^k$. </li><li> <b>Output:</b> With probability $0.99$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</li></ul>

 for $\delta = O(\log(k)/(\eps k))$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


</p><p>
The idea is, in Step 2 of Algorithm $\A$, we can generate the correct values $v$ for the inputs in set $A$, by querying the oracle. That is, we set $v := f(A)$ directly, instead of using our approximate circuit $C'$. In fact, if we have a perfect oracle for $f$ we can simplify the algorithm even further.
</p><p>
The algorithm is: </p><div class="framed"> $\mathcal{A}^f(C')$:
<p>


</p><ol> <li> Pick $T = O(\log(k)/\epsilon)$ random $(k-1)$-multisets $A_1, \dots A_T$, each $A_i$ containing $(k-1)$ random inputs from $\{0, 1\}^n$. </li><li> Query the $f$-oracle, and record the values of $v_{A_i} := \{f(x): x \in A_i\}$ for all sets $A_i$. </li><li> Output the circuit $C_{A, v}$ defined below (with the values $v_{A_i}$ on the subsets $A_i$ hardcoded).
</li></ol>

 </div>
 <p></p><p>
$C_{A, v}$ is defined as the circuit:<br />
 </p><div class="framed"> $C_{A, v}(x)$:
<p>
 For each $i = 1 \dots T = O(\log(k)/\epsilon)$:

</p><ol> <li> Let $B_i := \{x\} \cup A_i$. </li><li> Evaluate $C'(B_i)$. </li><li> If the answers of $C'(B_i)$ restricted to $A_i$ agree with the hardcoded values $v_{A_i} = f(A_i)$, then output $C'(B_i)|_x$, (the answer $C'$ gave for $x$), and stop.
</li></ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p></p><p>
<em>Proof of Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>:</em>
</p><p>
<b>Parameters:</b> We will have $\delta = 10000\log(k)/(\epsilon k)$ and $T = 100 \log(k)/ \epsilon$. (Think of aiming for $\delta \approx 1/k$).
</p><p>
We will argue that \begin{equation} \label{eqn:main} \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)] \leq \delta / 100 \end{equation} Where the probability is over the randomness of algorithm $\A^f$ (random choice of sets $A_i$), and random input $x \in \{0, 1\}^n$. Then, by Markov \[\Pr_{\A}\left[ \Pr_{C, x}[C_{A, v}(x) \neq f(x)] &gt; \delta \right] \leq 1/100\] so the algorithm $\A^f$ will produce a good circuit $C_{A, v}$ except with probability $1/100$.
</p><p>
In the execution of circuit $C_{A, v}(x)$, let us say “iteration $i$ fails” if Step 3 of the circuit at iteration $i$ outputs a wrong answer. That is, iteration $i$ fails if $C'(B_i)$ is correct on the $(k-1)$ values in $A_i = B_i \setminus \{x\}$, but wrong on $x$.
</p><p>
Consider the probability that iteration 1 fails. Notice that the distribution of $(x, A_1, B_1)$ is equivalently generated as:
</p><p>
</p><table align="center"><tbody><tr><td align="center"> $\{(x, A_1, B_1)\}$ </td><td align="center"> $\equiv$ </td><td align="center"> $\{(x, A_1, B_1)\}$ </td></tr><tr><td align="center"> $A_1 \sim$ random $(k-1)$-multiset </td><td align="center"> </td><td align="center"> $B_1 \sim$ random $k$-multiset </td></tr><tr><td align="center"> $x \in \{0, 1\}^n$ </td><td align="center"> </td><td align="center"> $x \in B_1$</td></tr><tr><td align="center"> $B_1 := \{x\} \cup A_1$ </td><td align="center"> </td><td align="center"> $A_1 := B_1 \setminus \{x\}$ </td></tr></tbody></table>
<p>
That is, we can think of first sampling a random $k$-multiset $B_1$, then sampling a random $x \in B_1$. Iteration 1 only returns an output when $C'(B_1)$ has at most $1$ wrong answer (since it checks correctness on the $(k-1)$ values of $A_1$). Thus iteration 1 only fails if the random $x \in B_1$ falls on this $1$ (of $k$) answers. So \begin{equation} \Pr_{x, A_1, B_1}[~\text{Iteration 1 fails}~] \leq \frac{1}{k} \end{equation}
</p><p>
 Now, we just union bound: \begin{align*} \Pr[\text{error}] &amp;= \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)]\\ &amp;\leq \Pr[\text{no output produced after $T$ iterations, or some iteration fails}]\\ &amp;\leq \Pr[\text{no output produced}] + T \cdot \Pr[\text{Iteration 1 fails}]\\ &amp;\leq \Pr[\text{no output produced}] + \frac{T}{k} \end{align*}
</p><p>
For our choice of $T, \delta$, the second term is $\frac{T}{k} \leq \delta / 200$. We will show the first term is $\leq \delta / 200$ as well, completing the proof.
</p><p>
<b>Produces output w.h.p.</b>
</p><p>
It remains to show that the circuit $C_{A, v}$ produces an output with high probability. In Step 3 of the circuit $C_{A, v}$, notice that if $C'$ is queried on a correct input $B_i$, it will pass the test and output a value.
</p><p>
The idea is: since $C'$ is correct on $\epsilon$-fraction of inputs, if we try $T = \Omega(\log(1/\delta)/\epsilon)$ iid random inputs, we will be sure to hit a correct input, except with probability $O(\delta)$. This doesn't quite work, since the inputs $B_i$ are not iid random (they all contain the input $x$) -- but this dependence is minimal, so it still works out.
</p><p>
Following [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>], it helps to think in term of this bipartite graph. Define $G$ as a biregular bipartite graph between inputs $x \in \{0, 1\}^n$, and $k$-<b>tuples</b><sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote6">6</a></sup><span id="footnote6" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote6" name="footnote6">6.</a> Going back to tuples just to simplify the notation, so we can deal with the uniform measure. </span> $B \in (\{0, 1\}^n)^k$, with an edge $(x, B)$ if $x \in B$. We can think of the circuit $C_{A, v}(x)$ as picking up to $T$ random neighbors of $x$ in the graph $G$, until hitting an input $B$ where $C'(B)$ is correct on all $B \setminus \{x\}$. We know that $\epsilon$-fraction of $k$-tuples $B$ are correct, and in fact we will show that almost all inputs $x$ have close to $\eps$-fraction of their neighbors as correct.
</p><p>
</p><p align="center"><img src="http://learningwitherrors.org/sources/uniform-DP/graph_color.png" width="350" /></p>
<p>

</p><blockquote><b>Lemma 3</b> <em> <a name="lemnotbad"></a> There are at most $O(\delta)$-fraction of “$\bad$” inputs $x \in \{0, 1\}^n$ for which \[\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10\] </em></blockquote>

<p>

 This is sufficient to show that $\Pr[\text{no output produced}] \leq O(\delta)$, since for inputs $x$ that are not $\bad$, sampling $T = \Omega(\log(k)/\eps)$ iid neighbors of $x$ will hit a correct neighbor, except with probability $O(1/k) \leq O(\delta)$. <sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote7">7</a></sup><span id="footnote7" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote7" name="footnote7">7.</a>  $(1-\eps/10)^{T} \leq e^{-T\eps / 10} \leq 1/k \leq \delta$.  </span>
</p><p>
It is easier to show the related property:
</p><blockquote><b>Lemma 4 (Mixing Lemma)</b> <em> <a name="lemmixing"></a> Let $H \subseteq \{0, 1\}^n$ be a set of inputs on the left of $G$, with the density of $H$ at least $\mu$. Then, except for some $2e^{-\Omega(\mu k)}$-fraction of tuples $B$, all tuples $B$ on the right of $G$ have \[\Pr_{x \in N(B)}[x \in H] = \mu \pm \mu/2\] </em></blockquote>

<p>

 <em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemmixing">4</a>:</em>  Drawing a uniformly random tuple $B$ on the right is exactly drawing $k$ iid samples of inputs $B := (x_1, x_2, \dots, x_k)$. Then, by definition of $G$, picking a random neighbor $x \in N(B)$ is just picking a random $x \in B$. Thus, it is sufficient to show that if we draw $k$ iid inputs $x_1, x_2, \dots, x_k$, the fraction of inputs that fall in $H$ is within a multiplicative factor $(1 \pm 1/2)$ of its expectation $\mu$ (with high probability). This follows immediately from Chernoff bounds. $$\tag*{$\blacksquare$}$$
</p><p>
From this, the above Lemma <a href="http://learningwitherrors.org/atom.xml#lemnotbad">3</a> follows easily:
</p><p>
<em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemnotbad">3</a>:</em>  Let $\bad$ be the set of “bad” inputs $x$, where $\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10$. Suppose the density of $\bad$ is $\mu$. Let us count fraction of total edges in $G$ that go between $\bad$, and the set of correct tuples (which we call $\good$). By the mixing lemma, there are at least $(\epsilon - 2e^{\Omega(\mu k)})$ fraction of tuples $B^*$ with $\Pr_{x \in N(B^*)}[\text{$x$ is bad}] \geq \mu / 2$. So there are at least $(\epsilon - 2e^{\Omega(\mu k)}) (\mu/2)$ fraction of edges between the $\bad$ and $\good$ sets.
</p><p>
But, each bad input $x$ has at most $\eps/10$ fraction of edges into $\good$ by definition, so the fraction of $\bad \leftrightarrow \good$ edges is at most $\mu (\eps/10)$.
</p><p>
Thus we must have \begin{align*} (\epsilon - 2e^{-\Omega(\mu k)}) (\mu/2) &amp;\leq \mu (\eps/10)\\ \implies \mu &amp;\leq O(\log(1/\eps)/k) \end{align*} This gives $\mu \leq \delta/200$ for our choice of $\delta$. $$\tag*{$\blacksquare$}$$
</p><p>
This concludes the proof of correctness of the oracle version (Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>). $$\tag*{$\blacksquare$}$$
</p><p>
</p><h2 class="tex">4. Closing Remarks </h2>
<p>


</p><ul> <li> Note that in the oracle version, we were able to output a good circuit with probability $0.99$, instead of w.p. $\Theta(\eps)$ as in the fully uniform version. This makes sense because if we have an $f$-oracle, we can “check” if our circuit is actually computing the desired $f$, so we don't run into the unique decoding problem. (Indeed, we can construct an optimal version of algorithm $\A^f$ of Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a> from the algorithm $\A$ of Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a> in a black-box way, by checking if the output circuit of $\A$ mostly agrees with $f$ on enough random inputs).
<p>
 </p></li><li> There were several simplifications we made from $\A$ to $\A^f$.<br />
 (1) We queried the oracle for the hardcoded values $v$, instead of the circuit.<br />
 (2) We hardcoded $(k-1)$-multisets instead of $(k/2)$-multisets.<br />
 (3) We hardcoded $T$ iid multisets $\{A_i\}$, instead of just one multiset $A$.<br />
 Note that we could not have done (2) without also doing (3) -- otherwise there would not have been enough mixing (the circuit would fail with probability close to $\eps$). Also, (3) would not have worked in the fully uniform case ($\A$, without the oracle) -- because then all the hardcoded sets will be correct with only very small probability.
<p>
 </p></li><li> The reason Theorem <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a> has suboptimal parameters (eg, compare the setting of $\delta$ to Theorem <a href="http://learningwitherrors.org/atom.xml#thmuniformDP">1</a>) is because our analysis used the loose union bound, instead of using the fact that circuit $C_{A, v}$, by only outputting values that pass a test, is doing rejection-sampling on a certain conditional probability space. The tight analysis in [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] takes advantage of this fact.
<p>
 </p></li><li> In the proof of Thereom <a href="http://learningwitherrors.org/atom.xml#thmoracle">2</a>, we used a property of the graph $G$ that was essentially like an “Expander Mixing Lemma”. We may hope that if we replace $G$ with something sufficiently expander-like, we could get a derandomized direct-product theorem. Indeed, something like this is done in [<a href="http://learningwitherrors.org/atom.xml#ref-IJKW10">IJKW10</a>] (“Uniform direct product theorems: simplified, optimized, and <i>derandomized</i>”).
<p>
 </p></li><li> I think the oracle version is sufficient for the applications in [<a href="http://learningwitherrors.org/atom.xml#ref-CIKK16">CIKK16</a>], since there we have query access to the function $f$ we are trying to learn/compress.
<p>
 </p></li><li> For a good survey on direct-product for non-uniform hardness amplification, and the related “Yao's XOR Lemma”, see [<a href="http://learningwitherrors.org/atom.xml#ref-GNW11">GNW11</a>] (which includes at least 3 different proofs of the non-uniform XOR lemma). For a clean proof of Impagliazzo's Hardore Set theorem, which is used in some proofs of the XOR lemma, see for example Arora-Barak.
<p>

</p></li></ul>


<p>
<br /></p><hr /><h3>References</h3>
<p>
<a name="ref-CIKK16">[CIKK16]</a> Marco~L Carmosino, Russell Impagliazzo, Valentine Kabanets, and Antonina
  Kolokolova.
 Learning algorithms from natural proofs.
 In <em>LIPIcs-Leibniz International Proceedings in Informatics</em>,
  volume~50. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
 URL:
  <a href="http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf">http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf</a>.
</p><p>

</p><p>
<a name="ref-GNW11">[GNW11]</a> Oded Goldreich, Noam Nisan, and Avi Wigderson.
 On yao's xor-lemma.
 In <em>Studies in Complexity and Cryptography. Miscellanea on the
  Interplay between Randomness and Computation</em>, pages 273--301. Springer,
  2011.
 URL: <a href="http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf">http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf</a>.
</p><p>

</p><p>
<a name="ref-IJKW10">[IJKW10]</a> Russell Impagliazzo, Ragesh Jaiswal, Valentine Kabanets, and Avi Wigderson.
 Uniform direct product theorems: simplified, optimized, and
  derandomized.
 <em>SIAM Journal on Computing</em>, 39(4):1637--1665, 2010.
 URL: <a href="http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf">http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf</a>.
</p><p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Preetum Nakkiran <a href="http://learningwitherrors.org/2016/08/24/uniform-direct-product/"><span class="datestr">at August 25, 2016 01:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/08/13/first-post">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/08/13/first-post/">New Theory Blog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><p>We’re starting a theory student blog!  The idea is, this is a collaborative blog
about theoretical computer science, where people can post about interesting
things they’re learning / have learnt. The goal is to help everyone learn from
each other, and also have a forum for student discussion.</p>

<p>Hopefully this will help make TCS concepts more accessible: Sometimes reading
the original paper is not the best / most efficient way to learn something, and
there are several perspectives or proofs of the same thing that are not
explicitly written down in the literature, but are known in the community. We
hope this blog will be a way to share this knowledge among theory students.</p>

<p>One important aspect is, we want this to feel like an informal place to learn
and discuss – think more like chalk talks than STOC talks. It’s fine to have
rough calculations, sketched figures, etc – the emphasis is on explaining
things nicely. And we want to encourage asking clarifying questions and
discussing in the comments.</p>

<h2 id="on-posts">On posts:</h2>
<ul>
  <li>
    <p>Posts can be about anything technical that other students may find
interesting or learn from. Anything from current research to classical
results.</p>
  </li>
  <li>
    <p>The length and thoroughness can vary, anything from “survey of this field” to
“summary of cool paper” to “interesting technical lemma” to “something cool I
learnt this week”, etc.</p>
  </li>
  <li>
    <p>You don’t need to be an expert on the topic to write about it (as the name
suggests, there may be some errors, but hopefully also some learning).</p>
  </li>
  <li>
    <p>The aim is to convey interesting or useful techniques and intuition (e.g. try
not to just announce a new result without explaining the ideas behind it)</p>
  </li>
</ul>

<h2 id="contributing">Contributing:</h2>
<p>Everyone is welcome (and encouraged!) to contribute – including
non-students, and generally anyone interested in TCS.</p>

<p>The easiest way is to
simply write a LaTeX or Markup document, and email it to me (preetum [at]
berkeley).
There is also a <a href="http://learningwitherrors.org/contributing/">harder way</a>.</p>

<p>Ideally, both readers and writers would get something out of this blog.
(Personally, I like to present topics to make sure I understand them
fully. And of course, we can have an interesting discussion about it.)</p>



<h2 id="comments-and-subreddit">Comments and Subreddit:</h2>
<p>We have comments below each post, which we encourage
people to use to discuss the post.</p>

<p>We also have the subreddit <a href="https://www.reddit.com/r/LWE">r/LWE</a>,
which we hope can be used as a more general
forum among theory students. Feel free to use this for both blog-related things
and general theory questions. Let’s see how this works.</p>

<h2 id="initial-posts">Initial Posts:</h2>
<p>We’re launching with posts on:</p>

<ul>
  <li>
    <p><a href="http://learningwitherrors.org/2016/06/23/intro-sos/">Intro to the Sum-of-Squares Hierarchy</a>  <br />
by Tselil Schramm.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos/">Pseudo-calibration for Planted Clique Sum-of-Squares Lower Bounds</a>  <br />
by Pasin Manurangsi.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/07/06/deterministic-sparsification/">Deterministic Sparsification</a>  <br />
by Chenyang Yuan.</p>
  </li>
  <li>
    <p><a href="http://learningwitherrors.org/2016/06/03/small-bias/">Simple Lower Bounds for Small-bias Spaces</a> and
<a href="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss/">Fast Johnson-Lindenstrauss</a>  <br />
by Preetum Nakkiran.</p>
  </li>
</ul>

<p>Thanks especially to the above people (and all future authors) for contributing.</p>

<h2 id="conclusion-and-open-questions">Conclusion and Open Questions</h2>
<p>When conceiving this blog, we had some other
ideas for things that should exist, such as a set of collaboratively-edited
pages on “How to best learn topic X”. Are people interested in contributing to
something like this?  In general, any suggestions for things you would like to
see (regarding this blog, or otherwise)?</p>

<p>Feel free to use the comments section below.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Preetum Nakkiran <a href="http://learningwitherrors.org/2016/08/13/first-post/"><span class="datestr">at August 13, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos/">Pseudo-calibration for Planted Clique Sum-of-Squares Lower Bound</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>

</p><p>
 Recently, Barak, Hopkins, Kelner, Kothari, Moitra and Potechin [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] proved an essentially tight Sum-of-Squares lower bound for the <em>planted clique</em> problem. Their result can be divided into two main parts: coming up with the <em>pseudo-distribution</em> and proving positivity of such pseudo-distribution. In this short blog, we summarize the first part of the paper, which provides a general systematic way to come up with pseudo-distributions for problems other than the planted clique problem, without going into details of the proof. We do not touch on the second part, which is more technically involved, here but we will hopefully do so in future posts.

</p><p>
</p><h2 class="tex">1. SoS Lower Bounds and the Planted Clique Problem </h2>
<p>
In this section, we provide some background for readers unfamiliar with proving Sum-of-Squares lower bounds and the planted clique problem. Those who are accustomed to the topic can skip this section. For SoS, we use notations from <a href="http://learningwitherrors.org/2016/06/23/intro-sos/">Tselil's blog</a> on Sum-of-Squares Hierarchy, which is also a good place to start for those unfamiliar with SoS Hierarchy.
</p><p>
In this blog, we do not need the optimization version of SoS Hierarchy but we will only use a feasibility one. Recall that, given a polynomial feasibility problem of the form \[Q = \left\{x \in \mathbb{R}^n : \forall i \in [m], g_i(x) = 0\right\},\] the degree-$2d$ Sum-of-Squares relaxation of the problem, which can be solved in $n^{O(d)}$ time, is<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> Note that, in Tselil's blog, the positivity condition is written as the pseudo-moment matrix being positive semidefinite but it is not hard to see that this is the same as requiring that $\tE[q^2] \geq 0$ for every $q$ with $\deg(q) \leq d$. </span> \begin{align} \label{eq:sos} \sos_d(Q) = \left\{\tE : \begin{array}{lr} {\tE}: \{q: \deg(q) \leq 2d\} \rightarrow \mathbb{R} \text{ is a linear operator with } \tE[1] = 1, \\ \forall q \text{ with } \deg(q) \leq d, \tE[q^2] \geq 0, \\ \forall i \in [m] \forall q \text{ with } \deg(q) \leq 2d - \deg(g_i), \tE[g_i q] = 0. \end{array} \right\}. \end{align}
</p><p>
Roughly speaking, if we want to show that degree-$2d$ SoS fails to certify that a polynomial feasibility problem $Q$ is infeasible, we need to come up with a degree-$2d$ pseudo-distribution $\tE$ that satisfies the conditions in (\ref{eq:sos}). For concreteness, let us consider the <em>planted clique</em> problem defined as follows.
</p><p>

</p><blockquote><b>Definition 1 (Planted Clique$(n, k)$)</b> <em> Given as an input a graph $G = (V, E)$ drawn from one of the two following distributions (each with probability $1/2$):

<ol> <li> $\cG(n, 1/2)$: the Erdos-Renyi random graph of $n$ vertices where each edge is included with probability 1/2, </li><li> $\cG(n, 1/2, k)$: the planted distribution, in which a graph $G$ is first drawn from $\cG(n, 1/2)$. Then, $k$ vertices of $G$ are chosen uniformly at random and an edge between each pair of chosen vertices are added to $G$.
</li></ol>

 The goal is to determine, with correctness probability $1/2 + \varepsilon$ for some constant $\varepsilon &gt; 0$, which distribution $G$ is drawn from. </em></blockquote>

<p>


</p><p>
In this blog, we always restrict ourselves to the case where $k \gg \log n$ so that the maximum clique sizes of the two cases are different. Since the largest clique in $\cG(n, 1/2)$ is of size $O(\log n)$ with high probability, brute-force search solves the planted clique problem with high probability in $n^{O(\log n)}$ time. On the other hand, the best known polynomial-time algorithm works only when $k = \Omega(\sqrt{n})$ [<a href="http://learningwitherrors.org/atom.xml#ref-AKS98">AKS98</a>]. A natural question is of course whether the SoS Hierarchy can do any better than this.
</p><p>
The most widely-used formulation of planted clique in terms of polynomial feasibility, and the one used in [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>], is to formulate it as “does $G$ have a clique of size $k$?”. For convenience, let $V = [n] = \{1, \dots, n\}$. This formulation can be written as follows.
</p><p>
\begin{align*} \cli_k(G) = \left\{x \in \mathbb{R}^n : \begin{array}{lr} \forall i \in [n], x_i^2 = x_i, \\ \forall (i, j) \notin E, x_ix_j = 0, \\ \sum_{i \in [n]} x_i = k \end{array} \right\} \end{align*}
</p><p>
When the constraints are satisfied, $x_i$ is simply a boolean indicator variable whether $i$ is included in the clique. If we can solve $\cli_k(G)$ in polynomial time, then we are done because $G \sim \cG(n, 1/2, k)$ always has clique of size $k$ whereas the maximum clique of $G \sim \cG(n, 1/2)$ is of size $O(\log n)$ w.h.p. Thus, there is always a solution in $\cli_k(G)$ for $G \sim \cG(n, 1/2, k)$ but, w.h.p., there is no feasible solution for $G \sim \cG(n, 1/2)$. But of course solving $\cli_k(G)$ is NP-hard so we will try to relax it using degree-$2d$ SoS which we can solve in $n^{2d}$ time.
</p><p>
Again, when $G \sim \cG(n, 1/2, k)$, $\sos_d(\cli_k(G))$ remains feasible. If we want to tell which distribution $G$ is drawn from by looking only at whether $\sos_d(\cli_k(G))$ is feasible, we need that, when $G \sim \cG(n, 1/2)$, $\sos_d(\cli_k(G))$ is infeasible with probability at least $\varepsilon$. The main result of [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] is that this is impossible. In particular, they show the following:
</p><p>

</p><blockquote><b>Theorem 2</b> <em> <a name="thmmain-clique"></a> For every $d \ll \log n$, when $k \leq n^{1/2 - O(\sqrt{d/\log n})}$ and $G$ is drawn from $\cG(n, 1/2)$, $\sos_d(\cli_k(G))$ is feasible with high probability. </em></blockquote>

<p>


</p><p>
In other words, Barak et al.'s result says that the SoS approach to planted clique is no better (up to the $O(\sqrt{d/\log n})$ factor in the exponent) than the known algorithm from [<a href="http://learningwitherrors.org/atom.xml#ref-AKS98">AKS98</a>].
</p><p>
From how $\sos_d(\cli_k(G))$ is defined, proving Theorem <a href="http://learningwitherrors.org/atom.xml#thmmain-clique">2</a> boils down to find a linear operator ${\tE}_G: \{q: \deg(q) \leq 2d\} \rightarrow \mathbb{R}$ for each graph $G$ such that, if $G = ([n], E)$ is drawn from $\cG(n, 1/2)$, the following conditions are satisfied with high probability:

</p><ol> <li> $\tE_G[1] = 1$, </li><li> $\forall i \in [n] \forall q$ with $ \deg(q) \leq 2d - 2, \tE_G[x_i^2q] = \tE_G[x_iq]$, </li><li> $\forall (i, j) \notin E \forall q$ with $\deg(q) \leq 2d - 2,\tE_G[x_ix_jq] = 0$, </li><li> $\tE_G[\sum_{i \in [n]} x_i] = k$, </li><li> $\forall q$ with $\deg(q) \leq d, \tE_G[q^2] \geq 0$.
</li></ol>


<p>
</p><h2 class="tex">2. Pseudo-calibration for Planted Clique </h2>
<p>
Coming up with degree-$2d$ pseudo-distribution $\tE_G$ with desired properties stated in the previous section is particularly hard for planted clique and past attempts often involve some ad-hoc fixes that prevent them from getting tight bound for large $d$. This is where Barak et al.'s so-called <em>pseudo-calibration</em> method, which is a systematic way to derive $\tE_G$, comes in. Since the method is more of an intuitive heuristic rather than a provable approach, we will be informal here. We also note that the explanation given here is somewhat different than that in [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] and the readers should consult the full paper for a more thorough view of pseudo-calibration.
</p><p>
Let us take a step back and think about our algorithm for planted clique for a moment. Given $G$, we try to solve $\sos_d(\cli_k(G))$. If it is infeasible, then we know for certain that $G$ is drawn from $\cG(n, 1/2)$. Otherwise, we do not seem to gain anything. However, this may not be entirely true; we actually get back $\tE_G$. One thing we can do here is to pick $f_G$ (which can depend on $G$) of degree (with respect to $x$) at most $2d$ as a test function and ask for $\tE_G[f_G]$. If the distributions of $\tE_G[f_G]$ under $G \sim \cG(n, 1/2)$ and $G \sim \cG(n, 1/2, k)$ are “very different”<sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span id="footnote2" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> In other words, they are distinguishable in polynomial time. </span>, then we should be able to tell $G$'s from the two distributions apart by just looking at $\tE_G[f_G]$. Hence, not only that $\sos_d(\cli_k(G))$ must be feasible with high probability when $G \sim \cG(n, 1/2)$ but the distributions of $\tE_G[f_G]$ when $G \sim \cG(n, 1/2)$ and when $G \sim \cG(n, 1/2, k)$ must also be indistinguishable in polynomial time for every test function $f_G$. An implication of this is that the expectation of $\tE_G[f_G]$ over the two distributions are roughly equal, i.e., \begin{align*} \E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx \E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G]. \end{align*}
</p><p>
We of course do not know what $\tE_G$ is even when $G$ is drawn from $\cG(n, 1/2, k)$ so the above equality does not tell us much yet. But recall that $\tE_G$ is our fake solution and we want it to resemble the actual solution as much as possible. Hence, a reasonable heuristic here is to try to make $\E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G]$ roughly equal to $\E_{G \sim \cG(n, 1/2, k)} f_G(x_G)$ where $x_G$ denote the actual solution, i.e., the indicator vector for the maximum clique in $G$.
</p><p>
For convenience, let us write $(G, x) \sim \cG(n, 1/2, k)$ to denote $G$ drawn from $\cG(n, 1/2, k)$ and $x$ being the indicator vector whether each vertex is included as part of the planted $k$-clique. Under this notation, the aforementioned condition can be written as \begin{align*} \E_{G \sim \cG(n, 1/2, k)} \tE_G[f_G] \approx \E_{(G, x) \sim \cG(n, 1/2, k)} f_G(x). \end{align*}
</p><p>
Combining the above two equations, we get \begin{align} \label{eq:calib} \E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx \E_{(G, x) \sim \cG(n, 1/2, k)} f_G(x). \end{align}
</p><p>
Condition (\ref{eq:calib}) is what Barak et al. called <em>pseudo-calibration</em><sup class="footnotemark"><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span id="footnote3" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a> In [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>], the pseudo-calibration condition is in fact slightly stronger that stated here; equality is required instead of approximate equality. However, it does not matter anyway since there will be approximations in subsequent calculations. </span>. As noted in the paper, this condition is quite strong. For example, for fixed $i, j \in [n]$ and $q$ with $\deg(q) \leq 2d - 2$, if we define $f$ as \begin{align*} f_G(x) = \begin{cases} 0 &amp; \text{ if } (i, j) \in E, \\ x_ix_jq(x) &amp; \text{ otherwise}, \end{cases} \end{align*} then $f_G(x)$ is always zero on the right hand side. Hence, $\E_{G \sim \cG(n, 1/2)} \tE_G[f_G] \approx 0$. If we assume that $\tE_G[f_G]$ is non-negative, then Condition 3 at the end of the previous section is almost immediately satisfied. In fact, as we will see next, Condition (\ref{eq:calib}) almost fully determines $\tE_G$ for every $G$.
</p><p>
</p><h3 class="tex">2.1. From Pseudo-Calibration to Pseudo-Distribution</h3>
<p>
We will now see how to arrive at $\tE_G$ from the pseudo-calibration condition. As stated earlier, the condition is quite strong; in fact, it is too that it cannot hold for every $f_G$. For instance, we can pick $f_G$ to simply be the indicator function of whether $G$ has a clique of size $k$. By doing so, the left hand side of (\ref{eq:calib}) is approximately zero whereas the right hand side is one. However, we are “cheating” by picking such $f_G$ because we do not even know how to compute this test function in polynomial time! Hence, roughly speaking, we need to restrict $f_G$ to only those that are not more “powerful” that the SoS relaxation itself.
</p><p>
To state the exact condition we enforce on $f_G$, let us think of $f_G(x)$ as a function $f(G, x)$ of both $G$ and $x$ where the graph $G$ is encoded naturally as a string in $\{\pm 1\}^{[n] \choose 2}$, i.e., the $(i, j)$-index of the input is $+1$ if there is an edge between $i$ and $j$ and $-1$ otherwise. Now, we can write $f$ as a polynomial on both $G$ and $x$: \begin{align*} f(G, x) = \sum_{T \subseteq {[n] \choose 2}, S \subseteq [n]} a_{(T, S)} \chi_T(G) x_S \end{align*} where $\chi_T(G)$ and $x_S$ denote $\prod_{e \in T} G_e$ and $\prod_{i \in S} x_i$ respectively, and, $a_{(T, S)}$'s are the coefficients of the polynomial. We will require the pseudo-calibration condition to hold only for $f_G$ such that each monomial depends on at most $\tau$ vertices where $\tau = O(d)$ is a truncation threshold. In other words, we only restrict ourselves to $f$ that can be written as \begin{align*} f(G, x) = \sum_{T \subseteq {[n] \choose 2}, S \subseteq [n] \atop |\cV(T) \cup S| \leq \tau} a_{(T, S)} \chi_T(G) x_S \end{align*} where $\cV(T)$ is the set of all vertices which are endpoints of edges in $T$. The intuition behind this heuristic is that, in the conditions on $\tE_G$ imposed by the SoS relaxation, each monomial involves at most $2d$ vertices because $\tE_G$ is defined only on polynomials on $x$ of degree at most $2d$. As a result, each monomial appearing in $f(G, x)$ should involve no more than $O(d)$ vertices in order to limit its “power” to be not much more than the SoS relaxation.
</p><p>
Now, let us use the pseudo-calibration condition to determine $\tE_G$. Fixed a subset $S \subseteq [n]$ of size at most $2d$, we will compute $\tE_G[x_S]$ for the monomial $x_S$. Note that, since $\tE_G$ is linear and $\tE_G[x_i^2] = x_i$ for all $i \in [n]$, these $\tE_G[x_S]$'s uniquely determine $\tE_G$. By viewing $\tE_G[x_S]$ as a function of $G$, $\tE_G[x_S]$ can be written as fourier expansion \begin{align*} \tE_G[x_S] = \sum_{T \subseteq {[n] \choose 2}} \widehat{\tE_G[x_S]}(T) \chi_T(G). \end{align*} The final heuristic employed by Barak et al. is to enforce $\tE_G[x_S]$ to be low degree by letting $\widehat{\tE_G[x_S]}(T) = 0$ for every $T$ with $|\cV(T) \cup S| &gt; \tau$. This heuristic makes sense since $\tE_G$ must be output by the SoS relaxation solver, which runs in $n^{O(d)}$ time; hence, $\tE_G$ cannot be too hard to compute. More importantly, as we will see shortly, this condition allows us to almost uniquely determine $\tE_G$ from the pseudo-calibration condition.
</p><p>
Recall that each fourier coefficient $\widehat{\tE_G[x_S]}(T)$ is simply equal to $\E_{G \sim \cG(n, 1/2)} \tE_G[x_S \chi_T(G)].$ Plugging in the pseudo-calibration condition with $f = x_S\chi_T(G)$, this is approximately $\E_{(G, x) \sim \cG(n, 1/2, k)} [x_S\chi_T(G)]$. It is not hard to see that this expression is equal to the probability that every vertex in $\cV(T) \cup S$ is in the planted clique, which is roughly $(k/n)^{|\cV(T) \cup S|}$ when $|\cV(T) \cup S|$ is small. Indeed, we will set $\widehat{\tE_G[x_S]}(T)$ to be exactly this. In other words, the final pseudo-distribution is \begin{align*} \tE_G[x_S] = \sum_{T \subseteq {[n] \choose 2} \atop |\cV(T) \cup S| \leq \tau} \left(\frac{k}{n}\right)^{|\cV(T) \cup S|} \chi_T(G). \end{align*}
</p><p>
It is not hard to see that $\tE_G$ indeed satisfies the pseudo-calibration condition for $f$'s of our interest. As explained right before the beginning of this subsection, this almost immediately implies that the third condition required for $\tE_G$ is satisfied; it is also pretty easy to check that the condition is indeed true (see Lemma 5.5 in the paper). Using concentration inequalities, Barak et al. also show that $\tE_G[1] = 1 \pm o(1)$ and $\tE_G[\sum_{i \in [n]} x_i] = k \pm o(1)$ (see full proof in Appendix A.2 of the paper). Note that while these two conditions are only approximately satisfied, $\tE_G$ can be scaled so that they are exactly satisfied as well. As mentioned briefly earlier, the proof of the positivity condition $\tE_G[q^2] \geq 0$ is much harder and is the paper's main technical contribution. We do not attempt to discuss it here but we will try to blog about it in the future.
</p><p>
</p><h2 class="tex">3. Further Reading </h2>
<p>
The authors of [<a href="http://learningwitherrors.org/atom.xml#ref-BHKKMP16">BHKKMP16</a>] have given talks on the paper and some of them are available online, such as <a href="https://www.youtube.com/watch?v=ZmFOsAB7Y1k">Moitra's</a> and <a href="https://www.youtube.com/watch?v=H2C2ZdgynX4">Kothari's</a>. Barak also wrote <a href="https://windowsontheory.org/2016/04/13/bayesianism-frequentism-and-the-planted-clique-or-do-algorithms-believe-in-unicorns/">a blog</a> regarding pseudo-calibration. All the materials mentioned discuss the pseudo-calibration in much more detail than in this post. Moitra's talk also contains the proof sketch of positivity of the pseudo-distribution, which is not covered in this blog post.
</p><p>
Apart from the paper, I am not aware of the pseudo-calibration technique being used to prove new lower bounds for other problems yet. I will update this section when I come across new results based on pseudo-calibration.
</p><p>
<br /></p><hr /><h3>References</h3>
<p>
<a name="ref-AKS98">[AKS98]</a> Noga Alon, Michael Krivelevich, and Benny Sudakov.
 Finding a large hidden clique in a random graph.
 <em>Random Struct. Algorithms</em>, 13(3-4):457--466, 1998.
</p><p>

</p><p>
<a name="ref-BHKKMP16">[BHKKMP16]</a> Boaz Barak, Samuel~B. Hopkins, Jonathan~A. Kelner, Pravesh Kothari, Ankur
  Moitra, and Aaron Potechin.
 A nearly tight sum-of-squares lower bound for the planted clique
  problem.
 <em>CoRR</em>, abs/1604.03084, 2016.
</p><p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Pasin Manurangsi <a href="http://learningwitherrors.org/2016/08/12/pseudocalibration-for-planted-clique-sos/"><span class="datestr">at August 12, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/07/06/deterministic-sparsification">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/07/06/deterministic-sparsification/">Deterministic Sparsification</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>

<p>Let $G$ be a dense graph. A sparse graph $H$ is a sparsifier of $G$
approximation of $G$ that preserves certain properties such as quadratic forms
of its Laplacian. This post will formally define spectral sparsification, then
present the intuition behind the deterministic construction of spectral
sparsifiers by Baston, Spielman and Srivastava [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>].</p>



<p>Benczúr and Karger [<a href="http://learningwitherrors.org/atom.xml#BK96">BK96</a>] introduced the cut sparsifier, which
ensures that the value of all cuts in $H$ approximates that of all cuts in $G$:</p>

<blockquote>
  <p><a name="def:cut-sp"></a><strong>Definition 1 (Cut Sparsification):</strong> A weighted
undirected graph $H = (V, E_H)$ is an $\epsilon$-cut sparsifier of a weighted
undirected graph $G = (V, E_G)$ if for all $S \subset V$,</p>

  

  <p>Where $E_G$ and $E_H$ are the sums of edge weights crossing the cuts in $G$ and
$H$ respectively.</p>
</blockquote>

<p>Spielman and Teng [<a href="http://learningwitherrors.org/atom.xml#ST08">ST08</a>] introduced another notion of graph sparsification
with the quadratic form of the Laplacian:</p>

<blockquote>
  <p><a name="def:spectral-sp"></a><strong>Definition 2 (Spectral Sparsification):</strong> A
weighted undirected graph $H = (V, E_H)$ is an $\epsilon$-spectral sparsifier
of a weighted undirected graph $G = (V, E_G)$ if for all $x \in
\mathbb{R}^{|V|}$,</p>

  

  <p>Where $L_G$ and $L_H$ are the graph Laplacians of $G$ and $H$ respectively.</p>
</blockquote>

<p>Cut sparsifiers can be used in approximating max-flow (via the max-flow min-cut
theorem) and spectral sparsifiers are a key ingredient in solving Laplacian
inear systems in near linear time.</p>



<p>Note that this stronger than cut sparsification, as we can fix $x$ to be the
indicator vectors of cuts to obtain <a href="http://learningwitherrors.org/atom.xml#def:cut-sp">definition 1</a>. Also note that
this notion of sparsifiers also provides bounds on Laplacian eigenvalues, and
thus spectral sparsifiers of complete graphs are also expanders. These
sparsifiers can be constructed in a randomized manner by sampling edges
proportional to their effective resistance [<a href="http://learningwitherrors.org/atom.xml#SS08">SS08</a>], but in this post we
will focus on a deterministic construction presented in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>], as
stated more precisely in the following theorem:</p>

<blockquote>
  <p><a name="thm:sparsifier"></a><strong>Theorem 1:</strong> For every $d &gt; 1$, every undirected
graph $G = (V, E)$ on $n$ vertices contains a weighted subgraph $H = (V, F,
\tilde{w})$ with $\lceil d(n-1) \rceil$ edges that satisfies:</p>

  
</blockquote>

<h2 id="preliminaries">Preliminaries</h2>
<p>The Laplacian $L$ of a graph can be seen as a linear transformation relating the
flow and demand in an electrical flow on the graph where each edge has unit
resistance. Let $B \in \mathbb{R}^{m \times n}$ be the vertex-edge incidence
matrix and $W$ is a diagonal matrix of edge weights, then $L = B^TWB$. $L$ also
has a pseudoinverse which acts like an actual inverse for all vectors
$x \bot \mathbb 1$, resulting from solving an electrical flow on $G$.</p>

<p>Let $\kappa = \frac{d+1+2\sqrt d}{d+1-2\sqrt d}$, we assume that the graph is
connected thus $x \bot \mathbf{1}$, and perform a transformation on the
condition in <a href="http://learningwitherrors.org/atom.xml#thm:sparsifier">Theorem 1</a>:</p>



<p>Let $b_{e = (u, v)} = \mathbf{1}_u - \mathbf{1}_v$ be a row of incidence matrix
$B$ , $s_e$ be the weight of edge $e$ in $E_H$ and $A \succeq B$ when $A - B$ is
a psd matrix. Then the above condition can be rewritten as:</p>

<p><a name="eq:sparse-approx"></a>
</p>

<p>We then define a vector $v_e = L_G^{-1/2}b_e^T$ for each $e \in E_G$. Notice
that over all edges of $G$, the rank 1 matrices formed by $v_eV_e^T$ sum to the
identity matrix:</p>



<p>Then <a href="http://learningwitherrors.org/atom.xml#eq:sparse-approx">equation 1</a> can be interpreted as choosing a sparse
subset of the edges in $G$, as well as weights $s_e$, so that the matrix
obtained by summing over the edges of $H$, $\sum_{e \in E_H} s_e v_ev_e^T$, has
a low condition number (ratio between the largest and smallest eigenvalues):</p>



<p>If we can find such a sparse set of edges and weights, then we have proved
<a href="http://learningwitherrors.org/atom.xml#thm:sparsifier">Theorem 1</a>. In [<a href="http://learningwitherrors.org/atom.xml#SS08">SS08</a>] this was done by randomly
sampling these rank-1 matrices based on their effective resistances of their
corresponding edges, using a distribution that has the identity matrix as the
expectation. Convergence is shown using a matrix concentration inequality. The
construction in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>] deterministically chooses each $v_e$ and
$s_e$, bounding the increase in $\kappa$ in each step using barrier
functions. One useful lemma for this procedure is:</p>

<blockquote>
  <p><a name="lem:matrix-det"></a><strong>Lemma 1 (Matrix Determinant Lemma):</strong> If $A$ is
nonsingular and $v$ is a vector, then:</p>

  
</blockquote>

<h2 id="main-proof">Main Proof</h2>

<p>Recall from the previous section the main theorem that need to be proved is:</p>

<blockquote>
  <p><a name="thm:rank1approx"></a><strong>Theorem 2:</strong>
Suppose $d &gt; 1$ and $v_1, \cdots, v_m$ are vectors in $\mathbb{R}^n$ with

Then there exist scalars $s_i &gt; 0$ with $|{i: s_i \ne 0 }| \le dn$ so that
</p>
</blockquote>

<p>This is equivalent to bounding the ratio of $\lambda_{\min}$ and
$\lambda_{\max}$ of the matrix $\sum_{i \le m} s_i v_iv_i^T$.</p>

<p>We start with a matrix $A = 0$, and build it by adding rank-1 updates
$s_ev_ev_e^T$. One interesting fact is that for any vector $v$, the eigenvalues
of $A$ and $A + vv^T$ interlace. Consider the characteristic polynomial of $A +
vv^T$:</p>



<p>Which can be written in terms of the characteristic polynomial of $A$ using
<a href="http://learningwitherrors.org/atom.xml#lem:matrix-det">Lemma 1</a>. $u_j$ are the eigenvectors of $A$.  Let $\lambda$ be
a zero of $p_{A + vv^T}(x)$. It can either:</p>

<ol>
  <li>Be a zero of $p_A(x)$, so $\lambda$ is equal to an eigenvalue $\lambda_i$
  of $A$, and the corresponding eigenvector $u_i$ is orthogonal to $v$. In this
  case, this eigenvalue doesn’t move.</li>
  <li>Strictly interlace with the old eigenvalues. This happens when
  $p_A(\lambda) \ne 0$ and
  
  This can be interpreted with a physical model. Consider $n$ positive charges
  arranged vertically with the $j$-th charge’s position corresponding to the
  $j$-th eigenvalue of $A$, and its charge is $\dotp{v, u_j}^2$. The points
  where the electric potential is 1 are the new eigenvalues. Since between any
  two charges the potential changes direction from $+ \infty$ to $- \infty$,
  there has to be a point between every two charges where the potential is 1,
  thus the new eigenvalues strictly interlace the old ones.</li>
</ol>

<p>To get some intuition, we see what happens when we sample $v_i$ uniformly
randomly. Since $\sum_j v_jv_j^T = I$, $\mathbb{E}_v[\dotp{v, u}^2]$ is constant
for any normalized vector $u$. Therefore adding the average $v$ increases the
charges by the same amount in the physical model, causing the new eigenvalues to
all increase by the same amount. Informally, we expect all the eigenvalues to
“march forward” at similar rates with each $vv^T$ added, so $\lambda_{\max} /
\lambda_{\min}$ is bounded.</p>

<p>We construct a sequence of matrices $A^{(0)}, \cdots, A^{(q)}$ by adding rank-1
updates $t vv^T$. To bound the condition number after each update, we create two
barriers $l &lt; \lambda_{\min}(A) &lt; \lambda_{\max}(A) &lt; u$ so that the eigenvalues
of $A$ lie between them. $\Phi_l(A)$ and $\Phi^u(A)$ are defined as the
potentials at the barriers respectively:</p>



<p>The crucial step is to show that there exists a $v_i$ and $t$ so that we can
add $t v_i v_i^T$ to $A$, so that each barrier is shifted by a constant, and the
potentials at each barrier doesn’t change. We will sketch out the proof briefly,
readers can pursue the details in [<a href="http://learningwitherrors.org/atom.xml#BSS08">BSS08</a>].</p>

<p>Let constants $\delta_U$ and $\delta_L$ be the maximum amount each barrier can
increase each round, and constants $\epsilon_U = \Phi^{u_0}(A^{(0)})$ and
$\epsilon_L = \Phi_{l_0}(A^{(0)})$ be the initial potentials at each
barrier. The first lemma shows that if $t$ is not too large, adding $t vv^T$ to
$A$ and shifting the upper barrier by $\delta_U$ will not increase the upper
potential $\Phi^u$.</p>

<blockquote>
  <p><strong>Lemma 2 (Upper Barrier Shift):</strong> Suppose $\lambda_{\max}(A) &lt; u$, and $v$ is
any vector. If

Then:
</p>
</blockquote>

<p>The second lemma shows that if $t$ is not too small, adding $t vv^T$ to $A$ and
shifting the lower barrier by $\delta_L$ will not increase the lower potential
$\Phi^u$.</p>

<blockquote>
  <p><strong>Lemma 3 (Lower Barrier Shift):</strong> Suppose $\lambda_{\min}(A) &gt; l$, $\Phi_l(A)
\le 1/\delta_L$ and $v$ is any vector. If

Then:
</p>
</blockquote>

<p>Finally, it can be shown that there exists a $t$ and $v_i$ that satisfy the
conditions of the above lemmas.</p>

<blockquote>
  <p><strong>Lemma 3 (Both Barriers):</strong> If $\lambda_{\max}(A) &lt; u$, $\lambda_{\min}(A) &gt;
l$, $\Phi^u(A) \le \epsilon_U$, $\Phi_l(A) \le \epsilon_L$, and $\epsilon_U$ ,
$\epsilon_L$, $\delta_U$, $\delta_L$ satisfy:

then there exists a $v_i$ and positive $t$ for which
</p>
</blockquote>

<p>This is proved by an averaging argument relating the behavior of vector $v$ to
the behavior of the expected vector, showing that

therefore there exists a $i$ for which there is a gap between
$L_A(v_i) - U_A(v_i)$. Choosing the constants carefully, we can get the required
bound on the condition number.</p>

<h2 id="extension">Extension</h2>

<p>There is a similarity between <a href="http://learningwitherrors.org/atom.xml#thm:rank1approx">Theorem 2</a> and the
Kadison-Singer conjecture. One formulation of it is stated below:</p>

<blockquote>
  <p><a name="prop:KSC"></a><strong>Proposition 1:</strong> There are universal constants
$\epsilon, \delta &gt; 0$ and $r \in \mathbb N$ for which the following statement
holds. If $v_1, \cdots, v_m \in \mathbb{R}^n$ satisfy $||v_i|| \le \delta$ for
all $i$ and

then there is a partition $X_1, \cdots X_r$ of ${1, \cdots, m }$ for which

for every $j = 1, \cdots, r$.</p>
</blockquote>

<p>This conjecture was positively resolved in [<a href="http://learningwitherrors.org/atom.xml#MSS13">MSS13</a>], using techniques
arising from generalizing the barrier function argument used to prove
<a href="http://learningwitherrors.org/atom.xml#thm:rank1approx">Theorem 2</a> to a multivariate version.</p>

<h2 id="references">References</h2>

<p><a name="BK96">[BK96]</a>
A. A. Benczúr and D. R. Karger. Approximating s-t minimum cuts in
$\tilde{O}(n^2)$ time. In <em>STOC ‘96</em>, pages 47-55, 1996.</p>

<p><a name="BSS08">[BSS08]</a>
J. Baston, D. A. Spielman and N. Srivastava. Twice-Ramanujan
Sparsifiers. Available at <a href="http://arxiv.org/abs/0808.0163">http://arxiv.org/abs/0808.0163</a>, 2008.</p>

<p><a name="MSS13">[MSS13]</a>
A. W. Marcus, D. A. Spielman and N. Srivastava. Interlacing Families
II: Mixed Characteristic Polynomials and The Kadison-Singer Problem. Available
at <a href="http://arxiv.org/abs/1306.3969">http://arxiv.org/abs/1306.3969</a>, 2013.</p>

<p><a name="SS08">[SS08]</a>
D. A. Spielman and N. Srivastava. Graph Sparsification by Effective
Resistances. In <em>STOC ‘08</em>, pages 563-568, 2008.</p>

<p><a name="ST08">[ST08]</a>
D. A. Spielman and S.-H. Teng. Spectral Sparsification of Graphs. Available at
<a href="http://arxiv.org/abs/0808.4134">http://arxiv.org/abs/0808.4134</a>, 2008.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Chenyang Yuan <a href="http://learningwitherrors.org/2016/07/06/deterministic-sparsification/"><span class="datestr">at July 06, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/06/23/intro-sos">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/06/23/intro-sos/">Intro to the Sum-of-Squares Hierarchy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>
This note is intended to introduce the Sum-of-Squares Hierarchy. We start by SDP relaxation, using the Goemans-Williamson Max-Cut SDP as a jumping off point. We then discuss duality and sum-of-squares proofs. Finally, we give an example non-trivial application of the sum-of-squares hierarchy: an algorithm for finding planted sparse vectors inside random subspaces. We will give no historical context, but in the final section there will be pointers to other resources which give a better sense of the history and alternative expositions of the same content.

</p><p>
</p><h2 class="tex">1. A Relaxation for Polynomial Optimization </h2>
<p>
Suppose we are interested in some polynomial optimization problem: \[ Q = \left\{\max_{x \in \R^n}\ p(x),\qquad s.t.\quad g_i(x) = 0 \quad \forall\ i \in [m]\right\}. \] That is, we want to maximize our objective function, the polynomial $p:\R^n\to \R$, subject to the polynomial constraints $g_1(x) = 0, g_2(x) = 0,\ldots, g_m(x) = 0$.
</p><p>
The problem $Q$ may be non-convex, and solving such programs is NP-complete (i.e. this captures integer programming). A standard approach for a situation like this is to relax our problem $Q$ to a semidefinite program (SDP). Perhaps the most famous example is the Goemans-Williamson relaxation for Max-Cut:
</p><p>

</p><blockquote><b>Example 1 (Max-Cut)</b> <em> We can formulate the max cut problem on an $n$-vertex graph $G$ as a polynomial optimization problem with objective function $p(x) = \sum_{(i,j) \in E(G)} \frac{1- x_i x_j}{2}$ and the constraint polynomials $g_i(x) = x_i^2 - 1 = 0\ \forall i\in[n]$, which ensure that each $x_i = \pm 1$.
</em><p><em>
The Goemans-Williamson SDP relaxation assigns program variables $X_{ij}$ for each $i,j \in [n]$, where $X_{ij}$ is a stand-in for the monomial $x_i x_j$. The SDP then becomes \[ \left\{ \max \sum_{(i,j) \in E(G)}\tfrac{1}{2}(1 - X_{ij}), \qquad s.t. \quad X_{ii} -1= 0\quad \forall i \in [n],\quad X \succeq 0 \right\} \] where $X$ is the $n \times n$ matrix with variable $X_{ij}$ in the $(i,j)$th entry. </em></p></blockquote>

<p>


</p><p>

</p><p>
<b>The Sum-of-Squares SDP: Extending Goemans-Williamson.</b> After seeing the Goemans-Williamson Max-Cut SDP, it seems natural to apply a similar relaxation to other polynomial optimization problems. Suppose that the maximum degree of any term in $p, g_1,\ldots,g_m$ is at most $2d$. The strategy is to relax the polynomial optimization problem by replacing each monomial $\prod_{i\in S \subset [n]} x_i$ which appears in the program $Q$ with an SDP variable $X_S$. So for each $S \subset [n]$, $|S| \le 2d$, we have an SDP variable. Then we arrange the variables into an $(n+1)^d \times (n+1)^d$ matrix $X$ in the natural way, with rows and columns indexed by every ordered subset of at most $d$ variables: </p><p align="center"><img src="http://learningwitherrors.org/sources/june2016-sos/sos-X.png" height="300" /></p> Now we enforce some natural constraints:

<ul> <li> “Commutativity” or “symmetry”: If the ordered multisets $S,T,U,V \subset [n]$ are such that $S \cup T = U \cup V$ as unordered multisets, then $X_{S\cup T} = X_{U \cup V}$. This is meant to reflect the commutative property of monomials. That is, for any $x\in \R^n$ \[ \prod_{i \in S}x_i \cdot \prod_{j\in T}x_j = \prod_{k \in U}x_k \cdot \prod_{\ell \in V} x_{\ell}. \] </li><li> “Normalization”: we set $X_{\emptyset} = 1$. This is the “scale” of the coefficients. One way to see that this is the correct scale is to think of $X_{\emptyset}$ as the monomial multiplier of a polynomial's constant term. </li><li> “PSDness”: we require that $X \succeq 0$, or that $X$ is positive-semidefinite. This constraint is natural because for any point $y \in \R^n$, if we take the matrix $X=X_y$ given by setting $X_{S} = \prod_{i\in S} y_i$, the resulting matrix $X$ is PSD. The proof is that if we take the vector $\tilde{y}$ so that $\tilde{y}^{\top} = [1 \ y^\top]$, then $X_y = \tilde{y}^{\otimes d}(\tilde{y}^{\otimes d})^\top$ (where $y^{\otimes d}$ is the $d$th Kronecker power<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> The Kronecker product of an $n \times m$ matrix $A$ and a $\ell \times k$ matrix $B$ is a $n\ell \times mk$ matrix $A \otimes B$, which we can naturally index by pairs so that the $(a,b),(c,d)$th entry is the product of $A_{ac}B_{bd}$. So, the Kronecker product of an $n \times 1$ vector $x$ with itself is a $n^2 \times 1$ vector whose $(i,j)$th entry is simply the product $x_ix_j$. </span> of $y$ ), and thus for any vector $v$, $v^\top X_yv = \langle v, \tilde{y}^{\otimes d}\rangle^2 \ge 0$.
<p>

</p></li></ul>


<p>

</p><blockquote><b>Remark 1</b> <em> Notice that any feasible solution $y \in \R^n$ for the program $Q$ yields a feasible solution to $\sos_d(Q)$: we just assign $X_S := \prod_{i\in S}y_i$, and the above arguments show that this is feasible. </em></blockquote>

<p>


</p><p>
One nice consequence of these constraints is that, if we evaluate the square of some degree-$d$ polynomial $q$ in the SDP monomials, the polynomial value will be non-negative! This is because, if $\hat q$ is the vector of coefficients of the polynomial $q$, then $q^2(X) = \hat{q}^\top X \hat{q} \ge 0$.
</p><p>

</p><blockquote><b>Example 2</b> <em> Consider the square polynomial $(x_1 + c \cdot x_2)^2$. We would evaluate this square in $X$ by taking the quadratic form \[ \begin{bmatrix} 0 &amp; 1 &amp; c \end{bmatrix} \begin{bmatrix} X_{\emptyset} &amp; X_{\{1\}} &amp; X_{\{2\}} \\ X_{1} &amp; X_{\{1,1\}} &amp; X_{\{1,2\}} \\ X_{2} &amp; X_{\{2,1\}} &amp; X_{\{2,2\}}\\ \end{bmatrix} \begin{bmatrix} 0\\ 1 \\ c \end{bmatrix} = X_{1,1} + c\cdot X_{1,2} + c\cdot X_{2,1} + c^2 \cdot X_{2,2}. \] </em></blockquote>

<p>


</p><p>
<br />

We now formalize the above definition.
</p><blockquote><b>Definition 1 (Sum-of-Squares Relaxation for $Q$ at degree $2d$)</b> <em> Given a polynomial optimization problem $Q$ with $\deg(p)\le 2d$ and $\deg(g_i) \le 2d\ \forall i \in [m]$, we define the <em>degree-$2d$ sum-of-squares relaxation</em> for $Q$, $\sos_{d}(Q)$.
</em><p><em>
We define a variable $X_{S}$ for each unordered multiset $S \subset [n]$ of size $|S| \le 2d$, and define the $(n+1)^d \times (n+1)^d$ matrix $X$, with rows and columns indexed by ordered multisets $U,V \subset [n]$, so that the $U,V$th entry of $X$ contains the variable $X_{U\cup V}$. Define the linear operator $\tilde{E}:\text{polynomials}_{\le 2d}\to \R$ such that $\tilde{E}[\prod_{i\in S} x_i] = X_S$ for $|S| \le 2d$. Then, \[ \sos_d(Q) = \left\{ \max \ \tilde{E}[p(x)] \quad s.t.\quad \begin{aligned} &amp;X \succeq 0,\\ &amp;X_{\emptyset} = 1,\\ &amp;\tilde{E}[ g_i(X)\cdot\prod_{i\in U} x_i] = 0\quad \forall i \in [m], U \subset [n], \deg(g_i) + |U| \le 2d \end{aligned} \right\} \] </em></p></blockquote>

<p>


</p><p>

</p><p>
<b>Sum-of-Squares Hierarchy.</b> Earlier, we only mentioned that we must have $2d \ge \deg(p), \deg(g_i) \forall i \in [m]$. In fact, we can choose $d$ to be as large as we wish--as long as we are willing to solve an SDP with $n^{O(d)}$ variables and $n^{O(d)}$ constraints. Taking successively larger values for $d$ gives us a systematic way of adding constraints to our program, giving us a family of larger but more powerful programs as we increase the value of $d$; this is why we call the family of relaxations $\{\sos_d\}_{d = 1}^{\infty}$ the <em>sum-of-squares hierarchy</em>.
</p><p>

</p><p>
<b>How to make sense of the Sum-of-Squares SDP relaxation?</b> In the Goemans-Williamson SDP relaxation, there is a natural interpretation of the SDP as a vector program: if we view the positive semidefinite matrix solution to the SDP, $X$, according to its Cholesky decomposition $X = VV^{\top}$, then we can identify each node in the underlying graph $G$ with a unit vector corresponding to a row of the matrix $V$, and we can see that the objective function tries to push vectors corresponding to adjacent nodes apart on the unit sphere.
</p><p>
This geometric intuition is extremely crisp, but unfortunately it is hard to come up with an analogue of this in programs where we care about more than $2$ variables interacting at a time (i.e. when we have variables $X_S$ with $|S| \ge 3$). <em>As of now, we do not have a similar geometric understanding of general sum-of-squares SDP relaxations.</em> We can instead develop alternative ways to (partially) understand these SDP relaxations.
</p><p>

</p><p>
<b>Pseudomoments.</b> As a start, one perspective is to think of the variables $X_S$ as the “moments” of a fake distribution over solutions to the program $Q$.
</p><p>
If we were to actually solve the (non-convex) problem $Q$ (using some inefficient algorithm), what we would have is either a single solution $y^*\in \R^n$, or a distribution over some set of solutions $Y\subset \R^n$, which maximize $p$, so that \[ OPT(Q) = \E_{y \in Y} [p(y)]. \] We cannot expect that the solution to the relaxation $\sos_d(Q)$ comes from an <em>actual</em> distribution over feasible solutions, but our constraints ensure that it still satisfies some of the properties of actual distributions.<sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span id="footnote2" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a>  Because of our constraints on $\sos_d(Q)$, the pseudoexpectation $\tilde{E}$ satisfies linearity of expectation \[ \tilde{E}[\alpha\cdot q_1(x) + \beta \cdot q_2(x)] = \alpha \cdot \tilde{E}[q_1(x)] + \beta \cdot \tilde{E}[q_2(x)]\quad \text{if} \quad \deg(q_1),\deg(q_2) \le 2d,\]  and also the non-negativity of low-degree squares, \[ \tilde{E}[q(x)^2] \ge 0 \quad \text{if} \quad \deg(q)\le d. \]
</span></p><p>
 
 For this reason we can also call the solution to $\sos_d(Q)$ a <em>pseudodistribution</em>, and that is why we use the notation \[ \tilde{E}\left[\prod_{i\in S} x_i \right] = X_S. \] In other words, we interpret the variable $X_S$ as being the <em>pseudomoment</em> of the monomial $\prod_{i\in S} x_i$ under a <em>pseudodistribution</em> over solutions to $Q$.
</p><p>
Thinking about the SDP solution in this way can be helpful in designing algorithms (and in proving lower bounds), but I will not discuss this perspective further here (maybe in a future post).
</p><p>
</p><h2 class="tex">2. Sum-of-Squares Proofs </h2>
<p>
One immediate question is, why should the sum-of-squares relaxation be a good relaxation? When we design SDP algorithms for maximization problems, we want to bound \[ OPT(Q) \le OPT(\sos_d(Q)) \le \alpha \cdot OPT(Q), \] for $\alpha$ as close to $1$ as possible. Why should we expect $\alpha$ to be small?
</p><p>
We can give a concrete but somewhat technical answer to this question by considering the dual program: the dual program will give us a “sum-of-squares” proof of an upper bound on the primal program. In my opinion this is most easily explained via demonstration, so let's write down the dual program.
</p><p>
For convenience, we'll start by re-writing the primal program $\sos_d(Q)$ in a matrix-based notation. For two matrices $A,B$ of equal dimension, define the inner product $\langle A,B \rangle = \sum_{(i,j)} A_{ij} B_{ij}$. Now, define $(n+1)^d \times (n+1)^d$ matrices $P,G_1,\ldots,G_{m}$ so that $\langle P, X \rangle = \tilde{E}[p(x)]$ and $\langle G_i,X\rangle = \tilde{E}[g_i(x)]$ (where have redefined the polynomial constraints $g_1,\ldots,g_m$ to include most of our SDP constraints: symmetry/commutativity, and $g_i(x)\cdot X_U = 0$).
</p><blockquote><b>Example 3</b> <em> If we have $p(x) = \sum_{i}x_i^2$, then one could choose the matrix $P$ to contain the identity in the submatrix indexed by sets of cardinality $1$, and $0$ elsewhere. </em></blockquote>

<p>


</p><p>
Our program can now be written as the minimization problem, \[ \sos_d'(Q) = \left\{ \min_{X\succeq 0} - \langle P,X\rangle \quad s.t.\quad \langle G_i,X\rangle = 0 \quad \forall i \in [m], \langle J_{\emptyset},X\rangle = 1 \right\}, \] where $J_{\emptyset}$ is the matrix with a single $1$ in the entry $\emptyset,\emptyset$ and zeros elsewhere, and the constraint $\langle J_\emptyset, X\rangle = 1$ enforces normalization. The optimal value of $\sos_d(Q)'$ is the negation of the optimal value of $\sos_d(Q)$.
</p><p>
The dual is the SDP problem \[ \sos_d^+(Q) = \left\{ \max_{y\in \R^{m+1}} y_{\emptyset} \qquad s.t.\quad \left(-P - y_{\emptyset}\cdot J_{\emptyset} - \sum_{j\in[m]}y_j \cdot G_j\right) = S \succeq 0 \right\}. \] Fixing $y^*$ to be the optimal dual point, from the dual constraints we have that \[ P = -y^*_{\emptyset} \cdot J_{\emptyset} - S + \sum_{j} y^*_j\cdot G_j. \] By duality, we have that in the optimal solution of $\sos_d^+(Q)$, \[ y_{\emptyset}^*= c + OPT(\sos'_d(Q) = c - OPT(\sos_d(Q)) \] for some $c \ge 0$, and therefore taking $S' = S + c\cdot J_{\emptyset} \succeq 0$, \[ P = OPT \cdot J_{\emptyset} - S' + \sum_{j} y^*_j\cdot G_j. \]
</p><p>
We will turn this matrix equation into a polynomial equation. Let $x \in \R^n$, and let $\tilde{x} = [1 \ x^{\top}]^\top$. Now, let $S'$ have the Cholesky decomposition $S' =\sum ss^{\top}$. We take the quadratic form of the Kronecker power of $\tilde{x}$ with the left- and right-hand sides, \begin{align*} (\tilde{x}^{\otimes d})^{\top} P (\tilde{x}^{\otimes d}) &amp;= OPT - \sum \langle s, \tilde{x}^{\otimes d}\rangle^2 + \sum_{j\in[m]} y_j^* \cdot (\tilde{x}^{\otimes d})^{\top} G_j (\tilde{x}^{\otimes d}) \end{align*} and re-writing each of the above vector products as polynomials, where $q_s$ is the polynomial encoded by the vector of coefficients $s$ \begin{align*} p(x) &amp;= OPT -\sum q_s(x)^2 - \sum_{j\in[m]} y_j^* \cdot g_j(x). \end{align*} This final line is a <em>sum-of-squares proof</em> that the value of $p(x)$ cannot exceed $OPT(\sos_d(Q))$ on the feasible region: any feasible point $x \in \R^n$ evaluates to $0$ for each $g_i$, and the square polynomials $q_s(x)^2$ can never contribute positively to the right-hand side. We have thus proven the following theorem:
</p><blockquote><b>Theorem 2</b> <em> The dual of the SDP $\sos_d(Q)$ provides a degree-$d$ sum-of-squares proof that $p(x) \le OPT(\sos_d(Q))$ for all $x$ in the feasible region of $Q$. </em></blockquote>

<p>


</p><p>
At the start of this section, our goal was to understand how to bound \[ OPT(Q) \le OPT(\sos_d(Q)) \le \alpha \cdot OPT(Q). \] This theorem gives us a primal-dual tool for bounding the value of $\sos_d(Q)$--{if we can provide a sum-of-squares proof of degree at most $d$ that $p(x) \le \alpha\cdot OPT(Q)$, then that sum-of-squares proof is a valid dual certificate!}
</p><p>

</p><p>
<b>Degree of the proof.</b> Notice that the dual can only use polynomials of degree at most $d$ in the sum-of-squares proof. So, suppose now that we write down two SDP relaxations for $Q$: $\sos_d(Q)$ and $\sos_{d'}(Q)$ for some $d' &gt; d$. Then clearly, \[ OPT(\sos_{d}(Q)) \ge OPT(\sos_{d'}(Q)) \ge OPT(Q), \] since the degree-$d'$ sum-of-squares program contains more constraints than the degree-$d$ program.
</p><p>
In the primal, it is difficult to understand exactly what these additional constraints buy you. From the perspective of the dual, the power of these additional constraints becomes clearer: the dual now has access to sum-of-squares proofs that use polynomials of <em>higher degree</em>, and this additional power may allow the dual to prove a potentially tighter upper bound. This is still a relatively mysterious condition, but in a later post I will give some concrete examples of situations in which it helps.
</p><p>
</p><h2 class="tex">3. Planted Sparse Vector </h2>
<p>
In this section, we give one algorithmic application: the planted sparse vector problem.
</p><p>
 Given an $n \times d$ matrix $A$, distinguish between the following two cases:

</p><ul> <li> If the columns of $A$ are uniformly sampled from a $d$-dimensional subspace of $\R^n$ which contains a vector with at most $k &lt; n/100$ nonzero entries, return YES, </li><li> If the columns of $A$ are sampled from a uniformly random $d$-dimensional subspace of $\R^n$, return NO with high probability.
</li></ul>

  This is a somewhat simple variant of the problem--other variants ask you to find the sparse vector as well. The exposition for this pared-down “distinguishing” version is simpler, and gets the main ideas across.
<p>
Without loss of generality, we may apply a random rotation $R \in \R^{d\times d}$ to the columns of $A$, then normalize by the maximum column norm, so that we work with $A \leftarrow \max_{i\in[d]} \frac{1}{\|ARe_i\|}AR$. This is to ensure that the columns of $A$ have roughly the same norm, are roughly orthogonal, and all have norm roughly $1$--for the remainder of the post we will assume that these conditions all hold.
</p><p>
We introduce the following polynomial optimization problem $Q_{sparse}$ for the planted sparse vector problem: \[ Q_{sparse}(A) = \left\{ \max_{x \in R^d} \| Ax \|^4_4 \qquad s.t. \qquad \|x\|^2_2 = 1 \right\} \] In other words, we want to find the linear combination of the columns of $A$ that will maximize the $4$-norm of $A$, while having $2$-norm roughly $1$. This program picks out sparse vectors over balanced vectors: a unit vector $e_i$ with only one nonzero entry has $\|e_i\|^2_2 = \|e_i\|^4_4 = 1$, while a unit vector $v$ with all $n$ entries of the same magnitude has $\|v\|_4^4 = n \cdot (1/\sqrt{n})^4 = 1/n \ll \|v\|_2^2$.
</p><p>
We prove the following theorem:
</p><blockquote><b>Theorem 3</b> <em>(Barak-Brandao-Harrow-Kelner-Steurer-Zhao '12) <a name="thmplsp"></a> If $1/k \ge \tilde{O}(\sqrt{d^3/n^3} + 1/n)$, then $\sos_4(Q_{sparse}(A))$ solves the planted $k$-sparse vector in a random subspace problem. </em></blockquote>

<p>


</p><p>
We will prove this by showing that the value of the program is large in the planted case, and small in the random case. It is actually possible to prove a better tradeoff between $k,d$ and $n$, but to simplify the arguments, we prove a weaker theorem. For the full details, see [Barak-Brandao-Harrow-Kelner-Steurer-Zhao '12].
</p><p>
<em>Proof:</em>  If the span of the columns of $A$ actually contains a $k$-sparse vector $v^*$, then $\|v^*\|_4^4$ is minimized when all entries of $v^*$ have equal magnitude. So, if we normalize $v^*$ so that $\|v^*\| = 1$, \[ \|v^*\|^4_4 \le k\cdot \left(\frac{1}{\sqrt{k}}\right)^{4} = \frac{1}{k}. \]
</p><p>
We will show that in the random case, the value is bounded by a function of $n$ and $d$:
</p><blockquote><b>Lemma 4</b> <em><a name="lemrandomcase"></a> If $A$ has iid Gaussian columns with $\E[A_{ij}^2] = \frac{1}{n}$, then with high probability the program $\sos_4(Q_{sparse}(A))$ has optimal value $\tilde{O}(\sqrt{d^3/n^3} + 1/n)$. </em></blockquote>

<p>

 Given this lemma, the proof of Theorem <a href="http://learningwitherrors.org/atom.xml#thmplsp">3</a> is essentially trivial--we know that the objective value at most $\tilde{O}(\sqrt{d^3/n^3} + 1/n)$ with high probability in the random case, and at least $1/k$ in the planted case, and so the objective value of $\sos_4(Q)$ distinguishes so long as $1/k \ge\tilde{O}(\sqrt{d^3/n^3} + 1/n)$. $$\tag*{$\blacksquare$}$$
</p><p>
Now, we prove the lemma, using sum-of-squares proofs to bound the objective value of the SDP in the random case. <em>Proof of Lemma <a href="http://learningwitherrors.org/atom.xml#lemrandomcase">4</a>:</em>  For any $d^2 \times d^2$ matrix $M$, there is a sum-of-squares proof of the following fact: \begin{align*} \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ M\right\rangle &amp;\le \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \|M\|\cdot \Id \right\rangle. \end{align*} The proof simply follows because $\|M\|\cdot \Id \succeq M$, and therefore $M = \|M\|\cdot \Id - S$ for some $S \succeq 0$; by taking the Cholesky decomposition of $S$ and using the vectors as polynomial coefficients, this gives us a sum-of-squares proof of the inequality.
</p><p>
We will use this sum-of-squares fact to bound the SDP value of our objective function. First, we re-interpret our objective function as an inner product of two matrices. Let $a_1,\ldots,a_n$ be the rows of $A$. We will re-write our objective function as a matrix inner-product: \begin{align*} \|Ax\|^4_4 &amp;= \sum_{i}\langle a_i, x \rangle^4 = \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \sum_i (a_i \otimes a_i)(a_i \otimes a_i)^{\top}\right\rangle. \end{align*} At this point, we could apply the above trick, but unfortunately, the maximum eigenvalue of $\sum_i (a_i\otimes a_i)(a_i \otimes a_i)^{\top}$ is $\approx d/n$--much larger than our goal of $\sqrt{d^3/n^3}$. This is because at indices $(\alpha\beta,\gamma\delta)$ where $\alpha = \beta$ and $\gamma = \delta$, our matrix has positive entries, whereas most entries have a random sign. These positive entries create a large eigenvalue in the matrix.
</p><p>
So, we will decompose this further--we will separate the portion of the matrix with entries corresponding to even-multiplicity indices. Define $B_{\neq}$ to be the matrix $\sum_{i} (a_i \otimes a_i)(a_i \otimes a_i)^{\top}$ in which all even-multiplicity entries are zeroed out. \begin{align*} \|Ax\|^4_4 &amp;= \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ B_{\neq} \right\rangle + \sum_{\alpha,\beta \in [n]} x_{\alpha}^2x_{\beta}^2\cdot \sum_{i} a_{i}(\alpha)^2 a_{i}(\beta)^2. \end{align*} By the above arguments, there is a sum-of-squares proof that \begin{align*} \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ B_{\neq}\right\rangle &amp;\le \left\langle x^{\otimes 2}(x^{\otimes 2})^{\top}, \ \|B_{\neq}\|\cdot \Id \right\rangle\\ &amp;= \|B_{\neq}\|\cdot \sum_{\alpha,\beta\in[d]} x_\alpha^2 x_{\beta}^2 = \|B_{\neq}\|\cdot \left(\sum_{\alpha\in[d]} x_{\alpha}^2 \right)^2. \end{align*}
</p><p>
For the other term, we will use an even simpler bound. Let $c_{\alpha,\beta} = \sum_{i} a_i(\alpha)^2 a_j(\beta)^2$, for convenience. Also, let $c^* = \max_{\alpha,\beta} c_{\alpha,\beta}$. The following equality, \[ \sum_{\alpha,\beta\in[d]}c_{\alpha,\beta}\cdot x_{\alpha}^2 x_{\beta}^2 = c^*\cdot \sum_{\alpha, \beta} x_{\alpha}^2 x_{\beta}^2 - \left(\sum_{\alpha,\beta} (c^* - c_{\alpha,\beta}) \cdot x_{\alpha}^2 x_{\beta}^2\right), \] is a sum-of-squares proof that \[ \sum_{\alpha,\beta\in[d]}c_{\alpha,\beta}\cdot x_{\alpha}^2 x_{\beta}^2 \le c^*\cdot \sum_{\alpha, \beta} x_{\alpha}^2 x_{\beta}^2, \] because $c^* - c_{\alpha,\beta} \ge 0$ for all $\alpha,\beta$ by definition, and thus the parenthesized term is a sum-of-squares.
</p><p>
Putting the two arguments together, we have a sum-of-squares proof that \[ \|Ax\|_4^4 \le \left(\|B_{\neq}\| + c^*\right)\cdot \left(\sum_{\alpha} x_{\alpha}^2\right) \] Because we have the SDP constraint that $\|x\|_2^2 = 1$, the objective value is thus bounded by \[ \tilde{E}[\|Ax\|_4^4] = (\|B_{\neq} \|+ c^*) \cdot \tilde{E}\left[\left(\sum_{\alpha\in [d]}x_{\alpha}^2\right)^2\right] = \|B_{\neq}\| + c^*. \] The final step in the proof consists of showing that with high probability over the choice of $A$, \[ \|B_{\neq}\| \le \tilde{O}(\sqrt{d^3/n^3})\quad \text{and}\quad c^* \le \tilde{O}(1/n). \] The first fact we can prove using a matrix Chernoff bound, and the second fact we can prove using a Chernoff bound and a union bound. This concludes the proof! $$\tag*{$\blacksquare$}$$
</p><p>

To get the theorem with the better parameters mentioned above, Barak et al.
remove the even-multiplicity indeces more carefully: they project away from the subspace containing the vectors which correlate too much with the even-multiplicity entries (whereas we just zeroed them out).
This more careful treatment lets them prove a better matrix concentration result.
</p><p>

</p><h2 class="tex">4. Other Resources </h2> Check out the following other resources for historical details and more sum-of-squares algorithms/lower bounds:

<ul> <li> For notes about SDPs and duality, I like these notes by Lap Chi Lau:
<p>
<a href="https://cs.uwaterloo.ca/ lapchi/cs270/notes.html">https://cs.uwaterloo.ca/ lapchi/cs270/notes.html</a>
</p><p>
I also like these notes by Anupam Gupta and Ryan O'Donnell:
</p><p>
<a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/">https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/</a> </p></li><li> Lecture notes from Boaz Barak on sum-of-squares:
<p>
<a href="http://www.boazbarak.org/sos/">http://www.boazbarak.org/sos/</a>
</p><p>
</p></li><li> Lecture notes from Massimo Lauria on sum-of-squares and other relaxations for polynomial optimization <a href="http://www.csc.kth.se/ lauria/sos14/">http://www.csc.kth.se/ lauria/sos14/</a> </li><li> The introduction of this paper by Barak, Kelner and Steurer: <a href="https://arxiv.org/pdf/1312.6652v1.pdf">https://arxiv.org/pdf/1312.6652v1.pdf</a>
<p>
The appendix of the paper also contains many sum-of-squares proofs of basic inequalities (e.g. Cauchy-Schwarz) that can be of use for providing good dual certificates.
</p></li></ul></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Tselil Schramm <a href="http://learningwitherrors.org/2016/06/23/intro-sos/"><span class="datestr">at June 23, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/06/03/small-bias">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/06/03/small-bias/">Simple Lower Bounds for Small-bias Spaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>
I was reading about PRGs recently, and I think a lemma mentioned last time (used for Johnson-Lindenstrauss lower-bounds) can give simple lower-bounds for $\epsilon$-biased spaces.
</p><p>
Notice:

</p><ul> <li> $2^n$ mutually orthogonal vectors requires dimension at least $2^n$, but $2^n$ “almost orthogonal” vectors with pairwise inner-products $|\innp{v_i, v_j}| \leq \epsilon$ exists in dimension $O(n/\epsilon^2)$, by Johnson-Lindenstrauss. </li><li> Sampling $n$ iid uniform bits requires a sample space of size $2^n$, but $n$ $\epsilon$-biased bits can be sampled from a space of size $O(n/\epsilon^2)$.
</li></ul>


<p>
First, let's look at $k$-wise independent sample spaces, and see how the lower-bounds might be extended to the almost $k$-wise independent case.
</p><p>
<i>Note: To skip the background, just see Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a>, and its application in Claim <a href="http://learningwitherrors.org/atom.xml#claimkeps">3</a>. </i>

</p><p>
</p><h2 class="tex">1. Preliminaries </h2> What “size of the sample space” means is: For some sample space $S$, and $\pm 1$ random variables $X_i$, we will generate bits $x_1, \dots x_n$ as an instance of the r.vs $X_i$. That is, by drawing a sample $s \in S$, and setting $x_i = X_i(s)$. We would like to have $|S| \ll 2^n$, so we can sample from it using less than $n$ bits.
<p>
Also, any random variable $X$ over $S$ can be considered as a vector $\t X \in \R^{|S|}$, with coordinates $\t X[s] := \sqrt{\Pr[s]} X(s)$. This is convenient because $\innp{\t X, \t Y} = \E[XY]$.
</p><p>
</p><h2 class="tex">2. Exact $k$-wise independence </h2> <a name="seckwise"></a> A distribution $D$ on $n$ bits is <em>$k$-wise independent</em> if any subset of $k$ bits are iid uniformly distributed. Equivalently, the distribution $D : \{\pm 1\}^n \to \R_{\geq 0}$ is $k$-wise independent iff the Fourier coefficients $\hat D(S) = 0$ for all $S \neq 0, |S| \leq k$.
<p>
$n$ such $k$-wise independent bits can be generated from a seed of length $O(k \log n)$ bits, using say Reed-Solomon codes. That is, the size of the sample space is $n^{O(k)}$. This size is optimal, as the below claim shows (adapted from Umesh Vazirani's lecture notes [<a href="http://learningwitherrors.org/atom.xml#ref-Vaz99">Vaz99</a>]).
</p><blockquote><b>Claim 1</b> <em> <a name="claimkwise"></a> Let $D$ be a $k$-wise independent distribution on $\{\pm 1\}$ random variables $x_1, \dots, x_n$, over a sample space $S$. Then, $|S| = \Omega_k(n^{k / 2})$. </em></blockquote>

<p>


</p><p>
<em>Proof:</em>  For subset $T \subseteq [n]$, let $\chi_T(x) = \prod_{i \in T} x_i$ be the corresponding Fourier character. Consider these characters as vectors in $\R^{|S|}$ as described above, with \[\innp{\chi_A, \chi_B} = \E_{x \sim D}[\chi_A(x)\chi_B(x)] \]
</p><p>
 Let $J$ be the family of all subsets of size $\leq k/2$. Note that, for $A, B \in J$, the characters $\chi_A, \chi_B$ are orthogonal: \begin{align*} \innp{\chi_A, \chi_B} &amp;= \E_{x \sim D}[\chi_A(x)\chi_B(x)]\\ &amp;= \E_{x \sim D}[(\prod_{i \in A \cap B} x_i^2)(\prod_{i \in A \Delta B} x_i)]\\ &amp;= \E_{x \sim D}[\chi_{A \Delta B}(x)] \note{since $x_i^2 = 1$}\\ &amp;= 0 \note{since $|A \Delta B| \leq k$, and $D$ is $k$-wise independent} \end{align*} Here $A \Delta B$ denotes symmetric difference, and the last equality is because $\chi_{A \Delta B}$ depends on $\leq k$ variables, so the expectation over $D$ is the same as over iid uniform bits.
</p><p>
 Thus, the characters $\{\chi_A\}_{A \in J}$ form a set of $|J|$ mutually-orthogonal vectors in $\R^{|S|}$. So we must have $|S| \geq |J| = \Omega_k(n^{k/2})$. $$\tag*{$\blacksquare$}$$
</p><p>
The key observation was relating independence of random variables to linear independence (orthogonality). Similarly, we could try to relate $\epsilon$-almost $k$-wise independent random variables to almost-orthogonal vectors.
</p><p>
</p><h2 class="tex">3. Main Lemma </h2> This result is Theorem 9.3 from Alon's paper [<a href="http://learningwitherrors.org/atom.xml#ref-Alo03">Alo03</a>]. The proof is very clean, and Section 9 can be read independently. <sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a>  Theorem 9.3 is stated in terms of lower bounding the rank of a matrix $B \in \R^{N \x N}$ where $B_{i,i} = 1$ and $|B_{i, j}| \leq \epsilon$. The form stated here follows by defining $B_{i, j} := \innp{v_i, v_j}$.  </span>
<p>

</p><blockquote><b>Lemma 1</b> <em> <a name="lemrank"></a> Let $\{v_i\}_{i \in [N]}$ be a collection of $N$ unit vectors in $\R^d$, such that $|\innp{v_i, v_j}| \leq \epsilon$ for all $i \neq j$. Then, for $\frac{1}{\sqrt{N}} \leq \epsilon \leq 1/2$, \[d \geq \Omega\left(\frac{\log N}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>


</p><p>
This lower-bound on the dimension of “almost-orthogonal” vectors translates to a nearly-tight lower-bound on Johnson-Lindenstrauss embedding dimension, and will also help us below.
</p><p>
</p><h2 class="tex">4. Small bias spaces </h2> A distribution $D$ on $n$ bits is <em>$\epsilon$-biased w.r.t linear tests</em> (or just “$\epsilon$-biased”) if all $\F_2$-linear tests are at most $\epsilon$-biased. That is, for $x \in \{\pm 1\}^n$, the following holds for all subsets $S \subseteq [n]$: \[\left|\E_{x \sim D}[\chi_S(x)]\right| = \left|\Pr_{x \sim D}[\chi_S(x) = 1] - \Pr_{x \sim D}[\chi_S(x) = -1]\right| \leq \epsilon\] Similarly, a distribution is <em>$\epsilon$-biased w.r.t. linear tests of size $k$</em> (or “$k$-wise $\epsilon$-biased) if the above holds for all subsets $S$ of size $\leq k$.
<p>
There exists an $\epsilon$-biased space on $n$ bits of size $O(n / \epsilon^2)$: a set of $O(n / \epsilon^2)$ random $n$-bit strings will be $\epsilon$-biased w.h.p. Further, explicit constructions exist that are nearly optimal: the such first construction was in [<a href="http://learningwitherrors.org/atom.xml#ref-NN93">NN93</a>], and was nicely simplified by [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>] (both papers are very readable).
</p><p>
These can be used to sample $n$ bits that are $k$-wise $\epsilon$-biased, from a space of size almost $O(k \log(n)/\epsilon^2)$; much better than the size $\Omega(n^k)$ required for perfect $k$-wise independence. For example<sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span id="footnote2" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a>  This can be done by composing an $(n, k')$ ECC with dual-distance $k$ and an $\epsilon$-biased distribution on $k' = k\log n$ bits. Basically, use a linear construction for generating $n$ exactly $k$-wise independent bits from $k'$ iid uniform bits, but use an $\epsilon$-biased distribution on $k'$ bits as the seed instead.  </span>, see [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>] or the lecture notes [<a href="http://learningwitherrors.org/atom.xml#ref-Vaz99">Vaz99</a>].
</p><p>
</p><h3 class="tex">4.1. Lower Bounds</h3> The best lower bound on size of an $\epsilon$-biased space on $n$ bits seems to be $\Omega(\frac{n}{\epsilon^2 \log(1/\epsilon)})$, which is almost tight. The proofs of this in the literature (to my knowledge) work by exploiting a nice connection to error-correcting codes: Say we have a sample space $S$ under the uniform measure. Consider the characters $\chi_T(x)$ as vectors $\t \chi_T \in \{\pm 1\}^{|S|}$ defined by $\t \chi_T[s] = \chi_T(x(s))$, similar to what we did in Section <a href="http://learningwitherrors.org/atom.xml#seckwise">2</a>. The set of $2^n$ vectors $\{\t \chi_T\}_{T \subseteq [n]}$ defines the codewords of a linear code of length $|S|$ and dimension $n$. Further, the hamming-weight of each codeword (number of $-1$s in each codeword, in our context), is within $n(\frac{1}{2} \pm \epsilon)$, since each parity $\chi_T$ is at most $\epsilon$-biased. Thus this code has relative distance at least $\frac{1}{2} - \epsilon$, and we can use sphere-packing-type bounds from coding-theory to lower-bound the codeword length $|S|$ required to achieve such a distance. Apparently the “McEliece-Rodemich-Rumsey-Welch bound” works in this case; a more detailed discussion is in [<a href="http://learningwitherrors.org/atom.xml#ref-AGHP92">AGHP92</a>, Section 7].
<p>
We can also recover this same lower bound using Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> in a straightforward way.
</p><p>

</p><blockquote><b>Claim 2</b> <em> <a name="claimepsbias"></a> Let $D$ be an $\epsilon$-biased distribution on $n$ bits $x_1, \dots, x_n$, over a sample space $S$. Then, \[|S| = \Omega\left(\frac{n}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>

 <em>Proof:</em>  Following the proof of Claim <a href="http://learningwitherrors.org/atom.xml#claimkwise">1</a>, consider the Fourier characters $\chi_T(x)$ as vectors $\t \chi_T \in \R^{|S|}$, with $\t \chi_T[s] = \sqrt{\Pr[s]} \chi_T(x(s))$. Then, for all distinct subsets $A, B \subseteq [n]$, we have \[\innp{\t \chi_A, \t \chi_B} = \E_{x \sim D}[\chi_A(x)\chi_B(x)] = \E_{x \sim D}[\chi_{A \Delta B}(x)]\] Since $D$ is $\epsilon$-biased, $\left|\E_{x \sim D}[\chi_{A \Delta B}(x)]\right| \leq \epsilon$ for all $A \neq B$. Thus, applying Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> to the collection of $N = 2^n$ unit vectors $\{\t \chi_T\}_{T \subseteq [n]}$ gives the lower bound $|S| = \Omega\left(\frac{n}{\epsilon^2 \log(1/\epsilon)}\right)$. $$\tag*{$\blacksquare$}$$
</p><p>
This also nicely generalizes the proof of Claim <a href="http://learningwitherrors.org/atom.xml#claimkwise">1</a>, to give an almost-tight lower bound on spaces that are $\epsilon$-biased w.r.t linear tests of size $k$.
</p><p>

</p><blockquote><b>Claim 3</b> <em> <a name="claimkeps"></a> Let $D$ be a distribution on $n$ bits that is $\epsilon$-biased w.r.t. linear tests of size $k$. Then, the size of the sample space is \[|S| = \Omega\left(\frac{k \log (n/k)}{\epsilon^2 \log(1/\epsilon)}\right)\] </em></blockquote>

<p>

 <em>Proof:</em>  As before, consider the Fourier characters $\chi_T(x)$ as vectors $\t \chi_T \in \R^{|S|}$, with $\t \chi_T[s] = \sqrt{\Pr[s]} \chi_T(x(s))$. Let $J$ be the family of all subsets $T \subseteq [n]$ of size $\leq k/2$. Then, for all distinct subsets $A, B \in J$, we have \[\left|\innp{\t \chi_A, \t \chi_B}\right| = \left|\E_{x \sim D}[\chi_{A \Delta B}(x)]\right| \leq \epsilon\] since $|A \Delta B| \leq k$, and $D$ is $\epsilon$-biased w.r.t such linear tests. Applying Lemma <a href="http://learningwitherrors.org/atom.xml#lemrank">1</a> to the collection of $|J|$ unit vectors $\{\t \chi_T\}_{T \in J}$ gives $|S| = \Omega(\frac{k \log (n/k)}{\epsilon^2 \log(1/\epsilon)})$. $$\tag*{$\blacksquare$}$$
</p><p>
<i>Note: I couldn't find the lower bound given by Claim <a href="http://learningwitherrors.org/atom.xml#claimkeps">3</a> in the literature, so please let me know if you find a bug or reference.
</i></p><p><i>
Also, these bounds do not directly imply nearly tight lower bounds for <em>$\epsilon$-almost $k$-wise independent</em> distributions (that is, distributions s.t. their marginals on all sets of $k$ variables are $\epsilon$-close to the uniform distribution, in $\ell_{\infty}$ or $\ell_{1}$ norm). Essentially because of the loss in moving between closeness in Fourier domain and closeness in distributions. <sup><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span id="footnote3" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a>  Eg, $\epsilon$-biased $\implies$ $\epsilon$-close in $\ell_{\infty}$, but $\epsilon$-close in $\ell_{\infty}$ can be up to $2^{k-1}\epsilon$-biased. And $2^{-k/2}\epsilon$-biased $\implies$ $\epsilon$-close in $\ell_{1}$, but not the other direction.  </span> </i>
</p><p>
<br /></p><hr /><h3>References</h3>
<p>
<a name="ref-AGHP92">[AGHP92]</a> Noga Alon, Oded Goldreich, Johan Håstad, and Ren{é} Peralta.
 Simple constructions of almost k-wise independent random variables.
 <em>Random Structures \&amp; Algorithms</em>, 3(3):289--304, 1992.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/aghp4.pdf">http://www.tau.ac.il/~nogaa/PDFS/aghp4.pdf</a>.
</p><p>

</p><p>
<a name="ref-Alo03">[Alo03]</a> Noga Alon.
 Problems and results in extremal combinatorics, part i.
 <em>Discrete Math</em>, 273:31--53, 2003.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf">http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf</a>.
</p><p>

</p><p>
<a name="ref-NN93">[NN93]</a> Joseph Naor and Moni Naor.
 Small-bias probability spaces: Efficient constructions and
  applications.
 <em>SIAM journal on computing</em>, 22(4):838--856, 1993.
 URL: <a href="http://www.wisdom.weizmann.ac.il/~naor/PAPERS/bias.pdf">http://www.wisdom.weizmann.ac.il/~naor/PAPERS/bias.pdf</a>.
</p><p>

</p><p>
<a name="ref-Vaz99">[Vaz99]</a> Umesh Vazirani.
 k-wise independence and epsilon-biased k-wise indepedence.
 1999.
 URL:
  <a href="https://people.eecs.berkeley.edu/~vazirani/s99cs294/notes/lec4.pdf">https://people.eecs.berkeley.edu/~vazirani/s99cs294/notes/lec4.pdf</a>.
</p><p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Preetum Nakkiran <a href="http://learningwitherrors.org/2016/06/03/small-bias/"><span class="datestr">at June 03, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lwe.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss/">Fast Johnson-Lindenstrauss</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div style="display: none;"></div>
<p>
The Johnson-Lindenstrauss (JL) Transform says that, informally, we can embed high-dimensional points into a much lower dimension, while still preserving their pairwise distances. In this post we'll start with the classical JL transform, then focus on the Fast JL Transform (FJLT) by Ailon and Chazelle [<a href="http://learningwitherrors.org/atom.xml#ref-AC09">AC09</a>], which achieves the JL embedding more efficiently (w.r.t. runtime and randomness). We'll look at the FJLT from the perspective of “preconditioning” a sparse estimator, which comes with nice intuition from Fourier duality. We conclude by mentioning more recent developments in this area (faster/sparser/derandomizeder).

</p><p>
<b>Motivation.</b> It's an interesting structural result, and the JL transform also has algorithmic and machine-learning applications. Eg, any application that only depends on approximate pairwise distances (such as nearest-neighbors), or even on pairwise inner-products<sup><a href="http://learningwitherrors.org/atom.xml#footnote1">1</a></sup><span id="footnote1" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote1" name="footnote1">1.</a> Because, if the norms $||x_i-x_j||$, $||x_i||$, and $||x_j||$ are approximately preserved, then so is the inner-product $\innp{x_i,x_j}$.  </span> (such as kernel methods) may equivalently work with the dimensionality-reduced version of their input. This also has applications in sketching and scarification.
</p><p>
</p><h2 class="tex">1. Classical JL </h2> The JL embedding theorem is:
<p>

</p><blockquote><b>Theorem 1</b> <em> Given points $x_1, \dots, x_n \in \R^d$, and $\epsilon &gt; 0$, there exists an embedding $f: \R^d \to \R^k$ such that \[\forall i, j: \quad (1-\epsilon) ||x_i - x_j||_2 \leq ||f(x_i) - f(x_j)||_2 \leq (1+\epsilon) ||x_i - x_j||_2 \] and $k = O(\epsilon^{-2}\log n)$. </em></blockquote>

<p>

 That is, the embedding roughly preserves pairwise distances in $\ell_2$. Think of the regime $n \gg d \gg k$. Note that the target dimension $k = O(\epsilon^{-2}\log n)$ is (perhaps surprisingly) independent of the source dimension $d$, and only logarithmic in the number of points $n$.
</p><p>
In fact, a random linear map works as an embedding (w.h.p.). This is established by the following lemma.
</p><p>

</p><blockquote><b>Lemma 2</b> <em> <a name="lemjl"></a> For any $\delta &gt; 0$, set $k = O(\epsilon^{-2}\log(1/\delta))$, and let $A \in \R^{k \x d}$ be a random matrix with iid normal $\N(0, 1/k)$ entries. Then \[\forall x \in \R^d: \quad \Pr_{A}\left[~ ||Ax||_2^2 \in (1 \pm \epsilon)||x||_2^2 ~\right] \geq 1- \delta\] </em></blockquote>

<p>

 That is, a random matrix preserves the norm of vectors with good probability. To see that this implies the JL Theorem, consider applying the matrix $A$ on the $O(n^2)$ vectors of pairwise differences $(x_i - x_j)$. For fixed $i,j$, the lemma implies that $||A(x_i - x_j)|| \approx ||x_i - x_j||$ except w.p. $\delta$. Thus, setting $\delta = 1/n^3$ and union bounding, we have that $A$ preserves the norm of <em>all</em> differences $(x_i - x_j)$ with high probability. Letting the embedding map $f = A$, we have \[\forall i,j: \quad ||f(x_i) - f(x_j)|| = ||Ax_i - Ax_j|| = ||A(x_i - x_j)|| \approx ||x_i - x_j||\] as desired. <sup><a href="http://learningwitherrors.org/atom.xml#footnote2">2</a></sup><span id="footnote2" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote2" name="footnote2">2.</a> Note that it was important that $f$ be linear for us to reduce preserving pairwise-distances to preserving norms. </span>
</p><p>
<b>Runtime.</b> The runtime of embedding a single vector with this construction (setting $\delta=1/n^3$ and $\epsilon=O(1)$) is $O(dk) = O(d \log n)$. In the Fast JL below, we will show how to do this in time almost $O(d \log d)$.
</p><p>
We will now prove Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. First, note that our setting is scale-invariant, so it suffices to prove the lemma for all unit vectors $x$.
</p><p>
As a warm-up, let $g \in \R^d$ be a random vector with entires iid $\N(0, 1)$, and consider the inner-product \[Y := \innp{g, x}\] (for a fixed unit vector $x \in \R^d$). Notice the random variable $Y$ has expectation $\E[Y] = 0$, and variance \[\E[Y^2] = \E[\innp{g,x}^2] = ||x||^2\] <b>Thus, $Y^2$ is an unbiased estimator for $||x||^2$.</b> Further, it concentrates well: assuming (wlog) that $||x||=1$, we have $Y \sim \N(0, 1)$, so $Y^2$ has constant variance (and more generally, is subguassian<sup><a href="http://learningwitherrors.org/atom.xml#footnote3">3</a></sup><span id="footnote3" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote3" name="footnote3">3.</a>  Subguassian with parameter $\sigma$ basically means tail probabilities behave as a guassian with variance $\sigma^2$ would behave. Formally, a zero-mean random variable $X$ is “subgaussian with parameter $\sigma$” if: $\E[e^{\lambda X}] \leq e^{\sigma^2\lambda^2/2}$ for all $\lambda \in \R$.  </span> with a constant parameter).
</p><p>
This would correspond to a random linear projection into 1 dimension, where the matrix $A$ in Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a> is just $A = g^T$. Then $||Ax||^2 = \innp{g, x}^2 = Y^2$. However, this estimator does not concentrate well enough (we want tail probability $\delta$ to eventually be inverse-poly, not a constant).
</p><p>
We can get a better estimator for $||x||^2$ by averaging many iid copies. In particular, for any iid subgaussian random variables $Z_i$, with expectation $1$ and subguassian parameter $\sigma$, the Hoeffding bound gives \[\Pr\left[ \left|\left(\frac{1}{k}\sum_{i=1}^k Z_i \right) - 1 \right| &gt; \epsilon \right] \leq e^{-\Omega(k\epsilon^2/\sigma^2)}\] Applying this for $Z_i = Y_i^2$ and $\sigma = O(1)$, we can set $k = O(\epsilon^{-2}\log(1/\delta))$ so the above tail probability is bounded by $\delta$.
</p><p>
This is exactly the construction of Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. Each row of $A$ is $\frac{1}{\sqrt{k}} g_i^T$, where $g_i \in \R^d$ is iid $\N(0, 1)$. Then \[||A x||^2 = \sum_{i=1}^k \innp{\frac{1}{\sqrt{k}} g_i, x}^2 = \frac{1}{k} \sum_{i=1}^k \innp{g_i, x}^2 = \frac{1}{k} \sum_{i=1}^k Y_i^2\]
</p><p>
And thus, for $||x||=1$, \[\Pr\left[ \left| ||A x||^2 - 1 \right| &gt; \epsilon\right] \leq \delta\] as desired in Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a>. $$\tag*{$\blacksquare$}$$
</p><p>
To recap, the key observation was that if we draw $g \sim N(0, I_d)$, then $\innp{g, x}^2$ is a “good” estimator of $||x||^2$, so we can average $O(\epsilon^{-2}\log(1/\delta))$ iid copies and get an estimator within a multiplicative factor $(1 \pm \epsilon)$ with probability $\geq 1-\delta$. Filling the transform matrix $A$ with guassians is clearly not necessary; any distribution with iid entries that are subguassian would work, with the same proof as above. For example, picking each entry in $A$ as iid $\pm \frac{1}{\sqrt k}$ would work.
</p><p>
We can now think of different JL transforms as constructing different estimators of $||x||^2$. For example, can we draw from a distribution such that $g$ is sparse? (Not quite, but with some “preconditioning” this will work, as we see below).
</p><p>
</p><h2 class="tex">2. Fast JL </h2>
<p>
</p><h3 class="tex">2.1. First Try: Coordinate Sampling</h3> As a (bad) first try, consider the estimator that randomly samples a coordinate of the given vector $x$, scaled appropriately. That is, \[Y := \sqrt{d} ~x_j \quad\text{for uniformly random coordinate $j \in [d]$}\] Equivalently, draw a random standard basis vector $e_j$, and let $Y := \sqrt{d} \innp{e_j, x}$.
<p>
Notice that $Y^2$ has the right expectation: \[\E[Y^2] = \E_j[(\sqrt{d} x_j)^2] = d \E_j[x_j^2] = ||x||^2\] However, it does not concentrate well. The variance is $Var[Y^2] = Var[d x_j^2] = d^2 Var[x_j]$. If $x$ is a standard basis vector (say $x = e_1$), then this could be as bad as $Var[Y^2] \approx d$. This is bad, because it means we would need to average $\Omega(d)$ iid samples to get a sufficiently good estimator, which does not help us in reducing the dimension of $x \in \R^d$.
</p><p>
The bad case in the above analysis is when $x$ is very concentrated/sparse, so sampling a random coordinate of $x$ is a poor estimator of its magnitude. However, if $x$ is very “spread out”, then sampling a random coordinate would work well. For example, if all entries of $x$ are bounded by $\pm O(\sqrt{\frac{1}{d}})$, then $Var[Y^2] = O(1)$, and taking iid copies of this estimator would work. This would be nice for runtime, since randomly sampling a coordinate can be done quickly (it is not a dense inner-product).
</p><p>
Thus, if we can (quickly) “precondition” our vector $x$ to have $||x||_{\infty} \leq O(\sqrt{\frac{1}{d}})$, we could then use coordinate sampling to achieve a fast JL embedding. We won't quite achieve this, but we will be able to precondition such that $||x||_\infty \leq O(\sqrt{\frac{\log(d/\delta)}{d}})$, as described in the next section. With this in mind, we will need the following easy claim (that with the weaker bound on $\ell_\infty$, coordinate sampling works to reduce the dimension to almost our target dimension).
</p><p>

</p><blockquote><b>Lemma 3</b> <em> <a name="lemS"></a> Let $t = \Theta(\epsilon^{-2}\log(1/\delta)\log(d/\delta))$. Let $S \in \R^{t \x d}$ be a matrix with rows $s_i := \sqrt{\frac{d}{t}} e^{(i)}_{j_i}$, where each $j_i \in [d]$ is an iid uniform index. (That is, each row of $S$ randomly samples a coordinate, scaled appropriately). Then, for all $x$ s.t $||x||_2=1$ and $||x||_\infty \leq O(\sqrt{\frac{\log(d/\delta)}{d}})$, we have
</em><p><em>
 \[\Pr_{S}[ ||Sx||^2 \in 1 \pm \epsilon] \geq 1-\delta\] </em></p></blockquote>

<p>

 <em>Proof:</em>  \[||Sx||^2 = \sum_{i=1}^t \innp{\sqrt{\frac{d}{t}} e_{j_i}, x}^2 = \frac{1}{t}\sum_{i=1}^t d (x_{j_i})^2\] Then, the r.vs $\{d (x_{j_i})\}$ are iid and absolutely bounded by $O(\sqrt{\log(d/\delta)})$, so by Chernoff-Hoeffding and our choice of $t$, \[\Pr[ |\frac{1}{t}\sum_{i=1}^t d (x_{j_i})^2 - 1| &gt; \epsilon] \leq e^{-\Omega(\epsilon^2 t / \log(d/\delta))} \leq \delta\] $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">2.2. FJLT: Preconditioning with random Hadamard</h3> The main idea of FJLT is that we can quickly precondition vectors to be “smooth”, by using the Fast Hadamard Transform.
<p>
Recall the $d \x d$ Hadamard transform $H_d$ (for $d$ a power of 2) is defined recursively as \[H_1 := 1 ,\quad H_{2d} := \frac{1}{\sqrt{2}} \bmqty{H_d &amp; H_d\\H_d &amp; -H_d}\] More explicitly, $H_d[i,j] = \frac{1}{\sqrt{d}} (-1)^{\innp{i, j}}$ where indices $i,j \in \{0, 1\}^{\log d}$, and the inner-product is mod 2. The Hadamard transform is just like the discrete Fourier transform<sup><a href="http://learningwitherrors.org/atom.xml#footnote4">4</a></sup><span id="footnote4" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote4" name="footnote4">4.</a> Indeed, it is exactly the Fourier transform over the group $(\Z_2)^n$. For more on Fourier transforms over abelian groups, see for example <a href="https://lucatrevisan.wordpress.com/2016/03/16/cs294-lecture-15-abelian-cayley-graphs/">Luca's notes</a>. </span> : it can be computed in time $O(d\log d)$ by recursion, and it is an orthonormal transform.
</p><p>
Intuitively, the Hadamard transform may be useful to “spread out” vectors, since Fourier transforms take things that are sparse/concentrated in time-domain to things that are spread out in frequency domain (by time-frequency duality/Uncertainty principle). Unfortunately this won't quite work, since duality goes both ways: It will also take vectors that are already spread out and make them sparse.
</p><p>
To fix this, it turns out we can first randomize the signs, then apply the Hadamard transform.
</p><p>

</p><blockquote><b>Lemma 4</b> <em> <a name="lemHadamard"></a> Let $H_d$ be the $d \x d$ Hadamard transform, and let $D$ be a random diagonal matrix with iid $\pm 1$ entries on the diagonal. Then, \[\forall x \in \R^d, ||x||=1: \quad \Pr_{D}[ ||H_d D x||_\infty &gt; \Omega(\sqrt{\frac{\log(d/\delta)}{d}}) ] \leq \delta\] </em></blockquote>

<p>


</p><p>
We may expect something like this to hold: randomizing the signs of $x$ corresponds to pointwise-multiplying by random white noise. White noise is spectrally flat, and multiplying by it in time-domain corresponds to convolving by its (flat) spectrum in frequency domain. Thus, multiplying by $D$ should “spread out” the spectrum of $x$. Applying $H_d$ computes this spectrum, so should yield a spread-out vector.
</p><p>
The above intuition seems messy to formalize but the proof is surprisingly simple. <sup><a href="http://learningwitherrors.org/atom.xml#footnote5">5</a></sup><span id="footnote5" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote5" name="footnote5">5.</a> This proof presented slightly differently from the one in Alon-Chazelle, but the idea is the same. </span>
</p><p>
<em>Proof:</em> } Consider the first entry of $H_d D x$. Let $D = diag(a_1, a_2, \dots, a_d)$ where $a_i$ are iid $\pm 1$. The first row of $H_d$ is $\frac{1}{\sqrt{d}} \bmqty{1 &amp; 1 &amp; \dots &amp; 1}$, so \[(H_d D x)[1] = \frac{1}{\sqrt{d}} \sum_i a_ix_i\] Here, the $x_i$ are fixed s.t. $||x||_2=1$, and the $a_i = \pm 1$ iid. Thus we can again bound this by Hoeffding <sup><a href="http://learningwitherrors.org/atom.xml#footnote6">6</a></sup><span id="footnote6" class="sidenote"><a href="http://learningwitherrors.org/atom.xml#footnote6" name="footnote6">6.</a>  The following form of Hoeffding bound is useful (it follows directly from Hoeffding for subgaussian variables, but is also a corollary of Azuma-Hoeffding): For iid zero-mean random variables $Z_i$, absolutely bounded by $1$, $\Pr[|\sum c_i Z_i| &gt; \epsilon] \leq 2exp(-\frac{\epsilon^2}{2 \sum_i c_i^2})$.  </span> (surprise), \[ \Pr[ | \sum_{i=1}^d a_i \frac{x_i}{\sqrt{d}}| &gt; \eta] \leq e^{-\Omega(\eta^2 d / ||x||_2^2)} \] For $\eta = \Omega(\sqrt{\frac{\log(d/\delta)}{d}})$, this probability is bounded by $(\frac{\delta}{d})$. Moreover, the same bound applies for all coordinates of $H_d Dx$, since all rows of $H_d$ have the form $\frac{1}{\sqrt{d}} \bmqty{\pm 1 &amp; \pm1 &amp; \dots &amp; \pm1}$. Thus, union bound over $d$ coordinates establishes the lemma. $$\tag*{$\blacksquare$}$$
</p><p>
</p><h3 class="tex">2.3. The Full Fast JL Transform</h3> <i>This presentation of FJLT is due to Jelani Nelson; see the notes [<a href="http://learningwitherrors.org/atom.xml#ref-Nel10">Nel10</a>].</i>
<p>
Putting all the pieces together, the FJLT is defined as: \[A = J S H_d D\] or, \[ A: \quad \R^d \overset{D}{\longrightarrow} \R^d \overset{H_d}{\longrightarrow} \R^d \overset{S}{\longrightarrow} \R^t \overset{J}{\longrightarrow} \R^k \] where

</p><ul> <li> $S$: the sparse coordinate-sampling matrix of Lemma <a href="http://learningwitherrors.org/atom.xml#lemS">3</a> </li><li> $H_d$: the $d \x d$ Hadamard transform. </li><li> $D$: diagonal iid $\pm 1$. </li><li> $J$: a dense “normal” JL matrix (iid Gaussian entries).
</li></ul>

 For parameters

<ul> <li> $t = \Theta(\epsilon^{-2}\log(1/\delta)\log(d / \delta))$ </li><li> $k = \Theta(\epsilon^{-2}\log(1/\delta))$
</li></ul>


<p>
That is, we first precondition with the randomized Hadamard transform, then sample random coordinates (which does most of the dimensionality reduction), then finally apply a normal JL transform to get rid of the last $\log(d/\delta)$ factor in the dimension.
</p><p>
<b>Correctness.</b> Since the matrix $D$ and the Hadamard transform are isometric, they do not affect the norms of vectors. Then, after the preconditioning, Lemma <a href="http://learningwitherrors.org/atom.xml#lemS">3</a> guarantees that $S$ only affects norms by $(1 \pm \epsilon)$, and Lemma <a href="http://learningwitherrors.org/atom.xml#lemjl">2</a> guarantees that the final step is also roughly isometric. These steps fail w.p. $\delta$, so the final transform affects norms by at most say $(1\pm 3\epsilon)$ except w.p. $3\delta$. This is sufficient to establish the JL embedding.
</p><p>
<b>Runtime.</b> For computing a JL embedding (ie, setting $\delta = 1/n^3, \epsilon=O(1)$), the time to embed a single vector is $O(d \log d + \log^3 n)$.
</p><p>
</p><h2 class="tex">3. Closing Remarks </h2>
<p>
<b>Optimality.</b> The target dimension given by the JL construction is known to be optimal. That is, one cannot embed $n$ points into dimension less than $k=\Omega(\epsilon^{-2}\log n)$ with distortion $\epsilon$. The first near-optimal lower-bound, in [<a href="http://learningwitherrors.org/atom.xml#ref-Alo03">Alo03</a>, Section 9] works by showing upper-bounds on the number of nearly-orthogonal vectors in a given dimension (so a too-good embedding of orthogonal vectors would violate this bound). A more recent, optimal bound, is in [<a href="http://learningwitherrors.org/atom.xml#ref-KMN11">KMN11</a>, Section 6]. They actually show optimality of the JL Lemma (that is, restricting to linear embeddings), which works (roughly) by arguing that if the target dimension is too small, then the kernel is too big, so a random vector is likely to be very distorted.
</p><p>
<b>Recent Advances.</b> Note that the FJLT is fast, but is not <em>sparse</em>. We may hope that embedding a sparse vector $x$ will take time proportional to the sparsity of $x$. A major result in this area was the sparse JL construction of [<a href="http://learningwitherrors.org/atom.xml#ref-KN14">KN14</a>]; see also the notes [<a href="http://learningwitherrors.org/atom.xml#ref-Nel10">Nel10</a>]. There is also work in derandomized JL, see for example [<a href="http://learningwitherrors.org/atom.xml#ref-KMN11">KMN11</a>].
</p><p>
<i>I'll stop here, since I haven't read these works yet, but perhaps we will revisit this another time.<br />
 This post was derived from my talk at Berkeley theory retreat, on the theme of “theoretical guarantees for machine learning.” </i>
</p><p>
<br /></p><hr /><h3>References</h3>
<p>
<a name="ref-AC09">[AC09]</a> Nir Ailon and Bernard Chazelle.
 The fast johnson-lindenstrauss transform and approximate nearest
  neighbors.
 <em>SIAM Journal on Computing</em>, 39(1):302--322, 2009.
 URL:
  <a href="https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf">https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf</a>.
</p><p>

</p><p>
<a name="ref-Alo03">[Alo03]</a> Noga Alon.
 Problems and results in extremal combinatorics, part i.
 <em>Discrete Math</em>, 273:31--53, 2003.
 URL: <a href="http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf">http://www.tau.ac.il/~nogaa/PDFS/extremal1.pdf</a>.
</p><p>

</p><p>
<a name="ref-KMN11">[KMN11]</a> Daniel Kane, Raghu Meka, and Jelani Nelson.
 Almost optimal explicit johnson-lindenstrauss families.
 In <em>Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques</em>, pages 628--639. Springer, 2011.
 URL:
  <a href="http://people.seas.harvard.edu/~minilek/papers/derand_jl.pdf">http://people.seas.harvard.edu/~minilek/papers/derand_jl.pdf</a>.
</p><p>

</p><p>
<a name="ref-KN14">[KN14]</a> Daniel~M Kane and Jelani Nelson.
 Sparser johnson-lindenstrauss transforms.
 <em>Journal of the ACM (JACM)</em>, 61(1):4, 2014.
 URL: <a href="https://arxiv.org/pdf/1012.1577v6.pdf">https://arxiv.org/pdf/1012.1577v6.pdf</a>.
</p><p>

</p><p>
<a name="ref-Nel10">[Nel10]</a> Jelani Nelson.
 Johnson-lindenstrauss notes.
 Technical report, Technical report, MIT-CSAIL, 2010.
 URL: <a href="http://web.mit.edu/minilek/www/jl_notes.pdf">http://web.mit.edu/minilek/www/jl_notes.pdf</a>.
</p><p></p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Preetum Nakkiran <a href="http://learningwitherrors.org/2016/05/27/fast-johnson-lindenstrauss/"><span class="datestr">at May 27, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/workshops-2018">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/workshops-2018/">Workshops in 2018</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p>I got invited to talk at quite a few workshops this year and am close to reaching the limit of travel I can handle. Just want to help the organizers advertise these events (most of them on sublinear algorithms and complexity). Please, consider attending and help spread the word!</p>

<ul>
  <li><a href="https://warwick.ac.uk/fac/sci/dcs/research/focs/conf2017">Workshop on Algorithms for Data Summarization</a> at the University of Warwick, UK (March 19-22). Organized by Graham Cormode and Artur Czumaj.</li>
  <li>68th Midwest Theory Day(s) at TTI-Chicago (April 12-13). Organized by Madhur Tulsiani, Aravindan Vijayaraghavan and Anindya De among others.</li>
  <li>Workshop on Sublinear Algorithms (June 11-13) and 2nd Workshop on Local Algorithms (June 14-15) at MIT. WoLA is organized by Mohsen Ghaffari, Reut Levi, Moti Medina, Andrea Montanari, Elchanan
Mossel and Ronitt Rubinfeld.</li>
  <li><a href="https://simons.berkeley.edu/complexity2018-2">Workshop on Interactive Complexity</a> at the Simons Institute, Berkeley (October 15-19). Organized by Kasper Green Larsen, Mark Braverman and Michael Saks.</li>
</ul>

<p>Hope to see some of you there!</p>


  <p><a href="http://grigory.github.io/blog/workshops-2018/">Workshops in 2018</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on February 23, 2018.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/workshops-2018/"><span class="datestr">at February 23, 2018 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017/">What's New in the Big Data Theory 2017</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><div align="center"><img src="http://grigory.github.io/blog/pics/o2017.png" alt="Happy 2018!" /> </div>

<p><br /></p>

<p>This year I will continue the <a href="http://grigory.us/blog/whats-new-in-big-data-theory-2016/">tradition started last year</a> and summarize a few papers on efficient algorithms for big data that caught my attention last year.
Same disclaimers as last year apply and this is by no means supposed to be the list of “best” papers in the field which is quite loosely defined anyway (e.g. I will intentionally avoid deep learning and gradient descent methods here as I am not actively working in these areas myself and there are a lot of resources on these topics already).
In particular, this year it was even harder to pick clear favorites so it is even more likely that I have missed some excellent work. 
Below I will assume familiary with the basics of streaming algorithms and the massively parallel computation model (MPC) discussed in an <a href="http://grigory.us/blog/mapreduce-model/">earlier post</a>.</p>

<p>Before we begin let me quickly plug some of my own work from last year.
With my student Adithya Vadapalli we have a new paper ``<a href="https://arxiv.org/pdf/1710.01431.pdf">Massively Parallel Algorithms and Hardness of Single-Linkage Clustering under -distances</a>’’. As it turns out, while single-linkage clustering and minimum spanning tree problems are the same for exact computation, for vector data round complexity of approximating these two problems in the MPC model is quite different.
In <a href="http://grigory.us/files/approx-linsketch.pdf">another paper</a> I introduce a study of approximate binary linear sketching of valuation functions.
This is an extension of our <a href="https://eccc.weizmann.ac.il/report/2016/174/">recent study</a> of binary linear sketching to the case when the function of interest should only be computed approximately.</p>


<h2>New Massively Parallel Algorithms for Matchings</h2>

<p>Search for new algorithms for matchings has lead to development of new algorithmic ideas for many decades (motivating the study of the class P of polynomial-time algorithms) and this year is no exception.
Two related papers on matchings caught my attention this year:</p>
<ul>
  <li>“<a href="https://arxiv.org/abs/1707.03478">Round Compression for Parallel Matching Algorithms</a>” by Czumaj, Lacki, Madry, Mitrovic, Onak and Sankowski.</li>
  <li>“<a href="https://arxiv.org/abs/1711.03076">Coresets Meet EDCS: Algorithms for Matching and Vertex Cover on Massive Graphs</a>” by Assadi, Bateni, Bernstein, Mirrokni and Stein.</li>
</ul>

<p>Both papers are highly technical but achieve similar results.
The first paper gives an -round MPC algorithm for the maximum matching problem that uses  memory per machine. The second paper improves the number of rounds down to  using slightly larger memory  per machine.
Using a standard reduction mentioned in the latter paper both papers can achieve multiplicative -approximation for any constant .
These results should be contrasted with the <a href="http://theory.stanford.edu/~sergei/papers/spaa11-matchings.pdf">previous work</a> by Lattanzi, Moseley, Suri and Vassilvitskii who give -round algorithms at the expense of using  memory per machine for any constant .
Overall, this is remarkable progress but likely not the end of the story.</p>

<h2>Massively Parallel Methods for Dynamic Programming</h2>
<p>Dynamic programming, <a href="https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf">pioneered by Bellman at RAND</a>, is one of the key techniques in algorithm design. Some would even go as far as saying that there are only two algorithmic tecniques and dynamic programming is one of them.
However, dynamic programming programming is notoriously sequential and difficult to use for sublinear time/space computation.
Most successful stories of speeding up dynamic programming so far have been problem-specific and often highly non-trivial.</p>

<p>In their paper “<a href="http://www.andrew.cmu.edu/user/moseleyb/papers/stoc17-main279.pdf">Efficient Massively Parallel Methods for Dynamic Programming</a>” (STOC’17) Im, Moseley and Sun suggest a fairly generic approach for designing massively parallel dynamic programming algorithms.
Three textbook dynamic programming problems can be handled within their framework:</p>

<ul>
  <li>Longest Increasing Subsequence: multiplicative -approximation in  rounds of MPC.</li>
  <li>Optimal Binary Search Tree: multiplicative -approximation in  rounds of MPC.</li>
  <li>Weighted Interval Scheduling: multiplicative -approxiamtion in  rounds of MPC.</li>
</ul>

<p>On a technical level this paper identifies two key properties that these problems have in common: monotonicity and decmoposability. Montonicity just requires that the answer to a subproblem should always be at most (for maximization)/at least(for minimization) the answer to the problem itself.
Decomposability is more subtle and requires that the problem can be decomposed into a two-level recursive family of subproblems where entries of the top level are called groups and entries of the bottom level are called blocks. 
It should then be possible to 1) construct a nearly optimal solution for the entire problem by concatenating solutions for subproblems, 2) construct a nearly optimal solution for each group from only a constant number of blocks.
While monotonicity holds for many standard dynamic problems, decomposability seems much more restrictive so it is interesting to see whether this technique can be extended to some other problems.</p>

<p>See Ben Moseley’s <a href="http://caml.indiana.edu/slides/ben.pdf">presentation</a> at the Midwest Theory Day for more details.</p>

<h2>Randomized Composable Coresets for Matching and Vertex Cover</h2>
<p>The simplest massively parallel algorithm one can think of can be described as follows: partition the data between  machines, let each machine select a small subset of the data points, collect these locally selected data points on one central machine and compute the solution there. 
The hardest part here is the design of the local subset selection procedures.
Such subsets are called coresets and have received a lot of attention the study of algorihtms for high-dimensional vectors, see e.g. <a href="http://sarielhp.org/p/04/survey/survey.pdf">this survey</a> by Agarwal, Har-Peled and Varadarajan.</p>

<p>Note that in the distributed setting construction of coresets can be affected by the initial distribution of data.
In fact, for the maximum mathcings problem non-trivially small coresets can’t be used to approximate the maximum matching up to a reasonable error (see <a href="http://grigory.us/files/soda16.pdf">our paper</a> with Assadi, Khanna and Li for the exact statement which in fact rules out not just coresets but any small-space representations) if no assumptions about the distribution of the data is made.</p>

<p>However, if the initial distribution of data is uniformly random then the situation changes quite dramatically.
As shown in “<a href="http://www.seas.upenn.edu/~sassadi/stuff/papers/randomized-coreset_matching-vc.pdf">Randomized Composable Coresets for Matching and Vertex Cover</a>” by Assadi and Khanna (best paper at SPAA’17) for uniformly distributed data coresets of size  can be computed locally and then combined to obtain -approximation.</p>

<h2>Optimal Lower Bounds for L<sub>p</sub>-Samplers, etc </h2>
<p>Consider the following problem: a vector  (initially consisting of all zeros) is changed by a very long sequence of updates that can flip an arbitrary coordinate of this vector. After seeing this sequence of updates can we retrieve some non-zero entry of this vector without storing all  bits used to represent ?
Surprisingly, the answer is “yes” and this can be done with only  space.
If we are required to generate a uniformly random non-zero entry of  then the corresponding problem is called -sampling.</p>

<p>-sampling turns out to be a remarkably useful primitive in the design of small-space algorithms. Almost all known streaming algorithms for dynamically changing graphs are based on -sampling or its relaxation where the uniformity requirement is removed.</p>

<p>While almost optimal upper and lower bounds on the amount of space necessary for -sampling have been known since <a href="https://arxiv.org/pdf/1012.4889.pdf">the work</a> of Jowhari, Saglam and Tardos, there were still gaps in terms of the dependence on success probability.
If our recovery of a non-zero entry of  has to be successful with probability  then the tight bound on space turns out to be .
This is one of the results of the <a href="http://people.seas.harvard.edu/~minilek/publications/papers/sampler_lb_merged.pdf">recent FOCS’17 paper</a> by Kapralov, Nelson, Pahocki, Wang, Woodruff and Yahyazadeh.</p>

<h2>Looking forward to more results in 2018!</h2>
<p>Please, let me know if there are any other interesting papers that I missed.
Also here is a quick shout out goes to some other papers that were close to making the above list:</p>

<ul>
  <li>“<a href="https://nips.cc/Conferences/2017/Schedule?showEvent=9453">Affinity Clustering: Hierarchical Clustering at Scale</a>” by Bateni, Behnezhad, Derakhshan, Hajiaghayi, Kiveris, Lattanzi and Mirrokni (NIPS’17).</li>
  <li>“<a href="https://www.ilyaraz.org/static/papers/lshforest.pdf">LSH Forest: Practical Algorithms Made Theoretical</a>” by Andoni, Razenshteyn and Shekel Nosatzki (SODA’17).</li>
  <li>“<a href="https://arxiv.org/abs/1610.08096">Almost Optimal Streaming Algorithms for Coverage Problems</a>” by Bateni, Esfandiari and Mirrokni (SPAA’17).</li>
  <li>“<a href="http://www.cs.utexas.edu/~ecprice/papers/compressed-generative.pdf">Compressed Sensing using Generative Models</a> by Bora, Jalal, Price and Dimakis (ICML’17).</li>
  <li>“<a href="https://arxiv.org/pdf/1707.08484.pdf">MST in O(1) Rounds of Congested Clique</a>” by Jurdzinski and Nowicki (SODA’18).</li>
</ul>

  <p><a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017/">What's New in the Big Data Theory 2017</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on January 27, 2018.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2017/"><span class="datestr">at January 27, 2018 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016/">What's New in the Big Data Theory 2016</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><div align="center"><img src="http://grigory.github.io/blog/pics/o2016.png" alt="Happy 2017!" /> </div>

<p><br /></p>

<p>This post will give an overview of papers on theory of algorithms for big data that caught my attention in 2016.
The basic rule that I used when making the list was whether I can see these results being included into some of the advanced graduate classes on algorithms in the future.
Also, while I obviously can’t include my own results here, among my own 2016 papers my two personal favorites are <a href="http://grigory.us/files/soda16.pdf">tight bounds on space complexity of computing approximate matchings in dynamic streams</a> (with S. Assadi, S. Khanna and Y. Li) and the <a href="http://eccc.hpi-web.de/report/2016/174/">-sketching paper</a> (with S. Kannan and E. Mossel and some special credit to Swagato Sanyal who subsequently improved the dependence on error in one of our main theorems).</p>

<p>It’s been a great year with several open problems resolved, old algorithms improved and new lines of research started.
All papers discussed below are presented in no particular order and their selection is clearly somewhat biased towards my own research interests.</p>

<h2>Maximum Weighted Matching in Semi-Streaming</h2>
<p>Sweeping both the best paper and the best student paper awards at the upcoming 28th ACM Symposium on Discrete Algorithms is a paper on semi-streaming algorithms for maximum weighted matching by graduate students Ami Paz and Gregory Schwartzman.
In semi-streaming we are given one pass over edges of an -vertex and only  bits of space.
It is easy to get a 2-approximation to the maximum matching by just maintaining the maxim<strong>al</strong> matching of the graph.
However, for weighted graphs maximal matching no longer guarantees a 2-approximation.</p>

<p>A long line of work has previously given constant factor approximations for this problem and finally we have a -approixmation. 
It is achieved via a careful implementation of the primal-dual algorithm for matchings in the semi-streaming setting.
It may seem somewhat surprising that primal-dual hasn’t been applied to this problem before since in the area of approximation algorithms it is a pretty standard way of reducing weighted problems to their unweighted versions, but the exact details of how to implement primal-dual in the streaming setting are quite delicate. I couldn’t find a version of this paper online so the best bet might be to wait for the SODA proceedings.</p>

<p>Now the big open question is whether one can beat the 2-approximation which is open even in the unweighted case.</p>

<h2>Shuffles and Circuits</h2>
<p>Best paper award at the 28th ACM Symposium on Parallelism in Algorithms and Architectures went to ‘‘<a href="http://theory.stanford.edu/~sergei/papers/spaa16-mrshuffle.pdf">Shuffles and Circuits</a>’’, a paper by Roughgarden, Vassilvitskii and Wang.
This paper emphasizes the difference between rounds of MapReduce and depth of a circuit.
Because some of the machines can choose to stay silent between the rounds a round of MapReduce can be more complex than a layer of a circuit as the machines sending input to the next round might depend on the original input data. 
The paper shows that nevertheless the standard circuit complexity ‘‘degree bound’’ can be applied to MapReduce computation.
I.e. any Boolean function whose polynomial representation has degree  requires  rounds of MapReduce using machines with space .
This implies an  lower bound on the number of rounds for computing connectivity of a graph.
The authors also make explicit a connection between the MapReduce model and  (see definition <a href="https://en.wikipedia.org/wiki/NC_(complexity)">here</a>) which implies that improving lower bounds beyond   for polynomially many machines would imply separating  from .</p>

<h2>Beating Counting Sketches for Insertion-Only Streams</h2>
<p>Both <a href="http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf">CountSketch</a> and <a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch">Count-Min Sketch</a>, which are textbook approximate data structures for storing very large dynamically changing numerical tables in small space, have been improved this year under the assumption that data in the table is only incremented.
These improvements are for the most common application of such sketches to ``heavy hitters’’– the task of recovering largest entries from the table approximately. 
For CountSketch see <a href="http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/bciw16.pdf">the paper</a> by Braverman, Chestnut, Ivkin, Woodruff from STOC’16 and for CountMin Sketch <a href="https://arxiv.org/abs/1603.00213">the paper</a> by Bhattacharyya, Dey and Woodruff from PODS’16.</p>

<h2>Optimality of the Johnson-Lindenstrauss Transform</h2>
<p>Two papers by <a href="https://arxiv.org/pdf/1609.02094v1.pdf">Green Larsen and Nelson</a> and by <a href="http://www.cs.tau.ac.il/~nogaa/PDFS/compression3.pdf">Alon and Klartag</a> have resolved the question of proving optimality of the Johnson-Lindenstrauss transform.
Based on doing a projection on random low-dimensional subspace JL-transform is the main theoretical tool for dimensionality reduction of high-dimensional vectors.
As these papers show no low-dimensional embedding and furthermore no data structure can achieve better bit complexity than  for -approximating all pairwise distances between  vectors in Euclidean space (for a certain regime of parameters).
This matches the Johnson and Lindenstrauss upper bound and improves an old lower bound of  due to Alon.
Even though Alon’s argument is significantly simpler getting an optimal lower bound is a very nice achievement.</p>

<h2>Fast Algorithm for Edit Distance if It's Small</h2>

<p><a href="https://en.wikipedia.org/wiki/Edit_distance">Edit distance</a> is one of the cornerstone metrics of text similairity in computer science. It can be computed in quadratic time using standard dynamic programming which is optimal assuming SETH due to the <a href="https://arxiv.org/abs/1412.0348">result of Backurs and Indyk</a>.
Edit distance also has a number of applications including comparing DNAs in computational biology.
In these applications it is usually reasonable to assume that edit distance is only interesting if it is not too large.
Unfortunately, this doesn’t help speed up the standard dynamic program.
A series of papers, including two papers from this year by <a href="http://iuuk.mff.cuni.cz/~koucky/papers/editDistance.pdf">Chakraborty, Goldenberg and Koucky</a> (STOC’16) and 
<a href="http://homes.soic.indiana.edu/qzhangcs/papers/focs16-ED.pdf">Belazzogui and Zhang</a> lead to the following result: sketches of size  bits suffice for computing edit distance . Such sketches can be applied not just in centralized but also in distributed and streaming settings making it possible to compress input strings down to size that (up to logarithmic factors) only depends on .</p>

<h2>Tight Bounds for Set Cover in Streaming</h2>
<p>Set Cover is a surprisingly powerful abstraction for a lot of applications that involve providing coverage for some set of terminals. 
Given a collection of sets  the goal is to find the smallest cardinality subcollection of these sets such that their union is , i.e. all of the underlying elements are covered.
In approximation algorithms a celebrated greedy algorithm gives an -approximation for this problem. 
In streaming there has been a lot of interest lately in approximating classic combinatorial optimization problems in small space with Set Cover being one of the main examples.
For an overview from last year check Piotr Indyk’s <a href="https://www.youtube.com/embed/_4mM1UGI9Dg?list=PLqxsGMRlY6u659-OgCvs3xTLYZztJpEcW">talk</a> from the <a href="http://grigory.us/mpc-workshop-dimacs.html">DIMACS Workshop on Big Data and Sublinear Algorithms</a>.</p>

<p>As <a href="http://www.seas.upenn.edu/~sassadi/stuff/papers/tbfsscotscp-conf.pdf">this STOC’16 paper</a> by Assadi, Khanna and Li shows savings in space for streaming Set Cover can only be proportional to the loss in approximation.  In particular, if we are interested in computing Set Cover which is within a multiplicative factor  of the optimum then:
1) for computing the cover itself space  is necessary and sufficient,
2) for just esimating the size space  is necessary and sufficient.</p>

<h2>Polynomial Lower Bound for Monotonicity Testing</h2>
<p>Finally a polynomial lower bound has been shown for adaptive algorithms for testing monotonicity of Boolean functions .
The lower bound implies that any algorithm that can tell whether  is monotone or differs from monotone on a constant fraction of inputs has to query at least  values of . 
This result is due to <a href="https://arxiv.org/abs/1511.05053">Belovs and Blais</a> (STOC’16) and is in contrast with the upper bound of  by Khot, Minzer and Safra from last year’s FOCS.
Probably the biggest result in property testing this year.</p>

<h2>Linear Hashing is Awesome</h2>
<p>While ‘‘<a href="http://ieee-focs.org/FOCS-2016-Papers/3933a345.pdf">Linear Hashing is Awesome</a>’’ by Mathias Bæk Tejs Knudsen doesn’t fall into the traditional ‘‘sublinear algorithms for big data’’ category this paper still has some sublinear flavor because of its focus on very fast query times.
Linear hashing is a classic hashing scheme 
 
where  are random. It is very often used in practice and discussed extensively in CLRS.
This paper proves that linear hashing <strike>is awesome</strike> results in expected length of the longest chain of only  compared to the previous simple bound of .</p>

<p>Finally, this paper also decisively wins my ‘‘Best Paper Title 2016’’ award.</p>

<h2>Looking forward to more cool results in 2017!</h2>
<p>There has been a lot of great results in 2016 and it’s hard to mention all of them in one post and I certainly might have missed some exciting papers. Here is a quick shout out to some other papers that were close to making the above list:</p>
<ul>
<li><a href="https://arxiv.org/abs/1507.04299">Tight Bounds for Data-Dependent LSH</a> by Andoni and Razenshteyn from SoCG'16.</li>
<li><a href="http://arxiv.org/abs/1603.05346">Optimal Quantile Estimation in Streams</a> by Karnin, Lang and Liberty from FOCS'16.
</li>
</ul>

<p>Happy 2017!</p>


  <p><a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016/">What's New in the Big Data Theory 2016</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on December 30, 2016.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/whats-new-in-big-data-theory-2016/"><span class="datestr">at December 30, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/video-recording-class-screencast">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/video-recording-class-screencast/">Video Recording Screencasts of Talks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p>Here is the setup I came up with for video recording live lectures and talks.</p>

<p>Hardware:</p>
<ul>
<li> Laptop with a camera: MacBook</li>
<li> Tablet with a pen/stylus: iPad (Pro), MSRP $1K</li>
<li> Microphone: Sennheiser ME 3-EW, MSRP $200</li>
<li> Projector (connected to the laptop)</li>
</ul>

<p>Software:</p>
<ul>
<li> Open Broadcasting Software (laptop), free</li>
<li> Microsoft Powerpoint and Office 365 (tablet), free from IU</li>
<li> AirServer (laptop) and AirServer Connect (tablet), $10</li>
</ul>

<p>Full description and demo on Youtube:
<br /></p>



  <p><a href="http://grigory.github.io/blog/video-recording-class-screencast/">Video Recording Screencasts of Talks</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on August 01, 2017.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/video-recording-class-screencast/"><span class="datestr">at August 01, 2017 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/video-recording-class-question">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/video-recording-class-question/">Advice on video recording lectures?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p>Looking for advice: I am thinking of making videos of my class this Fall, the setup I am thinking about is a tablet (just ordered a large iPad Pro) + mic and camera + some software (maybe Google Hangouts live?) to record a screencast of the tablet with my video on the side. Did anyone have experience doing something like this? What hardware and software did you use? If this sounds too clunky to set up on your own, is it worth creating a MOOC and/or having professionals do the recording for you?</p>


  <p><a href="http://grigory.github.io/blog/video-recording-class-question/">Advice on video recording lectures?</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on July 31, 2017.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/video-recording-class-question/"><span class="datestr">at July 31, 2017 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2018">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2018/">Theory Jobs 2018</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p><img src="http://grigory.github.io/blog/pics/theory-jobs-2018.png" />
<a href="https://docs.google.com/spreadsheets/d/1P5okKjeNlvkEEFMzX3l8VcL4SHpWBKNFET-5mPTWfDQ/edit#gid=0">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. Previously my academic uncle Lance Fortnow set it up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from last year which also has links to all the previous years), but this year he has kindly agreed to try and pass the baton. Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized.</p>

<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors.  <b>New:</b> As suggested by <a href="http://onak.pl">Krzysztof Onak</a> a new tab for sabbaticals was added.</li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>



  <p><a href="http://grigory.github.io/blog/theory-jobs-2018/">Theory Jobs 2018</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 25, 2018.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2018/"><span class="datestr">at May 25, 2018 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2017">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2017/">Theory Jobs 2017</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p><b>UPD</b>: Lance created his Theory Jobs spreadsheet, I’ve moved all the information there and changed the link below to Lance’s spreadsheet.</p>

<p><a href="https://docs.google.com/spreadsheets/d/1xBpgBZXcSxjEAbU7SYCeXOJOJtPeXYCOFArqL28Uho8/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created by Lance Fortnow that collects information about theory jobs this year. <strike>Previously Lance set it up, but this year it is getting late in the year so I decided to go ahead and create one myself. In previous years the jobs post was up a few weeks back so I hope I am not jumping the gun here. Rules about the spreadsheet have been copied from <a href="http://blog.computationalcomplexity.org/2016/05/theory-jobs-2016.html">Lance's last year post</a> and all edits to the document are anonymized.  
</strike></p>

<ul>
 <li>Separate sheets for faculty, industry and postdoc/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
 </ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2017/">Theory Jobs 2017</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 08, 2017.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2017/"><span class="datestr">at June 08, 2017 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data/">The Simple Economics of Algorithms for Big Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p>
In this blog post I want to suggest a simple reason why you should study your algorithms <b>really</b> well if you want to design algorithms that deal with big data.
This reason comes from <b>the way billings offered by cloud services work</b>.
</p>
<p>
Maybe you remember yourself taking that algorithms class and thinking: “Who really cares if that algorithm uses a bit more time? Can't we just wait a little longer?”.
Or “Ok, we can save some space here, but if it all fits into my RAM anyway then why bother?”.
These are both great reasons not to care too much about efficiency of your algorithms if your data is small, fits into RAM and the running times aren't significant enough to matter anyway.
So you would go on to program your favorite video game and not care about that professor talking about all that big-Oh nonsense.
And in the short run you would be right. While you are developing a prototype of your favorite video game you shouldn't care.
When I was working at a startup I remember myself learning the hard way that <a href="http://c2.com/cgi/wiki?PrematureOptimization">premature optimization is the root of all evil</a>.

</p>
<div align="center"><img src="http://grigory.github.io/blog/pics/abstruse-goose-video-games.png" alt="abstruse-goose-video-games" /> </div>

<p><br /></p>
<p>
However, once your video game becomes successful and you get to deal with big data that has to be stored and processed in the cloud this reasoning starts to fall short.
Let's say you developed <a href="https://en.wikipedia.org/wiki/Candy_Crush_Saga">Candy Crush Saga</a> (<a href="http://www.standard.co.uk/business/business-news/candy-crush-saga-owner-king-digital-entertainment-valued-at-7bn-9216058.html">valued at $7bn in 2014</a>) and now you are interested in doing some data analytics about your &gt;10 million active users.
You are now considering outsourcing your data storage and computation to the cloud.
Here is where you might want to learn why the design of space and time-efficient algorithms matters for the bottom line of your future business. 

</p><h1>100x more efficient algorithms = 100x less money in billings</h1>

So that time and space your professor was talking about – what does it have to do with your spending on the cloud services?
The answer is surprisingly simple – <b>if you need 100x more time and space then your billing increases 100 times</b>.
Below I used the pricing calculator that comes with Google Compute Engine to see how the cost scales if I want to use 100/1000/10000 identical machines for a year.
<div align="center"><img src="http://grigory.github.io/blog/pics/cloud-pricings.png" alt="abstruse-goose-video-games" /> </div>
<br />
<p>
I was myself surprised to find this out since I expected some economy of scale to kick in. In fact, sometimes it does but usually is quite negligible. Say, you can get an X% discount but that doesn't help much against linear scaling.
</p>





<p></p>

  <p><a href="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data/">The Simple Economics of Algorithms for Big Data</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on January 20, 2016.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/the-simple-economics-of-algorithms-for-big-data/"><span class="datestr">at January 20, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/the-binary-sketchman">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/the-binary-sketchman/">The Binary Sketchman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><p>In this post I will talk about some of my recent work with <a href="http://www.cis.upenn.edu/~kannan/">Sampath Kannan</a> and <a href="https://stat.mit.edu/people/elchanan-mossel/">Elchanan Mossel</a> on linear methods for binary data compression. The paper is <a href="http://eccc.hpi-web.de/report/2016/174/">available here</a>, slides from my talk at Penn are <a href="http://grigory.us/files/talks/penn16.pdf">here</a> and another talk at Columbia is <a href="http://www.cs.columbia.edu/theory/f16-theoryread.html#Grigory">coming up on Nov 21</a>.</p>

<p>Given very large data represented in binary format as a string of length , i.e.  
we are interested in a compression algorithm that can transform  into a much shorter binary string .
Here  so that we can achieve some non-trivial savings in space.
Moreover, if  changes in the future we would like to be able to update our compressed version of it (without having to store the original ).</p>

<p>Clearly compression introduces some loss making it impossible to recover certain properties of the original data from the compressed string.
However, if we know in advance which property of  we are interested in then efficient compression often becomes possible.
We will model the property of interest as a binary function  which labels all possible ’s with two labels.
So our goal will be to be able to: 1) perform this binary classification, i.e. compute  using compressed data  only, 2) do this even if  changes over time – updates for us will be bit flips in the coordinates of  specified by the index of the bit that is getting flipped.</p>

<p>Finally, if  is so big that it can’t be stored locally and has to be divided into chunks stored across multiple machines then we will be able to compress the chunks locally and then combine them on a central server into a compressed version of the entire data – one simple round of MapReduce or whatever your favorite distributed framework is.</p>

<p>To make the above discussion less abstract let’s consider a machine learning application – evaluating a linear classifier over binary data.
Let’s say we have trained a linear classifier of the form  where sign is the sign function. 
Is it possible to compress  in such a way that we can still evaluate our classifier in the scenarios described above?
Turns out we can compress the input down to  bits where  is a parameter of the linear classifier known as its margin. Moreover, no compression scheme can do better.</p>

<h1 id="introducing-the-binary-sketchman">Introducing the Binary Sketchman</h1>

<div align="center"><img src="http://grigory.github.io/blog/pics/binary-sketchman-final.png" alt="The Binary Sketchman" /> </div>

<p><br /></p>

<p>While the setting described above may seem quite challenging it can be handled through a framework of linear sketching.
In the binary case the interpretation of linear sketching is particularly simple as our binary sketchman is just going to compute  parities of the bits of , say for :</p>



<p>In a matrix form this corresponds to computing  where  is a  binary matrix and the operations are performed over .
Note that now our sketch easily satisfies all the requirement above since as  changes we can just update the corresponding parities. In the distributed case we can compute them locally and then add up on a central server.</p>

<p>Unfortunately the power of a deterministic sketchman who just uses a fixed set of parities is quite limited and no such sketchman can compress even a simple linear classifier down to less than  bits.
In fact, even for the OR function  no deterministic sketch can have less than  bits.
So our binary sketchman will “<a href="http://www.cs.cmu.edu/~haeupler/15859F14/">unleash the power of randomization</a>” in his quest for a perfect sketch.
According to <a href="http://www.cs.cmu.edu/~haeupler/">Bernhard Haeupler</a> this can be quite dramatic and looks kind of like this:</p>
<div align="center"><img src="http://www.cs.cmu.edu/~haeupler/15859F14/images/posternoinf.jpg" alt="The power of randomness unleashed" width="300px" /> </div>

<p><br />
So our sketchman will instead pick the matrix  randomly while the rest is the same as before.
Now the OR function is easy to handle: pick a parity over a random subset of  where each coordinate is included with probability .
If  then this parity catches a non-zero coordinate of  with probability  and thus evaluates to  with probability at least .
If  then the parity never evaluates to  so we can distinguish the two cases with probability  using  such parities.
This illustrates a more general idea – if  is a constant function on all but  different inputs then a sketch of size  suffices.</p>

<p>Now for linear thresholds the high-level ideas behind this sketching process are as follows:
1) observe that any linear threshold function takes the same value on all but  inputs,
2) apply the same argument as above to obtain a sketch of size .
The only thing missing in the above argument is that we still have dependence on .
This can be avoided if we first hash the domain reducing its size down to  which replaces  in the above calculations giving us .
While this compression method is quite simple the remarkable fact is that it can’t be improved.
Even for the simplest threshold function that corresponds to a threshold for the Hamming weight of , i.e. , any compression mechanism would require  bits as follows from <a href="http://link.springer.com/chapter/10.1007/978-3-642-32512-0_44">this work</a> by Dasgupta, Kumar and Sivakumar.
Note that it isn’t assumed that the protocol is based on linear sketching – it can be an arbitrary scheme.</p>

<h1 id="the-power-of-randomized-binary-sketchman">The Power of Randomized Binary Sketchman</h1>

<p>Linear sketching by itself is not a new idea and has been studied extensively in the last two decades.
See surveys by <a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-dpwoodru">Woodruff</a> and <a href="http://people.cs.umass.edu/~mcgregor/">McGregor</a> on how it can be applied to problems in <a href="http://researcher.ibm.com/files/us-dpwoodru/wNow3.pdf">numerical linear algebra</a> and <a href="http://link.springer.com/referenceworkentry/10.1007/978-3-642-27848-8_796-1">graph compression</a>.
However, this work focuses on linear sketching over large finite fields (used to represent real values with bounded precision).
Nevertheless some striking results are known about linear sketching that are applicable in our context as well.
In particular, if  is updated through a very long (triply exponential in ) stream of adversarial updates then linear sketches over finite fields are optimal for any function  as shown by Li, Nguyen and Woodruff <a href="https://pdfs.semanticscholar.org/bf89/98d76741f3ee7b4ba1f82524353e7083c3b5.pdf">here</a> in STOC’14.</p>

<p>As our paper shows the same result holds for much shorter random streams of length  in a simple model where each update flips uniformly at random chosen coordinate of .
In other words binary sketching is optimal if in the end of the stream the input  is uniformly distributed.
The proof of this fact is quite technical and relies on a notion of <i>approximate Fourier dimension</i> for Boolean functions that we use to characterize binary sketching under the uniform distribution – check the paper for details if you are interested.
Whether the same result holds for short (length , say) adversarial streams is the main open question left open.</p>


  <p><a href="http://grigory.github.io/blog/the-binary-sketchman/">The Binary Sketchman</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on October 07, 2016.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/the-binary-sketchman/"><span class="datestr">at October 07, 2016 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/teaching-algorithms-for-big-data">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/teaching-algorithms-for-big-data/">Teaching algorithms for Big Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div>

<p>“algorithms for Big Data” (sometimes the name can slightly vary) is a new graduate class that has been introduced by many top computer science programs in the recent years.
In this post I would like to share my experience teaching this class at the University of Pennsylvania this semester. Here is the <a href="http://grigory.us/big-data-class.html">homepage</a>.</p>

<div align="center"><img src="http://grigory.github.io/blog/pics/class-logo-large.png" alt="Keep calm and crunch data on o(N)" /> </div>

<p><br /></p>

<p>First off, let me get the most frequently asked question out of the way and say that by “big data” in this class I mean data that doesn’t fit into a local RAM
since if the data fits into RAM then algorithms from the standard algorithms curricula will do the job. 
At the moment a terabyte of data is already tricky to fit into RAM so this is where we will draw the line. 
In particular, this is so that the <a href="http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html">arguments about beating algorithms for big data using your laptop</a> don’t apply.</p>

<p>Second, I tried to focus as much as possible on algorithms that are known to work in practice and have implementations.
Because this is a theory class we didn’t do programming but I made sure to give links to publicly available implementations whenever possible.
As it is always the case, the best algorithms to teach are never exactly the same as the best implementations.
Even the most vanilla problem of sorting an array in RAM is handled in C++ STL via a combination of QuickSort, InsertionSort and HeapSort.
Picking the right level of abstraction is always a delicate decision to make when teaching algorithms and I am pretty happy with the set of choices made in this offering.</p>

<p>Finally, “algorithms for Big Data” isn’t an entirely new phenomenon as a class since it builds on its predecessors
typically called “Sublinear Algorithms”, “Streaming Algorithms”, etc.
Here is a <a href="http://grigory.us/big-data-class.html#sketch">list of closely related classes offered at some other schools</a>.
In fact, my version of this class consisted of <a href="http://grigory.us/big-data-class.html#lectures">four modules</a>:</p>

<ul>
<li><b>Part 1: Streaming Algorithms.</b> It is very convenient to start with this topic since techniques developed in streaming turn out to be useful later. In fact, I could as well call this part “linear sketching” since every streaming algorithm that I taught in this part was a linear sketch. I find single-pass streaming algorithms to be the most motivated and for so-called dynamic streams that can contain both insertions and deletions linear sketches are known to be almost optimal under fairly mild conditions.
Moreover, linear sketches are the baseline solution in the more advanced massively parallel computational models studied later.
</li>
<li><b>Part 2: Selected Topics.</b> This part became very eclectic, containing selected topics in numerical linear algebra, convex optimization and compressed sensing.
In fact, some of the algorithms in this part aren't even “algorithms for Big Data” according to the RAM size based definition.
However, I considered these topics to be too important to skip in a “big data” class. 
For example, right after we covered gradient descent methods for convex optimization Google released <a href="https://www.tensorflow.org/">TensorFlow</a>.
This state of the art machine learning library allows one to choose any of its <a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#optimizers">5 available versions</a> of gradient descent for optimizing learned models. These days when you can run into some <a href="https://aws.amazon.com/machine-learning/pricing/">pretty steep pricing</a> for outsourcing your machine learning to the cloud knowing what is under the hood of free publicly available frameworks I think is increasingly important.  
</li>
<li><b>Part 3: Massively Parallel Computation.</b> I am clearly biased here, but this is my favorite. Unlike, say, streaming where many results are already tight, we are still quite far from understanding full computational power of MapReduce-like systems. Potential impact of such algorithms I think is also likely to be the highest. In this class because of the time constraints I only touched the tip of the iceberg. This part will be expanded in the future.</li>
<li><b>Part 4: Sublinear Time Algorithms.</b> I always liked clever sublinear time algorithms, but for many years believed that they are not quite “big data“ since they operate under the assumption of random access to the data. Well, this year I had to change my mind after Google launched its <a href="https://code.google.com/codejam/distributed_index.html">Distributed Code Jam</a>.
I have to admit that I have no idea how this works on the systems level but apparently it is possible to implement reasonably fast random access to large data.
The problems that I have seen being used for Distributed Code Jam allow one to use 100 nodes each having small RAM. The goal is to process a large dataset available via random access.
</li>
</ul>

<p>Overall parts 1 and 4 are by now fairly standard. Part 2 has some new content from <a href="http://researcher.watson.ibm.com/researcher/files/us-dpwoodru/journal.pdf">David Woodruff’s great new survey</a>. Some algorithms from it are also available in IBM’s <a href="https://github.com/xdata-skylark/libskylark">Skylark library for fast computational linear algebra and machine learning</a>.
Part 3 is what makes this class most different from most other similar classes.</p>

<h1>Mental Notes</h1>
<p>Here is a quick summary of things I was happy with in this offering + potential changes in the future.</p>
<ul>
<li><b>Research insights.</b> One of the main reasons why I love teaching is that it often leads to research insights, especially when it comes to simple connections I have been missing. For example, I didn't previously realize that one can use <a href="http://grigory.us/files/publications/BRY14-Lp-Testing.pdf">L<sub>p</sub>-testing</a> as a tool for testing assumptions about convexity and Lipschitzness used in the analysis of the convergence rate of gradient descent methods. </li>
<li><b>Project.</b> Overall I am very happy with the students' projects. 
Some students implemented algorithms, some wrote surveys and some started new research projects.
Most unexpected to me were the projects done by non-theory students connecting their areas of expertise with the topics discussed in the class. E.g. surveys of streaming techniques used in natural language processing and bionformatics were really fun to read.</li> 
<li><b>Cross-list the class for other departments.</b> It was a serious blunder on my behalf to not cross-list this class for other departments, especially Statistics and Applied Math.
Given how much interest there is from other fields this is probably the easiest to fix and the most impactful mistake.
Somehow some students from other departments learned about the class anyway and expressed their interest, often too late.</li>
<li><b>New content.</b> Because of time constraints I couldn't fit in some of the topics I really wanted to cover.
These include coresets (there has been a resurgence of interest in coresets for massively parallel computing, but I didn't have time to cover it), nearest neighbor search (somehow I couldn't find a good source to teach from, suggestions are very welcome), Hyperloglog algorithm (same reason), more algorithms for massively parallel computing (no time), more sublinear time algorithms (no time).    
In the next version of this class I will make sure to cover at least some of these. 
</li>
<li><b>Better structure.</b> Overall I am pretty happy with the structure of the class but there is definitely room for improvement. A priority will be to better incorporate selected topics discussed in Part 2 into the overall structure of the class. In particular, convex optimization came a little out of the blue even though I am really glad I included it.</li>
<li><b>Slides and equipment.</b> I really like teaching with slides that contain only some of the material and use the blackboard to fill in the missing details and pictures.
On one hand, slides are a backbone that the students can later use to catch up on the parts they missed.  On the other hand, the risk of rushing through the slides too fast is minimized since the details are discussed on the board. Also a lot of time is saved on drawing pictures. I initially used Microsoft Surface Pro 2 to fill in the gaps on the tablet instead of the board but later gave up on this idea because of technical difficulties. Having a larger tablet would help too. I still think that the tablet can work but requires a better setup. Next time I will try to use the tablet again and post the final slides online. 
</li>
<li><b>Assign homework and get a TA.</b> Michael Kearns and I managed to teach “Computational Learning Theory” without a TA last semester so I decided against getting one for my class as well. This was fine except that having a TA for grading homework would have helped a lot.</li>
<li><b>Make lecture notes and maybe videos.</b> With fairly detailed slides I didn't consider lecture notes necessary. Next time it would be nice to have some since some of my fellow facutly friends asked for them. I think I will stick with the tested “a single scribe per lecture“ approach although I heard in France students sometimes collaboratively work on the same file during the lecture and the result comes out nice. When I had to scribe lectures I just LaTeXed them on the fly so I don't see why you can't do this collaboratively. 
As for videos, Jelani had <a href="http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html">videos</a> from his class this time and they look pretty good. </li> 
<li><b>Consider MOOCing.</b> Given that the area is in high demand doing a MOOC in the future is definitely an option. It would be nice to stabilize the content first so that the startup cost of setting up a MOOC could be amortized by running it multiple times.</li>
</ul>

<h1>Thanks</h1>
<p>I am very grateful to my friends and colleagues discussions with whom helped me a lot while developing this class.
Thanks to Alex Andoni, Ken Clarkson, Sampath Kannan, Andew McGregor, Jelani Nelson, Eric Price, Sofya Raskhodnikova, Ronitt Rubinfeld and David Woodruff (this is an incomplete list, sorry if I forgot to mention you). Special thanks to all the students who took the class and <a href="http://www.seas.upenn.edu/~sassadi/">Sepehr Assadi</a> who gave a guest lecture on our <a href="http://arxiv.org/pdf/1505.01467.pdf">joint paper about linear sketches of approximate matchings</a>.</p>

  <p><a href="http://grigory.github.io/blog/teaching-algorithms-for-big-data/">Teaching algorithms for Big Data</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on December 24, 2015.</p></div><div class="commentbar"><p></p></div></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/teaching-algorithms-for-big-data/"><span class="datestr">at December 24, 2015 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
