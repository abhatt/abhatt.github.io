<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at February 19, 2021 07:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21277">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/">Nostalgia corner: John Riordan’s referee report of my first paper</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  “Combinatorial Identities” and “Combinatorial Analysis”.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan1.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan1.png?w=640&amp;h=834" class="alignnone size-full wp-image-21279" height="834" /></a></p>
<p>I was surely very happy to read the sentence <span style="color: #0000ff;">“I think you have had a splendid idea”</span>.  Here is part of Riordan’s remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep1.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan3.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan3.png?w=640&amp;h=827" class="alignnone size-full wp-image-21280" height="827" /></a></p>
<p>It took me some time to revise the paper and get it printed. And here is the report for the second version.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan2.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan2.png?w=640&amp;h=839" class="alignnone size-full wp-image-21281" height="839" /></a></p>
<p>And here is part of Riordan’s second round of remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep2.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan4.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan4.png?w=640&amp;h=843" class="alignnone size-full wp-image-21282" height="843" /></a></p>
<p>I was certainly happy to read the following sentence: <span style="color: #0000ff;">“I would remark that the result for  <em>p = -1 </em> is new and perhaps <span style="color: #ff0000;">the simplest derivation of Abel’s result</span>.”</span></p>
<p>In 1978 I actually visited John Riordan in his office at Rockefeller University, NYC. I remember him as very cheerful and he told me that when his first book appeared he was working at Bell Labs and his managers wanted to talk to him. He was a bit worried that they would not approve of him spending time and effort to write a book in pure mathematics. But actually, they gave him a salary raise!</p>
<p>(If you have a picture of John Riordan, please send me.)</p>
<p>In 1979 the paper <a href="https://gilkalai.files.wordpress.com/2021/02/1-s2.0-0097316579900475-main-1.pdf">appeared</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan5.png"><img src="https://gilkalai.files.wordpress.com/2021/02/riordan5.png?w=640" alt="" class="alignnone size-full wp-image-21286" /></a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/"><span class="datestr">at February 19, 2021 09:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18125">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/">Computing’s Role In The Pandemic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>How can we help?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/unknown-145/" rel="attachment wp-att-18132"><img src="https://rjlipton.files.wordpress.com/2021/02/unknown.jpeg?w=600" alt="" class="alignright size-full wp-image-18132" /></a></p>
<p>
Joe Biden is the 46th president of the USA. Note <img src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{46}" class="latex" title="{46}" /> is called a <a href="https://en.wikipedia.org/wiki/Centered_triangular_number">centered triangular number</a>. These numbers obey the formula: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B3n%5E2+%2B+3n+%2B+2%7D%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{3n^2 + 3n + 2}{2} " class="latex" title="\displaystyle  \frac{3n^2 + 3n + 2}{2} " /></p>
<p>and start with <img src="https://s0.wp.com/latex.php?latex=%7B1%2C4%2C10%2C19%2C31%2C46%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1,4,10,19,31,46,\dots}" class="latex" title="{1,4,10,19,31,46,\dots}" /> The previous one, the <img src="https://s0.wp.com/latex.php?latex=%7B31%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{31^{st}}" class="latex" title="{31^{st}}" />, was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic">pandemic</a> one of his top priorities. </p>
<p>
Today I thought we would discuss how he might use computer technology to help get the virus under control. </p>
<p>
First, we thank the drug companies since we now have <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Vaccines">vaccines</a> that work against the virus. Without these we would have little chance to bring the pandemic under control at all. </p>
<p>
Second, we must state that we are worried that the virus is mutating and this may render the current vaccines less useful, if not useless. We hope this is not happening, or that the drug companies will be able to respond with vaccine boosters. Today there seems to be <a href="https://www.usnews.com/news/health-news/articles/2021-02-18/pfizer-coronavirus-vaccine-protects-against-uk-south-africa-variants-study-shows">good news</a> and <a href="https://www.reuters.com/article/us-health-coronavirus-vaccines-variants/pfizer-says-south-african-variant-could-significantly-reduce-protective-antibodies-idUSKBN2AH2VG">bad news</a>.  </p>
<p>
Results will fluctuate, but in any case, vaccines will definitely play a key role in defeating the pandemic. We want to ask the same about computing technology.</p>
<p>
</p><p></p><h2> Computing’s Role—I </h2><p></p>
<p></p><p>
There are many web sites that discuss how computing technology can play a role in defeating the pandemic. Here are some of the main points:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Tracking People:</i> Many places are interested in tracking who are sick. Tracking can by itself help stop the spreading of the virus, and thus help save lives. For example, <a href="https://www.computer.org/publications/tech-news/five-ways-tech-is-being-used-to-fight-covid-19">IEEE</a> says: </p>
<blockquote><p><b> </b> <em> “We believe software can help combat this global pandemic, and that’s why we’re launching the Code Against COVID-19 initiative…,” said Weiting Liu, founder and CEO of Arc. “From tracking outbreaks and reducing the spread to scaling testing and supporting healthcare, teams around the world are using software to flatten the curve. The eMask app (real-time mask inventory in Taiwan) and TraceTogether (contact tracing in Singapore) are just two of the many examples.” </em>
</p></blockquote>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Changing Behavior:</i> A powerful idea is to avoid human to human contact and thus stop the spread of the virus. For example, here are <a href="https://www.weforum.org/agenda/2020/04/10-technology-trends-coronavirus-covid19-pandemic-robotics-telehealth">examples</a> from a longer list of ideas: </p>
<ul>
<li>
Robot Deliveries; <p></p>
</li><li>
Digital and Contactless Payments; <p></p>
</li><li>
Remote Work and Remote Learning and more.
</li></ul>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Changing Health Delivery:</i> An important idea is how can we reduce the risk of health delivery. A paradox is that health care may need to be avoided, since traditional delivery requires human contact. There are many examples of ways to make health care online, and therefore safer. Shwetak Patel won the 2018 ACM Prize in Computing for contributions to creative and practical sensing systems for sustainability and health. He outlined here <a href="https://cccblog.org/2020/09/24/what-role-can-computing-play-in-battling-the-covid-19-pandemic/">CCC blog</a> how health care could be made more online.</p>
<p>
</p><p></p><h2> Computing’s Role—II </h2><p></p>
<p></p><p>
The above ideas are fine but I believe the real role for computing is simple: </p>
<blockquote><p><b> </b> <em> <i>Make signing up and obtaining an appointment for a vaccine easier, fairer, and sooner.</i> </em>
</p></blockquote>
<p></p><p>
In the US each state is in charge of running web sites that allow people to try and get an appointment for a vaccine shot. <i>Try</i> is the key word. Almost all sites require an appointment to get a shot—walk-ins are mostly not allowed. </p>
<p>
I cannot speak for all states and all web sites, but my direct experience is that the sites are terrible. Signing up for a vaccination shot is a disaster. The web sites that I have seen are poorly written, clumsy, and difficult to use. They are some of the worst sites I have ever needed to use, for anything. Some of the top issues: </p>
<ol>
<li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites rules are confusing and unclear. <p></p>
</li><li>
You may need to search for particular vaccine locations, rather than for any locations. <p></p>
</li><li>
And more <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" />
</li></ol>
<p>Repeating (1,2,3) is a poor joke, but one that reflects reality. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
If Amazon, Google, Apple had sites that worked this way, they would be out of business quickly. Perhaps this is the key: <i>Can our top companies help build the state sites?</i> Is it too late to help? See <a href="https://www.nytimes.com/2021/01/12/technology/the-problem-with-vaccine-websites.html">here</a> for a New York Times article on this issue: </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/game/" rel="attachment wp-att-18129"><img width="300" alt="" class="aligncenter wp-image-18129" src="https://rjlipton.files.wordpress.com/2021/02/game.png?w=300" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<blockquote><p><b> </b> <em> When you start to pull your hair out because you can’t register for a vaccine on a local website, remember that it’s not (only) the fault of a bad tech company or misguided choices by government leaders today. It’s a systematic failure years in the making. </em>
</p></blockquote>
<p></p><p>
Also is the issue of <a href="https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709">algorithmic fairness</a> relevant here? We know that it is unfortunately easy to have web sites that are unfair—that assign vaccine sign up dates unfairly, that favor one class of people over another. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/"><span class="datestr">at February 19, 2021 03:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5347">Brief thoughts on the Texas catastrophe</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or heat or clean water; the UT campus a short walk from me converted into a giant refugee camp.</p>



<p>For all those who asked: my family and I are fine.  While many we know were without power for days (or are <em>still</em> without power), we lucked out by living close to a hospital, which means that they can’t shut off the electricity to our block.  We <em>are</em> now on a boil-water notice, like all of Austin, and we can’t take deliveries or easily go anywhere, and the university and schools and daycares are all closed (even for remote learning).  Which means: we’re simply holed up in our house, eating through our stockpiled food, the kids running around being crazy, Dana and I watching them with one eye and our laptops with the other.  Could be worse.</p>



<p>In some sense, it’s not surprising that the Texas infrastructure would buckle under weather stresses outside the envelope of anything it was designed for or saw for decades.  The central problem is that our elected leaders have shown zero indication of understanding the urgent need, for Texas’ economic viability, to do whatever it takes to make sure nothing like this ever happens again.  Ted Cruz, as everyone now knows, left for Cancun; the mayor of Colorado City angrily told everyone to fend for themselves (and then resigned); and Governor Abbott has been blaming frozen wind turbines, a tiny percentage of the problem (frozen gas pipes are a much bigger issue) but one that plays with the base.  The bare minimum of a sane response might be, I dunno,</p>



<ul><li>acknowledging the reality that climate change means that “once-per-century” weather events will be every couple years from now on,</li><li>building spare capacity (nuclear would be ideal … well, I can dream),</li><li>winterizing what we have now, and</li><li>connecting the Texas grid to the rest of the US.</li></ul>



<p>If I were a Texas Democrat, I’d consider making Republican incompetence on infrastructure, utilities, and public health my <em>only</em> campaign issues.</p>



<p>Alright, now back to watching the Mars lander, which is apparently easier to build and deploy than a reliable electric grid.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5347"><span class="datestr">at February 18, 2021 09:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4494">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/">This year, for Lent, we realized it has been Lent all along</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give something up as a penance, such as giving up eating meat.</p>



<p>The period that immediately precedes Lent is known as Carnival, and, perhaps incongruously, it is a time for having fun, playing pranks, and eating special sweets, often deep-fried ones. Traditionally kids, and also grownups, dress up in costumes and attend costume parties. The idea being, let’s have fun and eat now, because soon we are “entirely voluntarily”  going to fast and to reflect on sin and death, and stuff like that. The day before Ash Wednesday, indeed, is called “Fat Tuesday”.</p>



<p>In Milan, however, the tradition is to power through Ash Wednesday and to continue the Carnival festivities until the following Sunday. There are a number of legends that explain this unique tradition, that is apparently ancient. One such legend is that a plague epidemic had been ravaging Milan in the IV century around the time that should have been Carnival, and life was beginning to go back to normal right around Ash Wednesday. So people rebelled against Lent, and were like, haven’t we suffered enough, what more penance do we need, and celebrated Carnival later.</p>



<p>It has now been nearly a year since the first lockdown, and we still cannot travel between regions (for example, we cannot travel from Milan to Bologna, or to Venice), cannot eat dinner in a restaurant, cannot go see a movie, a play or a sporting event, cannot ski, and so on.</p>



<p>My proposal is that when (if?) we go back to a normal life, we shorten Lent to three days (start with “Ash Thursday” the day before Good Friday), and that we make Carnival  start on Easter Monday and last for 361 days. Not because we have had it worse than a IV century plague epidemic: indeed, even in the best of times, IV century people in Milan did not usually eat in restaurants, travel to Venice, see movies, or ski. We, however, are spoiled XXI century people, we are not used to inconveniences, and when (if?) this is over we will need a lot of self-care, especially the eating-deep-fried-sweets-and-partying kind of self-care.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/"><span class="datestr">at February 18, 2021 01:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">Postdoc at Imperial College London in Complexity  (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based at the South Kensington campus at the heart of London.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br />
Email: iddo.tzameret@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/"><span class="datestr">at February 18, 2021 11:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">PhD positions at Imperial College London (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by Iddo Tzameret.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br />
Email: iddo.tzameret@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/"><span class="datestr">at February 18, 2021 11:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08905">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08905">The Complexity of Gerrymandering Over Graphs: Paths and Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koana:Tomohiro.html">Tomohiro Koana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08905">PDF</a><br /><b>Abstract: </b>Roughly speaking, gerrymandering is the systematic manipulation of the
boundaries of electoral districts to make a specific (political) party win as
many districts as possible. While typically studied from a geographical point
of view, addressing social network structures, the investigation of
gerrymandering over graphs was recently initiated by Cohen-Zemach et al. [AAMAS
2018]. Settling three open questions of Ito et al. [AAMAS 2019], we classify
the computational complexity of the NP-hard problem Gerrymandering over Graphs
when restricted to paths and trees. Our results, which are mostly of negative
nature (that is, worst-case hardness), in particular yield two complexity
dichotomies for trees. For instance, the problem is polynomial-time solvable
for two parties but becomes weakly NP-hard for three. Moreover, we show that
the problem remains NP-hard even when the input graph is a path.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08905"><span class="datestr">at February 18, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08885">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08885">Differentially Private Correlation Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bun:Mark.html">Mark Bun</a>, Marek Eliáš, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulkarni:Janardhan.html">Janardhan Kulkarni</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08885">PDF</a><br /><b>Abstract: </b>Correlation clustering is a widely used technique in unsupervised machine
learning. Motivated by applications where individual privacy is a concern, we
initiate the study of differentially private correlation clustering. We propose
an algorithm that achieves subquadratic additive error compared to the optimal
cost. In contrast, straightforward adaptations of existing non-private
algorithms all lead to a trivial quadratic error. Finally, we give a lower
bound showing that any pure differentially private algorithm for correlation
clustering requires additive error of $\Omega(n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08885"><span class="datestr">at February 18, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08808">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08808">Fast Graphical Population Protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alistarh:Dan.html">Dan Alistarh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gelashvili:Rati.html">Rati Gelashvili</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rybicki:Joel.html">Joel Rybicki</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08808">PDF</a><br /><b>Abstract: </b>Let $G$ be a graph on $n$ nodes. In the stochastic population protocol model,
a collection of $n$ indistinguishable, resource-limited nodes collectively
solve tasks via pairwise interactions. In each interaction, two randomly chosen
neighbors first read each other's states, and then update their local states. A
rich line of research has established tight upper and lower bounds on the
complexity of fundamental tasks, such as majority and leader election, in this
model, when $G$ is a clique. Specifically, in the clique, these tasks can be
solved fast, i.e., in $n \operatorname{polylog} n$ pairwise interactions, with
high probability, using at most $\operatorname{polylog} n$ states per node.
</p>
<p>In this work, we consider the more general setting where $G$ is an arbitrary
graph, and present a technique for simulating protocols designed for
fully-connected networks in any connected regular graph. Our main result is a
simulation that is efficient on many interesting graph families: roughly, the
simulation overhead is polylogarithmic in the number of nodes, and quadratic in
the conductance of the graph. As a sample application, we show that, in any
regular graph with conductance $\phi$, both leader election and exact majority
can be solved in $\phi^{-2} \cdot n \operatorname{polylog} n$ pairwise
interactions, with high probability, using at most $\phi^{-2} \cdot
\operatorname{polylog} n$ states per node. This shows that there are fast and
space-efficient population protocols for leader election and exact majority on
graphs with good expansion properties. We believe our results will prove
generally useful, as they allow efficient technology transfer between the
well-mixed (clique) case, and the under-explored spatial setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08808"><span class="datestr">at February 18, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08778">Large-Scale Benchmarks for the Job Shop Scheduling Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Col:Giacomo_Da.html">Giacomo Da Col</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teppan:Erich.html">Erich Teppan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08778">PDF</a><br /><b>Abstract: </b>This report contains the description of two novel job shop scheduling
benchmarks that resemble instances of real scheduling problem as they appear in
industry. In particular, the aim was to provide large-scale benchmarks (up to 1
million operations) to test the state-of-the-art scheduling solutions on
problems that are closer to what occurs in a real industrial context. The first
benchmark is an extension of the well known Taillard benchmark (1992), while
the second is a collection of scheduling instances with a known-optimum
solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08778"><span class="datestr">at February 18, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08765">Deterministic Algorithms for Compiling Quantum Circuits with Recurrent Patterns</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferrari:Davide.html">Davide Ferrari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tavernelli:Ivano.html">Ivano Tavernelli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amoretti:Michele.html">Michele Amoretti</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08765">PDF</a><br /><b>Abstract: </b>Current quantum processors are noisy, have limited coherence and imperfect
gate implementations. On such hardware, only algorithms that are shorter than
the overall coherence time can be implemented and executed successfully. A good
quantum compiler must translate an input program into the most efficient
equivalent of itself, getting the most out of the available hardware. In this
work, we present novel deterministic algorithms for compiling recurrent quantum
circuit patterns in polynomial time. In particular, such patterns appear in
quantum circuits that are used to compute the ground state properties of
molecular systems using the variational quantum eigensolver (VQE) method
together with the RyRz heuristic wavefunction Ansatz. We show that our
pattern-oriented compiling algorithms, combined with an efficient swapping
strategy, produces - in general - output programs that are comparable to those
obtained with state-of-art compilers, in terms of CNOT count and CNOT depth. In
particular, our solution produces unmatched results on RyRz circuits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08765"><span class="datestr">at February 18, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08703">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08703">Local Mending</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balliu:Alkida.html">Alkida Balliu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirvonen:Juho.html">Juho Hirvonen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Melnyk:Darya.html">Darya Melnyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olivetti:Dennis.html">Dennis Olivetti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rybicki:Joel.html">Joel Rybicki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suomela:Jukka.html">Jukka Suomela</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08703">PDF</a><br /><b>Abstract: </b>In this work we introduce the graph-theoretic notion of mendability: for each
locally checkable graph problem we can define its mending radius, which
captures the idea of how far one needs to modify a partial solution in order to
"patch a hole."
</p>
<p>We explore how mendability is connected to the existence of efficient
algorithms, especially in distributed, parallel, and fault-tolerant settings.
It is easy to see that $O(1)$-mendable problems are also solvable in $O(\log^*
n)$ rounds in the LOCAL model of distributed computing. One of the surprises is
that in paths and cycles, a converse also holds in the following sense: if a
problem $\Pi$ can be solved in $O(\log^* n)$, there is always a restriction
$\Pi' \subseteq \Pi$ that is still efficiently solvable but that is also
$O(1)$-mendable.
</p>
<p>We also explore the structure of the landscape of mendability. For example,
we show that in trees, the mending radius of any locally checkable problem is
$O(1)$, $\Theta(\log n)$, or $\Theta(n)$, while in general graphs the structure
is much more diverse.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08703"><span class="datestr">at February 18, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08670">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08670">Linear Time Runs over General Ordered Alphabets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ellert:Jonas.html">Jonas Ellert</a>, Johannes Fischer <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08670">PDF</a><br /><b>Abstract: </b>A run in a string is a maximal periodic substring. For example, the string
$\texttt{bananatree}$ contains the runs $\texttt{anana} = (\texttt{an})^{3/2}$
and $\texttt{ee} = \texttt{e}^2$. There are less than $n$ runs in any
length-$n$ string, and computing all runs for a string over a linearly-sortable
alphabet takes $\mathcal{O}(n)$ time (Bannai et al., SODA 2015). Kosolobov
conjectured that there also exists a linear time runs algorithm for general
ordered alphabets (Inf. Process. Lett. 2016). The conjecture was almost proven
by Crochemore et al., who presented an $\mathcal{O}(n\alpha(n))$ time algorithm
(where $\alpha(n)$ is the extremely slowly growing inverse Ackermann function).
We show how to achieve $\mathcal{O}(n)$ time by exploiting combinatorial
properties of the Lyndon array, thus proving Kosolobov's conjecture.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08670"><span class="datestr">at February 18, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08623">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08623">Reviews: Topological Distances and Losses for Brain Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chung:Moo_K=.html">Moo K. Chung</a>, Alexander Smith, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shiu:Gary.html">Gary Shiu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08623">PDF</a><br /><b>Abstract: </b>Almost all statistical and machine learning methods in analyzing brain
networks rely on distances and loss functions, which are mostly Euclidean or
matrix norms. The Euclidean or matrix distances may fail to capture underlying
subtle topological differences in brain networks. Further, Euclidean distances
are sensitive to outliers. A few extreme edge weights may severely affect the
distance. Thus it is necessary to use distances and loss functions that
recognize topology of data. In this review paper, we survey various topological
distance and loss functions from topological data analysis (TDA) and persistent
homology that can be used in brain network analysis more effectively. Although
there are many recent brain imaging studies that are based on TDA methods,
possibly due to the lack of method awareness, TDA has not taken as the
mainstream tool in brain imaging field yet. The main purpose of this paper is
provide the relevant technical survey of these powerful tools that are
immediately applicable to brain network data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08623"><span class="datestr">at February 18, 2021 11:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08598">Leveraging Public Data for Practical Private Query Release</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Terrance.html">Terrance Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vietri:Giuseppe.html">Giuseppe Vietri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steinke:Thomas.html">Thomas Steinke</a>, Jonathan Ullman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Zhiwei_Steven.html">Zhiwei Steven Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08598">PDF</a><br /><b>Abstract: </b>In many statistical problems, incorporating priors can significantly improve
performance. However, the use of prior knowledge in differentially private
query release has remained underexplored, despite such priors commonly being
available in the form of public datasets, such as previous US Census releases.
With the goal of releasing statistics about a private dataset, we present
PMW^Pub, which -- unlike existing baselines -- leverages public data drawn from
a related distribution as prior information. We provide a theoretical analysis
and an empirical evaluation on the American Community Survey (ACS) and ADULT
datasets, which shows that our method outperforms state-of-the-art methods.
Furthermore, PMW^Pub scales well to high-dimensional data domains, where
running many existing methods would be computationally infeasible.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08598"><span class="datestr">at February 18, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08569">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08569">Constructing a Distance Sensitivity Oracle in $O(n^{2.5794}M)$ Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yong.html">Yong Gu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ren:Hanlin.html">Hanlin Ren</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08569">PDF</a><br /><b>Abstract: </b>We continue the study of distance sensitivity oracles (DSOs). Given a
directed graph $G$ with $n$ vertices and edge weights in $\{1, 2, \dots, M\}$,
we want to build a data structure such that given any source vertex $u$, any
target vertex $v$, and any failure $f$ (which is either a vertex or an edge),
it outputs the length of the shortest path from $u$ to $v$ not going through
$f$. Our main result is a DSO with preprocessing time $O(n^{2.5794}M)$ and
constant query time. Previously, the best preprocessing time of DSOs for
directed graphs is $O(n^{2.7233}M)$, and even in the easier case of undirected
graphs, the best preprocessing time is $O(n^{2.6865}M)$ [Ren, ESA 2020]. One
drawback of our DSOs, though, is that it only supports distance queries but not
path queries.
</p>
<p>Our main technical ingredient is an algorithm that computes the inverse of a
degree-$d$ polynomial matrix (i.e. a matrix whose entries are degree-$d$
univariate polynomials) modulo $x^r$. The algorithm is adapted from [Zhou,
Labahn and Storjohann, Journal of Complexity, 2015], and we replace some of its
intermediate steps with faster rectangular matrix multiplication algorithms.
</p>
<p>We also show how to compute unique shortest paths in a directed graph with
edge weights in $\{1, 2, \dots, M\}$, in $O(n^{2.5286}M)$ time. This algorithm
is crucial in the preprocessing algorithm of our DSO. Our solution improves the
$O(n^{2.6865}M)$ time bound in [Ren, ESA 2020], and matches the current best
time bound for computing all-pairs shortest paths.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08569"><span class="datestr">at February 18, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08529">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08529">Efficient Maintenance of Distance Labelling for Incremental Updates in Large Dynamic Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farhan:Muhammad.html">Muhammad Farhan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Qing.html">Qing Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08529">PDF</a><br /><b>Abstract: </b>Finding the shortest path distance between an arbitrary pair of vertices is a
fundamental problem in graph theory. A tremendous amount of research has been
successfully attempted on this problem, most of which is limited to static
graphs. Due to the dynamic nature of real-world networks, there is a pressing
need to address this problem for dynamic networks undergoing changes. In this
paper, we propose an \emph{online incremental} method to efficiently answer
distance queries over very large dynamic graphs. Our proposed method
incorporates incremental update operations, i.e. edge and vertex additions,
into a highly scalable framework of answering distance queries. We
theoretically prove the correctness of our method and the preservation of
labelling minimality. We have also conducted extensive experiments on 12 large
real-world networks to empirically verify the efficiency, scalability, and
robustness of our method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08529"><span class="datestr">at February 18, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08476">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08476">Maximum Coverage in the Data Stream Model: Parameterized and Generalized</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGregor:Andrew.html">Andrew McGregor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tench:David.html">David Tench</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vu:Hoa_T=.html">Hoa T. Vu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08476">PDF</a><br /><b>Abstract: </b>We present algorithms for the Max-Cover and Max-Unique-Cover problems in the
data stream model. The input to both problems are $m$ subsets of a universe of
size $n$ and a value $k\in [m]$. In Max-Cover, the problem is to find a
collection of at most $k$ sets such that the number of elements covered by at
least one set is maximized. In Max-Unique-Cover, the problem is to find a
collection of at most $k$ sets such that the number of elements covered by
exactly one set is maximized. Our goal is to design single-pass algorithms that
use space that is sublinear in the input size. Our main algorithmic results
are:
</p>
<p>If the sets have size at most $d$, there exist single-pass algorithms using
$\tilde{O}(d^{d+1} k^d)$ space that solve both problems exactly. This is
optimal up to polylogarithmic factors for constant $d$.
</p>
<p>If each element appears in at most $r$ sets, we present single pass
algorithms using $\tilde{O}(k^2 r/\epsilon^3)$ space that return a $1+\epsilon$
approximation in the case of Max-Cover. We also present a single-pass algorithm
using slightly more memory, i.e., $\tilde{O}(k^3 r/\epsilon^{4})$ space, that
$1+\epsilon$ approximates Max-Unique-Cover.
</p>
<p>In contrast to the above results, when $d$ and $r$ are arbitrary, any
constant pass $1+\epsilon$ approximation algorithm for either problem requires
$\Omega(\epsilon^{-2}m)$ space but a single pass $O(\epsilon^{-2}mk)$ space
algorithm exists. In fact any constant-pass algorithm with an approximation
better than $e/(e-1)$ and $e^{1-1/k}$ for Max-Cover and Max-Unique-Cover
respectively requires $\Omega(m/k^2)$ space when $d$ and $r$ are unrestricted.
En route, we also obtain an algorithm for a parameterized version of the
streaming Set-Cover problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08476"><span class="datestr">at February 18, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08454">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08454">Lexicographically Fair Learning: Algorithms and Generalization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diana:Emily.html">Emily Diana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gill:Wesley.html">Wesley Gill</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Globus=Harris:Ira.html">Ira Globus-Harris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kearns:Michael.html">Michael Kearns</a>, Aaron Roth, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharifi=Malvajerdi:Saeed.html">Saeed Sharifi-Malvajerdi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08454">PDF</a><br /><b>Abstract: </b>We extend the notion of minimax fairness in supervised learning problems to
its natural conclusion: lexicographic minimax fairness (or lexifairness for
short). Informally, given a collection of demographic groups of interest,
minimax fairness asks that the error of the group with the highest error be
minimized. Lexifairness goes further and asks that amongst all minimax fair
solutions, the error of the group with the second highest error should be
minimized, and amongst all of those solutions, the error of the group with the
third highest error should be minimized, and so on. Despite its naturalness,
correctly defining lexifairness is considerably more subtle than minimax
fairness, because of inherent sensitivity to approximation error. We give a
notion of approximate lexifairness that avoids this issue, and then derive
oracle-efficient algorithms for finding approximately lexifair solutions in a
very general setting. When the underlying empirical risk minimization problem
absent fairness constraints is convex (as it is, for example, with linear and
logistic regression), our algorithms are provably efficient even in the worst
case. Finally, we show generalization bounds -- approximate lexifairness on the
training sample implies approximate lexifairness on the true distribution with
high probability. Our ability to prove generalization bounds depends on our
choosing definitions that avoid the instability of naive definitions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08454"><span class="datestr">at February 18, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.08446">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.08446">Smoothed Analysis with Adaptive Adversaries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haghtalab:Nika.html">Nika Haghtalab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shetty:Abhishek.html">Abhishek Shetty</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.08446">PDF</a><br /><b>Abstract: </b>We prove novel algorithmic guarantees for several online problems in the
smoothed analysis model. In this model, at each time an adversary chooses an
input distribution with density function bounded above by $\tfrac{1}{\sigma}$
times that of the uniform distribution; nature then samples an input from this
distribution. Crucially, our results hold for {\em adaptive} adversaries that
can choose an input distribution based on the decisions of the algorithm and
the realizations of the inputs in the previous time steps.
</p>
<p>This paper presents a general technique for proving smoothed algorithmic
guarantees against adaptive adversaries, in effect reducing the setting of
adaptive adversaries to the simpler case of oblivious adversaries. We apply
this technique to prove strong smoothed guarantees for three problems:
</p>
<p>-Online learning: We consider the online prediction problem, where instances
are generated from an adaptive sequence of $\sigma$-smooth distributions and
the hypothesis class has VC dimension $d$. We bound the regret by
$\tilde{O}\big(\sqrt{T d\ln(1/\sigma)} + d\sqrt{\ln(T/\sigma)}\big)$. This
answers open questions of [RST11,Hag18].
</p>
<p>-Online discrepancy minimization: We consider the online Koml\'os problem,
where the input is generated from an adaptive sequence of $\sigma$-smooth and
isotropic distributions on the $\ell_2$ unit ball. We bound the $\ell_\infty$
norm of the discrepancy vector by $\tilde{O}\big(\ln^2\!\big(
\frac{nT}{\sigma}\big) \big)$.
</p>
<p>-Dispersion in online optimization: We consider online optimization of
piecewise Lipschitz functions where functions with $\ell$ discontinuities are
chosen by a smoothed adaptive adversary and show that the resulting sequence is
$\big( {\sigma}/{\sqrt{T\ell}}, \tilde O\big(\sqrt{T\ell}
\big)\big)$-dispersed. This matches the parameters of [BDV18] for oblivious
adversaries, up to log factors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.08446"><span class="datestr">at February 18, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7992">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do deep networks learn and when do they learn it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://manosth.github.io/">Manos Theodosis</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through statistical learning theory</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c518b9e4-5f63-4278-871d-acc2017b8984">Lecture video</a> – <a href="https://boazbk.github.io/mltheoryseminar/lectures/seminar_lecture2.pdf">Slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_2.pptx">Slides (powerpoint with ink and animation)</a></p>



<p>In this lecture, we talk about <em>what</em> neural networks end up learning (in terms of their weights) and <em>when</em>, during training, they learn it.</p>



<p>In particular, we’re going to discuss</p>



<ul><li><strong>Simplicity bias</strong>: how networks favor “simple” features first.</li><li><strong>Learning dynamics</strong>: what is learned early in training.</li><li><strong>Different layers</strong>: do the different layers learn the same features?</li></ul>



<p>The type of results we will discuss are:</p>



<ul><li>Gradient-based deep learning algorithms have a bias toward learning simple classifiers. In particular this often holds when the optimization problem they are trying to solve is “underconstrained/overparameterized”, in the sense that there are exponentially many different models that fit the data.</li><li>Simplicity also affects the <em>timing</em> of learning. Deep learning algorithms tend to learn simple (but still predictive!) features first.</li><li>Such “simple predictive features” tend to be in lower (closer to input) levels of the network. Hence deep learning also tends to learn lower levels earlier.</li><li>On the other side, the above means that distributions that do not have “simple predictive features” pose significant challenges for deep learning. Even if there is a small neural network that works very well for the distribution, gradient-based algorithms will not “get off the ground” in such cases. We will see a lower bound for <em>learning parities</em> that makes this intuition formal.</li></ul>



<h2>What do neural networks learn, and when do they learn it?</h2>



<p>As a first example to showcase what is learned by neural networks, we’ll consider the following data distribution where we sample points <img src="https://s0.wp.com/latex.php?latex=%28X%2C+Y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X, Y)" class="latex" title="(X, Y)" />, with <img src="https://s0.wp.com/latex.php?latex=Y+%5Cin+%7B1%2C+-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y \in {1, -1}" class="latex" title="Y \in {1, -1}" /> (<img src="https://s0.wp.com/latex.php?latex=Y+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y = 1" class="latex" title="Y = 1" /> corresponding to orange points and <img src="https://s0.wp.com/latex.php?latex=Y%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y=-1" class="latex" title="Y=-1" /> corresponding to blue points).<br /><img src="https://i.imgur.com/xiXoqDj.png" alt="" /></p>



<p>If we <a href="http://tfmeter.icsi.berkeley.edu/#activation=tanh&amp;batchSize=10&amp;dataset=byod&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;trueLearningRate=0&amp;regularizationRate=0&amp;noise=35&amp;networkShape=10,6,4,2&amp;seed=0.36834921&amp;showTestData=false&amp;discretize=false&amp;percTrainData=74&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">train</a> a neural network to fit this distribution, we can see below that the neurons that are closest to the input data end up learning features that are highly correlated with the input (mostly linear subspaces at 45-degree angle, which correspond to one of the stripes). In the subsequent layers, the features learned are more sophisticated and have increased complexity.<br /><img src="https://i.imgur.com/p2FQdUd.png" alt="" /><br /></p>



<h3>Neural networks have simpler but useful features in lower layers</h3>



<p>Some people have spent a lot of time trying to understand what is learned by different layers. In a <a href="https://distill.pub/2020/circuits/early-vision/">recent</a> work, Olah et al. dig deep into a particular architecture for computer vision, trying to interpret the features learned by neurons at different layers.</p>



<p>They found that earlier layers learn features that resemble edge detectors.<br /><img src="https://i.imgur.com/I8veBVf.png" alt="" /><br />However, as we go deeper, the neurons at those layers start learning more convoluted (for example, these features from layer <img src="https://s0.wp.com/latex.php?latex=3b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3b" class="latex" title="3b" /> resemble heads).<br /><img src="https://i.imgur.com/rJsmKHR.png" alt="" /></p>



<h3>SGD learns simple (but still predictive) features earlier.</h3>



<p>There is evidence that <a href="https://arxiv.org/abs/1905.11604">SGD learns simpler classifiers first</a>. The following figure tracks how much of a learned classifier’s performance can be accounted for by a linear classifier. We see that up to a certain point in training <em>all</em> of the performance of the neural network learned by SGD (measured as mutual information with the label or as accuracy) can be ascribed to the linear classifier. They diverge only very near the point where the linear classifier “saturates,” in the sense that the classifier reachers the best possible accuracy for linear models. (We use the quantity <img src="https://s0.wp.com/latex.php?latex=I%28f%28x%29%3By+%7CL%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(f(x);y |L(x))" class="latex" title="I(f(x);y |L(x))" /> – the mutual information of <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> conditioned on the prediction of the linear classifier <img src="https://s0.wp.com/latex.php?latex=L%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(x)" class="latex" title="L(x)" /> – to measure how much of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />‘s performance <em>cannot</em> be accounted for by <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" />.)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/pGxap3e.png" alt="" /></figure>



<h3>The benefits and pitfalls of simplicity bias</h3>



<p>In general, simplicity bias is a very good thing. For example, the most “complex” function is a random function. However, if given some observed data <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29%7D_%7Bi%5Cin+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i)}_{i\in [n]}" class="latex" title="{ (x_i,y_i)}_{i\in [n]}" />, SFD were to find a random function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that perfectly fits it, then it would never generalize (since for every fresh <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, the value of <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> would be random).</p>



<p>At the same time, simplicity bias means that our algorithms might focus too much on simple solutions and miss more complex ones. Sometimes the complex solutions actually do perform better. In the following cartoon a person could go to the low-hanging fruit tree on the right-hand side and miss the bigger rewards on the left-hand side.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/eijQfpl.png" alt="" /></figure>



<p>This <a href="https://arxiv.org/abs/2006.07710">can actually happen</a> in neural networks. We also saw a simple example in class:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/LvONKsw.png" alt="" /></figure>



<p>The two datasets are equally easy to represent, but on the righthand side, there is a very strong “simple classifier” (the 45-degree halfspace) that SGD will “latch onto.” Once it gets stuck with that classifier, it is hard for SGD to get “unstuck.” As a result, SGD has a much harder time learning the righthand dataset than the lefthand dataset.</p>



<h2>Analysing SGD for over-parameterized linear regression</h2>



<p>So, what can we prove about the dynamics of gradient descent? Often we can gain insights by studying <em>linear regression</em>.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/T9VGkXj.png" alt="" /></figure>



<p>Formally, given <img src="https://s0.wp.com/latex.php?latex=%28x_i%2C+y_i%29_%7Bi%3D1%7D%5En+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" class="latex" title="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" /> with <img src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d\gg n" class="latex" title="d\gg n" /> we would like to find a vector <img src="https://s0.wp.com/latex.php?latex=w%5Cin%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w\in\mathbb{R}^d" class="latex" title="w\in\mathbb{R}^d" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+w%2C+x_i%5Crangle%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle w, x_i\rangle\approx y_i" class="latex" title="\langle w, x_i\rangle\approx y_i" />.</p>



<p>In this setting, we can prove that running SGD (from zero or tiny initialization) on the loss <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%29+%3D%5ClVert+Xw+-y+%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(w) =\lVert Xw -y \rVert^2" class="latex" title="\mathcal{L}(w) =\lVert Xw -y \rVert^2" /> will converge to solution <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> of minimum norm. To see whym note that SGD performs updates of the form<br /><img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta+x_i%5ET%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" class="latex" title="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" />.<br />However note that <img src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta(\langle x_i, w\rangle - y_i)" class="latex" title="\eta(\langle x_i, w\rangle - y_i)" /> is a scalar. Therefore all of the updates keep the updated vector <img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1}" class="latex" title="w_{t+1}" /> within <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" />. This implies that the converging solution <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" /> will also lie in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" />.<br /><img src="https://i.imgur.com/G8tFMrF.png" alt="" /><br />Geometrically this translates into <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" /> being the projection of <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> onto the the subspace <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" /> which results in the least norm solution.</p>



<p>Analyzing the dynamics of descent, we can write the distance between consecutive weight updates and the converging solution as<br /><img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+-+w_%7B%5Cinfty%7D+%3D+%28I+-+%5Ceta+X%5ETX%29%28w_t+-+w_%7B%5Cinfty%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" class="latex" title="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" />.<br />We see that we are applying the linear operator <img src="https://s0.wp.com/latex.php?latex=%28I+-+%5Ceta+X%5ETX%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(I - \eta X^TX)" class="latex" title="(I - \eta X^TX)" /> at every step we take. As long as this operator is contractive, we will continue to progress and converge to <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" />. Formally, to make progress, we require<br /><img src="https://s0.wp.com/latex.php?latex=0+%5Cprec+I+-%5Ceta+X%5ETX%5Cprec+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \prec I -\eta X^TX\prec 1" class="latex" title="0 \prec I -\eta X^TX\prec 1" />.<br />This directly translates into <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+%5Cfrac%7B1%7D%7B%5Clambda_1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta &lt; \frac{1}{\lambda_1}" class="latex" title="\eta &lt; \frac{1}{\lambda_1}" /> and then the progress we make is approximately <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda_d%7D%7B%5Clambda_1%7D%3D%5Cfrac%7B1%7D%7B%5Ckappa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" class="latex" title="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" />, where <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\kappa" class="latex" title="\kappa" /> is the <em>condition number</em> of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" />.</p>



<p>What happens now if the matrix <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> is random? Then, results from random matrix theory (specifically the <a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko-Pastur distribution</a>) state that</p>



<ul><li>if <img src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &lt; n" class="latex" title="d &lt; n" />, then the matrix <img src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X^\top X" class="latex" title="X^\top X" /> has <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Brank%7D%28X%5E%5Ctop+X%29%3Dd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{rank}(X^\top X)=d" class="latex" title="\mathrm{rank}(X^\top X)=d" /> and the eigenvalues are bounded away from <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" />. This means that the matrix is well conditioned.</li><li>if <img src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \approx n" class="latex" title="d \approx n" />, then the spectrum of <img src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X^\top X" class="latex" title="X^\top X" /> starts shifting towards <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" />, with some eigenvalues being equal to zero, resulting in an ill-conditioned matrix.</li><li>if <img src="https://s0.wp.com/latex.php?latex=d+%3E+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &gt; n" class="latex" title="d &gt; n" />, then the spectrum has some zero eigenvalues, but is otherwise bounded away from zero. If we restrict to the subspace of positive eigenvalues, we achieve again a good condition number.</li></ul>



<figure class="wp-block-image"><img src="https://i.imgur.com/hLZtfz5.png" alt="" /></figure>



<p></p>



<h2>Deep linear networks</h2>



<p>We now want to go beyond linear regression and talk about deep networks. As deep networks are very hard to understand, we will first start analyzing a depth <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2" class="latex" title="2" /> network. We will also consider a <em>linear</em> network and omit the nonlinearity. This might seem strange, as we could consider the corresponding linear model, which has exactly the same expressiveness. However, note that these two models have a different parameter space. This means that gradient-based algorithms will travel on different paths when optimizing these two models.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ekh9k8M.png" alt="" /></figure>



<p>Specifically, we can see that the minimum loss attained by the two models will coincide, i.e., <img src="https://s0.wp.com/latex.php?latex=%5Cmin+%5Cmathcal%7BL%7D%28A_1%2C+A_2%29+%3D+%5Cmin+%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" class="latex" title="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" />, but the SGD path and the solution will be different.</p>



<p>We will analyze the gradient flow on these two networks (which is gradient descent with the learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta \rightarrow 0" class="latex" title="\eta \rightarrow 0" />). We will make the simplifying assumption that <img src="https://s0.wp.com/latex.php?latex=A_1+%3D+A_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_1 = A_2" class="latex" title="A_1 = A_2" /> and symmetric. Then, we can see that <img src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2+%5CRightarrow+A+%3D+%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B = A^2 \Rightarrow A = \sqrt{B}" class="latex" title="B = A^2 \Rightarrow A = \sqrt{B}" />. We will try and compare the gradient flow of two different loss functions: <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(B)" class="latex" title="\mathcal{L}(B)" /> (doing gradient flow on a linear model) and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Cmathcal%7BL%7D%28A%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" class="latex" title="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" /> (doing gradient flow on a depth linear model).</p>



<p>Gradient flow on the linear model simply gives <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" class="latex" title="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" />, whereas for the deep linear network we have (using the chain rule)<br /><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%28t%29%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29A+%3D+A%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," class="latex" title="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," /><br />since <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> is symmetric.</p>



<p>For simplicity, let’s denote <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Cmathcal%7BL%7D%28B%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29+%3D+%5Cnabla&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" class="latex" title="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Ctilde%7B%5Cnabla%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" class="latex" title="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" />. We then have<br /><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D%3D%5Cfrac%7BdA%28t%29%7D%7Bdt%7DA%3D-%5Ctilde%7B%5Cnabla%7DA+%3D+-A+%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" class="latex" title="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" />.</p>



<p>Another way to view the comparison between the models of interest, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA^2(t)}{dt}" class="latex" title="\frac{dA^2(t)}{dt}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt}" class="latex" title="\frac{dB(t)}{dt}" /> is as follows: let <img src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B = A^2" class="latex" title="B = A^2" />, then <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-A%5Cnabla+A+%3D+-%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" title="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" />.<br />We can view this as follows: when we multiply the gradient with <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{B}" class="latex" title="\sqrt{B}" /> we end up making the “big bigger and the small smaller”. Basically, this accenuates the differences between the eigenvalues and is biasing <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> to become a low-rank matrix. </p>



<p>To see why, you can think of a low rank matrix has one that has few large eigenvalues and the others small. If <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> is already close low rank, then replacing a gradient by <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" title="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" /> encourages the gradient steps to mostly happen in the top eigenspace of <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" />. This result <a href="https://arxiv.org/abs/1910.05505">generalizes</a> to networks of greater depth, and the gradient evolves as <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-%5Cpsi_%7BB%28t%29%7D%28%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" class="latex" title="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" />, with <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D%28%5Cnabla%29+%3D+%5Csum+B%5E%7B%5Calpha%7D%5Cnabla+B%5E%7B1-%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" class="latex" title="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" />.</p>



<p>This means that we end up doing gradient flow on a <em>Riemannian manifold</em>. An interesting result is that the flow induced by the operator <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\psi_{B}" class="latex" title="\psi_{B}" /> is provably not equivalent to a regularized minimization problem <img src="https://s0.wp.com/latex.php?latex=%5Cmin%5Cmathcal%7BL%7D+%2B+%5Clambda+R%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min\mathcal{L} + \lambda R(B)" class="latex" title="\min\mathcal{L} + \lambda R(B)" /> for any <img src="https://s0.wp.com/latex.php?latex=R%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R(\cdot)" class="latex" title="R(\cdot)" />.</p>



<h2>What is learned at different layers?</h2>



<p>Finally, let’s discuss what is learned by the different layers in a neural network. Some intuition people have is that learning proceeds roughly like the following cartoon:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/rteesY5.png" alt="" /></figure>



<p>We can think of our data as being “built up” as a sequence of choices from higher level to lower level features. For example, the data is generated by first deciding that it would be a photo of a dog, then that it would be on the beach, and finally low-level details such as the type of fur and light. This is also how a human would describe this photo. In contrast, a neural network builds up the features in the opposite direction. It starts from the simplest (lowest-level) features in the image (edges, textures, etc.) and gradually builds up complexity until it finally classifies the image.</p>



<h2>How neural networks learn features?</h2>



<p>To build a bit of intuition, consider an example of combining different simple features. We can see that if we try to combine two good edge detectors with different orientations, the end result will hardly be an edge detector.<br /><img src="https://i.imgur.com/CsTB2LV.png" alt="" /></p>



<p>So the intuition is that there is competitive/evolutionary pressure on neurons to “specialize” and recognize useful features. Initially, all the neurons are random features, which can be thought of as random linear combination of the various detectors. However, after training, the symmetry will break between the neurons, and they will specialize (in this simple example, they will either become vertical or horizontal edge detectors).</p>



<p><a href="https://arxiv.org/abs/1706.05806">Raghu, Gilmer, Yosinski, and Sohl-Dickstein</a> tracked the speed at which features learned by different layers reach their final learned state. In the figure below the diagonal elements denote the similarity of the current state of a layer to its final one, where lighter color means that the state is more similar. We can see that earlier layer (more to the left) reach their final state earlier (with th exception of the 2 layers closest to the output that also converge very early).</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/qHTIAzz.png" alt="" /></figure>



<p>The “symmetry breaking” intuition is explored by a recent work of <a href="https://arxiv.org/abs/1912.05671">Frankle, Dziugaite, Roy, and Carbin</a>. Intuitively, because the average of two good features is generally <em>not</em> a good feature, averaging the weights of two neural networks with small loss will likely result in a network with large loss. That is, if we start from two random initializations <img src="https://s0.wp.com/latex.php?latex=w_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_0" class="latex" title="w_0" />, <img src="https://s0.wp.com/latex.php?latex=w%27_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_0" class="latex" title="w'_0" /> and train two networks until we reach weights <img src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_\infty" class="latex" title="w_\infty" /> and <img src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_\infty" class="latex" title="w'_\infty" /> with small loss, then we expect the average of <img src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_\infty" class="latex" title="w_\infty" /> and <img src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_\infty" class="latex" title="w'_\infty" /> to result in a network with poor loss:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/uLPBPpN.png" alt="" /></figure>



<p>In contrast, Frankle et al showed that sometimes, when we start from the same initialization (especially after pruning) and use random SGD noise (obtained by randomly shuffling the training set) then we reach a “linear plateu” of the loss function in which averaging two networks yields a network with similar loss:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/owtRGUL.png" alt="" /></figure>



<h2>The contrapositive of simplicity: lower bounds for learning parities</h2>



<p>If we believe that networks learn simple features first, and learn them in the early layers, then this has an interesting consequence. If the data has the form that simple features (e.g. linear or low degree) are completely uninformative (have no correlation with the label) then we may expect that learning cannot “get off the ground”. That is, even if there exists a small neural network that can learn the class, gradient based algorithms such as SGD will never find it. (In fact, it is possible that <em>no</em> efficient algorithm could find it.) There are some settings where we can prove such conjectures. (For gradient-based algorithms that is; proving this for all efficient algorithms would require settling the P vs NP question.)</p>



<p>We discuss one of the canonical “hard” examples for neural networks: parities. Formally, for <img src="https://s0.wp.com/latex.php?latex=I%5Csubset+%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I\subset [d]" class="latex" title="I\subset [d]" />, the distribution <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" /> is the distribution over <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+%7B+%5Cpm+1+%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \in { \pm 1 }^{d+1}" class="latex" title="(x,y) \in { \pm 1 }^{d+1}" /> defined as follows: <img src="https://s0.wp.com/latex.php?latex=x%5Csim+%7B%5Cpm+1%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim {\pm 1}^d" class="latex" title="x\sim {\pm 1}^d" /> and <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y = \prod_{i\in I}x_i" class="latex" title="y = \prod_{i\in I}x_i" />. The “learning parity” problem is as follows: given <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> samples <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i) }_{i=1..n}" class="latex" title="{ (x_i,y_i) }_{i=1..n}" /> drawn from <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />, either recover <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> or do the weaker task of finding a predictor <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> such that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=y" class="latex" title="f(x)=y" /> with high probability over future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \sim D_I" class="latex" title="(x,y) \sim D_I" />.</p>



<p>It turns out that if we don’t restrict ourselves to deep learning, given <img src="https://s0.wp.com/latex.php?latex=2d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2d" class="latex" title="2d" /> samples we can recover <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />. Consider the transformations <img src="https://s0.wp.com/latex.php?latex=Z_%7Bi%2Cj%7D+%3D+%281+-+x_%7Bi%2Cj%7D%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z_{i,j} = (1 - x_{i,j})/2" class="latex" title="Z_{i,j} = (1 - x_{i,j})/2" /> and <img src="https://s0.wp.com/latex.php?latex=b_i+%3D+%281+-+y_i%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b_i = (1 - y_i)/2" class="latex" title="b_i = (1 - y_i)/2" />. If we let <img src="https://s0.wp.com/latex.php?latex=s_i%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_i=1" class="latex" title="s_i=1" /> if <img src="https://s0.wp.com/latex.php?latex=i%5Cin+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\in I" class="latex" title="i\in I" /> and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> otherwise, we can write <img src="https://s0.wp.com/latex.php?latex=%5Csum_j+Z_%7Bi%2C+j%7Ds_j+%3D+b_i+%28%5Ctext%7Bmod+%7D+2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" class="latex" title="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" />. Basically, we transformed the problem of parity to a problem of counting if we have an odd or an even number of <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-1" class="latex" title="-1" />. In this setting, we can think of every sample <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \in D_I" class="latex" title="(x,y) \in D_I" /> as providing a <em>linear equation</em> moudlo 2 over the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> unknown variables <img src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1,\ldots,s_d" class="latex" title="s_1,\ldots,s_d" />. When <img src="https://s0.wp.com/latex.php?latex=n%3Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n&gt;d" class="latex" title="n&gt;d" />, these linear equations will be very likely to be of full rank, and hence we can use Gaussian elimination to find <img src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1,\ldots,s_d" class="latex" title="s_1,\ldots,s_d" /> and hence <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />.</p>



<p>Switching to the learning setting, we can express parities by using few ReLUs. In particular, we’ve shown that we can create a step function using <img src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="4" class="latex" title="4" /> ReLUs. Therefore for every <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%7B0%2C1%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k \in {0,1,\ldots, d }" class="latex" title="k \in {0,1,\ldots, d }" />, there is a combination of four ReLUs that computes the function <img src="https://s0.wp.com/latex.php?latex=f_k%3A%5Cmathbb%7BR%7D+%3A%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k:\mathbb{R} :\rightarrow \mathbb{R}" class="latex" title="f_k:\mathbb{R} :\rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k(s)" class="latex" title="f_k(s)" /> outputs <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> for <img src="https://s0.wp.com/latex.php?latex=s%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s=k" class="latex" title="s=k" />, and <img src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k(s)" class="latex" title="f_k(s)" /> outputs <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> if <img src="https://s0.wp.com/latex.php?latex=%7Cx-k%7C%3E0.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|x-k|&gt;0.5" class="latex" title="|x-k|&gt;0.5" />. We can then write the parity function (for example for <img src="https://s0.wp.com/latex.php?latex=I%3D%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I=[d]" class="latex" title="I=[d]" />) as <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Ctext%7B+odd+%7D+%5Cin+%5Bd%5D%7D+f_k%28%5Csum_%7Bi%3D1%7D%5Ed+%281-x_i%29%2F2+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" class="latex" title="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" />. This will be a linear combination of at most <img src="https://s0.wp.com/latex.php?latex=4d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="4d" class="latex" title="4d" /> ReLUs.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/NwQ31Jy.png" alt="" /></figure>



<p>Parities are an example of a case where simple feature are uninformative. For example, if <img src="https://s0.wp.com/latex.php?latex=%7CI%7C%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|I|&gt;1" class="latex" title="|I|&gt;1" /> then for every linear function <img src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="L:\mathbb{R}^d \rightarrow \mathbb{R}" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%28x%2Cy%29+%5Csim+D_I%7D%5B+L%28x%29y%5D+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" class="latex" title="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" /></p>



<p>in other words, there is no correlation between the linear function and the label.<br />To see why this is true, write <img src="https://s0.wp.com/latex.php?latex=L%28x%29+%3D+%5Csum+L_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(x) = \sum L_i x_i" class="latex" title="L(x) = \sum L_i x_i" />. By linearity of expectation, it suffices to show that $latex \mathbb{E}<em>{(x,y) \sim D_I}[ L_ix_i y] = L_i \mathbb{E}</em>{(x,y) \sim D_I}[ x_i y] = 0&amp;bg=ffffff$. Both <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> are just values in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\pm 1 }" class="latex" title="{\pm 1 }" />. To evaluate the expectation <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+y%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[x_i y]" class="latex" title="\mathbb{E}[x_i y]" /> we simply need to know the marginal distribution that <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" /> induces on <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }^2" class="latex" title="{ \pm 1 }^2" /> when we restrict it to these two coordinates. This distribution is just the uniform distribution. To see why this is the case, consider a coordinate <img src="https://s0.wp.com/latex.php?latex=j%5Cin+I+%5Csetminus+%7B+i+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j\in I \setminus { i }" class="latex" title="j\in I \setminus { i }" /> and let’s condition on the values of all coordinates other than <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j" class="latex" title="j" />. After conditioning on these values, <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Csigma+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y = \sigma x_i x_j" class="latex" title="y = \sigma x_i x_j" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Csigma+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma \in { \pm 1 }" class="latex" title="\sigma \in { \pm 1 }" /> and <img src="https://s0.wp.com/latex.php?latex=x_i%2Cx_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i,x_j" class="latex" title="x_i,x_j" /> are chosen uniformly and independently from <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }" class="latex" title="{ \pm 1 }" />. For every choice of <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" />, if we flip <img src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_j" class="latex" title="x_j" /> then that would flip the value of <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, and hence the marginal distribution on <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> will be uniform.</p>



<p>This lack of correlation turns out to be a real obstacle for gradient-based algorithms. While small neural networks for parities exist, and Gaussian elimination can find them, it turns out that gradient-based algorithms such as SGD will <em>fail</em> to do so. Parities are hard to learn, and even if the capacity of the network is such that it can memorize the input, it will still perform poorly in a test set. Indeed, we can prove that for <em>every</em> neural network architecture <img src="https://s0.wp.com/latex.php?latex=f_w%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_w(x)" class="latex" title="f_w(x)" />, running SGD on <img src="https://s0.wp.com/latex.php?latex=%5Cmin%5ClVert+f_w%28x%29+-%5Cprod_%7Bi%5Cin+I%7D+x_i%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" class="latex" title="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" /> will require <img src="https://s0.wp.com/latex.php?latex=e%5E%7B%5COmega%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{\Omega(d)}" class="latex" title="e^{\Omega(d)}" /> steps. (Note that if we add <em>noise</em> to parities, then Gaussian elimination will fail and it is believed that <em>no efficient algorithm</em> can learn the distribution in this case. This is known as the <a href="https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/Cryptography_from_learning_parity_with_noise.pdf">learning parity with noise</a> problem, which is also related to the <a href="https://en.wikipedia.org/wiki/Learning_with_errors">learning with errors</a> problem that is the foundation of modern lattice-based cryptography.)</p>



<p>We now sketch the proof that gradient-based algorithms require exponentially many steps to learn parities, following Theorem 1 of <a href="https://arxiv.org/abs/1703.07950">Shalev-Shwartz,Shamir and Shammah</a>. We think of an idealized setting where we have an unlimited number of samples and only use a sample <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \sim D_I" class="latex" title="(x,y) \sim D_I" /> only once (this should only make learning easier). We will show that we make very little progress in learning <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />, by showing that for any given <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />, the expected gradient over <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y)" class="latex" title="(x,y)" /> will be exponentially small, and hence we make very little progress toward learning <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />. Specifically, using the notation <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%3D%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x)=\prod_{i\in I}x_i" class="latex" title="\chi_I(x)=\prod_{i\in I}x_i" />, for any <img src="https://s0.wp.com/latex.php?latex=w%2Cx%2CI&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w,x,I" class="latex" title="w,x,I" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla_w+%5Cparallel+f_w%28x%29+-+%5Cchi_I%28x%29+%5Cparallel%5E2+%3D+2%5Csum_%7Bi%3D1%7D%5Ed+%5Cleft+%5B+f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29-+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" class="latex" title="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" /></p>



<p>The term <img src="https://s0.wp.com/latex.php?latex=f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" class="latex" title="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" /> is independent of <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and so does not contribute toward learning <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />. Hence intuitively to show we make exponentially small progress, it suffices to show that typically for every <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />, <img src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" class="latex" title="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" /> will be exponentially small. (That is, even if for a fixed <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> we make a large step, these all cancel out and give us exponentially small progress toward actually learning <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />.)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/lWCuan2.png" alt="" /></figure>



<p>Formally, we will prove the following lemma:</p>



<p><strong>Lemma:</strong> For every <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />, <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_I+%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2+%5Cleq+%5Ctfrac%7Bpoly%28d%2Cn%29%5Cmathbb%7BE%7D_x+%5Cfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5E2%7D%7B2%5Ed%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" class="latex" title="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" /></p>



<p><strong>Proof:</strong> Let us fix <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and define <img src="https://s0.wp.com/latex.php?latex=g%28x%29+%3D+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x) = \tfrac{d}{d x_i}f_{w}(x)" class="latex" title="g(x) = \tfrac{d}{d x_i}f_{w}(x)" />. The quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" class="latex" title="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" /> can be written as <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2Cg+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \chi_I,g \rangle" class="latex" title="\langle \chi_I,g \rangle" /> with respect to the inner product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+f%2Cg+%5Crangle+%3D+%5Cmathbb%7BE%7D_x+f%28x%29g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" class="latex" title="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" />. However, <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cchi_I+%7D%7BI%5Csubseteq+%5Bd%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \chi_I }{I\subseteq [d]}" class="latex" title="{ \chi_I }{I\subseteq [d]}" /> is an orhtonormal basis with respect to this inner product. To see this note that since <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x) \in { \pm 1 }" class="latex" title="\chi_I(x) \in { \pm 1 }" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+%5Cchi_I%28x%29%5E2+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x \chi_I(x)^2 = 1" class="latex" title="\mathbb{E}_x \chi_I(x)^2 = 1" /> for every <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />, and for <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />, <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%5Cchi_J%28x%29+%3D+%28%5Cprod%7Bi+%5Cin+I%7D+x_i%29%28%5Cprod_%7Bj+%5Cin+J%7D+x_j%29+%3D+%5Cprod_%7Bk+%5Cin+I+%5Coplus+H%7D+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" class="latex" title="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" /> where <img src="https://s0.wp.com/latex.php?latex=I%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I\oplus J" class="latex" title="I\oplus J" /> is the symmetric difference of <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />. The reason is that <img src="https://s0.wp.com/latex.php?latex=x_i%5E2+%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i^2 =1" class="latex" title="x_i^2 =1" /> for all <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and so elements that appear in both <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> “cancel out”. Since the coordinates of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> are distributed independently and uniformly, the expectation of the product is the product of expectations. This means that as long as <img src="https://s0.wp.com/latex.php?latex=I+%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \oplus J" class="latex" title="I \oplus J" /> is not empty (i.e., <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />) this will be a product of one or more terms of the form <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BE%7D+x_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mathbb{E} x_k)" class="latex" title="(\mathbb{E} x_k)" />. Since <img src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_k" class="latex" title="x_k" /> is uniform over <img src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cpm+1+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\{ \pm 1 \}" class="latex" title="\{ \pm 1 \}" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x_k+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} x_k = 0" class="latex" title="\mathbb{E} x_k = 0" /> and so we get that if <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2C%5Cchi_J+%5Crangle+%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \chi_I,\chi_J \rangle =0" class="latex" title="\langle \chi_I,\chi_J \rangle =0" />.</p>



<p>Given the above</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+g%28x%29%5E2+%3D+%5Clangle+g%2Cg%5Crangle+%3D+%5Csum_%7BI+%5Csubseteq+%5Bd%5D%7D+%5Clangle+g+%2C+%5Cchi_I+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" class="latex" title="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" /></p>



<p>which means that (since there are <img src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^d" class="latex" title="2^d" /> subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[d]" class="latex" title="[d]" />) on average <img src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C+%5Cchi_I+%5Crangle+%3D+%5Cparallel+g+%5Cparallel%5E2+%2F+2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" class="latex" title="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" />. In other words, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C%5Cchi_I+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle g,\chi_I \rangle" class="latex" title="\langle g,\chi_I \rangle" /> is typically exponentially small which is what we wanted to prove.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/"><span class="datestr">at February 17, 2021 09:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/">Alan Selman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>A special journal issue in his honor</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/all-9/" rel="attachment wp-att-18136"><img width="200" alt="" class="alignright  wp-image-18136" src="https://rjlipton.files.wordpress.com/2021/02/all.png?w=200" /></a></p>
<p>
Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal <a href="https://www.springer.com/journal/224">Theory of Computing Systems</a> dedicated to Alan Selman. Alan passed away this January 2021. </p>
<p>
Today we circulate their request for contributions.</p>
<p>
The details of the <a href="https://www.springer.com/journal/224/updates/18863610">call</a> say:  This special issue celebrates Alan’s life and commemorate his extraordinary contributions to the field. The topics of interest include but are not limited to: </p>
<ul>
<li>
average-case complexity <p></p>
</li><li>
circuit complexity <p></p>
</li><li>
comparison of reducibilities <p></p>
</li><li>
complexity theoretic characterizations of models <p></p>
</li><li>
function complexity <p></p>
</li><li>
hierarchy theorems <p></p>
</li><li>
	parameterized complexity <p></p>
</li><li>
promise problems and disjoint NP-pairs <p></p>
</li><li>
public-key cryptography <p></p>
</li><li>
relativization <p></p>
</li><li>
semi-feasible algorithms <p></p>
</li><li>
sparse sets <p></p>
</li><li>
structure of complete sets
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>Please look at <a href="https://www.springer.com/journal/224/updates/18863610">this</a> for details—the deadline for submission is 31st July 2021. You have 164 days to write your paper. Which is 3936 hours or 236160 minutes.</p>
<p>
Please send a contribution. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/"><span class="datestr">at February 17, 2021 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/02/17/stoc-2021-workshops/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/02/17/stoc-2021-workshops/">STOC 2021 workshops</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 21-25, 2021 Online http://acm-stoc.org/stoc2021/callforworkshops.html Submission deadline: March 15, 2021 STOC 2021 will hold workshops during the conference week, June 21–25, 2021. We invite groups of interested researchers to submit workshop proposals. The due date for proposals is March 15.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/02/17/stoc-2021-workshops/"><span class="datestr">at February 17, 2021 04:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings.html">Lattice Borromean rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A lot of topology is finding ways to prove things that are really obvious but where explaining why they’re obvious can be difficult. So I want to do this for a discrete analogue of <a href="https://en.wikipedia.org/wiki/Ropelength">ropelength</a>, the length of the shortest lattice representation, for the <a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean rings</a>. You can find several pretty lattice (and non-lattice) representations of the Borromean rings in a paper by Verhoeff &amp; Verhoeff, “<a href="https://archive.bridgesmathart.org/2015/bridges2015-53.pdf">Three families of mitered Borromean ring sculptures</a>” [<em>Bridges</em>, 2015]; the one in the middle of their figure 2, thinned down to use only lattice edges and not thick solid components, is the one I have in mind. It is formed by three \(2\times 4\) rectangles, shown below next to <a href="https://en.wikipedia.org/wiki/Jessen%27s_icosahedron">Jessen’s icosahedron</a> which has the same vertex coordinates. (You can do the same thing with a regular icosahedron but then you get non-lattice golden rectangles.)</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/Borromean-Jessen.svg" alt="Lattice Borromean rings and Jessen's icosahedron" /></p>

<p>Each of the three rectangles has perimeter \(12\), so the total length of the whole link is \(36\). Why should this be the minimum possible? One could plausibly run a brute force search over all small-enough realizations, but this would be tedious and some effort would be needed to prune the search enough to get it to run at all. Instead, I found an argument based on the lengths of the individual components of the link, allowing me to analyze them (mostly) separately.</p>

<p>Each component is unknotted, so it can be the boundary of a disk in space. Importantly, for the Borromean rings, every disk spanned by one of the components must be crossed at least twice by other components. If we could find a disk spanned by one component that was not crossed at all by other components, then we could shrink the first component topologically within its disk down to a size so small that it could easily be pulled apart from the other two components, something that is not possible with the Borromean rings. And if we could find a disk that was only crossed once by another component, then the <a href="https://en.wikipedia.org/wiki/Linking_number">linking number</a> of the two components would be one, something that doesn’t happen for the Borromean rings.</p>

<p>If you travel in some consistent direction around a cycle in a 3d lattice, every step in one direction along a coordinate axis must be cancelled by a step in the opposite direction elsewhere along the ring. So if a lattice cycle has length \(\ell\), there must be \(\ell/2\) pairs of opposite steps, partitioned somehow among the three dimensions. If the bounding box of the cycle has size \(a\times b\times c\), then we must have \(a+b+c\le\ell/2\), and we can classify the possible shapes of lattice cycles of length \(\ell\) by the possible shapes of their bounding boxes. This gives us the following cases:</p>

<ul>
  <li>
    <p>A lattice cycle of length \(\ell=4\) can only be a square, with bounding box dimensions \(1\times 1\times 0\) (the zero means that it lies in a single plane in 3d, not that it doesn’t exist at all). The square itself is a disk not crossed by any other lattice path, unusable as a component of the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=6\) can be a rectangle with bounding box \(2\times 1\times 0\), or fully 3-dimensional with bounding box \(1\times 1\times 1\). There are two fully 3-dimensional cases, one that avoids two opposite vertices of the bounding box and one that avoids two adjacent vertices. The rectangle can be its own spanning disk, and in the 3-dimensional cases we can use a spanning disk connecting the center of the bounding cube by a line segment to each point along the ring. Neither of these types of disks is crossed by any other lattice path.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/grid-6-cycles.svg" alt="Spanning disks for three grid 6-cycles" /></p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=8\) can be a rectangle with bounding box \(3\times 1\times 0\), a square with bounding box \(2\times 2\times 0\) or fully 3-dimensional with bounding box \(2\times 1\times 1\). It can also double back on itself and cover all vertices of a cube, with bounding box \(1\times 1\times 1\). All cases except the \(2\times 2\times 0\) square can be handled as in the length \(6\) cases; for instance, for the \(2\times 1\times 1\) bounding box we form a disk at the center of the box, connected by a line segment to all points of the ring. These cases cannot be crossed by any other lattice path. The \(2\times 2\times 0\) square can be crossed by a lattice path, through its center point, but only by one path. We can see from this that the shortest lattice representation of the <a href="https://en.wikipedia.org/wiki/Hopf_link">Hopf link</a> (two linked circles) is the obvious one formed from two length-\(8\) squares. However, these squares are still too small to be used in the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=10\) can be a rectangle with bounding box \(4\times 1\times 0\) or \(3\times 2\times 0\), or fully 3-dimensional with bounding box \(3\times 1\times 1\) or \(2\times 2\times 1\). The \(4\times 1\times 0\) rectangle and the center-point spanning disk of the \(3\times 1\times 1\) box cannot be crossed by any other lattice path, and the center-point spanning disk of the \(2\times 2\times 1\) can be crossed only by one, through its center edge. Using even-smaller bounding boxes doesn’t help.</p>
  </li>
</ul>

<p>That leaves only one problematic case, the \(3\times 2\times 0\) rectangle, of perimeter \(10\), which is shorter than the rectangles of the optimal representation but can nevertheless be crossed by two other lattice paths. In fact, this rectangle can be used as a component in a representation of the Borromean rings. It is even possible to use two of them! (I’ll leave this as an exercise.) So we need some other argument to prove that, when we use one or two of these short rectangles, we have to make up for it elsewhere by making something else extra-long.</p>

<p>If a \(3\times 2\times 0\) rectangle is a component of the Borromean rings, it must be twice by one of the other components, because if its two crossings were from different components it would have nonzero linking number with both of them, different from what happens in the Borromean rings. And the crossings must happen at the two interior lattice points of the rectangle, through paths that (to avoid each other and the boundary of the rectangle) must pass straight across the rectangle, at least for one unit on each side. The component that crosses the rectangle in this way consists of two loops connecting the pairs of ends of these two straight paths; any other connection pattern would lead to linking number \(2\), not zero. We can think of these two loops as being separate cycles, shortcut by the lattice edges between the endpoints of the two straight paths. And any disk that spans either of these two loops must itself be crossed by another component of the Borromean rings, because if one of the loops had an uncrossed spanning disk then we could wrap a spanning disk for the rectangle around it (like a glove around a hand) and create an uncrossed spanning disk for the rectangle as well.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/glove.png" alt="Spanning disk wrapping around a loop like a glove around a hand, adapted from https://commons.wikimedia.org/wiki/File:Disposable_nitrile_glove_with_transparent_background.png" /></p>

<p>By the analysis above, in order to be crossed by something else, both of the two shortcut loops of the component that crosses the \(3\times 2\times 0\) rectangle must have length at least \(8\). Adding in the two straight paths (and removing the two shortcut edges) shows that the component itself must have length at least \(18\). And if we have one component of length \(18\) and two more of length \(10\), we get total length at least \(38\), more than the length of the minimal representation. Since all representations that use components of length less than \(12\) are too long, the representation in which all three component lengths are exactly \(12\) must be the optimal one, QED.</p>

<p>Researching all this led me to an interesting paper by Dai, Ernst, Por, and Ziegler,  “<a href="https://doi.org/10.1142/S0218216519500858">The ropelengths of knots are almost linear in terms of their crossing numbers</a>” [<em>J. Knot Theory and its Ramifications</em>, 2019]. Ropelength is the minimum length of a 3d representation that can be thickened to a radius-1 tube without self-intersections. (Some sources use diameter in place of radius; this changes the numeric values by a factor of two but does not change the optimizing representations.) Doubling the dimensions of a lattice representation gives you such a representation, and on the other hand one can find short lattice representations by following the thickened tubes of a ropelength representation, so ropelength and lattice length are within constant factors of each other. Dai et al. use this to show that knots that can be drawn in the plane with few crossings also have small ropelength. It doesn’t work to use the plane embedding directly, adding a third coordinate to handle the crossings, because some planar graphs (like the <a href="https://en.wikipedia.org/wiki/Nested_triangles_graph">nested triangles graph</a>) have nonlinear total edge length in any planar lattice drawing. Instead, Dai et al show how to crumple up a planar drawing of any degree-four planar graph into a 3d integer lattice embedding of the graph, with near-linear total edge length, so that the faces of the drawing can also be embedded as disks that are not crossed by each other or the graph edges. One can then modify the lifted drawing to turn the degree-four vertices into crossings in the lifted topologically-planar surface formed by these faces, giving a grid representation of the original knot with near-linear total length.</p>

<p>The ropelength of the Borromean rings has also been the subject of some study. Doubling the grid rectangles and rounding off their corners produces three <a href="https://en.wikipedia.org/wiki/Stadium_(geometry)">stadia</a> with total perimeter \(12\pi+24\approx 61.7\). The same argument as above shows that each curve must be at least long enough for all its spanning disks to be crossable by two disjoint radius-1 tubes. Intuitively the smallest curve that can surround two tubes is a smaller stadium corresponding to the \(2\times 3\) rectangle, with length \(4\pi+4\). If so, this would give a lower bound of \(12\pi+12\approx 49.7\) for the total ropelength of the Borromean rings. The conjectured-optimal configuration, <a href="https://archive.bridgesmathart.org/2008/bridges2008-63.html">used for the logo of the International Mathematical Union</a>, uses three copies of a complicated two-lobed planar curve in roughly the same positions as the three rectangles or stadia; it is described carefully by Cantarella, Fu, Kusner, Sullivan, and Wrinkle, “<a href="http://dx.doi.org/10.2140/gt.2006.10.2055">Criticality for the Gehring link problem</a>” [<em>Geometry &amp; Topology</em> 2006] (section 10), and has length \(\approx 58.006\). The intuition that the \(2\times 3\) stadium is the shortest curve that can surround two others also appears to be stated as proven in this paper, in section 7.1. But they state that the best lower bound for the Borromean ropelength is \(12\pi\) so maybe the \(12\pi+12\) argument above is new?</p>

<p><strong>Update, February 17:</strong> In email, John Sullivan pointed me to a paper by Uberti, Janse van Rensburg, Orlandinit, Tesi, and Whittington, “<a href="https://doi.org/10.1007/978-1-4612-1712-1_9">Minimal links in the cubic lattice</a>” [<em>Topology and Geometry in Polymer Science</em>, 1998; see table 2, p. 97], which does the tedious computer search and comes up with the same result, that the shortest length for a lattice representation of the Borromean rings is 36. (I had searched for papers on lattice representations of the Borromean rings but didn’t find this one, probably failing because it identifies the Borromean rings only by the <a href="https://en.wikipedia.org/wiki/Alexander%E2%80%93Briggs_notation">Alexander–Briggs notation</a> \(6_2^3\), which is hard to search for.) John also tells me that the proof of ropelength-minimality of the \(2\times 3\) stadium is only for links in which it is linked with two other components, different enough from the situation here in which every spanning disk is crossed twice that the same proof doesn’t apply. So the question of whether this stadium really is the ropelength minimizer for components satisfying this crossed-twice condition seems to fall into the category of obvious topological facts that are difficult to prove, rather than being already known.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105743724683948185">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings.html"><span class="datestr">at February 16, 2021 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/016">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/016">TR21-016 |  Unambiguous DNFs from Hex | 

	Shalev Ben-David, 

	Mika Göös, 

	Siddhartha Jain, 

	Robin Kothari</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We exhibit an unambiguous $k$-DNF formula that requires CNF width $\tilde{\Omega}(k^{1.5})$. Our construction is inspired by the board game Hex and it is vastly simpler than previous ones, which achieved at best an exponent of $1.22$. Our result is known to imply several other improved separations in query and communication complexity (e.g., clique vs. independent set problem) and graph theory (Alon--Saks--Seymour problem).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/016"><span class="datestr">at February 16, 2021 12:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/015">TR21-015 |  Hitting Sets for Orbits of Circuit Classes and Polynomial Families | 

	Chandan Saha, 

	Bhargav Thankey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The orbit of an $n$-variate polynomial $f(\mathbf{x})$ over a field $\mathbb{F}$ is the set $\mathrm{orb}(f) := \{f(A\mathbf{x}+\mathbf{b}) : A \in \mathrm{GL}(n,\mathbb{F}) \ \mathrm{and} \ \mathbf{b} \in \mathbb{F}^n\}$. This paper studies explicit hitting sets for the orbits of polynomials computable by certain well-studied circuit classes. This version of the hitting set problem is interesting as $\mathrm{orb}(f)$ is a natural subset of the set of affine projections of $f$. Affine projections of polynomials computable by seemingly weak circuit classes can be quite powerful. For example, the polynomial $\mathrm{IMM}_{3,d}$ -- the $(1,1)$-th entry of a product of $d$ generic $3 \times 3$ matrices -- is computable by a constant-width read-once oblivious algebraic branching program (ROABP), yet every polynomial computable by a size-$s$ general arithmetic formula is an affine projection of $\mathrm{IMM}_{3,\mathrm{poly}(s)}$. To our knowledge, no efficient hitting set construction was known for even $\mathrm{orb}(\mathrm{IMM}_{3, d})$ before this work. 

In this work, we give efficient constructions of hitting sets for the orbits of several interesting circuit classes and polynomial families. In particular, we give quasi-polynomial time hitting sets for the orbits of:

1. Low-individual-degree polynomials computable by commutative ROABP. This implies quasi-polynomial time hitting sets for the orbits of multilinear sparse polynomials and the orbits of the elementary symmetric polynomials.

2. Multilinear polynomials computable by constant-width ROABP. This implies a quasi-polynomial time hitting set for the orbit of $\mathrm{IMM}_{3,d}$.

3. Polynomials computable by constant-depth, constant-occur formulas with low-individual-degree sparse polynomials at the leaves. This implies quasi-polynomial time hitting sets for the orbits of multilinear depth-4 circuits with constant top fan-in, and also poly-time hitting sets for the orbits of the power symmetric polynomials and the sum-product polynomials. 

4. Polynomials computable by occur-once formulas with low-individual-degree sparse polynomials at the leaves. 

We say a polynomial has low individual degree if the degree of every variable in the polynomial is at most $\mathrm{poly}(\log n)$, where $n$ is the number of variables.

The first two results are obtained by building upon the rank concentration by translation technique of [Agrawal-Saha-Saxena, STOC'13]; the second result also uses the merge-and-reduce idea from [Forbes-Shpilka, APPROX'13], [Forbes-Saptharishi-Shpilka, STOC'14]. The proof of the third result applies the algebraic independence based technique of [Agrawal-Saha-Saptharishi-Saxena, STOC'12], [Beecken-Mittmann-Saxena, ICALP'11] to reduce to the case of constructing hitting sets for orbits of sparse polynomials. A similar reduction using the Shpilka-Volkovich (SV) generator based argument in [Shpilka-Volkovich, STOC'08, APPROX-RANDOM'09] yields the fourth result. The SV generator plays an important role in all the four results.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/015"><span class="datestr">at February 16, 2021 08:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5330">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5330">On standing up sans backbone</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<blockquote class="wp-block-quote"><p><strong>Note:</strong> To get myself into the spirit of writing this post, tonight I watched the 2019 movie <a href="https://www.amazon.com/Mr-Jones-James-Norton/dp/B089XVJB9S/ref=sr_1_1?dchild=1&amp;keywords=Mr.+Jones+%282019%29&amp;qid=1613446265&amp;s=instant-video&amp;sr=1-1">Mr. Jones</a>, about the true story of the coverup of Stalin’s 1932-3 mass famine by <em>New York Times</em> journalist <a href="https://en.wikipedia.org/wiki/Walter_Duranty">Walter Duranty</a>.  Recommended!</p></blockquote>



<p>In my <a href="https://www.scottaaronson.com/blog/?p=5310">last post</a>, I wrote that despite all my problems with Cade Metz’s <em>New York Times</em> hit piece on Scott Alexander, I’d continue talking to journalists—even Metz himself, I added, assuming he’d still talk to me after my public disparagement of his work.  Over the past few days, though, the many counterarguments in my comments section and elsewhere gradually caused me to change my mind.  I now feel like to work with Metz again, even just on some quantum computing piece, would be to reward—and to be seen as rewarding—journalistic practices that are making the world worse, and that this consideration overrides even my extreme commitment to openness.</p>



<p>At the least, before I could talk to Metz again, I’d need a better understanding of how the hit piece happened.  What was the role of the editors?  How did the original hook—namely, the rationalist community’s early rightness about covid-19—disappear entirely from the article?  How did the piece manage to evince so little <em>curiosity</em> about such an unusual subculture and such a widely-admired writer?  How did it fail so completely to engage with the rationalists’ <em>ideas</em>, instead jumping immediately to “six degrees of Peter Thiel” and other reductive games?  How did an angry SneerClubber, David Gerard, end up (according to <a href="https://twitter.com/davidgerard/status/1360735880466604040">his own boast</a>) basically dictating the NYT piece’s content?</p>



<p>It’s always ripping-off-a-bandage painful to admit when trust in another person was wildly misplaced—for then who<em> else</em> can we not trust?  But sometimes that’s the truth of it.</p>



<p>I continue to believe passionately in the centrality of good journalism to a free society.  I’ll continue to talk to journalists often, about quantum computing or whatever else.  I also recognize that the NYT is a large, heterogeneous institution (I myself <a href="https://www.nytimes.com/2011/12/06/science/scott-aaronson-quantum-computing-promises-new-insights.html">published</a> in it <a href="https://www.nytimes.com/2019/10/30/opinion/google-quantum-computer-sycamore.html">twice</a>); it’s not hard to imagine that many of its own staff take issue with the SSC piece.</p>



<p>But let’s be clear about the stakes here.  In the discussion of my last post, I <a href="https://www.scottaaronson.com/blog/?p=5310#comment-1878641">described</a> the NYT as “still the main vessel of consensus reality in <s>human civilization</s>” [alright, alright, American civilization!].  What’s really at issue, beyond the treatment of a single blogger, is whether the NYT can continue serving that central role in a world reshaped by social media, resurgent fascism, and entitled wokery.</p>



<p>Sure, we all know that the NYT has been disastrously wrong before: it ridiculed Goddard’s dream of spaceflight, denied the Holodomor, relegated the Holocaust to the back pages while it was happening, published the fabricated justifications for the Iraq War.  But the NYT and a few other publications were still the blockchain of reality, the engine of the consensus of all that is, the last bulwark against the conspiracists and the anti-vaxxers and the empowered fabulists and the horned insurrectionists storming the Capitol, because there was no ability to coordinate around any serious alternative.  I’m <em>still</em> skeptical that there’s a serious alternative, but I now look more positively than I did just a few days ago on attempts to create one.</p>



<p>To all those who called me naïve or a coward for having cooperated with the NYT: believe me, I’m well aware that I wasn’t born with much backbone.  (I am, after all, that guy on the Internet who famously once planned on a life of celibate asceticism, or more likely suicide, rather than asking women out and thereby risking eternal condemnation as a misogynistic sexual harasser by the normal, the popular, the socially adept, the … <em>humanities grads</em> and the <em>journalists</em>.)  But whenever I need a pick-me-up, I tell myself that rather than being ashamed about my lack of a backbone, I can take pride in having occasionally managed to stand even without one.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5330"><span class="datestr">at February 16, 2021 05:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://cacm.acm.org/opinion/articles/250078-lets-not-dumb-down-the-history-of-computer-science/fulltext">Let’s not dumb down the history of computer science</a> (<a href="https://mathstodon.xyz/@11011110/105663355639241434">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/190214/Lets-Not-Dumb-Down-the-History-of-Computer-Science">via</a>). A 2014 plea from Knuth to historians of computer science to stop ignoring the technical parts of the history, reprinted this month in CACM.</p>
  </li>
  <li>
    <p><a href="https://www.newscientist.com/article/2266041-tom-gaulds-runaway-lobster-telephone-problem/">Studies in ethical surrealism: the runaway lobster telephone problem</a> (<a href="https://mathstodon.xyz/@11011110/105664424099324982">\(\mathbb{M}\)</a>). I was pleased to learn that <a href="https://en.wikipedia.org/wiki/Lobster_Telephone">the lobster telephone depicted in this cartoon is a real objet d’art</a>.</p>
  </li>
  <li>
    <p><a href="https://www.archim.org.uk/eureka/archive/">Archive of back issues of Eureka</a> (<a href="https://mathstodon.xyz/@11011110/105671433589477405">\(\mathbb{M}\)</a>, <a href="https://aperiodical.com/2021/02/aperiodical-news-roundup-january-2021/">via</a>), the recreational mathematics journal of the Cambridge Archimedeans, now online for open access. On Wikipedia, the popular articles from Eureka appear to be Dyson’s work on ranks of partitions, in #8, Haselgrove &amp; Haselgrove on polyominoes, in #23, Penrose on pentaplexity, in #39, and Leinster on his eponymous groups, in #55.</p>
  </li>
  <li>
    <p><a href="https://philpapers.org/rec/BOBFPT">In a new book chapter, Susanne Bobzien claims that famous philosopher of logic Gottlob Frege plagiarized extensively from the Stoic logicians</a> (<a href="https://mathstodon.xyz/@11011110/105680228594244342">\(\mathbb{M}\)</a>, <a href="https://dailynous.com/2021/02/03/frege-plagiarize-stoics/">via</a>, <a href="https://www.metafilter.com/190330/Frege-plagiarized-the-Stoics">via2</a>, <a href="https://handlingideas.blog/2021/02/05/the-stoic-foundations-of-analytic-philosophy-on-susanne-bobziens-groundbreaking-discovery-in-frege-and-prantl/">see also</a>).</p>
  </li>
  <li>
    <p>How did I not know about the <a href="https://civs.cs.cornell.edu/">Condorcet Internet Voting Service</a> before (<a href="https://mathstodon.xyz/@11011110/105683306227631389">\(\mathbb{M}\)</a>)? Set up public or private polls and collate the results with your favorite Condorcet rank aggregation method (at least, if your favorite is one of the five they implement, which it probably is). Their public polls are kind of insipid, though, and in comments David Bremner brings up their past history of enabling online abusers.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@RefurioAnachro/105684468712016832">Perkel’s graph and the 57-cell</a>, multi-post sequence on an abstract 4-polytope and associated distance-regular graph, by Refurio Anachro.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">The Victoria Delfino Problems</a> (<a href="https://mathstodon.xyz/@11011110/105694384937282737">\(\mathbb{M}\)</a>). Bill Gasarch blogs about mathematics problems named after non-mathematicians, in this case a Los Angeles based real estate agent.</p>
  </li>
  <li>
    <p>The speech recognition system Zoom and/or my university are using to auto-caption my recorded lectures (whatever it is) really doesn’t like the word “bipartite”, heavily used in my lecture on matching (<a href="https://mathstodon.xyz/@11011110/105700305493373685">\(\mathbb{M}\)</a>). It came out “bipartisan”, “invite part tight”, “by party”, “by protect”, “by apartheid”, “by part aight”, and “by partnership”. Also “spanning forest” is now “Hispanic forest”, but mysteriously it got “spanning tree” right.</p>
  </li>
  <li>
    <p><a href="https://retractionwatch.com/2021/02/09/20-ways-to-spot-the-work-of-paper-mills/">20 ways to spot the work of paper mills</a> (<a href="https://mathstodon.xyz/@11011110/105702699189961130">\(\mathbb{M}\)</a>). However one, using a non-institutional email address, is not “a bad global habit”, but deliberate. I have no thought of moving but do not want my entire professional life tied by email to my employer. My UCI address keeps student emails private but I tend to use gmail for off-campus concerns such as publishers. And not all scholars have institutions who can provide emails. If they refuse my email, I refuse to publish with them.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.01543">Ben Green presents super-polynomial lower bounds for off-diagonal van der Waerden numbers \(W(3,k)\)</a> (<a href="https://mathstodon.xyz/@11011110/105713896982428000">\(\mathbb{M}\)</a>, <a href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/">via</a>). \(W(3,k)\) is the smallest \(N\) such that a 2-coloring of \([N]\) has a 3-term arithmetic progression of one color or a \(k\)-term progression of the other. It was previously known to be subexponential and thought to be only quadratic.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/382940/440">The compound of an 11-simplex in an 11-hypercube (as a subset of its vertices) has the Mathieu group M11 as its symmetries</a> (<a href="https://mathstodon.xyz/@11011110/105717258433892969">\(\mathbb{M}\)</a>, <a href="https://cp4space.hatsya.com/2021/02/08/a-curious-construction-of-the-mathieu-group-m11/">via</a>). The via link goes on to describe how to find two dual 11-simplices in the same hypercube from the perfect ternary Golay code, much like the two simplices in a 3-cube that form the stella octangula.</p>
  </li>
  <li>
    <p><a href="https://distill.pub/selforg/2021/textures/">Self-organizing textures</a> (<a href="https://mathstodon.xyz/@11011110/105719504474157451">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26112959">via</a>). A small input image + “neural cellular automata” magic leads to organic-looking image textures.</p>
  </li>
  <li>
    <p><a href="https://cse.buffalo.edu/socg21/accepted.html">Accepted papers for the Symposium on Computational Geometry</a> (SoCG; <a href="https://mathstodon.xyz/@11011110/105727235598338446">\(\mathbb{M}\)</a>). Decisions are out for the Symposium on Theory of Computing (STOC) but I haven’t seen a public list yet. Upcoming submission deadlines include the <a href="https://projects.cs.dal.ca/wads2021/">Algorithms and Data Structures Symposium</a> (WADS, Feb. 20), <a href="https://wg2021.mimuw.edu.pl/">Graph-Theoretic Concepts in Computer Science</a> (WG, Mar. 3), and the new <a href="https://www.siam.org/conferences/cm/conference/acda21">SIAM Conference on Applied and Computational Discrete Algorithms</a> (ACDA21, Mar. 1).</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/ode-to-hippasus/">A new contribution of Hypatia to mathematics</a> (<a href="https://mathstodon.xyz/@11011110/105732195870260348">\(\mathbb{M}\)</a>). Not the ancient Hypatia, but Hypatia Hamkins, and her parents, philosopher Barbara Gail Montero and logician Joel David Hamkins; the contribution is a verse proof of the irrationality of \(\sqrt{2}\).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/channel/UC8bRNi3tJX-tfR_RMtyWR7w">Computational Geometry YouTube channel</a> (<a href="https://mathstodon.xyz/@11011110/105739350560629546">\(\mathbb{M}\)</a>). This has been set up by Sariel Har-Peled and Sándor Fekete, and is recording talks from the New York Geometry Seminar. So far there are eleven, of roughly an hour length each.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/15/linkage.html"><span class="datestr">at February 15, 2021 09:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/">Pigenhole Principle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Mathematics is based on the application of simple ideas over and over: From tiny nuts do big trees grow. </em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/jv/" rel="attachment wp-att-18113"><img width="150" alt="" class="alignright wp-image-18113" src="https://rjlipton.files.wordpress.com/2021/02/jv.png?w=150" /></a></p>
<p>
Jorgen Veisdal is an assistant professor at the Norwegian University of Science and Technology. He is also the editor in chief at <a href="https://medium.com/cantors-paradise">Cantor’s Paradise</a>, which is a publication of math and science essays. </p>
<p>
Today I thought we would discuss a <a href="https://medium.com/cantors-paradise/the-pigeonhole-principle-e4c637940619">post</a> of his on the famous Pigenhole Princeiple (PP).</p>
<p>
Recall the PP states that if <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> items are put into <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m}" class="latex" title="{m}" /> boxes, with <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3E+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n &gt; m}" class="latex" title="{n &gt; m}" />, then at least one box must contain more than one item.</p>
<p>
The paradox in my opinion is that this idea has any power at all. I wonder if I could explain why it was stated as an explicit <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle">principle</a> by the famous Peter Dirichlet under the name Schubfachprinzip (“drawer principle” or “shelf principle”) in 1834. </p>
<p>
Parts of mathematics not only use PP, but could not live without it. Other parts of mathematics—I believe—are almost untouched by it. Am I right about this? Number theory and combinatorics especially Ramsey theory could not survive without it. What happens in your favorite area? Is there some area of math that is almost untouched by PP?</p>
<p>
</p><table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/pigeons/" rel="attachment wp-att-18114"><img width="400" alt="" class="aligncenter  wp-image-18114" src="https://rjlipton.files.wordpress.com/2021/02/pigeons.png?w=400" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p></p><h2> An Example of PP </h2><p></p>
<p>
The main issue is why is PP so indispensable to some areas of math. But I though it might be fun to give a sample type of proof that uses PP.</p>
<p>
Prove that however one selects 55 integers 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%5Cle+x_%7B1%7D+%3C+x_%7B2%7D+%3C+x_%7B3%7D+%3C+%5Cdots+%3C+x_%7B55%7D+%5Cle+100%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100," class="latex" title="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100," /></p>
<p>there will be some two that differ by 9, some two that differ by 10, a pair that differ by 12, and a pair that differ by 13. Surprisingly, there need not be a pair of numbers that differ by 11. </p>
<p></p><h2> Proof </h2><p></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28y%2Cx%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(y,x)}" class="latex" title="{f(y,x)}" /> be the number of collisions when placing <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" /> into <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" />. Claim:	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Bd%2Cx%29+%5Cge+f%28x%2B1%2Cx%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x+d,x) \ge f(x+1,x), " class="latex" title="\displaystyle  f(x+d,x) \ge f(x+1,x), " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \ge 1}" class="latex" title="{x \ge 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d \ge 1}" class="latex" title="{d \ge 1}" /> and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2B1%2Cx%29+%5Cge+f%28x%2Cx-1%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x+1,x) \ge f(x,x-1), " class="latex" title="\displaystyle  f(x+1,x) \ge f(x,x-1), " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \ge 2}" class="latex" title="{x \ge 2}" />. </p>
<p>
Note the first is really simple. Consider the first <img src="https://s0.wp.com/latex.php?latex=%7Bx%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x+1}" class="latex" title="{x+1}" /> pigeons. They are placed into <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> places and the inequality follows. The second is about the same. Consider the first <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> pigeons. There are two cases. They are all placed in <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" /> places. Then we are done. So there must have been some placed into the last place. But if two are there then we are also done. So <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" /> are placed into <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" />. But where does the last one go? In either case we are done.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>Are there areas that almost never use the PP? I would like to hear about areas that just do not use PP. </p>
<p></p><p><br />
[some word fixes]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/"><span class="datestr">at February 15, 2021 05:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/">Postdoc at National University of Singapore (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One post-doctoral position is available. The goal of the intended projects is to develop new frameworks and techniques for testing properties of functions and distributions on high-dimensional spaces.</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~arnab/testing-postdoc.html">https://www.comp.nus.edu.sg/~arnab/testing-postdoc.html</a><br />
Email: arnabb@nus.edu.sg</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/"><span class="datestr">at February 15, 2021 11:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021/">Postdoc at National University of Singapore (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two post-doctoral positions are available. The goal of the intended projects is to develop statistical and computational guarantees for algorithms performing causal inference.</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~arnab/causal-postdoc.html">https://www.comp.nus.edu.sg/~arnab/causal-postdoc.html</a><br />
Email: arnabb@nus.edu.sg</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021/"><span class="datestr">at February 15, 2021 11:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/014">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/014">TR21-014 |  Hitting Sets and Reconstruction for Dense Orbits in VP$_e$ and $\Sigma\Pi\Sigma$ Circuits | 

	Dori Medini, 

	Amir Shpilka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this paper we study polynomials in VP$_e$ (polynomial-sized formulas) and in $\Sigma\Pi\Sigma$ (polynomial-size depth-$3$ circuits) whose orbits, under the action of the affine group GL$^{aff}_n({\mathbb F})$, are dense in their ambient class. We construct hitting sets and interpolating sets for these orbits as well as give reconstruction algorithms.

As VP$=$VNC$^2$, our results for VP$_e$ translate immediately to VP with a quasipolynomial blow up in parameters.

If any of our hitting or interpolating sets could be made robust then this would immediately yield a hitting set for the superclass in which the relevant class is dense, and as a consequence also a lower bound for the superclass. Unfortunately, we also prove that the kind of constructions that we have found (which are defined in terms of k-independent polynomial maps) do not necessarily yield robust hitting sets.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/014"><span class="datestr">at February 15, 2021 08:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-699230831808345084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/two-examples-of-journalists-being-wrong.html">Two examples of Journalists being... Wrong. One BIG one small</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Journalists sometimes get things wrong.</p><p>This is not news, but it is interesting when you KNOW they are wrong. </p><p>1) Scott Aaronson has a GREAT example regarding an IMPORTANT story. I recommend you to read his blog post <a href="https://www.scottaaronson.com/blog/?p=5310">here</a>. Most of the comments are good also, though they go off on some tangents (e.g., is the Universal Basic Income a progressive idea?)</p><p><br /></p><p>2) I have my own example. It is far less important than the one Scott discusses; however, inspired by Scott, I will discuss it. My example also involves Scott, but that's a coincidence. </p><p>Quanta Magazine emailed me that they wanted to talk to me about an upcoming article on <i>The Busy</i> <i>Beaver Problem</i>. Why me? Because Scott's (same Scott as above!) survey/open problems column appeared in the SIGACT News Open Problem Column that I edit. </p><p>This sounded fine (Spoiler Alert: It was fine, the errors they made were odd, not harmful).</p><p>Here is the Quanta Article (though I do not know if it is behind paywalls- I can never tell if I am getting access because I have a UMCP account of or anyone can have access or if I am breaking copyright laws by posting the link):    <a href="https://www.quantamagazine.org/the-busy-beaver-game-illuminates-the-fundamental-limits-of-math-20201210/">here</a></p><p>Here is Scotts article: <a href="https://www.scottaaronson.com/papers/bb.pdf">here</a></p><p>The interviewer asked me </p><p>a) Why did I ask Scott to write the article?</p><p>ANSWER: He had a blog post on it, and I was skeptical of why these numbers are interesting, so I asked a question in the comments. He gave a great answer, so I asked him if he wanted to write a column for my open problems column. Actually I asked him if either he or perhaps a grad student would do it- I assumed he would be too busy since his `day job' is quantum  computing. However, much to my surprise and delight he said YES he would do it.</p><p>b) Is the Busy Beaver Function important?</p><p>ANSWER: In my opinion the actual numbers are not that important but its really neat that (a) we know some of them, and (b) they are far smaller than I would have thought. Also these numbers are interesting for the following reason:  there is some  n so that proving </p><p>BB(n)=whatever it equals</p><p> is Ind of Peano Arithmetic. When I hear that I think the number must be really large. Its not. Its 27. NEAT! And stronger theories are related to bigger numbers. This is a way to order theories. For  ZF they have something in the 700's- MUCH SMALLER than I would have thought. Scott and others can even relate BB to open problems in Math! </p><p>There were some other questions also, but I don't recall what they were. </p><p>SO when the article came they mentioned me once, and its... not quite wrong but odd:</p><p><i>William Gasarch, a computer science professor at the University of Maryland College Park,</i></p><p><i>said he's less intrigued by the prospect of pinning down the Busy Beaver numbers than by </i></p><p><i>``the general concept that its actually uncomputable.'' He and other mathematicians are mainly interested in using the yardstick for gauging the difficulty of important open problems in mathematics--or for figuring out what is mathematically knowable at all. </i></p><p><br /></p><p>The oddest thing about the paragraph is they do not mention my connection to Scott and the article he wrote! I reread the article looking for something like `<i>Scotts article appeared in the SIGACT News</i> <i>Open Problems column edited by William Gasarch' </i>Nothing of that sort appears. </p><p>Without that its not clear why they are soliciting my opinion. My colleague Clyde says this is GOOD:  people will ASSUME I am some sort of expert. Am I an expert? I proofread Scott's paper so... there is that...</p><p>Also I come off as more down on BB than I really am. </p><p>Did I claim that Mathematicians are more interested in using it as a yardstick. Actually I may have said something like that. I don't know if its true. That's my bad- I should have said that I am interested in that.</p><p>After the article came out I asked the interviewer why my role was not in the article. He said it was cut by the editor. </p><p>NOW- NONE of this is important, but even on small and easily correctable things, they get it wrong. So imagine what happens on hard issues that are harder to get right. </p><p><br /></p><p>MISC: One comment on Scott's blog was about the Gell-Mann amnesia effect, see this article on it:</p><p><a href="https://www.tefter.io/bookmarks/45454/readable">here</a><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/two-examples-of-journalists-being-wrong.html"><span class="datestr">at February 14, 2021 08:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/013">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/013">TR21-013 |  Positive spectrahedrons: Geometric properties, Invariance principles and Pseudorandom generators | 

	Penghui Yao, 

	Srinivasan Arunachalam</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In a recent work, O'Donnell, Servedio and Tan (STOC 2019) gave explicit pseudorandom generators (PRGs) for arbitrary $m$-facet polytopes in $n$ variables with seed length poly-logarithmic in $m,n$, concluding a sequence of works in the last decade, that was started by Diakonikolas,  Gopalan,  Jaiswal,  Servedio, Viola (SICOMP 2010) and Meka, Zuckerman (SICOMP 2013) for fooling linear and polynomial threshold functions, respectively. In this work, we consider a natural extension of  PRGs for intersections of positive spectrahedrons. A positive spectrahedron  is a Boolean function $f(x)=[x_1A^1+\cdots +x_nA^n \preceq B]$ where the $A^i$s are $k\times k$ positive semidefinite matrices. We construct explicit PRGs  that $\delta$-fool  "regular" width-$M$ positive spectrahedrons (i.e., when none of the $A^i$s are dominant) over the Boolean space  with seed length $poly(\log k,\log n, M, 1/\delta)$.
    

    Our main technical contributions are the following. We first prove an invariance principle for positive spectrahedrons via the well-known Lindeberg method. As far as we are aware such a  generalization of the Lindeberg method was unknown. Second, we prove various geometric properties of positive spectrahedrons such as their noise sensitivity, Gaussian surface area and a Littlewood-Offord theorem for positive spectrahedrons. Using these results, we give applications for constructing PRGs for positive spectrahedrons, learning theory, discrepancy sets for positive spectrahedrons (over the Boolean cube) and PRGs for intersections of structured polynomial threshold functions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/013"><span class="datestr">at February 14, 2021 02:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/012">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/012">TR21-012 |  On the Power and Limitations of Branch and Cut | 

	Noah Fleming, 

	Toniann Pitassi, 

	Li-Yang Tan, 

	Mika Göös, 

	Russell Impagliazzo, 

	Robert Robere, 

	Avi Wigderson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Stabbing Planes proof system was introduced to model the reasoning carried out in practical mixed integer programming solvers. As a proof system, it is powerful enough to simulate Cutting Planes and to refute the Tseitin formulas -- certain unsatisfiable systems of linear equations mod 2 -- which are canonical hard examples for many algebraic proof systems. In a recent (and surprising) result, Dadush and Tiwari showed that these short refutations of the Tseitin formulas could be translated into quasi-polynomial size and depth Cutting Planes proofs, refuting a long-standing conjecture. This translation raises several interesting questions. First, whether all Stabbing Planes proofs can be efficiently simulated by Cutting Planes. This would allow for the substantial analysis done on the Cutting Planes system to be lifted to practical mixed integer programming solvers. Second, whether the quasi-polynomial depth of these proofs is inherent to Cutting Planes. 

In this paper we make progress towards answering both of these questions. First, we show that any Stabbing Planes proof with bounded coefficients SP* can be translated into Cutting Planes. As a consequence of the known lower bounds for Cutting Planes, this establishes the first exponential lower bounds on SP*. Using this translation, we extend the result of Dadush and Tiwari to show that Cutting Planes has short refutations of any unsatisfiable system of linear equations over a finite field. Like the Cutting Planes proofs of Dadush and Tiwari, our refutations also incur a quasi-polynomial blow-up in depth, and we conjecture that this is inherent. As a step towards this conjecture, we develop a new geometric technique for proving lower bounds on the depth of Cutting Planes proofs. This allows us to establish the first lower bounds on the depth of Semantic Cutting Planes proofs of the Tseitin formulas.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/012"><span class="datestr">at February 14, 2021 02:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5310">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5310">A grand anticlimax: the New York Times on Scott Alexander</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Updates (Feb. 14, 2021):</span></strong>  <a href="https://astralcodexten.substack.com/p/statement-on-new-york-times-article">Scott Alexander Siskind responds here</a>.</p>



<p>Last night, it occurred to me that despite how disjointed it feels, the <em>New York Times</em> piece does have a central thesis: namely, that rationalism is a “gateway drug” to dangerous beliefs.  And that thesis is 100% correct—insofar as <em>once you teach people that they can think for themselves about issues of consequence, some of them might think bad things</em>.  It’s just that many of us judge the benefit worth the risk!</p>



<p>Happy Valentine’s Day everyone!</p>



<p></p><hr /><p></p>



<p>Back in June, <em>New York Times</em> technology reporter Cade Metz, who I’d previously known from his reporting on quantum computing, told me that he was writing a story about Scott Alexander, Slate Star Codex, and the rationalist community.  Given my position as someone who <em>knew</em> the rationalist community without ever really being <em>part</em> of it, Cade wondered whether I’d talk with him.  I said I’d be delighted to.</p>



<p>I spent many hours with Cade, taking his calls and emails morning or night, at the playground with my kids or wherever else I was, answering his questions, giving context for his other interviews, suggesting people in the rationalist community for him to talk to, in exactly the same way I might suggest colleagues for a quantum computing story.  And then I spent just as much time urging those people to talk to Cade.  (“How could you possibly not want to talk?  It’s the <em>New York Times</em>!”)  Some of the people I suggested agreed to talk; others refused; a few were livid at me for giving a <em>New York Times</em> reporter their email addresses without asking them.  (I apologized; lesson learned.)</p>



<p>What happened next is already the stuff of Internet history: the NYT’s threat to publish Scott’s real surname; Scott deleting his blog as a way to preempt that ‘doxing’; 8,000 people, including me, signing a <a href="https://www.dontdoxscottalexander.com/">petition</a> urging the NYT to respect Scott’s wish to keep his professional and blog identities separate; Scott resigning from his psychiatry clinic and starting his own low-cost practice, <a href="https://lorienpsych.com/">Lorien Psychiatry</a>; his moving his blog, like so many other writers this year, to <a href="https://substack.com/">Substack</a>; then, a few weeks ago, his <a href="https://astralcodexten.substack.com/">triumphant return</a> to blogging under his real name of Scott Siskind.  All this against the backdrop of an 8-month period that was world-changingly historic in so many other ways: the failed violent insurrection against the United States and the ouster, by democratic means, of the president who incited it; the tragedy of covid and the long-delayed start of the vaccination campaign; the BLM protests; the well-publicized upheavals at the NYT itself, including firings for ideological lapses that would’ve made little sense to our remote ancestors of ~2010.</p>



<p>And now, as an awkward coda, the <em>New York Times</em> article itself is <a href="https://www.nytimes.com/2021/02/13/technology/slate-star-codex-rationalists.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage">finally out</a> (<a href="https://www.deccanherald.com/business/technology/why-slate-star-codex-is-silicon-valley-s-safe-space-950727.html">non-paywalled version here</a>).</p>



<p>It could’ve been worse.  I doubt it will do lasting harm.  Of the many choices I disagreed with, I don’t know which were Cade’s and which his editors’.  But no, I was not happy with it.  If you want a feature-length, pop condensation of the rationalist community and its ideas, I preferred <a href="https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media">this summer’s <em>New Yorker</em> article</a> (but much better still is the <a href="https://www.amazon.com/Does-Not-Hate-You-Superintelligence-ebook/dp/B07K258VCV">book by Tom Chivers</a>).</p>



<p>The trouble with the NYT piece is not that it makes any false statements, but just that it constantly <em>insinuates</em> nefarious beliefs and motives, via strategic word choices and omission of relevant facts that change the emotional coloration of the facts that it <em>does</em> present.  I repeatedly muttered to myself, as I read: “dude, you could make <em>anything</em> sound shady with this exact same rhetorical toolkit!” </p>



<p>Without further ado, here’s a partial list of my issues:</p>



<ol><li>The piece includes the following ominous sentence: “But in late June of last year, when I approached Siskind to discuss the blog, it vanished.”  This framing, it seems to me, would be appropriate for some conman trying to evade accountability without ever explaining himself.  It doesn’t make much sense for a practicing psychiatrist who took the dramatic step of deleting his blog <em>in order to preserve his relationship with his patients</em>—thereby complying with an ethical code that’s universal among psychiatrists, even if slightly strange to the rest of us—and who immediately explained his reasoning to the entire world.  In the latter framing, of course, Scott comes across less like a fugitive on the run and more like an innocent victim of a newspaper’s editorial obstinacy.<br /></li><li>As expected, the piece devotes enormous space to the idea of rationalism as an on-ramp to alt-right extremism.  The trouble is, it never presents the idea that rationalism also can be an <em>off-ramp</em> from extremism—i.e., that it can provide a model for how even after you realize that mainstream sources are confidently wrong on some issue, you don’t respond by embracing conspiracy theories and hatreds, you respond by simply thinking carefully about each individual question rather than buying a worldview wholesale from anyone.  Nor does the NYT piece mention how Scott, precisely <em>because</em> he gives right-wing views more charity than some of us might feel they deserve, actually succeeded in dissuading some of his readers from voting for Trump—which is more success than I can probably claim in that department!  I had many conversations with Cade about these angles that are nowhere reflected in the piece.<br /></li><li>The piece gets off on a weird foot, by describing the rationalists as “a group that aimed to re-examine the world through cold and careful thought.”  Why “cold”?  Like, let’s back up a few steps: what is even the <em>connection</em> in the popular imagination between rationality and “coldness”?  To me, as to many others, the humor, humanity, and <em>warmth</em> of Scott’s writing were always among its most notable features.<br /></li><li>The piece makes liberal use of scare quotes.  Most amusingly, it puts scare quotes around the phrase “Bayesian reasoning”!<br /></li><li>The piece never mentions that many rationalists (Zvi Mowshowitz, Jacob Falkovich, Kelsey Piper…) were right about the risk of covid-19 in early 2020, and then <em>again</em> right about masks, aerosol transmission, faster-spreading variants, the need to get vaccines into arms faster, and many other subsidiary issues, even while public health authorities and the mainstream press struggled for months to reach the same obvious (at least in retrospect) conclusions.  This omission is significant because Cade told me, in June, that the rationalist community’s early rightness about covid was part of what <em>led him to want to write the piece in the first place</em> <em>(!)</em>.  If readers knew about that clear success, would it put a different spin on the rationalists’ weird, cultlike obsession with “Bayesian reasoning” and “consequentialist ethics” (whatever those are), or their nerdy, idiosyncratic worries about the more remote future?<br /></li><li>The piece contains the following striking sentence: “On the internet, many in Silicon Valley believe, everyone has the right not only to say what they want but to say it anonymously.”  Well, yes, except this framing makes it sound like this is a fringe belief of some radical Silicon Valley tribe, rather than just the standard expectation of most of the billions of people who’ve used the Internet for most of its half-century of existence.<br /></li><li>Despite thousands of words about the content of SSC, the piece never gives Scott a few uninterrupted sentences in his own voice, to convey his style.  This is something the <em>New Yorker</em> piece did do, and which would help readers better understand the wit, humor, charity, and self-doubt that made SSC so popular.  To see what I mean, read the NYT’s radically-abridged quotations from Scott’s now-classic riff on the Red, Blue, and Gray Tribes and decide for yourself whether they capture the <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">spirit of the original</a> (alright, I’ll quote the relevant passage myself at the bottom of this post).  Scott has the property, shared by many of my favorite writers, that if you just properly <em>quote</em> him, the words leap off the page, wriggling free from the grasp of any bracketing explanations and making a direct run for the reader’s brain.  All the more reason to <a href="https://howthehell.substack.com/p/nyt-ssc-quoting">quote him</a>!<br /></li><li>The piece describes SSC as “astoundingly verbose.”  A more neutral way to put it would be that Scott has <em>produced a vast quantity of intellectual output</em>.  When I finish a Scott Alexander piece, only in a minority of cases do I feel like he spent more words examining a problem than its complexities really warranted.  Just as often, I’m left wanting more.<br /></li><li>The piece says that Scott once “aligned himself” with Charles Murray, then goes on to note Murray’s explosive views about race and IQ.  That might be fair enough, were it <em>also</em> mentioned that the positions ascribed to Murray that Scott endorses in the <a href="https://web.archive.org/web/20200615030810/https://slatestarcodex.com/2016/05/23/three-great-articles-on-poverty-and-why-i-disagree-with-all-of-them/">relevant post</a>—namely, “hereditarian leftism” and <a href="https://en.wikipedia.org/wiki/Universal_basic_income">universal basic income</a>—are not only unrelated to race but are actually <em>progressive</em> positions.<br /></li><li>The piece says that Scott once had neoreactionary thinker <a href="https://en.wikipedia.org/wiki/Nick_Land">Nick Land</a> on his blogroll.  Again, important context is missing: this was back when Land was mainly known for his strange writings on AI and philosophy, <em>before</em> his neoreactionary turn.<br /></li><li>The piece says that Scott compared “some feminists” to Voldemort.  It didn’t explain what it took for certain specific feminists (like Amanda Marcotte) to prompt that comparison, which might have changed the coloration.  (Another thing that would’ve complicated the picture: the rationalist community’s legendary openness to alternative gender identities and sexualities, before such openness became mainstream.)<br /></li><li>Speaking of feminists—yeah, I’m a minor part of the article.  One of the few things mentioned about me is that I’ve stayed in a rationalist group house.  (If you must know: for like two nights, when I was in Bay Area, with my wife and kids.  We appreciated the hospitality!)  The piece also says that I was “turned off by the more rigid and contrarian beliefs of the Rationalists.”  It’s true that I’ve <em>disagreed</em> with many beliefs espoused by rationalists, but not <em>because</em> they were contrarian, or because I found them noticeably more “rigid” than most beliefs—only because I thought they were mistaken!<br /></li><li>The piece describes Eliezer Yudkowsky as a “polemicist and self-described AI researcher.”  It’s true that Eliezer opines about AI despite a lack of conventional credentials in that field, and it’s also true that the typical NYT reader might find him to be comically self-aggrandizing.  But had the piece mentioned the universally recognized AI experts, like Stuart Russell, who credit Yudkowsky for a central role in the AI safety movement, wouldn’t that have changed what readers perceived as the take-home message?<br /></li><li>The piece says the following about Shane Legg and Demis Hassabis, the founders of DeepMind: “Like the Rationalists, they believed that AI could end up turning against humanity, and because they held this belief, they felt they were among the only ones who were prepared to build it in a safe way.”  This strikes me as a brilliant way to reframe a concern around AI safety as something vaguely sinister.  Imagine if the following framing had been chosen instead: “Amid Silicon Valley’s mad rush to invest in AI, here are the voices urging that it be done safely and in accord with human welfare…”</li></ol>



<p>Reading this article, some will say that they told me so, or even that I was played for a fool.  And yet I confess that, even with hindsight, I have no idea what I should have done differently, how it would’ve improved the outcome, or what I <em>will</em> do differently the next time.  Was there some better, savvier way for me to help out?  For each of the 14 points listed above, were I ever tempted to bang my head and say, “dammit, I wish I’d told Cade X, so his story could’ve reflected that perspective”—well, the truth of the matter is that I <em>did</em> tell him X!  It’s just that I don’t get to decide which X’s make the final cut, or which ideological filter they’re passed through first.</p>



<p>On reflection, then, I’ll continue to talk to journalists, whenever I have time, whenever I think I might know something that might improve their story.  I’ll continue to rank bend-over-backwards openness and honesty among my most fundamental values.  <s>Hell, I’d even talk to Cade for a future story, assuming he’ll talk to me after all the disagreements I’ve aired here!</s>  [<strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> commenters’ counterarguments caused me to change my stance on this; <a href="https://www.scottaaronson.com/blog/?p=5330">see here</a>.]</p>



<p>For one thing that became apparent from this saga is that I <em>do</em> have a deep difference with the rationalists, one that will likely prevent me from ever truly joining them.  Yes, there might be true and important things that one can’t say without risking one’s livelihood.  At least, there were in every <em>other</em> time and culture, so it would be shocking if Western culture circa 2021 were the lone exception.  But unlike the rationalists, I don’t feel the urge to form walled gardens in which to say those things anyway.  I simply accept that, in the age of instantaneous communication, <em>there are no walled gardens</em>: anything you say to a dozen or more people, you might as well broadcast to the planet.  Sure, we all have things we say only in the privacy of our homes or to a few friends—a privilege that I expect even the most orthodox would like to preserve, at any rate for themselves.  Beyond that, though, my impulse has always been to look for non-obvious truths that <em>can</em> be shared openly, and that might light little candles of understanding in one or two minds—and then to shout those truths from the rooftops under my own name, and learn what I can from whatever sounds come in reply.</p>



<p>So I’m thrilled that Scott Alexander Siskind has now rearranged his life to have the same privilege.  Whatever its intentions, I hope today’s <em>New York Times</em> article draws tens of thousands of curious new readers to Scott’s new-yet-old blog, <a href="https://astralcodexten.substack.com/">Astral Codex Ten</a>, so they can see for themselves what I and so many others saw in it.  I hope Scott continues blogging for decades.  And whatever obscene amount of money Substack is now paying Scott, I hope they’ll soon be paying him even more.</p>



<p></p><hr /><p></p>



<p>Alright, now for the promised quote, from <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">I Can Tolerate Anything Except the Outgroup</a>.</p>



<blockquote class="wp-block-quote"><p>The Red Tribe is most classically typified by conservative political beliefs, strong evangelical religious beliefs, creationism, opposing gay marriage, owning guns, eating steak, drinking Coca-Cola, driving SUVs, watching lots of TV, enjoying American football, getting conspicuously upset about terrorists and commies, marrying early, divorcing early, shouting “USA IS NUMBER ONE!!!”, and listening to country music.</p><p>The Blue Tribe is most classically typified by liberal political beliefs, vague agnosticism, supporting gay rights, thinking guns are barbaric, eating arugula, drinking fancy bottled water, driving Priuses, reading lots of books, being highly educated, mocking American football, feeling vaguely like they should like soccer but never really being able to get into it, getting conspicuously upset about sexists and bigots, marrying later, constantly pointing out how much more civilized European countries are than America, and listening to “everything except country”.</p><p>(There is a partly-formed attempt to spin off a Grey Tribe typified by libertarian political beliefs, Dawkins-style atheism, vague annoyance that the question of gay rights even comes up, eating paleo, drinking Soylent, calling in rides on Uber, reading lots of blogs, calling American football “sportsball”, getting conspicuously upset about the War on Drugs and the NSA, and listening to filk – but for our current purposes this is a distraction and they can safely be considered part of the Blue Tribe most of the time)</p><p>… Even in something as seemingly politically uncharged as going to California Pizza Kitchen or Sushi House for dinner, I’m restricting myself to the set of people who like cute artisanal pizzas or sophsticated foreign foods, which are classically Blue Tribe characteristics.</p></blockquote></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5310"><span class="datestr">at February 13, 2021 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/011">TR21-011 |  Classification of the streaming approximability of Boolean CSPs | 

	Santhoshini Velusamy, 

	Chi-Ning  Chou, 

	Madhu Sudan, 

	Alexander Golovnev</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A Boolean constraint satisfaction problem (CSP), Max-CSP$(f)$, is a maximization problem specified by a constraint $f:\{-1,1\}^k\to\{0,1\}$. An instance of the problem consists of $m$ constraint applications on $n$ Boolean variables, where each constraint application applies the constraint to $k$ literals chosen from the $n$ variables and their negations. The goal is to compute the maximum number of constraints that can be satisfied by a Boolean assignment to the $n$ variables. In the $(\gamma,\beta)$-approximation version of the problem for parameters $\gamma \geq \beta \in [0,1]$, the goal is to distinguish instances where at least $\gamma$ fraction of the constraints can be satisfied from instances where at most $\beta$ fraction of the constraints can be satisfied. 

In this work we completely characterize the approximability of all Boolean CSPs in the streaming model. Specifically, given $f$, $\gamma$ and $\beta$ we show that either (1) the $(\gamma,\beta)$-approximation version of Max-CSP$(f)$ has a probabilistic streaming algorithm using $O(\log n)$ space, or (2) for every $\epsilon &gt; 0$ the $(\gamma-\epsilon,\beta+\epsilon)$-approximation version of Max-CSP$(f)$ requires $\Omega(\sqrt{n})$ space for probabilistic streaming algorithms. Previously such a separation was known only for $k=2$. We stress that for $k=2$, there are only finitely many distinct problems to consider.

Our positive results show wider applicability of bias-based algorithms used previously by [Guruswami-Velingker-Velusamy APPROX'17], [Chou-Golovnev-Velusamy FOCS'20] by giving a systematic way to explore biases. Our negative results combine the Fourier analytic methods of [Kapralov-Khanna-Sudan SODA'15], which we extend to a wider class of CSPs, with a rich collection of reductions among communication complexity problems that lie at the heart of the negative results.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/011"><span class="datestr">at February 13, 2021 06:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/12/postdoc-in-algorithms-and-complexity-at-university-of-oxford-apply-by-march-26-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/12/postdoc-in-algorithms-and-complexity-at-university-of-oxford-apply-by-march-26-2021/">postdoc in algorithms and complexity  at University of Oxford (apply by March 26, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>4-year postdoctoral position working with Leslie Ann Goldberg from 1 Oct 2021.</p>
<p>Website: <a href="https://my.corehr.com/pls/uoxrecruit/erq_jobspec_details_form.jobspec?p_id=149618">https://my.corehr.com/pls/uoxrecruit/erq_jobspec_details_form.jobspec?p_id=149618</a><br />
Email: leslie.goldberg@cs.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/12/postdoc-in-algorithms-and-complexity-at-university-of-oxford-apply-by-march-26-2021/"><span class="datestr">at February 12, 2021 11:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=838">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/02/11/submitting-to-icalp-2021/">Submitting to ICALP 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am taking advantage of the pandemic to participate in conferences where it’s usually hard for me to participate because they require intercontinental travel.  I have a paper in CSR 2021, and now am submitting a paper to ICALP 2021 (I count submission as participation, even in case the paper gets rejected).  The latter requires submissions to be formatted in a specific way, <a href="https://emanueleviola.wordpress.com/tag/utopia-tcs/">a topic discussed at length on this blog</a>.</p>



<p>Begin 12:48</p>



<p>Download the LIPICs package.</p>



<p>Try to compile their sample paper.</p>



<p>Get error message:  ! LaTeX Error: File `l3backend-dvips.def’ not found.</p>



<p>Google solution.  Says to install packages.</p>



<p>Unfortunately, I am using windows but I only have LyX, and the solution expects MixTeX.</p>



<p>Google how to install packages in LyX.</p>



<p>Can’t find anything simple.</p>



<p>Create a new document on overleaf.</p>



<p>Copy all the LIPIcs files there.</p>



<p>Try to compile their sample.</p>



<p>It works!</p>



<p>Paste my latex.</p>



<p>Usual avalanche of problems to be fixed at the speed of light.</p>



<p>Add dummy section “Introduction” which wasn’t in my paper, otherwise theorem numbers look weird.</p>



<p>Numbers still look weird.  Something’s wrong with theorem statements.</p>



<p>Replace {thm} with {theorem}</p>



<p>Looks better.  Still some wrong stuff all around, however.</p>



<p>No it wasn’t that.  Remove the dummy section.  It seems their “paragraph” environment puts strange numbers like 0.0.0.1</p>



<p>Replace \paragaph with \paragaph* everywhere</p>



<p>Actually, looks weird the way they put the ack– put back the dummy Introduction section.</p>



<p>Check page limit: <strong>no more than 12 pages, excluding references</strong></p>



<p>I’m a little over.  Does this really matter?  Apparently, <a href="https://emanueleviola.wordpress.com/2014/09/30/eliminate-all-formatting-requirements-survival-tip/">it does!</a>  Move last proof to the appendix.  Actually, last proof is kind of short, I should move the penultimate proof. Update paper organization (next time I shouldn’t put it).</p>



<p>Final look.  Fix a few indentations.</p>



<p>OK, time to actually submit.  Go to the easychair website.  They want me to re-enter all the information!?  Why, after forcing me to enter title, keywords, etc. in <em>their </em>format, are they asking me to do this again?  Can’t we just send the .tex file and extract it from there?</p>



<p>Oh come one, it’s just a few seconds of copy-paste.</p>



<p>OK, done, paper submitted.</p>



<p>End: 3:05</p>



<p>Well, next time it will be easier.  Perhaps <em>easier</em>, but not <em>easy</em> because as the reader knows there will be another missing package, another incompatible system, etc.  And of course, if the paper is rejected, then I won’t even save the time to convert it into camera-ready format.  On the other hand, the benefit is non-existent.  It would be better for everyone if in order to submit a paper you have to complete a random 1-hour task on Amazon mechanical Turk and donate the profit to charity.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/02/11/submitting-to-icalp-2021/"><span class="datestr">at February 11, 2021 01:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/010">TR21-010 |  Cryptographic Hardness under Projections for Time-Bounded Kolmogorov Complexity | 

	Eric Allender, 

	John Gouwar, 

	Shuichi Hirahara, 

	Caleb Robelle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A version of time-bounded Kolmogorov complexity, denoted KT, has received attention in the past several years, due to its close connection to circuit complexity and to the Minimum Circuit Size Problem MCSP. Essentially all results about the complexity of MCSP hold also for MKTP (the problem of computing the KT complexity of a string). Both MKTP and MCSP are hard for SZK (Statistical Zero Knowledge) under BPP-Turing reductions; neither is known to be NP-complete. Recently, some hardness results for MKTP were proved that are not (yet) known to hold for MCSP. In particular, MKTP is hard for DET (a subclass of P) under nonuniform NC^0 m-reductions.

In this paper, we improve this, to show that MKTP is hard for the (apparently larger) class NISZK_L under not only NC^0 m-reductions but even under projections. Also MKTP is hard for NISZK under P/poly m-reductions. Here, NISZK is the class of problems with non-interactive zero-knowledge proofs, and NISZK_L is the non-interactive version of the class SZK_L that was studied by Dvir et al.

As an application, we provide several improved worst-case to average-case reductions to problems in NP.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/010"><span class="datestr">at February 11, 2021 12:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
