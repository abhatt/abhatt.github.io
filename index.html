<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 09, 2021 05:40 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/079">TR21-079 |  The zero-rate threshold for adversarial bit-deletions is less than 1/2 | 

	Venkatesan Guruswami, 

	Xiaoyu He, 

	Ray Li</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that there exists an absolute constant $\delta&gt;0$ such any binary code $C\subset\{0,1\}^N$ tolerating $(1/2-\delta)N$ adversarial deletions must satisfy $|C|\le 2^{\poly\log N}$ and thus have rate asymptotically approaching $0$. This is the first constant fraction improvement over the trivial bound that codes tolerating $N/2$ adversarial deletions must have rate going to $0$ asymptotically.  Equivalently, we show that there exists absolute constants $A$ and $\delta&gt;0$ such that any set $C\subset\{0,1\}^N$ of $2^{\log^A N}$ binary strings must contain two strings $c$ and $c'$ whose longest common subsequence has length at least $(1/2+\delta)N$. As an immediate corollary, we show that $q$-ary codes tolerating a fraction $1-(1+2\delta)/q$ of adversarial deletions must also have rate approaching $0$.
 
Our techniques include string regularity arguments and a structural lemma that classifies binary strings by their oscillation patterns.  Leveraging these tools, we find in any large code two strings with similar oscillation patterns, which is exploited to find a long common subsequence.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/079"><span class="datestr">at June 09, 2021 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21799">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/06/09/to-cheer-you-up-in-difficult-times-26-two-real-life-lectures-yesterday-at-the-technion/">To cheer you up in difficult times 26: Two real-life lectures yesterday at the Technion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>After 16 months without lecturing to an audience in my same location, I gave yesterday two lectures at the Technion in front of a live audience (and some additional audience in remote locations). The main lecture was in <a href="https://comsoc2021.net.technion.ac.il/">COMSOC 2021,</a> an international conference on computational social choice,  and earlier I gave a guest lecture in Roy Meshulam’s class about simple polytopes. I also met many friends. </p>
<p><a href="https://reshef.net.technion.ac.il/">Reshef Meir</a> who organized (with Bill Zwicker) COMSOC 2021 wrote:</p>
<blockquote>
<div><span style="color: #993366;"><em>Hi all, </em></span></div>
<div><span style="color: #993366;"><em>today was beyond expectations – the first feeling of a real actual conference after almost a year and a half!  We had about 40 people attending, viewing posters, and listening to talks. I truly hope this will return to be a common scene and that we can all meet face to face soon.</em></span></div>
</blockquote>
<div> </div>
<p>In my COMSOC lecture I talked about some earlier ideas and results in my work on social choice, starting with my paper with Ariel Rubinstein and Rani Spiegler on rationalizing individual choice by multiple rationals, and my subsequent attempt to use learnability as a tool for understanding choices of economic agents. This led to interesting questions on social choice <a href="https://gilkalai.wordpress.com/2009/06/02/social-choice-preview/">that are discussed in this 2009 post.</a></p>
<p>In Roy’s course I explained <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="h" class="latex" />-vectors of polytopes and the Dehn-Sommerville relations based on counting outdegrees of the graph of the polytope when we direct its edges based on a generic abstract objective function. I moved on to present a proof of Blind-Mani’s theorem that the graph of the polytope determines the full combinatorics. This proof is probably the one proof I presented the most and it is given in <a href="https://gilkalai.wordpress.com/2009/01/16/telling-a-simple-polytope-from-its-graph/">this 2009 post</a>.</p>
<p><img width="420" alt="sc1" src="https://gilkalai.files.wordpress.com/2021/06/sc1.png" class="alignnone size-full wp-image-21802" height="391" /></p>
<p><span style="color: #ff0000;">In my  COMSOC lecture I described how to fill the two question marks in the table above.</span></p>
<p><img width="420" alt="sc2" src="https://gilkalai.files.wordpress.com/2021/06/sc2.png" class="alignnone size-full wp-image-21803" height="391" /></p>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/06/09/to-cheer-you-up-in-difficult-times-26-two-real-life-lectures-yesterday-at-the-technion/"><span class="datestr">at June 09, 2021 06:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1526">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1526">News for May 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We hope you are all staying safe. With massive vaccination programs across the globe we hope you and your loved ones are getting back to what used to be normal. With that out of the way, let us circle back to Property Testing. This month was less sleepy as compared to the two preceding months and we saw six papers in total (two of them explore problems in quantum property testing). Without further ado, let us take a deeper dive.</p>



<p></p>



<p><strong>GSF-locality is not sufficient for proximity-oblivious testing</strong>, by Isolde Adler, Noleen Kohler, Pan Peng (<a href="https://arxiv.org/abs/2105.08490">arXiv</a>) The notion of proximity oblivious testers was made explicit in the seminal work of Goldreich and Ron in 2009 [GR09]. A proximity oblivious tester for a graph property is a constant query tester that rejects a graph with probability that monotonically increases with distance to the property. A property is called proximity oblivious testable (or PO testable) if there exists a one sided tester for the property which makes a constant number of queries to the graph and rejects graphs that are far from the property with probability at least \(2/3\). [GR09] gave a characterization of which properties \(\Pi\) are PO testable in the bounded degree model <em>if and only if</em> it is a “local” property of some kind which satisfies a certain non propagation condition. [GR09] conjectured that all such “local” properties satisfy this non propagation condition. This paper refutes the above conjecture from [GR09].</p>



<p></p>



<p>Coming up next. More action on triangle freeness.</p>



<p><strong>Testing Triangle Freeness in the General Model in Graphs with Arboricity \(O(\sqrt n)\)</strong>, by Reut Levi (<a href="https://arxiv.org/abs/2105.04809">arXiv</a>) PTReview readers are likely to be aware that triangle freeness has been a rich source of problems for developing new sublinear time algorithms. This paper considers the classic problem of testing triangle freeness in general graphs. In the dense case, algorithms with running time depending only on \(\varepsilon\) are known thanks to the work of Alon, Fischer, Krivelevich and Szegedy. In the bounded degree case, Goldreich and Ron gave testers with query complexity \(O(1/\varepsilon)\). This paper explores the problem in general graph case and proves an upper bound of \(O(\Gamma/d_{avg} + \Gamma)\) where \(\Gamma\) is the arboricity of the graph. The author also shows that this upperbound is tight for graphs with arboricity at most \(O(\sqrt n)\). Curiously enough, the algorithm does not take arboricity of the graph as an input and yet \(\Gamma\) (the arboricity) shows up in the upper and lower bounds.</p>



<p></p>



<p><strong>Testing Dynamic Environments: Back to Basics</strong>, by Yonatan Nakar and Dana Ron (<a href="https://arxiv.org/abs/2105.00759">arXiv</a>) Goldreich and Ron introduced the problem of testing “dynamic environments” in 2014. Here is the setup for this problem. You are given an environment that evolves according to a local rule.  Your goal is to query some of the states in the system at some point of time and determine if the system is evolving according to some fixed rule or is far from it. In this paper, the authors consider environments defined by elementary cellular automata which evolve according to threshold rules as one of the first steps towards understanding what makes a dynamic environment tested efficiently.  The main result proves the following: if your local rules satisfy some <em>conditions</em>, you can use a meta algorithm with query complexity \(poly(1/\varepsilon)\) which is non adaptive and has one sided error. And all the threshold rules indeed satisfy these <em>conditions</em> which means they can be tested efficiently. </p>



<p></p>



<p><strong>Identity testing under label mismatch</strong>, by Clement Canonne and Karl Wimmer (<a href="https://arxiv.org/abs/2105.01856">arXiv</a>) This paper considers a classic problem distribution testing with the following twist. Let \(q\) denote a distribution supported on \([n]\). You are given access to samples from another distribution \(p\) where \(p  = q \circ \pi\) where \(\pi\) is some unknown permutation. Thus, I relabel the data and I give you access to samples from the relabeled dataset. Under this promise, note that identity testing becomes a trivial problem if \(q\) is known to be uniform over \([n]\). The authors develop algorithms for testing and tolerant testing of distributions under this additional promise of \(p\) being a permutation of some known distribution \(q\). The main result shows as exponential gap between the sample complexity of testing and tolerant testing under this promise. In particular, identity testing under the promise of permutation has sample complexity \(\Theta(\log^2 n)\) whereas tolerant identity testing under this promise has sample complexity \(\Theta(n^{1-o(1)})\).</p>



<p></p>



<p><strong>Testing symmetry on quantum computers</strong>, by Margarite L. LaBorde and Mark M. Wilde (<a href="https://arxiv.org/abs/2105.12758">arXiv</a>) This paper develops algorithms which test symmetries of a quantum states and changes generated by quantum circuits. These tests additionally also quantify how symmetric these states (or channels) are. For testing what are called “Bose states” the paper presents efficient algorithms. The tests for other kinds of symmetry presented in the paper rely on some aid from a quantum prover.</p>



<p></p>



<p><strong>Quantum proofs of proximity</strong>, by Marcel Dall’Agnol, Tom Gur, Subhayan Roy Moulik, Justin Thaler (<a href="https://eccc.weizmann.ac.il/report/2021/068/">ECCC</a>) The sublinear time (quantum) computation model has been gathering momentum steadily over the past several years. This paper seeks to understand the power of \({\sf QMA}\) proofs of proximity for property testing (recall \({\sf QMA}\) is the quantum analogue of \({\sf NP}\)). On the algorithmic front, the paper develops sufficient conditions for properties to admit efficient \({\sf QMA}\) proofs of proximity. On the complexity front, the paper demonstrates a property which admits  an efficient \({\sf QMA}\) proof but does not admit a \({\sf MA}\) or an interactive proof of proximity.</p></div>







<p class="date">
by Akash <a href="https://ptreview.sublinear.info/?p=1526"><span class="datestr">at June 09, 2021 05:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5539">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5539">More quantum computing popularization!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I now have a feature article up at <em>Quanta</em> magazine, entitled <a href="https://www.quantamagazine.org/why-is-quantum-computing-so-hard-to-explain-20210608/">“What Makes Quantum Computing So Hard To Explain?”</a>  I.e., why do journalists, investors, etc. so consistently get central points wrong, even after the subject has been in public consciousness for more than 25 years?  Perhaps unsurprisingly, I found it hard to discuss that meta-level question, as <em>Quanta</em>‘s editors asked me to do, without also engaging in the object-level task of actually explaining QC.  For regular <em>Shtetl-Optimized</em> readers, there will be nothing new here, but I’m happy with how the piece turned out.</p>



<p>Accompanying the <em>Quanta</em> piece is a <a href="https://www.youtube.com/watch?v=jHoEjvuPoB8&amp;t=6s">10-minute YouTube explainer on quantum computing</a>, which (besides snazzy graphics) features interviews with me, John Preskill, and Dorit Aharonov.</p>



<p>On a different note, my colleague <a href="https://www.markwilde.com/">Mark Wilde</a> has recorded a <a href="https://soundcloud.com/mark-m-wilde/quantum-computer">punk-rock song about BosonSampling</a>.  I can honestly report that it’s some of the finest boson-themed music I’ve heard in years.  It includes the following lyrics:</p>



<blockquote class="wp-block-quote"><p>Quantum computer, Ain’t no loser<br />Quantum computer, Quantum computer</p><p>People out on the streets<br />They don’t know what it is<br />They think it finds the cliques<br />Or finds graph colorings<br />But it don’t solve anything<br />Said it don’t solve anything<br />Bosonic slot machine<br />My lil’ photonic dream</p></blockquote>



<p>Speaking of BosonSampling, A. S. Popova and A. N. Rubtsov, of the Skolkovo Institute in Moscow, have a new preprint entitled <a href="https://arxiv.org/abs/2106.01445">Cracking the Quantum Advantage threshold for Gaussian Boson Sampling</a>.  In it, they claim to give an efficient classical algorithm to simulate noisy GBS experiments, like the <a href="https://www.scottaaronson.com/blog/?p=5159">one six months ago</a> from USTC in China.  I’m still unsure how well this scales from 30-40 photons up to 50-70 photons; which imperfections of the USTC experiment are primarily being taken advantage of (photon losses?); and how this relates to the earlier proposed classical algorithms for simulating noisy BosonSampling, like the one by <a href="https://arxiv.org/abs/1409.3093">Kalai and Kindler</a>.  Anyone with any insight is welcome to share!</p>



<p>OK, one last announcement: the Simons Institute for the Theory of Computing, in Berkeley, has a new online lecture series called <a href="https://simons.berkeley.edu/news/institute-launches-breakthroughs-lecture-series">“Breakthroughs,”</a> which many readers of this blog might want to check out.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5539"><span class="datestr">at June 08, 2021 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-7554871212591440842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/06/machlne-learning-for-algorithms.html">Machine Learning for Algorithms Workshop (July 13-14)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>We're having an online workshop on "Machine Learning for Algorithms" on July 13-14, with a great group of speakers.  Announcement below, link at <a href="https://fodsi.us/ml4a.html">https://fodsi.us/ml4a.html</a>, free registration (but please register in advance)!</p><div style="font-size: small;" class="gmail_default">In recent years there has been increasing interest in using machine learning to improve the performance of classical algorithms in computer science, by fine-tuning their behavior to adapt to the properties of the input distribution. This "data-driven" or "learning-based" approach to algorithm design has the potential to significantly improve the efficiency of some of the most widely used algorithms. For example, they have been used to design better data structures, online algorithms, streaming and sketching algorithms, market mechanisms and algorithms for combinatorial optimization, similarity search and inverse problems.  This virtual workshop will feature talks from experts at the forefront of this exciting area.<br /><br />The workshop is organized by Foundations of Data Science Institute (FODSI), a project supported by the NSF TRIPODS program (see fodsi.us). To attend, please register at    <br /> <br /><a href="https://fodsi.us/ml4a.html">https://fodsi.us/ml4a.html</a>  </div></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/06/machlne-learning-for-algorithms.html"><span class="datestr">at June 08, 2021 07:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/078">TR21-078 |  A direct product theorem for quantum communication complexity with applications to device-independent QKD | 

	Rahul  Jain, 

	Srijita Kundu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a direct product theorem for the entanglement-assisted interactive quantum communication complexity of an $l$-player predicate $V$. In particular we show that for a distribution $p$ that is product across the input sets of the $l$ players, the success probability of any entanglement-assisted quantum communication protocol for computing $n$ copies of $V$, whose communication is $o(\log(\mathrm{eff}^*(V,p))\cdot n)$, goes down exponentially in $n$. Here $\mathrm{eff}^*(V, p)$ is a distributional version of the quantum efficiency or partition bound introduced by Laplante, Lerays and Roland (2014), which is a lower bound on the distributional quantum communication complexity of computing a single copy of $V$ with respect to $p$.
  As an application of our result, we show that it is possible to do device-independent quantum key distribution (DIQKD) without the assumption that devices do not leak any information after inputs are provided to them. We analyze the DIQKD protocol given by Jain, Miller and Shi (2017), and show that when the protocol is carried out with devices that are compatible with $n$ copies of the Magic Square game, it is possible to extract $\Omega(n)$ bits of key from it, even in the presence of $O(n)$ bits of leakage. Our security proof is parallel, i.e., the honest parties can enter all their inputs into their devices at once, and works for a leakage model that is arbitrarily interactive, i.e., the devices of the honest parties Alice and Bob can exchange information with each other and with the eavesdropper Eve in any number of rounds, as long as the total number of bits or qubits communicated is bounded.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/078"><span class="datestr">at June 08, 2021 12:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/3db-light/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/3db-light/">3DB: A Framework for Debugging Vision Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%; margin-bottom: 0;" href="https://arxiv.org/abs/2106.03805" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%; margin-bottom: 0;" href="https://github.com/3db/3db/" class="bbutton">
<i class="fab fa-github"></i>
   Project Repo
</a>
<a style="float: left; width: 45%;" href="https://3db.github.io/3db/usage/quickstart.html" class="bbutton">
<i class="fas fa-file-alt"></i>
   Documentation and Guides
</a> 
<a style="float: left; width: 45%;" href="https://github.com/3db/blog_demo/" class="bbutton">
<i class="fab fa-github"></i>
   Blog Demo
</a> 
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, in collaboration with Microsoft Research, we introduce
3DB: an extendable, unified framework for debugging and analyzing vision models
using photorealistic simulation. We’re releasing 3DB as a package, accompanied
by extensive API documentation, guides, and demos.</i></p>

<p><em>Note: You are now viewing the Javascript-free/lightweight version of this
post—to see the full version (with interactive plots, diagrams, and models!),
click <a href="https://gradientscience.org/3db/">here</a></em></p>

<p>Identifying failure modes and biases in vision models is a rapidly
emerging challenge in machine learning. In high-stakes
applications, simply deploying models and collecting failures that arise in
the wild is often difficult, expensive, and irresponsible. To this end, a
recent line of work in vision focuses on identifying model failure
modes via in-depth analyses of <a href="https://arxiv.org/abs/1712.02779">image transformations</a> and 
<a href="https://arxiv.org/abs/1903.12261">corruptions</a>, <a href="https://objectnet.dev">object orientations</a>,
<a href="https://gradientscience.org/background/">backgrounds</a>, or <a href="https://arxiv.org/abs/1811.12231v2">shape-texture conflicts</a>. These studies 
(and other similarly important ones) reveal a variety of patterns of
performance degradation in vision models. Still, performing each such study
requires time, developing (often complex)
toolingFor <a href="https://gradientscience.org/background/">our study of image backgrounds</a>, for example, we used a
combination of bounding boxes and classical computer vision tools to crop out
image backgrounds. We then had to manually filter out the images for which
the tools failed. Even for the images where the toolkit succeeded, there
remained inevitable cropping artifacts., and a willingness to settle for less than perfect simulations of each
potential failure mode. Our question is: can we support reliable discovery of model failures in a systematic,
automated, and unified way?</p>

<h2 id="3db-a-rendering-based-debugging-platform">3DB: A Rendering-based Debugging Platform</h2>

<p><img src="https://gradientscience.org/assets/3db/3db_headline.png" alt="A sampling of the analyses enabled by 3DB." class="bigimg" /></p>
<div style="margin-top: 0;" class="caption">
A sampling of the analyses enabled by 3DB.
</div>

<p>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, we try to make progress on this question and propose
3DB, a platform for automatically identifying and analyzing the failure modes
of computer vision models using 3D rendering. 3DB aims to allow users to go
from a testable, robustness-based hypothesis to concrete, photorealistic
experimental evidence with minimal time and effort.</p>

<p>The platform revolves around the modular workflow pictured below. First,
users specify a set of 3D objects and environments, as well as a set of 3D
(or 2D) transformations called controls that determine the space of
admissible object-environment configurations. 3DB then renders a myriad of
admissible scenes and feeds them through the user’s computer vision model of
choice. The user can finally stratify, aggregate, or otherwise analyze the
results either by reading the outputted JSON, or through the pre-packaged
dashboard.</p>

<p><img src="https://gradientscience.org/assets/3db/workflow.png" alt="An illustration of the 3DB workflow." class="bigimg" /></p>

<p>3DB easily adapts to a variety of use cases: in particular, users can
modify and swap out any part of this pipeline (e.g., the renderer, the
logger, the model type, or the controls) for their own custom-written
components, without needing to modify any of the 3DB codebase. We’ve compiled <a href="https://3db.github.io/3db/">guides</a>,
extensive <a href="https://3db.github.io/3db/api_doc.html">API documentation</a>, and a
<a href="https://github.com/3db/demo">full demo</a> showing how 3DB streamlines model debugging.</p>

<p><strong>In fact, this blog post will double as another demo! We’ll present the (short) code
necessary to reproduce every plot in the post below using 3DB. You can download 
the aggregated code for this blog post <a href="https://github.com/3db/blog_demo">here</a>.</strong></p>

<p><em>To set up, follow the steps below—then, in the remainder of this post, press
“Show/hide code and instructions” to see the steps necessary to reproduce each
experiment below.</em></p>
<div class="code-container">
  <input id="general-tab1" type="radio" checked="" class="tab1" name="general-tab" />
  <label for="general-tab1"><i class="fa fa-gear"></i>  Setup</label>
  <input id="general-tab2" type="radio" class="tab2" name="general-tab" />
  <label for="general-tab2"><i class="fa fa-code"></i>  Config (base.yaml)</label>
  <div class="line"></div>
  <div class="content-container">
<div class="content c1">

<ol class="instructions">
<li>Clone the  <a href="https://github.com/3db/blog_demo">blog demo <i class="fab fa-github"></i></a> repo</li>
<li>Run <pre>cd blog_demo</pre>, then <pre>bash setup.sh</pre> (assumes
<pre>unzip</pre> is installed) to download a large Blender environment, then
<pre>cd ../</pre></li>
<li>Install 3DB: <pre>curl -L https://git.io/Js8eT | bash /dev/stdin threedb</pre></li>
<li>Run <pre>conda activate threedb</pre></li>
<li>Our experiments below will need a <pre>BLENDER_DATA</pre> folder that contains two
subfolders: <pre>blender_models/</pre> containing 3D models (<pre>.blend</pre> files with a single
object whose name matches the filename), and <pre>blender_environments/</pre>
containing environments. We will provide you with these later</li> 
<li>Separately, make a file called <pre>base.yaml</pre> and paste in the configuration from the next pane.</li>
</ol>

</div>
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models'
  label_map: 'blog_demo/resources/imagenet_mapping.json'
  class: 'resnet18'
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.classification'
  args:
    classmap_path: 'blog_demo/resources/ycb_to_IN.json'
    topk: 1
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
policy:
  module: "threedb.policies.random_search"
  samples: 5
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"

</code></pre></div>
 </div>
</div>

<h2 id="using-3db">Using 3DB</h2>

<p>Prior works have already used 3D rendering (to great effect) to study biases
of machine learning models, including pose and context-based biases. Our goal
is not to propose a specific 3D-rendering based analysis, but
rather to provide an easy-to-use, highly extendable framework that unifies
prior analyses (both 3D and 2D) while enabling users to (a) conduct a host of
new analyses with the same ease and with realistic results; and (b)
effortlessly <em>compose</em> different factors of variation to understand their
interplay.</p>

<p>We’ll dedicate the rest of this post to illustrating how one
might actually use 3DB in practice, focusing on a single example 3D
model3DB works with any 3D model, and we refer the reader to 
<a href="https://arxiv.org/abs/2106.03805">our paper</a> for more examples and details.:</p>

<p style="text-align: center;">
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_1.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_2.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_3.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_4.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_5.png" style="display: inline; width: 18%;" />
</p>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    The 3D mug model that we'll be using throughout this post.
</div>

<p>In what follows, we will walk through example applications of 3DB to discover biases of
ML models (some previously documented, others not). For the sake of brevity,
we’ll highlight just a few of these (re-)discoveries—to see more, check out
the <a href="https://arxiv.org/abs/2106.03805">paper</a>. We’ll then demonstrate that the discoveries of 3DB
transfer pretty reliably to the real world!</p>

<p>Our experiments will all operate on an ImageNet-pretrained
ResNet-18The classifier has a ~70% validation-set
accuracy. that has 42% accuracy on images from the
“coffee mug” ImageNet subclass. While we only study classification in this blog post,
3DB also supports object detection and can be easily extended to support other
image-based tasks, such as semantic segmentation, too.</p>

<h2 id="image-background-sensitivity">Image Background Sensitivity</h2>

<p>In one of our <a href="https://gradientscience.org/background/">previous posts</a>,
we continued a long line of prior work (see, e.g.,
<a href="https://hal.archives-ouvertes.fr/hal-00171412/file/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">here</a>, <a href="https://arxiv.org/abs/1611.06596">here</a>, <a href="https://arxiv.org/abs/1911.08731">here</a>, etc.) showing that models can be
over-reliant on image backgrounds, and demonstrated that they are easily
broken by adversarially chosen backgrounds. To accomplish this, our prior
analysis used classical computer vision tools to separate foregrounds from
backgrounds, then pasted foregrounds from one image onto backgrounds from
another. This process was slow and required extensive quality control to
ensure that backgrounds and foregrounds were being extracted properly—and
even when they were, a few artifacts remained:</p>

<div style="margin-bottom: 15px; overflow: hidden; text-align: center;">
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/1.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/2.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/3.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/4.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/5.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/6.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
</div>

<p>3DB lets us reproduce these findings effortlessly and without introducing
such artifacts. To demonstrate this, we use 3DB to render our mug 3D model on
hundreds of HDRI backgrounds, resulting in images such as:</p>

<div style="margin-bottom: 15px; overflow: hidden;">
    
        <img src="https://gradientscience.org/assets/3db/background-renders/1.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/2.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/3.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/4.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/5.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/6.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/7.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/8.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/9.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/10.png" style="float: right; width: 20%;" />
    
</div>

<p>We then analyze the performance of a pretrained ResNet-18Recall that this model
obtains 42% accuracy on the corresponding "coffee mug" ImageNet class subset.on these images. We
find that the performance of the classifier varies significantly across
backgrounds, and that accuracy correlates with a crude measure of
“background simplicity” (the JPEG compressed size of the image—with smaller size corresponding to being more simple).</p>

<p><img src="https://gradientscience.org/assets/3db/mug_background_complexity.png" alt="Simplicity versus model accuracy for HDRI backgrounds" /></p>

<div class="caption">
A graph plotting average accuracy of our pre-trained ImageNet model (y-axis) on
images rendered by 3DB while varying the complexity of the rendering backgrounds
(x-axis). 
</div>

<p><strong>A note on compositionality</strong>: An important part of 3DB that we don’t discuss here is compositionality, i.e., the ability to put together multiple controls and study their joint effect. For example, in our paper we studied how a model’s prediction vary with various zoom levels and backgrounds of an object. We found that the optimal zoom level varies a lot by background.</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="bg-tab1" type="radio" checked="" class="tab1" name="bg-tab" />
    <label for="bg-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="bg-tab2" type="radio" class="tab2" name="bg-tab" />
    <label for="bg-tab2"><i class="fa fa-code"></i>  Config (backgrounds.yaml)</label>
    <input id="bg-tab3" type="radio" class="tab3" name="bg-tab" />
    <label for="bg-tab3"><i class="fa fa-search"></i>  Analysis (analyze_bgs.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="bash">
# $BLENDER_DATA/blender_environments` contains several backgrounds and
# $BLENDER_DATA/blender_models contains the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/backgrounds
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# (Optional) Download additional backgrounds you want---e.g., from
# https://hdrihaven.com/hdris/ (both `.hdr` and `.blend` files work) and put
# them in BLENDER_DATA/blender_environments. 
wget https://hdrihaven.com/hdris/PATH/TO/HDRI \
    -O $BLENDER_DATA/blender_environments

# Direct results
export RESULTS_FOLDER='results_backgrounds'

# Run 3DB (with the YAML file from the next pane saved as `backgrounds.yaml`):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA backgrounds.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (third pane)
python analyze_bgs.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_backgrounds/details.log').readlines()
class_map = json.load(open('results_backgrounds/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('environment').agg(accuracy=('is_correct', 'mean'),
        most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
</div>
</div>
</details>

<h2 id="texture-bias">Texture Bias</h2>
<p>Another <a href="https://arxiv.org/abs/1811.12231v2">recent study</a> of neural network biases showed that in
contrast to humans, convolutional neural networks (CNNs) rely more on texture
to recognize objects than on shape. The example below typifies this
phenomenon—a cat with an elephant texture is recognized as a cat by humans,
but as an elephant by CNNs:</p>

<p><img src="https://gradientscience.org/assets/3db/cue_conflict.png" alt="Cue-conflict images introduced by Geirhos et al." /></p>
<div class="caption">
An example of the cue-conflict images introduced by Geirhos et al. Combining the
elephant texture with the cat shape results in a mixed image on which CNNs
consistently predict with the texture signal.
</div>

<p>This example and others like it (dubbed ‘cue-conflict’ images) provide a
striking illustration of the contrast between human and CNN-based
classification mechanisms. Still, just as in the case of image backgrounds,
creating such images typically necessitates time, technical skill,
quality control, and/or introduction of unwanted artifacts (for example, in the above figure,
ideally we would modify only the texture of the cat without altering the background).</p>

<p>However, using 3DB we can easily collect photorealistic empirical evidence of
texture bias. Without modifying the internal 3DB codebase at all,
one can write a <a href="https://github.com/3db/3db/blob/main/threedb/controls/blender/material.py">custom control</a> that modifies the texture of
objects in the scene while keeping the rest intact. With this custom control
in placeIn fact, the texture-swapping control for this experiment is now 
pre-packaged with 3DB, since we already wrote it ourselves!, one can simply 
randomize the texture of the mug across various
backgrounds, poses and camera parameters before stratifying results:</p>

<p><img src="https://gradientscience.org/assets/3db/texture_swap_histograms.png" alt="Chungus" /></p>

<p>The performance of the pretrained model on mugs (and other objects)
deteriorates severely upon replacing the mug’s texture with a “wrong” one,
providing clear corroborating evidence of the texture bias! We noticed in our
experiments that for some textures (e.g., zebra), the coffee mug was
consistently misclassified as the corresponding animal, whereas for others
(e.g., crocodile), the mug is misclassified as either a related class (e.g.,
turtle or other reptile), or as an unrelated object (e.g., a trash can).</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="texture-tab1" type="radio" checked="" class="tab1" name="texture-tab" />
    <label for="texture-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="texture-tab2" type="radio" class="tab2" name="texture-tab" />
    <label for="texture-tab2"><i class="fa fa-code"></i>  Config (texture_swaps.yaml)</label>
    <input id="texture-tab3" type="radio" class="tab3" name="texture-tab" />
    <label for="texture-tab3"><i class="fa fa-search"></i>  Analysis (analyze_ts.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1">
<pre class="wrapped"><code class="bash">
# ${BLENDER_DATA}/blender_environments contains several backgrounds,
# ${BLENDER_DATA}/blender_models contain the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/texture_swaps

# List the materials that we will use for this post:
ls blog_demo/data/texture_swaps/blender_control_material
# You can also make or download blender materials corresponding 
# to other textures you want to test, and add them to that folder 

# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

export RESULTS_FOLDER=results_texture

# Run 3DB (with the YAML file from the next pane saved as texture_swaps.yaml):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA texture_swaps.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (copy from third pane)
python analyze_ts.py

</code></pre>
</div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.position"
    offset_x: 0.
    offset_y: 0.5
    offset_z: 0.
  - module: "threedb.controls.blender.pin_to_ground"
    z_ground: 0.25
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: [0., 1.]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.material"
    replacement_material: ["cow.blend", "elephant.blend", "zebra.blend", "crocodile.blend", "keep_original"]
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_texture/details.log').readlines()
class_map = json.load(open('results_texture/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df = df.drop('render_args', axis=1).join(pd.DataFrame(df.render_args.values.tolist()))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('MaterialControl.replacement_material').agg(acc=('is_correct', 'mean'),
      most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
    </div>
</div>
</details>

<h2 id="part-of-object-attribution">Part-of-Object Attribution</h2>

<p>Beyond general hypotheses about model biases, 3DB allows us to test vision
systems on a more fine-grained level. In the case of our running mug example,
for instance, we can use the platform to understand which specific parts of
its 3D mesh correlate with classifier accuracy. Specifically, below we
generate (and classify) scenes with random mug positions, rotations, and
backgrounds. Since 3DB stores texture-coordinate information for each
rendering, we can reconstruct a three-dimensional heatmap that encodes, for
each point on the surface of the mug, the classifier’s accuracy conditioned
on that point being visible:</p>

<p style="text-align: center;">
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_1.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_2.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_3.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_4.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_5.png" style="width: 18%; display: inline;" />
</p>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    A part-of-object attribution heatmap for our mug 3D model. <span style="color: red;">Red</span> pixels indicate areas whose visibility most
    improves accuracy, whereas <span style="color: blue;">blue</span> areas'
    visibility correlates with incorrect classifications.
</div>

<p>A number of phenomena stand out from this heatmap, including:</p>

<ol>
  <li>The classifier is worse when the side of the mug opposite the handle is seen.</li>
  <li>The classifier is more accurate when the bottom rim is visible.</li>
  <li>The classifier performs worst when the inside of the mug is visible.</li>
</ol>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="pobj-tab1" type="radio" checked="" class="tab1" name="pobj-tab" />
    <label for="pobj-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="pobj-tab2" type="radio" class="tab2" name="pobj-tab" />
    <label for="pobj-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <input id="pobj-tab3" type="radio" class="tab3" name="pobj-tab" />
    <label for="pobj-tab3"><i class="fa fa-search"></i>  Analysis (analyze_po.py)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# point BLENDER_DATA to the environments and models for this experiment
export BLENDER_DATA=$(realpath blog_demo)/data/part_of_object
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# Optionally: download additional backgrounds (`.hdr` or `.blend`) e.g.,
wget URL -O $BLENDER_DATA/blender_environments/new_env.hdr

# Point results folder to where you want output written
export RESULTS_FOLDER='results_part_of_object'

# Run 3DB (with the YAML file from the next pane):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA part_of_object.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Run `part_of_object.py` (third pane) to generate the heat map of the mug.
python po_analysis.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_uv: True
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: 1.
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"
  - module: "threedb.controls.blender.background"
    H: 1.
    S: 0.
    V: 1.
</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json
from PIL import Image

DIR = 'results_part_of_object'
log_lines = open(f'{DIR}/details.log').readlines()
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))

# From class index to class name (for readability)
class_map = json.load(open(f'{DIR}/class_maps.json'))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])

# We'll be a little lenient here to get a more interesting heatmap
df['is_correct'] = df['prediction'].isin(['cup', 'coffee mug'])

uv_num_correct = np.zeros((256, 256))
uv_num_visible = np.zeros((256, 256))
for imid in df["id"].unique().tolist():
    is_correct = float(df.set_index('id').loc[imid]['is_correct'])
    vis_coords_im = Image.open(f'{DIR}/images/{imid}_uv.png')
    vis_coords = np.array(vis_coords_im).reshape(-1, 3)
    # R and G channels encode texture coordinates (x, y), 
    # B channel is 255 for object and 0 for background
    # So we will filter by B then only look at R and G.
    vis_coords = vis_coords[vis_coords[:,2] &gt; 0][:,:2]

    uv_num_visible[vis_coords[:,0], vis_coords[:,1]] += 1.
    uv_num_correct[vis_coords[:,0], vis_coords[:,1]] += is_correct

# Accuracy = # correct / # visible
uv_accuracy = uv_num_correct / (uv_num_visible + 1e-4)

# Saves a black-and-white heatmap
Image.fromarray((255 * uv_accuracy).astype('uint8'))

</code></pre></div>
</div>
</div>
</details>
<p><br /></p>

<p>Now that we have hypotheses regarding model performance, we can test them! Inspecting
the ImageNet validation set, we found that our classifier indeed (a) struggles on coffee mugs when the
handle is not showing (providing a feasible explanation for (1), since the side
opposite the handle is only visible when the handle itself isn’t), and (b)
performs worse at higher camera angles (providing a plausible explanation for
(2)). We want to focus, however, on the third phenomenon,
i.e., that the classifier performs quite poorly whenever the inside of the
mug is visible. Why could this be the case? We can use 3DB to gain insight into the
phenomenon. Specifically, we want to test the following hypothesis: when
classifying mugs, does our ImageNet model rely on the exact liquid inside the
cup?</p>

<p>We investigate this hypothesis by writing a custom control that fills
our mug with various liquids (more precisely, a parameterized mixture of
water, milk, and coffee):</p>

<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_samples.png" alt="Examples of the mugs rendered in this experiment" /></p>
<div class="caption">
Our running example mug, filled with different parameterized liquids: 100% water
(top left), 100% coffee (top right), 100% milk (bottom right), and a coffee-milk
mixture (bottom left)
</div>

<p>In contrast to the last experiment (where we varied the orientation of the
mug), we render scenes containing the mug in a fixed set of poses that reveal
the contents—just as in the last experiment, however, we still vary
background and mug location. We visualize the results below—each cell in
the heatmap corresponds to a fixed mixture of coffee, water, and milk (i.e.,
the labeled corners are 100% coffee, 100% milk, and 100% water, and the other
cells are linear interpolations of these ratios) and the color of the cell
encodes the relative accuracy of the classifier when the mug is filled with
that liquid:</p>



<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_simplex.png" alt="" /></p>

<div class="caption">
Measuring the relative effect of the liquid mixture in the mug on model
predictions. Each cell represents a specific liquid mixture, and the color of
the cell represents the tendency of the model (averaged over random viewpoints
and relative to the other cells) to predict "cup"/"pill bottle," "bucket," or
"mug."
</div>

<p>It turns out that mug content indeed highly impacts classification:
our model is much less likely to correctly classify a mug that doesn’t contain coffee!
This is just one example of how 3DB can help in proving or disproving hypotheses
about model behavior.</p>

<h2 id="from-simulation-to-reality">From Simulation to Reality</h2>

<p>So far, we’ve used 3DB to discover ML models’ various failure modes and biases
via photorealistic rendering. To what extent though do the insights
gleaned from simulated 3DB experiments actually “transfer” to the physical
world?</p>

<p>To test such transferability, we began by creating a 3D model of a physical room we
had access to. We also collected eight different 3D models with closely matching
physical world counterparts—including the mug analyzed above. Next, we used 3DB to find correctly and incorrectly classified
configurations (pose, orientation, location) of these eight objects inside that
room. Finally, we replicated these poses (to
the best of our abilities) in the physical room, and took photos with a
cellphone camera:</p>

<p><img src="https://gradientscience.org/assets/3db/real_life_exp_samples.png" alt="Samples from our physical-world experiment." class="bigimg" /></p>

<div class="caption">
Examples of simulated scenes (top) and their re-created counterparts (bottom)
from our physical-world experiment.
</div>

<p>We classified these photos with the same vision model as before and measured
how often the simulated classifier correctness matched correctness on the
real photographs. We observed an ~85% match! So the failure
modes identified by 3DB are not merely simulation artifacts, and can indeed arise in
the real world.</p>

<h2 id="conclusion">Conclusion</h2>

<p>3DB is a flexible, easy-to-use, and extensible framework for
identifying model failure modes, uncovering biases, and testing fine-grained
hypotheses about model behavior. We hope it will prove to be a useful tool
for debugging vision models.</p>
<h2 id="bonus-object-detection-web-dashboard-and-more">Bonus: Object Detection, Web Dashboard, and more!</h2>

<p>We’ll wrap up by highlighting some additional capabilities of 3DB that we didn’t
get to demonstrate in this blog post:</p>

<h3 id="3dboard-a-web-interface-for-exploring-results">3DBoard: a web interface for exploring results</h3>

<p>In all of the code examples above, we showed how to analyze the results of a 3DB
experiment by loading the output into a <code class="language-plaintext highlighter-rouge">pandas</code> dataframe. For additional
convenience, however, 3DB also comes with a web-based dashboard for exploring
experimental results. The following command suffices to visualize the texture swaps experiment from earlier:</p>

<pre style="padding: 0; margin: 0;"><code class="bash">
python -m threedboard results_texture/ --port 3000

</code>
</pre>

<p>Navigating to <code class="language-plaintext highlighter-rouge">YOUR_IP:3000</code> should lead you to a page that looks like this:</p>

<p><img src="https://gradientscience.org/assets/3db/dashboard_screenshot.png" alt="Dashboard screenshot" /></p>

<h3 id="object-detection-and-other-tasks">Object detection and other tasks</h3>

<p>In this blog post, we focused on using 3DB to analyze image classification
models. However, the library also supports object detection out-of-the-box, and
can easily be extended to support a variety of image-based tasks (e.g.,
segmentation or regression-based tasks). For example, below we provide a simple
end-to-end object detection example:</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="objdet-tab1" type="radio" checked="" class="tab1" name="objdet-tab" />
    <label for="objdet-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="objdet-tab2" type="radio" class="tab2" name="objdet-tab" />
    <label for="objdet-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# The object detection example is separate from the rest of the blog demo, so
# run the following in a separate repo:
git clone https://github.com/3db/object_detection_demo

# The repo has a data/ folder containing the Blender model (a banana) and some
# HDRI backgrounds, a classmap.json file mapping the UID of the model to a COCO
# class, and the detection.yaml file from the next pane.
cd object_detection_demo/

export BLENDER_DATA=data/
export RESULTS_FOLDER=results/

# Run 3DB
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp; 
threedb_master $BLENDER_DATA detection.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

</code></pre></div> 
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models.detection'
  class: 'retinanet_resnet50_fpn'
  label_map: './resources/coco_mapping.json'
  normalization:
    mean: [0., 0., 0.]
    std: [1., 1., 1.]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.detection'
  args:
    iou_threshold: 0.5
    nms_threshold: 0.1
    max_num_boxes: 10
    classmap_path: 'classmap.json'
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_segmentation: true
policy:
  module: "threedb.policies.random_search"
  samples: 2
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
</div>
</div>
</details></div>







<p class="date">
<a href="https://gradientscience.org/3db-light/"><span class="datestr">at June 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/3db/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/3db/">3DB: A Framework for Debugging Vision Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%; margin-bottom: 0;" href="https://arxiv.org/abs/2106.03805" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%; margin-bottom: 0;" href="https://github.com/3db/3db/" class="bbutton">
<i class="fab fa-github"></i>
   Project Repo
</a>
<a style="float: left; width: 45%;" href="https://3db.github.io/3db/usage/quickstart.html" class="bbutton">
<i class="fas fa-file-alt"></i>
   Documentation and Guides
</a> 
<a style="float: left; width: 45%;" href="https://github.com/3db/blog_demo/" class="bbutton">
<i class="fab fa-github"></i>
   Blog Demo
</a> 
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, in collaboration with Microsoft Research, we introduce
3DB: an extendable, unified framework for debugging and analyzing vision models
using photorealistic simulation. We’re releasing 3DB as a package, accompanied
by extensive API documentation, guides, and demos.</i></p>

<p><em>Note: this post contains some interactive plots and 3D models that use
JavaScript: click <a href="https://gradientscience.org/3db-light/">here</a> for a JS-free version of this post.</em></p>

<p>Identifying failure modes and biases in vision models is a rapidly
emerging challenge in machine learning. In high-stakes
applications, simply deploying models and collecting failures that arise in
the wild is often difficult, expensive, and irresponsible. To this end, a
recent line of work in vision focuses on identifying model failure
modes via in-depth analyses of <a href="https://arxiv.org/abs/1712.02779">image transformations</a> and 
<a href="https://arxiv.org/abs/1903.12261">corruptions</a>, <a href="https://objectnet.dev">object orientations</a>,
<a href="https://gradientscience.org/background/">backgrounds</a>, or <a href="https://arxiv.org/abs/1811.12231v2">shape-texture conflicts</a>. These studies 
(and other similarly important ones) reveal a variety of patterns of
performance degradation in vision models. Still, performing each such study
requires time, developing (often complex)
toolingFor <a href="https://gradientscience.org/background/">our study of image backgrounds</a>, for example, we used a
combination of bounding boxes and classical computer vision tools to crop out
image backgrounds. We then had to manually filter out the images for which
the tools failed. Even for the images where the toolkit succeeded, there
remained inevitable cropping artifacts., and a willingness to settle for less than perfect simulations of each
potential failure mode. Our question is: can we support reliable discovery of model failures in a systematic,
automated, and unified way?</p>

<h2 id="3db-a-rendering-based-debugging-platform">3DB: A Rendering-based Debugging Platform</h2>

<p><img src="https://gradientscience.org/assets/3db/3db_headline.png" alt="A sampling of the analyses enabled by 3DB." class="bigimg" /></p>
<div style="margin-top: 0;" class="caption">
A sampling of the analyses enabled by 3DB.
</div>

<p>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, we try to make progress on this question and propose
3DB, a platform for automatically identifying and analyzing the failure modes
of computer vision models using 3D rendering. 3DB aims to allow users to go
from a testable, robustness-based hypothesis to concrete, photorealistic
experimental evidence with minimal time and effort.</p>

<p>The platform revolves around the modular workflow pictured below. First,
users specify a set of 3D objects and environments, as well as a set of 3D
(or 2D) transformations called controls that determine the space of
admissible object-environment configurations. 3DB then renders a myriad of
admissible scenes and feeds them through the user’s computer vision model of
choice. The user can finally stratify, aggregate, or otherwise analyze the
results either by reading the outputted JSON, or through the pre-packaged
dashboard.</p>

<p><img src="https://gradientscience.org/assets/3db/workflow.png" alt="An illustration of the 3DB workflow." class="bigimg" /></p>

<p>3DB easily adapts to a variety of use cases: in particular, users can
modify and swap out any part of this pipeline (e.g., the renderer, the
logger, the model type, or the controls) for their own custom-written
components, without needing to modify any of the 3DB codebase. We’ve compiled <a href="https://3db.github.io/3db/">guides</a>,
extensive <a href="https://3db.github.io/3db/api_doc.html">API documentation</a>, and a
<a href="https://github.com/3db/demo">full demo</a> showing how 3DB streamlines model debugging.</p>

<p><strong>In fact, this blog post will double as another demo! We’ll present the (short) code
necessary to reproduce every plot in the post below using 3DB. You can download 
the aggregated code for this blog post <a href="https://github.com/3db/blog_demo">here</a>.</strong></p>

<p><em>To set up, follow the steps below—then, in the remainder of this post, press
“Show/hide code and instructions” to see the steps necessary to reproduce each
experiment below.</em></p>
<div class="code-container">
  <input id="general-tab1" type="radio" checked="" class="tab1" name="general-tab" />
  <label for="general-tab1"><i class="fa fa-gear"></i>  Setup</label>
  <input id="general-tab2" type="radio" class="tab2" name="general-tab" />
  <label for="general-tab2"><i class="fa fa-code"></i>  Config (base.yaml)</label>
  <div class="line"></div>
  <div class="content-container">
<div class="content c1">

<ol class="instructions">
<li>Clone the  <a href="https://github.com/3db/blog_demo">blog demo <i class="fab fa-github"></i></a> repo</li>
<li>Run <pre>cd blog_demo</pre>, then <pre>bash setup.sh</pre> (assumes
<pre>unzip</pre> is installed) to download a large Blender environment, then
<pre>cd ../</pre></li>
<li>Install 3DB: <pre>curl -L https://git.io/Js8eT | bash /dev/stdin threedb</pre></li>
<li>Run <pre>conda activate threedb</pre></li>
<li>Our experiments below will need a <pre>BLENDER_DATA</pre> folder that contains two
subfolders: <pre>blender_models/</pre> containing 3D models (<pre>.blend</pre> files with a single
object whose name matches the filename), and <pre>blender_environments/</pre>
containing environments. We will provide you with these later</li> 
<li>Separately, make a file called <pre>base.yaml</pre> and paste in the configuration from the next pane.</li>
</ol>

</div>
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models'
  label_map: 'blog_demo/resources/imagenet_mapping.json'
  class: 'resnet18'
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.classification'
  args:
    classmap_path: 'blog_demo/resources/ycb_to_IN.json'
    topk: 1
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
policy:
  module: "threedb.policies.random_search"
  samples: 5
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"

</code></pre></div>
 </div>
</div>

<h2 id="using-3db">Using 3DB</h2>

<p>Prior works have already used 3D rendering (to great effect) to study biases
of machine learning models, including pose and context-based biases. Our goal
is not to propose a specific 3D-rendering based analysis, but
rather to provide an easy-to-use, highly extendable framework that unifies
prior analyses (both 3D and 2D) while enabling users to (a) conduct a host of
new analyses with the same ease and with realistic results; and (b)
effortlessly <em>compose</em> different factors of variation to understand their
interplay.</p>

<p>We’ll dedicate the rest of this post to illustrating how one
might actually use 3DB in practice, focusing on a single example 3D
model3DB works with any 3D model, and we refer the reader to 
<a href="https://arxiv.org/abs/2106.03805">our paper</a> for more examples and details.:</p>





<div style="text-align: center;" id="mug-blocker">
    
        
            
        
        
    
</div>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    The 3D mug model that we'll be using throughout this post. <strong>
    Click to <a id="mug-interact">enable 
    interactivity</a>.</strong>
</div>

<p>In what follows, we will walk through example applications of 3DB to discover biases of
ML models (some previously documented, others not). For the sake of brevity,
we’ll highlight just a few of these (re-)discoveries—to see more, check out
the <a href="https://arxiv.org/abs/2106.03805">paper</a>. We’ll then demonstrate that the discoveries of 3DB
transfer pretty reliably to the real world!</p>

<p>Our experiments will all operate on an ImageNet-pretrained
ResNet-18The classifier has a ~70% validation-set
accuracy. that has 42% accuracy on images from the
“coffee mug” ImageNet subclass. While we only study classification in this blog post,
3DB also supports object detection and can be easily extended to support other
image-based tasks, such as semantic segmentation, too.</p>

<h2 id="image-background-sensitivity">Image Background Sensitivity</h2>

<p>In one of our <a href="https://gradientscience.org/background/">previous posts</a>,
we continued a long line of prior work (see, e.g.,
<a href="https://hal.archives-ouvertes.fr/hal-00171412/file/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">here</a>, <a href="https://arxiv.org/abs/1611.06596">here</a>, <a href="https://arxiv.org/abs/1911.08731">here</a>, etc.) showing that models can be
over-reliant on image backgrounds, and demonstrated that they are easily
broken by adversarially chosen backgrounds. To accomplish this, our prior
analysis used classical computer vision tools to separate foregrounds from
backgrounds, then pasted foregrounds from one image onto backgrounds from
another. This process was slow and required extensive quality control to
ensure that backgrounds and foregrounds were being extracted properly—and
even when they were, a few artifacts remained:</p>

<div style="margin-bottom: 15px; overflow: hidden; text-align: center;">
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/1.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/2.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/3.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/4.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/5.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/6.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
</div>

<p>3DB lets us reproduce these findings effortlessly and without introducing
such artifacts. To demonstrate this, we use 3DB to render our mug 3D model on
hundreds of HDRI backgrounds, resulting in images such as:</p>

<div style="margin-bottom: 15px; overflow: hidden;">
    
        <img src="https://gradientscience.org/assets/3db/background-renders/1.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/2.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/3.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/4.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/5.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/6.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/7.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/8.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/9.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/10.png" style="float: right; width: 20%;" />
    
</div>

<p>We then analyze the performance of a pretrained ResNet-18Recall that this model
obtains 42% accuracy on the corresponding "coffee mug" ImageNet class subset.on these images. We
find that the performance of the classifier varies significantly across
backgrounds, and that accuracy correlates with a crude measure of
“background simplicity” (the JPEG compressed size of the image—with smaller size corresponding to being more simple).</p>

<div>
  <canvas id="myChart"></canvas>
</div>

<div class="caption">
A graph plotting average accuracy of our pre-trained ImageNet model (y-axis) on
images rendered by 3DB while varying the complexity of the rendering backgrounds
(x-axis). 
</div>

<p><strong>A note on compositionality</strong>: An important part of 3DB that we don’t discuss here is compositionality, i.e., the ability to put together multiple controls and study their joint effect. For example, in our paper we studied how a model’s prediction vary with various zoom levels and backgrounds of an object. We found that the optimal zoom level varies a lot by background.</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="bg-tab1" type="radio" checked="" class="tab1" name="bg-tab" />
    <label for="bg-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="bg-tab2" type="radio" class="tab2" name="bg-tab" />
    <label for="bg-tab2"><i class="fa fa-code"></i>  Config (backgrounds.yaml)</label>
    <input id="bg-tab3" type="radio" class="tab3" name="bg-tab" />
    <label for="bg-tab3"><i class="fa fa-search"></i>  Analysis (analyze_bgs.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="bash">
# $BLENDER_DATA/blender_environments` contains several backgrounds and
# $BLENDER_DATA/blender_models contains the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/backgrounds
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# (Optional) Download additional backgrounds you want---e.g., from
# https://hdrihaven.com/hdris/ (both `.hdr` and `.blend` files work) and put
# them in BLENDER_DATA/blender_environments. 
wget https://hdrihaven.com/hdris/PATH/TO/HDRI \
    -O $BLENDER_DATA/blender_environments

# Direct results
export RESULTS_FOLDER='results_backgrounds'

# Run 3DB (with the YAML file from the next pane saved as `backgrounds.yaml`):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA backgrounds.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (third pane)
python analyze_bgs.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_backgrounds/details.log').readlines()
class_map = json.load(open('results_backgrounds/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('environment').agg(accuracy=('is_correct', 'mean'),
        most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
</div>
</div>
</details>

<h2 id="texture-bias">Texture Bias</h2>
<p>Another <a href="https://arxiv.org/abs/1811.12231v2">recent study</a> of neural network biases showed that in
contrast to humans, convolutional neural networks (CNNs) rely more on texture
to recognize objects than on shape. The example below typifies this
phenomenon—a cat with an elephant texture is recognized as a cat by humans,
but as an elephant by CNNs:</p>

<p><img src="https://gradientscience.org/assets/3db/cue_conflict.png" alt="Cue-conflict images introduced by Geirhos et al." /></p>
<div class="caption">
An example of the cue-conflict images introduced by Geirhos et al. Combining the
elephant texture with the cat shape results in a mixed image on which CNNs
consistently predict with the texture signal.
</div>

<p>This example and others like it (dubbed ‘cue-conflict’ images) provide a
striking illustration of the contrast between human and CNN-based
classification mechanisms. Still, just as in the case of image backgrounds,
creating such images typically necessitates time, technical skill,
quality control, and/or introduction of unwanted artifacts (for example, in the above figure,
ideally we would modify only the texture of the cat without altering the background).</p>

<p>However, using 3DB we can easily collect photorealistic empirical evidence of
texture bias. Without modifying the internal 3DB codebase at all,
one can write a <a href="https://github.com/3db/3db/blob/main/threedb/controls/blender/material.py">custom control</a> that modifies the texture of
objects in the scene while keeping the rest intact. With this custom control
in placeIn fact, the texture-swapping control for this experiment is now 
pre-packaged with 3DB, since we already wrote it ourselves!, one can simply 
randomize the texture of the mug across various
backgrounds, poses and camera parameters before stratifying results:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <div style="width: 32%; margin-right: 2%;" class="image-container">
            <h4 style="text-align: center;">Average accuracy (compare to 42% on ImageNet val set)</h4>
            <canvas id="gen1"></canvas>
        </div>
        <div style="width: 66%;" class="image-container" id="gen2">
            <img class="example-texture" id="gen2-0" />
            <img class="example-texture" id="gen2-1" />
            <img class="example-texture" id="gen2-2" />
            <img class="example-texture" id="gen2-3" />
            <img class="example-texture" id="gen2-4" />
            <img class="example-texture" id="gen2-5" />
            <img class="example-texture" id="gen2-6" />
            <img class="example-texture" id="gen2-7" />
        </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="caption">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>The performance of the pretrained model on mugs (and other objects)
deteriorates severely upon replacing the mug’s texture with a “wrong” one,
providing clear corroborating evidence of the texture bias! We noticed in our
experiments that for some textures (e.g., zebra), the coffee mug was
consistently misclassified as the corresponding animal, whereas for others
(e.g., crocodile), the mug is misclassified as either a related class (e.g.,
turtle or other reptile), or as an unrelated object (e.g., a trash can).</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="texture-tab1" type="radio" checked="" class="tab1" name="texture-tab" />
    <label for="texture-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="texture-tab2" type="radio" class="tab2" name="texture-tab" />
    <label for="texture-tab2"><i class="fa fa-code"></i>  Config (texture_swaps.yaml)</label>
    <input id="texture-tab3" type="radio" class="tab3" name="texture-tab" />
    <label for="texture-tab3"><i class="fa fa-search"></i>  Analysis (analyze_ts.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1">
<pre class="wrapped"><code class="bash">
# ${BLENDER_DATA}/blender_environments contains several backgrounds,
# ${BLENDER_DATA}/blender_models contain the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/texture_swaps

# List the materials that we will use for this post:
ls blog_demo/data/texture_swaps/blender_control_material
# You can also make or download blender materials corresponding 
# to other textures you want to test, and add them to that folder 

# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

export RESULTS_FOLDER=results_texture

# Run 3DB (with the YAML file from the next pane saved as texture_swaps.yaml):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA texture_swaps.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (copy from third pane)
python analyze_ts.py

</code></pre>
</div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.position"
    offset_x: 0.
    offset_y: 0.5
    offset_z: 0.
  - module: "threedb.controls.blender.pin_to_ground"
    z_ground: 0.25
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: [0., 1.]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.material"
    replacement_material: ["cow.blend", "elephant.blend", "zebra.blend", "crocodile.blend", "keep_original"]
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_texture/details.log').readlines()
class_map = json.load(open('results_texture/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df = df.drop('render_args', axis=1).join(pd.DataFrame(df.render_args.values.tolist()))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('MaterialControl.replacement_material').agg(acc=('is_correct', 'mean'),
      most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
    </div>
</div>
</details>

<h2 id="part-of-object-attribution">Part-of-Object Attribution</h2>

<p>Beyond general hypotheses about model biases, 3DB allows us to test vision
systems on a more fine-grained level. In the case of our running mug example,
for instance, we can use the platform to understand which specific parts of
its 3D mesh correlate with classifier accuracy. Specifically, below we
generate (and classify) scenes with random mug positions, rotations, and
backgrounds. Since 3DB stores texture-coordinate information for each
rendering, we can reconstruct a three-dimensional heatmap that encodes, for
each point on the surface of the mug, the classifier’s accuracy conditioned
on that point being visible:</p>

<div style="text-align: center;" id="heatmap-blocker">
    
        
            
        
        
        
    
</div>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    A part-of-object attribution heatmap for our mug 3D model. <span style="color: red;">Red</span> pixels indicate areas whose visibility most
    improves accuracy, whereas <span style="color: blue;">blue</span> areas'
    visibility correlates with incorrect classifications. <strong>Click here to <a id="heatmap-interact">enable
    interactivity</a>.</strong>
</div>

<p>A number of phenomena stand out from this heatmap, including:</p>

<ol>
  <li>The classifier is worse when the side of the mug opposite the handle is seen.</li>
  <li>The classifier is more accurate when the bottom rim is visible.</li>
  <li>The classifier performs worst when the inside of the mug is visible.</li>
</ol>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="pobj-tab1" type="radio" checked="" class="tab1" name="pobj-tab" />
    <label for="pobj-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="pobj-tab2" type="radio" class="tab2" name="pobj-tab" />
    <label for="pobj-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <input id="pobj-tab3" type="radio" class="tab3" name="pobj-tab" />
    <label for="pobj-tab3"><i class="fa fa-search"></i>  Analysis (analyze_po.py)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# point BLENDER_DATA to the environments and models for this experiment
export BLENDER_DATA=$(realpath blog_demo)/data/part_of_object
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# Optionally: download additional backgrounds (`.hdr` or `.blend`) e.g.,
wget URL -O $BLENDER_DATA/blender_environments/new_env.hdr

# Point results folder to where you want output written
export RESULTS_FOLDER='results_part_of_object'

# Run 3DB (with the YAML file from the next pane):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA part_of_object.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Run `part_of_object.py` (third pane) to generate the heat map of the mug.
python po_analysis.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_uv: True
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: 1.
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"
  - module: "threedb.controls.blender.background"
    H: 1.
    S: 0.
    V: 1.
</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json
from PIL import Image

DIR = 'results_part_of_object'
log_lines = open(f'{DIR}/details.log').readlines()
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))

# From class index to class name (for readability)
class_map = json.load(open(f'{DIR}/class_maps.json'))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])

# We'll be a little lenient here to get a more interesting heatmap
df['is_correct'] = df['prediction'].isin(['cup', 'coffee mug'])

uv_num_correct = np.zeros((256, 256))
uv_num_visible = np.zeros((256, 256))
for imid in df["id"].unique().tolist():
    is_correct = float(df.set_index('id').loc[imid]['is_correct'])
    vis_coords_im = Image.open(f'{DIR}/images/{imid}_uv.png')
    vis_coords = np.array(vis_coords_im).reshape(-1, 3)
    # R and G channels encode texture coordinates (x, y), 
    # B channel is 255 for object and 0 for background
    # So we will filter by B then only look at R and G.
    vis_coords = vis_coords[vis_coords[:,2] &gt; 0][:,:2]

    uv_num_visible[vis_coords[:,0], vis_coords[:,1]] += 1.
    uv_num_correct[vis_coords[:,0], vis_coords[:,1]] += is_correct

# Accuracy = # correct / # visible
uv_accuracy = uv_num_correct / (uv_num_visible + 1e-4)

# Saves a black-and-white heatmap
Image.fromarray((255 * uv_accuracy).astype('uint8'))

</code></pre></div>
</div>
</div>
</details>
<p><br /></p>

<p>Now that we have hypotheses regarding model performance, we can test them! Inspecting
the ImageNet validation set, we found that our classifier indeed (a) struggles on coffee mugs when the
handle is not showing (providing a feasible explanation for (1), since the side
opposite the handle is only visible when the handle itself isn’t), and (b)
performs worse at higher camera angles (providing a plausible explanation for
(2)). We want to focus, however, on the third phenomenon,
i.e., that the classifier performs quite poorly whenever the inside of the
mug is visible. Why could this be the case? We can use 3DB to gain insight into the
phenomenon. Specifically, we want to test the following hypothesis: when
classifying mugs, does our ImageNet model rely on the exact liquid inside the
cup?</p>

<p>We investigate this hypothesis by writing a custom control that fills
our mug with various liquids (more precisely, a parameterized mixture of
water, milk, and coffee):</p>

<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_samples.png" alt="Examples of the mugs rendered in this experiment" /></p>
<div class="caption">
Our running example mug, filled with different parameterized liquids: 100% water
(top left), 100% coffee (top right), 100% milk (bottom right), and a coffee-milk
mixture (bottom left)
</div>

<p>In contrast to the last experiment (where we varied the orientation of the
mug), we render scenes containing the mug in a fixed set of poses that reveal
the contents—just as in the last experiment, however, we still vary
background and mug location. We visualize the results below—each cell in
the heatmap corresponds to a fixed mixture of coffee, water, and milk (i.e.,
the labeled corners are 100% coffee, 100% milk, and 100% water, and the other
cells are linear interpolations of these ratios) and the color of the cell
encodes the relative accuracy of the classifier when the mug is filled with
that liquid:</p>



<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_simplex.png" alt="" /></p>

<div class="caption">
Measuring the relative effect of the liquid mixture in the mug on model
predictions. Each cell represents a specific liquid mixture, and the color of
the cell represents the tendency of the model (averaged over random viewpoints
and relative to the other cells) to predict "cup"/"pill bottle," "bucket," or
"mug."
</div>

<p>It turns out that mug content indeed highly impacts classification:
our model is much less likely to correctly classify a mug that doesn’t contain coffee!
This is just one example of how 3DB can help in proving or disproving hypotheses
about model behavior.</p>

<h2 id="from-simulation-to-reality">From Simulation to Reality</h2>

<p>So far, we’ve used 3DB to discover ML models’ various failure modes and biases
via photorealistic rendering. To what extent though do the insights
gleaned from simulated 3DB experiments actually “transfer” to the physical
world?</p>

<p>To test such transferability, we began by creating a 3D model of a physical room we
had access to. We also collected eight different 3D models with closely matching
physical world counterparts—including the mug analyzed above. Next, we used 3DB to find correctly and incorrectly classified
configurations (pose, orientation, location) of these eight objects inside that
room. Finally, we replicated these poses (to
the best of our abilities) in the physical room, and took photos with a
cellphone camera:</p>

<p><img src="https://gradientscience.org/assets/3db/real_life_exp_samples.png" alt="Samples from our physical-world experiment." class="bigimg" /></p>

<div class="caption">
Examples of simulated scenes (top) and their re-created counterparts (bottom)
from our physical-world experiment.
</div>

<p>We classified these photos with the same vision model as before and measured
how often the simulated classifier correctness matched correctness on the
real photographs. We observed an ~85% match! So the failure
modes identified by 3DB are not merely simulation artifacts, and can indeed arise in
the real world.</p>

<h2 id="conclusion">Conclusion</h2>

<p>3DB is a flexible, easy-to-use, and extensible framework for
identifying model failure modes, uncovering biases, and testing fine-grained
hypotheses about model behavior. We hope it will prove to be a useful tool
for debugging vision models.</p>
<h2 id="bonus-object-detection-web-dashboard-and-more">Bonus: Object Detection, Web Dashboard, and more!</h2>

<p>We’ll wrap up by highlighting some additional capabilities of 3DB that we didn’t
get to demonstrate in this blog post:</p>

<h3 id="3dboard-a-web-interface-for-exploring-results">3DBoard: a web interface for exploring results</h3>

<p>In all of the code examples above, we showed how to analyze the results of a 3DB
experiment by loading the output into a <code class="language-plaintext highlighter-rouge">pandas</code> dataframe. For additional
convenience, however, 3DB also comes with a web-based dashboard for exploring
experimental results. The following command suffices to visualize the texture swaps experiment from earlier:</p>

<pre style="padding: 0; margin: 0;"><code class="bash">
python -m threedboard results_texture/ --port 3000

</code>
</pre>

<p>Navigating to <code class="language-plaintext highlighter-rouge">YOUR_IP:3000</code> should lead you to a page that looks like this:</p>

<p><img src="https://gradientscience.org/assets/3db/dashboard_screenshot.png" alt="Dashboard screenshot" /></p>

<h3 id="object-detection-and-other-tasks">Object detection and other tasks</h3>

<p>In this blog post, we focused on using 3DB to analyze image classification
models. However, the library also supports object detection out-of-the-box, and
can easily be extended to support a variety of image-based tasks (e.g.,
segmentation or regression-based tasks). For example, below we provide a simple
end-to-end object detection example:</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="objdet-tab1" type="radio" checked="" class="tab1" name="objdet-tab" />
    <label for="objdet-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="objdet-tab2" type="radio" class="tab2" name="objdet-tab" />
    <label for="objdet-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# The object detection example is separate from the rest of the blog demo, so
# run the following in a separate repo:
git clone https://github.com/3db/object_detection_demo

# The repo has a data/ folder containing the Blender model (a banana) and some
# HDRI backgrounds, a classmap.json file mapping the UID of the model to a COCO
# class, and the detection.yaml file from the next pane.
cd object_detection_demo/

export BLENDER_DATA=data/
export RESULTS_FOLDER=results/

# Run 3DB
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp; 
threedb_master $BLENDER_DATA detection.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

</code></pre></div> 
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models.detection'
  class: 'retinanet_resnet50_fpn'
  label_map: './resources/coco_mapping.json'
  normalization:
    mean: [0., 0., 0.]
    std: [1., 1., 1.]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.detection'
  args:
    iou_threshold: 0.5
    nms_threshold: 0.1
    max_num_boxes: 10
    classmap_path: 'classmap.json'
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_segmentation: true
policy:
  module: "threedb.policies.random_search"
  samples: 2
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
</div>
</div>
</details></div>







<p class="date">
<a href="https://gradientscience.org/3db/"><span class="datestr">at June 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02981">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02981">APMF &lt; APSP? Gomory-Hu Tree for Unweighted Graphs in Almost-Quadratic Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abboud:Amir.html">Amir Abboud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trabelsi:Ohad.html">Ohad Trabelsi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02981">PDF</a><br /><b>Abstract: </b>We design an $n^{2+o(1)}$-time algorithm that constructs a cut-equivalent
(Gomory-Hu) tree of a simple graph on $n$ nodes. This bound is almost-optimal
in terms of $n$, and it improves on the recent $\tilde{O}(n^{2.5})$ bound by
the authors (STOC 2021), which was the first to break the cubic barrier.
Consequently, the All-Pairs Maximum-Flow (APMF) problem has time complexity
$n^{2+o(1)}$, and for the first time in history, this problem can be solved
faster than All-Pairs Shortest Paths (APSP). We further observe that an
almost-linear time algorithm (in terms of the number of edges $m$) is not
possible without first obtaining a subcubic algorithm for multigraphs.
</p>
<p>Finally, we derandomize our algorithm, obtaining the first subcubic
deterministic algorithm for Gomory-Hu Tree in simple graphs, showing that
randomness is not necessary for beating the $n-1$ times max-flow bound from
1961. The upper bound is $\tilde{O}(n^{2\frac{2}{3}})$ and it would improve to
$n^{2+o(1)}$ if there is a deterministic single-pair maximum-flow algorithm
that is almost-linear. The key novelty is in using a ``dynamic pivot''
technique instead of the randomized pivot selection that was central in recent
works.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02981"><span class="datestr">at June 08, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02947">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02947">Complexity of Modular Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Paweł M. Idziak, Piotr Kawałek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krzaczkowski:Jacek.html">Jacek Krzaczkowski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02947">PDF</a><br /><b>Abstract: </b>We study how the complexity of modular circuits computing AND depends on the
depth of the circuits and the prime factorization of the modulus they use. In
particular our construction of subexponential circuits of depth 2 for AND helps
us to classify (modulo Exponential Time Hypothesis) modular circuits with
respect to the complexity of their satisfiability. We also study a precise
correlation between this complexity and the sizes of modular circuits realizing
AND. On the other hand showing that AND can be computed by a polynomial size
probabilistic modular circuit of depth 2 (with O(log n) random bits) providing
a probabilistic computational model that can not be derandomized.
</p>
<p>We apply our methods to determine (modulo ETH) the complexity of solving
equations over groups of symmetries of regular polygons with an odd number of
sides. These groups form a paradigm for some of the remaining cases in
characterizing finite groups with respect to the complexity of their equation
solving.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02947"><span class="datestr">at June 08, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02942">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02942">Time-Optimal Sublinear Algorithms for Matching and Vertex Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behnezhad:Soheil.html">Soheil Behnezhad</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02942">PDF</a><br /><b>Abstract: </b>We present a near-tight analysis of the average "query complexity" -- \`a la
Nguyen and Onak [FOCS'08] -- of the randomized greedy maximal matching
algorithm, improving over the bound of Yoshida, Yamamoto and Ito [STOC'09]. For
any $n$-vertex graph of average degree $\bar{d}$, this leads to the following
sublinear-time algorithms for estimating the size of maximum matching and
minimum vertex cover, all of which are provably time-optimal up to logarithmic
factors:
</p>
<p>$\bullet$ A multiplicative $(2+\epsilon)$-approximation in
$\widetilde{O}(n/\epsilon^2)$ time using adjacency list queries. This (nearly)
matches an $\Omega(n)$ time lower bound for any multiplicative approximation
and is, notably, the first $O(1)$-approximation that runs in $o(n^{1.5})$ time.
</p>
<p>$\bullet$ A $(2, \epsilon n)$-approximation in $\widetilde{O}((\bar{d} +
1)/\epsilon^2)$ time using adjacency list queries. This (nearly) matches an
$\Omega(\bar{d}+1)$ lower bound of Parnas and Ron [TCS'07] which holds for any
$(O(1), \epsilon n)$-approximation, and improves over the bounds of [Yoshida et
al. STOC'09; Onak et al. SODA'12] and [Kapralov et al. SODA'20]: The former two
take at least quadratic time in the degree which can be as large as
$\Omega(n^2)$ and the latter obtains a much larger approximation.
</p>
<p>$\bullet$ A $(2, \epsilon n)$-approximation in $\widetilde{O}(n/\epsilon^3)$
time using adjacency matrix queries. This (nearly) matches an $\Omega(n)$ time
lower bound in this model and improves over the $\widetilde{O}(n\sqrt{n})$-time
$(2, \epsilon n)$-approximate algorithm of [Chen, Kannan, and Khanna ICALP'20].
It also turns out that any non-trivial multiplicative approximation in the
adjacency matrix model requires $\Omega(n^2)$ time, so the additive $\epsilon
n$ error is necessary too.
</p>
<p>As immediate corollaries, we get improved sublinear time estimators for
(variants of) TSP and an improved AMPC algorithm for maximal matching.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02942"><span class="datestr">at June 08, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02871">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02871">Discrete Frechet distance for closed curves</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vodolazskiy:Evgeniy.html">Evgeniy Vodolazskiy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02871">PDF</a><br /><b>Abstract: </b>The paper presents a discrete variation of the Frechet distance between
closed curves, which can be seen as an approximation of the continuous measure.
A rather straightforward approach to compute the discrete Frechet distance
between two closed sequences of m and n points using binary search takes O(mn
log mn) time. We present an algorithm that takes O(mn log* mn) time, where log*
is the iterated logarithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02871"><span class="datestr">at June 08, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02848">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02848">Numerical Composition of Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopi:Sivakanth.html">Sivakanth Gopi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wutschitz:Lukas.html">Lukas Wutschitz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02848">PDF</a><br /><b>Abstract: </b>We give a fast algorithm to optimally compose privacy guarantees of
differentially private (DP) algorithms to arbitrary accuracy. Our method is
based on the notion of privacy loss random variables to quantify the privacy
loss of DP algorithms. The running time and memory needed for our algorithm to
approximate the privacy curve of a DP algorithm composed with itself $k$ times
is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela
et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We
demonstrate the utility of our algorithm by accurately computing the privacy
loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm
speeds up the privacy computations by a few orders of magnitude compared to
prior work, while maintaining similar accuracy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02848"><span class="datestr">at June 08, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02839">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02839">Upward planar drawings with two slopes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klawitter:Jonathan.html">Jonathan Klawitter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mchedlidze:Tamara.html">Tamara Mchedlidze</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02839">PDF</a><br /><b>Abstract: </b>In an upward planar 2-slope drawing of a digraph, edges are drawn as
straight-line segments in the upward direction without crossings using only two
different slopes. We investigate whether a given upward planar digraph admits
such a drawing and, if so, how to construct it. For the fixed embedding
scenario, we give a simple characterisation and a linear-time construction by
adopting algorithms from orthogonal drawings. For the variable embedding
scenario, we describe a linear-time algorithm for single-source digraphs, a
quartic-time algorithm for series-parallel digraphs, and a fixed-parameter
tractable algorithm for general digraphs. For the latter two classes, we make
use of SPQR-trees and the notion of upward spirality. As an application of this
drawing style, we show how to draw an upward planar phylogenetic network with
two slopes such that all leaves lie on a horizontal line.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02839"><span class="datestr">at June 08, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02812">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02812">Optimizing Ansatz Design in QAOA for Max-cut</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Majumdar:Ritajit.html">Ritajit Majumdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Madan:Dhiraj.html">Dhiraj Madan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhoumik:Debasmita.html">Debasmita Bhoumik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinayagamurthy:Dhinakaran.html">Dhinakaran Vinayagamurthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghunathan:Shesha.html">Shesha Raghunathan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sur=Kolay:Susmita.html">Susmita Sur-Kolay</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02812">PDF</a><br /><b>Abstract: </b>Quantum Approximate Optimization Algorithm (QAOA) has been studied widely in
the literature, primarily for finding an approximate value of the maximum cut
size of a graph. QAOA is composed of a problem hamiltonian and a mixer
hamiltonian which are applied alternately for $p \geq 1$ layers. The circuit
for this algorithm requires $2m$ CNOT gates in each layer, where $m$ is the
number of edges in the graph. CNOT gate is one of the primary sources of error
in modern quantum computers. In this paper, we propose two techniques for
reducing the number of CNOT gates in the circuit which are independent of the
hardware architecture. For a graph with $n$ vertices, we first propose a
technique based on edge coloring that can reduce upto $\lfloor \frac{n}{2}
\rfloor$ CNOT gates in the circuit. Next, we propose another technique based on
Depth First Search (DFS) that can reduce $n-1$ CNOT gates at the cost of some
increased depth. We analytically derive the criteria for which the reduction in
the number of CNOT gates due to the DFS based technique can provide lower error
probability even with some increased depth, and show that all graphs conform to
this criteria, making this technique universal. We further show that this
proposed optimization holds even in the post transpilation stage of the
circuit, which is actually executed in the IBM Quantum hardware. We simulate
these two techniques for graphs of various sparsity with the ibmq_manhattan
noise model and show that the DFS based technique outperforms the edge coloring
based technique, which in turn, outperforms the traditional QAOA circuit in
terms of reduction in the number of CNOT gates, and hence the probability of
error of the circuit.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02812"><span class="datestr">at June 08, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02774">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02774">Sparsification for Sums of Exponentials and its Algorithmic Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Allen.html">Allen Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02774">PDF</a><br /><b>Abstract: </b>Many works in signal processing and learning theory operate under the
assumption that the underlying model is simple, e.g. that a signal is
approximately $k$-Fourier-sparse or that a distribution can be approximated by
a mixture model that has at most $k$ components. However the problem of fitting
the parameters of such a model becomes more challenging when the
frequencies/components are too close together.
</p>
<p>In this work we introduce new methods for sparsifying sums of exponentials
and give various algorithmic applications. First we study Fourier-sparse
interpolation without a frequency gap, where Chen et al. gave an algorithm for
finding an $\epsilon$-approximate solution which uses $k' = \mbox{poly}(k, \log
1/\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in
one dimension without a separation condition. Kernel density estimators give an
$\epsilon$-approximation that uses $k' = O(k/\epsilon^2)$ components. These
methods both output models that are much more complex than what we started out
with. We show how to post-process to reduce the number of
frequencies/components down to $k' = \widetilde{O}(k)$, which is optimal up to
logarithmic factors. Moreover we give applications to model selection. In
particular, we give the first algorithms for approximately (and robustly)
determining the number of components in a Gaussian mixture model that work
without a separation condition.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02774"><span class="datestr">at June 08, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02762">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02762">Faster and Generalized Temporal Triangle Counting, via Degeneracy Ordering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pashanasangi:Noujan.html">Noujan Pashanasangi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02762">PDF</a><br /><b>Abstract: </b>Triangle counting is a fundamental technique in network analysis, that has
received much attention in various input models. The vast majority of triangle
counting algorithms are targeted to static graphs. Yet, many real-world graphs
are directed and temporal, where edges come with timestamps. Temporal triangles
yield much more information, since they account for both the graph topology and
the timestamps.
</p>
<p>Temporal triangle counting has seen a few recent results, but there are
varying definitions of temporal triangles. In all cases, temporal triangle
patterns enforce constraints on the time interval between edges (in the
triangle). We define a general notion $(\delta_{1,3}, \delta_{1,2},
\delta_{2,3})$-temporal triangles that allows for separate time constraints for
all pairs of edges.
</p>
<p>Our main result is a new algorithm, DOTTT (Degeneracy Oriented Temporal
Triangle Totaler), that exactly counts all directed variants of $(\delta_{1,3},
\delta_{1,2}, \delta_{2,3})$-temporal triangles. Using the classic idea of
degeneracy ordering with careful combinatorial arguments, we can prove that
DOTTT runs in $O(m\kappa\log m)$ time, where $m$ is the number of (temporal)
edges and $\kappa$ is the graph degeneracy (max core number). Up to log
factors, this matches the running time of the best static triangle counters.
Moreover, this running time is better than existing.
</p>
<p>DOTTT has excellent practical behavior and runs twice as fast as existing
state-of-the-art temporal triangle counters (and is also more general). For
example, DOTTT computes all types of temporal queries in Bitcoin temporal
network with half a billion edges in less than an hour on a commodity machine.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02762"><span class="datestr">at June 08, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02755">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02755">Kernel approximation on algebraic varieties</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Altschuler:Jason_M=.html">Jason M. Altschuler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parrilo:Pablo_A=.html">Pablo A. Parrilo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02755">PDF</a><br /><b>Abstract: </b>Low-rank approximation of kernels is a fundamental mathematical problem with
widespread algorithmic applications. Often the kernel is restricted to an
algebraic variety, e.g., in problems involving sparse or low-rank data. We show
that significantly better approximations are obtainable in this setting: the
rank required to achieve a given error depends on the variety's dimension
rather than the ambient dimension, which is typically much larger. This is true
in both high-precision and high-dimensional regimes. Our results are presented
for smooth isotropic kernels, the predominant class of kernels used in
applications. Our main technical insight is to approximate smooth kernels by
polynomial kernels, and leverage two key properties of polynomial kernels that
hold when they are restricted to a variety. First, their ranks decrease
exponentially in the variety's co-dimension. Second, their maximum values are
governed by their values over a small set of points. Together, our results
provide a general approach for exploiting (approximate) "algebraic structure"
in datasets in order to efficiently solve large-scale data science problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02755"><span class="datestr">at June 08, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02703">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02703">Dissipative search of an unstructured database</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allahverdyan:Armen_E=.html">Armen E. Allahverdyan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Petrosyan:David.html">David Petrosyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02703">PDF</a><br /><b>Abstract: </b>The search of an unstructured database amounts to finding one element having
a certain property out of $N$ elements. The classical search with an oracle
checking one element at a time requires on average $N/2$ steps. The Grover
algorithm for the quantum search, and its unitary Hamiltonian evolution
analogue, accomplish the search asymptotically optimally in $\mathcal{O}
(\sqrt{N})$ time steps. We reformulate the search problem as a dissipative
Markov process acting on an $N$-level system weakly coupled to a thermal bath.
Assuming that the energy levels of the system represent the database elements,
we show that, with a proper choice of the spectrum and physically admissible,
long-range transition rates between the energy levels, the system relaxes to
the ground state, corresponding to the sought element, in time $\mathcal{O}
(\ln N)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02703"><span class="datestr">at June 08, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02685">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02685">Massively Parallel and Dynamic Algorithms for Minimum Size Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epasto:Alessandro.html">Alessandro Epasto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahdian:Mohammad.html">Mohammad Mahdian</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Peilin.html">Peilin Zhong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02685">PDF</a><br /><b>Abstract: </b>In this paper, we study the $r$-gather problem, a natural formulation of
minimum-size clustering in metric spaces. The goal of $r$-gather is to
partition $n$ points into clusters such that each cluster has size at least
$r$, and the maximum radius of the clusters is minimized. This additional
constraint completely changes the algorithmic nature of the problem, and many
clustering techniques fail. Also previous dynamic and parallel algorithms do
not achieve desirable complexity. We propose algorithms both in the Massively
Parallel Computation (MPC) model and in the dynamic setting. Our MPC algorithm
handles input points from the Euclidean space $\mathbb{R}^d$. It computes an
$O(1)$-approximate solution of $r$-gather in $O(\log^{\varepsilon} n)$ rounds
using total space $O(n^{1+\gamma}\cdot d)$ for arbitrarily small constants
$\varepsilon,\gamma &gt; 0$. In addition our algorithm is fully scalable, i.e.,
there is no lower bound on the memory per machine. Our dynamic algorithm
maintains an $O(1)$-approximate $r$-gather solution under insertions/deletions
of points in a metric space with doubling dimension $d$. The update time is $r
\cdot 2^{O(d)}\cdot \log^{O(1)}\Delta$ and the query time is $2^{O(d)}\cdot
\log^{O(1)}\Delta$, where $\Delta$ is the ratio between the largest and the
smallest distance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02685"><span class="datestr">at June 08, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02680">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02680">How to Decompose a Tensor with Group Structure</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Allen.html">Allen Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02680">PDF</a><br /><b>Abstract: </b>In this work we study the orbit recovery problem, which is a natural
abstraction for the problem of recovering a planted signal from noisy
measurements under unknown group actions. Many important inverse problems in
statistics, engineering and the sciences fit into this framework. Prior work
has studied cases when the group is discrete and/or abelian. However
fundamentally new techniques are needed in order to handle more complex group
actions.
</p>
<p>Our main result is a quasi-polynomial time algorithm to solve orbit recovery
over $SO(3)$ - i.e. the cryo-electron tomography problem which asks to recover
the three-dimensional structure of a molecule from noisy measurements of
randomly rotated copies of it. We analyze a variant of the frequency marching
heuristic in the framework of smoothed analysis. Our approach exploits the
layered structure of the invariant polynomials, and simultaneously yields a new
class of tensor decomposition algorithms that work in settings when the tensor
is not low-rank but rather where the factors are algebraically related to each
other by a group action.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02680"><span class="datestr">at June 08, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8127">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/06/07/new-seminar-series-in-simons-institute/">New seminar series in Simons Institute</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons institute started a <a href="https://simons.berkeley.edu/news/institute-launches-breakthroughs-lecture-series">new virtual seminar</a> series highlighting recent advances in theoretical computer science. The first two talks in the series will be:</p>



<ul><li>June 16th 10am-11am PDT (1pm-2pm EDT). Virginia Vassilevska Williams on a  <a href="https://simons.berkeley.edu/events/breakthroughs-refined-laser-method-and-faster-matrix-multiplication">Refined Laser Method and Faster Matrix Multiplication</a></li><li>August 5 10am-11am PDT (1pm-2pm EDT) Yuansi Chen on <a href="https://simons.berkeley.edu/events/breakthroughs-almost-constant-lower-bound-isoperimetric-coefficient-kls-conjecture-0">An Almost Constant Lower Bound of the Isoperimetric Coefficient in the KLS Conjecture</a></li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/06/07/new-seminar-series-in-simons-institute/"><span class="datestr">at June 07, 2021 06:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/icml2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/icml2021/">Conference Digest - ICML 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://icml.cc/Conferences/2021">ICML 2021</a>, one of the biggest conferences in machine learning, naturally has a ton of interesting sounding papers on the topic of differential privacy.
We went through this year’s <a href="https://icml.cc/Conferences/2021/AcceptedPapersInitial">accepted papers</a> and aggregated all the relevant papers we could find.
In addition, this year features three workshops on the topic of privacy, as well as a tutorial.
As always, please inform us if we overlooked any papers on differential privacy.</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li>
    <p><a href="http://federated-learning.org/fl-icml-2021/">Federated Learning for User Privacy and Data Confidentiality</a></p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/ml4data">Machine Learning for Data: Automated Creation, Privacy, Bias</a></p>
  </li>
  <li>
    <p><a href="https://tpdp.journalprivacyconfidentiality.org/2021/">Theory and Practice of Differential Privacy</a></p>
  </li>
</ul>

<h2 id="tutorial">Tutorial</h2>

<ul>
  <li><a href="https://icml.cc/Conferences/2021/Schedule?showEvent=10839">Privacy in Learning: Basics and the Interplay</a><br />
<a href="https://www.microsoft.com/en-us/research/people/huzhang/">Huishuai Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2009.02668">A Framework for Private Matrix Analysis in Sliding Window Model</a><br />
<a href="https://sites.google.com/view/jalajupadhyay/home">Jalaj Upadhyay</a>, <a href="https://www.fujitsu.com/us/about/businesspolicy/tech/rd/research-staff/sarvagya.html">Sarvagya Upadhyay</a></p>
  </li>
  <li>
    <p>Accuracy, Interpretability, and Differential Privacy via Explainable Boosting<br />
<a href="https://scholar.google.com/citations?user=HmxjgMAAAAAJ">Harsha Nori</a>, <a href="https://www.microsoft.com/en-us/research/people/rcaruana/">Rich Caruana</a>, <a href="https://sites.google.com/view/zhiqi-bu">Zhiqi Bu</a>, <a href="https://heyyjudes.github.io/">Judy Hanwen Shen</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a></p>
  </li>
  <li>
    <p>Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message<br />
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, <a href="https://rasmuspagh.net/">Rasmus Pagh</a>, <a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2011.00467">Differentially Private Bayesian Inference for Generalized Linear Models</a><br />
<a href="https://warwick.ac.uk/fac/sci/dcs/people/u1554597">Tejas Kulkarni</a>, <a href="https://users.aalto.fi/~jalkoj1/">Joonas Jälkö</a>, <a href="https://scholar.google.com/citations?user=Y_EvCPAAAAAJ">Antti Koskela</a>, <a href="https://people.aalto.fi/samuel.kaski">Samuel Kaski</a>, <a href="https://www.cs.helsinki.fi/u/ahonkela/">Antti Honkela</a></p>
  </li>
  <li>
    <p>Differentially-Private Clustering of Easy Instances<br />
<a href="http://www.cohenwang.com/edith/">Edith Cohen</a>, <a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a>, <a href="https://www.linkedin.com/in/eliad-tsfadia-21482b96/">Eliad Tsfadia</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08885">Differentially Private Correlation Clustering</a><br />
<a href="https://cs-people.bu.edu/mbun/">Mark Bun</a>, <a href="https://elias.ba30.eu/">Marek Elias</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.13287">Differentially Private Densest Subgraph Detection</a><br />
<a href="https://biocomplexity.virginia.edu/person/dung-nguyen">Dung Nguyen</a>, <a href="https://engineering.virginia.edu/faculty/anil-vullikanti">Anil Vullikanti</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08244">Differentially Private Quantiles</a><br />
<a href="http://jgillenw.com/">Jennifer Gillenwater</a>, <a href="https://www.majos.net/">Matthew Joseph</a>, <a href="https://www.alexkulesza.com/">Alex Kulesza</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.06641">Differentially Private Query Release Through Adaptive Projection</a><br />
<a href="https://sergulaydore.github.io/">Sergul Aydore</a>, <a href="https://wibrown.github.io/">William Brown</a>, <a href="https://www.cis.upenn.edu/~mkearns/">Michael Kearns</a>, <a href="http://www-cs-students.stanford.edu/~kngk/">Krishnaram Kenthapadi</a>, <a href="https://www.lucamel.is/">Luca Melis</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, <a href="https://ankitsiva.xyz/">Ankit Siva</a></p>
  </li>
  <li>
    <p>Differentially Private Sliced Wasserstein Distance<br />
<a href="http://asi.insa-rouen.fr/enseignants/~arakoto/">Alain Rakotomamonjy</a>, <a href="https://pageperso.lif.univ-mrs.fr/~liva.ralaivola/doku.php">Liva Ralaivola</a></p>
  </li>
  <li>
    <p>Large Scale Private Learning via Low-rank Reparametrization<br />
<a href="https://scholar.google.com/citations?user=FcRGdiwAAAAJ">Da Yu</a>, <a href="https://www.microsoft.com/en-us/research/people/huzhang/">Huishuai Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a>, Jian Yin, <a href="https://www.microsoft.com/en-us/research/people/tyliu/">Tie-Yan Liu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08598">Leveraging Public Data for Practical Private Query Release</a><br />
<a href="https://www.linkedin.com/in/terrance-liu-26796974/">Terrance Liu</a>, <a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://zstevenwu.com/">Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.09734">Locally Private k-Means in One Round</a><br />
Alisa Chang, <a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.12099">Lossless Compression of Efficient Private Local Randomizers</a><br />
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.08233">Oneshot Differentially Private Top-k Selection</a><br />
<a href="https://lsa.umich.edu/stats/people/phd-students/qiaogang.html">Gang Qiao</a>, <a href="http://www-stat.wharton.upenn.edu/~suw/">Weijie Su</a>, <a href="https://research.google/people/LiZhang/">Li Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.12321">PAPRIKA: Private Online False Discovery Rate Control</a><br />
<a href="https://wanrongz.github.io/">Wanrong Zhang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.00039">Practical and Private (Deep) Learning without Sampling or Shuffling</a><br />
<a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://research.google/people/author35837/">Brendan McMahan</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="http://www.omthakkar.com/">Om Thakkar</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a>, <a href="https://research.google/people/106689/">Zheng Xu</a></p>
  </li>
  <li>
    <p>Private Adaptive Gradient Methods for Convex Optimization<br />
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="https://web.stanford.edu/~jduchi/">John Duchi</a>, <a href="https://afallah.lids.mit.edu/">Alireza Fallah</a>, <a href="https://scholar.google.com/citations?user=_JXjrEp9FhYC">Omid Javidbakht</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p>Private Alternating Least Squares: (Nearly) Optimal Privacy/Utility Trade-off for Matrix Completion<br />
Steve Chien, <a href="https://www.prateekjain.org/">Prateek Jain</a>, <a href="http://walid.krichene.net/">Walid Krichene</a>, <a href="https://scholar.google.com/citations?user=yR-ugIoAAAAJ">Steffen Rendle</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a>, <a href="https://research.google/people/LiZhang/">Li Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.01516">Private Stochastic Convex Optimization: Optimal Rates in L1 Geometry</a><br />
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06387">The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation</a><br />
<a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://kenziyuliu.github.io/">Ziyu Liu</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/icml2021/"><span class="datestr">at June 07, 2021 04:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=857">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/">Data-structure lower bounds without encoding arguments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>                        </p>
<p style="text-align: justify;">I have recently posted the paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#Xviola-rank-samp">Vio21</a>]</span> (<a href="https://eccc.weizmann.ac.il/report/2021/073/">download</a>) which does something that I have been trying to do for a long time, more than ten years, on and off. Consider the basic data-structure problem of storing <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m" class="latex" /> bits of data <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bm%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="x\in \{0,1\}^{m}" class="latex" /> into <img src="https://s0.wp.com/latex.php?latex=m%2Br&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m+r" class="latex" /> bits so that the <em>prefix-sum queries</em></p>
<div style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+%7B%5Ctext+%7B%5Ctextsc+%7BRank%7D%7D%7D%28i%29%3A%3D%5Csum+_%7Bj%5Cle+i%7Dx_%7Bj%7D+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb {\text {\textsc {Rank}}}(i):=\sum _{j\le i}x_{j} \end{aligned}" class="latex" /></div>
<p style="text-align: justify;">   can be computed by probing <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> <em>cells</em> (or <em>words</em>) of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w" class="latex" /> bits each. (You can think <img src="https://s0.wp.com/latex.php?latex=w%3D%5Clog+m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w=\log m" class="latex" /> throughout this post.) The paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> with Pǎtraşcu shows that <img src="https://s0.wp.com/latex.php?latex=r%5Cge+m%2Fw%5E%7BO%28q%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r\ge m/w^{O(q)}" class="latex" />, and this was recently shown to be tight by Yu <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/conf/stoc/Yu19">Yu19</a>]</span> (building on the breakthrough data structure <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascu08Succincter">Pǎt08</a>]</span> which motivated the lower bound and is not far from it).</p>
<p style="text-align: justify;">   As is common in data-structure lower bounds, the proof in <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> is an <em>encoding argument</em>. In the recently posted paper, an alternative proof is presented which avoids the encoding argument and is perhaps more in line with other proofs in complexity lower bounds. Of course, <em>everything</em> is an encoding argument, and <em>nothing </em>is an encoding argument, and this post won’t draw a line.</p>
<p style="text-align: justify;">   The new proof establishes an <em>intrinsic property</em> of efficient data structures, whereas typical proofs including <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> are somewhat tailored to the problem at hand. The property is called the <em>separator</em> and is a main technical contribution of the work. At the high level the separator shows that in any efficient data structure you can restrict the input space a little so that many queries are nearly <em>pairwise independent</em>.</p>
<p style="text-align: justify;">   Also, the new proof rules out a stronger object: a <em>sampler</em> (<a href="https://emanueleviola.wordpress.com/2014/11/09/is-nature-a-low-complexity-sampler/">see previous post here</a> on sampling lower bounds). Specifically, the distribution Rank<img src="https://s0.wp.com/latex.php?latex=%28U%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="(U)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="U" class="latex" /> is the uniform distribution cannot be sampled, not even slightly close, by an efficient cell-probe algorithm. This implies the data-structure result, and it can be informally interpreted as saying that the “reason” why the lower bound holds is not that the data is compressed, but rather that one can’t generate the type of dependencies occurring in Rank via an efficient cell-probe algorithm, regardless of what the input is.</p>
<p style="text-align: justify;">   Building on this machinery, one can prove several results about sampling, like showing that cell-probe samplers are strictly weaker than AC0 samplers. While doing this, it occurred to me that one gets a corollary for data structures which I had not seen in the literature. The corollary is a <em>probe hierarchy</em>, showing that some problem can be solved with zero redundancy (<img src="https://s0.wp.com/latex.php?latex=r%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r=0" class="latex" />) with <img src="https://s0.wp.com/latex.php?latex=O%28q%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="O(q)" class="latex" /> probes, while it requires almost linear <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> probes. For example I don’t know of a result yielding this for small <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> such as <img src="https://s0.wp.com/latex.php?latex=q%3DO%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q=O(1)" class="latex" />; I would appreciate a reference. (As mentioned in the                                                                                                                                                        paper, the sampling viewpoint is not essential and just like for Rank one can prove the data-structure corollaries directly. Personally, and obviously, I find the sampling viewpoint useful.)</p>
<p style="text-align: justify;">   One of my favorite open problems in the area still is: can a uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[m]" class="latex" /> be approximately sampled by an efficient cell-probe algorithm? I can’t even rule out samplers making <em>two </em>probes!</p>
<h3 class="likesectionHead"><a id="x1-1000"></a>References</h3>
<p style="text-align: justify;">
</p><div class="thebibliography">
<p class="bibitem"><span class="biblabel">  [Pǎt08]<span class="bibsp">   </span></span><a id="XPatrascu08Succincter"></a>Mihai  Pǎtraşcu.      Succincter.      In  49th  IEEE  Symp. on         Foundations of Computer Science (FOCS). IEEE, 2008.</p>
<p class="bibitem"><span class="biblabel">  [PV10]<span class="bibsp">   </span></span><a id="XPatrascuV10"></a>Mihai Pǎtraşcu and Emanuele Viola.  Cell-probe lower bounds         for succinct partial sums.  In 21th ACM-SIAM Symp. on Discrete         Algorithms (SODA), pages 117–122, 2010.</p>
<p class="bibitem"><span class="biblabel">  [Vio21]<span class="bibsp">   </span></span><a id="Xviola-rank-samp"></a>Emanuele       Viola.                    Lower       bounds       for         samplers and data structures via the cell-probe separator. Available         at <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>, 2021.</p>
<p class="bibitem"><span class="biblabel">  [Yu19] <span class="bibsp">   </span></span><a id="XDBLP:conf/stoc/Yu19"></a>Huacheng  Yu.     Optimal  succinct  rank  data  structure  via         approximate nonnegative tensor decomposition.  In Moses Charikar         and Edith Cohen, editors, ACM Symp. on the Theory of Computing         (STOC), pages 955–966. ACM, 2019.</p>
</div></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/"><span class="datestr">at June 07, 2021 03:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18838">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/">Happy Mom’s Day</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>You know, I loved math. My mom was a math teacher—Joan Cusack</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/kfrlmothers/" rel="attachment wp-att-18865"><img width="212" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/KFRLmothers.jpg?resize=212%2C139&amp;ssl=1" class="alignright size-full wp-image-18865" height="139" /></a></p>
<p>
Mary Kay Farley, my dear wife’s mom, and Dorothy Lipton, my mom, have unfortunately both passed away. Kathryn and I miss them greatly. Both women shared keen mathematical skills, a fascination with the game of baseball and a commitment to living a well-ordered life.  </p>
<p>
Today is <b>not</b> Mother’s Day. We still hope all mothers everywhere are enjoying their day.</p>
<p>
We will take this time to thank all of you out there. We missed doing so last month, but the pandemic has distended time anyway. What we are hearing now are stories of mothers and children and grandchildren finally being able to think of seeing each other in person rather than via video.</p>
<p>
</p><p></p><h2> Another Kind of Parentage </h2><p></p>
<p></p><p>
This has blended with musings on our recent <a href="https://rjlipton.wpcomstaging.com/2021/03/26/congrats-avi-and-laci-on-the-abel-prize/">post</a> in which I (Dick) noted that Dorit Aharonov is an academic grandchild of mine, in that Avi Wigderson co-supervised her doctoral thesis and I supervised Avi’s. </p>
<p>
Years ago we featured on Father’s Day a <a href="https://rjlipton.wpcomstaging.com/2011/06/19/who's-your-doktorvater/">post</a> with the title “Who’s Your <em>Doktorvater</em>?”—which was a play on the <a href="https://bosoxinjection.com/2014/09/24/ten-years-gone-pedro-martinez-calls-yankees-daddy/">expression</a> “who’s your daddy?” Now it is high time to note that there are many “doctor mothers”—as Dorit has herself <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=102209">become</a>. </p>
<p>
One difference from human genealogy is that most often there is only one “doctor parent.” My <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=86340&amp;fChrono=1">advisor</a>, David Parnas, has two: Alan Perlis and Everard Williams. From Perlis it is a straight shot back to Siméon Poisson, whose 1800 dissertation was co-advised by Joseph Lagrange and Pierre Laplace. For Lagrange there is a strange <a href="https://www.mathgenealogy.org/id.php?id=17864">note</a> of Leonhard Euler as a virtual advisor, but the real one is Giovanni Beccaria—who has no listed parent. Going through Laplace also dead-ends. But selecting Euler includes a chain that ends in the 1100s with Sharaf al-Dīn al-Ṭūsī, who <a href="https://en.wikipedia.org/wiki/Sharaf_al-Din_al-Tusi#Mathematics">improved</a> the complexity of approximately solving cubic equations.</p>
<p>
I appear not to have any female ancestors in my doctoral genealogy. I have two female PhD graduates, one of whom is a <em>Doktormutter</em>. Ken’s first female doctoral student, co-advised, had a successful thesis defense last week; he has another nearing the ABD stage. But I have known quite a few other “doctor mothers” personally. Today, Ken and I thought to recognize them.</p>
<p>
</p><p></p><h2> Some Doctor Moms I Know </h2><p></p>
<p></p><p>
Here are some that I have had the honor to know. They are in a certain order—do you see what it is? I give only the surname on purpose—click the second name and note its URL for a singular reflection of this.</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/Monica_S._Lam">Lam</a>: <a href="https://mathgenealogy.org/id.php?id=50307">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Blum/">Blum</a>: <a href="http://www.cs.cmu.edu/~lblum/PAPERS/lblumShortVita.pdf">co-advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Mary_Shaw_(computer_scientist)">Shaw</a>: <a href="https://www.mathgenealogy.org/id.php?id=50083">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Chung/">Chung</a>: <a href="https://www.mathgenealogy.org/id.php?id=23154">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Maria_Klawe">Klawe</a>: <a href="https://www.mathgenealogy.org/id.php?id=43243">advisee</a> <p></p>
</li><li>
<a href="https://people.csail.mit.edu/lynch/">Lynch</a>: <a href="https://www.mathgenealogy.org/id.php?id=81227">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Ruzena_Bajcsy">Bajcsy</a>: <a href="https://www.mathgenealogy.org/id.php?id=39957">advisees</a> <p></p>
</li><li>
<a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/graham-s.html">Graham</a>: <a href="https://www.mathgenealogy.org/id.php?id=22787">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Barbara_Liskov">Liskov</a>: <a href="https://www.mathgenealogy.org/id.php?id=61932">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Eva_Tardos">Tardos</a>: <a href="https://www.mathgenealogy.org/id.php?id=39422">advisees</a> <p></p>
</li><li>
<a href="https://annacgilbert.github.io/">Gilbert</a>: <a href="https://www.mathgenealogy.org/id.php?id=24150">advisees</a> <p></p>
</li><li>
<a href="https://people.math.gatech.edu/~randall/">Randall</a>: <a href="https://www.mathgenealogy.org/id.php?id=81757">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Rasiowa/">Rasiowa</a>: <a href="https://www.mathgenealogy.org/id.php?id=22601">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Sheila_Greibach">Greibach</a>: <a href="https://www.mathgenealogy.org/id.php?id=25274">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Shafi_Goldwasser">Goldwasser</a>: <a href="https://www.mathgenealogy.org/id.php?id=35879">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Daubechies/">Daubechies</a>: <a href="https://www.mathgenealogy.org/id.php?id=44561">advisees</a>
</li></ul>
<p>
The last gives us an all-female tree, not just one branch, of people we know. Besides Anna Gilbert, another of Ingrid Daubechies’s students, who herself has <a href="https://www.mathgenealogy.org/id.php?id=92059">advisees</a>, is Cynthia Rudin of Duke, whom Ken knew and taught while she was an undergraduate at Buffalo.</p>
<p>
There are others I could mention who went into research labs where there are different relationships besides PhD advising. They include Irene Greif, Tal Rabin, Lynn Conway, and Jean Sammet. I could include Jamie Morgenstern, whom we recently <a href="https://rjlipton.wpcomstaging.com/2021/03/10/making-algorithms-fair/">featured</a> and who his <a href="https://jamiemorgenstern.com/">advising</a> her first students at the University of Washington—do they have to be “born” yet to count you as a <em>Doktormutter</em>? I’ve left others out—apologies for that—but the ones I’ve listed make a nice <img src="https://s0.wp.com/latex.php?latex=%7B4+%5Ctimes+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4 \times 4}" class="latex" /> collage:</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/comoms2/" rel="attachment wp-att-18841"><img width="450" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/comoms2.png?resize=450%2C486&amp;ssl=1" class="aligncenter size-full wp-image-18841" height="486" /></a></p>
<p>
</p><p></p><h2> My Upbringing </h2><p></p>
<p></p><p>
Of these, the one with the most formative impact on me was Helena Rasiowa. I learned advanced logic from her when I was an undergraduate. </p>
<p>
Here is a tribute to <a href="http://comet.lehman.cuny.edu/fitting/bookspapers/pdf/papers/Rasiowa.pdf">her</a> by Melvin Fitting: </p>
<blockquote><p><b> </b> <em> I once heard Dana Scott criticize her <a href="https://www.amazon.com/Mathematics-Metamathematics-Helena-SIKORSKI-RASIOWA/dp/B005JGKZXW">book</a>, <em>The Mathematics of Metamathematics</em> with Roman Sikorski, because, while it took an algebraic approach to logic, it did not carry the work further and consider set theory. If it had, then <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">forcing</a> would have been discovered years earlier than it was. This is not, at heart, a criticism, but a tribute. The building of mathematics always goes on. Foundations, firmly laid, enable later construction, and the foundations laid by that book were powerfully firm. </em>
</p></blockquote>
<p>
Ken also points to logic as a formative influence—though from men at Oxford.  Both of us were attracted to Gödel-type undecidability issues in complexity theory in the early 1980s.  The nexus of logic and algebra has been important to us in different ways, Ken more with finite automata and descriptive complexity.  Courses in logic gave both of us a habit of framing problems along formal lines.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Which “doctor mothers” have you known or been influenced by?</p>
<p></p><p><br />
[Added and re-formatted photos at top]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/"><span class="datestr">at June 07, 2021 04:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">Workshop on Machine Learning for Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 13-14, 2021 Foundations of Data Science Institute (FODSI) https://fodsi.us/ml4a.html In recent years there has been increasing interest in using machine learning to improve the performance of classical algorithms in computer science, by fine-tuning their behavior to adapt to the properties of the input distribution. This “data-driven” or “learning-based” approach to algorithm design has the … <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">Workshop on Machine Learning for Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/"><span class="datestr">at June 07, 2021 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-562506050674619397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/when-do-you-use-et-al-as-opposed-to.html">When do you use et al. as opposed to listing out the authors? First names? Middle initials? Jr?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> If I was refering to the paper with bibtex entry: </p><p><br /></p><p>@misc{BCDDL-2018,</p><p>  author    = {Jeffrey Bosboom and</p><p>               Spencer Congero and</p><p>               Erik D. Demaine and</p><p>               Martin L. Demaine and</p><p>               Jayson Lynch},</p><p>  title     = {Losing at Checkers is Hard},</p><p>  year      = {2018},</p><p>  note      = {\newline\url{http://arxiv.org/abs/1806.05657}},</p><p>}</p><p>(The paper is <a href="http://arxiv.org/abs/1806.05657">here</a>.)</p><p>I would write </p><p><i>Bosboom et al.~\cite{BCDDL-2018} proved that trying to lose at checkers (perhaps you are playing a child and want to keep up their self-esteem, or a Wookie and don't want your arms to be torn off your shoulders   (see <a href="https://www.starwars.com/video/let-the-wookiee-win">here</a>),  or a Wookie child) is hard. </i></p><p><br /></p><p>Why did I use<i> et al. </i>?<i> </i>Because it would be a pain to write out all of those names. </p><p>How many names does it take to make you write <i>et al. </i>? Are there exceptions? </p><p>I have not seen any discussion of this point on the web. So here are my rules of thumb and some questions.(CORRECTION- a commenter points out that this IS discussed on the web. Even so, you can comment on my thoughts or give your thoughts or whatever you like.) </p><p>1) If  there are 3 or more people than use et al. Exception: If the three are VERY WELL KNOWN as a triple. E.g., <i>the double play was Tinker to Evers to Chance. </i>Well, that would not really come up since in baseball nobody ever says <i>the double play was Tinker et al.</i>  More relevant examples:</p><p>Aho, Hopcroft, and Ullman</p><p>Cormen, Leiserson, and Rivest, also known as CLR</p><p>The more recent edition is</p><p>Cormen, Leiserson, Rivest, and Stein. I have heard CLRS but I don't know if people WRITE all four names. </p><p>Lenstra-Lenstra-Lovasz also usually mentions all three. </p><p>2) If there is one name do not use <i>et al</i>.  unless that one person has a multiple personality disorder.</p><p>3) If there are 2 people it can be tricky and perhaps unfair. If the second one has a long name then I am tempted to use<i> et al.</i> For example</p><p>Lewis and Papadimitriou (If I mispelled Christos's name-- well- that's  the point!- to avoid spelling errors I want to use <i>et al.</i> )</p><p>Lampropoulos and Paraskevopoulou (the first one is UMCP new faculty!). After typing in the first name I would not be in the mood to type in the second. </p><p>Similar if there are lots of accents in the name making it hard to type in LaTeX (though I have macros for some people like Erdos who come up a lot) then I might use<i> et al. </i></p><p>(ADDED LATER- some of the commenters object to my `rule' of leaving out the last name if its complicated. That is not really my rule- the point of this post was to get a discussion going about the issue, which I am happy to say has happened.) </p><p>----------</p><p>There are other issues along these lines: when to include the first name (when there is more than one person with that last name, e.g. Ryan Williams and Virginia  Vassilevska Williams), when to use middle initials (in the rare case where there is someone with the same first and last name- Michael  J. Fox added the J and uses it since there was an actor named Michael Fox.)</p><p>I will soon give a quote from a math paper that amused me, but first some context.  The problem of determining if a poly in n variables over Z has an integer solution is called E(n). By the solution to Hilbert's 10th problem we know that there exists n such that E(n) is undecidable. E(9) is undecidable, but the status of E(8) is unknown (as of May 2021) and has been since the early 1980's. </p><p>Here is the quote (from <a href="http://maths.nju.edu.cn/~zwsun/14z.pdf">here</a>).</p><p><i>Can we replace 9 by a smaller number? It is believed so. In fact, A. Baker, Matiyasevich and J.Robinson  even conjectured that E(3) is undecidable over N.</i></p><p>Note that Baker and Robinson get their first initial but Matiyasevich does not.</p><p>I suspect that they use J. Robinson since there is another mathematician with last name Robinson: Rafael Robinson who was Julia's Robinson's husband (to my younger readers--- there was a time when a women who got married took her husband's last name). There is at least one other Math-Robinson: Robert W Robinson. I do not think he is closely related. </p><p>Baker: I do not know of another mathematician named Baker. I tried Google, but the Bakers  I found were   not in the right time frame. I also kept finding hits to an anecdote about Poincare and a man whose profession was a baker (see <a href="https://gilkalai.wordpress.com/2019/10/13/the-story-of-poincare-and-his-friend-the-baker/">here</a> though its later in that blog post). However, I suspect there was another mathematician named Baker which is why the author uses the first initial.  Its possible the author did not want to confuse Alan  Baker with Theodore Baker, one of the authors of Baker-Gill-Solovay that showed there were oracles that made P = NP and others that made P NE NP.  But somehow, that just doesn't seem right to me. I suspect there is only one mathematician with last name Matijsavic. </p><p>Thomas Alva Edison named his son Thomas Alva Edison Jr.  This was a bad idea but not for reasons of authorship, see <a href="http://edisontinfoil.com/taejr/edisonjr.htm">here</a>.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/when-do-you-use-et-al-as-opposed-to.html"><span class="datestr">at June 07, 2021 03:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/077">TR21-077 |  Lower Bounds on Stabilizer Rank | 

	Shir Peleg, 

	Amir Shpilka, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The stabilizer rank of a quantum state $\psi$ is the minimal $r$ such that $\left| \psi \right \rangle = \sum_{j=1}^r c_j \left|\varphi_j \right\rangle$ for $c_j \in \mathbb{C}$ and stabilizer states $\varphi_j$. The running time of several classical simulation methods for quantum circuits is determined by the stabilizer rank of the $n$-th tensor power of single-qubit magic states.

We prove a lower bound of $\Omega(n)$ on the stabilizer rank of such states, improving a previous lower bound of $\Omega(\sqrt{n})$ of Bravyi, Smith and Smolin [BSS16]. Further, we prove that for a sufficiently small constant $\delta$, the stabilizer rank of any state which is $\delta$-close to those states is $\Omega(\sqrt{n}/\log n)$. This is the first non-trivial lower bound for approximate stabilizer rank.

Our techniques rely on the representation of stabilizer states as quadratic functions over affine subspaces of $\mathbb{F}_2^n$, and we use tools from analysis of boolean functions and complexity theory. The proof of the first result involves a careful analysis of directional derivatives of quadratic polynomials, whereas the proof of the second result uses Razborov-Smolensky low degree polynomial approximations and correlation bounds against the majority function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/077"><span class="datestr">at June 06, 2021 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/076">TR21-076 |  Pseudorandom Generators, Resolution and Heavy Width | 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Following the paper of Alekhnovich, Ben-Sasson, Razborov, Wigderson \cite{ABRW04} we call a pseudorandom generator $\mathrm{PRG}\colon \{0, 1\}^n \to \{0, 1\}^m$ hard for for a propositional proof system $\mathrm{P}$ if $\mathrm{P}$ cannot efficiently prove the (properly encoded) statement $b \notin \mathrm{Im}(\mathrm{PRG})$ for any string $b \in \{0, 1\}^m$.

In \cite{ABRW04} authors suggested the ``functional encoding'' of considered statement for Nisan--Wigderson generator that allows the introduction of ``local'' extension variables. These extension variables may potentially significantly increase the power of the proof system. In \cite{ABRW04} authors gave a lower bound $\exp\left[\frac{n^2}{m \Omega\left(2^{2^{\Delta}}\right)}\right]$ on the length of Resolution proofs where $\Delta$ is the degree of the dependency graph of the generator. This lower bound meets the barrier for the restriction technique.

In this paper, we introduce a ``heavy width'' measure for Resolution that allows showing a lower bound $\exp\left[\frac{n^2}{m 2^{O(\varepsilon \Delta)}}\right]$ on the length of Resolution proofs of the considered statement for the Nisan--Wigderson generator. This gives an exponential lower bound up to $\Delta := \log^{2 - \delta} n$ (the bigger degree the more extension variables we can use). It is a solution to an open problem from \cite{ABRW04}.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/076"><span class="datestr">at June 04, 2021 02:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/inference-is-not-a-privacy-violation/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/inference-is-not-a-privacy-violation/">Statistical Inference is Not a Privacy Violation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>On April 28, 2021, the US Census Bureau <a href="https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/2020-das-updates.html">released</a> a new demonstration of its differentially private Disclosure Avoidance System (DAS) for the 2020 US Census. The public were given a month to submit feedback before the system is finalized.
This demonstration data and the feedback has generated a lot of discussion, including media coverage on <a href="https://www.npr.org/2021/05/19/993247101/for-the-u-s-census-keeping-your-data-anonymous-and-useful-is-a-tricky-balance">National Public Radio</a>, in <a href="https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c94b46e-c30d-11eb-93f5-ee9558eecf4b_story.html">the Washington Post</a>, and via <a href="https://apnews.com/article/business-census-2020-technology-e701e313e841674be6396321343b7e49">the Associated Press</a>. The DAS is also the subject of an <a href="https://www.courtlistener.com/docket/59728874/state-v-united-states-department-of-commerce/">ongoing lawsuit</a>.</p>

<p>The following is a response from experts on differential privacy and cryptography to the <a href="https://alarm-redist.github.io/posts/2021-05-28-census-das/Harvard-DAS-Evaluation.pdf">working paper of Kenny et al.</a> on the impact of the 2020 U.S. Census Disclosure Avoidance System (DAS) on redistricting.</p>

<p>This paper makes a <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2016-06-14.md">common but serious mistake</a>, from which the authors wrongfully conclude the Census Bureau should not modernize its privacy-protection technology.  Not only do the results not support this conclusion, but they instead show the power of the methodology, known as differential privacy, adopted by the Bureau, precisely the opposite of the authors’ erroneous conclusions.</p>

<p>Trust is essential; once destroyed it can be nearly impossible to rebuild, and getting privacy wrong in this Census will have an impact on all future government surveys.  The Census Bureau has shown that their <a href="https://desfontain.es/privacy/index.html">2010 (DAS) does not survive modern privacy threats</a>, and in fact was roughly equivalent to publishing nearly three quarters of the responses.  The Census Bureau’s decision to modernize its Disclosure Avoidance System (DAS) for the 2020 Decennial Census to be differentially private is the correct response to decades of theoretical and empirical work on the privacy risks inherent in releasing large numbers of statistics derived from a dataset.</p>

<p>The importance of the Census, and the reality that no technology competing with differential privacy exists for meeting their confidentiality obligations, makes it very important that the public and policy makers have accurate information. We imagine you will be reporting on this topic in the future.  Others have <a href="https://gerrymander.princeton.edu/DAS-evaluation-Kenny-response">addressed flaws</a> in the paper regarding implications for redistricting; we want to provide you with an understanding of the privacy mistake in the study.</p>

<p>To understand the flaw in the paper’s argument, consider the role of smoking in determining cancer risk.  Statistical study of medical data has taught us that smoking causes cancer.   Armed with this knowledge, if we are told that 40 year old Mr. S is a smoker, we can conclude that he has an elevated cancer risk.  The statistical inference of elevated cancer risk—made before Mr. S was born—did not violate Mr. S’s privacy. To conclude otherwise is to define science to be a privacy attack.  This is the mistake made in the paper.</p>

<p>This is basically what Kenny et al. found.</p>

<p>The authors looked at three different predictors: one built directly from (swapped) 2010 Census data and the other two built using differential privacy applied to (swapped) 2010 Census data, and evaluated all three “on approximately 5.8 million registered voters included in the North Carolina February 2021 voter file.”  What did they find?</p>

<blockquote>
  <p>“Our analysis shows that across three main racial and ethnic groups, the predictions based on the [differential privacy based] DAS data appear to be as accurate as those based on the 2010 Census data.”</p>
</blockquote>

<p>This makes perfect sense. Bayesian Improved Surname Geocoding, or BISG, is a statistical method of building a predictor inferring ethnicity (or race) from name and geography.  Here, name and geography play the role of the information as to whether or not one smokes, and the prediction of ethnicity corresponds to the cancer risk prediction.  The predictor is constructed from census data on the ethnic makeup of individual census blocks and statistical information about the popularity of individual surnames within different ethnic groups.  With such a predictor, moving across the country can change the outcome, as can changing one’s name.  But a BISG prediction is not about the individual, it is about the statistical—population-level—relationship between name, geography, and ethnicity.</p>

<p>The differentially private DAS enabled learning to make statistical inferences about ethnicity from name and geography, without compromising the privacy of any Census respondent, exactly as it was intended to do.  In other words, the paper establishes fitness-for-use of the DAS data for the BISG statistical method!  Because differential privacy permits learning statistical patterns without compromising the privacy of individual members of the dataset, it should not interfere with learning the predictor, which is exactly what the authors found. Returning to our “smoking causes cancer” example, the researchers found that it was just as easy to detect this statistical pattern with a modern disclosure avoidance system in place as it was with the older, less protective system.</p>

<p>The authors’ conclusions –“ the DAS data may not provide universal privacy protection” – are simply not supported by their findings.</p>

<p>They have confused learning that smoking causes cancer—and applying this predictor to an individual smoker—with learning medical details of individual patients in the dataset. Change the input to the predictor—replace “smoker” with “non-smoker” or move across the country, for example—and the prediction changes.</p>

<p>The BISG prediction is not about the individual, it does not accompany her as she relocates from one neighborhood to another, it is a statistical relationship between name, geography, and ethnicity.  It is not a privacy compromise, it is science.</p>

<p>Signed:</p>
<ul>
  <li>Mark Bun, Assistant Professor of Computer Science, Boston University</li>
  <li>Damien Desfontaines, Privacy Engineer, Google</li>
  <li>Cynthia Dwork, Professor of Computer Science, Harvard University</li>
  <li>Moni Naor, Professor of Computer Science, The Weizmann Institute of Science</li>
  <li>Kobbi Nissim, Professor of Computer Science, Georgetown University</li>
  <li>Aaron Roth, Professor of Computer and Information Science, University of Pennsylvania</li>
  <li>Adam Smith, Professor of Computer Science, Boston University</li>
  <li>Thomas Steinke, Research Scientist, Google</li>
  <li>Jonathan Ullman, Assistant Professor of Computer Science, Northeastern University</li>
  <li>Salil Vadhan, Professor of Computer Science and Applied Mathematics, Harvard University</li>
</ul>

<p>Please contact Cynthia Dwork for contact information for authors happy to speak about this on the record.</p></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/inference-is-not-a-privacy-violation/"><span class="datestr">at June 03, 2021 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/075">TR21-075 |  Affine Extractors for Almost Logarithmic Entropy | 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	Jyun-Jie Liao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give an explicit construction of an affine extractor (over $\mathbb{F}_2$) that works for affine sources on $n$ bits with min-entropy $k \ge~  \log n \cdot (\log \log n)^{1 + o(1)}$. This improves prior work of Li (FOCS'16) that requires  min-entropy at least $\mathrm{poly}(\log n)$.
    
Our construction is based on the framework of using correlation breakers and resilient functions, a paradigm that was also used by Li. On a high level, the key sources of our improvement are based on the following new ingredients: (i) A new construction of  an affine somewhere random extractor, that we use in a crucial step instead of a linear seeded extractor (for which optimal constructions are not known) that was used by Li. (ii) A near optimal construction of a correlation breaker for linearly correlated sources. The construction of our correlation breaker takes inspiration from an exciting line of recent work that constructs two-source extractors for near logarithmic min-entropy.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/075"><span class="datestr">at June 03, 2021 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-583257573096655269">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html">What happened to self-driving cars?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In 2014, I wrote a blog post about a fake company <a href="https://blog.computationalcomplexity.org/2014/07/elfdrive.html">Elfdrive</a>.</p><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p>With a near record-setting investment announced last week, the self-driving car service Elfdrive is the hottest, most valuable technology start-up on the planet. It is also one of the most controversial.</p><p>The company, which has been the target of protests across Europe this week, has been accused of a reckless attitude toward safety, of price-gouging its customers, of putting existing cabbies out of work and of evading regulation. And it has been called trivial. In The New Yorker last year, George Packer huffed that Elfdrive typified Silicon Valley’s newfound focus on “solving all the problems of being 20 years old, with cash on hand.”</p><p>It is impossible to say whether Elfdrive is worth the $117 billion its investors believe it to be; like any start-up, it could fail. But for all its flaws, Elfdrive is anything but trivial. It could well transform transportation the way Amazon has altered shopping — by using slick, user-friendly software and mountains of data to completely reshape an existing market, ultimately making many modes of urban transportation cheaper, more flexible and more widely accessible to people across the income spectrum.</p></blockquote><p>It was a spoof on Uber but now it looks more like Tesla, expect that Tesla's market value is over half a trillion, about six times larger than General Motors.</p><p>The post was really about self-driving cars which I thought at the time would be commonplace by 2020. We are mostly there but there are issues of silent communication between drivers or between a driver and a pedestrian on who goes first that's hard to duplicate for a self-driving car. There is the paradox that if we make a car that will always stop if someone runs in front of it, then some people will run in front of it.</p><p>There is also the man-bites-dog problem. Any person killed by a self-driving car will be a major news item while the person killed by a human-driven car while you've been reading this post will never be reported.</p><p>We'll get to self-driving cars eventually, it just won't be all at once. We're already have basically self-driving cars on highways and in many other roads as well. As the technology improves and people see that it's safe at some point people will say, "So why do we even need the steering wheel anymore?"</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html"><span class="datestr">at June 03, 2021 08:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/074">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/074">TR21-074 |  Space characterizations of complexity measures and size-space trade-offs in propositional proof systems | 

	Theodoros Papamakarios, 

	Alexander Razborov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify two new big clusters of proof complexity measures equivalent up to
polynomial and $\log n$ factors. The first cluster contains, among others,
the logarithm of tree-like resolution size, regularized (that is, multiplied
by the logarithm of proof length) clause and monomial space, and clause
space, both ordinary and regularized, in regular and tree-like resolution. As
a consequence, separating clause or monomial space from the (logarithm of)
tree-like resolution size is the same as showing a strong trade-off between
clause or monomial space and proof length, and is the same as showing a
super-critical trade-off between clause space and depth. The second cluster
contains width, $\Sigma_2$ space (a generalization of clause
space to depth 2 Frege systems), both ordinary and regularized, as well as
the logarithm of tree-like size in the system $R(\log)$. As an application of some of
these simulations, we improve a known size-space trade-off for polynomial calculus with resolution. In
terms of lower bounds, we show a quadratic lower bound on tree-like
resolution size for formulas refutable in clause space $4$. We introduce on
our way yet another proof complexity measure intermediate between depth and
the logarithm of tree-like size that might be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/074"><span class="datestr">at June 03, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/073">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/073">TR21-073 |  Lower bounds for samplers and data structures via the cell-probe separator | 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose that a distribution $S$ can be approximately sampled by an
efficient cell-probe algorithm. It is shown to be possible to restrict
the input to the algorithm so that its output distribution is still
not too far from $S$, and at the same time many output coordinates
are almost pairwise independent.

Building on this several results are obtained, including:

- A lower bound for sampling prefix sums.

- A lower bound for sampling a variant of the predecessor problem.

- A separation between AC0 and cell-probe sampling.

- A separation between sampling with $O(q)$ and $q$ probes.

- A new proof of the Patrascu-Viola data-structure lower bound for
prefix sums, demonstrating the feasibility of obtaining data-structure
lower bounds via sampling.

- A separation between data structures making $O(q)$ and $q$ probes.

The only previous cell-probe lower bounds for sampling followed from
the AC0 lower bounds and applied to pseudorandom objects like error-correcting
and extractors, making them inadequate for the above applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/073"><span class="datestr">at June 03, 2021 02:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/">TCS+ talk: Wednesday, June 9 — Pravesh Kothari (CMU) and Ankur Moitra (MIT)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, June 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC).<strong> <a href="https://www.cs.cmu.edu/~praveshk/">Pravesh Kothari</a>  and <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a> </strong>from CMU and MIT will (jointly) speak about “<em>Robustly Learning Mixtures of Gaussians</em>” (abstract below).</p>
<p>Note that the seminar will be a bit longer than the usual: it’s a double feature!</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: For a while now the problem of robustly learning a high-dimensional mixture of Gaussians has had a target on its back. The first works in algorithmic robust statistics gave provably robust algorithms for learning a single Gaussian. Since then there has been steady progress, including algorithms for robustly learning mixtures of spherical Gaussians, mixtures of Gaussians under separation conditions, and arbitrary mixtures of two Gaussians. In this talk we will discuss two recent works that essentially resolve the general problem. There are important differences in their techniques, setup, and overall quantitative guarantees, which we will discuss.</p>
<p>The talk will cover the following independent works:</p>
<ul>
<li>Liu, Moitra, “Settling the Robust Learnability of Mixtures of Gaussians”</li>
<li>Bakshi, Diakonikolas, Jia, Kane, Kothari, Vempala, “Robustly Learning Mixtures of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" /> Arbitrary Gaussians”</li>
</ul>
</blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/"><span class="datestr">at June 03, 2021 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-9052295508162981109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/06/frank-p-ramsey-on-research-and.html">Frank P. Ramsey on research and publication rates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Spurred by the excellent 1978 radio programme <a href="https://sms.cam.ac.uk/media/20145" target="_blank">'Better than the Stars'</a> by <a href="https://en.wikipedia.org/wiki/Hugh_Mellor" target="_blank">D. H. Mellor</a> about <a href="https://plato.stanford.edu/entries/ramsey/" target="_blank">Frank Ramsey</a> and by the <a href="https://traffic.libsyn.com/secure/philosophybites/Cheryl_Misak_on_Frank_Ramsey_and_Ludwig_Wittgenstein.mp3" target="_blank">Philosophy Bites interview with Cheryl Misak on Frank Ramsey and Ludwig Wittgenstein</a>, I started reading <a href="https://www.cherylmisak.com/" target="_blank">Cheryl Misak</a>'s <a href="https://global.oup.com/academic/product/frank-ramsey-9780198755357?cc=is&amp;lang=en&amp;" target="_blank">biography of Frank Ramsey</a>. (FWIW, I strongly recommend the radio programme, the podcast and the book.)<br /></p><p>The following quote from pages 169-170 of Cheryl Misak's book describes Ramsey's views on publications and research, as stated in a letter to his father Arthur:<br /></p><div style="margin-left: 40px; text-align: left;">Arthur tried a different tack, suggesting that Frank was going to be in trouble for wasting all this time on analysis, rather than on his career. On 24 September, in what seems to be his last letter home before he left Vienna, Frank wrote: </div><blockquote style="margin-left: 80px; text-align: left;">I don’t see how there can be any such inquisition into my conduct in Vienna as you suppose seem to want to guard against.... No one can suppose that you can’t research for six months without having a paper ready by the end. If everyone wrote a paper every six months the amount of trivial literature would swell beyond all bounds. Given time I shall produce a good paper. But if I hurry it will be ill written and unintelligible and unconvincing. <br /></blockquote><blockquote style="margin-left: 80px; text-align: left;">It seems to me perfectly proper to spend a scholarship being analysed, as it is likely to make me cleverer in the future, and discoveries of importance are made by remarkable people not by remarkable diligence. </blockquote><div style="margin-left: 40px; text-align: left;"> While it may not be persuasive that psychoanalysis makes one cleverer, Frank was prescient that the numbers of journal articles would eventually swell and he was right that diligence isn't enough to produce discoveries of importance.</div><div style="margin-left: 40px; text-align: left;"> </div><div style="text-align: left;">Of course, academia has changed since those times and I do value "remarkable diligence". However, I will try to remember Ramsey's words next time someone proposes to make academic hirings and promotions conditional to having a certain number of papers or citations or whatever per year on average. <br /></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/06/frank-p-ramsey-on-research-and.html"><span class="datestr">at June 01, 2021 09:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5536">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5536">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>Hooray, I’m <a href="https://www.acm.org/articles/people-of-acm/2021/scott-aaronson">today’s “Featured ACM Member”</a>!  Which basically means, yet another interview with me about quantum computing, with questions including what’s most surprised me about the development of QC, and what students should do to get into the field.</li><li>I’m proud to announce that <a href="https://arxiv.org/abs/2105.14697">An Automated Approach to the Collatz Conjecture</a>, a paper by Emre Yolcu, myself, and Marijn Heule that we started working on over four years ago, is finally available on the arXiv, and will be presented at the 2021 <a href="https://www.cs.cmu.edu/~mheule/CADE28/">Conference on Automated Deduction</a>.  Long story short: no, we didn’t prove <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz</a>, but we have an approach that can for the first time prove certain Collatz-like statements in a fully automated way, so hopefully that’s interesting!  There was also a <a href="https://www.quantamagazine.org/can-computers-solve-the-collatz-conjecture-20200826/"><em>Quanta</em> article</a> even before our paper had come out (I wasn’t thrilled about the timing).</li><li>The legendary <a href="https://en.wikipedia.org/wiki/Baba_Brinkman">Baba Brinkman</a> has a <a href="https://www.youtube.com/watch?v=kVcOx9Bg3a4">new rap about quantum computing</a> (hat tip to blog commenter YD).  Having just watched the music video, I see it as one of the better popularization efforts our field has seen in the past 25 years—more coherent than the average journalistic account and with a <em>much</em> better backbeat.  (I do, however, take a more guarded view than Brinkman of the potential applications, especially to e.g. autonomous driving and supply-chain optimization.)</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5536"><span class="datestr">at June 01, 2021 08:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3526">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2021/06/01/wine21-call-for-papers/">WINE’21 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>WINE 2021: The 17th Conference on Web and Internet Economics </strong></p>



<p><strong>December 14-17, 2021</strong><br /><strong>Hasso Plattner Institute, Potsdam, Germany</strong></p>



<p></p><blockquote class="wp-embedded-content"><a href="https://hpi.de/wine2021/">Overview</a></blockquote><p></p>



<p>Over the past two decades, researchers in theoretical computer science, artificial intelligence, operations research, and economics have joined forces to understand the interplay of incentives and computation. These issues are of particular importance in the Web and the Internet that enable the interaction of large and diverse populations. The Conference on Web and Internet Economics (WINE) is an interdisciplinary forum for the exchange of ideas and results on incentives and computation arising from these various fields. WINE 2021 continues the successful tradition of the Conference on Web and Internet Economics (named Workshop on Internet &amp; Network Economics until 2013), which was held annually from 2005 to present.</p>



<p>The program will feature invited talks, tutorials, paper presentations, and a poster session. All paper submissions will be peer-reviewed and evaluated on the basis of the quality of their contribution, originality, soundness, and significance. Submissions are invited in, but not limited to, the following topics:</p>



<ul><li>Algorithmic Game Theory</li><li>Algorithmic Mechanism Design</li><li>Auction Algorithms and Analysis</li><li>Computational Advertising</li><li>Computational Aspects of Equilibria</li><li>Computational Social Choice</li><li>Learning in Markets and Mechanism Design</li><li>Learning under Strategic Behavior</li><li>Coalitions, Coordination, and Collective Action</li><li>Economic Aspects of Security and Privacy</li><li>Economic Aspects of Distributed Computing and Cryptocurrencies</li><li>Econometrics, ML, and Data Science</li><li>Behavioral Economics and Behavioral Modeling</li><li>Fairness and Trust in Games and Markets</li><li>Price Differentiation and Price Dynamics</li><li>Revenue Management</li><li>Social Networks and Network Games</li></ul>



<p><strong>Authors of the accepted papers will have a choice to attend the conference virtually.</strong></p>



<p><strong>Important Dates</strong></p>



<p>Paper submission deadline: July 12, 2021, 11:59pm Pacific Time</p>



<p>Author notification: September (exact date TBA)</p>



<p><strong>Submission Format</strong></p>



<p>Authors are invited to submit extended abstracts presenting original research on any of the research fields related to WINE 2021.</p>



<p>An extended abstract submitted to WINE 2021 should start with the title of the paper, each author’s name, affiliation and e-mail address, followed by a one-paragraph summary of the results to be presented. This should then be followed by a technical exposition of the main ideas and techniques used to achieve these results, including motivation and a clear comparison with related work.</p>



<p>The extended abstract should not exceed 18 single-spaced pages (excluding references) using reasonable margins (at least one-inch margins all around) and at least 11-point font. If the authors believe that more details are essential to substantiate the claims of the paper, they may include a clearly marked appendix (with no space limit) that will be read at the discretion of the Program Committee. It is strongly recommended that submissions adhere to the specified format and length. Submissions that are clearly too long may be rejected immediately. The above specifications are meant to provide more freedom to the authors at the time of submission. Note that accepted papers will be allocated 14 pages (including references) in the LNCS format in the proceedings (see below).</p>



<p>The proceedings of the conference will be published by Springer-Verlag in the ARCoSS/LNCS series, and will be available for distribution at the conference. Accepted papers will be allocated 14 pages total in the LNCS format in the proceedings. Submissions are encouraged, though not required, to follow the LNCS format (Latex, Word). More information about the LNCS format can be found on the <a href="https://www.springer.com/cn/computer-science/lncs/conference-proceedings-guidelines" target="_blank" rel="noreferrer noopener">author instructions page of Springer-Verlag</a>. </p>



<p><strong>Best Paper Award</strong></p>



<p>The program committee will decide upon a best paper award and a best student paper award.</p>



<p><strong>Important Notice</strong></p>



<p>To accommodate the publishing traditions of different fields, authors of accepted papers can ask that only a one-page abstract of the paper appear in the proceedings, along with a URL pointing to the full paper. The authors should guarantee the link to be reliable for at least two years. This option is available to accommodate subsequent publication in journals that would not consider results that have been published in preliminary form in conference proceedings. Such papers must be submitted and formatted just like papers submitted for full-text publication.</p>



<p>Simultaneous submission of results to another conference with published proceedings is not allowed. Results previously published or presented at another archival conference prior to WINE 2021, or published (or accepted for publication) at a journal prior to the submission deadline of WINE 2021, will not be considered. Simultaneous submission of results to a journal is allowed only if the authors intend to publish the paper as a one-page abstract in WINE 2021. Papers that are accepted and appear as a one-page abstract can be subsequently submitted for publication in a journal but may not be submitted to any other conference that has a published proceeding.</p>



<p>Program PC co-chairs: Michal Feldman (chair), Hu Fu and Inbal Talgam-Cohen</p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2021/06/01/wine21-call-for-papers/"><span class="datestr">at June 01, 2021 09:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/forc-2021-is-coming-up/">FORC 2021 is coming up!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The Second annual Symposium on the Foundations of Responsible Computing (FORC) will be held virtually (on Gather.town) June 9-11, 2021.<a href="https://docs.google.com/forms/d/e/1FAIpQLSfELj0MvgcI83dmYThUPiynqWj8-G9Xve2dVf84EqKzgXXsHQ/viewform" target="_blank" rel="noreferrer noopener"> Registration is free, but required, by June 7</a>. Instructions for joining the event will be emailed to registered participants and posted online on June 8.</p>



<p>The program will feature 24 papers, six exciting panel discussions, three social hours featuring interactive board games, keynotes by Julie Owono (of the Facebook Oversight Board) and Kate Crawford (on her new book, the Atlas of AI), and a mentoring meetup.</p>



<p>The full program is here: <a href="https://responsiblecomputing.org/forc-2021-program/" target="_blank" rel="noreferrer noopener">https://responsiblecomputing.org/forc-2021-program/</a></p>



<p>The Symposium on Foundations of Responsible Computing (FORC) is a forum for mathematical research in computation and society writ large. The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>



<figure class="wp-block-image size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/white-logo-no-background.png?resize=800%2C858&amp;ssl=1" class="wp-image-1770" height="858" /></figure></div>







<p class="date">
by galoosh33 <a href="https://toc4fairness.org/forc-2021-is-coming-up/"><span class="datestr">at June 01, 2021 06:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/05/ememem-street-mosaics/">Ememem’s mosaic interventions in the potholes of Lyon</a> (<a href="https://mathstodon.xyz/@11011110/106251596474278206">\(\mathbb{M}\)</a>, <a href="https://www.ememem-flacking.net/flacking">see also</a>, <a href="https://boingboing.net/2021/05/17/this-artist-repairs-damaged-sidewalks-with-beautiful-mosaic-tiles.html">and also</a>). I find this kind of urban kintsugi pleasing, not just for its aspect of guerilla art, but for the sharp geometric patterns of the mosaics and the rough boundaries where they meet the landscape around them.</p>
  </li>
  <li>
    <p><a href="https://tilings.math.uni-bielefeld.de/">Tilings encyclopedia</a> (<a href="https://mathstodon.xyz/@11011110/106255097813445827">\(\mathbb{M}\)</a>). An online collection of many pretty substitution tilings. What’s missing to make it closer to OEIS in usefulness is a way to search by tiling rather than by keyword in the description of the tiling.</p>
  </li>
  <li>
    <p>At UC Irvine, we’ve been using Piazza for online course forums, but Piazza insists on serving ads to students (despite being paid), so we’re moving to <a href="https://edstem.org/us/">Ed Discussion</a> instead (<a href="https://mathstodon.xyz/@11011110/106258490049583719">\(\mathbb{M}\)</a>). You can see similar moves at <a href="https://rtl.berkeley.edu/news/piazza-and-new-online-discussion-platform-update">Berkeley</a> and <a href="https://news.psu.edu/story/641227/2020/12/07/academics/piazza-class-discussion-platform-start-displaying-advertisements">Penn State</a>. Usually I mistrust campus choices of software infrastructure — they tend to go for big crufty hard-to-use packages that are a poor fit for our needs — but this appears to be for good reason and I support it.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-answer-old-question-about-odd-graphs-20210519/"><em>Quanta</em> highlights one of my UCI colleagues, Asaf Ferber, for his work with Michael Krivelevich proving that every graph without isolated vertices has an induced subgraph of \(\Omega(n)\) vertices in which all vertices have odd degree</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/106265785749549559">\(\mathbb{M}\)</a>).</span> As usual, they also get important details wrong, forgetting to mention that the subgraphs must be induced, and also not mentioning the requirement of no isolated vertices. For the actual preprint see <a href="https://arxiv.org/abs/2009.05495">arXiv:2009.05495</a>.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/05/20/unc-chapel-hill-board-doesnt-approve-tenure-noted-journalist">Academic freedom dead at  U. of North Carolina</a> (<a href="https://mathstodon.xyz/@11011110/106269430445568583">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191525/Nikole-Hannah-Jones-Denied-Tenure-at-UNC">via</a>, <a href="https://www.nytimes.com/2021/05/19/business/media/nikole-hannah-jones-unc.html">see also</a>): right-wing board of trustees cancel-cultures MacArthur and Pulitzer winner Nikole Hannah-Jones for racist/political reasons (they don’t want anyone to talk about US slavery and ongoing systemic racism). To be clear, she still has a position at UNC; after the trustees denied her tenure, her department gave her a five-year-renewable spot not requiring trustee approval.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/">Some mathematical news</a> (<a href="https://mathstodon.xyz/@11011110/106277146476845581">\(\mathbb{M}\)</a>). Gil Kalai rounds up a lot of recent developments in knot recognition complexity, exotic spheres, numbers of fixed points, graph theory, and additive combinatorics.</p>
  </li>
  <li>
    <p><a href="https://www34.homepage.villanova.edu/robert.jantzen/notes/torus/cavatappo20/">Tilted cavatappo surfaces</a> (<a href="https://mathstodon.xyz/@11011110/106280088634102191">\(\mathbb{M}\)</a>). Robert Jantzen studies the shape of <a href="https://en.wikipedia.org/wiki/Cavatappi">corkscrew tube pasta</a> and of shortest paths on its surface. See also his preprints <a href="https://arxiv.org/abs/1301.0013">arXiv:1301.0013</a> and <a href="https://arxiv.org/abs/1402.3284">arXiv:1402.3284</a>.</p>
  </li>
  <li>
    <p>Here’s an unexpected property of hyperbolic geometry (<a href="https://mathstodon.xyz/@11011110/106288718744948273">\(\mathbb{M}\)</a>), or at least, it struck me as counterintuitive: all lines through two opposite quadrants of two perpendicular lines pass near their crossing, within distance \(\ln(1+\sqrt2)\). The figure below uses the upper halfplane model to show two quadrants (dark yellow), their convex hull (light yellow, from which the lines cannot escape), and a circle of radius \(\ln(1+\sqrt2)\) centered at the crossing, separating the quadrants.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/double-wedge-hull.svg" alt="Convex hull of the quadrants formed in the hyperbolic plane by two perpendicular lines, and a circle centered at the crossing separating the quadrants" /></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=HeQX2HjkcNo">Veritaserium on Cantor, Gödel, and Turing</a> (<a href="https://mathstodon.xyz/@gnivasch/106290995069247640">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>At a time when national borders have largely shut down, significantly decreasing the opportunities for higher education for people with the misfortune to be born in the wrong part of the world, <a href="https://www.latimes.com/california/story/2021-05-25/bold-plan-for-uc-admissions-reduce-out-of-state-students">California’s legislators are pushing to enact the same barriers to education across state lines</a> (<a href="https://mathstodon.xyz/@11011110/106297102605074297">\(\mathbb{M}\)</a>), preventing access to education and hurting the state itself by discouraging the best and brightest from coming here.</p>
  </li>
  <li>
    <p>Cellphone snapshot of the UC Irvine Ecological Preserve on a recent cloudy day (<a href="https://mathstodon.xyz/@11011110/106305347918888735">\(\mathbb{M}\)</a>). The foreground is mostly mustard, of no particular ecological significance; the ecological part is the coastal sage scrub on the sunny hillside in the background.</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="UCI Ecological Preserve" src="https://www.ics.uci.edu/~eppstein/pix/uciep2/DriedMustard-m.jpg" /></p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/BF02764015">Aperiodic tilings of the hyperbolic plane by convex polygons</a> (<a href="https://mathstodon.xyz/@11011110/106314334930093898">\(\mathbb{M}\)</a>), Margulis and Mozes, 1998. I knew about the <a href="https://en.wikipedia.org/wiki/Binary_tiling">binary tilings</a> but not this, which gets aperiodicity differently: if a tile’s area isn’t a rational multiple of \(\pi\), it cannot be periodic. This works even for certain hyperbolic rhombi, with angles chosen so they can tile. The tilings can still have 1d symmetry. If some single tile  avoids all symmetries, I don’t know about it.</p>
  </li>
  <li>
    <p><a href="http://www.geometrictomography.com/">Geometric tomography</a> (<a href="https://mathstodon.xyz/@11011110/106317414925334494">\(\mathbb{M}\)</a>). A nicely presented brief web survey of this subject, on the reconstruction of 3d shapes from 2d information (such as its brightness function, the areas of its perpendicular projections), by Richard J. Gardner based on his 1995 book of the same title.</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">Collusion rings threaten the integrity of computer science research</a> (<a href="https://mathstodon.xyz/@11011110/106320077489874249">\(\mathbb{M}\)</a>), Michael L. Littman, CACM. Relatedly: <a href="https://www.chemistryworld.com/news/publishers-grapple-with-an-invisible-foe-as-huge-organised-fraud-hits-scientific-journals/4013652.article">“huge organised fraud” in scientific journals</a>, <a href="https://retractionwatch.com/2021/05/29/weekend-reads-gibberish-papers-persist-the-academic-who-faked-cherokee-heritage-organised-fraud-hits-scientific-journals/">via Retraction Watch</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=8Sv_FaEN8zE">Five talks from CanaDAM 2021 introducing graph product structure theory and its applications</a> (<a href="https://mathstodon.xyz/@patmorin/106300452133561408">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://isohedral.ca/heesch-numbers-of-unmarked-polyforms/">Big progress in classifying polyforms by their Heesch numbers, obtained by using a SAT solver in place of an ad-hoc backtracking search for tilings</a> (<a href="https://mathstodon.xyz/@11011110/106331731989277653">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191587/TikTok-teen-points-to-inside-elbow-bites-lip-Heeeeeeeesch">via</a>). See also <a href="https://twitter.com/mathpuzzle/status/1397040707706236928">Ed Pegg on an infinite set of polyforms with Heesch = 3</a>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/31/linkage.html"><span class="datestr">at May 31, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
