<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 21, 2020 11:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4912">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4912">Is this blog obsolete?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Longtime friend-of-the-blog <a href="http://michaelnielsen.org/">Michael Nielsen</a>, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following <a href="https://twitter.com/michael_nielsen/status/1284988349418098688">exchange</a> with the astounding <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> text generator / chatbot:</p>



<blockquote class="wp-block-quote"><p>Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?<br />A: It is not possible to use quantum computers to solve NP-complete problems in polynomial time.</p><p>Q: Why not?<br />A: Because NP-complete problems are not in BQP.</p><p>Q: You’ve just restated your earlier assertion!  What I’d like is to understand the underlying reason why not!<br />A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p><p>Q: No offence, but this seems really bogus.  I don’t believe you understand the reason why!<br />A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p></blockquote>



<p>(And then there are a few more rounds of the AI not having time to “explain the underlying reason why not.”)</p>



<p>This is, like, 80% of the way to replacing <em>Shtetl-Optimized</em>!</p>



<p>For much more discussion of GPT-3 and its implications, and samples of its output, see for example the <a href="https://www.reddit.com/r/slatestarcodex/">SSC subreddit</a>.  At the moment, as far as I can tell, the closest a person off the street can easily come to experimenting with GPT-3 themselves is using a website called <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>



<p>And yes, as many have already remarked, this is clearly the <a href="https://en.wikipedia.org/wiki/Altair_8800">MITS Altair</a> of text-generating AI, an amusing toy that’s also the start of something that will change the world.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4912"><span class="datestr">at July 21, 2020 12:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09793">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09793">$2$-blocks in strongly biconnected directed graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaberi:Raed.html">Raed Jaberi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09793">PDF</a><br /><b>Abstract: </b>A directed graph $G=(V,E)$ is called strongly biconnected if $G$ is strongly
connected and the underlying graph of $G$ is biconnected. A strongly
biconnected component of a strongly connected graph $G=(V,E)$ is a maximal
vertex subset $L\subseteq V$ such that the induced subgraph on $L$ is strongly
biconnected. Let $G=(V,E)$ be a strongly biconnected directed graph. A
$2$-edge-biconnected block in $G$ is a maximal vertex subset $U\subseteq V$
such that for any two distict vertices $v,w \in U$ and for each edge $b\in E$,
the vertices $v,w$ are in the same strongly biconnected components of
$G\setminus\left\lbrace b\right\rbrace $. A $2$-strong-biconnected block in $G$
is a maximal vertex subset $U\subseteq V$ of size at least $2$ such that for
every pair of distinct vertices $v,w\in U$ and for every vertex $z\in
V\setminus\left\lbrace v,w \right\rbrace $, the vertices $v$ and $w$ are in the
same strongly biconnected component of $G\setminus \left\lbrace v,w
\right\rbrace $. In this paper we study $2$-edge-biconnected blocks and
$2$-strong biconnected blocks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09793"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09773">Shortest Secure Path in a Voronoi Diagram</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Har=Peled:Sariel.html">Sariel Har-Peled</a>, Rajgopal Varadharajan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09773">PDF</a><br /><b>Abstract: </b>We investigate the problem of computing the shortest secure path in a Voronoi
diagram. Here, a path is secure if it is a sequence of touching Voronoi cells,
where each Voronoi cell in the path has a uniform cost of being secured.
Importantly, we allow inserting new sites, which in some cases leads to
significantly shorter paths. We present an $O(n \log n)$ time algorithm for
solving this problem in the plane, which uses a dynamic additive weighted
Voronoi diagram to compute this path. The algorithm is an interesting
combination of the continuous and discrete Dijkstra algorithms. We also
implemented the algorithm using CGAL.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09773"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09768">FPT Algorithms for Finding Dense Subgraphs in $c$-Closed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Husic:Edin.html">Edin Husic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09768">PDF</a><br /><b>Abstract: </b>Dense subgraph detection is a fundamental problem in network analysis for
which few worst-case guarantees are known, motivating its study through the
lens of fixed-parameter tractability. But for what parameter? Recent work has
proposed parameterizing graphs by their degree of triadic closure, with a
$c$-closed graph defined as one in which every vertex pair with at least $c$
common neighbors are themselves connected by an edge. The special case of
enumerating all maximal cliques (and hence computing a maximum clique) of a
$c$-closed graph is known to be fixed-parameter tractable with respect to $c$
(Fox et al., SICOMP 2020).
</p>
<p>In network analysis, sufficiently dense subgraphs are typically as notable
and meaningful as cliques. We investigate the fixed-parameter tractability
(with respect to $c$) of optimization and enumeration in $c$-closed graphs, for
several notions of dense subgraphs. We focus on graph families that are the
complements of the most well-studied notions of sparse graphs, including graphs
with bounded degree, bounded treewidth, or bounded degeneracy, and provide
fixed-parameter tractable enumeration and optimization algorithms for these
families. To go beyond the special case of maximal cliques, we use a new
combinatorial bound (generalizing the Moon-Moser theorem); new techniques for
exploiting the $c$-closed condition; and more sophisticated enumeration
algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09768"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09640">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09640">Exploitation of Multiple Replenishing Resources with Uncertainty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Amos.html">Amos Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Emek:Yuval.html">Yuval Emek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Collet:Simon.html">Simon Collet</a>, Aya Goldshtein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yovel:Yossi.html">Yossi Yovel</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09640">PDF</a><br /><b>Abstract: </b>We consider an optimization problem in which a (single) bat aims to exploit
the nectar in a set of $n$ cacti with the objective of maximizing the expected
total amount of nectar it drinks. Each cactus $i \in [n]$ is characterized by a
parameter $r_{i} &gt; 0$ that determines the rate in which nectar accumulates in
$i$. In every round, the bat can visit one cactus and drink all the nectar
accumulated there since its previous visit. Furthermore, competition with other
bats, that may also visit some cacti and drink their nectar, is modeled by
means of a stochastic process in which cactus $i$ is emptied in each round
(independently) with probability $0 &lt; s_i &lt; 1$. Our attention is restricted to
purely-stochastic strategies that are characterized by a probability vector
$(p_1, \ldots, p_n)$ determining the probability $p_i$ that the bat visits
cactus $i$ in each round. We prove that for every $\epsilon &gt; 0$, there exists
a purely-stochastic strategy that approximates the optimal purely-stochastic
strategy to within a multiplicative factor of $1 + \epsilon$, while exploiting
only a small core of cacti. Specifically, we show that it suffices to include
at most $\frac{2 (1 - \sigma)}{\epsilon \cdot \sigma}$ cacti in the core, where
$\sigma = \min_{i \in [n]} s_{i}$. We also show that this upper bound on core
size is asymptotically optimal as a core of a significantly smaller size cannot
provide a $(1 + \epsilon)$-approximation of the optimal purely-stochastic
strategy. This means that when the competition is more intense (i.e., $\sigma$
is larger), a strategy based on exploiting smaller cores will be favorable.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09640"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09634">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09634">GRMR: Generalized Regret-Minimizing Representatives</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yanhao.html">Yanhao Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathioudakis:Michael.html">Michael Mathioudakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuchen.html">Yuchen Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Kian=Lee.html">Kian-Lee Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09634">PDF</a><br /><b>Abstract: </b>Extracting a small subset of representative tuples from a large database is
an important task in multi-criteria decision making. The regret-minimizing set
(RMS) problem is recently proposed for representative discovery from databases.
Specifically, for a set of tuples (points) in $d$ dimensions, an RMS problem
finds the smallest subset such that, for any possible ranking function, the
relative difference in scores between the top-ranked point in the subset and
the top-ranked point in the entire database is within a parameter $\varepsilon
\in (0,1)$. Although RMS and its variations have been extensively investigated
in the literature, existing approaches only consider the class of nonnegative
(monotonic) linear functions for ranking, which have limitations in modeling
user preferences and decision-making processes.
</p>
<p>To address this issue, we define the generalized regret-minimizing
representative (GRMR) problem that extends RMS by taking into account all
linear functions including non-monotonic ones with negative weights. For
two-dimensional databases, we propose an optimal algorithm for GRMR via a
transformation into the shortest cycle problem in a directed graph. Since GRMR
is proven to be NP-hard even in three dimensions, we further develop a
polynomial-time heuristic algorithm for GRMR on databases in arbitrary
dimensions. Finally, we conduct extensive experiments on real and synthetic
datasets to confirm the efficiency, effectiveness, and scalability of our
proposed algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09634"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09599">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09599">Reconstructing weighted voting schemes from partial information about their power indices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bennett:Huck.html">Huck Bennett</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Servedio:Rocco_A=.html">Rocco A. Servedio</a>, Emmanouil V. Vlatakis-Gkaragkounis <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09599">PDF</a><br /><b>Abstract: </b>A number of recent works [Goldberg 2006; O'Donnell and Servedio 2011; De,
Diakonikolas, and Servedio 2017; De, Diakonikolas, Feldman, and Servedio 2014]
have considered the problem of approximately reconstructing an unknown weighted
voting scheme given information about various sorts of ``power indices'' that
characterize the level of control that individual voters have over the final
outcome. In the language of theoretical computer science, this is the problem
of approximating an unknown linear threshold function (LTF) over $\{-1, 1\}^n$
given some numerical measure (such as the function's $n$ ``Chow parameters,''
a.k.a. its degree-1 Fourier coefficients, or the vector of its $n$ Shapley
indices) of how much each of the $n$ individual input variables affects the
outcome of the function.
</p>
<p>In this paper we consider the problem of reconstructing an LTF given only
partial information about its Chow parameters or Shapley indices; i.e. we are
given only the Chow parameters or the Shapley indices corresponding to a subset
$S \subseteq [n]$ of the $n$ input variables. A natural goal in this partial
information setting is to find an LTF whose Chow parameters or Shapley indices
corresponding to indices in $S$ accurately match the given Chow parameters or
Shapley indices of the unknown LTF. We refer to this as the Partial Inverse
Power Index Problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09599"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09556">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09556">A $2^{n/2}$-Time Algorithm for $\sqrt{n}$-SVP and $\sqrt{n}$-Hermite SVP, and an Improved Time-Approximation Tradeoff for (H)SVP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aggarwal:Divesh.html">Divesh Aggarwal</a>, Zeyong Li, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephens=Davidowitz:Noah.html">Noah Stephens-Davidowitz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09556">PDF</a><br /><b>Abstract: </b>We show a $2^{n/2+o(n)}$-time algorithm that finds a (non-zero) vector in a
lattice $\mathcal{L} \subset \mathbb{R}^n$ with norm at most
$\tilde{O}(\sqrt{n})\cdot \min\{\lambda_1(\mathcal{L}),
\det(\mathcal{L})^{1/n}\}$, where $\lambda_1(\mathcal{L})$ is the length of a
shortest non-zero lattice vector and $\det(\mathcal{L})$ is the lattice
determinant. Minkowski showed that $\lambda_1(\mathcal{L}) \leq \sqrt{n}
\det(\mathcal{L})^{1/n}$ and that there exist lattices with
$\lambda_1(\mathcal{L}) \geq \Omega(\sqrt{n}) \cdot \det(\mathcal{L})^{1/n}$,
so that our algorithm finds vectors that are as short as possible relative to
the determinant (up to a polylogarithmic factor).
</p>
<p>The main technical contribution behind this result is new analysis of (a
simpler variant of) an algorithm from <a href="http://export.arxiv.org/abs/1412.7994">arXiv:1412.7994</a>, which was only
previously known to solve less useful problems. To achieve this, we rely
crucially on the ``reverse Minkowski theorem'' (conjectured by Dadush
<a href="http://export.arxiv.org/abs/1606.06913">arXiv:1606.06913</a> and proven by <a href="http://export.arxiv.org/abs/1611.05979">arXiv:1611.05979</a>), which can be thought of as a
partial converse to the fact that $\lambda_1(\mathcal{L}) \leq \sqrt{n}
\det(\mathcal{L})^{1/n}$.
</p>
<p>Previously, the fastest known algorithm for finding such a vector was the
$2^{.802n + o(n)}$-time algorithm due to [Liu, Wang, Xu, and Zheng, 2011],
which actually found a non-zero lattice vector with length $O(1) \cdot
\lambda_1(\mathcal{L})$. Though we do not show how to find lattice vectors with
this length in time $2^{n/2+o(n)}$, we do show that our algorithm suffices for
the most important application of such algorithms: basis reduction. In
particular, we show a modified version of Gama and Nguyen's slide-reduction
algorithm [Gama and Nguyen, STOC 2008], which can be combined with the
algorithm above to improve the time-length tradeoff for shortest-vector
algorithms in nearly all regimes, including the regimes relevant to
cryptography.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09556"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09461">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09461">Controllability of reaction systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ivanov:Sergiu.html">Sergiu Ivanov</a>, Ion Petre IBISC, Université Évry, Université Paris-Saclay, France, Department of Mathematics and Statistics, University of Turku, Finland, National Institute for Research and Development in Biological Sciences, Romania) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09461">PDF</a><br /><b>Abstract: </b>Controlling a dynamical system is the ability of changing its configuration
arbitrarily through a suitable choice of inputs. It is a very well studied
concept in control theory, with wide ranging applications in medicine, biology,
social sciences, engineering. We introduce in this article the concept of
controllability of reaction systems as the ability of transitioning between any
two states through a suitable choice of context sequences. We show that the
problem is PSPACE-hard. We also introduce a model of oncogenic signalling based
on reaction systems and use it to illustrate the intricacies of the
controllability of reaction systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09461"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09345">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09345">Combinatorial and computational investigations of Neighbor-Joining bias</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davidson:Ruth.html">Ruth Davidson</a>, Abraham Martin del Campo <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09345">PDF</a><br /><b>Abstract: </b>The Neighbor-Joining algorithm is a popular distance-based phylogenetic
method that computes a tree metric from a dissimilarity map arising from
biological data. Realizing dissimilarity maps as points in Euclidean space, the
algorithm partitions the input space into polyhedral regions indexed by the
combinatorial type of the trees returned. A full combinatorial description of
these regions has not been found yet; different sequences of Neighbor-Joining
agglomeration events can produce the same combinatorial tree, therefore
associating multiple geometric regions to the same algorithmic output. We
resolve this confusion by defining agglomeration orders on trees, leading to a
bijection between distinct regions of the output space and weighted Motzkin
paths. As a result, we give a formula for the number of polyhedral regions
depending only on the number of taxa. We conclude with a computational
comparison between these polyhedral regions, to unveil biases introduced in any
implementation of the algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09345"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09333">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09333">Additive Approximation Schemes for Load Balancing Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Moritz Buchem, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vredeveld:Tjark.html">Tjark Vredeveld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiese:Andreas.html">Andreas Wiese</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09333">PDF</a><br /><b>Abstract: </b>In this paper we introduce the concept of additive approximation schemes and
apply it to load balancing problems. Additive approximation schemes aim to find
a solution with an absolute error in the objective of at most $\epsilon h$ for
some suitable parameter $h$. In the case that the parameter $h$ provides a
lower bound an additive approximation scheme implies a standard multiplicative
approximation scheme and can be much stronger when $h \ll$ OPT. On the other
hand, when no PTAS exists (or is unlikely to exist), additive approximation
schemes can provide a different notion for approximation.
</p>
<p>We consider the problem of assigning jobs to identical machines with lower
and upper bounds for the loads of the machines. This setting generalizes
problems like makespan minimization, the Santa Claus problem (on identical
machines), and the envy-minimizing Santa Claus problem. For the last problem,
in which the objective is to minimize the difference between the maximum and
minimum load, the optimal objective value may be zero and hence it is NP-hard
to obtain any multiplicative approximation guarantee. For this class of
problems we present additive approximation schemes for $h = p_{\max}$, the
maximum processing time of the jobs.
</p>
<p>Our technical contribution is two-fold. First, we introduce a new relaxation
based on integrally assigning slots to machines and fractionally assigning jobs
to the slots (the slot-MILP). We identify structural properties of
(near-)optimal solutions of the slot-MILP, which allow us to solve it
efficiently, assuming that there are $O(1)$ different lower and upper bounds on
the machine loads (which is the relevant setting for the three problems
mentioned above). The second technical contribution is a local-search based
algorithm which rounds a solution to the slot-MILP introducing an additive
error on the target load intervals of at most $\epsilon\cdot p_{\max}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09333"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09318">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09318">Monochromatic Triangles, Triangle Listing and APSP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yinzhan.html">Yinzhan Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09318">PDF</a><br /><b>Abstract: </b>One of the main hypotheses in fine-grained complexity is that All-Pairs
Shortest Paths (APSP) for $n$-node graphs requires $n^{3-o(1)}$ time. Another
famous hypothesis is that the $3$SUM problem for $n$ integers requires
$n^{2-o(1)}$ time. Although there are no direct reductions between $3$SUM and
APSP, it is known that they are related: there is a problem,
$(\min,+)$-convolution that reduces in a fine-grained way to both, and a
problem Exact Triangle that both fine-grained reduce to.
</p>
<p>In this paper we find more relationships between these two problems and other
basic problems. P\u{a}tra\c{s}cu had shown that under the $3$SUM hypothesis the
All-Edges Sparse Triangle problem in $m$-edge graphs requires $m^{4/3-o(1)}$
time. The latter problem asks to determine for every edge $e$, whether $e$ is
in a triangle. It is equivalent to the problem of listing $m$ triangles in an
$m$-edge graph where $m=\tilde{O}(n^{1.5})$, and can be solved in $O(m^{1.41})$
time [Alon et al.'97] with the current matrix multiplication bounds, and in
$\tilde{O}(m^{4/3})$ time if $\omega=2$.
</p>
<p>We show that one can reduce Exact Triangle to All-Edges Sparse Triangle,
showing that All-Edges Sparse Triangle (and hence Triangle Listing) requires
$m^{4/3-o(1)}$ time also assuming the APSP hypothesis. This allows us to
provide APSP-hardness for many dynamic problems that were previously known to
be hard under the $3$SUM hypothesis.
</p>
<p>We also consider the previously studied All-Edges Monochromatic Triangle
problem. Via work of [Lincoln et al.'20], our result on All-Edges Sparse
Triangle implies that if the All-Edges Monochromatic Triangle problem has an
$O(n^{2.5-\epsilon})$ time algorithm for $\epsilon&gt;0$, then both the APSP and
$3$SUM hypotheses are false. We also connect the problem to other
``intermediate'' problems, whose runtimes are between $O(n^\omega)$ and
$O(n^3)$, such as the Max-Min product problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09318"><span class="datestr">at July 21, 2020 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09282">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09282">An APX for the Maximum-Profit Routing Problem with Variable Supply</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Armaselu:Bogdan.html">Bogdan Armaselu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09282">PDF</a><br /><b>Abstract: </b>In this paper, we study the Maximum-Profit Routing Problem with Variable
Supply (MPRP-VS). This is a more general version of the Maximum-Profit Public
Transportation Route Planning Problem, or simply Maximum-Profit Routing Problem
(MPRP), introduced in \cite{Armaselu-PETRA}. In this new version, the quantity
$q_i(t)$ supplied at site $i$ is linearly increasing in time $t$, as opposed to
\cite{Armaselu-PETRA}, where the quantity is constant in time. Our main result
is a $5.5 \log{T} (1 + \epsilon) (1 + \frac{1}{1 + \sqrt{m}})^2$ approximation
algorithm, where $T$ is the latest time window and $m$ is the number of
vehicles used. In addition, we improve upon the MPRP algorithm in
\cite{Armaselu-PETRA} under certain conditions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09282"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09281">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09281">Efficient Iterative Solutions to Complex-Valued Nonlinear Least-Squares Problems with Mixed Linear and Antilinear Operators</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Tae_Hyung.html">Tae Hyung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haldar:Justin_P=.html">Justin P. Haldar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09281">PDF</a><br /><b>Abstract: </b>We consider a setting in which it is desired to find an optimal complex
vector $\mathbf{x}\in\mathbb{C}^N$ that satisfies $\mathcal{A}(\mathbf{x})
\approx \mathbf{b}$ in a least-squares sense, where $\mathbf{b} \in
\mathbb{C}^M$ is a data vector (possibly noise-corrupted), and
$\mathcal{A}(\cdot): \mathbb{C}^N \rightarrow \mathbb{C}^M$ is a measurement
operator. If $\mathcal{A}(\cdot)$ were linear, this reduces to the classical
linear least-squares problem, which has a well-known analytic solution as well
as powerful iterative solution algorithms. However, instead of linear
least-squares, this work considers the more complicated scenario where
$\mathcal{A}(\cdot)$ is nonlinear, but can be represented as the summation
and/or composition of some operators that are linear and some operators that
are antilinear. Some common nonlinear operations that have this structure
include complex conjugation or taking the real-part or imaginary-part of a
complex vector. Previous literature has shown that this kind of mixed
linear/antilinear least-squares problem can be mapped into a linear
least-squares problem by considering $\mathbf{x}$ as a vector in
$\mathbb{R}^{2N}$ instead of $\mathbb{C}^N$. While this approach is valid, the
replacement of the original complex-valued optimization problem with a
real-valued optimization problem can be complicated to implement, and can also
be associated with increased computational complexity. In this work, we
describe theory and computational methods that enable mixed linear/antilinear
least-squares problems to be solved iteratively using standard linear
least-squares tools, while retaining all of the complex-valued structure of the
original inverse problem. An illustration is provided to demonstrate that this
approach can simplify the implementation and reduce the computational
complexity of iterative solution algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09281"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09261">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09261">Frequency Estimation in Data Streams: Learning the Optimal Hashing Scheme</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bertsimas:Dimitris.html">Dimitris Bertsimas</a>, Vassilis Digalakis Jr <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09261">PDF</a><br /><b>Abstract: </b>We present a novel approach for the problem of frequency estimation in data
streams that is based on optimization and machine learning. Contrary to
state-of-the-art streaming frequency estimation algorithms, which heavily rely
on random hashing to maintain the frequency distribution of the data steam
using limited storage, the proposed approach exploits an observed stream prefix
to near-optimally hash elements and compress the target frequency distribution.
We develop an exact mixed-integer linear optimization formulation, as well as
an efficient block coordinate descent algorithm, that enable us to compute
near-optimal hashing schemes for elements seen in the observed stream prefix;
then, we use machine learning to hash unseen elements. We empirically evaluate
the proposed approach on real-world search query data and show that it
outperforms existing approaches by one to two orders of magnitude in terms of
its average (per element) estimation error and by 45-90% in terms of its
expected magnitude of estimation error.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09261"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09202">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09202">Query Complexity of Global Minimum Cut</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishnu:Arijit.html">Arijit Bishnu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paraashar:Manaswi.html">Manaswi Paraashar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09202">PDF</a><br /><b>Abstract: </b>In this work, we resolve the query complexity of global minimum cut problem
for a graph by designing a randomized algorithm for approximating the size of
minimum cut in a graph, where the graph can be accessed through local queries
like {\sc Degree}, {\sc Neighbor}, and {\sc Adjacency} queries.
</p>
<p>Given $\epsilon \in (0,1)$, the algorithm with high probability outputs an
estimate $\hat{t}$ satisfying the following $(1-\epsilon) t \leq \hat{t} \leq
(1+\epsilon) t$, where $m$ is the number of edges in the graph and $t$ is the
size of minimum cut in the graph. The expected number of local queries used by
our algorithm is $\min\left\{m+n,\frac{m}{t}\right\}\mbox{poly}\left(\log
n,\frac{1}{\epsilon}\right)$ where $n$ is the number of vertices in the graph.
Eden and Rosenbaum showed that $\Omega(m/t)$ many local queries are required
for approximating the size of minimum cut in graphs. These two results together
resolve the query complexity of the problem of estimating the size of minimum
cut in graphs using local queries.
</p>
<p>Building on the lower bound of Eden and Rosenbaum, we show that, for all $t
\in \mathbb{N}$, $\Omega(m)$ local queries are required to decide if the size
of the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t
\in \mathbb{N}$, $\Omega(m)$ local queries are required to find all the minimum
cut edges even if it is promised that the input graph has a minimum cut of size
$t$. Both of our lower bound results are randomized, and hold even if we can
make {\sc Random Edge} query apart from local queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09202"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09192">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09192">The Edit Distance to $k$-Subsequence Universality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fleischmann:Pamela.html">Pamela Fleischmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kosche:Maria.html">Maria Kosche</a>, Tore Koß, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manea:Florin.html">Florin Manea</a>, Stefan Siemer Kiel University, Computer Science Department, Germany, Göttingen University, Computer Science Department, Germany) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09192">PDF</a><br /><b>Abstract: </b>A word $u$ is a subsequence of another word $w$ if $u$ can be obtained from
$w$ by deleting some of its letters. The word $w$ with alph$(w)=\Sigma$ is
called $k$-subsequence universal if the set of subsequences of length $k$ of
$w$ contains all possible words of length $k$ over $\Sigma$. We propose a
series of efficient algorithms computing the minimal number of edit operations
(insertion, deletion, substitution) one needs to apply to a given word in order
to reach the set of $k$-subsequence universal words.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09192"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09172">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09172">Improved Approximations for Min Sum Vertex Cover and Generalized Min Sum Set Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bansal:Nikhil.html">Nikhil Bansal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Batra:Jatin.html">Jatin Batra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farhadi:Majid.html">Majid Farhadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tetali:Prasad.html">Prasad Tetali</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09172">PDF</a><br /><b>Abstract: </b>We study the generalized min sum set cover (GMSSC) problem, wherein given a
collection of hyperedges $E$ with arbitrary covering requirements $k_e$, the
goal is to find an ordering of the vertices to minimize the total cover time of
the hyperedges; a hyperedge $e$ is considered covered by the first time when
$k_e$ many of its vertices appear in the ordering. We give a $4.642$
approximation algorithm for GMSSC, coming close to the best possible bound of
$4$, already for the classical special case (with all $k_e=1$) of min sum set
cover (MSSC) studied by Feige, Lov\'{a}sz and Tetali, and improving upon the
previous best known bound of $12.4$ due to Im, Sviridenko and van der Zwaan.
Our algorithm is based on transforming the LP solution by a suitable kernel and
applying randomized rounding. This also gives an LP-based $4$ approximation for
MSSC. As part of the analysis of our algorithm, we also derive an inequality on
the lower tail of a sum of independent Bernoulli random variables, which might
be of independent interest and broader utility.
</p>
<p>Another well-known special case is the min sum vertex cover (MSVC) problem,
in which the input hypergraph is a graph and $k_e = 1$, for every edge. We give
a $16/9$ approximation for MSVC, and show a matching integrality gap for the
natural LP relaxation. This improves upon the previous best $1.999946$
approximation of Barenholz, Feige and Peleg. (The claimed $1.79$ approximation
result of Iwata, Tetali and Tripathi for the MSVC turned out have an
unfortunate, seemingly unfixable, mistake in it.) Finally, we revisit MSSC and
consider the $\ell_p$ norm of cover-time of the hyperedges. Using a dual
fitting argument, we show that the natural greedy algorithm achieves tight, up
to NP-hardness, approximation guarantees of $(p+1)^{1+1/p}$, for all $p\ge 1$.
For $p=1$, this gives yet another proof of the $4$ approximation for MSSC.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09172"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09116">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09116">The Combinatorial Santa Claus Problem or: How to Find Good Matchings in Non-Uniform Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Etienne Bamas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Paritosh.html">Paritosh Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09116">PDF</a><br /><b>Abstract: </b>We consider hypergraphs on vertices $P\cup R$ where each hyperedge contains
exactly one vertex in $P$. Our goal is to select a matching that covers all of
$P$, but we allow each selected hyperedge to drop all but an
$(1/\alpha)$-fraction of its intersection with $R$ (thus relaxing the matching
constraint). Here $\alpha$ is to be minimized. We dub this problem the
Combinatorial Santa Claus problem, since we show in this paper that this
problem and the Santa Claus problem are almost equivalent in terms of their
approximability.
</p>
<p>The non-trivial observation that any uniform regular hypergraph admits a
relaxed matching for $\alpha = O(1)$ was a major step in obtaining a constant
approximation rate for a special case of the Santa Claus problem, which
received great attention in literature. It is natural to ask if the uniformity
condition can be omitted. Our main result is that every (non-uniform) regular
hypergraph admits a relaxed matching for $\alpha = O(\log\log(|R|))$, when all
hyperedges are sufficiently large (a condition that is necessary). In
particular, this implies an $O(\log\log(|R|))$-approximation algorithm for the
Combinatorial Santa Claus problem with large hyperedges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09116"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09075">Efficient Linear and Affine Codes for Correcting Insertions/Deletions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Kuan.html">Kuan Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haeupler:Bernhard.html">Bernhard Haeupler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xin.html">Xin Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09075">PDF</a><br /><b>Abstract: </b>This paper studies \emph{linear} and \emph{affine} error-correcting codes for
correcting synchronization errors such as insertions and deletions. We call
such codes linear/affine insdel codes.
</p>
<p>Linear codes that can correct even a single deletion are limited to have
information rate at most $1/2$ (achieved by the trivial 2-fold repetition
code). Previously, it was (erroneously) reported that more generally no
non-trivial linear codes correcting $k$ deletions exist, i.e., that the
$(k+1)$-fold repetition codes and its rate of $1/(k+1)$ are basically optimal
for any $k$. We disprove this and show the existence of binary linear codes of
length $n$ and rate just below $1/2$ capable of correcting $\Omega(n)$
insertions and deletions. This identifies rate $1/2$ as a sharp threshold for
recovery from deletions for linear codes, and reopens the quest for a better
understanding of the capabilities of linear codes for correcting
insertions/deletions.
</p>
<p>We prove novel outer bounds and existential inner bounds for the rate vs.
(edit) distance trade-off of linear insdel codes. We complement our existential
results with an efficient synchronization-string-based transformation that
converts any asymptotically-good linear code for Hamming errors into an
asymptotically-good linear code for insdel errors. Lastly, we show that the
$\frac{1}{2}$-rate limitation does not hold for affine codes by giving an
explicit affine code of rate $1-\epsilon$ which can efficiently correct a
constant fraction of insdel errors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09075"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09065">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09065">Improved Approximation Factor for Adaptive Influence Maximization via Simple Greedy Strategies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Gianlorenzo D'Angelo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Poddar:Debashmita.html">Debashmita Poddar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinci:Cosimo.html">Cosimo Vinci</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09065">PDF</a><br /><b>Abstract: </b>In the adaptive influence maximization problem, we are given a social network
and a budget $k$, and we iteratively select $k$ nodes, called seeds, in order
to maximize the expected number of nodes that are reached by an influence
cascade that they generate according to a stochastic model for influence
diffusion. Differently from the non-adaptive influence maximization problem,
where all the seeds must be selected beforehand, here nodes are selected
sequentially one by one, and the decision on the $i$th seed is based on the
observed cascade produced by the first $i-1$ seeds. We focus on the myopic
feedback model, in which we can only observe which neighbors of previously
selected seeds have been influenced and on the independent cascade model, where
each edge is associated with an independent probability of diffusing influence.
Previous works showed that the adaptivity gap is at most $4$, which implies
that the non-adaptive greedy algorithm guarantees an approximation factor of
$\frac{1}{4}\left(1-\frac{1}{e}\right)$ for the adaptive problem. In this
paper, we improve the bounds on both the adaptivity gap and on the
approximation factor. We directly analyze the approximation factor of the
non-adaptive greedy algorithm, without passing through the adaptivity gap, and
show that it is at least $\frac{1}{2}\left(1-\frac{1}{e}\right)$. Therefore,
the adaptivity gap is at most $\frac{2e}{e-1}\approx 3.164$. To prove these
bounds, we introduce a new approach to relate the greedy non-adaptive algorithm
to the adaptive optimum. The new approach does not rely on multi-linear
extensions or random walks on optimal decision trees, which are commonly used
techniques in the field. We believe that it is of independent interest and may
be used to analyze other adaptive optimization problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09065"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09018">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09018">Solving hard cut problems via flow-augmentation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wahlstr=ouml=m:Magnus.html">Magnus Wahlström</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09018">PDF</a><br /><b>Abstract: </b>We present a new technique for designing FPT algorithms for graph cut
problems in undirected graphs, which we call flow augmentation. Our technique
is applicable to problems that can be phrased as a search for an (edge)
$(s,t)$-cut of cardinality at most $k$ in an undirected graph $G$ with
designated terminals $s$ and $t$.
</p>
<p>More precisely, we consider problems where an (unknown) solution is a set $Z
\subseteq E(G)$ of size at most $k$ such that (1) in $G-Z$, $s$ and $t$ are in
distinct connected components, (2) every edge of $Z$ connects two distinct
connected components of $G-Z$, and (3) if we define the set $Z_{s,t} \subseteq
Z$ as these edges $e \in Z$ for which there exists an $(s,t)$-path $P_e$ with
$E(P_e) \cap Z = \{e\}$, then $Z_{s,t}$ separates $s$ from $t$. We prove that
in this scenario one can in randomized time $k^{O(1)} (|V(G)|+|E(G)|)$ add a
number of edges to the graph so that with $2^{-O(k \log k)}$ probability no
added edge connects two components of $G-Z$ and $Z_{s,t}$ becomes a minimum cut
between $s$ and $t$.
</p>
<p>We apply our method to obtain a randomized FPT algorithm for a notorious
"hard nut" graph cut problem we call Coupled Min-Cut. This problem emerges out
of the study of FPT algorithms for Min CSP problems, and was unamenable to
other techniques for parameterized algorithms in graph cut problems, such as
Randomized Contractions, Treewidth Reduction or Shadow Removal.
</p>
<p>To demonstrate the power of the approach, we consider more generally Min
SAT($\Gamma$), parameterized by the solution cost. We show that every problem
Min SAT($\Gamma$) is either (1) FPT, (2) W[1]-hard, or (3) able to express the
soft constraint $(u \to v)$, and thereby also the min-cut problem in directed
graphs. All the W[1]-hard cases were known or immediate, and the main new
result is an FPT algorithm for a generalization of Coupled Min-Cut.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09018"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08914">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08914">All-Pairs LCA in DAGs: Breaking through the $O(n^{2.5})$ barrier</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grandoni:Fabrizio.html">Fabrizio Grandoni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Italiano:Giuseppe_F=.html">Giuseppe F. Italiano</a>, Aleksander Łukasiewicz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parotsidis:Nikos.html">Nikos Parotsidis</a>, Przemysław Uznański <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08914">PDF</a><br /><b>Abstract: </b>Let $G=(V,E)$ be an $n$-vertex directed acyclic graph (DAG). A lowest common
ancestor (LCA) of two vertices $u$ and $v$ is a common ancestor $w$ of $u$ and
$v$ such that no descendant of $w$ has the same property. In this paper, we
consider the problem of computing an LCA, if any, for all pairs of vertices in
a DAG. The fastest known algorithms for this problem exploit fast matrix
multiplication subroutines and have running times ranging from $O(n^{2.687})$
[Bender et al.~SODA'01] down to $O(n^{2.615})$ [Kowaluk and Lingas~ICALP'05]
and $O(n^{2.569})$ [Czumaj et al.~TCS'07]. Somewhat surprisingly, all those
bounds would still be $\Omega(n^{2.5})$ even if matrix multiplication could be
solved optimally (i.e., $\omega=2$). This appears to be an inherent barrier for
all the currently known approaches, which raises the natural question on
whether one could break through the $O(n^{2.5})$ barrier for this problem.
</p>
<p>In this paper, we answer this question affirmatively: in particular, we
present an $\tilde O(n^{2.447})$ ($\tilde O(n^{7/3})$ for $\omega=2$) algorithm
for finding an LCA for all pairs of vertices in a DAG, which represents the
first improvement on the running times for this problem in the last 13 years. A
key tool in our approach is a fast algorithm to partition the vertex set of the
transitive closure of $G$ into a collection of $O(\ell)$ chains and $O(n/\ell)$
antichains, for a given parameter $\ell$. As usual, a chain is a path while an
antichain is an independent set. We then find, for all pairs of vertices, a
\emph{candidate} LCA among the chain and antichain vertices, separately. The
first set is obtained via a reduction to min-max matrix multiplication. The
computation of the second set can be reduced to Boolean matrix multiplication
similarly to previous results on this problem. We finally combine the two
solutions together in a careful (non-obvious) manner.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08914"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08840">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08840">Adaptive Gradient Methods for Constrained Convex Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ene:Alina.html">Alina Ene</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vladu:Adrian.html">Adrian Vladu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08840">PDF</a><br /><b>Abstract: </b>We provide new adaptive first-order methods for constrained convex
optimization. Our main algorithm AdaAGD+ is an accelerated method, which is
universal in the sense that it achieves nearly-optimal convergence rates for
both smooth and non-smooth functions, even when it only has access to
stochastic gradients. In addition, it does not require any prior knowledge on
how the objective function is parametrized, since it automatically adjusts its
per-coordinate learning rate. This can be seen as a truly accelerated AdaGrad
method for constrained optimization.
</p>
<p>We complement it with a simpler algorithm AdaGrad+ which enjoys the same
features, and achieves the standard non-accelerated convergence rate. We also
present a set of new results involving adaptive methods for unconstrained
optimization and monotone operators.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08840"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08836">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08836">Efficient Exact Algorithms for Maximum Balanced Biclique Search in Bipartite Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lu.html">Lu Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Chengfei.html">Chengfei Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Rui.html">Rui Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiajie.html">Jiajie Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jianxin.html">Jianxin Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08836">PDF</a><br /><b>Abstract: </b>Given a bipartite graph, the maximum balanced biclique (\textsf{MBB})
problem, discovering a mutually connected while equal-sized disjoint sets with
the maximum cardinality, plays a significant role for mining the bipartite
graph and has numerous applications. Despite the NP-hardness of the
\textsf{MBB} problem, in this paper, we show that an exact \textsf{MBB} can be
discovered extremely fast in bipartite graphs for real applications. We propose
two exact algorithms dedicated for dense and sparse bipartite graphs
respectively. For dense bipartite graphs, an $\mathcal{O}^{*}( 1.3803^{n})$
algorithm is proposed. This algorithm in fact can find an \textsf{MBB} in near
polynomial time for dense bipartite graphs that are common for applications
such as VLSI design. This is because, using our proposed novel techniques, the
search can fast converge to sufficiently dense bipartite graphs which we prove
to be polynomially solvable. For large sparse bipartite graphs typical for
applications such as biological data analysis, an $\mathcal{O}^{*}(
1.3803^{\ddot{\delta}})$ algorithm is proposed, where $\ddot{\delta}$ is only a
few hundreds for large sparse bipartite graphs with millions of vertices. The
indispensible optimizations that lead to this time complexity are: we transform
a large sparse bipartite graph into a limited number of dense subgraphs with
size up to $\ddot{\delta}$ and then apply our proposed algorithm for dense
bipartite graphs on each of the subgraphs. To further speed up this algorithm,
tighter upper bounds, faster heuristics and effective reductions are proposed,
allowing an \textsf{MBB} to be discovered within a few seconds for bipartite
graphs with millions of vertices. Extensive experiments are conducted on
synthetic and real large bipartite graphs to demonstrate the efficiency and
effectiveness of our proposed algorithms and techniques.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08836"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08811">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08811">Parameterized Complexity of Graph Burning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08811">PDF</a><br /><b>Abstract: </b>Graph Burning asks, given a graph $G = (V,E)$ and an integer $k$, whether
there exists $(b_{0},\dots,b_{k-1}) \in V^{k}$ such that every vertex in $G$
has distance at most $i$ from some $b_{i}$. This problem is known to be
NP-complete even on connected caterpillars of maximum degree $3$. We study the
parameterized complexity of this problem and answer all questions arose by Kare
and Reddy [IWOCA 2019] about parameterized complexity of the problem. We show
that the problem is W[2]-complete parameterized by $k$ and that it does no
admit a polynomial kernel parameterized by vertex cover number unless
$\mathrm{NP} \subseteq \mathrm{coNP/poly}$. We also show that the problem is
fixed-parameter tractable parameterized by clique-width plus the maximum
diameter among all connected components. This implies the fixed-parameter
tractability parameterized by modular-width, by treedepth, and by distance to
cographs. Although the parameterization by distance to split graphs cannot be
handled with the clique-width argument, we show that this is also tractable by
a reduction to a generalized problem with a smaller solution size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08811"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08787">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08787">Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors, and Jumbled-Index Queries in String Reconstruction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Afshar:Ramtin.html">Ramtin Afshar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amir:Amihood.html">Amihood Amir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goodrich:Michael_T=.html">Michael T. Goodrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matias:Pedro.html">Pedro Matias</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08787">PDF</a><br /><b>Abstract: </b>We study the query complexity of exactly reconstructing a string from
adaptive queries, such as substring, subsequence, and jumbled-index queries.
Such problems have applications, e.g., in computational biology. We provide a
number of new and improved bounds for exact string reconstruction for settings
where either the string or the queries are "mixed-up".
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08787"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08761">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08761">Dominated Minimal Separators are Tame (Nearly All Others are Feral)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gartland:Peter.html">Peter Gartland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08761">PDF</a><br /><b>Abstract: </b>A class ${\cal F}$ of graphs is called {\em tame} if there exists a constant
$k$ so that every graph in ${\cal F}$ on $n$ vertices contains at most $O(n^k)$
minimal separators, {\em strongly-quasi-tame} if every graph in ${\cal F}$ on
$n$ vertices contains at most $O(n^{k \log n})$ minimal separators, and {\em
feral} if there exists a constant $c &gt; 1$ so that ${\cal F}$ contains
$n$-vertex graphs with at least $c^n$ minimal separators for arbitrarily large
$n$. The classification of graph classes into tame or feral has numerous
algorithmic consequences, and has recently received considerable attention.
</p>
<p>A key graph-theoretic object in the quest for such a classification is the
notion of a $k$-{\em creature}. In a recent manuscript [Abrishami et al., Arxiv
2020] conjecture that every hereditary class ${\cal F}$ that excludes
$k$-creatures for some fixed constant $k$ is tame. We give a counterexample to
this conjecture and prove the weaker result that a hereditary class ${\cal F}$
is strongly quasi-tame if it excludes $k$-creatures for some fixed constant $k$
and additionally every minimal separator can be dominated by another fixed
constant $k'$ number of vertices. The tools developed also lead to a number of
additional results of independent interest.
</p>
<p>{\bf (i) We obtain a complete classification of all hereditary graph classes
defined by a finite set of forbidden induced subgraphs into strongly quasi-tame
or feral. This generalizes Milani\v{c} and Piva\v{c} [WG'19]. {\bf (ii)} We
show that hereditary class that excludes $k$-creatures and additionally
excludes all cycles of length at least $c$, for some constant $c$, are tame.
This generalizes the result of [Chudnovsky et al., Arxiv 2019]. {\bf (iii)} We
show that every hereditary class that excludes $k$-creatures and additionally
excludes a complete graph on $c$ vertices for some fixed constant $c$ is tame.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08761"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08669">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08669">Memoryless Algorithms for the Generalized $k$-server Problem on Uniform Metrics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Christou:Dimitris.html">Dimitris Christou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fotakis:Dimitris.html">Dimitris Fotakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koumoutsos:Grigorios.html">Grigorios Koumoutsos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08669">PDF</a><br /><b>Abstract: </b>We consider the generalized $k$-server problem on uniform metrics. We study
the power of memoryless algorithms and show tight bounds of $\Theta(k!)$ on
their competitive ratio. In particular we show that the \textit{Harmonic
Algorithm} achieves this competitive ratio and provide matching lower bounds.
This improves the $\approx 2^{2^k}$ doubly-exponential bound of Chiplunkar and
Vishwanathan for the more general setting of uniform metrics with different
weights.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08669"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08643">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08643">Dynamic Geometric Independent Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhore:Sujoy.html">Sujoy Bhore</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cardinal:Jean.html">Jean Cardinal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iacono:John.html">John Iacono</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koumoutsos:Grigorios.html">Grigorios Koumoutsos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08643">PDF</a><br /><b>Abstract: </b>We present fully dynamic approximation algorithms for the Maximum Independent
Set problem on several types of geometric objects: intervals on the real line,
arbitrary axis-aligned squares in the plane and axis-aligned $d$-dimensional
hypercubes.
</p>
<p>It is known that a maximum independent set of a collection of $n$ intervals
can be found in $O(n\log n)$ time, while it is already \textsf{NP}-hard for a
set of unit squares. Moreover, the problem is inapproximable on many important
graph families, but admits a \textsf{PTAS} for a set of arbitrary pseudo-disks.
Therefore, a fundamental question in computational geometry is whether it is
possible to maintain an approximate maximum independent set in a set of dynamic
geometric objects, in truly sublinear time per insertion or deletion. In this
work, we answer this question in the affirmative for intervals, squares and
hypercubes.
</p>
<p>First, we show that for intervals a $(1+\varepsilon)$-approximate maximum
independent set can be maintained with logarithmic worst-case update time. This
is achieved by maintaining a locally optimal solution using a constant number
of constant-size exchanges per update.
</p>
<p>We then show how our interval structure can be used to design a data
structure for maintaining an expected constant factor approximate maximum
independent set of axis-aligned squares in the plane, with polylogarithmic
amortized update time. Our approach generalizes to $d$-dimensional hypercubes,
providing a $O(4^d)$-approximation with polylogarithmic update time.
</p>
<p>Those are the first approximation algorithms for any set of dynamic arbitrary
size geometric objects; previous results required bounded size ratios to obtain
polylogarithmic update time. Furthermore, it is known that our results for
squares (and hypercubes) cannot be improved to a
$(1+\varepsilon)$-approximation with the same update time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08643"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08585">Planar Distance Oracles with Better Time-Space Tradeoffs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Long:Yaowei.html">Yaowei Long</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pettie:Seth.html">Seth Pettie</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08585">PDF</a><br /><b>Abstract: </b>In a recent breakthrough, Charalampopoulos, Gawrychowski, Mozes, and Weimann
(STOC 2019) showed that exact distance queries on planar graphs could be
answered in $n^{o(1)}$ time by a data structure occupying $n^{1+o(1)}$ space,
i.e., up to $o(1)$ terms, optimal exponents in time (0) and space (1) can be
achieved simultaneously. Their distance query algorithm is recursive: it makes
successive calls to a point-location algorithm for planar Voronoi diagrams,
which involves many recursive distance queries. The depth of this recursion is
non-constant and the branching factor logarithmic, leading to $(\log
n)^{\omega(1)} = n^{o(1)}$ query times.
</p>
<p>In this paper we present a new way to do point-location in planar Voronoi
diagrams, which leads to a new exact distance oracle. At the two extremes of
our space-time tradeoff curve we can achieve either
</p>
<p>$n^{1+o(1)}$ space and $\log^{2+o(1)}n$ query time, or
</p>
<p>$n\log^{2+o(1)}n$ space and $n^{o(1)}$ query time.
</p>
<p>All previous oracles with $\tilde{O}(1)$ query time occupy space
$n^{1+\Omega(1)}$, and all previous oracles with space $\tilde{O}(n)$ answer
queries in $n^{\Omega(1)}$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08585"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.08575">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.08575">Polyhedral value iteration for discounted games and energy games</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozachinskiy:Alexander.html">Alexander Kozachinskiy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08575">PDF</a><br /><b>Abstract: </b>We present a deterministic algorithm solving discounted games with $n$ nodes
in strongly $n^{O(1)}\cdot (2 + \sqrt{2})^n$-time. For a special case of
bipartite discounted games our algorithm runs in $n^{O(1)}\cdot 2^n$-time.
Prior to our work no deterministic algorithm running in time $2^{o(n\log n)}$
regardless of the discount factor was known.
</p>
<p>We call our approach polyhedral value iteration. We rely on a well-known fact
that the values of a discounted game can be found from the so-called optimality
equations. In the algorithm we consider a polyhedron obtained by relaxing
optimality equations. We iterate the points on the border of this polyhedron by
moving each time along a carefully chosen shift as far as possible. This
continues until the current point satisfies optimality equations.
</p>
<p>Our approach is heavily inspired by a recent algorithm of Dorfman et al.
(ICALP 2019) for energy games. For completeness, we present their algorithm in
terms of polyhedral value iteration. Our exposition, unlike the original
algorithm, does not require edge weights to be integers and works for arbitrary
real weights.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.08575"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.01811">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.01811">JAMPI: efficient matrix multiplication in Spark using Barrier Execution Mode</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foldi:Tamas.html">Tamas Foldi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Csefalvay:Chris_von.html">Chris von Csefalvay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perez:Nicolas_A=.html">Nicolas A. Perez</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.01811">PDF</a><br /><b>Abstract: </b>The new barrier mode in Apache Spark allows embedding distributed deep
learning training as a Spark stage to simplify the distributed training
workflow. In Spark, a task in a stage does not depend on any other tasks in the
same stage, and hence it can be scheduled independently. However, several
algorithms require more sophisticated inter-task communications, similar to the
MPI paradigm. By combining distributed message passing (using asynchronous
network IO), OpenJDK's new auto-vectorization and Spark's barrier execution
mode, we can add non-map/reduce based algorithms, such as Cannon's distributed
matrix multiplication to Spark. We document an efficient distributed matrix
multiplication using Cannon's algorithm, which improves significantly on the
performance of the existing MLlib implementation. Used within a barrier task,
the algorithm described herein results in an up to 24 percent performance
increase on a 10,000x10,000 square matrix with a significantly lower memory
footprint. Applications of efficient matrix multiplication include, among
others, accelerating the training and implementation of deep convolutional
neural network based workloads, and thus such efficient algorithms can play a
ground-breaking role in faster, more efficient execution of even the most
complicated machine learning tasks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.01811"><span class="datestr">at July 21, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/stoc2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/stoc2020/">Conference Digest - STOC 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1911.08339">The Power of Factorization Mechanisms in Local and Central Differential Privacy</a> (<a href="https://www.youtube.com/watch?v=hSenRTxhZhM">video</a>)<br />
<a href="https://dblp.uni-trier.de/pers/hd/e/Edmonds:Alexander">Alexander Edmonds</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time</a> (<a href="https://www.youtube.com/watch?v=Tlc-z-MFAmM">video</a>)<br />
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints</a> (<a href="https://www.youtube.com/watch?v=AWgzaFOU_HM">video</a>)<br />
<a href="https://yuvaldagan.wordpress.com/">Yuval Dagan</a>, <a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail</a> (<a href="https://www.youtube.com/watch?v=sV59uoWJRnk">video</a>, <a href="https://www.youtube.com/watch?v=Fp7cgHRl8Yc">longer video</a>)<br />
<a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/stoc2020/"><span class="datestr">at July 20, 2020 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/transfer-learning/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/transfer-learning/">Transfer Learning with Adversarially Robust Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2007.08489" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/Microsoft/robust-models-transfer" class="bbutton">
<i class="fab fa-github"></i>
   Models and Code
</a>
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2007.08489">latest paper</a>, in collaboration with <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, we explore adversarial
robustness as an avenue for training computer vision models with more transferrable
features. We find that robust models outperform their standard counterparts on
a variety of transfer learning tasks.</i></p>

<h2 id="what-is-transfer-learning">What is transfer learning?</h2>

<p>Transfer learning is a paradigm where one leverages information
from a “source” task to better solve another “target” task. Particularly when there is little training data or compute available for solving the target
task, transfer learning provides a simple and efficient way to obtain performant
machine learning models.</p>

<p>Transfer learning has already proven its utility in many ML contexts. In natural language processing, for example, one can leverage language models pre-trained on large
text corpora to beat state-of-the-art performance on
tasks like query answering, entity recognition or part-of-speech classification.</p>

<p>In our work we focus on computer vision; in this context, a standard—and
remarkably successful—transfer learning pipeline is “ImageNet pre-training.”
This pipeline starts with a deep neural network trained on the <a href="http://image-net.org">ImageNet-1K</a>
dataset, and then refines this pre-trained model for a target task. The target task can range
from classification of smaller datasets (e.g., <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>) to more complex
tasks like object detection (e.g., <a href="http://host.robots.ox.ac.uk/pascal/VOC/">VOC</a>).</p>

<p>Although there are many ways in which one can refine a pre-trained model, we
will restrict our attention to the two most popular methods:</p>

<ul>
  <li><strong>Fixed-feature</strong>: In fixed-feature transfer learning, we replace the final
(linear) layer of the neural network with a new layer that has the correct
number of outputs for the target task. Then, keeping the rest of the layers
<em>fixed</em>, we train the newly replaced layer on the target task.</li>
  <li><strong>Full-network</strong>: In full-network transfer learning, we also replace the last
layer but do not freeze any layers afterwards. Instead, we use the pre-trained
network
as a sort of “initialization,” and continue training <em>all</em> the layers on the
target task.</li>
</ul>

<p>When at least a moderate amount of data is available, full-network transfer
learning typically outperforms the fixed-feature strategy.</p>

<h2 id="how-can-we-improve-transfer-learning">How can we improve transfer learning?</h2>

<p>Although we don’t have a comprehensive understanding of what makes transfer
learning algorithms tick, there has been a long line of work focused on identifying 
factors that improve (or worsen) performance (examples include
<a href="https://arxiv.org/abs/1406.5774">[1]</a>,
<a href="https://arxiv.org/abs/1608.08614">[2]</a>,
<a href="https://arxiv.org/abs/1805.08974">[3]</a>,
<a href="https://arxiv.org/abs/1804.08328">[4]</a>,
<a href="https://arxiv.org/abs/1411.1792">[5]</a>).</p>

<p>By design, the pre-trained ImageNet model itself plays a major role here:
indeed, a recent study by <a href="https://arxiv.org/abs/1805.08974">Kornblith, Shlens, and Le</a> finds that
pre-trained models which achieve a higher ImageNet accuracy also perform better when
transferred to downstream classification tasks, with a tight linear
correspondence between ImageNet accuracy and the accuracy on the target task:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/ksl.png" alt="A scatter plot of ImageNet accuracy versus downstream     transfer accuracy showing the linear relation." class="bigimg" /></p>
<div class="footnote">
Reproduced from <a href="https://arxiv.org/abs/1805.08974">[KSL19]</a>. 
Each dot is a pre-trained model whose $x$ coordinate is given by its 
ImageNet accuracy and $y$ coordinate is given by its downstream 
accuracy on the target task (after the corresponding refinement on that task).
</div>

<p>But is improving ImageNet accuracy of the pre-trained model the <em>only</em> way to improve transfer learning performance?</p>

<p>After all, we want to obtain models that have learned broadly applicable features from the
source dataset. ImageNet accuracy likely correlates with the quality of
features that a model has learned, but may not fully describe the downstream
utility of these features.
Ultimately, the nature of learned features stems from the <em>priors</em> placed on
them during training. For example, there have been studies of the (sometimes
implicit) priors imposed by architectural components (e.g., <a href="https://dmitryulyanov.github.io/deep_image_prior">convolutional layers</a>),
<a href="https://www.tandfonline.com/doi/abs/10.1198/10618600152418584">data</a>
<a href="https://arxiv.org/abs/1911.09071">augmentation</a>, 
<a href="https://arxiv.org/abs/1811.00401">loss functions</a> and even
<a href="https://stats385.github.io/assets/lectures/Stanford_Donoho_class_Nov_19.pdf">gradient descent</a> on neural network training.</p>

<p>In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, we study another prior: <em>adversarial robustness</em>.
Adversarial robustness—a rather frequent subject on this blog—refers to
model’s invariance to small (often imperceptible) perturbations of natural
inputs, called <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.</p>

<p>Standard neural networks (i.e., trained with the goal of maximizing
accuracy) are extremely vulnerable to such adversarial examples. For example,
with just a tiny perturbation to the pig image below, a pre-trained ImageNet
classifier will predict it as an “airliner” with 99% confidence:</p>

<p><img src="https://gradientscience.org/images/piggie.png" alt="An adversarial example: a pig on the left which is imperceptibly perturbed to be classified as an airliner on the right." /></p>
<div class="footnote">
A "pigs-can-fly" adversarial example: The "pig" image on the left is correctly classified by a standard ML model, but its imperceptibly perturbed counterpart on the right is classified as an "airliner" with 99% confidence.
</div>

<p>Adversarial robustness is thus typically induced at training time by replacing
the standard loss minimization objective with a <em>robust optimization</em> objective
(see our <a href="https://gradientscience.org/robust_opt_pt1">post on robust optimization</a> for more background):</p>



<p>The above objective trains models to be robust to image perturbations that are
small in (pixel-wise) $\ell_2$ (Euclidean) normIn reality, an $\ell_2$ ball doesn't perfectly capture the
set of imperceptible perturbations we want models to be robust to—but robustness with respect to this fairly rudimentary notion of perturbations turns out to be already non-trivial and very helpful.. 
The parameter $\varepsilon$ is a hyperparameter
governing the intended degree of invariance of the resulting models to the
corresponding perturbations. Setting 
$\varepsilon = 0$ corresponds to standard training, and increasing $\varepsilon$
asks the model to be robust to increasingly large perturbations.
In short, the objective asks the model to minimize risk on not only the 
training datapoints but also the entire radius-$\varepsilon$
neighbourhood around them.</p>

<p><em>[A quick plug: Our <a href="https://github.com/MadryLab/robustness"><code class="language-plaintext highlighter-rouge">robustness</code> Python library</a>, used for the code release of this paper, enables one to easily train and manipulate both standard and adversarially robust models.]</em></p>

<p>Although adversarial robustness has been initially studied solely through the lens of machine learning security, a line
of recent work (including some that’s been <a href="https://gradientscience.org/adv">previously</a> 
<a href="https://gradientscience.org/robust_apps">covered</a> on this blog) has begun to study
adversarially robust models in their own right, framing adversarial robustness
as a prior that forces models to learn features that are locally stable.
These works have found that on the one hand, adversarially robust models tend
to attain lower accuracy than their standardly-trained
counterparts.</p>

<p>On the other hand, recent work suggests that the feature
representations of robust models carry several advantages over those of
standard models, such as <a href="https://arxiv.org/abs/1805.12152">better-behaved</a>
<a href="https://arxiv.org/abs/1905.09797">gradients</a>, <a href="https://arxiv.org/abs/1910.08640">representation
invertibility</a>, and more <a href="https://arxiv.org/abs/2005.10190">specialized
features</a>.
We’ve actually discussed some of these observations in earlier posts on this
blog—see, e.g., our posts about 
<a href="https://gradientscience.org/robust_reps">representation learning</a> and 
<a href="https://gradientscience.org/robust_apps">image synthesis</a>.</p>

<p>These desirable properties
might suggest that robust neural networks are learning better feature
representations than standard networks, which could improve transfer
performance.</p>

<h3 id="adversarial-robustness-and-transfer-learning">Adversarial robustness and transfer learning</h3>

<p>So in summary, we have standard models with high accuracy on the source task but
little (or no) robustness; and we have adversarially robust models, which are
worse in terms of ImageNet accuracy, but have the “nice”
representational properties identified and discussed by prior works. Which
models are better for transfer learning?</p>

<p>To answer this question, we trained and examined a large collection
of standard and robust ImageNet models, while grid searching over a wide range of
hyperparameters and architectures to find the best model of each type. (All
models are available for download via our <a href="https://github.com/microsoft/robust-models-transfer">code/model
release</a> and more
details on our training procedure can be found there and in <a href="https://arxiv.org/abs/2007.08489">our
paper</a>). We then performed transfer
learning (using both fixed-feature and full-network refinement) from each
trained model to 12 downstream classification tasks.</p>

<p>It turns out that 
adversarially robust source models fairly consistently outperform their standard counterparts in
terms of downstream accuracy. In the table below, we compare the accuracies of
the best standard model (searching over hyperparameters and
architecture) and the best robust model (searching over the
previous factors as well as robustness level $\varepsilon$):</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/results-table.svg" style="width: 100%;" class="bigimg" alt="Table showing that robust models     perform better than their standard counterparts." /></p>
<div class="footnote">
    The main result: Adversarially robust models outperform their standard counterparts when transferred to downstream classification tasks.
</div>

<p>This difference in performance tends to be particularly striking in the context of fixed-feature transfer learning. The following graph shows, for each architecture and
downstream classification task, the best standard model compared to the best
robust model in that setting. As we can see, adversarially robust models
improve on the performance of their standard counterparts, and the gap tends to
<em>increase</em> as networks increase in width:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/LogisticRegression.svg" alt="A bar chart showing that robust models improve on     standard ones even without taking the maximum over architectures." class="bigimg" /></p>
<div class="footnote">
    Adversarially robust models tend to improve over standard networks for
    individual architectures too. (An analogous graph for full-network
    transfer learning is given in Figure 3 of <a href="https://arxiv.org/abs/2007.08489">our paper</a>.)
</div>

<p>Adversarial robustness improved downstream transfer
performance even when the target task was not a classification one. For example, the
following table compares standard and robust pre-training for use in downstream
object detection and instance segmentation:</p>

<p><img src="https://gradientscience.org/../assets/robust-transfer-learning/obj-det-results.svg" style="width: 80%;" class="bigimg" /></p>
<div class="footnote">
</div>

<h3 id="robustness-versus-accuracy">Robustness versus accuracy</h3>

<p>So it seems like robust models, despite being less accurate on the source task, are actually
better for transfer learning purposes. Indeed, the linear relation between
 ImageNet accuracy and transfer performance observed in prior work (see our discussion above) doesn’t seem
 to hold when the robustness parameter is varied. Compare the graphs below to the ones at the very start of this post:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/wide_resnet50_4_LogisticRegression.svg" class="bigimg" /></p>
<div class="footnote">
    Source-task (ImageNet) versus target (fixed-feature) accuracy for models with the same
    architecture while varying the robustness levels. Each dot is a
    WideResNet-50x4 model with $x$ coordinate given by source-task accuracy and
    $y$ coordinate given by fixed-feature transfer learning accuracy.
    Contrast the trends here with the "fixed-feature" trend in the first
    figure of this post—the linear trend depicted there largely disappears as less
    accurate but more robust models perform better in terms of transfer.
</div>

<p>How do we reconcile our observations with these trends observed by prior work?</p>

<p>We hypothesize that robustness and accuracy have <em>disentangled</em> effects on
transfer performance. That is, for a fixed level of robustness, higher
accuracy on the source task helps transfer, and for a fixed level of
accuracy, increased robustness helps transfer. Indeed, as shown below, for a
fixed level of robustness, the accuracy-transfer relation tends to hold
strongly:</p>

<p><img src="https://gradientscience.org/assets/robust-transfer-learning/fixed-robustness.svg" class="bigimg" /></p>
<div class="footnote">
Even though robust models appear to break the linear
accuracy-transfer trend, this trend is actually preserved for a fixed value of
robustness. Each dot in the graph is a different architecture, trained for the same level of robustness ($\varepsilon = 3.0$). The $x$ coordinate is source task (ImageNet) accuracy, and the $y$ coordinate is the downstream accuracy on each target dataset.
</div>

<p>In addition to reconciling our results with those of prior work, these findings suggest that ongoing work on developing more accurate robust models
may have the added benefit of further improving transfer learning performance.</p>

<h3 id="other-empirical-mysteries-and-future-work">Other empirical mysteries and future work</h3>

<p>This post discussed how adversarially robust models might constitute a promising
avenue for improving transfer learning, and already often outperform standard
models in terms of downstream accuracy. In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, 
we study this phenomenon more closely: for example, we examine the effects of
model width, and we compare adversarial robustness to other notions of
robustness. We also uncover a few somewhat mysterious properties: for example,
resizing images seems to have a non-trivial effect on the relationship between
robustness and downstream accuracy.</p>

<p>Finally, while our work provides evidence that adversarially
robust computer vision models transfer better, understanding precisely <em>why</em> this is the case remains open. More broadly, the results we
observe indicate that we still do not yet fully understand (even empirically)
the ingredients that make transfer learning successful. We hope that our work
prompts an inquiry into the underpinnings of modern transfer learning.</p></div>







<p class="date">
<a href="https://gradientscience.org/transfer-learning/"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09099">A dichotomy theorem for nonuniform CSPs simplified</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bulatov:Andrei_A=.html">Andrei A. Bulatov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09099">PDF</a><br /><b>Abstract: </b>In a non-uniform Constraint Satisfaction problem CSP(G), where G is a set of
relations on a finite set A, the goal is to find an assignment of values to
variables subject to constraints imposed on specified sets of variables using
the relations from G. The Dichotomy Conjecture for the non-uniform CSP states
that for every constraint language G the problem CSP(G) is either solvable in
polynomial time or is NP-complete. It was proposed by Feder and Vardi in their
seminal 1993 paper. In this paper we confirm the Dichotomy Conjecture.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09099"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09045">Integer factorization and Riemann's hypothesis: Why two-item joint replenishment is hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulz:Andreas_S=.html">Andreas S. Schulz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telha:Claudio.html">Claudio Telha</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09045">PDF</a><br /><b>Abstract: </b>Distribution networks with periodically repeating events often hold great
promise to exploit economies of scale. Joint replenishment problems are a
fundamental model in inventory management, manufacturing, and logistics that
capture these effects. However, finding an efficient algorithm that optimally
solves these models, or showing that none may exist, has long been open,
regardless of whether empty joint orders are possible or not. In either case,
we show that finding optimal solutions to joint replenishment instances with
just two products is at least as difficult as integer factorization. To the
best of the authors' knowledge, this is the first time that integer
factorization is used to explain the computational hardness of any kind of
optimization problem. Under the assumption that Riemann's Hypothesis is
correct, we can actually prove that the two-item joint replenishment problem
with possibly empty joint ordering points is NP-complete under randomized
reductions, which implies that not even quantum computers may be able to solve
it efficiently. By relating the computational complexity of joint replenishment
to cryptography, prime decomposition, and other aspects of prime numbers, a
similar approach may help to establish (integer factorization) hardness of
additional open periodic problems in supply chain management and beyond, whose
solution has eluded standard methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09045"><span class="datestr">at July 20, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.09023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.09023">Parameterized Complexity of Scheduling Chains of Jobs with Delays</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodlaender:Hans_L=.html">Hans L. Bodlaender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wegen:Marieke_van_der.html">Marieke van der Wegen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09023">PDF</a><br /><b>Abstract: </b>In this paper, we consider the parameterized complexity of the following
scheduling problem. We must schedule a number of jobs on $m$ machines, where
each job has unit length, and the graph of precedence constraints consists of a
set of chains. Each precedence constraint is labelled with an integer that
denotes the exact (or minimum) delay between the jobs. We study different
cases; delays can be given in unary and in binary, and the case that we have a
single machine is discussed separately. We consider the complexity of this
problem parameterized by the number of chains, and by the thickness of the
instance, which is the maximum number of chains whose intervals between release
date and deadline overlap.
</p>
<p>We show that this scheduling problem with exact delays in unary is
$W[t]$-hard for all $t$, when parameterized by the thickness, even when we have
a single machine ($m = 1$). When parameterized by the number of chains, this
problem is $W[1]$-complete when we have a single or a constant number of
machines, and $W[2]$-complete when the number of machines is a variable. The
problem with minimum delays, given in unary, parameterized by the number of
chains (and as a simple corollary, also when parameterized by the thickness) is
$W[1]$-hard for a single or a constant number of machines, and $W[2]$-hard when
the number of machines is variable.
</p>
<p>With a dynamic programming algorithm, one can show membership in XP for exact
and minimum delays in unary, for any number of machines, when parameterized by
thickness or number of chains. For a single machine, with exact delays in
binary, parameterized by the number of chains, membership in XP can be shown
with branching and solving a system of difference constraints. For all other
cases for delays in binary, membership in XP is open.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.09023"><span class="datestr">at July 20, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2418581440113974615">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html">Erdos-Turan for k=3 is True!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
(All of the math in this post is summarized (without proofs) in a writeup by Erik Metz and myself which you can find <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/3apblog.pdf">here</a>. It is a pdf file so you can click on links in it to get to the papers it refers to. There have been posts on this topic by <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">Gil Kalai</a> and  <a href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/">Luca Trevisan</a>. If you know of others then let me know so I can add them to this post.)<br />
<br />
<br />
<br />
This is a sequel to <a href="https://blog.computationalcomplexity.org/2010/12/breakthrough-result-on-density-and-3.html">A BREAKTHROUGH result on density and 3-AP's</a> and <a href="https://blog.computationalcomplexity.org/2017/06/big-news-on-w3r.html">Big news on W(3,r)!</a><br />
<br />
For this post N is large, and all inequalites have a big-O or a big-Omega.<br />
<br />
For this post [N] is {1,...,N}<br />
<br />
Let<br />
<br />
r(N) be the least w such that if A is a subset of [N] and |A|  &gt;  w, then A has a 3-AP.<br />
<br />
There has been a long sequence of results getting smaller and smaller upper bounds on r(N).<br />
<br />
The motivation for getting these results is that if r(N) is &lt; N/(log N)^{1+\delta} with delta&gt;0 then the following holds:<br />
<br />
If sum_{x\in A} 1/x diverges then A has a 3-AP.<br />
<br />
This is the k=3 case of one of the Erdos-Turan Conjectures.<br />
<br />
Bloom and Sisack HAVE gotten N/(log N)^{1+delta} so they HAVE gotten ET k=3. Wow!<br />
<br />
1) I am NOT surprised that its true.<br />
<br />
2) I am SHOCKED and DELIGHTED that it was proven.  Shocked because the results leading up to it (see the write up referenced at the beginning of this post) seemed Zeno-like, approaching the result needed got but not getting there. Delighted because... uh, as the kids say, just cause.<br />
<br />
I've heard that k=4 really is much harder (see my comments and Gil's response on his blog post, pointed to at the beginning of this post)  and it is true that there has been far less progress on that case (the write up I pointed to at the beginning of this post says what is known). Hence I will again be <i>shocked </i>if it is proven.  So, unlike The Who (see <a href="https://www.youtube.com/watch?v=UDfAdHBtK_Q">here</a>) I CAN be fooled again. That's okay--- I will  be <i>delighted</i>. (ADDED LATER- there are more comments no Gil's website, from Thomas Bloom and Ben Green about what is likely to happen in the next 10 years.)<br />
<br />
Erdos offered a prize of $3000 for a proof that A has, for all k, a k-AP.  The prize is now $5000. After Erdos passed away Ronald Graham became the Erdos-Bank and paid out the money when people solved a problem Erdos put a bounty on. What happens now? (If I have the facts wrong and/or if you know the answer, please leave a polite and enlightening comment.)<br />
<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html"><span class="datestr">at July 19, 2020 07:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/109">TR20-109 |  On Testing Hamiltonicity in the Bounded Degree Graph Model | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that testing Hamiltonicity in the bounded-degree graph model requires a linear number of queries. This refers to both the path and the cycle versions of the problem, and similar results hold also for the directed analogues.
In addition, we present an alternative proof for the known fact that testing Independent Set Size (in this model) requires a linear number of queries.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/109"><span class="datestr">at July 19, 2020 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
