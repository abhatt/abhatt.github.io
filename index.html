<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 04, 2020 02:30 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01929">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01929">The Complexity of Gradient Descent: CLS = PPAD $\cap$ PLS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fearnley:John.html">John Fearnley</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldberg:Paul_W=.html">Paul W. Goldberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hollender:Alexandros.html">Alexandros Hollender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Savani:Rahul.html">Rahul Savani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01929">PDF</a><br /><b>Abstract: </b>We study search problems that can be solved by performing Gradient Descent on
a bounded convex polytopal domain and show that this class is equal to the
intersection of two well-known classes: PPAD and PLS. As our main underlying
technical contribution, we show that computing a Karush-Kuhn-Tucker (KKT) point
of a continuously differentiable function over the domain $[0,1]^2$ is PPAD
$\cap$ PLS-complete. This is the first natural problem to be shown complete for
this class. Our results also imply that the class CLS (Continuous Local Search)
- which was defined by Daskalakis and Papadimitriou as a more "natural"
counterpart to PPAD $\cap$ PLS and contains many interesting problems - is
itself equal to PPAD $\cap$ PLS.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01929"><span class="datestr">at November 04, 2020 02:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01898">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01898">Periodic Scheduling and Packing Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanen:Claire.html">Claire Hanen</a>, Zdenek Hanzalek <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01898">PDF</a><br /><b>Abstract: </b>This paper is motivated by periodic data transmission in autonomous cars. We
considered periodic tasks (with different periods) on one or several machines.
After reviewing the literature on the subject, we managed to generalize a
result of Lukasiewicz et al. (i.e., the equivalence of periodic scheduling with
the power of two periods and special 2D bin packing) to harmonic periods.
Furthermore, we use quite old results by Coffman, Garey, and Johnson to get an
approximation algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01898"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01851">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01851">On the Computability of Continuous Maximum Entropy Distributions: Adjoint Orbits of Lie Groups</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leake:Jonathan.html">Jonathan Leake</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vishnoi:Nisheeth_K=.html">Nisheeth K. Vishnoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01851">PDF</a><br /><b>Abstract: </b>Given a point $A$ in the convex hull of a given adjoint orbit
$\mathcal{O}(F)$ of a compact Lie group $G$, we give a polynomial time
algorithm to compute the probability density supported on $\mathcal{O}(F)$
whose expectation is $A$ and that minimizes the Kullback-Leibler divergence to
the $G$-invariant measure on $\mathcal{O}(F)$. This significantly extends the
recent work of the authors (STOC 2020) who presented such a result for the
manifold of rank $k$-projections which is a specific adjoint orbit of the
unitary group $\mathrm{U}(n)$. Our result relies on the ellipsoid method-based
framework proposed in prior work; however, to apply it to the general setting
of compact Lie groups, we need tools from Lie theory. For instance, properties
of the adjoint representation are used to find the defining equalities of the
minimal affine space containing the convex hull of $\mathcal{O}(F)$, and to
establish a bound on the optimal dual solution. Also, the Harish-Chandra
integral formula is used to obtain an evaluation oracle for the dual objective
function. While the Harish-Chandra integral formula allows us to write certain
integrals over the adjoint orbit of a Lie group as a sum of a small number of
determinants, it is only defined for elements of a chosen Cartan subalgebra of
the Lie algebra $\mathfrak{g}$ of $G.$ We show how it can be applied to our
setting with the help of Kostant's convexity theorem. Further, the convex hull
of an adjoint orbit is a type of orbitope, and the orbitopes studied in this
paper are known to be spectrahedral. Thus our main result can be viewed as
extending the maximum entropy framework to a class of spectrahedra.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01851"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01777">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01777">Near-Optimal Entrywise Sampling of Numerically Sparse Matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Vladimir.html">Vladimir Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, Aditya Krishnan, Shay Sapir <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01777">PDF</a><br /><b>Abstract: </b>Many real-world data sets are sparse or almost sparse. One method to measure
this for a matrix $A\in \mathbb{R}^{n\times n}$ is the \emph{numerical
sparsity}, denoted $\mathsf{ns}(A)$, defined as the minimum $k\geq 1$ such that
$\|a\|_1/\|a\|_2 \leq \sqrt{k}$ for every row and every column $a$ of $A$. This
measure of $a$ is smooth and is clearly only smaller than the number of
non-zeros in the row/column $a$. The seminal work of Achlioptas and McSherry
[2007] has put forward the question of approximating an input matrix $A$ by
entrywise sampling. More precisely, the goal is to quickly compute a sparse
matrix $\tilde{A}$ satisfying $\|A - \tilde{A}\|_2 \leq \epsilon \|A\|_2$
(i.e., additive spectral approximation) given an error parameter $\epsilon&gt;0$.
The known schemes sample and rescale a small fraction of entries from $A$. We
propose a scheme that sparsifies an almost-sparse matrix $A$ -- it produces a
matrix $\tilde{A}$ with $O(\epsilon^{-2}\mathsf{ns}(A) \cdot n\ln n)$ non-zero
entries with high probability. We also prove that this upper bound on
$\mathsf{nnz}(\tilde{A})$ is \emph{tight} up to logarithmic factors. Moreover,
our upper bound improves when the spectrum of $A$ decays quickly (roughly
replacing $n$ with the stable rank of $A$). Our scheme can be implemented in
time $O(\mathsf{nnz}(A))$ when $\|A\|_2$ is given. Previously, a similar upper
bound was obtained by Achlioptas et. al [2013] but only for a restricted class
of inputs that does not even include symmetric or covariance matrices. Finally,
we demonstrate two applications of these sampling techniques, to faster
approximate matrix multiplication, and to ridge regression by using sparse
preconditioners.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01777"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01770">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01770">The Complexity of Finding Fair Independent Sets in Cycles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haviv:Ishay.html">Ishay Haviv</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01770">PDF</a><br /><b>Abstract: </b>Let $G$ be a cycle graph and let $V_1,\ldots,V_m$ be a partition of its
vertex set into $m$ sets. An independent set $S$ of $G$ is said to fairly
represent the partition if $|S \cap V_i| \geq \frac{1}{2} \cdot |V_i| -1$ for
all $i \in [m]$. It is known that for every cycle and every partition of its
vertex set, there exists an independent set that fairly represents the
partition (Aharoni et al., A Journey through Discrete Math., 2017). We prove
that the problem of finding such an independent set is $\mathsf{PPA}$-complete.
As an application, we show that the problem of finding a monochromatic edge in
a Schrijver graph, given a succinct representation of a coloring that uses
fewer colors than its chromatic number, is $\mathsf{PPA}$-complete as well. The
work is motivated by the computational aspects of the `cycle plus triangles'
problem and of its extensions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01770"><span class="datestr">at November 04, 2020 02:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01726">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01726">Search Problems in Trees with Symmetries: near optimal traversal strategies for individualization-refinement algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anders:Markus.html">Markus Anders</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schweitzer:Pascal.html">Pascal Schweitzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01726">PDF</a><br /><b>Abstract: </b>We define a search problem on trees that closely captures the backtracking
behavior of all current practical graph isomorphism algorithms. Given two trees
with colored leaves, the goal is to find two leaves of matching color, one in
each of the trees. The trees are subject to an invariance property which
promises that for every pair of leaves of equal color there must be a symmetry
(or an isomorphism) that maps one leaf to the other.
</p>
<p>We describe a randomized algorithm with errors for which the number of
visited leaves is quasilinear in the square root of the size of the smaller of
the two trees. For inputs of bounded degree, we develop a Las Vegas algorithm
with a similar running time.
</p>
<p>We prove that these results are optimal up to logarithmic factors. We show a
lower bound for randomized algorithms on inputs of bounded degree that is the
square root of the tree sizes. For inputs of unbounded degree, we show a linear
lower bound for Las Vegas algorithms. For deterministic algorithms we can prove
a linear bound even for inputs of bounded degree. This shows why randomized
algorithms outperform deterministic ones.
</p>
<p>Our results explain why the randomized "breadth-first with intermixed
experimental path" search strategy of the isomorphism tool Traces (Piperno
2008) is often superior to the depth-first search strategy of other tools such
as nauty (McKay 1977) or bliss (Junttila, Kaski 2007). However, our algorithm
also provides a new traversal strategy, which is theoretically near optimal
with better worst case behavior than traversal strategies that have previously
been used.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01726"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01649">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01649">The Long, the Short and the Random</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Camerani:Giorgio.html">Giorgio Camerani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01649">PDF</a><br /><b>Abstract: </b>We furnish solid evidence, both theoretical and empirical, towards the
existence of a deterministic algorithm for random sparse $\#\Omega(\log n)$-SAT
instances, which computes the exact counting of satisfying assignments in
sub-exponential time. The algorithm uses a nice combinatorial property that
every CNF formula has, which relates its number of unsatisfying assignments to
the space of its monotone sub-formulae.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01649"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01647">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01647">Uncertainty Quantification of Darcy Flow through Porous Media using Deep Gaussian Process</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daneshkhah:A=.html">A. Daneshkhah</a>, M. Mousavi Nezhad, O. Chatrabgoun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esmaeilbeigi:M=.html">M. Esmaeilbeigi</a>, T. Sedighi, S. Abolfathi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01647">PDF</a><br /><b>Abstract: </b>A computational method based on the non-linear Gaussian process (GP), known
as deep Gaussian processes (deep GPs) for uncertainty quantification &amp;
propagation in modelling of flow through heterogeneous porous media is
presented. The method is also used for reducing dimensionality of model output
and consequently emulating highly complex relationship between hydrogeological
properties and reduced order fluid velocity field in a tractable manner. Deep
GPs are multi-layer hierarchical generalisations of GPs with multiple,
infinitely wide hidden layers that are very efficient models for deep learning
and modelling of high-dimensional complex systems by tackling the complexity
through several hidden layers connected with non-linear mappings. According to
this approach, the hydrogeological data is modelled as the output of a
multivariate GP whose inputs are governed by another GP such that each single
layer is either a standard GP or the Gaussian process latent variable model. A
variational approximation framework is used so that the posterior distribution
of the model outputs associated to given inputs can be analytically
approximated. In contrast to the other dimensionality reduction, methods that
do not provide any information about the dimensionality of each hidden layer,
the proposed method automatically selects the dimensionality of each hidden
layer and it can be used to propagate uncertainty obtained in each layer across
the hierarchy. Using this, dimensionality of the full input space consists of
both geometrical parameters of modelling domain and stochastic hydrogeological
parameters can be simultaneously reduced without the need for any
simplifications generally being assumed for stochastic modelling of subsurface
flow problems. It allows estimation of the flow statistics with greatly reduced
computational efforts compared to other stochastic approaches such as Monte
Carlo method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01647"><span class="datestr">at November 04, 2020 02:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01584">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01584">Estimating decision tree learnability with polylogarithmic sample complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Neha.html">Neha Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01584">PDF</a><br /><b>Abstract: </b>We show that top-down decision tree learning heuristics are amenable to
highly efficient learnability estimation: for monotone target functions, the
error of the decision tree hypothesis constructed by these heuristics can be
estimated with polylogarithmically many labeled examples, exponentially smaller
than the number necessary to run these heuristics, and indeed, exponentially
smaller than information-theoretic minimum required to learn a good decision
tree. This adds to a small but growing list of fundamental learning algorithms
that have been shown to be amenable to learnability estimation.
</p>
<p>En route to this result, we design and analyze sample-efficient minibatch
versions of top-down decision tree learning heuristics and show that they
achieve the same provable guarantees as the full-batch versions. We further
give "active local" versions of these heuristics: given a test point $x^\star$,
we show how the label $T(x^\star)$ of the decision tree hypothesis $T$ can be
computed with polylogarithmically many labeled examples, exponentially smaller
than the number necessary to learn $T$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01584"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01564">Fast Computation of Strong Control Dependencies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalupa:Marek.html">Marek Chalupa</a>, David Klaška, Jan Strejček, Lukáš Tomovič <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01564">PDF</a><br /><b>Abstract: </b>We introduce new algorithms for computing non-termination sensitive control
dependence (NTSCD) and decisive order dependence (DOD). These relations on
control flow graph vertices have many applications including program slicing
and compiler optimizations. Our algorithms are asymptotically faster than the
current algorithms. We also show that the original algorithms for computing
NTSCD and DOD may produce incorrect results. We implemented the new as well as
fixed versions of the original algorithms for the computation of NTSCD and DOD
and we experimentally compare their performance and outcome. Our algorithms
dramatically outperform the original ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01564"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01559">Secretary Matching with General Arrivals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ezra:Tomer.html">Tomer Ezra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Michal.html">Michal Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gravin:Nick.html">Nick Gravin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Zhihao_Gavin.html">Zhihao Gavin Tang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01559">PDF</a><br /><b>Abstract: </b>We provide online algorithms for secretary matching in general weighted
graphs, under the well-studied models of vertex and edge arrivals. In both
models, edges are associated with arbitrary weights that are unknown from the
outset, and are revealed online. Under vertex arrival, vertices arrive online
in a uniformly random order; upon the arrival of a vertex $v$, the weights of
edges from $v$ to all previously arriving vertices are revealed, and the
algorithm decides which of these edges, if any, to include in the matching.
Under edge arrival, edges arrive online in a uniformly random order; upon the
arrival of an edge $e$, its weight is revealed, and the algorithm decides
whether to include it in the matching or not. We provide a $5/12$-competitive
algorithm for vertex arrival, and show it is tight. For edge arrival, we
provide a $1/4$-competitive algorithm. Both results improve upon state of the
art bounds for the corresponding settings. Interestingly, for vertex arrival,
secretary matching in general graphs outperforms secretary matching in
bipartite graphs with 1-sided arrival, where $1/e$ is the best possible
guarantee.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01559"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01489">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01489">Formal Validation of Recursive Backtracking Algorithms: The Case of Listing Stable Extensions in the Directed Graphs of Argumentation Frameworks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nofal:Samer.html">Samer Nofal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jabal:Amani_Abu.html">Amani Abu Jabal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alfarrarjeh:Abdullah.html">Abdullah Alfarrarjeh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hababeh:Ismail.html">Ismail Hababeh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01489">PDF</a><br /><b>Abstract: </b>An \textit{abstract argumentation framework} ({\sc af} for short) is a
directed graph $(A,R)$ where $A$ is a set of \textit{abstract arguments} and
$R\subseteq A \times A$ is the \textit{attack} relation. Let $H=(A,R)$ be an
{\sc af}, $S \subseteq A$ be a set of arguments and $S^+ = \{y \mid \exists
x\in S \text{ with }(x,y)\in R\}$. Then, $S$ is a \textit{stable extension} in
$H$ if and only if $S^+ = A\setminus S$. In this paper, we present a thorough,
formal validation of a known backtracking algorithm for listing all stable
extensions in a given {\sc af}.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01489"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01441">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01441">Balanced Partitioning of Several Cache-Oblivious Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Yuan.html">Yuan Tang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Weiguo.html">Weiguo Gao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01441">PDF</a><br /><b>Abstract: </b>Frigo et al. proposed an ideal cache model and a recursive technique to
design sequential cache-efficient algorithms in a cache-oblivious fashion.
Ballard et al. pointed out that it is a fundamental open problem to extend the
technique to an arbitrary architecture. Ballard et al. raised another open
question on how to parallelize Strassen's algorithm exactly and efficiently on
an arbitrary number of processors.
</p>
<p>We propose a novel way of partitioning a cache-oblivious algorithm to achieve
perfect strong scaling on an arbitrary number, even a prime number, of
processors within a certain range in a shared-memory setting. Our approach is
Processor-Aware but Cache-Oblivious (PACO). We demonstrate our approach on
several important cache-oblivious algorithms, including LCS, 1D, GAP, classic
rectangular matrix multiplication on a semiring, and Strassen's algorithm. We
discuss how to extend our approach to a distributed-memory architecture, or
even a heterogeneous computing system. Hence, our work may provide a new
perspective on the fundamental open problem of extending the recursive
cache-oblivious technique to an arbitrary architecture. We provide an almost
exact solution to the open problem on parallelizing Strassen. Our approach may
provide a new perspective on extending the recursive cache-oblivious technique
to an arbitrary architecture. All our algorithms demonstrate better scalability
or better overall parallel cache complexities than the best known algorithms.
Preliminary experiments justify our theoretical prediction that the PACO
algorithms can outperform significantly state-of-the-art Processor-Oblivious
(PO) and Processor-Aware (PA) counterparts.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01441"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01435">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01435">Robust Algorithms for Online Convex Problems via Primal-Dual</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molinaro:Marco.html">Marco Molinaro</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01435">PDF</a><br /><b>Abstract: </b>Primal-dual methods in online optimization give several of the state-of-the
art results in both of the most common models: adversarial and
stochastic/random order. Here we try to provide a more unified analysis of
primal-dual algorithms to better understand the mechanisms behind this
important method. With this we are able of recover and extend in one goal
several results of the literature.
</p>
<p>In particular, we obtain robust online algorithm for fairly general online
convex problems: we consider the MIXED model where in some of the time steps
the data is stochastic and in the others the data is adversarial. Both the
quantity and location of the adversarial time steps are unknown to the
algorithm. The guarantees of our algorithms interpolate between the (close to)
best guarantees for each of the pure models. In particular, the presence of
adversarial times does not degrade the guarantee relative to the stochastic
part of the instance.
</p>
<p>Concretely, we first consider Online Convex Programming: at each time a
feasible set $V_t$ is revealed, and the algorithm needs to select $v_t \in V_t$
to minimize the total cost $\psi(\sum_t v_t)$, for a convex function $\psi$.
Our robust primal-dual algorithm for this problem on the MIXED model recovers
and extends, for example, a result of Gupta et al. and recent work on
$\ell_p$-norm load balancing by the author. We also consider the problem of
Welfare Maximization with Convex Production Costs: at each time a customer
presents a value $c_t$ and resource consumption vector $a_t$, and the goal is
to fractionally select customers to maximize the profit $\sum_t c_t x_t -
\psi(\sum_t a_t x_t)$. Our robust primal-dual algorithm on the MIXED model
recovers and extends the result of Azar et al.
</p>
<p>Given the ubiquity of primal-dual algorithms we hope the ideas presented here
will be useful in obtaining other robust algorithm in the MIXED or related
models.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01435"><span class="datestr">at November 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.01366">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.01366">Recent Advances on the Graph Isomorphism Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grohe:Martin.html">Martin Grohe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neuen:Daniel.html">Daniel Neuen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.01366">PDF</a><br /><b>Abstract: </b>We give an overview of recent advances on the graph isomorphism problem. Our
main focus will be on Babai's quasi-polynomial time isomorphism test and
subsequent developments that led to the design of isomorphism algorithms with a
quasi-polynomial parameterized running time of the from $n^{\polylog k}$, where
$k$ is a graph parameter such as the maximum degree. A second focus will be the
combinatorial Weisfeiler-Leman algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.01366"><span class="datestr">at November 04, 2020 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00503">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00503">A Lower Bound for Dynamic Fractional Cascading</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Afshani:Peyman.html">Peyman Afshani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00503">PDF</a><br /><b>Abstract: </b>We investigate the limits of one of the fundamental ideas in data structures:
fractional cascading. This is an important data structure technique to speed up
repeated searches for the same key in multiple lists and it has numerous
applications. Specifically, the input is a "catalog" graph, $G$, of constant
degree together with a list of values assigned to every vertex of $G$. The goal
is to preprocess the input such that given a connected subgraph $H$ of $G$ and
a single query value $q$, one can find the predecessor of $q$ in every list
that belongs to $\scat$. The classical result by Chazelle and Guibas shows that
in a pointer machine, this can be done in the optimal time of $\O(\log n +
|\scat|)$ where $n$ is the total number of values. However, if insertion and
deletion of values are allowed, then the query time slows down to $\O(\log n +
|\scat| \log\log n)$. If only insertions (or deletions) are allowed, then once
again, an optimal query time can be obtained but by using amortization at
update time.
</p>
<p>We prove a lower bound of $\Omega( \log n \sqrt{\log\log n})$ on the
worst-case query time of dynamic fractional cascading, when queries are paths
of length $O(\log n)$. The lower bound applies both to fully dynamic data
structures with amortized polylogarithmic update time and incremental data
structures with polylogarithmic worst-case update time. As a side, this also
roves that amortization is crucial for obtaining an optimal incremental data
structure.
</p>
<p>This is the first non-trivial pointer machine lower bound for a dynamic data
structure that breaks the $\Omega(\log n)$ barrier. In order to obtain this
result, we develop a number of new ideas and techniques that hopefully can be
useful to obtain additional dynamic lower bounds in the pointer machine model.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00503"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00319">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00319">A Secure Two-Party Computation Protocol for Intersection Detection between Two Convex Hulls</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Amirahmad Chapnevis, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadeghiyan:Babak.html">Babak Sadeghiyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00319">PDF</a><br /><b>Abstract: </b>Intersection detection between three-dimensional bodies has various
applications in computer graphics, video game development, robotics as well as
military industries. In some respects, entities do not want to disclose
sensitive information about themselves, including their location. In this
paper, we present a secure two-party protocol to determine the existence of an
intersection between entities. The protocol presented in this paper allows for
intersection detection in three-dimensional spaces in geometry. Our approach is
to use an intersecting plane between two spaces to determine their separation
or intersection. For this purpose, we introduce a computational geometry
protocol to determine the existence of an intersecting plane. In this paper, we
first use the Minkowski difference to reduce the two-space problem into
one-space. Then, the separating set is obtained and the separation of two
shapes is determined based on the inclusion of the center point. We then secure
the protocol by modifying the separating set computation method as a
privacy-preserver and changing the Minkowski difference method to achieve this
goal. The proposed protocol applies to any form of convex three-dimensional
shape. The experiments successfully found a secure protocol for intersection
detection between two convex hulls in geometrical shapes such as the pyramid
and cuboid.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00319"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7875">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/">Yet another backpropagation tutorial</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am teaching deep learing this week in Harvard’s CS 182 (Artificial Intelligence) course. As I’m preparing the back-propagation lecture, Preetum Nakkiran told me about <a href="https://github.com/karpathy/micrograd">Andrej Karpathy’s awesome micrograd package</a> which implements automatic differentiation for scalar variables in very few lines of code.</p>



<p>I couldn’t resist using this to show how simple back-propagation and stochastic gradient descents are. To make sure we leave nothing “under the hood” we will not import anything from the package but rather only copy paste the few things we need. I hope that the text below is generally accessible to anyone familiar with partial derivatives. See this <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing">colab notebook</a> for all the code in this tutorial. In particular, aside from libraries for plotting and copy pasting a few dozen lines from Karpathy this code uses absolutely no libraries (no numpy, no pytorch, etc..) and can train (slowly..) neural networks using stochastic gradient descent. </p>



<p><strong>Automatic differentiation</strong> is a mechanism that allows you to write a Python functions such as</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def f(x,y): return (x+y)+x**3
</pre>


<p>and enables one to automatically obtain the partial derivatives <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7Bx%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial \text{x}}" class="latex" title="\tfrac{\partial f}{\partial \text{x}}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7By%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial \text{y}}" class="latex" title="\tfrac{\partial f}{\partial \text{y}}" />. Numerically we could do this by choosing some small value <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> and computing both <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2B%5Cdelta%2Cy%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}" class="latex" title="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2Cy%2B%5Cdelta%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}" class="latex" title="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}" />.<br />However, if we generalize this approach to <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> variables, we get an algorithm that requires roughly <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> evaluations of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />. <em>Back-propagation</em> enables computing <em>all</em> of the partial derivatives at only constant overhead over the cost of a single evaluation of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</p>



<h2>Back propagation and the chain rule</h2>



<p>Back-propagation is a direct implication of the <strong>multi-variate chain rule</strong>. Let’s illustrate this for the case of two variables. Suppose that <img src="https://s0.wp.com/latex.php?latex=v%2Cw%3A+%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v,w: \mathbb{R} \rightarrow \mathbb{R}" class="latex" title="v,w: \mathbb{R} \rightarrow \mathbb{R}" /> and <img src="https://s0.wp.com/latex.php?latex=z%3A%5Cmathbb%7BR%7D%5E2+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z:\mathbb{R}^2 \rightarrow \mathbb{R}" class="latex" title="z:\mathbb{R}^2 \rightarrow \mathbb{R}" /> are functions, and define</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28u%29+%3D+z%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u) = z(v(u),w(u))" class="latex" title="f(u) = z(v(u),w(u))" />.</p>



<p>That is, we have the following situation:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/KGVphzL.png" alt="" /></figure>



<p>where <img src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u)" class="latex" title="f(u)" /> is the value <img src="https://s0.wp.com/latex.php?latex=z%3Dz%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z=z(v(u),w(u))" class="latex" title="z=z(v(u),w(u))" /></p>



<p>Then the <strong>chain rule</strong> states that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+u%7D+%3D+%28+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )" class="latex" title="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )" /></p>



<p>You can take this on faith, but it also has a simple proof. To see the intuition, note that for small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" />, <img src="https://s0.wp.com/latex.php?latex=v%28u%2B%5Cdelta%29+%5Capprox+v%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)" class="latex" title="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)" /> and <img src="https://s0.wp.com/latex.php?latex=w%28u%2B%5Cdelta%29+%5Capprox+w%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)" class="latex" title="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)" />. For small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_1%2C%5Cdelta_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta_1,\delta_2" class="latex" title="\delta_1,\delta_2" />, <img src="https://s0.wp.com/latex.php?latex=z%28v%2B%5Cdelta_1%2Cw%2B%5Cdelta_2%29+%5Capprox+z%28v%2Cw%29+%2B+%5Cdelta_1+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D%28z%2Cw%29+%2B+%5Cdelta_2+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}" class="latex" title="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}" />. Hence, if we ignore terms with powers of delta two or higher,</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28u+%2B%5Cdelta%29%3D+z%28w%28u%2B%5Cdelta%29%2Cv%28u%2B%5Cdelta%29%29+%5Capprox+f%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}" class="latex" title="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}" /></p>



<p>The chain rule generalizes naturally to the case that <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> is a function of more variables than <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. Generally, if the value <img src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u)" class="latex" title="f(u)" /> is obtained by first computing some intermediate values <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" /> from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and then computing <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in some arbitrary way from <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" />, then <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Csum_%7Bi%3D1%7D%5Ek+%5Ctfrac%7B%5Cpartial+v_i%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}" class="latex" title="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}" />.</p>



<p>As a corollary, if you already managed to compute the values <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_1%7D%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}" class="latex" title="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}" />, and you kept track of the way that <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" /> were obtained from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />, then you can compute <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u}" class="latex" title="\tfrac{\partial z}{\partial u}" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/35jj2wz.png" alt="" /></figure>



<p>This suggests a simple recursive algorithm by which you compute the derivative of the final value <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> with respect to an intermediate value <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> in the computation using recursive calls to compute the values <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial w'}" class="latex" title="\tfrac{\partial z}{\partial w'}" /> for all the values <img src="https://s0.wp.com/latex.php?latex=w%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'" class="latex" title="w'" /> that were directly computed from <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />. Back propagation is this algorithm.</p>



<h2>Implementing automatic differentiation using back propagation in Python</h2>



<p>We now describe how to do this in Python, following Karpathy’s code. The basic class we use is <code>Value</code>. Every member <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> of <code>Value</code> is a container that holds:</p>



<p>Each such container <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> will contain the following attributes:</p>



<ol><li>The actual scalar (i.e., floating point) value that <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> holds. We call this <code>data</code>.</li><li>The gradient of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> with respect to some future unknown value <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that will use it. We call this <code>grad</code> and it is initialized to zero.</li><li>Pointers to all the values that were used in the computation of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. We call this <code>_prev</code></li><li>The method that adds (using the current value of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and other values) the contribution of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> to the gradient of all its previous values <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> to their gradients. We call this function <code>_backward</code>. Specifically, at the time we call <code>_backward</code> we assume that <code>u.grad</code> already contains <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u}" class="latex" title="\tfrac{\partial z}{\partial u}" /> where <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> is the final value we are interested in. For every value <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> that was used to compute <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />, we add to <code>v.grad</code> the quantity <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+u%7D%7B%5Cpartial+v%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}" class="latex" title="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}" />. For the latter quantity we need to keep track of <em>how</em> <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> was computed from <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b" class="latex" title="b" />.</li><li>If we call the method <code>backwards</code> (without an underscore) on a variable <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> then this will compute the derivative of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> with respect to <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> for all values <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> that were used in the computation of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. We do this by first recursively building a list <img src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,v_2,\ldots,v_k" class="latex" title="v_1,v_2,\ldots,v_k" /> of these values in reverse topoligical order (i.e., if <img src="https://s0.wp.com/latex.php?latex=v_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_i" class="latex" title="v_i" /> used <img src="https://s0.wp.com/latex.php?latex=v_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_j" class="latex" title="v_j" /> in the computation then <img src="https://s0.wp.com/latex.php?latex=i%3Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i&lt;j" class="latex" title="i&lt;j" />) and then call <code>_backwards</code> on each one of them.</li></ol>



<p>Let’s now describe this in code. We start off with a simple version that only supports addition and multiplication. The constructor for the class is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Value:
    """ stores a single scalar value and its gradient """

    def __init__(self, data, _children=()):
        self.data = data
        self.grad = 0
        self._backward = lambda: None
        self._prev = set(_children)
</pre>


<p>which fairly directly matches the description above. This constructor creates a value not using prior ones, which is why the <code>_backward</code> function is empty.<br />However, we can also create values by adding or multiplying prior ones, by adding the following methods:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">  def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other))

        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward

        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other))

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward

        return out
</pre>


<p>That is, if we create <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> by adding the values <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />, then the <code>_backward</code> function of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> works by adding <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w}" class="latex" title="= \tfrac{\partial z}{\partial w}" /> to both <code>u.grad</code> and <code>v.grad</code>.<br />If we <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> is obtain by multiplying <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> then we add <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cdot" class="latex" title="\cdot" /> <code>v.data</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w} v" class="latex" title="= \tfrac{\partial z}{\partial w} v" /> to <code>u.grad</code> and similarly add <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cdot" class="latex" title="\cdot" /> <code>u.data</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w} u" class="latex" title="= \tfrac{\partial z}{\partial w} u" /> to <code>v.grad</code>.</p>



<p>The <code>backward</code> function is obtained by setting the gradient of the current value to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> and then running <code>_backwards</code> on all other values in reverse topological order:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">    def backward(self):
        visited= set()
        def topo(v):# return children in topological order
            visited.add(v)
            return [x for L in [topo(child) for child in v._prev if not child in visited] for x in L]
        self.grad = 1
        for v in reversed(topo(self)):
          v.__backward()
</pre>


<p>For example, if we run the following code</p>


<pre class="brush: python; gutter: false; title: ; notranslate">a = Value(5)
print(a.grad)
def f(x): return (x+2)**2 + x**3
f(a).backward()
print(a.grad)
</pre>


<p>then the values printed will be <code>0</code> and <code>89</code> since the derivative of <img src="https://s0.wp.com/latex.php?latex=%28x%2B2%29%5E2+%2B+x%5E3+%3D+x%5E3+%2B+x%5E2+%2B+4x+%2B+4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4" class="latex" title="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4" /> equals <img src="https://s0.wp.com/latex.php?latex=3x%5E2+%2B+4x+%2B+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3x^2 + 4x + 2" class="latex" title="3x^2 + 4x + 2" />.</p>



<p>In the <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing#scrollTo=0_nveKyfxXQK">notebook</a> you can see that we implement also the power function, and have some “convenience methods” (division etc..).</p>



<h3>Linear regression using back propagation and stochastic gradient descent</h3>



<p>In <em>stochastic gradient descent</em> we are given some data <img src="https://s0.wp.com/latex.php?latex=%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_1,y_1),\ldots,(x_n,y_n)" class="latex" title="(x_1,y_1),\ldots,(x_n,y_n)" /> and want to find an hypothesis <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> that minimizes the empirical loss <img src="https://s0.wp.com/latex.php?latex=L%28h%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+L%28h%28x_i%29%2Cy_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)" class="latex" title="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)" /> where <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> is a <em>loss function</em> mapping two labels <img src="https://s0.wp.com/latex.php?latex=y%2C+y%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y, y'" class="latex" title="y, y'" /> to a real number. If we let <img src="https://s0.wp.com/latex.php?latex=L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_i(h)" class="latex" title="L_i(h)" /> be the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />-th term of this sum, then, identifying <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> with the parameters (i.e., real numbers) that specify it, stochastic gradient descent is the following algorithm:</p>



<ol><li>Set <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> to be a random vector. Set <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> to be some small number (e.g., <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3D+0.1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta = 0.1" class="latex" title="\eta = 0.1" />)</li><li>For <img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%7B1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \in {1,\ldots, T}" class="latex" title="t \in {1,\ldots, T}" /> (where <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> is the number of <em>epochs</em>):</li></ol>



<ul><li>For <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%7B1%2C%5Cldots%2C+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i \in {1,\ldots, n}" class="latex" title="i \in {1,\ldots, n}" />: (in random order)<ul><li>Let <img src="https://s0.wp.com/latex.php?latex=h+%5Cleftarrow+h+-+%5Ceta+%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \leftarrow h - \eta \nabla_h L_i(h)" class="latex" title="h \leftarrow h - \eta \nabla_h L_i(h)" /></li></ul></li></ul>



<p>If <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> is specified by the parameters <img src="https://s0.wp.com/latex.php?latex=h_1%2C%5Cldots%2Ch_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h_1,\ldots,h_k" class="latex" title="h_1,\ldots,h_k" /> <img src="https://s0.wp.com/latex.php?latex=%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla_h L_i(h)" class="latex" title="\nabla_h L_i(h)" /> is the vector <img src="https://s0.wp.com/latex.php?latex=%28+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_1%7D%28h%29%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_2%7D%28h%29%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_k%7D%28h%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))" class="latex" title="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))" />. This is exactly the vector we can obtain using back propagation.</p>



<p>For example, if we want a linear model, we can use <img src="https://s0.wp.com/latex.php?latex=%28a%2Cb%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a,b)" class="latex" title="(a,b)" /> as our parameters and the function will be <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+a%5Ccdot+%2B+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \mapsto a\cdot + b" class="latex" title="x \mapsto a\cdot + b" />. We can generate random points <code>X</code>,<code>Y</code> as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/4Do0KI4.png" alt="" /></figure>



<p>Now we can define a linear model as follows:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Linear:
  def __init__(self):
    self.a,self.b = Value(random.random()),Value(random.random())
  def __call__(self,x): return self.a*x+self.b

  def zero_grad(self):
    self.a.grad, self.b.grad = 0,0
</pre>


<p>And train it directly by using SGD:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">η = 0.03, epochs = 20
for t in range(epochs):
  for x,y in zip(X,Y):
    model.zero_grad()
    loss = (model(x)-y)**2
    loss.backward()
    model.a , model.b = (model.a - η*model.a.grad  , model.b - η*model.b.grad)
</pre>


<p>Which as you can see works very well:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/19kDCPM.gif" alt="" /></figure>



<h2>From linear classifiers to Neural Networks.</h2>



<p>The above was somewhat of an “overkill” for linear models, but the beautify of automatic differentiation is that we can easily use more complex computation.</p>



<p>We can follow <a href="https://github.com/karpathy/micrograd/blob/master/demo.ipynb">Karpathy’s demo</a> and us the same approach to train a neural network.</p>



<p>We will use a neural network that takes two inputs and has two hidden layers of width 16. A neuron that takes input <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_k" class="latex" title="x_1,\ldots,x_k" /> will apply the ReLU function (<img src="https://s0.wp.com/latex.php?latex=max%7B0%2Cx%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="max{0,x}" class="latex" title="max{0,x}" />) to <img src="https://s0.wp.com/latex.php?latex=%5Csum+w_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum w_i x_i" class="latex" title="\sum w_i x_i" /> where <img src="https://s0.wp.com/latex.php?latex=w_1%2C%5Cldots%2Cw_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_1,\ldots,w_k" class="latex" title="w_1,\ldots,w_k" /> are its <em>weight</em> parameters. (It’s easy to add support for relu for our <code>Value</code> class. Also we won’t have a bias term in this example.)</p>



<p>The code for this Neural Network is as follows: (when <code>Value() </code>is called without a parameter the value is random number in <img src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=eeeeee&amp;fg=ffffff&amp;s=0&amp;c=20201002" alt="[-1,1]" class="latex" title="[-1,1]" />)</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def Neuron(weights,inputs, relu =True):
  # Evaluate neuron with given weights on given inputs
  v =  sum(weights[i]*x for i,x in enumerate(inputs))
  return v.relu() if relu else v


class Net:
  # Depth 3 fully connected neural net with one two inputs and output
  def __init__(self,  N=16):
    self.layer_1 = [[Value(),Value()] for i in range(N)]
    self.layer_2 = [ [Value() for j in range(N)] for i in range(N)]
    self.output =  [ Value() for i in range(N)]
    self.parameters = [v for L in [self.layer_1,self.layer_2,[self.output]] for w in L for v in w]


  def __call__(self,x):
    layer_1_vals = [Neuron(w,x) for w in self.layer_1]
    layer_2_vals = [Neuron(w,layer_1_vals) for w in self.layer_2]
    return Neuron(self.output,layer_2_vals,relu=False) 
    # the last output does not have the ReLU on top

  def zero_grad(self):
    for p in self.parameters:
      p.grad=0
</pre>


<p>We can train it in the same way as above.<br />We will follow Karpathy and train it to classify the following points:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/cXv93KU.png" alt="" /></figure>



<p>The training code is very similar, with the following differences:</p>



<ul><li>Instead of the square loss, we use the function <img src="https://s0.wp.com/latex.php?latex=L%28y%2Cy%27%29%3D+%5Cmax%7B+1-+y%5Ccdot+y%27%2C+0+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(y,y')= \max{ 1- y\cdot y', 0 }" class="latex" title="L(y,y')= \max{ 1- y\cdot y', 0 }" /> which is <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> if <img src="https://s0.wp.com/latex.php?latex=y+%5Ccdot+y%27+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y \cdot y' \geq 1" class="latex" title="y \cdot y' \geq 1" />. This makes sense since our data labels will be <img src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pm 1" class="latex" title="\pm 1" /> and we say we classify correctly if we get the same sign. We get zero loss if we classify correctly all samples with a margin of at least <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" />.</li><li>Instead of stochastic gradient descent we will do standard gradient descent, using all the datapoints before taking a gradient step. The optimal for neural networks is actually often something in the middle – <em>batch gradient descent</em> where we take a batch of samples and perform the gradient over them.</li></ul>



<p>The resulting code is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">for t in range(epochs):
  loss = sum([(1+ -y*model(x)).relu() for (x,y) in zip(X,Y)])/len(X)
  model.zero_grad()
  loss.backward()
  for p in model.parameters:
    p.data -= η*p.grad
</pre>


<p>If we use this, we get a decent approximation for this training set (see image below). As Karpathy shows, by adjusting the learning rate and using regularization, one can in fact get 100% accuracy.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/nbpNZyu.png" alt="" /></figure></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/"><span class="datestr">at November 03, 2020 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/">Postdoc at University of Augsburg (apply by November 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The group of Tobias Mömke at University of Augsburg is inviting applications for a three-year Postdoc position, starting at the earliest possible date. The topic of the position is research on Approximation Algorithm.</p>
<p>Website: <a href="https://www.uni-augsburg.de/en/fakultaet/fai/informatik/prof/raa/hiring/#postdoc">https://www.uni-augsburg.de/en/fakultaet/fai/informatik/prof/raa/hiring/#postdoc</a><br />
Email: moemke@informatik.uni-augsburg.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/"><span class="datestr">at November 03, 2020 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17775">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/">The Election Night Time Warp</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Has Election Night—not just the election—been modeled adequately?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/godelinthought/" rel="attachment wp-att-17777"><img src="https://rjlipton.files.wordpress.com/2020/11/godelinthought.jpg?w=600" alt="" class="alignright size-full wp-image-17777" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">The Conversation <a href="https://theconversation.com/kurt-godel-from-loopholes-and-dictators-to-the-incompleteness-theorems-72376">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Kurt Gödel famously found solutions to Albert Einstein’s equations of general relativity that allow trajectories to loop through time. </p>
<p>
Today, early on Election Day and a usual time for our posts on Gödel, I talk about the trajectory of counting votes after tomorrow’s polls close and time-warp effects that everyone watching the returns will see.</p>
<p>
I am picking up the vein of ethical algorithms in yesterday’s <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">post</a>, but with a different take about ethical modeling in novel situations where reliable training data is unavailable. I believe there is a responsibility for running simulations not just of tomorrow’s <em>election</em>, such as FiveThirtyEight <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">conducts</a>, but also of tomorrow’s <em>count</em> as it may unfold hour by hour and stretch over many following days. </p>
<p>
Gödel also famously believed he had found a logical flaw in the U.S. Constitution that allowed a mechanism for legally instituting a dictatorship, but his argument was never reported. We <a href="https://rjlipton.wordpress.com/2013/12/06/judging-a-book-by-its-coverage/">discussed</a> this at length in 2013, including noting a 2012 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2010183">paper</a> by Enrique Guerra-Pujol of the University of Central Florida College of Business, who is a frequent reader of this blog. Guerra-Pujol offers a detailed construction of a mechanism based on self-reference applied to the short <a href="https://www.archives.gov/federal-register/constitution/article-v.html">Article V</a> on amending the Constitution. This topic has been <a href="https://slate.com/technology/2018/08/is-there-a-logical-inconsistency-in-the-constitution.html">addressed</a> <a href="https://harvardlawreview.org/2020/01/pack-the-union-a-proposal-to-admit-new-states-for-the-purpose-of-amending-the-constitution-to-ensure-equal-representation/">several</a> <a href="https://theconversation.com/kurt-godel-from-loopholes-and-dictators-to-the-incompleteness-theorems-72376">times</a> <a href="https://thehill.com/opinion/civil-rights/473613-a-contradiction-at-the-heart-of-the-american-system">since</a>, but what stands out to us is a 2019 <a href="http://ceur-ws.org/Vol-2632/MIREL-19_paper_1.pdf">paper</a> by Valeria Zahoransky and Christoph Benzmüller. This paper attempts to find a proof of Guerra-Pujol’s mechanism by automated logical inference using the <a href="https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)">Isabelle</a>/<a href="https://en.wikipedia.org/wiki/HOL_(proof_assistant)">HOL</a> proof assistant. They found a model that satisfies the argument. </p>
<p></p><h2> Election Evolution Over Time </h2><p></p>
<p></p><p>
My <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">post</a> on Election Day 2016 contrasted the relatively stable time evolution of polls in 2012 with the gyrations of 2016. That post began by defending Nate Silver of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> and his 30% likelihood for Donald Trump against those who had Hillary Clinton over 90%. At the time of our 2012 <a href="https://rjlipton.wordpress.com/2012/11/05/the-election-outcome/">post</a> on Barack Obama versus Mitt Romney, I had thought Silver’s error bars were too wide, but in 2016 they looked right on the basis of last-minute decision making by impulse that my chess model also registers.</p>
<p>
This year, the polls have been even steadier than in 2012, and undecided voters are found to be scarce. Of course, the pandemic has been the greatest factor in the poll numbers, but my first point is that election forecasting uses only the numbers as primary. The algorithms do not have an input that could take values Covid 19, Covid 23, Covid 17… In a normal election, the polling numbers would seem to point to an easier call than in 2012. Silver has, however, expressed <a href="https://fivethirtyeight.com/features/im-here-to-remind-you-that-trump-can-still-win/">caution</a> in explaining why his odds stay just short of 90% for Joe Biden as I write at midnight. My first point of further disquiet begins with a simple fact:</p>
<blockquote><p><b> </b> <em> The models have been trained on data from past elections. </em>
</p></blockquote>
<p></p><p>
A major difference on our flight path to Election Day is the upsurge in early voting and turnout overall. Texas reports that it has already registered more early votes than total votes cast in 2016. We are not saying this difference has been overlooked—of course, models are being adjusted for it every hour. What we are doubting is the existence of a basis for making those adjustments with high confidence. Before we discuss the major issue of the flight path <em>after</em> the polls close, let me insert an analogy from my current chess work.</p>
<p>
</p><p></p><h2> My Own Ethical Interpolation </h2><p></p>
<p></p><p>
My statistical chess model is trained on millions of moves from games at the hours-long pace of standard in-person chess. The pandemic has led to chess moving online where fast time controls are the norm. For instance, the hallowed US Championships were just held in a format of three games per day at a pace of 25 minutes for the whole game plus 5 seconds for each move played, which equates to G/30 (game in 30 minutes) under a simplification used also <a href="http://universalrating.com/about-us.php">here</a>. The Online Olympiad held in August by the International Chess Federation (FIDE) gave only 15 minutes plus 5 seconds per move, equating to G/20. I have been asked for input on games at paces all the way down to 1-minute “Bullet” chess. </p>
<p>
My solid estimates of how less time affects skill come from the annual FIDE World Rapid and Blitz Championships, which are contested at equivalents of G/25 and G/5, respectively, by hundreds of elite male and female players. For time controls in-between I face twin problems of scant data from in-person chess—basically none for player below master level—and online chess having evident contamination from cheating, as I discussed in a <a href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/">post</a> last June. Hence I <em>interpolate</em> to determine model settings for other time controls. </p>
<p>
This <a href="https://cse.buffalo.edu/~regan/chess/RatingTimeCurves.jpg">diagram</a> shows internal evidence supporting the orange curve obtained via <a href="https://en.wikipedia.org/wiki/Aitken's_delta-squared_process">Aitken extrapolation</a>, in that inverse polynomial curves based on other reasoning converge to it. The curve has been supported by recent field tests, including a restricted invitational tournament recently run by Chess.com at G/3 and a large junior league in Britain at G/15 equivalent. Still, the <em>ethical</em> status is:</p>
<ol>
<li>
The pandemic has injected me into online fast chess ahead of the year-long timeframe that would be needed to clean the available large data and rebuild my model directly for all the gamut of different time controls. <p></p>
</li><li>
So it is ethical for me to give my best estimates, as supported by field tests and <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">cross-checks</a> in my model, but with caveat of their being “an extrapolation of an interpolation.” <p></p>
</li><li>
But in principle there are more-reliable ways to do the modeling.
</li></ol>
<p>
My second point is that this year’s election models are in similar boats: The pandemic forces their cantilevered use in new situations. As with chess the data needed for direct training may not be available. As we’ve said in the <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">post</a>, this year’s election <em>task</em> may be “Ethics-Hard.” Now we come to my third and main point about responsibility.</p>
<p>
</p><p></p><h2> A Night of Waves and Blue/Red Shifts </h2><p></p>
<p></p><p>
The new dimension of time is the order in which all the following categories of votes will be counted, in the states that variously allow them:</p>
<ol>
<li>
Early votes cast in-person, such as my wife and I did on the first possible <a href="https://buffalonews.com/news/local/government-and-politics/nearly-15-000-turn-out-on-first-day-of-early-voting-in-erie-county/article_f35cc7f0-153b-11eb-bbd6-43fce059203c.html">day</a> in New York. <p></p>
</li><li>
Early votes sent by mail. <p></p>
</li><li>
In-person votes on Election Day. <p></p>
</li><li>
Regular votes by mail in states that vote that way. <p></p>
</li><li>
Absentee ballots, which are the only non-Tuesday option in some states.
</li></ol>
<p>
This is approximately the order in which votes will be counted, again with state-by-state differences, which especially may lead to votes in category 2 being counted later. The issue is that the Democratic and Republican shares are expected to vary greatly across the categories, enough to cause large shifts in the perceived leader over real time. </p>
<p>
Geoffrey Skelley of FiveThirtyEight has an <a href="https://fivethirtyeight.com/features/why-pennsylvanias-vote-count-could-change-after-election-night/">article</a> showing how a 5-point win for Biden in Pennsylvania might still present as a <b>16-point</b> lead for President Trump on Election Night, when all in-person votes are counted but only some of the votes by mail. Here are the article’s key graphics, the left one showing actual proportions from Pennsylvania’s June primary.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/skelleyblueshiftcombo/" rel="attachment wp-att-17778"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2020/11/skelleyblueshiftcombo.png?w=550&amp;h=243" class="alignright wp-image-17778" height="243" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of two figures from FiveThirtyEight Skelley <a href="https://fivethirtyeight.com/features/why-pennsylvanias-vote-count-could-change-after-election-night/">article</a></font>
</td>
</tr>
</tbody></table>
<p>
Other states that allow early tallying of early votes may show an initial Biden lead before a red-shift toward Trump on Election Night, with more of Biden’s vote share remaining to be counted. Still others are less clear. FiveThirtyEight on Saturday posted a useful state-by-state <a href="https://projects.fivethirtyeight.com/election-results-timing/">guide</a>.</p>
<p>
</p><p></p><h2> Why Important to Model the Count? </h2><p></p>
<p></p><p>
Awareness of the time-shifts is important not only for perception but also for the timing of legal challenges that are expected to arise. For instance, there is a continued specter of <a href="https://www.texastribune.org/2020/11/01/texas-drive-thru-votes-harris-county/">invalidating</a> 127,000 early votes already cast by a novel drive-up system in Harris County Texas; despite its <a href="https://www.nytimes.com/live/2020/11/02/us/trump-vs-biden/a-federal-judge-denies-a-bid-to-throw-out-more-than-127000-votes-in-texas-the-republicans-who-sued-have-already-appealed">rejection</a> by a district judge today, it has been appealed higher. There is expectation of legal <a href="https://www.nytimes.com/2020/11/02/us/politics/election-day-ballot-counting.html">battles</a> nationwide over procedures that have been altered by measures to cope with the pandemic.</p>
<p>
Public perception, however, is the most immediate concern. President Trump <a href="https://rjlipton.wordpress.com/feed/is totally inappropriate, and I don't believe that's by our laws">stated</a> in a flagged Tweet that we “must have final total on November 3rd” and followed up by saying, “…instead of counting ballots for two weeks, which is totally inappropriate, and I don’t believe that’s by our laws.” Justice Brett Kavanaugh wrote in a formal <a href="https://www.supremecourt.gov/opinions/20pdf/20a66_new_m6io.pdf#page=6">opinion</a> that states “want to avoid the chaos and suspicions of impropriety that can ensue if thousands of absentee ballots flow in after Election Day and potentially flip the results of an election.” Note his phrase “flip the results”—a sure sign that perception is reality.</p>
<p>
Of course many outlets besides FiveThirtyEight are aware of the new election-reporting physics and <a href="https://www.nytimes.com/interactive/2020/10/27/upshot/election-results-timing.html">have</a> <a href="https://www.brookings.edu/blog/fixgov/2020/10/30/what-to-watch-for-on-election-night-2020/">published</a> <a href="https://www.washingtonpost.com/politics/2020/10/26/timing-election-results/">their</a> <a href="https://www.wsj.com/articles/will-we-know-who-is-elected-president-on-election-night-a-guide-to-possible-delays-11596629410">own</a> <a href="https://www.cnn.com/2020/10/30/politics/red-blue-mirage-election-results/index.html">guides</a>. They are adjusting their election-night projection models accordingly. Our main question goes further in terms of responsibility:</p>
<blockquote><p><b> </b> <em> Has anyone been running simulations of how Election Night vote-counting may unfold? </em>
</p></blockquote>
<p></p><p>
We believe such simulations are just as important as the ones they have run of a timeless election. Showing them is not only important to inform the many who will be watching, it would be a vaccine against pressure that exploits unexpected perception. </p>
<p>
Yet it must be acknowledged that there is neither hard data nor sure knowledge of county-level vote-tallying policies and schedules on which to train such simulations. Nor may there be as many cross-checks as my chess interpolation situation. </p>
<p>
This may be another “Ethics-Hard” problem. But it is one already involved in adjusting the projection models that definitely are being deployed. Aside from the many novel and vital modeling problems from the pandemic, it may be the most important one of our near future. And we are thinking of this less than 48 hours—now less than 24 hours—before the first polls close.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How do you think Election Night results will unfold, apart from your estimate of the time-independent “ground truth” of the electorate’s intentions?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/"><span class="datestr">at November 03, 2020 06:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5061">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5061">A Drawing for Singularity Eve</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<figure class="wp-block-image size-large"><a href="https://www.scottaaronson.com/hamburgur2.jpg"><img src="https://www.scottaaronson.com/hamburgur2-sm.jpg" alt="" /></a></figure>



<p>Lily, my 7-year-old, asked me to share the above on my blog.  She says it depicts the US Army luring Trump out of the White House with a hamburger, in order to lock the front door once he’s out—what she proposes should happen if Trump refuses to acknowledge a loss.</p>



<p>If you haven’t yet voted, especially if you live in a contested state, <strong>please do so tomorrow</strong>.  Best wishes to us all!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Nov. 3):</span></strong> Even if it comes 4-5 years late, <a href="https://samharris.org/podcasts/224-key-trumps-appeal/">this 8-minute podcast by Sam Harris</a> gives perhaps the sharpest solution ever articulated to the mystery of how tens of millions of Americans could enthusiastically support an obvious fraud, liar, incompetent, and threat to civilization.  Briefly, it’s not <em>despite</em> his immense failings but <em>because of</em> them—because by flaunting his failings he absolves his supporters for their own, even while the other side serves those same supporters relentless moral condemnation and scorn.  I <em>think</em> I had known this—I even said something similar as the tagline of this blog (“The Far Right is destroying the world, and the Far Left thinks it’s my fault!”).  But Sam Harris expresses it as only he can.  If this analysis is right—and I feel virtually certain it is—then it bodes well that Biden, unlike Hillary Clinton, isn’t seen as especially sanctimonious or judgmental.  Biden’s own gaffes and failings probably help him.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5061"><span class="datestr">at November 03, 2020 01:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00542">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00542">Robust Sequence Submodular Maximization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sallam:Gamal.html">Gamal Sallam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Zizhan.html">Zizhan Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Jie.html">Jie Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Ji:Bo.html">Bo Ji</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00542">PDF</a><br /><b>Abstract: </b>Submodularity is an important property of set functions and has been
extensively studied in the literature. It models set functions that exhibit a
diminishing returns property, where the marginal value of adding an element to
a set decreases as the set expands. This notion has been generalized to
considering sequence functions, where the order of adding elements plays a
crucial role and determines the function value; the generalized notion is
called sequence (or string) submodularity. In this paper, we study a new
problem of robust sequence submodular maximization with cardinality
constraints. The robustness is against the removal of a subset of elements in
the selected sequence (e.g., due to malfunctions or adversarial attacks).
Compared to robust submodular maximization for set function, new challenges
arise when sequence functions are concerned. Specifically, there are multiple
definitions of submodularity for sequence functions, which exhibit subtle yet
critical differences. Another challenge comes from two directions of
monotonicity: forward monotonicity and backward monotonicity, both of which are
important to proving performance guarantees. To address these unique
challenges, we design two robust greedy algorithms: while one algorithm
achieves a constant approximation ratio but is robust only against the removal
of a subset of contiguous elements, the other is robust against the removal of
an arbitrary subset of the selected elements but requires a stronger assumption
and achieves an approximation ratio that depends on the number of the removed
elements. Finally, we generalize the analyses to considering sequence functions
under weaker assumptions based on approximate versions of sequence
submodularity and backward monotonicity
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00542"><span class="datestr">at November 03, 2020 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00511">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00511">Best Match Graphs with Binary Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>David Schaller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gei=szlig=:Manuela.html">Manuela Geiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00511">PDF</a><br /><b>Abstract: </b>Best match graphs (BMG) are a key intermediate in graph-based orthology
detection and contain a large amount of information on the gene tree. We
provide a near-cubic algorithm to determine wether a BMG can be explained by a
fully resolved gene tree and, if so, to construct such a tree. Moreover, we
show that all such binary trees are refinements of the unique binary-resolvable
tree (BRT), which in general is a substantial refinement of the also unique
least resolved tree of a BMG.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00511"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00364">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00364">Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Jelena.html">Jelena Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daskalakis:Constantinos.html">Constantinos Daskalakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00364">PDF</a><br /><b>Abstract: </b>The use of min-max optimization in adversarial training of deep neural
network classifiers and training of generative adversarial networks has
motivated the study of nonconvex-nonconcave optimization objectives, which
frequently arise in these applications. Unfortunately, recent results have
established that even approximate first-order stationary points of such
objectives are intractable, even under smoothness conditions, motivating the
study of min-max objectives with additional structure. We introduce a new class
of structured nonconvex-nonconcave min-max optimization problems, proposing a
generalization of the extragradient algorithm which provably converges to a
stationary point. The algorithm applies not only to Euclidean spaces, but also
to general $\ell_p$-normed finite-dimensional real vector spaces. We also
discuss its stability under stochastic oracles and provide bounds on its sample
complexity. Our iteration complexity and sample complexity bounds either match
or improve the best known bounds for the same or less general
nonconvex-nonconcave settings, such as those that satisfy variational coherence
or in which a weak solution to the associated variational inequality problem is
assumed to exist.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00364"><span class="datestr">at November 03, 2020 11:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00172">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00172">Generalized Sorting with Predictions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Pinyan.html">Pinyan Lu</a>, Xuandi Ren, Enze Sun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yubo.html">Yubo Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00172">PDF</a><br /><b>Abstract: </b>Generalized sorting problem, also known as sorting with forbidden
comparisons, was first introduced by Huang et al. together with a randomized
algorithm which requires $\tilde O(n^{3/2})$ probes. We study this problem with
additional predictions for all pairs of allowed comparisons as input. We
propose a randomized algorithm which uses $O(n \log n+w)$ probes with high
probability and a deterministic algorithm which uses $O(nw)$ probes, where $w$
is the number of mistakes made by prediction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00172"><span class="datestr">at November 03, 2020 11:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00164">Differentially Private ADMM Algorithms for Machine Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Tao.html">Tao Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shang:Fanhua.html">Fanhua Shang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yuanyuan.html">Yuanyuan Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Hongying.html">Hongying Liu</a>, Longjie Shen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gong:Maoguo.html">Maoguo Gong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00164">PDF</a><br /><b>Abstract: </b>In this paper, we study efficient differentially private alternating
direction methods of multipliers (ADMM) via gradient perturbation for many
machine learning problems. For smooth convex loss functions with (non)-smooth
regularization, we propose the first differentially private ADMM (DP-ADMM)
algorithm with performance guarantee of $(\epsilon,\delta)$-differential
privacy ($(\epsilon,\delta)$-DP). From the viewpoint of theoretical analysis,
we use the Gaussian mechanism and the conversion relationship between R\'enyi
Differential Privacy (RDP) and DP to perform a comprehensive privacy analysis
for our algorithm. Then we establish a new criterion to prove the convergence
of the proposed algorithms including DP-ADMM. We also give the utility analysis
of our DP-ADMM. Moreover, we propose an accelerated DP-ADMM (DP-AccADMM) with
the Nesterov's acceleration technique. Finally, we conduct numerical
experiments on many real-world datasets to show the privacy-utility tradeoff of
the two proposed algorithms, and all the comparative analysis shows that
DP-AccADMM converges faster and has a better utility than DP-ADMM, when the
privacy budget $\epsilon$ is larger than a threshold.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00164"><span class="datestr">at November 03, 2020 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00131">Approximating the clustered selected-internal Steiner tree problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yen_Hung.html">Yen Hung Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00131">PDF</a><br /><b>Abstract: </b>Given a complete graph $G=(V,E)$, with nonnegative edge costs, two subsets $R
\subset V$ and $R^{\prime} \subset R$, a partition
$\mathcal{R}=\{R_1,R_2,\ldots,R_k\}$ of $R$, $R_i \cap R_j=\phi$, $i \neq j$
and $\mathcal{R}^{\prime}=\{R^{\prime}_1,R^{\prime}_2,\ldots,R^{\prime}_k\}$ of
$R^{\prime}$, $R^{\prime}_i \subset R_i$, a clustered Steiner tree is a tree
$T$ of $G$ that spans all vertices in $R$ such that $T$ can be cut into $k$
subtrees $T_i$ by removing $k-1$ edges and each subtree $T_i$ spanning all
vertices in $R_i$, $1 \leq i \leq k$. The cost of a clustered Steiner tree is
defined to be the sum of the costs of all its edges. A clustered
selected-internal Steiner tree of $G$ is a clustered Steiner tree for $R$ if
all vertices in $R^{\prime}_i$ are internal vertices of $T_i$, $1 \leq i \leq
k$. The clustered selected-internal Steiner tree problem is concerned with the
determination of a clustered selected-internal Steiner tree $T$ for $R$ and
$R^{\prime}$ in $G$ with minimum cost. It is not hard to see the clustered
selected-internal Steiner tree problem is NP-hard. Hence, in this paper, we
present the first known approximation algorithm with performance ratio $\rho+4$
for the clustered selected-internal Steiner tree problem if the cost function
is metric (i.e., the costs of edges satisfy the triangle inequality), where
$\rho$ is the best-known performance ratio for the Steiner tree problem whose
performance ratio is $\ln {4}+\epsilon\approx 1.39$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00131"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00130">Approximability results for the $p$-centdian and the converse centdian problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yen_Hung.html">Yen Hung Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00130">PDF</a><br /><b>Abstract: </b>Given an undirected graph $G=(V,E,l)$ with a nonnegative edge length function
$l$, and two integers $p$, $\lambda$, $0&lt;p&lt;|V|$, $0\le\lambda\le 1$, let
$V^{\prime}$ be a subset of $V$ with $|V^{\prime}|=p$. For each vertex $v \in
V$, we let $d(v,V^{\prime})$ denote the shortest distance from $v$ to
$V^{\prime}$. An {\it eccentricity} $\pounds_C(V^{\prime})$ of $V^{\prime}$
denotes the maximum distance of $d(v,V^{\prime})$ for all $v \in V$. A
{median-distance} $\pounds_M(V^{\prime})$ of $V^{\prime}$ denotes the total
distance of $d(v,V^{\prime})$ for all $v \in V$. The $p$-centdian problem is to
find a vertex set $V^{\prime}$ of $V$ with $|V^{\prime}|=p$, such that $\lambda
\pounds_C(V^{\prime})+(1-\lambda) \pounds_M(V^{\prime})$ is minimized. The
vertex set $V^{\prime}$ is called as the {\it centdian set} and $\lambda
\pounds_C(V^{\prime})+(1-\lambda) \pounds_M(V^{\prime})$ is called as the
{centdian-distance}. If we converse the two criteria, that is given the bound
$U$ of the {centdian-distance} and the objective function is to minimize the
cardinality of the {centdian set}, this problem is called as the converse
centdian problem. In this paper, we prove the $p$-centdian problem is
NP-Complete even when the {centdian-distance} is
$\pounds_C(V^{\prime})+\pounds_M(V^{\prime})$. Then we design the first
non-trivial brute force exact algorithms for the $p$-centdian problem and the
converse centdian problem, respectively. Finally, we design a
$(1+\epsilon)$-approximation (respectively,
$(1+1/\epsilon)(ln|V|+1)$-approximation) algorithm for the $p$-centdian problem
(respectively, converse centdian problem) satisfying the cardinality of
{centdian set} is less than or equal to $(1+1/\epsilon)(ln|V|+1)p$
(respectively, $(1+\epsilon)U$), in which $\epsilon&gt;0$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00130"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00083">Estimating Sparse Discrete Distributions Under Local Privacy and Communication Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yuhan.html">Yuhan Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00083">PDF</a><br /><b>Abstract: </b>We consider the task of estimating sparse discrete distributions under local
differential privacy and communication constraints. Under local privacy
constraints, we present a sample-optimal private-coin scheme that only sends a
one-bit message per user. For communication constraints, we present a
public-coin scheme based on random hashing functions, which we prove is optimal
up to logarithmic factors. Our results show that the sample complexity only
depends logarithmically on the ambient dimension, thus providing significant
improvement in sample complexity under sparsity assumptions. Our lower bounds
are based on a recently proposed chi-squared contraction method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00083"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00029">Monitoring the edges of a graph using distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foucaud:Florent.html">Florent Foucaud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kao:Shih=Shun.html">Shih-Shun Kao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klasing:Ralf.html">Ralf Klasing</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miller:Mirka.html">Mirka Miller</a>, Joe Ryan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00029">PDF</a><br /><b>Abstract: </b>We introduce a new graph-theoretic concept in the area of network monitoring.
A set $M$ of vertices of a graph $G$ is a \emph{distance-edge-monitoring set}
if for every edge $e$ of $G$, there is a vertex $x$ of $M$ and a vertex $y$ of
$G$ such that $e$ belongs to all shortest paths between $x$ and $y$. We denote
by $dem(G)$ the smallest size of such a set in $G$. The vertices of $M$
represent distance probes in a network modeled by $G$; when the edge $e$ fails,
the distance from $x$ to $y$ increases, and thus we are able to detect the
failure. It turns out that not only we can detect it, but we can even correctly
locate the failing edge.
</p>
<p>In this paper, we initiate the study of this new concept. We show that for a
nontrivial connected graph $G$ of order $n$, $1\leq dem(G)\leq n-1$ with
$dem(G)=1$ if and only if $G$ is a tree, and $dem(G)=n-1$ if and only if it is
a complete graph. We compute the exact value of $dem$ for grids, hypercubes,
and complete bipartite graphs.
</p>
<p>Then, we relate $dem$ to other standard graph parameters. We show that
$demG)$ is lower-bounded by the arboricity of the graph, and upper-bounded by
its vertex cover number. It is also upper-bounded by twice its feedback edge
set number. Moreover, we characterize connected graphs $G$ with $dem(G)=2$.
</p>
<p>Then, we show that determining $dem(G)$ for an input graph $G$ is an
NP-complete problem, even for apex graphs. There exists a polynomial-time
logarithmic-factor approximation algorithm, however it is NP-hard to compute an
asymptotically better approximation, even for bipartite graphs of small
diameter and for bipartite subcubic graphs. For such instances, the problem is
also unlikey to be fixed parameter tractable when parameterized by the solution
size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00029"><span class="datestr">at November 03, 2020 11:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.00001">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.00001">Distance problems within Helly graphs and $k$-Helly graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ducoffe:Guillaume.html">Guillaume Ducoffe</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.00001">PDF</a><br /><b>Abstract: </b>The ball hypergraph of a graph $G$ is the family of balls of all possible
centers and radii in $G$. It has Helly number at most $k$ if every subfamily of
$k$-wise intersecting balls has a nonempty common intersection. A graph is
$k$-Helly (or Helly, if $k=2$) if its ball hypergraph has Helly number at most
$k$. We prove that a central vertex and all the medians in an $n$-vertex
$m$-edge Helly graph can be computed w.h.p. in $\tilde{\cal O}(m\sqrt{n})$
time. Both results extend to a broader setting where we define a non-negative
cost function over the vertex-set. For any fixed $k$, we also present an
$\tilde{\cal O}(m\sqrt{kn})$-time randomized algorithm for radius computation
within $k$-Helly graphs. If we relax the definition of Helly number (for what
is sometimes called an "almost Helly-type" property in the literature), then
our approach leads to an approximation algorithm for computing the radius with
an additive one-sided error of at most some constant.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.00001"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00308">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00308">The Minimization of Random Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, Tobias Friedrich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schirneck:Martin.html">Martin Schirneck</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00308">PDF</a><br /><b>Abstract: </b>We investigate the maximum-entropy model $\mathcal{B}_{n,m,p}$ for random
$n$-vertex, $m$-edge multi-hypergraphs with expected edge size $pn$. We show
that the expected size of the minimization of $\mathcal{B}_{n,m,p}$, i.e., the
number of its inclusion-wise minimal edges, undergoes a phase transition with
respect to $m$. If $m$ is at most $1/(1-p)^{(1-p)n}$, then the minimization is
of size $\Theta(m)$. Beyond that point, for $\alpha$ such that $m =
1/(1-p)^{\alpha n}$ and $\mathrm{H}$ being the entropy function, it is
$\Theta(1) \cdot \min\!\left(1, \, \frac{1}{(\alpha\,{-}\,(1-p))
\sqrt{(1\,{-}\,\alpha) n}}\right) \cdot 2^{(\mathrm{H}(\alpha) + (1-\alpha)
\log_2 p) n}.$ This implies that the maximum expected size over all $m$ is
$\Theta((1+p)^n/\sqrt{n})$. Our structural findings have algorithmic
implications for minimizing an input hypergraph, which in turn has applications
in the profiling of relational databases as well as for the Orthogonal Vectors
problem studied in fine-grained complexity. The main technical tool is an
improvement of the Chernoff--Hoeffding inequality, which we make tight up to
constant factors. We show that for a binomial variable $X \sim
\mathrm{Bin}(n,p)$ and real number $0 &lt; x \le p$, it holds that $\mathrm{P}[X
\le xn] = \Theta(1) \cdot \min\!\left(1, \, \frac{1}{(p-x) \sqrt{xn}}\right)
\cdot 2^{-\!\mathrm{D}(x \,{\|}\, p) n}$, where $\mathrm{D}$ denotes the
Kullback--Leibler divergence between Bernoulli distributions. The result
remains true if $x$ depends on $n$ as long as it is bounded away from $0$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00308"><span class="datestr">at November 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/">Faculty at Tufts University (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Tufts Computer Science seeks Assistant/Associate Professor with research in Quantum Computation and Information, and a strong background in theoretical computer science whose research connects with current faculty in quantum information and beyond. Candidates should demonstrate attention to diversity and inclusion as related to teaching, research, and engagement. Tufts is an EO/AA employer.</p>
<p>Website: <a href="https://apply.interfolio.com/78094">https://apply.interfolio.com/78094</a><br />
Email: ttsearch@cs.tufts.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/"><span class="datestr">at November 02, 2020 10:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/11/02/constant-width-involutes">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/11/02/constant-width-involutes.html">Constant width from involutes of pseudotriangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In his <a href="https://www.eecs.yorku.ca/~jeff/courses/fun/">online collection of fun stuff</a>, Jeff Edmonds recently posted <a href="https://www.eecs.yorku.ca/~jeff/courses/fun/Equal_Distance.docx">a method of constructing curves of constant width</a> by spinning a pencil on a flat surface, with a varying axis, and tracking the movement of its ends. It is pretty similar to the classical method of crossed lines described by Martin Gardner in <em>The Unexpected Hanging</em>, in which one constructs an arrangement of lines in the plane, sorts them in circular order by slope, and builds a curve out of circular arcs centered at the crossing points of consecutive lines in this sorted order. However, it grows the curve at both ends simultaneously, rather than only at one end, and chooses the lines dynamically rather than in advance. Regardless, the result is the same: a piecewise-circular constant-width curve.</p>

<p>This got me wondering how we might go about constructing curves of constant width that are not piecewise-circular. Instead of a finite set of lines, we could use a continuous family of lines, one of each slope, but that’s a little difficult to visualize. Instead, there’s a simpler method that works more like Jeff’s spinning pencil, which Robinson (<a href="https://doi.org/10.1112/blms/16.3.264">“Smooth curves of constant width and transnormality”, <em>Bull. LMS</em> 1984</a>) attributes to Euler (<a href="https://scholarlycommons.pacific.edu/euler-works/513/">“De curvis triangularibus”, 1778</a>):</p>

<ul>
  <li>
    <p>Draw a closed curve in the plane that has only one tangent line of each slope.</p>
  </li>
  <li>
    <p>Rotate a “long enough” tangent line segment of some fixed length around this curve without sliding it.</p>
  </li>
  <li>
    <p>Trace the paths of the endpoints of the line segment.</p>
  </li>
</ul>

<p>The first step may already seem a little confusing. Don’t curves usually have at least two tangent lines of each slope, their support lines from opposite sides? Well, yes, for convex curves. But for a <a href="https://en.wikipedia.org/wiki/Pseudotriangle">pseudotriangle</a>, a curve whose boundary is concave everywhere except at three extreme points, there might only be one tangent line of each slope. A standard example of such a curve is the <a href="https://en.wikipedia.org/wiki/Deltoid_curve">deltoid</a>, and the following animation stolen from the <a href="https://mathcurve.com/courbes2d.gb/deltoid/deltoid.shtml">mathcurve.com page on deltoids</a> shows a tangent line segment (black) rotating around a deltoid (blue) and tracing out a curve of constant width (red).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/involute-deltoid.gif" alt="Animation of a tangent line segment rolling around a deltoid, with its endpoints tracing out a curve of constant width, from https://mathcurve.com/courbes2d.gb/deltoid/deltoid.shtml" /></p>

<p>The traced curve is always perpendicular to the rotating line segment, so locally at least this segment behaves like the width of the curve in each given direction. And since we don’t change the length of the segment while it rotates, the width stays constant. The same deltoid, the same curve of constant width, and two positions of its tangent line segment can also be seen in the illustration below from a 1954 mathematics paper, <a href="https://doi.org/10.2307/2307215">“Rotors within rotors” by Michael Goldberg in the <em>Amer. Math. Monthly</em></a>. I’ve overlaid a red Reuleaux triangle to show that, like <a href="https://11011110.github.io/blog/2020/08/30/linkage.html">so</a> <a href="https://11011110.github.io/blog/2020/07/05/shape-wankel-rotor.html">many</a> <a href="https://11011110.github.io/blog/2020/06/30/linkage.html">other</a> <a href="https://11011110.github.io/blog/2018/06/24/la-maddalena-non-reuleaux.html">curvy</a> <a href="https://11011110.github.io/blog/2018/04/17/mythical-reuleaux-manhole.html">triangles</a>, this is not a Reuleaux triangle, even though it has constant width. Although its corners are drawn to look kind of pointy, they should actually be smooth, and the rest of the curve bulges farther out from its sides than a Reuleaux triangle would.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/rotors-within-rotors.png" alt="The involute of deltoid, as depicted by Goldberg in &quot;Rotors within rotors&quot;, with an overlaid Reuleaux triangle" /></p>

<p>More formally, this process of rotating and tracing tangent line segments produces a curve called the <a href="https://en.wikipedia.org/wiki/Involute">involute</a> of the deltoid. An involute of a curve is more typically described as what you get when you fix one end of a length of string at a point on the curve, wrap it tightly around the curve, and then unwrap it while keeping it taut, tracing a curve with the other end of the string as you do. The two ends of the rotating tangent line segment can both be thought of as being formed in the same way from two strings, with one of them unwrapping the deltoid from one direction while the other wraps it back up in the other direction. In the deltoid example the segment was the same length as the sides of the deltoid, which were all equal, but it also works with unequal sides or longer segments, as long as the rotating segment is long enough to reach all three cusps.</p>

<p>You might worry whether the segment always comes back to its starting position after each rotation, and this does require a little care in the initial choice of length and placement of the line segment. If the length of the rotating segment and eventual width of the traced curve are \(w\), the pseudotriangle sides have lengths \(a\), \(b\), and \(c\), and the segment starts with \(x\) units of extra length extending past the cusp prior to side \(a\) in its rotation, then it will have \(w-a-x\) units of extra length at the next cusp, \(w-b-(w-a-x)=x+a-b\) units at the third cusp, and \(w-c-(x+a-b)=w-x-a+b-c\) units at the last cusp. To make a curve of constant width we need the amount of extra length at the start and end to be equal, which happens when we set this length to be \(x=(w-a+b-c)/2\).</p>

<p>If the pseudotriangle that the tangent segment rotates around includes a line segment, there will be a discontinuity in its rotating movement as the axis of rotation shifts from one end of the segment to the other, much like the changes of axis described by Edmonds, but the same process still works.  If the pseudotriangle has a point where its slope changes discontinuously (for instance, if it is a polygon rather than a smooth curve), then the rotating segment will rotate around this point, with its ends tracing circular arcs, as it continuously moves between the same slopes; this can happen either at the three convex points of the pseudotriangle or along the concave curves between them. In particular, if your pseudotriangle is actually an equilateral triangle and the rotating segment has the same length as its sides, you get a Reuleaux triangle.</p>

<p>It’s also possible to form closed curves with only one tangent line of each slope that are not pseudotriangles. An example is the standard pentagram (whose involute is the Reuleaux pentagon), or a cuspy and irregular pentagram like the one below (whose involute is another curve of constant width without circular arcs). The same process works for these, with a slightly more complicated calculation of how to place a rotating segment of a given length for a given starting curve, involving alternating sums of side lengths.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/cuspy-star.svg" alt="A curved five-point star with one tangent line in each direction" /></p>

<p>What happens when the rotating line segment is too short, so that it doesn’t reach one or more of the cusps? I’m not sure in general, but for the deltoid the result can be the same deltoid (for which the rotating line segment is one possibility for Goldberg’s “rotor within a rotor”, although the rotor he describes is larger) or another similar but smaller deltoid inside it. See the mathcurve link for details.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105144037558097550">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/11/02/constant-width-involutes.html"><span class="datestr">at November 02, 2020 05:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9123933489422342245">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html">I polled my class about the election</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In 2016 I had the Sophomore discrete math class do a poll of who they wanted for president.</p><p>In 2020 I had the  both my  Senior Crypto class and Clyde's Sophomore algorithms course do a poll of who they wanted for president.</p><p>All of these polls were anonymous. One big difference- in 2016 it was paper, they could check offwho they wanted or put in a write in, whereas in 2020 it was on elms without a mechanism for a write in--- so no votes for Bernie or Bill or Kruskal (not sure if they were voting for the man or the algorithm) were possible. In all cases I included everyone who was on the Maryland Ballot (so Libertarian and Green votes were possible). </p><p><br /></p><p>Discrete Math 2016: 428 students took the poll. Write ins allowed. </p><p>Clinton- 305 which is 71%</p><p>Trump- 44 which is  10%</p><p>Johnson (Libertarian)- 21 which is 5%</p><p>Stein (Green)-  11which is 3%</p><p>Sanders-6 which is 1%</p><p>Silly answers: 41 which is 10%</p><p>I was NOT surprised that Trump got 44 votes- every year I do this and every year the </p><p>republican gets between 10 and 20 percent. Romney go 17% in 2012 (see <a href="https://blog.computationalcomplexity.org/2012/11/random-thoughts-on-election.html">here</a>). </p><p><br /></p><p>Algorithms, 2020, 161 students took the poll</p><p>Biden: 127 (79%)</p><p>Trump: 25 (16%)</p><p>Hawkins (Green): 4 (2%)</p><p>Jorgenson (Libertarian): 4 (2%)</p><p>Segal (Bread and Roses Party) 1 (1%)</p><p><br /></p><p>Cryptography in 2020: </p><p>Biden- 40 which is 78%</p><p>Trump-6 which is 12%</p><p>Hawkins (Green ) 3 which is 6%</p><p>Jorgenson (Libertarian) 2 which is 4%</p><p>Segal (Bread and Roses) 0 which is 0%</p><p><br /></p><p>I have no idea what these numbers mean. College students tend to be liberal- we knew that. That Trump went from 10% to 16% would be interesting if it was a larger sample size. I wonder if forcing them to NOT have a write-in had an effect. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html"><span class="datestr">at November 02, 2020 03:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/160">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/160">TR20-160 |  Non-adaptive vs Adaptive Queries in the Dense Graph Testing Model | 

	Oded Goldreich, 

	Avi Wigderson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the relation between the query complexity of adaptive and non-adaptive testers in the dense graph model. 
It has been known for a couple of decades that the query complexity of non-adaptive testers is at most quadratic in the query complexity of adaptive testers. 
We show that this general result is essentially tight; that is, there exist graph properties for which any non-adaptive tester must have query complexity that is almost quadratic in the query complexity of the best general (i.e., adaptive) tester. 

More generally, for every $q:\N\to\N$ such that $q(n)\leq{\sqrt n}$ and constant $c\in[1,2]$, we show a graph property that is testable in $\Theta(q(n))$ queries, but its non-adaptive query complexity is $\Theta(q(n)^c)$, omitting $\poly(\log n)$ factors and ignoring the effect of the proximity parameter $\epsilon$. Furthermore, the upper bounds hold for one-sided error testers,
and are at most quadratic in $1/\epsilon$. 

These results are obtained through the use of general reductions that transport properties of ordered structured (like bit strings) to those of unordered structures (like unlabeled graphs). 
The main features of these reductions are query-efficiency and preservation of distance to the properties.
This method was initiated in our prior work ({\em ECCC}, TR20-149), and we significantly extend it here.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/160"><span class="datestr">at November 02, 2020 03:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/159">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/159">TR20-159 |  Relating existing powerful proof systems for QBF | 

	Leroy Chew</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We advance the theory of QBF proof systems by showing the first simulation of the universal checking format QRAT by a theory-friendly system. We show that the sequent system G fully p-simulates QRAT, including the Extended Universal Reduction (EUR) rule which was recently used to show QRAT does not have strategy extraction. Because EUR heavily uses resolution paths our technique also brings resolution path dependency and sequent systems closer together. 
	While we do not recommend G for practical applications this work can potentially show what features are needed for a new QBF checking format stronger than QRAT.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/159"><span class="datestr">at November 02, 2020 09:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">The Night of the Ethical Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Algorithms for the Election</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png"><img width="142" alt="" src="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png?w=142&amp;h=110" class="alignright wp-image-17767" height="110" /></a></p>
<p>
Michael Kearns and Aaron Roth are the authors of the <a href="https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205">book</a> <em>Ethical Algorithms and the The Science of Socially Aware Algorithm Design</em>. It has earned strong reviews including this <a href="https://www.nature.com/articles/s41567-019-0768-1.epdf?shared_access_token=PLLHPNTn5pByQUPaJ-n_KdRgN0jAjWel9jnR3ZoTv0OmI9hygrW1-UKQon3ZVGlMNsZowBl_5psAIvdU-Dic4gtf_j0e7S_897lCTN3vdbqv3cbSQwUtbHG78r5ycJNB2VKFndoPOT_8w0OYjS-pjQ%3D%3D">one</a> in <i>Nature</i>—impressive.</p>
<p>
Michael is a long-time friend who is a leader in machine learning, artificial intelligence, and much <a href="https://www.cis.upenn.edu/~mkearns/">more</a>. He also overlapped with Ken at Oxford while visiting Les Valiant there in the mid-1980s. He is at the University of Pennsylvania in computer science along with his co-author Roth. Cynthia Dwork and Roth wrote an earlier <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">book</a> on the related issue of Differential Privacy.</p>
<p>
Today we will talk about making algorithms ethical.</p>
<p>
Tuesday is the 2020 US national election for President, for Congress, and for state and local offices. Every four years we have a national election, and we cannot imagine a better motivation for making sure that algorithms are ethical. </p>
<p>
The word “algorithm” appears 157 times in their book. Two words used hand-in-hand with it are “data” (132 times) and “model” (103 times), both spread through all of the book’s 232 pages. Models of electorates, trained on data from past elections, inform the algorithms used by news agencies to make election-night projections. These carry more responsibilities than election-eve forecasts. There have been infamous mistakes, most notably the premature calls of Florida both ways in the 2000 election. </p>
<p>
We believe that Tuesday’s election in our novel pandemic situation requires attention to ethics from first principles. We will discuss why this is important. What it means to be ethical here? And how one can make an algorithm ethical? </p>
<p>
</p><p></p><h2> The Issue </h2><p></p>
<p></p><p>
Algorithms have been around forever. Euclid devised his <a href="https://en.wikipedia.org/wiki/Euclidean_algorithm">gcd</a> algorithm in 300 BCE. In the first half of the last century, the central issue was how to define that an algorithm is <b>effective</b>. This led to showing that some problems are <em>uncomputable</em>, so that algorithms for them are impossible.</p>
<p>
In the second half, the emphasis shifted to whether algorithms are <b>efficient</b>. This led to classifying problems as <em>feasible</em> or (contingently) <em>hard</em>. Although many algorithms for feasible problems have been improved in ways that redouble the effect of faster and cheaper hardware, the study of complexity classes such as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> has given reasons why algorithms for hard problems may never be improvable.</p>
<p>
The new territory, that of Kearns and Roth, is whether algorithms are <b>ethical</b>. Current ones that they and <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">others</a> have critqued as unethical accompany models for the likes of mortgages, small-business loans, parole decisions, and college admissions. The training data for these models often bakes in past biases. Besides problems of racial and gender bias and concerns of societal values, the raw fact is that past biases cause the models to miss the mark for today’s applications. For algorithms with such direct application to society, ethical design is critical. </p>
<p>
But this requirement is further reaching that one might initially imagine, so that as with computability and complexity, the factors can be ingrained in the <em>problems</em>. </p>
<p>
Consider a simple problem: We given a collection of pairs of numbers <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x,y)}" class="latex" title="{(x,y)}" />. We are to predict whether this number pair has the property </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%2B+y+%5Cge+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x + y \ge 0. " class="latex" title="\displaystyle  x + y \ge 0. " /></p>
<p>This is pretty easy if we can use both <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" />. But imagine a world where <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> is allowed to be viewed but <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" /> is secret. Perhaps the law requires that we cannot use <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" />—it is illegal. Now we might do as poorly as <img src="https://s0.wp.com/latex.php?latex=%7B50%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{50\%}" class="latex" title="{50\%}" />. Suppose that the data consists of </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%281%2C-1%29%2C+%281%2C1%29%2C+%282%2C-2%29%2C+%282%2C2%29+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots " class="latex" title="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots " /></p>
<p>Then seeing only <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> gives no advantage, while giving both is perfect. Thus what in these simplified terms counts as an ethical algorithm is a poor predictor, whereas an unethical one is perfect. </p>
<p>
The blurb for the Kearns-Roth book says that they “…explain how we can better embed human principles into machine code—without halting the advance of data-driven scientific exploration.” While we agree their approach is vital, we suspect that as with complexity there will be indelibly ethically hard tasks. We wonder whether election modeling has already become one of them. </p>
<p>
Ken and I have two separate takes on this. We will do the first and then the other in a second post.</p>
<p>
</p><p></p><h2> Red/Blue Leakage, Bias, and Ethics </h2><p></p>
<p></p><p>
One question on everyone’s minds is whether we will see a repeat of the forecasting misses from 2016. Let us remind that our own Election Day 2016 <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">post</a> started by defending Nate Silver of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for giving Donald Trump as much as a 30% chance to defeat Hillary Clinton. He had been attacked by representatives of many news and opinion agencies whose models had Clinton well over 90%. </p>
<p>
We wonder whether these models were affected by the kind of biases highlighted in the book by Kearns and Roth. We must say right away that we are neither alleging conscious biases nor questioning the desire for prediction accuracy. One issue in ethical modeling (for parole, loans, admissions) is the divergence between algorithm outcomes that are most <em>predictive</em> versus those that are best for society. Here we agree that accurate prediction—and accurate projections as results come in after the polls close—is paramount. However, the algorithms used for the latter projections (which were not at fault in 2016 but have been wrong previously) may be even more subject to what we as computer scientists with crypto background see as a “leakage” issue.</p>
<p>
Here is the point. Ideally, models using polling data and algorithms reading the Election Night returns should read the numbers as if they did not have ‘R’ and ‘D’ attached to them. Their own workings should be invariant under transformations that interchange Joe Biden and Donald Trump, or whoever are opposed in a local race. However, a crucial element in the projection models in particular is knowledge of voting geography. They must use data on the general voting preferences of regions where much of the vote is still extant. Thus they cannot avoid intimate knowledge of who is ‘R’ and who is ‘D.’ There is no double-blind or zero-knowledge approach to the subjects being projected.</p>
<p>
There is also the question of error bars. A main point of our 2016 post (and of Silver’s analysis) was the high uncertainty factor that could be read from how the Clinton-Trump race unfolded. Underestimating uncertainty causes overconfidence in models. This can result from “groupthink” of the kind we perceive in newsrooms of many of the same outlets that are doing the projections. The algorithms ought to be isolated from opinions of those in the organization, but again there is reason from the last election to wonder about leakage.</p>
<p>
Unlike cases addressed by Kearns and Roth, we do not see a solution to suggest. As in our simple <img src="https://s0.wp.com/latex.php?latex=%7Bx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x+y}" class="latex" title="{x+y}" /> example, prior knowledge of the data in full may be needed for prediction. This may just be an “Ethics-Hard” problem. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
The word “election” does <i>not</i> appear in Kearns and Roth’s book.  What further application of their standpoint to elections would you make?</p>
<p>Ken sees a larger question of ethical modeling decisions given the unprecedented circumstances of the current election.  This comes not from spatial geography distorted by the pandemic but rather from the dimension of time injected by massive early voting and late counting of many mailed ballots.  He will address this next.</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/"><span class="datestr">at November 02, 2020 05:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-10-31-ebb-and-flow-protocols-a-resolution-of-the-availability-finality-dilemma/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-10-31-ebb-and-flow-protocols-a-resolution-of-the-availability-finality-dilemma/">Resolving the Availability-Finality Dilemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Joachim Neu, Ertem Nusret Tas, and David Tse DLS and Nakamoto: Where to Go From Synchrony? An earlier blog post has explained classical models of the consensus literature from the 1980s, synchrony, asynchrony and partial synchrony. To recapitulate, the most basic network/adversary model is synchrony, where it...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-10-31-ebb-and-flow-protocols-a-resolution-of-the-availability-finality-dilemma/"><span class="datestr">at November 01, 2020 05:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/10/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/10/31/linkage.html">Linkage for a trick-or-treat-less Halloween</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="http://www.math.uwaterloo.ca/tsp/star/gaia1.html">3d flythrough of a near-optimal TSP tour through a dataset of nearly 221 stars</a> (<a href="https://mathstodon.xyz/@11011110/105048388019149239">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24807080">via</a>). I found the “full view of tour” a lot easier to navigate than the mini-view on the main page.</p>
  </li>
  <li>
    <p><a href="http://lightherder.blogspot.com/">The light herder</a> (<a href="https://mathstodon.xyz/@11011110/105056854067105557">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2020/10/18/amazing-in-camera-patterns-with-a-video-feedback-kinetic-sculpture.html">via</a>). Dave Blair makes dynamic fractals from old-school video feedback.</p>
  </li>
  <li>
    <p><a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda21-accepted-papers">Symposium on Discrete Algorithms (SODA 2021) accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105062503320962361">\(\mathbb{M}\)</a>). Lots of interesting looking titles there, but you’ll have to search online for links to the corresponding papers.</p>
  </li>
  <li>
    <p><a href="https://tomlehrersongs.com/">Tom Lehrer has made his song lyrics public domain, or as close to it as one can legally get</a> (<a href="https://mathstodon.xyz/@11011110/105076781151866254">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24833683">via</a>, <a href="https://boingboing.net/2020/10/20/brilliant-satirist-tom-lehrers-catalog-now-in-the-public-domain.html">via2</a>). But you have to download them within four years because his domain may go away after that. In honor of which, here’s a link to an (audio-only) version of <a href="https://www.youtube.com/watch?v=IL4vWJbwmqM">a little ditty about plagiarism</a> (only be sure always to call it please research).</p>
  </li>
  <li>
    <p><a href="https://dev.scottdarby.com/chaos-ink/">Chaos ink</a> (<a href="https://mathstodon.xyz/@11011110/105082966399876773">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2020/10/18/chaos-ink-disturb-a-tank-of-virtual-liquid-metal.html">via</a>). It’s rendered to look like waves in liquid metal, but I think it’s actually some kind of reaction-diffusion equation, in which you can move your mouse around to try to control where the reactions are centered.</p>
  </li>
  <li>
    <p><a href="https://nebusresearch.wordpress.com/2020/10/21/my-all-2020-mathematics-a-to-z-statistics/">2020 Mathematics A to Z: Statistics</a> (<a href="https://mathstodon.xyz/@nebusj/105085756046257623">\(\mathbb{M}\)</a>). On the differences between statistics and mathematics, historical connections between statistics and eugenics, and new connections to algorithmic fairness.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/bad-math-day-spring-2020/home">Bay Area Discrete Math Day, November 21</a> (<a href="https://mathstodon.xyz/@11011110/105091339250415859">\(\mathbb{M}\)</a>). This year, it’s a day of online discrete math talks, so you don’t actually need to be in the SF Bay Area to participate.</p>
  </li>
  <li>
    <p><a href="https://liorpachter.wordpress.com/2020/09/10/sexual-harassment-case-number-1052/">Lior Pachter reports on confirmed sexual harassment within the computational geometry community, at SoCG 2016 in Boston, by Adrian Dumitrescu</a> (<a href="https://mathstodon.xyz/@11011110/105097516664950105">\(\mathbb{M}\)</a>). According to the victim, <a href="https://twitter.com/RupeiXu/status/1302069912286957571">SoCG organizers told her they would try to bar Dumitrescu from future events, but told Dumitrescu he could not be barred</a> (see also <a href="https://twitter.com/RupeiXu/status/1310211818716049409">this update</a>). She says <a href="https://twitter.com/RupeiXu/status/1303309158427615234">there was also a second victim, whose academic career was “ruined” as a result</a>.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/wepa2020">Fourth International Workshop on Enumeration Problems and Applications</a> (<a href="https://mathstodon.xyz/@11011110/105102101211362136">\(\mathbb{M}\)</a>). I haven’t participated in previous instances but this year I’m on the program committee. Submission deadline November 8; online workshop December 7–10.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2020/10/26/the-guest-publishing-scam/">Igor Pak hates journal special issues</a> (<a href="https://mathstodon.xyz/@11011110/105116591625994062">\(\mathbb{M}\)</a>). The underlying problem appears to be loss of quality control compared to regular papers. He suggests handling festschrifts as books instead, and publishing surveys and reminiscences instead of research papers in them.</p>
  </li>
  <li>
    <p><a href="http://atlas.gregas.eu/graphs/31">The 84-vertex cubic symmetric graph</a> (<a href="https://mathstodon.xyz/@11011110/105121991976224491">\(\mathbb{M}\)</a>) is drawn nicely on <a href="http://www.mathpuzzle.com/">mathpuzzle.com</a> (update of June 27) using its structure as a <a href="https://www.abstract-polytopes.com/atlas/504/156/3.html">36-heptagon symmetric tiling of a non-orientable surface of Euler characteristic \(-6\)</a>. The Petrie dual of this tiling is <a href="https://www.abstract-polytopes.com/atlas/504/156/9.html">another symmetric tiling of the same graph with 28 nonagons on a higher-genus surface</a>. Does anyone know of other sources on these tilings? Or nice 3d embeddings of their surfaces?</p>
  </li>
  <li>
    <p><a href="https://plus.maths.org/content/prize-young-mathematicians"><em>Plus</em> magazine on Cambridge’s Whitehead Prize winners</a> (<a href="https://mathstodon.xyz/@11011110/105128113002070698">\(\mathbb{M}\)</a>):  A nice general-audience explainer of</p>

    <ul>
      <li>
        <p>Maria Bruna’s derivation of macro-level models from micro-level behavior, applied to vacuum cleaner design</p>
      </li>
      <li>
        <p>Holly Krieger’s connections between prime factors in integer sequences and special points on the Mandelbrot set</p>
      </li>
      <li>
        <p>Henry Wilton on the impossibility of determining whether infinite symmetry groups have finite quotients</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.iflscience.com/technology/ai-camera-ruins-soccar-game-for-fans-after-mistaking-referees-bald-head-for-ball/">Silly computer news of the day: automatically aimed soccer game video camera follows bald referee’s head instead of the ball, causing fans to miss key plays</a> (<a href="https://mathstodon.xyz/@11011110/105131999819480420">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24955651">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/10/31/linkage.html"><span class="datestr">at October 31, 2020 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
