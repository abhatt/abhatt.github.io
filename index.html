<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 24, 2021 05:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6958131418382007261">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">I went to the ``debate'' about Program Verif and the Lipton-Demillo-Perlis paper</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>On Thursday June 17 I went to (on zoom- does that need to be added anymore?)</p><p><i>A Debate on Program Correctness</i></p><p>There was no subtitle but it could have been:</p><p>Have the points made in <i>Social Processes and Proofs of Theorems and Programs</i> by DeMillo, Lipton, Perlis, survived the test of time ? (Spoiler Alert: Yes.)</p><p>I found out about it from the Lipton-Regan blog <a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/">here</a></p><p>The debaters were Richard DeMillo and Richard Lipton and the moderator was Harry Lewis (Alan Perlis passed away in 1990).  Calling it a debate is not correct since DeMillo and Lipton (and Lewis) all agree. (DeMillo and Lipton even have the same first name!)  The DLP paper is in Harry Lewis's collection<i> Ideas that created the future.</i>  The event  should have been advertised as a discussion. However, it was a good discussion so this is not a complaint.</p><p>Here are some things that came out of the discussion.</p><p>1) The main topic was the 1979 DeMillo-Lipton-Perlis paper (see <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">here</a>) that gave arguments why Proofs of Program correctness could not work.</p><p>An all-to-brief summary of the DLP paper: Some researchers are trying to set up frameworks for doing proofs that programs are correct, analogous to the certainty  we get with a proof of a Theorem in Mathematics. But proofs in Mathematics are, in reality, NOT that rigorous. Often details are left out or left to the reader. This is fine for mathematics (more on that later) but unacceptable for programs which need rather precise and rigorous proofs.</p><p>How do theorems in mathematics really get verified? By having enough people look at them and make sure they match intuitions, what DLP call <i>A Social Process</i>.  (NOTE FROM BILL: Papers that are not important do not get looked at so there may well be errors.)</p><p>2) The notion of proving-programs-correct was very seductive; however, the people who were trying to do this had a blind spot about how the analogy of proving-programs-correct and proving-theorem-correct differ.  In particular, a program is rather complicated and even stating carefully what you want to prove is difficult. By contrast, for most math statements, what you want to prove is clear. Note also that a program has lots of code (far more now than when DLP was written) and so much can happen that you cannot account for.</p><p>3) The DLP paper had a large effect on the field of program verification.  Funding for it was reduced and students were discouraged from going into it.</p><p>4) When DLP appeared DeMillo and Lipton were pre-tenure. Hence it took lots of courage to publish it. Alan Perlis had tenure and had already won a Turing award.  This did give DeMillo and Lipton some cover; however, it still took courage.</p><div><div>5) How did the Program Verification  Community deal with the objections in DLP?  DeMillo said that he looked at a large set of papers in the field, and very few even mentioned DLP. He recommends reading the book <i>Mechanizing Proof: Computing, Risk, and Trust by David McKenzie </i>see <a href="https://www.amazon.com/exec/obidos/ASIN/0262632950">here</a>.</div><div><br /></div><div>6) So how can we be more certain that programs are correct?</div><div><br /></div><div>a) Testing.</div><div>b) Modularize and test. Fix errors. Modularize  and test. Fix errors...</div><div>c) Try to isolate side effects.</div><div>d) More testing.</div><div><br /></div><div>Some point to Model Checking, which could be considered very sophisticated testing, but that's used to verify circuits and perhaps low-level code, not programs. Model checking is a success story and note that Ed Clark, E. Allen Emerson, and Joseph Sifakis shared a (well deserved) Turing award for this work. But see next note.</div><div><br /></div><div>6.5) An audience member pointed out that Program Verification people have won several Turing Awards</div><div><br /></div><div>Dijkstra 1972</div><div><br /></div><div>Floyd 1978 </div><div><br /></div><div>Hoare 1980</div><div><br /></div><div>Pnueli 1996</div><div><br /></div><div>(Are there more?) </div><div><br /></div><div>so the field is alive and healthy. DeMillo responded that prizes for academic research are a poor measure of  success. </div><div><br /></div><div>7) Can computers themselves help with proofs of correctness? That is the only hope; however, there are scaling problems.</div><div><br /></div><div>8) When DLP was written a program with 100,000 lines of code was considered large. Now we have programs with millions of lines of code. And now we have more concurrency. So the lessons of the DLP paper are probably more relevant now then they were then.</div><div><br /></div><div>9) Since Program Verification does not seem to be used, how come we don't have a Software crisis?</div><div><br /></div><div>a) We do! The Q+A mechanism at the meeting was terrible. </div><div>b) We do! FILL IN YOUR OWN FAVORITE STORY OF BAD SOFTWARE.</div><div>c) See the answer to question 6.</div><div><br /></div><div>10) SPECS are a problem. Tony Hoare once gave a talk where he proves that a program sorted correctly and then pointed out that if the program just output 0,...0 that would have also satisfied the SPEC since all that was required was that the output be sorted, not the (overlooked!) requirement that it be the same numbers as the input. So one needs to be careful!</div><div><br /></div><div>11) Despite being a leader in the field, Tony Hoare has come to see the limitations of the Proofing-programs-correct approach to Software Verification.  In his paper An Axiomatic basis for Computer Programming (1969)  (which is also in Harry Lewis's collection <i>Ideas that Created the Future</i>) Hoare says the following:</div><div><br /></div><div>Ten years ago, researchers into formal methods (and I was the most mistaken among them) predicted that the programming world would embrace with gratitude every assistance promised by formalization to solve the problems of reliability that arise when programs get large and more safety-critical. Programs have now got very large and very critical--well beyond the scale which can be comfortably tackled by formal methods. There have been many problems and failures, but these have nearly always been attributable to inadequate analysis of requirements or inadequate management control. It has turned out that the world just does not suffer significantly from the kind of problem that our research was originally intended to solve.'</div><div><br /></div></div><div><div>12) Richard Lipton told a story where he showed that the program in question satisfied the SPEC, but the SPEC was a tautology that any program would satisfy.  Again, one needs to be careful!</div><div><br /></div><div>13) The test of time: Verifying large scale programs does not seem to be common in industry. Is industrial adaptation a fair measure? </div><div><br /></div><div>14) Harry Lewis's  book <i>Ideas that created the future</i> collects up, edits, and comments on 46 important papers in Computer Science (I reviewed it in the issue of SIGACT News that is in your mailbox---I will blog about it at a later time.) There are several papers in it about program verification, including DLP, Hoare's paper, and three papers by Dijkstra.</div><div><br /></div><div>a) When Harry discussed including DLP some people said `You're going to include that!  Its a polemic, not a paper!'</div><div><br /></div><div>b) When Harry teaches a course from this book (it must be an awesome class!) and asks the students at the end which papers they learned the most from, the top two are an excerpt from Fred Brooks <i>The</i> <i>Mythical Man Month</i> (see my post on Brook's work <a href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html">here</a> ) and DLP.</div><div><br /></div><div>c) I am hoping that this is just one of 46 talks with authors of the papers in his book.  I look forward to his interview with Aristotle, Leibnitz, Boole, Turing, ...</div></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html"><span class="datestr">at June 24, 2021 03:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5549">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5549">STOC’2021 and BosonSampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Happy birthday to Alan Turing!</p>



<p>This week I’m participating virtually in <a href="http://acm-stoc.org/stoc2021/">STOC’2021</a>, which today had a celebration of the 50th anniversary of NP-completeness (featuring Steve Cook, Richard Karp, Leonid Levin, Christos Papadimitriou, and Avi Wigderson), and which tomorrow will have a day’s worth of quantum computing content, including a tutorial on MIP*=RE, two quantum sessions, and an invited talk on quantum supremacy by John Martinis.  I confess that I’m not a fan of GatherTown, the platform being used for STOC.  Basically, you get a little avatar who wanders around a virtual hotel lobby and enters sessions—but it seems to reproduce all of the frustrating and annoying parts of experience without any of the good parts.</p>



<p>Ah!  But I got the surprising news that Alex Arkhipov and I are among the winners of STOC’s first-ever <a href="https://sigact.org/prizes/stoc_tot.html">“Test of Time Award,”</a> for our <a href="https://www.scottaaronson.com/papers/optics.pdf">paper on BosonSampling</a>.  It feels strange to win a “Test of Time” award for work that we did in 2011, which still seems like yesterday to me.  All the more since the experimental status and prospects of quantum supremacy via BosonSampling are still very much live, unresolved questions.</p>



<p>Speaking of which: on Monday, Alexey Rubtsov, of the Skolkovo Institute in Moscow, gave a talk for our quantum information group meeting at UT, about his <a href="https://arxiv.org/abs/2106.01445">recent work with Popova</a> on classically simulating Gaussian BosonSampling.  From the talk, I learned something extremely important.  I had imagined that their simulation must take advantage of the high rate of photon loss in actual experiments (like the <a href="https://www.scottaaronson.com/blog/?p=5159">USTC experiment</a> from late 2020), because how else are you going to simulate BosonSampling efficiently?  But Rubtsov explained that that’s not how it works at all.  While their algorithm is heuristic and remains to be rigorously analyzed, numerical studies suggest that it works even with <em>no</em> photon losses or other errors.  Having said that, their algorithm works:</p>



<ul><li>only for Gaussian BosonSampling, not Fock-state BosonSampling (as Arkhipov and I had originally proposed),</li><li>only for threshold detectors, not photon-counting detectors, and</li><li>only for a small number of modes (say, linear in the number of photons), not for a large number of modes (say, quadratic in the number of photons) as in the original proposal.</li></ul>



<p>So, bottom line, it now looks like the USTC experiment, amazing engineering achievement though it was, is not hard to spoof with a classical computer.  If so, this is because of multiple ways in which the experiment differed from my and Arkhipov’s original theoretical proposal.  We know exactly what those ways are—indeed, you can find them in my earlier blog posts on the subject—and hopefully they can be addressed in future experiments.  All in all, then, we’re left with a powerful demonstration of the continuing relevance of formal hardness reductions, and the danger of replacing them with intuitions and “well, it still seems hard to <em>me</em>.”  So I hope the committee won’t rescind my and Arkhipov’s Test of Time Award based on these developments in the past couple weeks!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5549"><span class="datestr">at June 23, 2021 04:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1325">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2021/06/23/tcs-visioning-2020-report-and-slides/">TCS Visioning 2020 report and slides</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In July 2020, the <a href="https://thmatters.wordpress.com/catcs/">CATCS</a> organized a <a href="https://thmatters.wordpress.com/2020/06/05/tcs-visioning-workshop-call-for-participation/">visioning workshop</a>. We are happy to announce the release of a report and posters based on this workshop. Material produced from this workshop is available and free to use by any member of the TCS community. We gratefully acknowledge financial as well as organizational support by the <a href="https://www.sigact.org/">SIGACT</a> and <a href="https://cra.org/ccc/">CCC</a> for this activity.</p>



<p>We are planning a follow-up event for disseminating the report and posters to funding agencies in the next few months. Details are forthcoming.</p>



<p>TCS Visioning Full Report: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-report.pdf">here</a></p>



<p>Short Report (CCC Quadrennial paper): <a href="https://cra.org/ccc/wp-content/uploads/sites/2/2020/10/Theoretical-Computer-Science_.pdf">here</a></p>



<p>TCS Visioning Slides: <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides.pptx">in PPT</a> and <a href="https://thmatters.files.wordpress.com/2021/06/visioning-slides-1.pdf">in PDF</a></p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2021/06/23/tcs-visioning-2020-report-and-slides/"><span class="datestr">at June 23, 2021 04:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/088">TR21-088 |  Open Problems in Property Testing of Graphs | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We briefly discuss a few open problems in the study of various models of testing graph properties, focusing on the query complexity of the various tasks. In the dense graph model, we discuss several open problems, including:

* Determining the complexity of testing triangle-freeness.
* Characterizing the class of properties that are testable within extremely low complexity. 

Turning to the bounded-degree graph model, we discuss several open problems, including:

* Characterizing the class of properties that are testable within size-oblivious complexity.
* Determining the complexity of graph isomorphism. 
In each of the foregoing models, we also discuss a favorite open problem that was recently resolved. Lastly, we discuss the vast lack of knowledge with respect to testing graph properties in the general graph model.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/088"><span class="datestr">at June 23, 2021 01:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11938">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11938">Robust Regression Revisited: Acceleration and Improved Estimation Rates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jambulapati:Arun.html">Arun Jambulapati</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schramm:Tselil.html">Tselil Schramm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11938">PDF</a><br /><b>Abstract: </b>We study fast algorithms for statistical regression problems under the strong
contamination model, where the goal is to approximately optimize a generalized
linear model (GLM) given adversarially corrupted samples. Prior works in this
line of research were based on the robust gradient descent framework of Prasad
et. al., a first-order method using biased gradient queries, or the Sever
framework of Diakonikolas et. al., an iterative outlier-removal method calling
a stationary point finder.
</p>
<p>We present nearly-linear time algorithms for robust regression problems with
improved runtime or estimation guarantees compared to the state-of-the-art. For
the general case of smooth GLMs (e.g. logistic regression), we show that the
robust gradient descent framework of Prasad et. al. can be accelerated, and
show our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs
(e.g. support vector machines), answering several open questions in the
literature.
</p>
<p>For the well-studied case of robust linear regression, we present an
alternative approach obtaining improved estimation rates over prior
nearly-linear time algorithms. Interestingly, our method starts with an
identifiability proof introduced in the context of the sum-of-squares algorithm
of Bakshi and Prasad, which achieved optimal error rates while requiring large
polynomial runtime and sample complexity. We reinterpret their proof within the
Sever framework and obtain a dramatically faster and more sample-efficient
algorithm under fewer distributional assumptions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11938"><span class="datestr">at June 23, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11886">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11886">A Negative Answer to $P\overset{?}{=}PSPACE$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Tianrong.html">Tianrong Lin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11886">PDF</a><br /><b>Abstract: </b>There is a conjecture on $P\overset{?}{=}PSPACE$ in computational complexity
zoo. It is a widespread belief that $P\neq PSPACE$, otherwise $P=NP$ which is
extremely impossible. In this short work, we assert that $P\neq PSPACE$ no
matter what outcome is on $P\overset{?}{=}NP$. We accomplishe this via showing
$NP\neq PSPACE$. The method is by the result that Circuit-SAT$\in DSPACE[n]$
and the known result $DSPACE[n]\subset DSPACE[n^2]$ by the space complexity
hierarchy theorem. Closely related consequences are summarized.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11886"><span class="datestr">at June 23, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11884">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11884">Parallel decomposition of persistence modules through interval bases</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gregorio:Alessandro_De.html">Alessandro De Gregorio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guerra:Marco.html">Marco Guerra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scaramuccia:Sara.html">Sara Scaramuccia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaccarino:Francesco.html">Francesco Vaccarino</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11884">PDF</a><br /><b>Abstract: </b>We introduce an algorithm to decompose any finite-type persistence module
with coefficients in a field into what we call an {\em interval basis}. This
construction yields both the standard persistence pairs of Topological Data
Analysis (TDA), as well as a special set of generators inducing the interval
decomposition of the Structure theorem. The computation of this basis can be
distributed over the steps in the persistence module. This construction works
for general persistence modules on a field $\mathbb{F}$, not necessarily
deriving from persistent homology. We subsequently provide a parallel algorithm
to build a persistent homology module over $\mathbb{R}$ by leveraging the Hodge
decomposition, thus providing new motivation to explore the interplay between
TDA and the Hodge Laplacian.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11884"><span class="datestr">at June 23, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11877">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11877">Eliminating Intermediate Measurements using Pseudorandom Generators</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Girish:Uma.html">Uma Girish</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raz:Ran.html">Ran Raz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11877">PDF</a><br /><b>Abstract: </b>We show that quantum algorithms of time $T$ and space $S\ge \log T$ with
intermediate measurements can be simulated by quantum algorithms of time $T
\cdot \mathrm{poly}(S)$ and space $O(S\cdot \log T )$ without intermediate
measurements. The best simulations prior to this work required either
$\Omega(T)$ space (by the deferred measurement principle) or
$\mathrm{poly}(2^S)$ time [FR21, GRZ21]. Our result is thus a time-efficient
and space-efficient simulation of algorithms with intermediate measurements by
algorithms without intermediate measurements.
</p>
<p>To prove our result, we study pseudorandom generators for quantum
space-bounded algorithms. We show that (an instance of) the INW pseudorandom
generator for classical space-bounded algorithms [INW94] also fools quantum
space-bounded algorithms. More precisely, we show that for quantum
space-bounded algorithms that have access to a read-once tape consisting of
random bits, the final state of the algorithm when the random bits are drawn
from the uniform distribution is nearly identical to the final state when the
random bits are drawn using the INW pseudorandom generator.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11877"><span class="datestr">at June 23, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11863">Graph coarsening: From scientific computing to machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Jie.html">Jie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saad:Yousef.html">Yousef Saad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Zechen.html">Zechen Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11863">PDF</a><br /><b>Abstract: </b>The general method of graph coarsening or graph reduction has been a
remarkably useful and ubiquitous tool in scientific computing and it is now
just starting to have a similar impact in machine learning. The goal of this
paper is to take a broad look into coarsening techniques that have been
successfully deployed in scientific computing and see how similar principles
are finding their way in more recent applications related to machine learning.
In scientific computing, coarsening plays a central role in algebraic multigrid
methods as well as the related class of multilevel incomplete LU
factorizations. In machine learning, graph coarsening goes under various names,
e.g., graph downsampling or graph reduction. Its goal in most cases is to
replace some original graph by one which has fewer nodes, but whose structure
and characteristics are similar to those of the original graph. As will be
seen, a common strategy in these methods is to rely on spectral properties to
define the coarse graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11863"><span class="datestr">at June 23, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11744">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11744">Fully Dynamic Algorithms for Minimum Weight Cycle and Related Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karczmarz:Adam.html">Adam Karczmarz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11744">PDF</a><br /><b>Abstract: </b>We consider the directed minimum weight cycle problem in the fully dynamic
setting. To the best of our knowledge, so far no fully dynamic algorithms have
been designed specifically for the minimum weight cycle problem in general
digraphs. One can achieve $\tilde{O}(n^2)$ amortized update time by simply
invoking the fully dynamic APSP algorithm of Demetrescu and Italiano [J.
ACM'04]. This bound, however, yields no improvement over the trivial
recompute-from-scratch algorithm for sparse graphs.
</p>
<p>Our first contribution is a very simple deterministic
$(1+\epsilon)$-approximate algorithm supporting vertex updates (i.e., changing
all edges incident to a specified vertex) in conditionally near-optimal
$\tilde{O}(m\log{(W)}/\epsilon)$ amortized time for digraphs with real edge
weights in $[1,W]$. Using known techniques, the algorithm can be implemented on
planar graphs and also gives some new sublinear fully dynamic algorithms
maintaining approximate cuts and flows in planar digraphs.
</p>
<p>Additionally, we show a Monte Carlo randomized exact fully dynamic minimum
weight cycle algorithm with $\tilde{O}(mn^{2/3})$ worst-case update that works
for real edge weights. To this end, we generalize the exact fully dynamic APSP
data structure of Abraham et al. [SODA'17] to solve the ``multiple-pairs
shortest paths problem'', where one is interested in computing distances for
some $k$ (instead of all $n^2$) fixed source-target pairs after each update. We
show that in such a scenario, $\tilde{O}((m+k)n^{2/3})$ worst-case update time
is possible.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11744"><span class="datestr">at June 23, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11696">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11696">Diversity-aware $k$-median : Clustering with fair center representation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thejaswi:Suhas.html">Suhas Thejaswi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ordozgoiti:Bruno.html">Bruno Ordozgoiti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gionis:Aristides.html">Aristides Gionis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11696">PDF</a><br /><b>Abstract: </b>We introduce a novel problem for diversity-aware clustering. We assume that
the potential cluster centers belong to a set of groups defined by protected
attributes, such as ethnicity, gender, etc. We then ask to find a minimum-cost
clustering of the data into $k$ clusters so that a specified minimum number of
cluster centers are chosen from each group. We thus require that all groups are
represented in the clustering solution as cluster centers, according to
specified requirements. More precisely, we are given a set of clients $C$, a
set of facilities $\pazocal{F}$, a collection $\mathcal{F}=\{F_1,\dots,F_t\}$
of facility groups $F_i \subseteq \pazocal{F}$, budget $k$, and a set of
lower-bound thresholds $R=\{r_1,\dots,r_t\}$, one for each group in
$\mathcal{F}$. The \emph{diversity-aware $k$-median problem} asks to find a set
$S$ of $k$ facilities in $\pazocal{F}$ such that $|S \cap F_i| \geq r_i$, that
is, at least $r_i$ centers in $S$ are from group $F_i$, and the $k$-median cost
$\sum_{c \in C} \min_{s \in S} d(c,s)$ is minimized. We show that in the
general case where the facility groups may overlap, the diversity-aware
$k$-median problem is \np-hard, fixed-parameter intractable, and inapproximable
to any multiplicative factor. On the other hand, when the facility groups are
disjoint, approximation algorithms can be obtained by reduction to the
\emph{matroid median} and \emph{red-blue median} problems. Experimentally, we
evaluate our approximation methods for the tractable cases, and present a
relaxation-based heuristic for the theoretically intractable case, which can
provide high-quality and efficient solutions for real-world datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11696"><span class="datestr">at June 23, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11689">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11689">On the Parameterized Complexity of the Connected Flow and Many Visits TSP Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Isja Mannens, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, Céline Swennenhuis, Krisztina Szilágyi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11689">PDF</a><br /><b>Abstract: </b>We study a variant of Min Cost Flow in which the flow needs to be connected.
Specifically, in the Connected Flow problem one is given a directed graph $G$,
along with a set of demand vertices $D \subseteq V(G)$ with demands
$\mathsf{dem}: D \rightarrow \mathbb{N}$, and costs and capacities for each
edge. The goal is to find a minimum cost flow that satisfies the demands,
respects the capacities and induces a (strongly) connected subgraph. This
generalizes previously studied problems like the (Many Visits) TSP. We study
the parameterized complexity of Connected Flow parameterized by $|D|$, the
treewidth $tw$ and by vertex cover size $k$ of $G$ and provide:
</p>
<p>(i) $\mathsf{NP}$-completeness already for the case $|D|=2$ with only unit
demands and capacities and no edge costs, and fixed-parameter tractability if
there are no capacities,
</p>
<p>(ii) a fixed-parameter tractable $\mathcal{O}^{\star}(k^{\mathcal{O}(k)})$
time algorithm for the general case, and a kernel of size polynomial in $k$ for
the special case of Many Visits TSP,
</p>
<p>(iii) an $|V(G)|^{\mathcal{O}(tw)}$ time algorithm and a matching
$|V(G)|^{o(tw)}$ time conditional lower bound conditioned on the Exponential
Time Hypothesis.
</p>
<p>To achieve some of our results, we significantly extend an approach by
Kowalik et al.~[ESA'20].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11689"><span class="datestr">at June 23, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11675">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11675">Preprocessing to Reduce the Search Space: Antler Structures for Feedback Vertex Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Donkers:Huib.html">Huib Donkers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Bart_M=_P=.html">Bart M. P. Jansen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11675">PDF</a><br /><b>Abstract: </b>The goal of this paper is to open up a new research direction aimed at
understanding the power of preprocessing in speeding up algorithms that solve
NP-hard problems exactly. We explore this direction for the classic Feedback
Vertex Set problem on undirected graphs, leading to a new type of graph
structure called antler decomposition, which identifies vertices that belong to
an optimal solution. It is an analogue of the celebrated crown decomposition
which has been used for Vertex Cover. We develop the graph structure theory
around such decompositions and develop fixed-parameter tractable algorithms to
find them, parameterized by the number of vertices for which they witness
presence in an optimal solution. This reduces the search space of
fixed-parameter tractable algorithms parameterized by the solution size that
solve Feedback Vertex Set.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11675"><span class="datestr">at June 23, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11626">Morse-Smale complexes on convex polyhedra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Balázs Ludmány, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=aacute=ngi:Zsolt.html">Zsolt Lángi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Domokos:G=aacute=bor.html">Gábor Domokos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11626">PDF</a><br /><b>Abstract: </b>Motivated by applications in geomorphology, the aim of this paper is to
extend Morse-Smale theory from smooth functions to the radial distance function
(measured from an internal point), defining a convex polyhedron in
3-dimensional Euclidean space. The resulting polyhedral Morse-Smale complex may
be regarded, on one hand, as a generalization of the Morse-Smale complex of the
smooth radial distance function defining a smooth, convex body, on the other
hand, it could be also regarded as a generalization of the Morse-Smale complex
of the piecewise linear parallel distance function (measured from a plane),
defining a polyhedral surface. Beyond similarities, our paper also highlights
the marked differences between these three problems and it also includes the
design, implementation and testing of an explicit algorithm computing the
Morse-Smale complex on a convex polyhedron.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11626"><span class="datestr">at June 23, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11621">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11621">Near-Delaunay Metrics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beusekom:Nathan_van.html">Nathan van Beusekom</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buchin:Kevin.html">Kevin Buchin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koerts:Hidde.html">Hidde Koerts</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meulemans:Wouter.html">Wouter Meulemans</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodatz:Benjamin.html">Benjamin Rodatz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Speckmann:Bettina.html">Bettina Speckmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11621">PDF</a><br /><b>Abstract: </b>We study metrics that assess how close a triangulation is to being a Delaunay
triangulation, for use in contexts where a good triangulation is desired but
constraints (e.g., maximum degree) prevent the use of the Delaunay
triangulation itself. Our near-Delaunay metrics derive from common Delaunay
properties and satisfy a basic set of design criteria, such as being invariant
under similarity transformations. We compare the metrics, showing that each can
make different judgments as to which triangulation is closer to Delaunay. We
also present a preliminary experiment, showing how optimizing for these metrics
under different constraints gives similar, but not necessarily identical
results, on random and constructed small point sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11621"><span class="datestr">at June 23, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11565">Practical Near Neighbor Search via Group Testing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Joshua Engels, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coleman:Benjamin.html">Benjamin Coleman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11565">PDF</a><br /><b>Abstract: </b>We present a new algorithm for the approximate near neighbor problem that
combines classical ideas from group testing with locality-sensitive hashing
(LSH). We reduce the near neighbor search problem to a group testing problem by
designating neighbors as "positives," non-neighbors as "negatives," and
approximate membership queries as group tests. We instantiate this framework
using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups
(FLINNG). We prove that FLINNG has sub-linear query time and show that our
algorithm comes with a variety of practical advantages. For example, FLINNG can
be constructed in a single pass through the data, consists entirely of
efficient integer operations, and does not require any distance computations.
We conduct large-scale experiments on high-dimensional search tasks such as
genome search, URL similarity search, and embedding search over the massive
YFCC100M dataset. In our comparison with leading algorithms such as HNSW and
FAISS, we find that FLINNG can provide up to a 10x query speedup with
substantially smaller indexing time and memory.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11565"><span class="datestr">at June 23, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11426">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11426">Efficient Inference via Universal LSH Kernel</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Zichang.html">Zichang Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coleman:Benjamin.html">Benjamin Coleman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11426">PDF</a><br /><b>Abstract: </b>Large machine learning models achieve unprecedented performance on various
tasks and have evolved as the go-to technique. However, deploying these compute
and memory hungry models on resource constraint environments poses new
challenges. In this work, we propose mathematically provable Representer
Sketch, a concise set of count arrays that can approximate the inference
procedure with simple hashing computations and aggregations. Representer Sketch
builds upon the popular Representer Theorem from kernel literature, hence the
name, providing a generic fundamental alternative to the problem of efficient
inference that goes beyond the popular approach such as quantization, iterative
pruning and knowledge distillation. A neural network function is transformed to
its weighted kernel density representation, which can be very efficiently
estimated with our sketching algorithm. Empirically, we show that Representer
Sketch achieves up to 114x reduction in storage requirement and 59x reduction
in computation complexity without any drop in accuracy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11426"><span class="datestr">at June 23, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.11372">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.11372">First Approximation for Uniform Lower and Upper Bounded Facility Location Problem avoiding violation in Lower Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grover:Sapna.html">Sapna Grover</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Neelima.html">Neelima Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dabas:Rajni.html">Rajni Dabas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.11372">PDF</a><br /><b>Abstract: </b>With growing emphasis on e-commerce marketplace platforms where we have a
central platform mediating between the seller and the buyer, it becomes
important to keep a check on the availability and profitability of the central
store. A store serving too less clients can be non-profitable and a store
getting too many orders can lead to bad service to the customers which can be
detrimental for the business. In this paper, we study the facility location
problem(FL) with upper and lower bounds on the number of clients an open
facility serves. Constant factor approximations are known for the restricted
variants of the problem with only the upper bounds or only the lower bounds.
The only work that deals with bounds on both the sides violates both the bounds
\cite{friggstad_et_al_LBUFL}. In this paper, we present the first (constant
factor) approximation for the problem violating the upper bound by a factor of
%\textcolor{magenta}{$(2 + \epsilon)$} $(5/2)$ without violating the lower
bounds when both the lower and the upper bounds are uniform. We first give a
tri-criteria (constant factor) approximation violating both the upper and the
lower bounds and then get rid of violation in lower bounds by transforming the
problem instance to an instance of capacitated facility location problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.11372"><span class="datestr">at June 23, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/087">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/087">TR21-087 |  Eliminating Intermediate Measurements using Pseudorandom Generators | 

	Uma Girish, 

	Ran Raz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that quantum algorithms of time T and space $S \ge \log T$ with intermediate measurements can be simulated by quantum algorithms of time $T\cdot \mathrm{poly}(S)$ and space $O(S\cdot \log T)$ without intermediate measurements. The best simulations prior to this work required either $\Omega(T)$ space (by the deferred measurement principle) or $\mathrm{poly}(2^S)$ time [FR21, GRZ21]. Our result is thus a time-efficient and space-efficient simulation of algorithms with intermediate measurements by algorithms without intermediate measurements.

To prove our result, we study pseudorandom generators for quantum space-bounded algorithms. We show that (an instance of) the INW pseudorandom generator for classical space-bounded algorithms [INW94] also fools quantum space-bounded algorithms. More precisely, we show that for quantum space-bounded algorithms that have access to a read-once tape consisting of random bits, the final state of the algorithm when the random bits are drawn from the uniform distribution is nearly identical to the final state when the random bits are drawn using the INW pseudorandom generator.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/087"><span class="datestr">at June 22, 2021 04:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/086">TR21-086 |  Linear Space Streaming Lower Bounds for Approximating CSPs | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Ameya Velingker, 

	Santhoshini Velusamy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the approximability of constraint satisfaction problems in the streaming setting. For every constraint satisfaction problem (CSP) on $n$ variables taking values in $\{0,\ldots,q-1\}$, we prove that improving over the trivial approximability by a factor of $q$ requires $\Omega(n)$ space even on instances with $O(n)$ constraints. We also identify a broad subclass of problems for which any improvement over the trivial approximability requires $\Omega(n)$ space. The key technical core is an optimal, $q^{-(k-1)}$-inapproximability for the case where every constraint is given by a system of $k-1$ linear equations $\bmod\; q$ over $k$ variables. Prior to our work, no such hardness was known for an approximation factor less than $1/2$ for any CSP. Our work builds on and extends the work of Kapralov and Krachun (Proc. STOC 2019) who showed a linear lower bound on any non-trivial approximation of the max cut in graphs. This corresponds roughly to the case of  Max $k$-LIN-$\bmod\; q$ with $k=q=2$. Each one of the extensions provides non-trivial technical challenges that we overcome in this work.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/086"><span class="datestr">at June 22, 2021 02:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 4, 2021 – July 7, 2021 Wrocław, Poland and online https://cpm2021.ii.uni.wroc.pl/ The Annual Symposium on Combinatorial Pattern Matching (CPM) has by now over 30 years of tradition and is considered to be the leading conference for the community working on Stringology. The objective of the annual CPM meetings is to provide an international forum … <a href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/" class="more-link">Continue reading <span class="screen-reader-text">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/"><span class="datestr">at June 22, 2021 08:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/085">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/085">TR21-085 |  The Final Nail in the Coffin of Statistically-Secure Obfuscator. | 

	Ilya Volkovich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present an elementary, self-contained proof of the result of Goldwasser and Rothblum [GR07] that the existence of a (perfect) statistically secure obfuscator implies a collapse of the polynomial hierarchy. In fact, we show that an existence of a weaker object implies a somewhat stronger statement. In addition, we extend the result of [GR07] to the case of imperfect statistically secure obfuscator.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/085"><span class="datestr">at June 21, 2021 04:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2146860754670937912">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html">Collusion Rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i>Grigory Yaroslavtsev now maintains the <a href="https://docs.google.com/spreadsheets/d/1kobGLgx5QgJb9AvDENg5c8wEbwBUUkVlMTsBV9quEEY/edit?usp=sharing">Theory Jobs Spreadsheet</a>, a crowd-sourced listing of where your favorite CS theorists have taken new positions. Read his <a href="http://grigory.us/blog/theory-jobs-2021/">blog post</a> for more info.</i></p><p><i>Michael Littman wrote a <a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">CACM Viewpoint</a> on collusion rings designed to tilt the odds into getting papers into conferences. See also <a href="https://medium.com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d">this post</a> by T. N. Vijaykumar. I started to write a post on the topic but couldn't get it going so I asked the AI algorithm GPT-3 to write a guest post. Here is its story.</i></p><hr /><p>I have been part of a collusion ring and I know how it feels. I was a postdoc at the University of California, Berkeley, in the early 2000s when I first learned about the practice of “paper rings.” A paper ring is a group of researchers who agree to submit each other’s papers to conferences without disclosing this fact to the conference organizers. The idea is that if you have several submissions from the same conference, your chances of getting accepted are higher. The pressure to be part of such a ring can be immense. You are told that you need to publish or perish and that you need to get papers into top conferences. If you don’t play along, you will be left behind. I was lucky enough to have a mentor who warned me against being part of such rings. He told me that if I did not want my name on a paper, then I should not submit it to a conference. He also told me that if I did not want to be part of a paper ring, then I should not submit papers for others in the ring. He said that he would help me with my own research and that he would help me get my own papers into conferences. He was right: My own papers were accepted into conferences and they were published in journals.</p><p>I am now an associate professor at the University of Massachusetts Amherst and I have been on the other side of this issue as well. In 2012, I was asked by an editor at a top computer science conference to review a paper submitted by one of my students. The student had been part of a paper ring and had submitted his own paper along with those submitted by his collaborators in the ring. The editor told me that he had rejected the student’s paper because it was too similar to other papers submitted by members of the ring. However, he wanted me to review it so that he could reconsider accepting it for publication if I recommended doing so.</p><p>What should I do? Should I tell him about the paper ring? Should I recommend accepting the paper? Should I tell him that there is no way he should accept it? Should I tell him that he should reject it but then publish it anyway? What would you do? I decided not to tell him about the paper ring and not to recommend accepting the paper for publication. Instead, I told him that he should reject it but then publish it anyway because this would send a strong message about what we expect from our students and what we think about such practices. The editor agreed with my decision and published the student’s paper along with two other papers submitted by members of the same paper ring.</p><p>The student received his PhD degree but did not get any credit for his work on these three papers because they were considered as part of his dissertation work when he defended his thesis later that year. In retrospect, I think that my decision was wrong because it allowed these students to continue their unethical behavior without any consequences for them or their advisors (who were also part of this ring). But at least in this case, there were no negative consequences for anyone involved in this incident except for these students who did not get credit for their work on these three papers because they were considered as part of their dissertation work when they defended their thesis later that year.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/collusion-rings.html"><span class="datestr">at June 21, 2021 02:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/06/21/postdoc-at-bar-ilan-university-apply-by-july-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/06/21/postdoc-at-bar-ilan-university-apply-by-july-15-2021/">postdoc at Bar-Ilan University (apply by July 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position is available at the Network Computation group<br />
at Bar-Ilan University (Israel). We invite applicants interested in the fields of distributed, online, and approximated computing on graph and network problems; Algorithms in hardware; circuit complexity; communication complexity; coding theory and distributed coding schemes.</p>
<p>Website: <a href="https://docs.google.com/document/d/1rCXAByE0J8VDLSXXq4p5a6LDVkBBRaRBrRQhOiCuo7U/edit">https://docs.google.com/document/d/1rCXAByE0J8VDLSXXq4p5a6LDVkBBRaRBrRQhOiCuo7U/edit</a><br />
Email: moti.medina@biu.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/06/21/postdoc-at-bar-ilan-university-apply-by-july-15-2021/"><span class="datestr">at June 21, 2021 12:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/084">TR21-084 |  PCPs and Instance Compression from a Cryptographic Lens | 

	Ron Rothblum, 

	Liron Bronfman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Modern cryptography fundamentally relies on the assumption that the adversary trying to break the scheme is computationally bounded. This assumption lets us construct cryptographic protocols and primitives that are known to be impossible otherwise. In this work we explore the effect of bounding the adversary's power in other information theoretic proof-systems and show how to use this assumption to bypass impossibility results.

We first consider the question of constructing succinct PCPs. These are PCPs whose length is polynomial only in the length of the original NP witness (in contrast to standard PCPs whose length is proportional to the non-deterministic verification time). 
Unfortunately, succinct PCPs are known to be impossible to construct under standard complexity assumptions. Assuming the sub-exponential hardness of the learning with errors (LWE) problem, we construct succinct probabilistically checkable arguments or PCAs (Zimand 2001, Kalai and Raz 2009), which are PCPs in which soundness is guaranteed against efficiently generated false proofs. Our PCA construction is for every NP relation that can be verified by a small-depth circuit (e.g., SAT, clique, TSP, etc.) and in contrast to prior work is publicly verifiable and has constant query complexity. Curiously, we also show, as a proof-of-concept, that such publicly-verifiable PCAs can be used to derive hardness of approximation results.

Second, we consider the notion of Instance Compression (Harnik and Naor, 2006). An instance compression scheme lets one compress, for example, a CNF formula $\varphi$ on $m$ variables and $n \gg m$ clauses to a new formula $\varphi'$ with only $poly(m)$ clauses, so that $\varphi$ is satisfiable if and only if $\varphi'$ is satisfiable. Instance compression has been shown to be closely related to succinct PCPs and is similarly highly unlikely to exist. We introduce a computational analog of instance compression in which we require that if $\varphi$ is unsatisfiable then $\varphi'$ is effectively unsatisfiable, in the sense that it is computationally infeasible to find a satisfying assignment for $\varphi'$ (although such an assignment may exist). Assuming the same sub-exponential LWE assumption, we construct such computational instance compression schemes for every bounded-depth NP relation. As an application, this lets one compress $k$ formulas $\phi_1,\dots,\phi_k$ into a single short formula $\phi$ that is effectively satisfiable if and only if at least one of the original formulas was satisfiable.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/084"><span class="datestr">at June 21, 2021 10:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/">Social at STOC 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I’ve been asked to pass along a message by <a href="https://ccanonne.github.io/">Clément Canonne</a>, social chair for STOC 2021. This STOC might have the best social program of any I’ve ever seen, either virtual or in-person, so be sure to check it out!</p>



<hr class="wp-block-separator" />



<p>STOC’21 is around the corner, starting tomorrow; [don’t forget to register](<a href="http://acm-stoc.org/stoc2021/">http://acm-stoc.org/stoc2021/</a>), if you haven’t yet! This year, the (virtual) conference will include several social activities (games, TCS trivia, mystery hunt…); among which, two “junior/senior lunches,” on Monday and Friday.<br /><br />Those both will be held in the Gather space for STOC (<a href="http://acm-stoc.org/stoc2021/venue.html">http://acm-stoc.org/stoc2021/venue.html</a>), and — as in previous years — are the occasion for senior researchers in the field, broadly construed, to have an informal chat with students, postdocs, and junior faculty, answer their questions, discuss their research, and generally have a nice conversation.<br /><br />If you are interested, don’t forget to sign up! This is done through the “feedback box” placed on the Information Desk in the Gather space’s Lobby, which gives access to a spreadsheet.<br /><br />Hoping to see you at STOC!</p>



<figure class="wp-block-image size-large"><img src="https://kamathematics.files.wordpress.com/2021/06/image.png?w=778" alt="" class="wp-image-325" /></figure></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/"><span class="datestr">at June 21, 2021 02:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/083">TR21-083 |  Tight Space Complexity of the Coin Problem | 

	Mark Braverman, 

	Sumegha Garg, 

	Or Zamir</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In the coin problem we are asked to distinguish, with probability at least $2/3$, between $n$ $i.i.d.$ coins which are heads with probability $\frac{1}{2}+\beta$ from ones which are heads with probability $\frac{1}{2}-\beta$. We are interested in the space complexity of the coin problem, corresponding to the width of a read-once branching program solving the problem. 

The coin problem becomes more difficult as $\beta$ becomes smaller. Statistically, it can be solved whenever $\beta = \Omega(n^{-1/2})$, using counting. It has been previously shown that for $\beta = O(n^{-1/2})$, counting is essentially optimal (equivalently, width $poly(n)$ is necessary [Braverman-Garg-Woodruff FOCS'20]). On the other hand, the coin problem only requires $O(\log n)$ width for $\beta&gt;n^{-c}$ for any constant $c&gt;\log_2(\sqrt{5}-1)\approx 0.306$ (following low-width simulation of AND-OR tree of [Valiant Journal of Algorithms'84]).

In this paper, we close the gap between the bounds, showing a tight threshold between the values of $\beta=n^{-c}$ where $O(\log n)$ width suffices and the regime where $poly(n)$ width is needed, with a transition at $c=1/3$. This gives a complete characterization (up to constant factors) of the memory complexity of solving the coin problem, for all values of bias $\beta$. 

We introduce new techniques in both bounds. For the upper bound, we give a construction based on recursive majority that does not require a memory stack of size $\log n$ bits. For the lower bound, we introduce new combinatorial techniques for analyzing progression of the success probabilities in read-once branching programs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/083"><span class="datestr">at June 20, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18926">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">The Shape of This Summer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Looking farther than Father’s Day</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/ellenbergbucksdec2019/" rel="attachment wp-att-18928"><img width="127" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/EllenbergBucksDec2019.jpg?resize=127%2C170&amp;ssl=1" class="alignright wp-image-18928" height="170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://twitter.com/JSEllenberg/status/1209913559720837121">src</a></font></td>
</tr>
</tbody>
</table>
<p>Jordan Ellenberg is the author of the new <a href="https://www.penguinrandomhouse.com/books/612131/shape-by-jordan-ellenberg/9781984879059/">book</a> <em>Shape: The Hidden Geometry of Information, Biology, Strategy, Democracy, and Everything Else</em>. I just received it as a Father’s Day present.</p>
<p>
Today we convey some short musings on some ideas in the book.</p>
<p>
Ellenberg is both a biological father and a <em>Doktorvater</em>. Indeed, if we make a correct inference from his Mathematics Genealogy <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=31096">count</a> of “12 students and 13 descendants,” he must be a doctor-grandfather. His own advisor was Barry Mazur. Ellenberg holds the John D. MacArthur professorship in Mathematics at UW-Madison and a distinguished professorship named for former Wisconsin senator William Vilas. (We’ve had UW-Madison on our minds for over a month about something else, but various events including new kinds of chess-cheating cases have delayed the work needed to do it justice. While writing this paragraph, I have had a seconding role in a disqualification by an online chess platform of a player in a tournament played today.)</p>
<p>
I made my inference in the last paragraph by counting—I did not take time to click on his twelve students to verify which one has a graduated student. What strikes us about the new book is the aspect of making inferences <em>not</em> by counting.</p>
<p>
</p><p></p><h2> Pandemic Phasing </h2><p></p>
<p></p><p>
Our picture above was <a href="https://twitter.com/JSEllenberg/status/1209913559720837121">tweeted</a> by Ellenberg on Christmas Day, 2019, a half-hour before the start of the Milwaukee Bucks <a href="https://www.basketball-reference.com/boxscores/201912250PHI.html">playing</a> at the Philadelphia 76ers. Notice that the seats behind him have not yet filled, as of course they did for a pre-pandemic holiday game. Last night, I had on the terrific Game 7 between the Bucks and the Brooklyn Nets while processing chess data. The stands at the Brooklyn Barclays Center were within 1,000 of full capacity, but there seemed to be some spaced-out sections nearest the court that would have been teeming usually.</p>
<p>
My wife and I ventured out two weeks ago to see the Toronto Blue Jays play in Buffalo’s <a href="https://www.milb.com/buffalo">Sahlen Field</a>, their home until the border with Canada reopens. Our stadium has increased the allowed capacity from 35% that day to 80% now, but our paper noted today that by month’s end it may be the only venue not allowing 100%. Will the return to large close-packed crowds be safe—will there be sufficient “herd immunity”—involves questions raised in two chapters of Ellenberg’s book. Here is a main idea—in my words, because I haven’t had time to actually <em>read</em> the book yet (and while writing this section, it seems I am having a primary role in another impending ban from the same overseas chess event).</p>
<blockquote><p><b> </b> <em> Often the most immediately valuable inferences are made from judging the shapes of curves rather than calculating numerical projections. </em>
</p></blockquote>
<p></p><p>
Here is an example composed from today’s new-cases charts of Florida and the UK from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/floridaukcov2/" rel="attachment wp-att-18929"><img width="352" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/FloridaUKCov2.png?resize=352%2C342&amp;ssl=1" class="aligncenter wp-image-18929" height="342" /></a></p>
<p>
The recent uptick in the UK is said to be caused by the new “delta” strain of the virus. I’ve chosen Florida rather than the whole US for similarity of scale and because Florida began a June upsurge this time a year ago. The US on the whole and most states show a similar fully-downward trend. Of course we hope it stays that way everywhere as the reopening phase continues. </p>
<p>
The point we are making is that if you just go by the numbers—and if you give highest weight to recent trends in those numbers—then your numerical projections can vary wildly. The logic of shape rather than number may be a more stable basis for judgment. This aligns with what we said about Ayanna Howard’s giving primacy to human rules of judgment in designs for robots in our <a href="https://rjlipton.wpcomstaging.com/2021/05/24/acm-athena-lecturer-award/">post</a> a month ago. </p>
<p>
</p><p></p><h2> A Caveat </h2><p></p>
<p></p><p>
There is, however, a caveat that falls in with the opening example of Ellenberg’s chapter 11, “The Terrible Law of Increase”:</p>
<blockquote><p><b> </b> <em> An early judgment based on a seemingly best-fitting shape has maximum potential to go wrong. </em>
</p></blockquote>
<p></p><p>
The example is an over-optimistic projection of Covid-19 cases and fatalities based on an initial expectation of a cubic curve fit. We neither wish to be academically impassive or blaming over the terrible toll which we have all had to endure, but to promote depth and wisdom in spatial thinking in line with the book’s purpose.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How can we best get quick and accurate judgments of what is in store this summer and fall?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/"><span class="datestr">at June 20, 2021 07:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-1664874822018320731">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/06/interview-with-concur-2021-tot-award.html">Interview with CONCUR 2021 ToT Award recipients, Part 1: Rajeev Alur, Thomas Henzinger, Orna Kupferman and Moshe Vardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Last year, the CONCUR conference series inaugurated its Test-of-Time Award, whose purpose is to recognise important achievements in  Concurrency Theory that were published at the CONCUR conference and have  stood the test of time.  This year, the following <a href="https://qonfest2021.lacl.fr/test-of-time.php" target="_blank">four papers</a> were chosen to receive the CONCUR Test-of-Time  Awards for the periods 1994–1997 and 1996–1999 by a jury consisting of Rob van Glabbeek (chair), Luca de Alfaro, Nathalie  Bertrand, Catuscia Palamidessi, and Nobuko Yoshida: </p><ul style="text-align: left;"><li>  David Janin and Igor Walukiewicz. <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F31F028FFA1E754B87E7378032EAD6E7?doi=10.1.1.35.9156&amp;rep=rep1&amp;type=pdf" target="_blank">On the Expressive Completeness of the Propositional mu-Calculus with respect to Monadic Second Order Logic</a>. </li><li>Uwe Nestmann and Benjamin C. Pierce. <a href="http://www.cis.upenn.edu/~bcpierce/papers/choice.ps" target="_blank">Decoding Choice Encodings</a>.</li><li>Ahmed Bouajjani, Javier Esparza, and the late Oded Maler. <a href="http://www-verimag.imag.fr/~maler/Papers/pda.pdf" target="_blank">Reachability Analysis of Pushdown Automata: Application to Model-checking</a>.</li><li>Rajeev Alur, Thomas A. Henzinger, Orna Kupferman, and Moshe Y. Vardi. <a href="https://doi.org/10.1007/BFb0055622" target="_blank">Alternating Refinement Relations</a>. <br /></li></ul><p>Last year, I interviewed the <a href="http://concur2020.forsyte.at/test-of-time.html" target="_blank">CONCUR 2020 Test-of-Time Award recipients</a> and was asked by Javier Esparza (chair of the CONCUR SC) and Ilaria Castellani (outgoing chair of the IFIP WG 1.8 on Concurrency Theory) to do the same with the current batch of awardees. (In passing, let me thank <a href="http://people.rennes.inria.fr/Nathalie.Bertrand/" target="_blank">Nathalie Bertrand</a> and <a href="https://www.imperial.ac.uk/people/n.yoshida" target="_blank">Nobuko Yoshida</a> for their kind help with the interviews!)<br /></p><p>This post is devoted to the interview I conducted via email with <a href="https://www.cis.upenn.edu/~alur/" target="_blank">Rajeev Alur</a>, <a href="http://pub.ist.ac.at/~tah/" target="_blank">Thomas A. Henzinger</a>, <a href="https://www.cs.huji.ac.il/~ornak/" target="_blank">Orna Kupferman</a> and <a href="https://www.cs.rice.edu/~vardi/" target="_blank">Moshe Y. Vardi</a>. Reading the answers I received from that dream team of colleagues was like a masterclass for me and I trust that their thoughts on their award-winning paper will be of interest to many of the readers of this blog. Enjoy!<br /></p><p><b>Luca:</b> You receive the CONCUR ToT Award 2021 for your paper <a href="https://doi.org/10.1007/BFb0055622" target="_blank">Alternating Refinement Relations</a>, which appeared at CONCUR 1998. In that article, you gave what I consider to be a fundamental contribution, namely the introduction of refinement relations for alternating transition systems. Could you briefly explain to our readers what alternating transition systems are? Could you also tell us how you came to study the question addressed in your award-winning article and why you focused on simulation- and trace-based refinement relations? Which of the results in your paper did you find most surprising or challenging? </p><p><b>Answer:</b> When we model a system by a graph, our model abstracts away some details of the system. In particular, even when systems are deterministic, states in the model may have several successors. The nondeterminism introduced in the model often corresponds to different actions taken by the system when it responds to different inputs from its environment. Indeed, a transition in a graph that models a composite system corresponds to a step of the system that may involve some components. Alternating transition systems (ATSs) enable us to model composite systems in more detail. In an ATS, each transition corresponds to a possible move in a game between the components, which are called agents. In each move of the game, all agents choose actions, and the successor state is deterministically determined by all actions. Consequently, ATSs can distinguish between collaborative and adversarial relationships among components in a composite system. For example, the environment is typically viewed adversarially, meaning that a component may be required to meet its specification no matter how the environment behaves. </p><p>In an earlier <a href="https://www.cis.upenn.edu/~alur/Jacm02.pdf" target="_blank">paper</a>, some of us introduced ATSs and Alternating Temporal Logics, which can specify properties of agents in a composite system. The CONCUR 1998 paper provided refinement relations between ATSs which correspond to alternating temporal logics. Refinement is a central issue in a formal approach to the design and analysis of reactive systems. The relation “I refines S '' intuitively means that system S has more behaviors than system I. It is useful to think about S being a specification and I an implementation. Now, if we consider a composite implementation I||E and specification S||E and we want to check that the component I refines the component S, then the traditional refinement preorders are inappropriate, as they allow I to achieve refinement of I||E with respect to S||E by constraining its environment E.  Alternating refinement relations are defined with respect to ATSs that model the interaction among the underlying components, and they enable us to check, for example, that component I has fewer behaviors than component S no matter how component E behaves. They are called “alternating” because refinement may restrict implementation actions but must not restrict environment actions. In other words, refinement may admit fewer system actions but, at the same time, more environment actions. </p><p>It was nice to see how theoretical properties of preorders in the traditional setting are carried over to the game setting, and so are the results known then about the computational price of moving to a game setting. First, the efficiency of the local preorder of simulation with respect to the global preorder of trace containment is maintained. As in the traditional setting, alternating simulation can be checked in polynomial time, whereas alternating trace-containment is much more complex. Second, the branching vs. linear characterizations of the two preorders is preserved: alternating simulation implies alternating trace containment, and the logical characterization of simulation and trace-containment by CTL and LTL, respectively, is carried over to their alternating temporal logics counterparts. The doubly-exponential complexity of alternating trace containment, as opposed to the PSPACE complexity of trace containment, is nicely related to the doubly-exponential complexity of LTL synthesis, as opposed to its PSPACE model-checking complexity,</p><p>   <b>Luca:</b> In your paper, you give logical characterisations of your alternating refinement relations in terms of fragments of alternating temporal logic. Logical characterisations of refinement relations are classic results in our field and I find them very satisfying. Since I teach a number of those results in my courses, I'd be interested in hearing how you would motivate their interest and usefulness to a student or a colleague. What would your "sales pitch" be? </p><p><b>Answer:</b> There is extensive research on the expressive power of different formalisms. Logical characterization of refinement relations tells us something about the distinguishing power of formalisms. For example, while the temporal logic CTL* is more expressive than the temporal logic CTL, the two logics have the same distinguishing power: if you have two systems and can distinguish between them with a CTL* formula (that is, your formula is satisfied only in one of the systems), then you should be able to distinguish between the two systems also with a CTL formula. Moreover, while CTL is not more expressive than LTL, we know that CTL is “more distinguishing” than LTL. These results have to do with the logical characterizations of trace containment and simulation. The distinguishing power of a specification formalism is useful when we compare systems, in particular an implementation and its abstraction: if we know that the properties we care about are specified in some formalism L, and our system refines the abstraction according to a refinement relation in which the satisfaction of specifications in L is preserved, then we can perform verification on the abstraction.</p><p> <b>Luca:</b> I am interested in how research collaborations start, as I like to tell "research-life stories" to PhD students and young researchers of all ages. Could you tell us how you started your collaboration on the award-winning paper? </p><p><b>Answer:</b>Subsets of us were already collaborating on other topics related to reactive models and model checking, and all of us shared a common belief that the field was in need to move from the limited setting of closed systems to a more general setting of open systems, that is, systems that interact with an environment. Open systems occur not only when the environment is fully or partly unknown, but also when a closed system is decomposed into multiple components, each of them representing an open system. To build “openness” into models and specifications as first-class citizens quickly leads to the game-theoretic (or “alternating”) setting. It was this realization and the joint wish to provide a principled and systematic foundation for the modeling and verification of open systems which naturally led to this collaboration.</p><p> <b>Luca:</b> Did any of your subsequent research build explicitly on the results and the techniques you developed in your award-winning paper? Which of your subsequent results on alternating transition systems and their refinement relations do you like best? Is there any result obtained by other researchers that builds on your work and that you like in particular or found surprising? </p><p><b>Answer: </b>Various subsets of us pursued multiple research directions that developed the game-theoretic setting for modeling and verification further, and much remains to be done. Here are two examples. First, the game-theoretic setting and the alternating nature of inputs and outputs are now generally accepted as providing the proper semantic foundation for interface and contract formalisms for component-based design. Second, studying strategic behavior in multi-player games quickly leads to the importance of probabilistic behavior, say in the form of randomized decisions and strategies, of equilibria, when players have non-complementary objectives, and of auctions, when players need to spend resources for decisions. All of these are still very active topics of research in computer-aided verification, and they also form a bridge to the algorithmic game theory community.    </p><p><b>Luca:</b> One can view your work as a bridge between concurrency theory and multi-agent systems. What impact do you think that your work has had on the multi-agent-system community? And what has our community learnt from the work done in the field of multi-agent systems? To your mind, what are the main differences and points of contact in the work done within those communities? </p><p><b>Answer:</b> Modeling interaction in multi-agent systems is of natural interest to planning problems studied in the AI community. In 2002, the <a href="http://www.ifaamas.org/index.html" target="_blank">International Foundation for Autonomous Agents and Multiagent Systems</a> (IFAAMAS) was formed and the annual <a href="https://aamas2021.soton.ac.uk/" target="_blank">International Conference on Autonomous Agents and Multiagent Systems</a> (AAMAS) was launched. The models, logics, and algorithms developed in the concurrency and formal methods communities have had a strong influence on research presented at AAMAS conferences over the past twenty years. Coincidentally, this year our <a href="https://www.cis.upenn.edu/~alur/Jacm02.pdf" target="_blank">paper on Alternating-Time Temporal Logic</a> was chosen for the <a href="http://www.ifaamas.org/award-influential.html#:~:text=IFAAMAS%3A%20Awards%3A%20Influential%20Paper&amp;text=The%20Influential%20Paper%20Award%20seeks,lasting%20contributions%20to%20the%20field." target="_blank">IFAAMAS Influential Paper Award</a>. </p> <b>Luca:</b> What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved? <p><b>Answer:</b>Research on formal verification and synthesis, including our paper, assumes that the model of the system is known. Reinforcement learning has emerged as a promising approach to the design of policies in scenarios where the model is not known and has to be learned by agents by exploration. This leads to an opportunity for research at the intersection of reactive synthesis and reinforcement learning. A potentially promising direction is to consider reinforcement learning for systems with multiple agents with both cooperative and adversarial interactions. </p><p>The realization that reactive systems have to satisfy their specifications in all environments has led to extensive research relating formal methods with game theory. Our paper added alternation to refinement relations. The transition from one to multiple players has been studied in computer science in several other contexts. For the basic problem of reachability in graphs, it amounts to moving from reachability to alternating reachability. We recently studied this shift in other fundamental graph problems, like the generation of weighted spanning trees, flows in networks, vertex covers, and more. In all these extensions, we consider a game between two players that take turns in jointly generating the outcome. One player aims at maximizing the value of the outcome (e.g., maximize the weight of the spanning tree, the amount of flow that travels in the network, or the size of the vertex cover), whereas the second aims at minimizing the value. It is interesting to see how some fundamental properties of graph algorithms are lost in the alternating setting. For example, following a greedy strategy is not beneficial in alternating spanning trees, optimal strategies in alternating flow networks may use fractional flows, and while the vertex-cover problem is NP-complete, an optimal strategy for the maximizer player can be found in polynomial time. Many more questions in this setting are still open. </p><p><b>Luca:</b> What advice would you give to a young researcher who is keen to start working on topics related to alternating transition systems and logics? </p><b>Answer:</b> One important piece of advice to young researchers is to question the orthodoxy. Sometimes it is necessary to learn everything that is known about a topic but then take a step back, look at the bigger picture, reexamine some of the fundamental assumptions behind the established ways of thinking, change the models that everyone has been using, and go beyond the incremental improvement of previous results. This is particularly true in formal methods, where no single model or approach fits everything. And young researchers stand a much better chance of having a really fresh new thought than those who have been at it for many years.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/06/interview-with-concur-2021-tot-award.html"><span class="datestr">at June 18, 2021 04:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5861">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/quest-for-adaptivity/">The quest for adaptivity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Most machine learning classes and textbooks mention that there is no universal supervised learning algorithm that can do reasonably well on all learning problems. Indeed, a series of “no free lunch theorems” state that even in a simple input space, for any learning algorithm, there always exists a bad conditional distribution of outputs given inputs where this algorithm performs arbitrarily bad. </p>



<p class="justify-text">For example, from the classic book of Luc Devroye, László Györfi, and Gábor Lugosi [<a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">1</a>, Theorem 7.2 and its extensions], for inputs uniformly distributed in \([0,1]\), for any decreasing sequence \((\varepsilon_n)_{n \geqslant 0}\) which is less than 1/16, for any learning algorithm (which takes pairs of observations and outputs a prediction function), there exists a conditional distribution on \(\{-1,1\}\) for which the expected risk of the classifier learned from \(n\) independent and identically distributed observations of (input,output) pairs is greater than \(\varepsilon_n\) for all \(n \geqslant 1\), while the best possible expected error rate is zero.</p>



<p class="justify-text">Such theorems do not imply that all learning methods are equally bad, but rather that all learning methods will suffer from some weaknessess. Throughout this blog we will try to better understand the weaknessess and strengths of popular methods through learning theory.</p>



<p class="justify-text">The key is to control the potential weaknesses of a learning method by making sure that in “favorable” scenarios, it leads to strong guarantees. When taking the simplest example of vectorial inputs in \(\mathbb{R}^d\), we can construct model classes of increasing complexity for which we can start to draw useful comparisons between learning methods.</p>



<p class="justify-text">Several aspects of the joint distribution of (input,output) \((X,Y)\) make the problem easy or hard. For concreteness and simplicity, I will focus on regression problems where the output space is \(\mathcal{Y} = \mathbb{R}\) and with the square loss, so that the optimal function, the “target” function, is \(f^\ast(x) = \mathbb{E}(Y|X=x)\). But much of the discussion extends to classification or even more complex outputs (see, e.g., [1]).</p>



<p class="justify-text"><strong>Curse of dimensionality.</strong> In the context of vectorial inputs, the slowness of universal learning algorithms can be characterized more precisely, and leads to the classical <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>. Only assuming that the optimal target function is Lipschitz-continuous, that is, for all \(x,x’\), \(| f^\ast(x)-f^\ast(x’)| \leqslant  L \| x – x’\|\) (for any arbitrary norm on \(\mathbb{R}^d\)), the optimal excess risk of a prediction function \(\hat{f}\) obtained from \(n\) observations, that is, the expected squared difference between \(f^\ast(x)\) and \(\hat{f}(x)\) cannot be less than a constant times \(n^{-2/(d+2)}\), that is, in order for this rate to be smaller than some \(\varepsilon &lt;1 \), we need \(n\) to be larger than \(\displaystyle ( {1}/{\varepsilon} )^{d/2+1}\), with thus an exponential dependence in \(d\) (see [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>]).</p>



<p class="justify-text">In other words, exponentially many observations are needed for a reasonable performance on all problems with a minimal set of assumptions (here Lipschitz-continuity), and this bad behavior is unavoidable unless extra assumptions are added, which we now describe.</p>



<h2>Support, smoothness and latent variables</h2>



<p class="justify-text"><strong>Low-dimensional support. </strong>If the data occupy only a \(r\)-dimensional subspace of \(\mathbb{R}^d\), with \(r \leqslant d\) (and typically much smaller), then one should expect a better convergence rate. This also extends to data supported on a (smooth) <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>, as illustrated below. Note that this assumption does not concern the outputs, and can reasonably be checked given the data by performing principal component analysis or some form of <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">manifold learning</a>. Essentially, in terms of convergence rates, \(d\) is replaced by \(r\). This is obvious if the learning algorithm has access to the \(r\)-dimensional representation, and it requires more work if not.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="508" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/support.png" class="wp-image-5959" height="225" />Left: samples from the uniform distribution in the unit disk in \(\mathbb{R}^2\). Right: samples from the uniform distribution in the unit circle in \(\mathbb{R}^2\), which is a one-dimensional smooth manifold.</figure></div>



<p class="justify-text"><strong>Smoothness of the target function. </strong>This is done by assuming some bounded derivatives for the target function \(f^\ast\). With bounded \(s\)-th order derivatives, we could expect that the problem is easier (note that Lipschitz-continuity corresponds to \(s=1\)). This is illustrated below with one-dimensional inputs. Essentially, in terms of rates, \(d\) is replaced by \(d/s\) [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>, Theorem 3.2]. Therefore, when the smoothness order \(s\) is of order \(d\), the dependence in the dimension disappears.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="517" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/smooth-1024x445.png" class="wp-image-5961" height="225" />Regression problem with one-dimensional inputs, with target function in red, and observations in blue. Left: non-smooth (even not Lipschitz-continuous) function, right: smooth function.</figure></div>



<p class="justify-text"><strong>Latent variables.</strong> If we assume that the target function depends only on a \(r\)-dimensional linear projection of the input, then we should expect a better complexity. The most classical example is the dependence on a subset of the \(d\) original variables. Essentially, in terms of rates, \(d\) is replaced by \(r\). This is obvious when the latent variables are known (as we can replace input data by the \(r\) latent variables), totally not otherwise, as this requires some form of <em>adaptivity</em> (see below).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="553" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/latent-1024x467.png" class="wp-image-5964" height="252" />Regression problem with two-dimensional inputs, with target function (with no noise). Left:  function that cannot be written with a latent variable, right: function that has a (linear) latent variable representation (there exists a direction with no variation).</figure></div>



<p class="justify-text"><strong>Need for adaptivity.</strong> We typically don’t know in advance if these properties are satisfied or not, as some are easily testable (support of distribution) without knowing the target function \(f^\ast\), while others are not.</p>



<p class="justify-text">The goal is to have a single method that can adapt to all of these situations (which are non-exclusive). That is, if the problem has any of these reasons to be easy, will the learning method benefit from it? Typically, most learning methods have at least one hyperparameter controlling overfitting (e.g., a regularization parameter), and the precise value of this hyperparameter will depend on the difficulty of the problem, and we will assume that we have a reasonable way to estimate this hyperparameter (e.g., cross-validation). A method is then said adaptive if with a well chosen value of the hyperparameter, we get the optimal (or close to optimal) rate of estimation that benefits from the extra assumption.</p>



<p class="justify-text"><strong>Quest for adaptivity: who wins?</strong> Among classical learning techniques, which ones are adaptive to which properties? In short, <em>barring computational and optimization issues</em>: $$\mbox{ local averaging } &lt; \mbox{ positive definite kernels } &lt; \mbox{ neural networks }.$$ Every time, the next method in the list gains adaptivity to support, smoothness and then latent variables. <span class="has-inline-color has-vivid-red-color">Note however that optimization for neural networks is more delicate (see below).</span></p>



<p class="justify-text">Let’s now briefly look at these methods one by one. I will assume basic knowledge of these, for more details see [<a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">4</a>, <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">5</a>] or my <a href="https://francisbach.com/i-am-writing-a-book/">new book in preparation</a>.</p>



<h2>Local averaging</h2>



<p class="justify-text">The earliest and simplest learning methods that could adapt to any target functions were <em>local averaging</em> methods aiming at approximating directly the conditional expectation \(f^\ast(x) = \mathbb{E}(Y|X=x)\), with the most classical examples being <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><em>k</em>-nearest neighbors</a> and <a href="https://en.wikipedia.org/wiki/Kernel_regression">Nadaraya-Watson</a> estimators.</p>



<p class="justify-text">These methods are naturally adaptive to having a reduced support for inputs. Indeed, in the simplest case of a distribution supported in a low-dimensional subspace, the global metric is equivalent to a local metric on the support, without any need to explicitly know the subspace, a situation which extends to smooth manifolds. In order to be adaptive, the hyperparameter has to depend on the dimension \(r\) of the manifold, e.g., for \(k\)-nearest-neighbors, \(k\) has to be taken proportional to \(n^{2/(2+r)}\) (see [<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf">3</a>]).</p>



<p class="justify-text">However, there is no adaptivity to the smoothness of the target function or to potential latent variables unless dedicated algorithms are used such as <a href="https://en.wikipedia.org/wiki/Local_regression">local regression</a>. Kernel methods and neural networks lead to such adaptivity.</p>



<h2>From kernels to neural networks</h2>



<p class="justify-text">We consider prediction functions of the form $$f(x) = \sum_{j=1}^m a_j ( b_j^\top x )_+,$$ which is the traditional single hidden layer fully connected neural network with ReLU activation functions (a constant term can be added to the linear term within the ReLU by simply appending \(1\) to \(x\), please do not call these terms “biases” as this has <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">another meaning in statistics</a>).</p>



<p class="justify-text">The vector \(a \in \mathbb{R}^m\) represents <em>output weights</em>, while the matrix \(b \in \mathbb{R}^{m \times d}\) represents <em>input weights</em>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="564" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/nn_blog-1024x546.png" class="wp-image-5975" height="300" /></figure></div>



<p class="justify-text"><strong>Empirical risk minimization (ERM). </strong>We assume given \(n\) i.i.d. observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathbb{R}^d \times \mathbb{R}\), and we will fit models by minimizing the \(\ell_2\)-regularized empirical risk (you can call it “weight decay” but it  already has a better name in machine learning and statistics). That is, we minimize $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ where \(\lambda &gt; 0\) is a regularization parameter.</p>



<p class="justify-text"><strong>Overparametrization. </strong>We will consider the limit of large number \(m\) of hidden neurons. Depending whether we optimize over both input weights \(b\) and output weights \(a\), or simply output weights (with then a proper initialization of the input weights), we get different behaviors.</p>



<h2>Kernel regime</h2>



<p class="justify-text">We assume that the input weights \(b_j \in \mathbb{R}^d\) are sampled uniformly from the Euclidean sphere of radius \(1 / \sqrt{m}\), and that we only optimize over the output weights \(a_j, j = 1,\dots,m\). This is exactly a <a href="https://en.wikipedia.org/wiki/Ridge_regression">ridge regression</a> problem (square loss and squared Euclidean penalty), for which the theory of positive definite kernels applies and will lead to an interesting behavior for infinite \(m\) [7, <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>]. That is, the solution can be obtained by using the kernel function \(\hat{k}\) defined as $$\hat{k}(x,x’) = \sum_{j=1}^m ( b_j^\top x )_+( b_j^\top x’ )_+,$$ and looking for prediction functions of the form $$f(x) = \sum_{i=1}^n \alpha_i \hat{k}(x,x_i).$$ See, e.g., [6] for details. In particular, as long as \(\hat{k}(x,x’)\) can be computed efficiently, the complexity of solving the ERM problem is independent of the number of neurons \(m\). We will now discuss in further detail the over-parameterized case where \(m=\infty\).</p>



<p class="justify-text">When \(m\) tends to infinity, If the input weights are fixed and initialized randomly from the \(\ell_2\)-sphere of radius \(1/\sqrt{m}\), then by the law of large numbers, \(\hat{k}(x,x’)\) tends to $$k(x,x’) = \mathbb{E}_{b \sim {\rm uniform} (\mathbb{S}^{d-1}) } (b^\top x )_+ (b^\top x’)_+,$$ where \(\mathbb{S}^{d-1}\) is the unit \(\ell_2\)-sphere in \(d\) dimensions. This kernel has a closed form and happens to be equal to (see [<a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">9</a>]) $$ \frac{1}{2\pi d}  \|x\|_2   \| x’\|_2 \big[ ( \pi \, – \varphi) \cos \varphi + \sin \varphi \big],$$ where \(\cos \varphi = \frac{ x^\top x’  }{{ \|x\|_2  } { \| x’\|_2   } }\). It can also be seen (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] for details) as using predictors of the form $$f(x) = \int_{\mathbb{S}^{d-1}}  ( b^\top x)_+ d \mu(b),$$ for some measure \(\mu\) on \(\mathbb{S}^{d-1}\), with the penalty $$\frac{\lambda}{2} \int_{\mathbb{S}^{d-1}} \big| \frac{d\mu}{d\tau}(b) \big|^2 d\tau(b),$$ for the uniform probability measure \(d\tau\) on the hypersphere \(\mathbb{S}^{d-1}\). This representation will be useful for the comparison with neural networks.</p>



<p class="justify-text">Note that here we use random features in a different way from their common use [<a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>], where the kernel \(k\) comes first and is approximated by \(\hat{k}\) to obtain fast algorithms, typically with \(m &lt; n\). Here we start from \(\hat{k}\) and find the limiting \(k\) to understand the behavior of overparameterization. The resulting function space is a (<a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev</a>) space of functions on the sphere with all \(s\)-th order derivatives which are square-integrable, with \(s = d/2+3/2\) [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>].</p>



<p>Finally, the number of neurons \(m\) needed to reach the kernel regime is well understood, at least in simple situations [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>, <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">16</a>].</p>



<p class="justify-text">We can now look at the various forms of adaptivity of kernel methods based on Sobolev spaces.</p>



<p class="justify-text"><strong>Adaptivity to reduced support.</strong> Like model averaging techniques, adaptivity to input data supported on a subspace is rather straightforward, and it extends to smooth manifolds (see, e.g., [<a href="https://arxiv.org/pdf/2003.06202.pdf">12</a>] for a proof for the Gaussian kernel).</p>



<p class="justify-text"><strong>Adaptive to smoothness.</strong> A key attractive feature of kernel methods is that they can circumvent the curse of dimensionality for <em>smooth</em> target functions, and, by simply ajusting the regularization parameter \(\lambda\), ridge regression will typically adapt to the smoothness of the target function, and thus benefit from easy problems.</p>



<p class="justify-text">The simplest instance is when the target function is within the space of functions defined above, where we immediately get estimation rates which are independent of dimension (at least in the exponent). This however requires at least \(s&gt;d/2\) derivatives (because Sobolev spaces are <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert spaces</a> (RKHS) only in this situation), but the adaptivity extends to functions outside of the RKHS (see, e.g., [11]).</p>



<p class="justify-text"><strong>Adaptivity to linear latent variables.</strong> Unfortunately, kernel methods are not adaptive even to basic linear structures. That is, if \(f^\ast\) depends only on the first component of \(x\), then ridge regression, if not modified, will not take advantage of it. In the context of neural networks, one striking example is that a single neuron \(x \mapsto (b^\top x)_+\) <em>does not belong to the RKHS</em>, and leads to bad estimation rates (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]).</p>



<p class="justify-text">Before looking at some experiments, let’s look at some optimization considerations, that will be important later for neural networks.</p>



<p class="justify-text"><strong>Avoiding overparameterized neuron representations.</strong> In the kernel regime, optimizing directly the cost function with \(m\) neurons by gradient descent is problematic for two reasons when \(m\) is large:</p>



<ul class="justify-text"><li>The optimization problem, although convex, is severely ill-conditioned as can be seen from the spectrum of the covariance matrix of the feature vector, with the \(i\)-th eigenvalue equivalent to \(i^{-1-3/d}\) for the uniform input distribution (see [14, Section 2.3]). Therefore, (stochastic) gradient descent will take a lot of time to converge.</li><li>We can use kernels directly (for \(m = +\infty\)) to avoid using \(m\) very large (which would be useless in practice). The running-time complexity is then of the order \(O(n^2)\) if done naively, with lots of ways to go below \(O(n^2)\), such as column sampling (see a nice analysis in [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>]).</li></ul>



<p class="justify-text"><strong>Experiments.</strong> We first consider a very simple one-dimensional example, where we look at how the estimation with the kernel function \(k\) varies as a function of the hyperparameter \(\lambda\), from underfitting to overfitting. We can observe that all learned functions are smooth, as expected.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="574" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/rff_biplot.gif" class="wp-image-6045" height="252" />Regression problems in one dimension with kernels (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, the kernel method adapts to it.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="925" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_rff_subplots.png" class="wp-image-6042" height="241" />Left: estimation with the optimal regularization parameter for smooth target function and kernel methods. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure></div>



<h2>Neural networks</h2>



<p class="justify-text">We can now optimize over all weights, output \(a\) <em>and</em> input \(b\). Four natural questions come to mind:</p>



<ol class="justify-text"><li>Where does it converge to when the number \(m\) of hidden neurons tends to infinity?</li><li>What are the adaptivity properties of the resulting predictor?</li><li>How big \(m\) needs to be to achieve the infinite width behavior?</li><li>Can we actually solve the non-convex optimization problem?</li></ol>



<p class="justify-text"><strong>Overparameterization limit (feature learning regime).</strong> When optimizing over both sets of weights, we can first note that the prediction function is invariant by the change of variable $$ a_j \leftarrow \mu_j a_j \ , \ \ b_j \leftarrow \mu_j^{-1} b_j.$$ Then by optimizing over \(\mu_j\) which is only involved in the penalty term \(\frac{\lambda}{2} \big( a_j^2 + \|b_j\|_2^2 \big)\), we get \(\mu_j^\ast = a_j^{-2} \| b_j \|_2^2\), and the penalty \(\lambda |a_j| \| b_j\|_2\). We thus get the equivalent optimization problem of minimizing $$\tilde{R}(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + {\lambda} \sum_{j=1}^m |a_j| \| b_j\|_2 .$$ By restricting the \(b_j\) on the unit sphere (which is OK because by optimizing over \(\mu_j\) we become scale-invariant), we can write $$\sum_{j=1}^m a_j ( b_j^\top x_i )_+ = \int_{\mathbb{S}^{d-1}} (b^\top x)_+ d\mu(b), $$ for $$ \mu = \sum_{j=1}^m a_j \delta_{b_j}$$ a weighted sum of Diracs, and \(\sum_{j=1}^m |a_j| \| b_j\|_2 = \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|\) the <a href="https://en.wikipedia.org/wiki/Total_variation">total variation</a> of \(\mu\). Thus, letting \(m\) go to infinity, the infinite sums become integrals of general measures, and we end up considering the set of functions that can be written as (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] and the many references therein for details): $$f(x) = \int_{\mathbb{S}^{d-1}}   ( b^\top x)_+ d \mu(b),$$ with the penalty $$\lambda \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|.$$ It has an \(\ell_1\)-norm flavor (as opposed to the \(\ell_2\)-norm for kernels), and leads to further adaptivity, if optimization problems are resolved (which is itself not easy in polynomial time, see below).</p>



<p class="justify-text">The minimal number of neurons to achieve the limiting behavior in terms of predictive performance (assuming optimization problems are resolved) can also be characterized, and grows exponentially in dimension without strong assumptions like the ones made in this blog post (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). We will now study the adaptivity properties of using the function space above.</p>



<p class="justify-text"><strong>Adaptive to low-dimensional support and smoothness.</strong> It turns out that the adaptivity properties of kernel methods are preserved, both with respect to the support and the smoothness of the target function (see, e.g., [17, <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). However, neural networks can do better!</p>



<p class="justify-text"><strong>Adaptive to linear substructures for one hidden layer.</strong> Given that the training of neural networks involves finding the best possible input weights, which are involved in the first linear layer, it is no surprise that we obtain adaptivity to linear latent variables. That is, if we can write \(f^\ast(x) = g(c_1^\top x, \dots, c_r^\top x)\) for some function \(g: \mathbb{R}^r \to \mathbb{R}\), then the vectors \(c_j\)’s will be essentially estimated among the \(b_j\)’s by the optimization algorithm. Therefore, when using the \(\ell_1\)-based function space, the convergence rate in the excess risk will depend on \(r\) and not \(d\) in the exponent [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]. In particular, neural networks can perform <em>non-linear</em> variable selection (while the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> only performs <em>linear</em> variable selection). See a nice experiment in [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>] for binary classification.</p>



<p class="justify-text">One could imagine that with more hidden layers, this extends to non-linear smooth projections of the data, that is, to cases where we assume that \(f^\ast(x) = g(h(x))\) where \(h: \mathbb{R}^d \to \mathbb{R}^r\) is a smooth function. </p>



<p class="justify-text">Thus, we obtain a stronger adaptivity for infinite \(m\) and the good choice of regularization parameter \(\lambda\). We could then try to reproduce for neural networks the figures obtained for kernel methods with varying \(\lambda\). Unfortunately, this is where non-convex optimization will make everything harder.</p>



<h2>Overfitting with a single hidden layer is hard!</h2>



<p class="justify-text"><strong>Global convergence of gradient flow for infinite width. </strong>The traditional algorithm to minimize the empirical risk is gradient descent and its stochastic extensions. In this blog post, we consider gradient descent with small step-sizes, which can be approximated by a gradient flow (as explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>). All parameters are randomly initialized, and we consider \(b_j\) uniformly distributed on the sphere of radius \(1/\sqrt{m}\), and \(a_j\) uniformly distributed in \(\{-1/\sqrt{m},1/\sqrt{m}\}\) (this is essentially equivalent to the traditional “Glorot” initialization [<a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">18</a>]).</p>



<p class="justify-text">This corresponds to the “mean-field” scaling of initialization where neurons from both layers move (see more details in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last </a><a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/">July</a><a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/"> blog post</a> for other scalings). As shown in a joint work with Lénaïc Chizat [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>] and explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>, when \(m\) tends to infinity, then the gradient flow can only converge to the global minimum of the objective function, which is a non-trivial result because the cost function is not convex in \((a,b)\). </p>



<p class="justify-text"><strong>Apparent good properties for small \(m\).</strong> A key property which is not yet well understood is that the global convergence behavior can be observed for \(m\) relatively small. For example, considering the one-dimensional regression example below, where the target function is the combination of 5 hidden neurons, with \(m=32\) hidden neurons, it is already possible to learn a good function with high probability (this would not be the case with \(m=5\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="319" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_triangular_noisy_32.png" class="wp-image-6048" height="292" />One-dimensional regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">One extra benefit here is that no regularization (e.g., no penalty) seems needed to obtain this good behavior. Therefore it seems that we get the best of both worlds: reasonably small \(m\) (so not too costly algorithmically), and no need to regularize.</p>



<p class="justify-text">However, like in any situation where overfitting is not observed, there is the possibility of <em>underfitting</em>. To study this, let us consider a noiseless problem where \(y\) is a deterministic function of \(x\), and with a sligthly more complicated target function, which now requires 9 hidden neurons.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="327" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_hard_noiseless_8192.png" class="wp-image-6077" height="277" />One-dimensional noiseless regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">Even with \(m = 8192\) hidden neurons, the resulting function is not able to fit the data, which is one extreme form of simplicity bias [<a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">21</a>]. There is thus strong underfitting with a single hidden layer (the situation seems to be different with deeper networks). How can this be alleviated? </p>



<h2>Kernel learning to the rescue</h2>



<p class="justify-text">A simple way to avoid the underfitting phenomenon above (but potentially get overfitting, see below) is to minimize the cost \(R(a,b)\) by leveraging a convex sub-problem, as done in the glorious days of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a>.</p>



<p class="justify-text">In the cost function $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ the problem is convex with respect to all \((a_j)\)’s; moreover, it is a least-squares problems which can be solved in closed form by matrix inversion (with algorithms that are much more robust to ill-conditioning than gradient descent). </p>



<p class="justify-text">Then, if we denote by \(\Phi(b) \in \mathbb{R}^{n \times m}\) the matrix with elements \((b_j^\top x_i)_+\), the cost function can be written as a function of \(a \in \mathbb{R}^m\) as $$R(a,b) = \frac{1}{2n} \| y \, – \Phi(b) a \|_2^2 +\frac{\lambda}{2}\| a\|_2^2 +  \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ It can be minimized in closed form as $$ a = ( \Phi(b)^\top \Phi(b) + n \lambda I)^{-1} \Phi(b)^\top y = \Phi(b)^\top ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y,$$ where the matrix inversion lemma has been used. This leads to: $$S(b) = \inf_{a \in \mathbb{R}^m} R(a,b) = \frac{\lambda}{2} y^\top  ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y + \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ The matrix \(\Phi(b)\Phi(b)^\top \in \mathbb{R}^{n \times n}\) is the traditional kernel matrix of the associated non-linear features. The resulting optimization problem is exactly one used in kernel learning [<a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">20</a>].</p>



<p class="justify-text">Computing \(S(b)\) and its gradient with respect to \(b\) can be done in time \(O(n^2 m)\) when \(m\) is large. Since given the hidden neurons, we get the global optimum, we avoid the underfitting problem as well as the ill-conditioning issues.</p>



<p class="justify-text">When \(m\) tends to infinity, then the optimization problem fits our analysis in [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>], that is, we can see the limiting flow as a Wasserstein gradient flow that can only converge to the global optimum of the associated cost function although the overall problem is not convex (as opposed to what happens in multiple kernel learning described in this <a href="https://francisbach.com/the-eta-trick-reloaded-multiple-kernel-learning/">older post</a>). While there is currently no further quantitative theoretical evidence that a good optimization behavior can be reached for a smaller \(m\), it empirically solves the underfitting issue above, as shown below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="607" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/two_flows_hard_noiseless_128.png" class="wp-image-6050" height="254" />Comparison of the the two flows. Right: regular gradient flow on both sets of weights (normal training), left: gradient flow on the cost function where the output weights are maximized out.</figure></div>



<p>We can now perform experiments with varying \(\lambda\)’s.</p>



<p class="justify-text"><strong>Experiments. </strong>The two plots below depict the learnt functions for a smooth and non-smooth objective as \(\lambda\) is varied, highlighting the importance of regularization.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="592" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/nn_biplot.gif" class="wp-image-6046" height="261" />Regression problems in one dimension with neural networks (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, neural networks adapt to it. For adaptivity to linear latent variables, see [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>].</p>



<figure class="wp-block-image size-full justify-text"><img width="1833" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_nn_subplots.png" class="wp-image-6043" height="480" />Left: estimation with the optimal regularization parameter for smooth target function and neural networks. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure>



<h2>Conclusion</h2>



<p class="justify-text">As I tried to show in this blog post, adaptivity is a key driver of good predictive performance of learning methods. It turns out that for the three classes of methods I considered, the more complex algorithms led to more adaptivity: no optimization for local averaging, finite-dimensional convex optimization for kernel methods and then non-convex optimization for neural networks (or equivalently infinite-dimensional convex optimization).</p>



<p class="justify-text">While neural networks led to more adaptivity, they are still not totally well understood, in particular in terms of the various biases that their training implies, both for shallow and deep networks. This makes a lot of research directions to follow!</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat and Lawrence Stewart for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Luc Devroye, László Györfi, and Gábor Lugosi. <em><a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">A Probabilistic Theory of Pattern Recognition</a></em>,<br />volume 31. Springer Science &amp; Business Media, 1996.<br />[2] László Györfi, Michael Kohler, Adam Krzyżak, Harro Walk<em>.</em> <em><a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">A Distribution-free Theory of Nonparametric Regression</a></em>. New York : Springer, 2002.<br />[3] Samory Kpotufe.<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf"> k-NN regression adapts to local intrinsic dimension</a>. In Advances in Neural Information Processing Systems, 2011.<br />[4] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. <em><a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">Foundations of Machine Learning</a></em>. MIT Press, 2018.<br />[5] Shai Shalev-Shwartz and Shai Ben-David. <em><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a></em>. Cambridge University Press, 2014.<br />[6] Bernhard Schölkopf and Alexander J. Smola. <em>Learning with Kernels</em>. MIT Press, 2001.<br />[7] Radford M. Neal. <a href="https://www.cs.toronto.edu/~radford/ftp/thesis.ps">Bayesian Learning for Neural Networks</a>. PhD thesis, University of Toronto, 1995.<br />[8] Ali Rahimi and Benjamin Recht. <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">Random features for large-scale kernel machines</a>. In Advances in Neural Information Processing Systems, 2008.<br />[9] Youngmin Cho and Lawrence K. Saul. <a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">Kernel methods for deep learning</a>. In Advances in Neural Information Processing Systems, 2009.<br />[10] Francis Bach. <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks</a>. The Journal of Machine Learning Research, 18(1):629–681, 2017.<br />[11] Andreas Christmann, Ingo Steinwart. Support Vector Machines. Springer, 2008.<br />[12] Thomas Hamm, Ingo Steinwart. <a href="https://arxiv.org/pdf/2003.06202.pdf">Adaptive Learning Rates for Support Vector Machines Working on Data with Low Intrinsic Dimension</a>. Technical report, Arxiv 2003.06202, 2021.<br />[13] Lénaïc Chizat, Francis Bach. <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. <em>Proceedings of the Conference on Learning Theory (COLT)</em>, 2020.<br />[14] Francis Bach. <a href="http://jmlr.org/papers/volume18/15-178/15-178.pdf">On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions</a>. <em>Journal of Machine Learning Research</em>, 18(19):1-38, 2017. <br />[15] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. <a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">Less is more: Nyström computational regularization</a>. In Advances in Neural Information Processing Systems, pages 1657–1665, 2015.<br />[16] Alessandro Rudi, Lorenzo Rosasco. <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">Generalization properties of learning with random features</a>. In Advances in Neural Information Processing Systems, 2017.<br />[17] Ryumei Nakada, Masaaki Imaizumi. <a href="https://jmlr.org/papers/volume21/20-002/20-002.pdf">Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality</a>. <em>Journal of Machine Learning Research</em>, 21(174):1−38, 2020.<br />[18] Xavier Glorot, Yoshua Bengio. <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, 2010.<br />[19] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018<br />[20] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, Sayan Mukherjee. <a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">Choosing multiple parameters for support vector machines</a>. <em>Machine Learning</em>, 46(1):131-159, 2002.<br />[21] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli. <a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">The Pitfalls of Simplicity Bias in Neural Networks</a>. In Advances in Neural Information Processing Systems, 2020.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/quest-for-adaptivity/"><span class="datestr">at June 17, 2021 04:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1222818111730421374">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/benny-chor-1956-2021.html">Benny Chor (1956-2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p></p><div style="clear: both; text-align: center;" class="separator"><a style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-MVD3X8JUWlI/YMtAmUG2rnI/AAAAAAAB8VM/GFZrtRW5Y707KiQ4KZOTB3c7vaCVV-kHgCLcBGAsYHQ/s137/benny3.jpg"><img width="118" src="https://1.bp.blogspot.com/-MVD3X8JUWlI/YMtAmUG2rnI/AAAAAAAB8VM/GFZrtRW5Y707KiQ4KZOTB3c7vaCVV-kHgCLcBGAsYHQ/w118-h200/benny3.jpg" border="0" height="200" /></a></div><br />Benny Chor passed away on June 10, 2021. Luca Trevisan had a <a href="https://lucatrevisan.wordpress.com/2021/06/10/benny-chor/">blog post</a> on the news.<p></p><p>We present a guest post on Benny Chor's life and works by Oded Goldreich. The post has many refs to papers. They will appear at the end with pointers to them.</p><hr /><p>Benny Chor was born on December 23rd 1956  and grew-up in Tel-Aviv, Israel. He studied Mathematics in the Hebrew University, receiving a B.Sc. in 1980 and an M.Sc. in 1981. He then switched to studying Computer Science at MIT, and graduated in 1985 with a PhD thesis titled</p><p> <i>Two Issues in Public Key Cryptography -- RSA Bit Security and a New Knapsack Type System</i></p><p>which received an ACM Distinguished Dissertation award. After post-doctoral periods at MIT and Harvard, he took a faculty position at the Computer Science Department of the Technion (1987-2001), and then at Tel-Aviv University, where he served as the chairman of the department for two years. He died on June 10th 2021, from a terminal disease.</p><p>Although Benny was a very articulated and verbal person, I find it impossible to describe his personality in words. The point is that words cannot capture the experience of interacting with him, which was always sheer fun. Still, I guess I should say something personal as a close friend of his. So I will just mention that he lived happily with Metsada, whom he met in the summer of 1981, and that they lovingly raised three kids: Arnon, Omer and Aya. Actually, let me also mention his life-long close relationship with his brother, Kobi.</p><p>Focusing on his contributions to science, I will confine myself to the areas of cryptography and randomized computation. Still, let me mention that in the mid 1990s, Benny's research interests gradually shifted to computational biology, but I will not review his contributions to that area, since it is very remote from my own expertise.</p><div><div>In my opinion, Benny's most important contribution to cryptography is the fundamental</div><div>paper on Verifiable Secret Sharing [CGMA].  Loosely speaking, Verifiable Secret Sharing</div><div>is a method to share a secret so that any majority of share-holders may reconstruct the</div><div>secret, whereas no minority may do so, and still each share-holder can verify that the</div><div>share that they got is a valid one.  This primitive had a tremendous impact on the</div><div>development of secure multi-party protocols, and almost all subsequent works in this</div><div>area utilize it.</div><div><br /></div><div>Additional research milestones of Benny, in the area of Cryptography, include the proof</div><div>of the existence of a ``hard-core'' in the RSA and Rabin functions [BCS, ACGS], the</div><div>introduction and design of Private Information Retrieval (PIR) schemes [CGKS] (as well</div><div>as their computational counterparts [CG97]), and the introduction and design of</div><div>Tracing Traitor schemes [CFNP].  Each of these works deserves more than the foregoing</div><div>brief mention, yet I'm not even mentioning other important works like [CK].</div><div><br /></div><div>Turning to the area of Randomized Computation, Benny's contributions span diverse areas</div><div>ranging from the manipulation of randomness to the use of randomness in complexity theory</div><div>and distributed computing.  In particular, his work on weak sources of randomness [CG88]</div><div>identified min-entropy (of the source) as the key parameter for randomness extraction and</div><div>presented a simple two-source extractor.  Distilling an aspect of [ACGS], his work on</div><div>pairwise-independent sampling [CG89] demonstrates the general applicability of this method. His works in distributed computing include a randomized Byzantine Agreement protocol [CC] that beats the deterministic bound without using unproven assumptions.  Lastly, let me just mention a couple of his complexity-theoretic works: [BCGL, CCGHHRR].</div><div><br /></div><div>Benny was a superb teacher and a devoted graduate student adviser.  It is tempting to try</div><div>listing the numerous educational projects that he initiated, and his former students, but</div><div>these lists will be too long and the risk of a crucial omissions is too high.  So let me</div><div>end by expressing the feeling of loss that is shared by many in our community,</div><div>which adds to my personal sorrow.</div></div><div><br /></div><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://1.bp.blogspot.com/-VxGM4xSHMwU/YMtBVdfDCMI/AAAAAAAB8Vo/47D34yqw-pEpjSLuzOk-lvHn6HFOPTyVACLcBGAsYHQ/s634/benny2.jpg"><img width="320" border="0" src="https://1.bp.blogspot.com/-VxGM4xSHMwU/YMtBVdfDCMI/AAAAAAAB8Vo/47D34yqw-pEpjSLuzOk-lvHn6HFOPTyVACLcBGAsYHQ/s320/benny2.jpg" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">Oded and Benny</td></tr></tbody></table><br /><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://1.bp.blogspot.com/-TJNVzF4e9HE/YMtBVWGdtXI/AAAAAAAB8Vs/jvxumCSZxPw-Oqr4cqQGf35n8ds3KLaDwCLcBGAsYHQ/s1415/benny4.jpg"><img width="320" border="0" src="https://1.bp.blogspot.com/-TJNVzF4e9HE/YMtBVWGdtXI/AAAAAAAB8Vs/jvxumCSZxPw-Oqr4cqQGf35n8ds3KLaDwCLcBGAsYHQ/s320/benny4.jpg" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">Benny with his family's namesake (Chor = Bull)</td></tr></tbody></table><br /><div>[ACGS] Werner Alexi, Benny Chor, Oded Goldreich, Claus-Peter Schnorr:</div><div><div>RSA and Rabin Functions: Certain Parts are as Hard as the Whole.</div><div>SIAM J. Comput. 17(2): 194-209 (1988) <a href="https://epubs.siam.org/doi/abs/10.1137/0217013">LINK</a></div><div><br /></div><div>[BCGL] Shai Ben-David, Benny Chor, Oded Goldreich, Michael Luby:</div><div>On the Theory of Average Case Complexity. J. Comput. Syst. Sci. 44(2): 193-219 (1992) <a href="https://www.sciencedirect.com/science/article/pii/002200009290019F">LINK</a></div><div><br /></div><div>[BCS] Michael Ben-Or, Benny Chor, Adi Shamir: On the Cryptographic Security of Single RSA Bits STOC 1983: 421-430. <a href="https://dl.acm.org/doi/10.1145/800061.808773">LINK</a></div><div><br /></div><div>[CCGHHRR] Richard Chang, Benny Chor, Oded Goldreich, Juris Hartmanis, Johan Håstad, Desh Ranjan, Pankaj Rohatgi: The Random Oracle Hypothesis Is False.</div><div>J. Comput. Syst. Sci. 49(1): 24-39 (1994) <a href="https://www.sciencedirect.com/science/article/pii/S0022000005800844">LINK</a></div><div><br /></div><div>[CC] Benny Chor, Brian A. Coan:</div><div>A Simple and Efficient Randomized Byzantine Agreement Algorithm.</div><div>IEEE Trans. Software Eng. 11(6): 531-539 (1985) <a href="https://ieeexplore.ieee.org/document/1702050">LINK</a></div><div><br /></div><div>[CFNP] Benny Chor, Amos Fiat, Moni Naor, Benny Pinkas:</div><div>Tracing traitors. IEEE Trans. Inf. Theory 46(3): 893-910 (2000) <a href="https://ieeexplore.ieee.org/document/841169">LINK</a></div><div><br /></div><div>[CG97] Benny Chor, Niv Gilboa:</div><div>Computationally Private Information Retrieval. STOC 1997: 304-313 <a href="https://dl.acm.org/doi/10.1145/258533.258609">LINK</a></div><div><br /></div><div>[CG88] Benny Chor, Oded Goldreich:</div><div>Unbiased Bits from Sources of Weak Randomness and Probabilistic Communication Complexity.</div><div>SIAM J. Comput. 17(2): 230-261 (1988) <a href="https://epubs.siam.org/doi/10.1137/0217015">LINK</a></div><div><br /></div><div>[CG89] Benny Chor, Oded Goldreich:</div><div>On the power of two-point based sampling. J. Complex. 5(1): 96-106 (1989) <a href="https://doi.org/10.1016/0885-064X(89)90015-0">LINK</a></div><div><br /></div><div>[CGKS] Benny Chor, Oded Goldreich, Eyal Kushilevitz, Madhu Sudan:</div><div>Private Information Retrieval. J. ACM 45(6): 965-981 (1998) <a href="https://doi.org/10.1145/293347.293350">LINK</a></div><div><br /></div><div>[CGMA] Benny Chor, Shafi Goldwasser, Silvio Micali, Baruch Awerbuch:</div><div>Verifiable Secret Sharing and Achieving Simultaneity in the Presence of Faults.</div><div>FOCS 1985: 383-395. <a href="https://doi.org/10.1109/SFCS.1985.64">LINK</a></div></div><div><div><br /></div><div>[CK] Benny Chor, Eyal Kushilevitz:</div><div>A Zero-One Law for Boolean Privacy.  STOC 1989: 62-72. <a href="https://doi.org/10.1145/73007.73013">LINK</a></div><div><br /></div></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/benny-chor-1956-2021.html"><span class="datestr">at June 17, 2021 02:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/082">TR21-082 |  Hardness on any Samplable Distribution Suffices: New Characterizations of One-Way Functions by Meta-Complexity | 

	Rahul Santhanam, 

	Hanlin Ren, 

	Rahul Ilango</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that one-way functions exist if and only if there is some samplable distribution D such that it is hard to approximate the Kolmogorov complexity of a string sampled from D. Thus we characterize the existence of one-way functions by the average-case hardness of a natural \emph{uncomputable} problem on samplable distributions, extending a recent line of work by Liu and Pass (FOCS'20, STOC'21) and Ren and Santhanam (CCC'21).

    We also show that the average-case hardness of approximating Minimum Circuit Size on a locally samplable distribution (where the sampler runs in sub-linear time by using random access to its input) is equivalent to the existence of one-way functions. This is the first characterization of one-way functions by a natural average-case hardness assumption on the Minimum Circuit Size Problem. We present several other characterizations and connections between one-way functions and average-case hardness of meta-complexity problems (problems about complexity) on samplable distributions.

    We give various applications of these results to the foundations of cryptography and the theory of meta-complexity. We show that the average-case hardness of deciding k-SAT or Clique on any samplable distribution of high enough entropy implies the existence of one-way functions. Thus one-way functions follow from general assumptions on the average-case hardness of NP-complete problems. We observe that our assumptions are implied by standard cryptographic assumptions such as the Planted Clique hypothesis and the pseudorandomness of Goldreich's local functions.

    Our results imply a range of \emph{equivalences} between various meta-complexity problems, showing that the theory of meta-complexity is very \emph{robust} when considering average-case complexity. We use our results to unconditionally solve various meta-complexity problems in CZK (computational zero-knowledge) on average, and give implications of our results for the classic question of proving NP-hardness for the Minimum Circuit Size Problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/082"><span class="datestr">at June 16, 2021 03:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">What is a natural question?</a> (<a href="https://mathstodon.xyz/@11011110/106339474749439957">\(\mathbb{M}\)</a>) Gasarch on distinguishing notions of interestingness of mathematical problems based on ability to answer them, versus whether they lead to deeper mathematics.</p>
  </li>
  <li>
    <p>Bad geometry in an illustration of parallel parking advice to change steering lock-to-lock when you reach a 45° angle (<a href="https://mathstodon.xyz/@11011110/106345620085904810">\(\mathbb{M}\)</a>): this car is going to end up halfway over the curb. Really the inflection angle depends on the distance from curb and your car’s turning radius. You should switch direction when the rear axle midpoint is halfway between its starting and ending lines of motion, or approximately when the passenger rear wheel is a bit less than halfway to the curb. This car went past that point and can’t straighten before reaching the curb.</p>

    <p style="text-align: center;"><img width="60%" alt="Top-down view of parallel parking, with the path of the rear-axle centerline shown in red. With the centerline ending up on the curb at the end of the path, the car itself will be well over the curb." src="https://11011110.github.io/blog/assets/2021/parallel-park.jpg" /></p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-identify-threshold-at-which-shapes-give-way-20210603/">How smooth can Nash–Kuiper embeddings be?</a> (<a href="https://mathstodon.xyz/@11011110/106347809760355726">\(\mathbb{M}\)</a>) Recent progress identifies a strict threshold at \(C^{1,1/2}\). A new popularized and high-level survey on <em>Quanta</em>. See Wikipedia for more on the <a href="https://en.wikipedia.org/wiki/Nash_embedding_theorem">Nash embedding theorem</a> (here, in the form that all 2d surfaces have non-smooth embeddings in 3d preserving distances along surface curves) and on <a href="https://en.wikipedia.org/wiki/H%C3%B6lder_condition">the notions of fractional smoothness considered here</a>, left unexplained by the <em>Quanta</em> article.</p>
  </li>
  <li>
    <p><a href="https://xenaproject.wordpress.com/2021/06/05/half-a-year-of-the-liquid-tensor-experiment-amazing-developments/">Interesting progress report by Peter Scholze on the formalized and computer-verified proof of some difficult material in analytic geometry</a> (<a href="https://mathstodon.xyz/@11011110/106359088815982703">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="http://web.stanford.edu/~mleake/projects/paperpiecing/">A mathematical foundation for foundation paper pieceable quilts</a> (<a href="https://mathstodon.xyz/@11011110/106362497443237500">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191666/Quilters-are-the-best-at-graph-theory">via</a>). They formalize things by hypergraphs, but I prefer <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a>: a pattern is piecable if and only if you can slice off one piece at a time by guillotine cuts, eventually slicing everything. The possible slicing orders form an antimatroid, because a piece, once sliceable, remains so until it is sliced. The piecing order is the reverse of a slicing order.</p>
  </li>
  <li>
    <p>Firefox’s “reader mode”, in which you can click on little page-with-text icon next to the url, and get view of text stripped of extraneous framing, is useful (<a href="https://mathstodon.xyz/@11011110/106370852134898457">\(\mathbb{M}\)</a>), but it would be more useful <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1466044">if it actually worked for pages with mathematical content</a>.</p>
  </li>
  <li>
    <p>One of the footpaths through my neighborhood is next to a small field with two chess tables (<a href="https://mathstodon.xyz/@11011110/106372538313055882">\(\mathbb{M}\)</a>). They’ve been overgrown and largely unused for quite a while, but a few months back the neighborhood association’s gardeners cleared the field and planted some ginkgo trees around the tables. Someone has been playing chess there with pine cones:</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="UCI Ecological Preserve" src="https://www.ics.uci.edu/~eppstein/pix/uhchess/Chess-m.jpg" /></p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/why-is-quantum-computing-so-hard-to-explain-20210608/">Scott Aaronson, in <em>Quanta</em>, on why the glib popularizations of quantum computing typically found in publications like <em>Quanta</em> are usually wrong</a> (<a href="https://mathstodon.xyz/@11011110/106377151244377180">\(\mathbb{M}\)</a>, <a href="https://www.scottaaronson.com/blog/?p=5539">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://nautil.us/issue/23/dominoes/the-amazing-autotuning-sandpile">The amazing, autotuning sandpile</a> (<a href="https://mathstodon.xyz/@11011110/106389220646874351">\(\mathbb{M}\)</a>). Jordan Ellenberg on <a href="https://en.wikipedia.org/wiki/Abelian_sandpile_model">abelian sandpile models</a>, with pretty pictures of the fractal patterns they generate, from 2015. These patterns are still a topic of active research; see e.g. “<a href="https://annals.math.princeton.edu/2017/186-1/p01">The Apollonian structure of integer superharmonic matrices</a>”, <em>Ann. Math.</em> 2017, , and “<a href="https://doi.org/10.1073/pnas.1812015116">Harmonic dynamics of the abelian sandpile</a>”, <em>PNAS</em> 2019.</p>
  </li>
  <li>
    <p><a href="https://mathemalchemy.org/2021/01/19/dodecahedral-trajectory/">Dodecahedral trajectory</a> (<a href="https://mathstodon.xyz/@11011110/106395952293245848">\(\mathbb{M}\)</a>). Mathematical glass artist Bronna Butler visualizes closed geodesics from and returning to a single vertex on the dodecahedron, using stained glass. These paths exist only on the dodecahedron, among the Platonic solids: geodesics starting from a vertex on the other solids will either hit another vertex before returning, or never return.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/twitter-photo-crop-algorithm-favors-white-faces-women/">Automatic face-recognition systems used to auto-crop social media photos discovered to be racist and sexist</a> (<a href="https://mathstodon.xyz/@11011110/106402230076815482">\(\mathbb{M}\)</a>). This sort of thing is part of a long line of stories of computer systems learning to replicate the biases of their developers, and one of many reasons we need greater diversity in CS.</p>
  </li>
  <li>
    <p><em><a href="https://doi.org/10.1007/978-981-15-4470-5">Introduction to Computational Origami</a></em> (<a href="https://mathstodon.xyz/@11011110/106407675350595758">\(\mathbb{M}\)</a>), Ryuhei Uehara’s 2020 book, is the subject of <a href="https://www.maa.org/press/maa-reviews/introduction-to-computational-origami">a new featured review by Tom Hull on <em>MAA Reviews</em></a>.</p>
  </li>
  <li>
    <p><a href="http://gregegan.net/SCIENCE/Howell/Howell.html">Howell’s moving orbits</a> (<a href="https://mathstodon.xyz/@11011110/106413218848247270">\(\mathbb{M}\)</a>). Greg Egan explains the early research of <a href="https://en.wikipedia.org/wiki/Kathleen_Howell">Kathleen Howell</a> on the existence of first-order-stable <a href="https://en.wikipedia.org/wiki/Halo_orbit">halo orbits</a> of a third small body under the gravitational influence of two larger bodies, related to the (unstable) L1 and L2 Lagrange points.</p>
  </li>
  <li>
    <p><a href="http://gallery.bridgesmathart.org/exhibitions/2021-bridges-conference">The 2021 Bridges Conference Mathematical Art Galleries</a> (<a href="https://mathstodon.xyz/@11011110/106418890356129959">\(\mathbb{M}\)</a>) are online even though <a href="https://www.bridgesmathart.org/b2021/">the online conference itself is not until August</a>. I haven’t yet had time to explore all of them but there are lots of good pieces in there.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/15/linkage.html"><span class="datestr">at June 15, 2021 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18896">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/">Thursday—June 17th—A Debate on Program Correctness</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>There are two ways to write error free programs; only the third one works–Alan Perlis</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/1_qkgjghs7wcpoq09rklc_jq/" rel="attachment wp-att-18919"><img width="200" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/1_QkGJGhS7wCPoq09rkLC_JQ.jpeg?resize=200%2C130&amp;ssl=1" class="alignright wp-image-18919" height="130" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Perlis on Coding Joy <a href="https://www.freecodecamp.org/news/perlis-on-tech-evangelism-10-epigrams-5ea6dcf3faf5/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Alan Perlis, the first Turing Award winner, summarized the whole issue of program correctness in his single quote.  Maybe there is nothing more to say about it.</p>
<p>
But this coming Thursday, if you want to hear more about it, you can tune in for a debate with Rich de Millo and myself.  It will be 7:00–8:30pm Eastern time.</p>
<p>
Harry Lewis of Harvard University will moderate.  It will be broadcast <a href="https://scp.cc.gatech.edu/2021/05/26/debate-that-changed-programming-living-history/">LIVE HERE</a>. It’s free.</p>
<p>
 <br />
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/pic-10/" rel="attachment wp-att-18898"><img width="550" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/pic.png?resize=550%2C312&amp;ssl=1" class="aligncenter wp-image-18898" height="312" /></a></p>
<p></p><p>
 <br />
</p><h2> The Paper </h2><p></p>
<p></p><p>
Perlis and Rich DeMillo and I wrote the <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">paper</a> “Social Processes and Proofs of Theorems<br />
and Programs” in the middle of the second half of the last century—pushing towards fifty years ago. We were responding to the then-common view that programs should be proved correct—proved in the same manner that one proves theorems like: </p>
<blockquote><p><b>Theorem 1 (Euclid)</b> <em> There are an infinite number of prime numbers. </em>
</p></blockquote>
<p>We begged to object: We felt that correctness of programs was fundamentally a different issue. This is what the debate is all about. </p>
<p>
The debate will give us all a chance to reflect now almost 50 years later when programs enrich and regulate so much more of our lives.  Perhaps the real winner in society goes according to the proverb: “the one who ends up with the most toys.”  In that case, Harry the moderator will win hands down:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/harrylewis_harvard_officeknickknacks/" rel="attachment wp-att-18914"><img width="240" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/HarryLewis_Harvard_OfficeKnickKnacks.jpg?resize=240%2C160&amp;ssl=1" class="aligncenter wp-image-18914" height="160" /></a></p>
<p>
</p><p></p><h2> The Writing </h2><p></p>
<p></p><p>
Mary-Claire van Leunen is an expert on writing, especially for technical articles. See her famous <a href="https://www.amazon.com/s?i=stripbooks&amp;rh=p_27%3AMary-Claire+van+Leunen&amp;s=relevancerank&amp;text=Mary-Claire+van+Leunen&amp;ref=dp_byline_sr_book_1">book</a>.</p>
<p>
The article was heavily written by Rich with input from Alan and myself. The ideas are due to all of us. The actual details, the words, the punctuation owe more to Rich with strong input from Mary-Claire. </p>
<p>
Our paper is one of forty-six papers included in the <a href="https://mitpress.mit.edu/books/ideas-created-future">book</a> <i>Ideas That Created the Future: Classic Papers of Computer Science</i>, which was edited by Harry for MIT Press (2020).  They are presented in chronological order beginning with Aristotle (~350 BCE).  The median year is 1962-63.  </p>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
See <a href="https://www.sigplan.org/Resources/Advice/VanLeunen-Lipton/">How to Have Your Abstract Rejected</a> for a tongue-in-cheek view of writing technical material. It was written by Mary-Claire and myself.</p>
<p>
See you Thursday. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/06/15/thursday-june-17th-a-debate-on-program-correctness/"><span class="datestr">at June 15, 2021 02:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/081">TR21-081 |  Superpolynomial Lower Bounds Against Low-Depth Algebraic Circuits | 

	Sébastien Tavenas, 

	Srikanth Srinivasan, 

	Nutan Limaye</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An Algebraic Circuit for a polynomial $P\in F[x_1,\ldots,x_N]$ is a computational model for constructing the polynomial $P$ using only additions and multiplications. It is a \emph{syntactic} model of computation, as opposed to the Boolean Circuit model, and hence lower bounds for this model are widely expected to be easier to prove than lower bounds for Boolean circuits. Despite this, we do not have superpolynomial lower bounds against general algebraic circuits of depth 3 (except over constant-sized finite fields) and depth 4 (over any field), while constant-depth Boolean circuit lower bounds have been known since the early 1980s.

In this paper, we prove the first superpolynomial lower bounds against general algebraic circuits of all constant depths over all fields of characteristic $0$ (or large). We also prove the first lower bounds against homogeneous algebraic circuits of constant depth over any field.

Our approach is surprisingly simple. We first prove superpolynomial lower bounds for constant-depth Set-Multilinear circuits. While strong lower bounds were already known against such circuits, most previous lower bounds were of the form $f(d)\cdot poly(N)$, where $d$ denotes the degree of the polynomial.  In analogy with Parameterized complexity, we call this an FPT lower bound. We extend a well-known technique of Nisan and Wigderson (FOCS 1995) to prove non-FPT lower bounds against constant-depth set-multilinear circuits computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}$ (which computes a fixed entry of the product of $d$ $n\times n$ matrices). More precisely, we prove that  any set-multilinear circuit of depth $\Delta$ computing $IMM_{n,d}$ must have size at least $n^{d^{\exp(-O(\Delta))}}.$ This result holds over any field.

We then show how to convert any constant-depth algebraic circuit of size $s$ to a constant-depth set-multilinear circuit with a blow-up in size that is exponential in $d$ but only polynomial in $s$ over fields of characteristic $0$. (For depths greater than $3$, previous results of this form increased the depth of the resulting circuit to $\Omega(\log s)$.) This implies our constant-depth circuit lower bounds for $d$ that is a slow-growing function of $n$.

Finally, we observe that our superpolynomial lower bound for constant-depth circuits  implies the first deterministic sub-exponential time algorithm for solving the Polynomial Identity Testing (PIT) problem for all small depth circuits  using the known connection between algebraic hardness and randomness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/081"><span class="datestr">at June 15, 2021 05:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-06-14-neither-non-equivocation-nor-transferability-alone-is-enough-for-tolerating-minority-corruptions-in-asynchrony/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-06-14-neither-non-equivocation-nor-transferability-alone-is-enough-for-tolerating-minority-corruptions-in-asynchrony/">Neither Non-equivocation nor Transferability alone is enough for tolerating minority corruptions in asynchrony</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post, we explore a theorem of Clement, Junqueira, Kate, and Rodrigues from PODC 2012 regarding the limits of non-equivocation. Informally, this theorem says that neither Non-equivocation nor Transferability alone is enough for tolerating minority corruptions in asynchrony. Theorem CJKR12: Neither non-equivocation nor transferability is individually sufficient to solve...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-06-14-neither-non-equivocation-nor-transferability-alone-is-enough-for-tolerating-minority-corruptions-in-asynchrony/"><span class="datestr">at June 14, 2021 04:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/06/14/machine-learning-for-algorithms-virtual-workshop/">Machine Learning for Algorithms – virtual workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>[H/T Jelani Nelson]</p>



<p>In recent years there has been increasing interest in using machine<br />learning to improve the performance of classical algorithms in<br />computer science, by fine-tuning their behavior to adapt to the<br />properties of the input distribution. This “data-driven” or<br />“learning-based” approach to algorithm design has the potential to<br />significantly improve the efficiency of some of the most widely used<br />algorithms. For example, it has been used to design better data<br />structures, online algorithms, streaming and sketching algorithms,<br />market mechanisms and algorithms for combinatorial optimization,<br />similarity search and inverse problems. This virtual workshop will<br />feature talks from experts at the forefront of this exciting area.</p>



<p>The workshop will take place virtually on <strong>July 13-14, 2021</strong>. <br />Registration is <strong>free but mandatory</strong>. Link to register: <a href="https://fodsi.us/ml4a.html" target="_blank" rel="noreferrer noopener">https://fodsi.us/ml4a.html</a></p>



<p><strong>Confirmed Speakers:</strong></p>



<ul><li>Alex Dimakis (UT Austin)</li><li>Yonina Eldar (Weizmann)</li><li>Anna Goldie (Google Brain, Stanford)</li><li>Reinhard Heckel (Technical University of Munich)</li><li>Stefanie Jegelka (MIT)</li><li>Tim Kraska (MIT)</li><li>Benjamin Moseley (CMU)</li><li>David Parkes (Harvard)</li><li>Ola Svensson (EPFL)</li><li>Tuomas Sandholm (CMU, Optimized Markets, Strategy Robot, Strategic Machine)</li><li>Sergei Vassilvitski (Google)</li><li>Ellen Vitercik (CMU/UC Berkeley)</li><li>David Woodruff (CMU)</li></ul>



<p><strong>Organizers:</strong></p>



<ul><li>Costis Daskalakis (MIT)</li><li>Paul Hand (Northeastern)</li><li>Piotr Indyk (MIT)</li><li>Michael Mitzenmacher (Harvard)</li><li>Ronitt Rubinfeld (MIT)</li><li>Jelani Nelson (UC Berkeley)</li></ul>



<h2 style="color: #a30023;" class="has-text-color"><strong>Unrelated announcement: JACM looking for editor in chief</strong> </h2>



<p>(h/t Salil Vadhan)</p>



<p>The Journal of the ACM is looking for a new editor in chief: see the <a href="https://dl.acm.org/pb-assets/static_journal_pages/jacm/pdf/JACM-Call-2021-BAM-1623181914103.pdf">call for nominations</a>. The (soft) deadline to submit nominations (including self nominations) is <strong>July 19th</strong> and you can do so by emailing  Chris Hankin at c.hankin@imperial.ac.uk </p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/06/14/machine-learning-for-algorithms-virtual-workshop/"><span class="datestr">at June 14, 2021 01:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18876">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/12/tardos-is-off-with-good-behavior/">Tardos Is Off With Good Behavior</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>A call for nominations for her successor</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/12/tardos-is-off-with-good-behavior/20170912-142623-1_edit/" rel="attachment wp-att-18887"><img width="114" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/20170912-142623-1_edit.jpg?resize=114%2C123&amp;ssl=1" class="alignright size-full wp-image-18887" height="123" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">In her original <a href="https://research.cornell.edu/researchers/eva-tardos">orange</a></font></td>
</tr>
</tbody>
</table>
<p>
Éva Tardos’s sentence as Editor-in-Chief (EiC) of the <i>Journal of the ACM</i> (JACM) is coming to an end. She has served almost six years. She will be paroled soon for her good behavior. </p>
<p>
Today we announce that a grand jury is being convened. This group is charged with finding the right person to be indicted as the next editor-in-chief of the JACM.<br />
<span id="more-18876"></span></p>
<p>
Tips on possible suspects should be directed to lieutenant Chris Hankin <a href="https://www.imperial.ac.uk/people/c.hankin">here</a>. Note that the location is not far from Scotland Yard. He is in charge of the grand jury:</p>
<p>
Laura Haas (University of Massachusetts Amherst, US) <br />
Orna Kupferman (Hebrew University, Israel) <br />
Marta Kwiatkowska (University of Oxford, UK) <br />
Brad Myers (Carnegie Mellon University, US) <br />
Salil Vadhan (Harvard University, US) <br />
Albert Zomaya (University of Sydney, Australia) <br />
Divesh Srivastava (AT&amp;T, US), ACM Pubs Board Liaison</p>
<p>
</p><p></p><h2> History </h2><p></p>
<p></p><p>
The JACM was established in 1954 and has published over 3,000 papers, since then. There were 196 submissions received in 2020 alone. The current editorial board is <a href="https://dl.acm.org/journal/jacm/editorial-board#editor-in-chief">this</a>. Previous editors-in-chief are:</p>
<p>
1954 – 1958: Franz Alt<br />
1959 – 1962: Mario Juncosa<br />
1963 – 1965: Richard Hamming<br />
1966 – 1968: Calvin Gotlieb<br />
1969 – 1972: Gerard Salton<br />
1973 – 1975: Raymond Miller<br />
1976 – 1979: Edward Coffman, Jr.<br />
1979 – 1982: Michael Garey<br />
1983 – 1986: Michael Fischer<br />
1986 – 1990: Daniel Rosenkrantz<br />
1991 -1997: Tom Leighton<br />
1997 – 2003: Joseph Halpern<br />
2003 – 2009: Prabhakar Raghavan<br />
2009 – 2015: Victor Vianu<br />
2015 – currently: Éva Tardos</p>
<p>
</p><p></p><h2> Selection </h2><p></p>
<p></p><p>
Typically many candidates are available, but only the most dramatic cases will be considered. Nominations should include a vita along with a brief statement of why the nominee should be considered. Why they are <b>un</b>-worthy to be made into the editor-in-chief? A vision statement is highly encouraged. The relevant law is found here:</p>
<p>
<a href="https://www.acm.org/publications/policies/roles-and-responsibilities">Roles and Responsibilities in ACM Publishing</a></p>
<p>
<a href="https://www.acm.org/publications/policies/eic-evaluation">ACM’s Evaluation Criteria for Editors-in-Chief</a></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>All kidding aside. We thank Éva for her hard work keeping the JACM one of the top journals in the world. We can only hope that the next chief will be nearly as sucessful as she has been.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/06/12/tardos-is-off-with-good-behavior/"><span class="datestr">at June 12, 2021 12:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html">Carrying as chip-firing for the Zeckendorf representation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>You may have heard of the <a href="https://en.wikipedia.org/wiki/Zeckendorf%27s_theorem">Zeckendorf representation</a> according to which any positive integer can be represented as a sum of non-consecutive Fibonacci numbers. Its uses include the optimal strategy in the game of <a href="https://en.wikipedia.org/wiki/Fibonacci_nim">Fibonacci nim</a>. But did you know that <a href="https://www.irif.fr/~cf/publications/AUFP.pdf">it’s possible to efficiently add and subtract Zeckendorf representations</a>?<sup id="fnref:a"><a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fn:a" class="footnote" rel="footnote">1</a></sup> The algorithm from the paper linked above takes three passes over the input digit sequences using finite state automata, much like binary number addition can be performed by a single pass of a finite state automaton. I thought it might be interesting to describe an alternative path to the same result, using <a href="https://en.wikipedia.org/wiki/Chip-firing_game">chip-firing games</a>.</p>

<h1 id="the-chip-firing-antimatroid">The chip-firing antimatroid</h1>

<p><a href="https://en.wikipedia.org/wiki/Chip-firing_game">Chip-firing games</a>, or <a href="https://en.wikipedia.org/wiki/Abelian_sandpile_model">sandpile models</a>, are systems described by a graph, with markers of some kind such as coins on its vertices. The graph may possibly be directed, although in the simplest examples it is undirected, and it may be infinite. Each vertex may have arbitrarily many coins on it. But, if a vertex has at least as many coins as it has outgoing edges, we can “fire” the vertex, moving one coin to each neighbor. This repeats until no more such events are possible. Doing this on an infinite square grid with an initial state that piles many coins on a single vertex leads to pretty fractals; here’s a detail of <a href="https://commons.wikimedia.org/wiki/File:Sandpile_on_infinite_grid,_3e7_grains.png">the result of starting with 30 million coins</a>:</p>

<p style="text-align: center;"><img width="80%" alt="Detail of abelian sandpile model on square grid with 3x10^7 starting coins, extracted from CC-BY-SA image https://commons.wikimedia.org/wiki/File:Sandpile_on_infinite_grid,_3e7_grains.png by colt_browning" src="https://11011110.github.io/blog/assets/2021/sandpile-30M-detail.png" /></p>

<p>However, we’ll be looking at this sort of system on one-dimensional infinite graphs, where their behavior is not quite as complicated.</p>

<p>Chip-firing, on any graph and any placement of coins for which it terminates, forms an <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroid</a>.<sup id="fnref:bls"><a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fn:bls" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:k"><a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fn:k" class="footnote" rel="footnote">3</a></sup> A vertex \(v\) can fire for the \(i\)th time, as long as it has already fired \(i-1\) times and a total of \(i\cdot\deg(v)\) coins have reached it. Once these conditions are met, they remain true until \(v\) fires. This idea, that once the item \((v,i)\) becomes available to be added to the sequence of firings it remains available until it is added, is the defining principle of an antimatroid. From it one can prove that, if any firing sequence terminates in a stable configuration, then all sequences terminate, they all fire the same vertices the same numbers of times, and they all end in the same stable configuration.</p>

<h1 id="binary-carrying-as-chip-firing">Binary carrying as chip-firing</h1>

<p>A binary representation of a number \(x\) is just a set of distinct powers of two, summing to \(x\). If you add two binary numbers \(x\) and \(y\), you can combine their sets into a single multiset; carrying can be thought of as a systematic method of getting rid of the duplicate powers of two in this multiset. Whenever you have two equal powers of two in a multiset, whose sum you are trying to represent, you can merge them with the fusion rule</p>

\[2^i+2^i \Rightarrow 2^{i+1}.\]

<p>This can be thought of as a chip-firing game on a graph where there is a vertex for every power of two, an edge to the next larger power of two, and an edge to a “<a href="https://en.wikipedia.org/wiki/Bit_bucket">bit bucket</a>” vertex with no outgoing edges (which cannot be fired). Each instantiation of the fusion rule fires vertex \(2^i\), moving one coin to \(2^{i+1}\) and one to the bit bucket.</p>

<p>This is exactly what we are doing when we carry a term in binary addition: taking two powers of two from a column of the addition problem and fusing them into a single power of two in the next column. We can also think of this physically, using a one-dimensional array of cells with poker chips or coins on them, with a coin on cell \(i\) representing the number \(2^i\). The fusion rule takes two coins from any cell and replaces them by a single coin in the next cell. In binary addition of pairs of numbers, there are at most three coins per cell (the two that started out there and one carry), but you could use the same fusion rule for addition of more than two numbers, using an array of cells that can each contain a large pile of coins.</p>

<p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="Five stacks of coins. Image Money-2180330 1920.jpg on Wikimedia commons, uploaded by Stella Vogt from pixabay free images, described as &quot;probably free&quot; but tagged with a CC-BY-SA license." src="https://11011110.github.io/blog/assets/2021/stacks-of-coins.jpg" /></p>

<p>Carrying the coin analogy further, we might imagine that the coins have values that are powers of two, and that we are making change by replacing pairs of small coins by a single larger coin of equal value.</p>

<p>Conventional binary arithmetic does these fusion steps in a systematic order, from low-order bits of the binary representation (smaller powers of two) to higher-order bits (larger powers of two). It’s that systematic order, together with the observation that each pile has at most three coins, that makes this method suitable for a finite state machine. But actually, you could apply the fusion rule in any order, and it would work equally well. Each step reduces the number of coins by one, so you can never do more steps than your starting number of coins. We can only halt when we reach the binary representation of the sum, which is uniquely determined, so this process is <a href="https://en.wikipedia.org/wiki/Confluence_(abstract_rewriting)">confluent</a>: every sequence of choices leads to the same eventual outcome. In this case, confluence can also be seen from the antimatroid property of chip-firing games.</p>

<p>This observation about reducing the number of powers of two gives an immediate proof of a fact about binary representations that you may not have known: the binary representation of \(x+y\) has at most as many nonzero bits as there are in \(x\) and \(y\) separately.</p>

<h1 id="chip-firing-for-fibonacci-numbers">Chip-firing for Fibonacci numbers</h1>

<p>Now, instead of coins with power-of-two values, let’s suppose that they have Fibonacci numbers as their values, representing any multiset of Fibonacci numbers. We can still combine pairs of Fibonacci numbers by a fusion rule, that now applies when we have two coins in adjacent piles:</p>

\[F_i + F_{i+1} \Rightarrow F_{i+2}.\]

<p>But in order to get the Zeckendorf representation of the sum of the multiset, we also need to deal with pairs of coins that have the same value, because repeated copies of the same Fibonacci number are not allowed in the Zeckendorf representation. We can accomplish this by a “fission” rule, that steps backwards according to the fusion rule in order to take a bigger step forwards:</p>

\[F_i+F_i \Rightarrow F_{i-2}+F_{i-1}+F_i\Rightarrow F_{i-2}+F_{i+1}.\]

<p>This makes sense even when \(F_i\) is \(1\) or \(2\): for \(F_i=1\), the \(F_{i-2}\) term is zero and can be dropped from the sum, and for \(F_i=2\) it equals one, a Fibonacci number. So as special cases of this rule we have \(1+1\Rightarrow 2\) and \(2+2\Rightarrow 1+3\).</p>

<p>Several issues complicate the analysis of this replacement system. First, although fission by itself is a chip-firing rule on an infinite graph with outdegree two, fission and fusion together make a more complicated system that is not an antimatroid, so different orderings of choosing what to do may lead to different numbers of steps. Second, the fission rule changes the piles of coins both to the left and to the right of the pile to which it applies, making it harder to find a consistent ordering in which to apply these rules. And third, fission preserves the number of coins rather than reducing them, making both the eventual termination of the system and the analysis of how many steps it takes less obvious. But assuming it does terminate, it can only terminate at the Zeckendorf representation, which is unique. So termination implies confluence.</p>

<h1 id="fission-without-fusion">Fission without fusion</h1>

<p>First let’s see what happens if we just use the fission rule, forgetting about the fusion rule. Because fission by itself is a chip-firing rule, any initial state has an invariant number of firings and an invariant final state, regardless of the order of operations. For an initial state with \(n\) copies of \(F_i\), the final state gives us an expansion of \(F_i\) into a sum of \(n\) distinct Fibonacci numbers:</p>

\[F_i = F_i\]

\[2\,F_i = F_{i-2}+F_{i+1}\]

\[3\,F_i=F_{i-2}+F_i+F_{i+1}\]

\[4\,F_i=F_{i-4}+F_{i-3}+F_i+F_{i+2}\]

\[\vdots\]

<p>The indexes appearing in these identities can be summarized in a <a href="https://en.wikipedia.org/wiki/Category:Triangles_of_numbers">triangle of numbers</a>:</p>

\[\begin{array}{ccccccc}
&amp;&amp;&amp;&amp;&amp;&amp;0&amp;&amp;&amp;&amp;&amp;&amp;\\
&amp;&amp;&amp;&amp;&amp;-2&amp;&amp;\ 1\ &amp;&amp;&amp;&amp;&amp;\\
&amp;&amp;&amp;&amp;-2&amp;&amp;0&amp;&amp;\ 1\ &amp;&amp;&amp;&amp;\\
&amp;&amp;&amp;-4&amp;&amp;-3&amp;&amp;0&amp;&amp;\ 2\ &amp;&amp;&amp;\\
&amp;&amp;-4&amp;&amp;-3&amp;&amp;-2&amp;&amp;1&amp;&amp;\ 2\ &amp;&amp;\\
&amp;-4&amp;&amp;-3&amp;&amp;-2&amp;&amp;0&amp;&amp;1&amp;&amp;\ 2\ &amp;\\
-6&amp;&amp;-5&amp;&amp;-4&amp;&amp;-3&amp;&amp;-1&amp;&amp;1&amp;&amp;\ 3\ \\
&amp;&amp;&amp;&amp;&amp;&amp;\vdots&amp;&amp;&amp;&amp;&amp;&amp;\\
\end{array}\]

<p>The largest number in each row grows only logarithmically with \(n\), because of the way the values of the Fibonacci numbers grow exponentially as a function of their index. The fission rule cannot create gaps of three or more empty cells,
so consecutive numbers in a row differ by at most three, implying that the smallest number in each row is greater than \(-3n\). Because of this linear bound on how far fission can move the coins, we get a quadratic bound on the total number of firing events: each firing decreases the total distance of the coins from cell \(-3n\). This total distance starts at \(3n^2\) and remains positive, so there can  be at most \(3n^2\) firings. In computational experiments up to \(n=10000\) the actual number of firings never exceeded \(n^2\) and appeared to be growing as \(n^2\bigl(1-o(1)\bigr)\). This is a big contrast from the behavior of the similar-looking chip-firing rule on a line of cells that replaces pairs of coins by a coin one cell to the left and a coin one cell to the right, which (with \(n\) chips starting in a pile on one cell) produces a <a href="https://en.wikipedia.org/wiki/Square_pyramidal_number">square pyramidal number</a> of firings, cubic in \(n\).</p>

<p>For initial coins that are not all in one big pile, it remains impossible for fission to produce new long gaps, and we can decompose the sequence of cells into subsequences of linear length separated by gaps too long to be crossed. The same analysis of total distance of coins from the starts of sequences of cells shows that, regardless of starting state, and even if we also allow fusion rules, we get an \(O(n^2)\) bound on the total number of steps. In particular, this system always terminates.</p>

<h1 id="taking-few-steps">Taking few steps</h1>

<p>Fortunately, we don’t have to take quadratic time to simplify sums of Fibonacci numbers into their Zeckendorf representations using these rules. The problem with the previous analysis was that we were doing too much fission before we did any fusion. Suppose we sidestep that with the following prioritization:</p>

<ul>
  <li>If any fusion step would put a coin into a pile that is currently empty, do it.</li>
  <li>Otherwise, perform a fission step at the pile with the largest possible index.</li>
</ul>

<p>If any fusion steps are possible, there must be at least one (for instance, the one with the largest index) that is prioritized. To analyze these prioritized steps, define \(N\) to be the current number of coins, and \(M\) to be the number of coins that are at or below a pile with two or more coins on it. The key insight is that whenever a step moves a coin to a pile with a larger index, it cannot form a new larger-index multi-coin pile, so both \(M\) and \(N\) are non-decreasing. They can stay the same only for a fission step at a pile of three or more coins, but in that case the next step must be a fusion. \(M\) and \(N\) are both \(\le n\), and their sum decreases at least once for every two steps, so the total number of steps is \(\le 4n\). Thus, this prioritized chip-firing method can reduce any sum of Fibonacci numbers to its Zeckendorf representation in a linear number of steps. It’s also not difficult to implement so that it takes linear time.</p>

<h1 id="zeckendorf-arithmetic">Zeckendorf arithmetic</h1>

<p>Anyway, now that we have a way of simplifying sums of Fibonacci numbers to their Zeckendorf form, let’s use it for arithmetic on Zeckendorf representations, without the need to reduce them to simpler forms.</p>

<p>First, addition. This is very easy: Add the two Zeckendorf representations as multisets (producing piles of coins that might be adjacent, or might have two coins in them) and then simplify. The result is a linear time for adding two Zeckendorf representations, as was already known. But because it isn’t based on finite state machines, it can also handle more than two coins, for instance by adding multiple numbers at once rather than having to reduce the problem to multiple pairwise additions. As with binary arithmetic, the chip-firing method gives a quick proof of a non-obvious fact: the Zeckendorf representation of \(x+y\) has at most as many nonzero terms as there are in \(x\) and \(y\) put together.</p>

<p>It’s also possible to use this technique (or really, any linear-time Zeckendorf addition algorithm) as part of a long multiplication algorithm for Zeckendorf representations. To multiply two numbers \(x\) and \(y\), both given by their Zeckendorf representations, first perform a sequence of additions to compute the representations of the numbers \(F_i\cdot x\), each computed as \((F_{i-1}\cdot x)+(F_{i-2}\cdot x)\) in a single addition from earlier numbers in the sequence. Then pick out the subset of these representations for which \(F_i\) belongs to the Zeckendorf representation of \(y\), and add them together. (Or interleave the computation of \(F_i\cdot x\) and the sum of these representations, to save space.) The total time for \(\ell\)-bit inputs is \(O(\ell^2)\), as it is for the usual long multiplication algorithm. I’m using the different variable \(\ell\), because the time here is a function of all the bits in the input Zeckendorf representation, rather than just the number of nonzero bits.</p>

<p>You can convert a binary number to Zeckendorf with the same idea, computing the sequence of powers of two and then picking out and adding together the subset of these powers used in the binary representation of the input. In the other direction, you can convert a Zeckendorf representation to binary by computing the Fibonacci sequence in binary, picking out the terms from the given Zeckendorf representation, and adding them. Both directions of conversion take time \(O(\ell^2)\). A recent paper by Sergeev shows how to do these conversions much more efficient, only logarithmically slower than binary-number multiplication.<sup id="fnref:s"><a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fn:s" class="footnote" rel="footnote">4</a></sup> Based on Sergeev’s results it would also be much more efficient (in theory at least, if not necessarily in practice) to multiply Zeckendorf representations by converting to binary, multiplying in binary, and converting back.</p>

<p>As the article I linked at the start hints, the direct algorithms described here are a bit unsatisfactory from the point of view of circuit complexity: the circuit size is ok but the circuit depth is much larger than it would be for binary arithmetic. I’m not convinced that this is a serious deficiency, because we’re not likely to build computers that use this arithmetic at the circuit level. Any algorithms like this are more likely to be used only in specialized situations where Zeckendorf arithmetic is relevant, like the analysis of certain combinatorial games.</p>

<div class="footnotes">
  <ol>
    <li id="fn:a">
      <p>Connor Ahlbach, Jeremy Usatine, Christiane Frougny, and Nicholas Pippenger (2013), “Efficient algorithms for Zeckendorf arithmetic”, <em>Fibonacci Quarterly</em> 51 (3): 249–255, <a href="https://arxiv.org/abs/1207.4497">arXiv:1207.4497</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=3093678">MR3093678</a>. <a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fnref:a" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:bls">
      <p>Anders Björner, László Lovász, and Peter Shor (1991), “Chip-firing games on graphs”, <em>European Journal on Combinatorics</em> 12 (4): 283–291, <a href="https://doi.org/10.1016/S0195-6698(13)80111-4">doi:10.1016/S0195-6698(13)80111-4</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=1120415">MR1120415</a>. <a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fnref:bls" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:k">
      <p>Kolja Knauer (2009), “Chip-firing, antimatroids, and polyhedra”, EuroComb 2009, <em>Electronic Notes in Discrete Mathematics</em> 34: 9–13, <a href="https://doi.org/10.1016/j.endm.2009.07.002">doi:10.1016/j.endm.2009.07.002</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=2591410">MR2591410</a>. <a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fnref:k" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:s">
      <p>I. S. Sergeev (2018), “On the complexity of Fibonacci coding”, <em>Problems of Information Transmission</em> 54: 343–350, <a href="https://doi.org/10.1134/S0032946018040038">doi:10.1134/S0032946018040038</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=3917588">MR3917588</a>. <a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html#fnref:s" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/106399026271211456">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/12/carrying-chip-firing.html"><span class="datestr">at June 12, 2021 10:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2021/">Theory Jobs 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>While in theory bipartite matching should be easy, it has been observed in practice that real instances of matching theoreticians with jobs are hard. It’s been a particularly unusual year with the entire cycle going virtual. Congrats to matched vertices on both sides!</p>

<p><a href="https://docs.google.com/spreadsheets/d/1kobGLgx5QgJb9AvDENg5c8wEbwBUUkVlMTsBV9quEEY/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory hires this year. 
I put in a biased pseudorandom seed, please help populate and share!
Rules for the spreadsheet have been copied from previous years and all edits to the document are anonymized. Please, feel free to contact me directly or post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>You are welcome to add yourself, or people your department has hired. </li>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>Hires should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are <b>absolutely sure have been offered and accepted</b>. This is not the place for speculation and rumors. Please, be particularly careful when adding senior hires (people who already have an academic or industrial job) -- end dates of their current positions might be still in the future. </li>
</ul>


  <p><a href="http://grigory.github.io/blog/theory-jobs-2021/">Theory Jobs 2021</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 12, 2021.</p></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2021/"><span class="datestr">at June 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
