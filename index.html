<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 04, 2020 09:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/070">TR20-070 |  On the list recoverability of randomly punctured codes | 

	Ben Lund, 

	Aditya Potukuchi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that a random puncturing of a code with good distance is list recoverable beyond the Johnson bound.
In particular, this implies that there are Reed-Solomon codes that are list recoverable beyond the Johnson bound.
It was previously known that there are Reed-Solomon codes that do not have this property. 
As an immediate corollary to our main theorem, we obtain better degree bounds on unbalanced expanders that come from Reed-Solomon codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/070"><span class="datestr">at May 04, 2020 09:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1297">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1297">News for April 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>April is now behind us, and we hope you and your families are all staying safe and healthy. We saw six property papers appear online last month, so at least there is some reading ahead of us! A mixture of privacy, quantum, high-dimensional distributions, and juntas (juntæ?). A lot of distribution testing, overall.</p>



<p><strong>Connecting Robust Shuffle Privacy and Pan-Privacy</strong>, by Victor Balcer, Albert Cheu, Matthew Joseph, and Jieming Mao (<a href="https://arxiv.org/abs/2004.09481">arXiv</a>) This paper considers a recent notion of differential privacy called<em> shuffle privacy</em>, where users have sensitive data, a central untrusted server wants to do something with that data (for instance, say… testing its distribution), and a trusted middle-man/entity shuffles the users’ messages u.a.r. to bring in a bit more anonymity. As it turns out, testing uniformity (or identity) of distributions in the shuffle privacy model is (i) much harder than without privacy constraints; (ii) much harder than with ‘usual’ (weaker) differential privacy (iii) much easier than with local privacy; (iv) related to the sample complexity under another privacy notion, <em>pan-privacy</em>. It’s a brand exciting new world out there!</p>



<p><em>(Note: for the reader interested in keeping track of identity/uniformity testing of probability distributions under various privacy models, I wrote a very short summary of the current results <a href="https://github.com/ccanonne/probabilitydistributiontoolbox/blob/master/private-goodness-of-fit.pdf">here</a>.)</em></p>



<p><strong>Entanglement is Necessary for Optimal Quantum Property Testing, </strong>by Sebastien Bubeck, Sitan Chen, and Jerry Li (<a href="https://arxiv.org/abs/2004.07869">arXiv</a>)  The analogue of uniformity testing, in the quantum world, is testing whether a quantum state is equal (or far from) the maximally mixed state. It’s known that this task  has “quantum sample complexity” (number of measurements) \(\Theta(d/\varepsilon^2)\) (i.e., square root dependence on  the dimension of the state, \(d^2\)). But this requires <em>entangled</em> measurements, which may be tricky to get (or, in my case, understand): what happens if the measurements can be adaptive, but not entangled? In this work, the authors show that, under this weaker access model \(\Omega(d^{4/3}/\varepsilon^2)\) measurements are necessary: adaptivity alone won’t cut it. It may still help though: without either entanglement <em>nor</em> adaptivity, the authors also show a \(\Omega(d^{3/2}/\varepsilon^2)\) measurements lower bound.</p>



<p><strong>Testing Data Binnings</strong>, by Clément Canonne and Karl Wimmer (<a href="https://eccc.weizmann.ac.il/report/2020/062/">ECCC</a>) More identity testing! Not private and not quantum for this one, but… not <em>quite</em> identity testing either. To paraphrase the abstract: this paper introduces (and gives near matching bounds for)  the related question of <em>identity up to binning</em>, where the reference distribution \(q\) is over \(k \ll n\) elements: the question is then whether there exists a suitable binning of the domain \([n]\) into \(k\) intervals such that, <em>once binned</em>, \(p\) is equal to \(q\).” </p>



<p><strong>Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models</strong>, by Antonio Blanca, Zongchen Chen, Daniel Štefankovič, and Eric Vigoda (<a href="https://arxiv.org/abs/2004.10805">arXiv</a>) Back to identity testing of distributions, but for high-dimensional structured ones this one. Specifically, this paper focuses on the undirected graphical models known as <em>restricted Boltzmann machines, </em>and provides efficient algorithms for identity testing and conditional hardness lower bounds depending on the type of correlations allowed in the graphical models.</p>



<p><strong>Robust testing of low-dimensional functions</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/2004.11642">arXiv</a>) Junta testing is a classical, central problem in property testing, with motivations and applications in machine learning and complexity. The related (and equally well-motivated) question of junta testing of functions on \(\mathbb{R}^d\) (instead of the Boolean hypercube) was recently studied by the same authors; and the related (and, again, equally well-motivated) question of <em>tolerant</em> junta testing on the Boolean hypercube was also recently studied (among other works) by the same authors. Well, this paper does it all, and tackles the challenging (and, for a change, equally well-motivated!) question of <em>tolerant</em> testing of juntas  on \(\mathbb{R}^d\).</p>



<p><strong>Differentially Private Assouad, Fano, and Le Cam</strong>, by Jayadev Acharya, Ziteng Sun, and Huanyu Zhang (<a href="https://arxiv.org/abs/2004.06830">arXiv</a>) Back to probability distributions and privacy. This paper provides differentially private analogues of the classical eponymous statistical inference results (Assouad’s lemma, Fano’s inequality, and Le Cam’s method). In particular, it gives ready-to-use, blackbox tools to prove testing and learning lower bounds for distributions in the differentially private setting, and shows how to use them to easily derive, and rederive, several lower bounds.</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1297"><span class="datestr">at May 04, 2020 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00518">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00518">Distributions of restricted rotation distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cleary:Sean.html">Sean Cleary</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nadeem:Haris.html">Haris Nadeem</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00518">PDF</a><br /><b>Abstract: </b>Rotation distances measure the differences in structure between rooted
ordered binary trees. There are no known efficient algorithms to compute
rotation distance between trees, where rotations are permitted at any node.
Limiting the allowed locations of where rotations are permitted gives rise to a
number of notions of restricted rotation distances. Allowing rotations at a
minimal such set of locations gives restricted rotation distance. There are
linear-time algorithms to compute restricted rotation distance, where there are
only two permitted locations for rotations to occur. There are linear upper and
lower bounds on restricted rotation distance with respect to the sizes of the
reduced tree pairs. Here, we experimentally investigate the expected restricted
rotation distance between two trees selected at random of increasing size and
find that it lies typically in a narrow band well within the earlier proven
linear upper and lower bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00518"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00515">The Hypervolume Indicator: Problems and Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guerreiro:Andreia_P=.html">Andreia P. Guerreiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fonseca:Carlos_M=.html">Carlos M. Fonseca</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paquete:Lu=iacute=s.html">Luís Paquete</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00515">PDF</a><br /><b>Abstract: </b>The hypervolume indicator is one of the most used set-quality indicators for
the assessment of stochastic multiobjective optimizers, as well as for
selection in evolutionary multiobjective optimization algorithms. Its
theoretical properties justify its wide acceptance, particularly the strict
monotonicity with respect to set dominance which is still unique of
hypervolume-based indicators. This paper discusses the computation of
hypervolume-related problems, highlighting the relations between them,
providing an overview of the paradigms and techniques used, a description of
the main algorithms for each problem, and a rundown of the fastest algorithms
regarding asymptotic complexity and runtime. By providing a complete overview
of the computational problems associated to the hypervolume indicator, this
paper serves as the starting point for the development of new algorithms, and
supports users in the identification of the most appropriate implementations
available for each problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00515"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00448">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00448">Optimal Join Algorithms Meet Top-k</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tziavelis:Nikolaos.html">Nikolaos Tziavelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gatterbauer:Wolfgang.html">Wolfgang Gatterbauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riedewald:Mirek.html">Mirek Riedewald</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00448">PDF</a><br /><b>Abstract: </b>Top-k queries have been studied intensively in the database community and
they are an important means to reduce query cost when only the "best" or "most
interesting" results are needed instead of the full output. While some
optimality results exist, e.g., the famous Threshold Algorithm, they hold only
in a fairly limited model of computation that does not account for the cost
incurred by large intermediate results and hence is not aligned with typical
database-optimizer cost models. On the other hand, the idea of avoiding large
intermediate results is arguably the main goal of recent work on optimal join
algorithms, which uses the standard RAM model of computation to determine
algorithm complexity. This research has created a lot of excitement due to its
promise of reducing the time complexity of join queries with cycles, but it has
mostly focused on full-output computation. We argue that the two areas can and
should be studied from a unified point of view in order to achieve optimality
in the common model of computation for a very general class of top-k-style join
queries. This tutorial has two main objectives. First, we will explore and
contrast the main assumptions, concepts, and algorithmic achievements of the
two research areas. Second, we will cover recent, as well as some older,
approaches that emerged at the intersection to support efficient ranked
enumeration of join-query results. These are related to classic work on
k-shortest path algorithms and more general optimization problems, some of
which dates back to the 1950s. We demonstrate that this line of research
warrants renewed attention in the challenging context of ranked enumeration for
general join queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00448"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00417">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00417">Improved Bound for Matching in Random-Order Streams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernstein:Aaron.html">Aaron Bernstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00417">PDF</a><br /><b>Abstract: </b>We study the problem of computing an approximate maximum cardinality matching
in the semi-streaming model when edges arrive in a \emph{random} order. In the
semi-streaming model, the edges of the input graph G = (V,E) are given as a
stream e_1, ..., e_m, and the algorithm is allowed to make a single pass over
this stream while using $O(n \textrm{polylog}(n))$ space ($m = |E|$ and $n =
|V|$). If the order of edges is adversarial, a simple single-pass greedy
algorithm yields a $1/2$-approximation in $O(n)$ space; achieving a better
approximation in adversarial streams remains an elusive open question.
</p>
<p>A line of recent work shows that one can improve upon the $1/2$-approximation
if the edges of the stream arrive in a random order. The state of the art for
this model is two-fold: Assadi et al. [SODA 2019] show how to compute a
$2/3(\sim.66)$-approximate matching, but the space requirement is $O(n^{1.5}
\textrm{polylog}(n))$. Very recently, Farhadi et al. [SODA 2020] presented an
algorithm with the desired space usage of $O(n \textrm{polylog}(n))$, but a
worse approximation ratio of $6/11(\sim.545)$, or $3/5(=.6)$ in bipartite
graphs.
</p>
<p>In this paper, we present an algorithm that computes a
$2/3(\sim.66)$-approximate matching using only $O(n \log(n))$ space, improving
upon both results above. We also note that for adversarial streams, a lower
bound of Kapralov [SODA 2013] shows that any algorithm that achieves a
$1-1/e(\sim.63)$-approximation requires $(n^{1+\Omega(1/\log\log(n))})$ space.
Our result for random-order streams is the first to go beyond the
adversarial-order lower bound, thus establishing that computing a maximum
matching is provably easier in random-order streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00417"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00198">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00198">Multi-dimensional Arrays with Levels</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Artjoms {Š}inkarovs <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00198">PDF</a><br /><b>Abstract: </b>We explore a data structure that generalises rectangular multi-dimensional
arrays. The shape of an n-dimensional array is typically given by a tuple of n
natural numbers. Each element in that tuple defines the length of the
corresponding axis. If we treat this tuple as an array, the shape of that array
is described by the single natural number n. A natural number itself can be
also treated as an array with the shape described by the natural number 1 (or
the element of any singleton set). This observation gives rise to the hierarchy
of array types where the shape of an array of level l+1 is a level-l array of
natural numbers. Such a hierarchy occurs naturally when treating arrays as
containers, which makes it possible to define both rank- and level-polymorphic
operations. The former can be found in most array languages, whereas the latter
gives rise to partial selections on a large set of hyperplanes, which is often
useful in practice. In this paper we present an Agda formalisation of arrays
with levels. We show that the proposed formalism supports standard
rank-polymorphic array operations, while type system gives static guarantees
that indexing is within bounds. We generalise the notion of ranked operator so
that it becomes applicable on arrays of arbitrary levels and we show why this
may be useful in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00198"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00168">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00168">Cutting Bamboo Down to Size</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bil=ograve=:Davide.html">Davide Bilò</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gual=agrave=:Luciano.html">Luciano Gualà</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leucci:Stefano.html">Stefano Leucci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Proietti:Guido.html">Guido Proietti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scornavacca:Giacomo.html">Giacomo Scornavacca</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00168">PDF</a><br /><b>Abstract: </b>This paper studies the problem of programming a robotic panda gardener to
keep a bamboo garden from obstructing the view of the lake by your house.
</p>
<p>The garden consists of $n$ bamboo stalks with known daily growth rates and
the gardener can cut at most one bamboo per day. As a computer scientist, you
found out that this problem has already been formalized in [G\k{a}sieniec et
al., SOFSEM'17] as the Bamboo Garden Trimming (BGT) problem, where the goal is
that of computing a perpetual schedule (i.e., the sequence of bamboos to cut)
for the robotic gardener to follow in order to minimize the makespan, i.e., the
maximum height ever reached by a bamboo.
</p>
<p>Two natural strategies are Reduce-Max and Reduce-Fastest(x). Reduce-Max trims
the tallest bamboo of the day, while Reduce-Fastest(x) trims the fastest
growing bamboo among the ones that are taller than $x$. It is known that
Reduce-Max and Reduce-Fastest(x) achieve a makespan of $O(\log n)$ and $4$ for
the best choice of $x=2$, respectively. We prove the first constant upper bound
of $9$ for Reduce-Max and improve the one for Reduce-Fastest(x) to
$\frac{3+\sqrt{5}}{2} &lt; 2.62$ for $x=1+\frac{1}{\sqrt{5}}$.
</p>
<p>Another critical aspect stems from the fact that your robotic gardener has a
limited amount of processing power and memory. It is then important for the
algorithm to be able to quickly determine the next bamboo to cut while
requiring at most linear space. We formalize this aspect as the problem of
designing a Trimming Oracle data structure, and we provide three efficient
Trimming Oracles implementing different perpetual schedules, including those
produced by Reduce-Max and Reduce-Fastest(x).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00168"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00144">An Efficient Noisy Binary Search in Graphs via Median Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dereniowski:Dariusz.html">Dariusz Dereniowski</a>, Aleksander Łukasiewicz, Przemysław Uznański <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00144">PDF</a><br /><b>Abstract: </b>Consider a generalization of the classical binary search problem in linearly
sorted data to the graph-theoretic setting. The goal is to design an adaptive
query algorithm, called a strategy, that identifies an initially unknown target
vertex in a graph by asking queries. Each query is conducted as follows: the
strategy selects a vertex $q$ and receives a reply $v$: if $q$ is the target,
then $v=q$, and if $q$ is not the target, then $v$ is a neighbor of $q$ that
lies on a shortest path to the target. Furthermore, there is a noise parameter
$0\leq p&lt;\frac{1}{2}$, which means that each reply can be incorrect with
probability $p$. The optimization criterion to be minimized is the overall
number of queries asked by the strategy, called the query complexity. The query
complexity is well understood to be $O(\varepsilon^{-2}\log n)$ for general
graphs, where $n$ is the order of the graph and $\varepsilon=\frac{1}{2}-p$.
However, implementing such a strategy is computationally expensive, with each
query requiring possibly $O(n^2)$ operations.
</p>
<p>In this work we propose two efficient strategies that keep the optimal query
complexity. The first strategy achieves the overall complexity of
$O(\varepsilon^{-1}n\log n)$ per a single query. The second strategy is
dedicated to graphs of small diameter $D$ and maximum degree $\Delta$ and has
the average complexity of $O(n+\varepsilon^{-2}D\Delta\log n)$ per query. We
stress out that we develop an algorithmic tool of graph median approximation
that is of independent interest: the median can be efficiently approximated by
finding a vertex minimizing the sum of distances to a randomly sampled vertex
subset of size $O(\varepsilon^{-2}\log n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00144"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00134">A Parameterized Approximation Scheme for Min $k$-Cut</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Saket.html">Saket Saurabh</a>, Vaishali Surianarayanan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00134">PDF</a><br /><b>Abstract: </b>In the Min $k$-Cut problem, input is an edge weighted graph $G$ and an
integer $k$, and the task is to partition the vertex set into $k$ non-empty
sets, such that the total weight of the edges with endpoints in different parts
is minimized. When $k$ is part of the input, the problem is NP-complete and
hard to approximate within any factor less than $2$. Recently, the problem has
received significant attention from the perspective of parameterized
approximation. Gupta et al.~[SODA 2018] initiated the study of
FPT-approximation for the Min $k$-Cut problem and gave an
$1.9997$-approximation algorithm running in time
$2^{\mathcal{O}(k^6)}n^{\mathcal{O}(1)}$. Later, the same set of authors~[FOCS
2018] designed an $(1 +\epsilon)$-approximation algorithm that runs in time
$(k/\epsilon)^{\mathcal{O}(k)}n^{k+\mathcal{O}(1)}$, and a $1.81$-approximation
algorithm running in time $2^{\mathcal{O}(k^2)}n^{\mathcal{O}(1)}$. More,
recently, Kawarabayashi and Lin~[SODA 2020] gave a $(5/3 +
\epsilon)$-approximation for Min $k$-Cut running in time $2^{\mathcal{O}(k^2
\log k)}n^{\mathcal{O}(1)}$.
</p>
<p>In this paper we give a parameterized approximation algorithm with best
possible approximation guarantee, and best possible running time dependence on
said guarantee (up to Exponential Time Hypothesis (ETH) and constants in the
exponent). In particular, for every $\epsilon &gt; 0$, the algorithm obtains a $(1
+\epsilon)$-approximate solution in time
$(k/\epsilon)^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$. The main ingredients of our
algorithm are: a simple sparsification procedure, a new polynomial time
algorithm for decomposing a graph into highly connected parts, and a new exact
algorithm with running time $s^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ on
unweighted (multi-) graphs. Here, $s$ denotes the number of edges in a minimum
$k$-cut. The later two are of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00134"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.00010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.00010">A Primer on Private Statistics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ullman:Jonathan.html">Jonathan Ullman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.00010">PDF</a><br /><b>Abstract: </b>Differentially private statistical estimation has seen a flurry of
developments over the last several years. Study has been divided into two
schools of thought, focusing on empirical statistics versus population
statistics. We suggest that these two lines of work are more similar than
different by giving examples of methods that were initially framed for
empirical statistics, but can be applied just as well to population statistics.
We also provide a thorough coverage of recent work in this area.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.00010"><span class="datestr">at May 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html">Hanoi vs Sierpiński</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The Hanoi graphs and Sierpiński graphs both look like the <a href="https://en.wikipedia.org/wiki/Sierpi%C5%84ski_triangle">Sierpiński triangle</a>, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The <a href="https://en.wikipedia.org/wiki/Hanoi_graph">Hanoi graphs</a> (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/hanoi-vs-sierpinski.svg" alt="Hanoi graphs and Sierpiński graphs" /></p>

<p>The difference between them comes out much more strongly when you generalize them to higher dimensions. The Sierpiński triangle generalizes to tetrahedra (a popular shape for kites) and higher-dimensional simplices; <a href="https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif">the photo below</a> shows <a href="https://en.wikipedia.org/wiki/Mabel_Gardiner_Hubbard">Mabel Bell</a> and <a href="https://en.wikipedia.org/wiki/Alexander_Graham_Bell">Alexander Graham Bell</a>, seemingly about to kiss, in a three-dimensional Sierpiński graph, the framework for a kite.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/bell-kite-kiss.jpg" alt="Mabel Bell and Alexander Graham Bell kissing in a Sierpiński tetrahedron kite frame, from https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif" /></p>

<p>Again, the -dimensional Sierpiński graph has a recursive construction from  smaller graphs of the same type, identified at extreme vertices (the vertices of degree  at the  corners of the layout). Because the number of vertices separating the subgraphs at each level of the recursion is so small, these graphs have bounded <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>, and a few years ago on the TCS stackexchange <a href="https://cstheory.stackexchange.com/q/36542">I calculated the treewidth of the Sierpiński triangle graphs explicitly as being four</a>. The same bound transfers easily enough to the three-peg Hanoi graphs.</p>

<p>The analogue of higher dimensions for the Hanoi graphs is to use more pegs. The Hanoi graph with  pegs and  rings has  states, more or less the same as the Sierpiński graph for -dimensional Sierpiński fractals with  levels of recursion. Here’s the one with two rings; each state is described by a pair of letters, using a capital letter for the peg holding the larger ring and a lowercase letter for the peg holding the smaller ring.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/Hanoi-4-2.svg" alt="Hanoi graph for two rings on four pegs" /></p>

<p>The recursive construction for these graphs combines  copies of a smaller graph of the same type: one copy for each position where the largest ring can be placed, and a smaller graph describing the placements of the smaller rings once the largest ring has been placed. These copies of the smaller graph are connected together by edges describing the movements of the largest ring. But I’ve only drawn an example for two rings because these graphs get messy and hard to draw very quickly. The reason is not the exponential number of total vertices, but the large number of connections from one recursive subgraph to another. Two recursive subgraphs are connected whenever the largest ring can move from its peg in one subgraph to its peg in the other, and this is allowed whenever these two pegs have no smaller rings on them. So in a Hanoi graph with  pegs,  rings, and  vertices, each pair of recursive subgraphs has  edges between them, one for each placement of the smaller rings on the  remaining pegs.</p>

<p>The recursive subdivision with  edges between subgraphs leads to a tree-decomposition with treewidth , and this naturally raises the question of whether this is tight or whether some other less-intuitive recursive decomposition has smaller cuts between its recursive subgraphs. This is the question studied in my newest preprint, “On the treewidth of Hanoi graphs” (<a href="https://arxiv.org/abs/2005.00179">arXiv:2005.00179</a>), with UCI student Daniel Frishberg and Oregon State student Will Maxwell, to appear at <a href="https://sites.google.com/view/fun2020/home">FUN 2020</a> (supposedly to be held in person in September in Italy after being rescheduled from June, but I’m not holding my breath). We don’t get a precise answer, but we succeed in proving bounds on the treewidth of the form <span style="white-space: nowrap;">.</span> This is nearly tight for fixed  and variable : we get the same exponential function of  as the upper bound, and are smaller than the upper bound by only a much lower-order polynomial factor. But the exact treewidth remains elusive.</p>

<p>To put it in a possibly more familiar form, when one of these graphs (for a fixed number of pegs and variable number of rings) has  vertices, it has separators of size , where . For the four-peg Hanoi graphs, this means separators of size , more or less the same as for planar graphs (although these graphs seem far from planar). But that nice exponent is just a coincidence caused by the fact that  is a power of . For other choices of , that doesn’t happen and we get a transcendental exponent . So these graphs don’t even act like -dimensional graphs, for which a reasonable separator exponent might be the rational number . And they certainly don’t act like the Sierpiński graphs, for which the exponent is zero.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104108481482094736">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html"><span class="datestr">at May 03, 2020 10:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/069">TR20-069 |  Optimal Error Pseudodistributions for Read-Once Branching Programs | 

	Eshan Chattopadhyay, 

	Jyun-Jie Liao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In a seminal work, Nisan (Combinatorica'92) constructed a pseudorandom generator for length $n$ and width $w$ read-once branching programs  with seed length $O(\log n\cdot \log(nw)+\log n\cdot\log(1/\varepsilon))$ and  error $\varepsilon$. It remains a central question  to reduce the seed length to $O(\log (nw/\varepsilon))$, which would prove that $\mathbf{BPL}=\mathbf{L}$. However, there has been no improvement on Nisan's construction for the case $n=w$, which is most relevant to space-bounded derandomization.




Recently, in a beautiful work, Braverman, Cohen and Garg (STOC'18) introduced the notion of a \emph{pseudorandom pseudo-distribution} (PRPD) and gave an explicit construction of a PRPD with seed length $\tilde{O}(\log n\cdot \log(nw)+\log(1/\varepsilon))$. A PRPD is a relaxation of a pseudorandom generator, which suffices for derandomizing $\mathbf{BPL}$ and also implies a hitting set. Unfortunately, their construction is quite involved and complicated. Hoza and Zuckerman (FOCS'18) later constructed a much simpler hitting set generator with seed length $O(\log n\cdot \log(nw)+\log(1/\varepsilon))$, but their techniques are restricted to hitting sets.

In this work, we construct a PRPD with seed length 
$$O(\log n\cdot \log (nw)\cdot \log\log(nw)+\log(1/\varepsilon)).$$
This improves upon the construction in \cite{BCG18} by a $O(\log\log(1/\varepsilon))$ factor, and is optimal in the small error regime. In addition, we believe our construction and analysis to be   simpler than the work of Braverman, Cohen and Garg.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/069"><span class="datestr">at May 03, 2020 10:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/068">TR20-068 |  One-Sided Error Testing of Monomials and Affine Subspaces | 

	Oded Goldreich, 

	Dana Ron</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the query complexity of three versions of the problem of testing monomials and affine (and linear) subspaces with one-sided error, and obtain the following results: 
\begin{itemize}
\item The general problem, in which the arity of the monomial (resp., co-dimension of the subspace) is not specified, has query complexity ${\widetilde{O}}(1/\epsilon)$, where $\epsilon$ denotes the proximity parameter. 
\item The bounded problem, in which the arity of the monomial (resp., co-dimension of the subspace) is upper bounded by a fixed parameter, has query complexity ${\widetilde{O}}(1/\epsilon)$.
\item The exact problem, in which the arity of the monomial (resp., co-dimension of the subspace) is required to equal a fixed parameter (e.g., equals~2), has query complexity ${\widetilde{\Omega}}(\log n)$, where $n$ denotes the length of the argument for the tested function.  
\end{itemize}
The running time of the testers in the positive results is linear in their query complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/068"><span class="datestr">at May 03, 2020 08:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4379">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/">Talagrand’s Generic Chaining</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 Welcome to phase two of <em>in theory</em>, in which we again talk about math. I spent last Fall teaching two courses and getting settled, I mostly traveled in January and February, and I have spent the last two months on my sofa catching up on TV series. Hence I will reach back to last Spring, when I learned about Talagrand’s machinery of generic chaining and majorizing measures from Nikhil Bansal, in the context of our work with Ola Svensson on <a href="https://arxiv.org/abs/1905.01495">graph and hypergraph sparsification</a>. Here I would like to record what I understood about the machinery, and in a follow-up post I plan to explain the application to hypergraph sparsification.</p>
<p>
<span id="more-4379"></span></p>
<p>
</p><p><b>1. A Concrete Setting </b></p>
<p></p><p>
Starting from a very concrete setting, suppose that we have a subset <img src="https://s0.wp.com/latex.php?latex=%7BT+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T \subseteq {\mathbb R}^n}" class="latex" title="{T \subseteq {\mathbb R}^n}" />, we pick a random Gaussian vector <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> from <img src="https://s0.wp.com/latex.php?latex=%7BN%28%7B%5Cbf+0%7D%2C+I%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N({\bf 0}, I)}" class="latex" title="{N({\bf 0}, I)}" />, and we are interested in the random variable</p>
<p>
<a name="eqg"></a></p><a name="eqg">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \ \ \ \ \ (1)" class="latex" title="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \ \ \ \ \ (1)" /></p>
</a><p><a name="eqg"></a> In theoretical computer science, for example, a random variable like <a href="https://lucatrevisan.wordpress.com/feed/#eqg">(1)</a> comes up often in the study of rounding algorithms for semidefinite programming, but this is a problem of much broader interest. </p>
<p>
A first observation is that each <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+g%2Ct+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle g,t \rangle}" class="latex" title="{\langle g,t \rangle}" /> is Gaussian with mean zero and variance <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+t%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| t||^2}" class="latex" title="{|| t||^2}" />. If <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is finite, we can use a union bound to estimate the tail of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t%2Cg+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sup_{t\in T} \langle t,g \rangle}" class="latex" title="{\sup_{t\in T} \langle t,g \rangle}" />, and we can compute the upper bound <a name="equb"></a></p><a name="equb">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Clog+%7CT%7C%7D+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%7C%7C+t%7C%7C+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \leq O\left(\sqrt{\log |T|} \cdot \sup_{t\in T} || t|| \right) \ \ \ \ \ (2)" class="latex" title="\displaystyle  \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \leq O\left(\sqrt{\log |T|} \cdot \sup_{t\in T} || t|| \right) \ \ \ \ \ (2)" /></p>
</a><p><a name="equb"></a> The above bound can be tight, but it is poor if the points of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> are densely clustered, and it is useless if <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is infinite. </p>
<p>
It is useful to note that, if we fix, arbitrarily, an element <img src="https://s0.wp.com/latex.php?latex=%7Bt_0+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t_0 \in T}" class="latex" title="{t_0 \in T}" />, then we have <a name="eqgtzero"></a></p><a name="eqgtzero">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+-+t_0+%5Crangle+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle = \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t - t_0 \rangle \ \ \ \ \ (3)" class="latex" title="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle = \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t - t_0 \rangle \ \ \ \ \ (3)" /></p>
</a><p><a name="eqgtzero"></a> because <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Clangle+g%2C+t_0+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E} \langle g, t_0 \rangle = 0}" class="latex" title="{\mathop{\mathbb E} \langle g, t_0 \rangle = 0}" />. The latter expression is nicer to work with because it makes it more explicit that what we are trying to compute is invariant under shifts of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, and only depends on pairwise distances of the elements of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, rather than their norm. </p>
<p>
In the cases in which <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a> gives a poor bound, a natural approach is to reason about an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-net <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T'}" class="latex" title="{T'}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, that is, a subset <img src="https://s0.wp.com/latex.php?latex=%7BT%27%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T'\subseteq T}" class="latex" title="{T'\subseteq T}" /> such that for every <img src="https://s0.wp.com/latex.php?latex=%7Bt%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t\in T}" class="latex" title="{t\in T}" /> there is an element <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%28t%29+%5Cin+T%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi(t) \in T'}" class="latex" title="{\pi(t) \in T'}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7Ct+-+%5Cpi%28t%29+%7C%7C%5Cleq+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{||t - \pi(t) ||\leq \epsilon}" class="latex" title="{||t - \pi(t) ||\leq \epsilon}" />. Then we can say that</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_t+%5Clangle+t+-+t_0%2C+g+%5Crangle+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t+-+%5Cpi%28t%29+%2C+g+%5Crangle+%2B+%5Clangle+%5Cpi%28t%29+-t_0%2C+g+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_t \langle t - t_0, g \rangle = \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle + \langle \pi(t) -t_0, g \rangle " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_t \langle t - t_0, g \rangle = \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle + \langle \pi(t) -t_0, g \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t+-+%5Cpi%28t%29+%2C+g+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D%5Clangle+%5Cpi%28t%29+-t_0%2C+g+%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \left( \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle \right) + \left( \mathop{\mathbb E} \sup_{t\in T}\langle \pi(t) -t_0, g \rangle \right) " class="latex" title="\displaystyle  \leq \left( \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle \right) + \left( \mathop{\mathbb E} \sup_{t\in T}\langle \pi(t) -t_0, g \rangle \right) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%28+%5Csqrt%7B%5Clog+%7CT%7C%7D+%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%7C%7Ct-%5Cpi%28t%29+%7C%7C+%2B+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%27%5Cin+T%27%7D%5Clangle+t%27+-t_0%2C+g+%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq O( \sqrt{\log |T|} ) \cdot \sup_{t\in T} ||t-\pi(t) || + \left( \mathop{\mathbb E} \sup_{t'\in T'}\langle t' -t_0, g \rangle \right) " class="latex" title="\displaystyle  \leq O( \sqrt{\log |T|} ) \cdot \sup_{t\in T} ||t-\pi(t) || + \left( \mathop{\mathbb E} \sup_{t'\in T'}\langle t' -t_0, g \rangle \right) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%5Cleft%28+%5Csqrt%7B%5Clog+%7CT%7C%7D+%5Ccdot+%5Cepsilon+%2B+%5Csqrt+%7B%5Clog+%7CT%27%7C%7D+%5Ccdot+%5Csup_%7Bt%27%5Cin+T%27%7D+%7C%7C+t%27+-+t_0%7C%7C+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq O\left( \sqrt{\log |T|} \cdot \epsilon + \sqrt {\log |T'|} \cdot \sup_{t'\in T'} || t' - t_0|| \right) " class="latex" title="\displaystyle  \leq O\left( \sqrt{\log |T|} \cdot \epsilon + \sqrt {\log |T'|} \cdot \sup_{t'\in T'} || t' - t_0|| \right) " /></p>
<p>
which can be a much tighter bound. Notice that we used <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a> to bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%27%7D+%5Clangle+t%27+-+t_0+%2C+g%5Crangle+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T'} \langle t' - t_0 , g\rangle \ , " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T'} \langle t' - t_0 , g\rangle \ , " /></p>
<p> but it might actually be better to find an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon'}" class="latex" title="{\epsilon'}" />-net <img src="https://s0.wp.com/latex.php?latex=%7BT%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T''}" class="latex" title="{T''}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T'}" class="latex" title="{T'}" />, and so on. In general, a tighter analysis would be to choose a sequence of nested sets <img src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+%5Csubseteq+T_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_0 \subseteq T_1 \subseteq \cdots \subseteq T_k}" class="latex" title="{T_0 \subseteq T_1 \subseteq \cdots \subseteq T_k}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BT_0+%3D+%5C%7B+t_0+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_0 = \{ t_0 \}}" class="latex" title="{T_0 = \{ t_0 \}}" />, <img src="https://s0.wp.com/latex.php?latex=%7BT_k+%3D+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_k = T}" class="latex" title="{T_k = T}" />, and we have that <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{i-1}}" class="latex" title="{T_{i-1}}" /> is an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" />-net of <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{i}}" class="latex" title="{T_{i}}" />, that is, for every element <img src="https://s0.wp.com/latex.php?latex=%7Bt_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t_{i}}" class="latex" title="{t_{i}}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_i}" class="latex" title="{T_i}" /> there is an element <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_i+%28t_i%29+%5Cin+T_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi_i (t_i) \in T_{i-1}}" class="latex" title="{\pi_i (t_i) \in T_{i-1}}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+t_i+-+%5Cpi_i+%28t_i%29+%7C%7C+%5Cleq+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| t_i - \pi_i (t_i) || \leq \epsilon_i}" class="latex" title="{|| t_i - \pi_i (t_i) || \leq \epsilon_i}" />. Then, by generalizing the above reasoning, we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t-t_0%2Cg+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+%5Ccdot+%5Csup_%7Bt_i+%5Cin+T_i%7D+%7C%7C+t_i+-+%5Cpi_i%28t_i%29+%7C%7C+%5Cleq+O%281%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+%5Cepsilon_i+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sum_{i=1}^k \sqrt{\log |T_i | } \cdot \sup_{t_i \in T_i} || t_i - \pi_i(t_i) || \leq O(1) \cdot \sum_{i=1}^n \epsilon_i \sqrt{\log |T_i | } " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sum_{i=1}^k \sqrt{\log |T_i | } \cdot \sup_{t_i \in T_i} || t_i - \pi_i(t_i) || \leq O(1) \cdot \sum_{i=1}^n \epsilon_i \sqrt{\log |T_i | } " /></p>
<p>
Finally, if the cardinality of the <img src="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_i}" class="latex" title="{T_i}" /> grows sufficiently fast, namely, if we have <img src="https://s0.wp.com/latex.php?latex=%7B%7CT_i%7C+%3E+%7CT_%7Bi-1%7D%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|T_i| &gt; |T_{i-1}|^2}" class="latex" title="{|T_i| &gt; |T_{i-1}|^2}" />, it is possible to refine the estimate to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t-t_0%2Cg+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+%5Ccdot+%7C%7C+t+-+%5Cpi_i+%28t%29+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{i=1}^k \sqrt{\log |T_i | } \cdot || t - \pi_i (t) || " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{i=1}^k \sqrt{\log |T_i | } \cdot || t - \pi_i (t) || " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_i%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi_i(t)}" class="latex" title="{\pi_i(t)}" /> is the closest element to <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{i-1}}" class="latex" title="{T_{i-1}}" />. This is done by avoiding to write the expectation of the sup of a sum as a sum of expectations of sups and then using <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a>, but by bounding the tail of the sup of the sum directly.</p>
<p>
At this point, we do not even need to assume that <img src="https://s0.wp.com/latex.php?latex=%7BT_k%3D+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_k= T}" class="latex" title="{T_k= T}" />, that <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is finite, or that the sequence of sets is finite, and we have the following result.</p>
<blockquote><p><b>Theorem 1 (Talagrand’s generic chaining inequality)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BT+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T \subseteq {\mathbb R}^n}" class="latex" title="{T \subseteq {\mathbb R}^n}" /> be an arbitrary set, let <img src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+T_k+%5Csubseteq+%5Ccdots+%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" class="latex" title="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" /> be a countable sequence of finite subsets of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%7CT_0%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|T_0| = 1}" class="latex" title="{|T_0| = 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CT_k%7C+%5Cleq+2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|T_k| \leq 2^{2^k}}" class="latex" title="{|T_k| \leq 2^{2^k}}" />. Then </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t%2C+g+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bk%3D1%7D%5E%5Cinfty+2%5E%7Bk%2F2%7D+%7C%7C+t+-+%5Cpi_k%28t%29%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t, g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} || t - \pi_k(t)|| " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t, g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} || t - \pi_k(t)|| " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi_k(t)}" class="latex" title="{\pi_k(t)}" /> is the element of <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{k-1}}" class="latex" title="{T_{k-1}}" /> closest to <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. </em></p></blockquote>
<p></p><p>
A short complete proof is in these <a href="https://tcsmath.wordpress.com/2010/06/15/the-generic-chaining/">notes by James Lee</a>.</p>
<p>
While the above Theorem has a very simple proof, the amazing thing, which is rather harder to prove, is that it is <em>tight</em>, in the sense that for every <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> there is a sequence of sets such that the bound of the above theorem has a matching lower bound, up to an absolute constant. This is why it is called <em>generic chaining</em>. <em>Chaining</em> because the projection <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+t-t_0%2Cg%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle t-t_0,g\rangle}" class="latex" title="{\langle t-t_0,g\rangle}" /> of <img src="https://s0.wp.com/latex.php?latex=%7Bt-t_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-t_0}" class="latex" title="{t-t_0}" /> on <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> is estimated based on the “chain” </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_k+%5Clangle+%5Cpi_%7Bk%2B1%7D+%28t%29+-+%5Cpi_k%28t%29+%2C+g+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_k \langle \pi_{k+1} (t) - \pi_k(t) , g \rangle" class="latex" title="\displaystyle  \sum_k \langle \pi_{k+1} (t) - \pi_k(t) , g \rangle" /></p>
<p> of projections of the intermediate steps of a path that goes from <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bt_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t_0}" class="latex" title="{t_0}" /> passing through the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi_k(t)}" class="latex" title="{\pi_k(t)}" />. <em>Generic</em> because this upper bound technique works as well as any other possible upper bound, up to an absolute constant.</p>
<p>
</p><p><b>2. An Abstract Setting </b></p>
<p></p><p>
Let now <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> be a completely arbitrary set, and suppose that we have a distribution <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" /> over functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+T+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f: T \rightarrow {\mathbb R}}" class="latex" title="{f: T \rightarrow {\mathbb R}}" /> and we want to upper bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7BF+%5Csim+%7B%5Ccal+D%7D%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D+%5C+F%28t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}_{F \sim {\cal D}} \ \ \sup_{t\in T} \ F(t) " class="latex" title="\displaystyle  \mathop{\mathbb E}_{F \sim {\cal D}} \ \ \sup_{t\in T} \ F(t) " /></p>
<p> That is, we have a random optimization problem with a fixed feasible set <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, and we want to know the typical value of the optimum. For example, <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> could be the set of cuts of a vertex set <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" /> describe a distribution of random graphs <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t)}" class="latex" title="{F(t)}" /> is the number of edges cut in a random graph by the cut <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. Then the above problem is to estimate the average value of the max cut in the random graphs of the distribution. Or <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> could be the unit sphere <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x+%5Cin+%7B%5Cmathbb+R%7D%5EN+%3A+%7C%7Cx%7C%7C+%3D+1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ x \in {\mathbb R}^N : ||x|| = 1 \}}" class="latex" title="{\{ x \in {\mathbb R}^N : ||x|| = 1 \}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" /> describe a distribution of random Hermitian matrices such that <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t)}" class="latex" title="{F(t)}" /> is the quadratic form of a random matrix evaluated at <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. In this case, the above problem is to estimate the average value of the largest eigenvalue of such a random matrix.</p>
<p>
We will call the collection of random variables <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> a <em>random process</em>, where <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is a random variable distributed according to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" />.</p>
<p>
If every <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t)}" class="latex" title="{F(t)}" />, and every finite linear combination <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5Ek+%5Calpha_i+F%28t_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^k \alpha_i F(t_i)}" class="latex" title="{\sum_{i=1}^k \alpha_i F(t_i)}" />, has a Gaussian distribution, then we say that <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a <em>Gaussian process</em>, and if, in addition, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+F%28t%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E} F(t) = 0}" class="latex" title="{\mathop{\mathbb E} F(t) = 0}" /> for every <img src="https://s0.wp.com/latex.php?latex=%7Bt%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t\in T}" class="latex" title="{t\in T}" /> then we say that it is a <em>centered Gaussian process</em>.</p>
<p>
If <img src="https://s0.wp.com/latex.php?latex=%7BT%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T\subseteq {\mathbb R}^n}" class="latex" title="{T\subseteq {\mathbb R}^n}" />, and we define <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3D+%5Clangle+t%2Cg%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t) = \langle t,g\rangle}" class="latex" title="{F(t) = \langle t,g\rangle}" /> for a random standard Gaussian <img src="https://s0.wp.com/latex.php?latex=%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2CI%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g\sim N({\bf 0},I)}" class="latex" title="{g\sim N({\bf 0},I)}" />, then <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a centered Gaussian process and, in this case, upper bounding <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+F%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E} \sup F(t)}" class="latex" title="{\mathop{\mathbb E} \sup F(t)}" /> is precisely the problem we studied before. </p>
<p>
If <img src="https://s0.wp.com/latex.php?latex=%7BT%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T\subseteq {\mathbb R}^n}" class="latex" title="{T\subseteq {\mathbb R}^n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3D+%5Clangle+t%2Cg+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t) = \langle t,g \rangle}" class="latex" title="{F(t) = \langle t,g \rangle}" /> for a random standard Gaussian <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, then, for every <img src="https://s0.wp.com/latex.php?latex=%7Bt_1%2Ct_2+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t_1,t_2 \in T}" class="latex" title="{t_1,t_2 \in T}" />, we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft%28F%28t_1%29+-+F%28t_2%29%5Cright%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bg+%5Csim+N%28%7B%5Cbf+0%7D%2CI%29%7D+%5C+%5C+%5Clangle+t_1-t_2%2Cg+%5Crangle%5E2+%3D+%7C%7Ct_1+-+t_2+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \left(F(t_1) - F(t_2)\right)^2 = \mathop{\mathbb E}_{g \sim N({\bf 0},I)} \ \ \langle t_1-t_2,g \rangle^2 = ||t_1 - t_2 ||^2 " class="latex" title="\displaystyle  \mathop{\mathbb E} \left(F(t_1) - F(t_2)\right)^2 = \mathop{\mathbb E}_{g \sim N({\bf 0},I)} \ \ \langle t_1-t_2,g \rangle^2 = ||t_1 - t_2 ||^2 " /></p>
<p> and, by analogy, if <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a centered Gaussian process, we will define the following distance function on <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d%28t_1%2Ct_2+%29+%3A%3D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28F%28t_1%29+-+F%28t_2%29%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  d(t_1,t_2 ) := \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2)^2} " class="latex" title="\displaystyle  d(t_1,t_2 ) := \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2)^2} " /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a centered Gaussian process then one can prove that the above distance function is a semi-metric on <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />.</p>
<p>
We will not need this fact, but if <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a centered Gaussian process and <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is finite, then there is an embedding <img src="https://s0.wp.com/latex.php?latex=%7Bh%3A+T+%5Crightarrow+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h: T \rightarrow {\mathbb R}^n}" class="latex" title="{h: T \rightarrow {\mathbb R}^n}" />, for some <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, such that the process can be equivalently defined as picking <img src="https://s0.wp.com/latex.php?latex=%7Bg%5Csim+N%28%7B%5Cbf+0%7D+%2C+I%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g\sim N({\bf 0} , I)}" class="latex" title="{g\sim N({\bf 0} , I)}" /> and setting <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3A%3D+%5Clangle+h%28t%29%2C+g+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t) := \langle h(t), g \rangle}" class="latex" title="{F(t) := \langle h(t), g \rangle}" />, so that <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h}" class="latex" title="{h}" /> is also an isometric embedding of the above distance function into <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2}" class="latex" title="{\ell_2}" />.</p>
<p>
The arguments of the previous section apply to centered Gaussian processes without change, and so we have.</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> be an arbitrary set, and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> be a centered Gaussian process. Let <img src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+T_k+%5Csubseteq+%5Ccdots+%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" class="latex" title="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" /> be a countable sequence of finite subsets of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%7CT_0%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|T_0| = 1}" class="latex" title="{|T_0| = 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CT_k%7C+%5Cleq+2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|T_k| \leq 2^{2^k}}" class="latex" title="{|T_k| \leq 2^{2^k}}" />. Then </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+F%28t%29+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bk%3D1%7D%5E%5Cinfty+2%5E%7Bk%2F2%7D+d+%28t+%2C+%5Cpi_k%28t%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} F(t) \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} d (t , \pi_k(t)) " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} F(t) \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} d (t , \pi_k(t)) " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7Bd%28%5Ccdot%2C%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d(\cdot,\cdot)}" class="latex" title="{d(\cdot,\cdot)}" /> is the distance function <img src="https://s0.wp.com/latex.php?latex=%7Bd%28t_1+%2C+t_2%29+%3D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28F%28t_1%29+-+F%28t_2%29%29%5E2+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d(t_1 , t_2) = \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2 }}" class="latex" title="{d(t_1 , t_2) = \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2 }}" />and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi_k(t)}" class="latex" title="{\pi_k(t)}" /> is the element of <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{k-1}}" class="latex" title="{T_{k-1}}" /> closest to <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> according to <img src="https://s0.wp.com/latex.php?latex=%7Bd%28%5Ccdot%2C%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d(\cdot,\cdot)}" class="latex" title="{d(\cdot,\cdot)}" />. </em></p></blockquote>
<p>
</p><p><b>3. Sub-Gaussian Random Processes </b></p>
<p></p><p>
This theory does not seem to apply to problems such as bounding the max cut of a <img src="https://s0.wp.com/latex.php?latex=%7BG_%7Bn%2Cp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G_{n,p}}" class="latex" title="{G_{n,p}}" /> unweighted graph, or bounding the largest eigenvalue of a random symmetric matrix with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 1}" class="latex" title="{\pm 1}" /> entries, because such problems have a finite sample space and so cannot be modeled as Gaussian processes.</p>
<p>
Fortunately, there is a notion of a <em>sub-Gaussian</em> process, which applies to such problems and which reduces their analysis to the analysis of a related Gaussian process. </p>
<p>
First, recall that a centered real-valued random variable <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is <em>sub-Gaussian</em> if there is a centered Gaussian random variable whose tail dominates the tail of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />, that is, if we have two constants <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> such that, for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5B+X+%5Cgeq+t+%5D+%5Cleq+k+%5Ccdot+e%5E%7B-t%5E2+%2Fv+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Pr [ X \geq t ] \leq k \cdot e^{-t^2 /v } " class="latex" title="\displaystyle  \Pr [ X \geq t ] \leq k \cdot e^{-t^2 /v } " /></p>
<p>
An equivalent condition is that there is a <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> such that</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7BX%5E2+%2F+v%7D+%3D+O%281%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} e^{X^2 / v} = O(1) " class="latex" title="\displaystyle  \mathop{\mathbb E} e^{X^2 / v} = O(1) " /></p>
<p>
In that case, we can define a norm, called the <img src="https://s0.wp.com/latex.php?latex=%7B%5CPsi_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Psi_2}" class="latex" title="{\Psi_2}" /> norm as</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+X+%7C%7C_%7B%5CPsi_2%7D+%3A%3D+%5C%7B+%5Cinf+%5Csigma+%3A+%5Cmathop%7B%5Cmathbb+E%7D%5E%7BX%5E2+%2F+%5Csigma%5E2%7D+%5Cleq+2+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || X ||_{\Psi_2} := \{ \inf \sigma : \mathop{\mathbb E}^{X^2 / \sigma^2} \leq 2 \} " class="latex" title="\displaystyle  || X ||_{\Psi_2} := \{ \inf \sigma : \mathop{\mathbb E}^{X^2 / \sigma^2} \leq 2 \} " /></p>
<p>
which is, roughly, the standard deviation of a centered Gaussian that dominates <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />.</p>
<blockquote><p><b>Example 1</b> <em> All bounded random variables are sub-Gaussian. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 2</b> <em> If </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  X = \sum_i X_i " class="latex" title="\displaystyle  X = \sum_i X_i " /></p>
</em><p><em> where the <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> are independent Rademacher random variables, that is, if each <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> is equally likely to be +1 or -1, then <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is sub-Gaussian with <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X%7C%7C_%7B%5CPsi_2%7D+%3D+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| X||_{\Psi_2} = O(\sqrt n)}" class="latex" title="{|| X||_{\Psi_2} = O(\sqrt n)}" />, which is within a constant factor of its actual standard deviation. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 3</b> <em> If </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  X = \sum_i X_i " class="latex" title="\displaystyle  X = \sum_i X_i " /></p>
</em><p><em> where the <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> are independent and each <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> has probability <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> of being equal to <img src="https://s0.wp.com/latex.php?latex=%7B1-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-p}" class="latex" title="{1-p}" /> and probability <img src="https://s0.wp.com/latex.php?latex=%7B1-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-p}" class="latex" title="{1-p}" /> of being equal to <img src="https://s0.wp.com/latex.php?latex=%7B-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-p}" class="latex" title="{-p}" /> (that is, each <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> is a centered Bernoulli random variable), then <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X%7C%7C_%7B%5CPsi_2%7D+%3D+%5COmega+%5Cleft%28+%5Csqrt+%7B%5Cfrac+%7Bn%7D+%7B%5Clog+1%2Fp+%7D+%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| X||_{\Psi_2} = \Omega \left( \sqrt {\frac {n} {\log 1/p } } \right)}" class="latex" title="{|| X||_{\Psi_2} = \Omega \left( \sqrt {\frac {n} {\log 1/p } } \right)}" />, which is much more than the standard deviation of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> when <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> is small. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 4</b> <em> If </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+a_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  X = \sum_i a_i X_i " class="latex" title="\displaystyle  X = \sum_i a_i X_i " /></p>
</em><p><em> where the <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> are independent Rademacher random variables, and the <img src="https://s0.wp.com/latex.php?latex=%7Ba_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a_i}" class="latex" title="{a_i}" /> are arbitrary real scalars, then <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X+%7C%7C_%7B%5CPsi_2%7D+%3D+O%28%5Csqrt+%7B%5Csum_i+a_i%5E2+%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| X ||_{\Psi_2} = O(\sqrt {\sum_i a_i^2 })}" class="latex" title="{|| X ||_{\Psi_2} = O(\sqrt {\sum_i a_i^2 })}" />, which is within a constant factor of the standard deviation that we would get by replacing each <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> with a standard Gaussian. </em></p></blockquote>
<p></p><p>
Let now <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> be a centered random process. We will say that a Gaussian process <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ \hat F(t) \}_{t\in T} }" class="latex" title="{\{ \hat F(t) \}_{t\in T} }" /> <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />-dominates <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7Bt_1%2Ct_2+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t_1,t_2 \in T}" class="latex" title="{t_1,t_2 \in T}" /> we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%28t_1%29+-+F%28t_2%29+%7C%7C_%7B%5CPsi_2%7D+%5Cleq+K+%5Ccdot+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft+%28%5Chat+F%28t_1%29+-+%5Chat+F%28t_2%29+%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || F(t_1) - F(t_2) ||_{\Psi_2} \leq K \cdot \sqrt{\mathop{\mathbb E} \left (\hat F(t_1) - \hat F(t_2) \right)^2 }" class="latex" title="\displaystyle  || F(t_1) - F(t_2) ||_{\Psi_2} \leq K \cdot \sqrt{\mathop{\mathbb E} \left (\hat F(t_1) - \hat F(t_2) \right)^2 }" /></p>
<p>
That is, every random variable of the form <img src="https://s0.wp.com/latex.php?latex=%7BF%28t_1%29+-+F%28t_2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t_1) - F(t_2)}" class="latex" title="{F(t_1) - F(t_2)}" /> is sub-Gaussian, and its tail is dominated by the tail of the Gaussian distribution <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%28t_1%29+-+%5Chat+F%28t_2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F(t_1) - \hat F(t_2)}" class="latex" title="{\hat F(t_1) - \hat F(t_2)}" /></p>
<blockquote><p><b>Theorem 3 (Talagrand’s comparison inequality)</b> <em> There is an absolute constant <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> such that if <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> is a centered random process that is <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />-dominated by a centered Gaussian random process <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ \hat F(t) \}_{t\in T} }" class="latex" title="{\{ \hat F(t) \}_{t\in T} }" />, then </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5C+F%28t%29+%5Cleq+CK+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Chat+F%28t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \ F(t) \leq CK \mathop{\mathbb E} \sup_{t\in T} \hat F(t) " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \ F(t) \leq CK \mathop{\mathbb E} \sup_{t\in T} \hat F(t) " /></p>
<p> Furthermore, for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell \geq 0}" class="latex" title="{\ell \geq 0}" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5Cleft%5B+%5Csup_%7Bt_1%2Ct_2+%5Cin+T%7D+F%28t_1%29+-+F%28t_2%29+%5Cgeq+CK+%5C+%5Cleft%28+%5Cell+%5Ccdot+diam%28T%29+%2B+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Chat+F%28t%29+%5Cright%29+%5Cright%5D+%5Cleq+2+e%5E%7B-%5Cell%5E2%7D+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Pr \left[ \sup_{t_1,t_2 \in T} F(t_1) - F(t_2) \geq CK \ \left( \ell \cdot diam(T) + \mathop{\mathbb E} \sup_{t\in T} \hat F(t) \right) \right] \leq 2 e^{-\ell^2} \ , " class="latex" title="\displaystyle  \Pr \left[ \sup_{t_1,t_2 \in T} F(t_1) - F(t_2) \geq CK \ \left( \ell \cdot diam(T) + \mathop{\mathbb E} \sup_{t\in T} \hat F(t) \right) \right] \leq 2 e^{-\ell^2} \ , " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7Bdiam%28T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{diam(T)}" class="latex" title="{diam(T)}" /> is the diameter of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> with respect to the distance function <img src="https://s0.wp.com/latex.php?latex=%7Bd%28s%2Ct%29+%3A%3D+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft+%28%5Chat+F%28s%29+-+%5Chat+F%28t%29+%5Cright%29%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d(s,t) := \sqrt{\mathop{\mathbb E} \left (\hat F(s) - \hat F(t) \right)^2}}" class="latex" title="{d(s,t) := \sqrt{\mathop{\mathbb E} \left (\hat F(s) - \hat F(t) \right)^2}}" />. </em></p></blockquote>
<p></p><p>
The way to apply this theory is the following. </p>
<p>
Suppose that we want estimate, on average or with high probability, the optimum of an optimization problem with feasible set <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> over the randomness of the choice of a random instance. We model this problem like a centered random process <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ F(t) \}_{t\in T}}" class="latex" title="{\{ F(t) \}_{t\in T}}" /> in which <img src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(t)}" class="latex" title="{F(t)}" /> is the difference between the cost of solution <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> in a random instance minus the average cost of <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />.</p>
<p>
Then we think about a related random experiment, in which the random choices involved in constructing our instance are replaced by Gaussian choices (for example, instead of a <img src="https://s0.wp.com/latex.php?latex=%7BG_%7Bn%2C1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G_{n,1/2}}" class="latex" title="{G_{n,1/2}}" /> random graph we may think of a complete graph with Gaussian weights on the edges chosen with expectation 1/2 and constant variance) and we let <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ \hat F(t) \}_{t\in T}}" class="latex" title="{\{ \hat F(t) \}_{t\in T}}" /> be the analogous process in this Gaussian model.</p>
<p>
If we can argue that <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F}" class="latex" title="{\hat F}" /> dominates <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, then it remains to estimate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E} \sup \hat F}" class="latex" title="{\mathop{\mathbb E} \sup \hat F}" />, which we can do either by the generic chaining theorem or by other methods.</p>
<p>
</p><p><b>4. An Example </b></p>
<p></p><p>
We will now use this machinery to show that the largest eigenvalue of a random symmetric <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\times n}" class="latex" title="{n\times n}" /> matrix with Rademacher entries is <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt n)}" class="latex" title="{O(\sqrt n)}" />. This is certainly not the simplest way of proving such a result, but it will give a sense of how these techniques can be applied.</p>
<p>
We let <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x%5Cin+%7B%5Cmathbb+R%7D%5En+%3A+%7C%7Cx+%7C%7C%3D1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = \{ x\in {\mathbb R}^n : ||x ||=1 \}}" class="latex" title="{T = \{ x\in {\mathbb R}^n : ||x ||=1 \}}" /> be the unit sphere. </p>
<p>
Our Gaussian process will be to pick standard Gaussians <img src="https://s0.wp.com/latex.php?latex=%7Bg_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g_{i,j}}" class="latex" title="{g_{i,j}}" />, for each <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+j+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \leq i \leq j \leq n}" class="latex" title="{1 \leq i \leq j \leq n}" />, define the matrix <img src="https://s0.wp.com/latex.php?latex=%7B+%5Chat+M_%7Bi%2Cj%7D+%3D+%5Chat+M_%7Bj%2Ci%7D+%3D+g_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ \hat M_{i,j} = \hat M_{j,i} = g_{i,j}}" class="latex" title="{ \hat M_{i,j} = \hat M_{j,i} = g_{i,j}}" /> and let</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+F%28x%29+%3D+x%5ET+%5Chat+M+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \hat F(x) = x^T \hat M x " class="latex" title="\displaystyle  \hat F(x) = x^T \hat M x " /></p>
<p> for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in T}" class="latex" title="{x\in T}" />.</p>
<p>
Our “sub-Gaussian” random process is to pick Rademacher random variables <img src="https://s0.wp.com/latex.php?latex=%7Br_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r_{i,j}}" class="latex" title="{r_{i,j}}" />, for each <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+j+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \leq i \leq j \leq n}" class="latex" title="{1 \leq i \leq j \leq n}" />, define the matrix <img src="https://s0.wp.com/latex.php?latex=%7B+M_%7Bi%2Cj%7D+%3D+M_%7Bj%2Ci%7D+%3D+r_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ M_{i,j} = M_{j,i} = r_{i,j}}" class="latex" title="{ M_{i,j} = M_{j,i} = r_{i,j}}" /> and let</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+%3D+x%5ET+M+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x) = x^T M x " class="latex" title="\displaystyle  F(x) = x^T M x " /></p>
<p> for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in T}" class="latex" title="{x\in T}" />.</p>
<p>
We will argue that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />-dominated by <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F}" class="latex" title="{\hat F}" /> and that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5Chat+F+%3D+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E} \sup \hat F = O(\sqrt n)}" class="latex" title="{\mathop{\mathbb E} \sup \hat F = O(\sqrt n)}" />.</p>
<p>
For the first claim, we see that for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y \in T}" class="latex" title="{x,y \in T}" />, we can write <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%29-F%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x)-F(y)}" class="latex" title="{F(x)-F(y)}" /> as</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+-+F%28y%29+%3D+%5Csum_%7Bi%2Cj%7D+M_%7Bi%2Cj%7D+%28x_ix_j+-+y_iy_j%29+%3D+2+%5Csum_%7Bi%3Cj%7D+r_%7Bi%2Cj%7D+%28x_ix_j+-+y_iy_j%29+%2B+%5Csum_i+r_%7Bi%2Ci%7D+%28x_i%5E2+-+y_i%5E2%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x) - F(y) = \sum_{i,j} M_{i,j} (x_ix_j - y_iy_j) = 2 \sum_{i&lt;j} r_{i,j} (x_ix_j - y_iy_j) + \sum_i r_{i,i} (x_i^2 - y_i^2) " class="latex" title="\displaystyle  F(x) - F(y) = \sum_{i,j} M_{i,j} (x_ix_j - y_iy_j) = 2 \sum_{i&lt;j} r_{i,j} (x_ix_j - y_iy_j) + \sum_i r_{i,i} (x_i^2 - y_i^2) " /></p>
<p>
So, as noted in one of our examples above, we can say that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%28x%29+-+F%28y%29+%7C%7C%5E2_%7B%5CPsi_2%7D+%5Cleq+O+%5Cleft%28+4+%5Csum_%7Bi%3Cj%7D+%28x_ix_j+-+y_iy_j%29+%5E2+%2B+%5Csum_i+%28x_i%5E2+-+y_i%5E2%29%5E2+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} \leq O \left( 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 \right) " class="latex" title="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} \leq O \left( 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 \right) " /></p>
<p> and we see that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%28+%5Chat+F%28x%29+-+%5Chat+F%28y%29+%29%5E2+%3D+4+%5Csum_%7Bi%3Cj%7D+%28x_ix_j+-+y_iy_j%29+%5E2+%2B+%5Csum_i+%28x_i%5E2+-+y_i%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} ( \hat F(x) - \hat F(y) )^2 = 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 " class="latex" title="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} ( \hat F(x) - \hat F(y) )^2 = 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 " /></p>
<p> so that, indeed, <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />-dominated by <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F}" class="latex" title="{\hat F}" />.</p>
<p>
Now we need to apply generic chaining to <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. It is very helpful to note that the distance function defined on <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> by the Gaussian process is dominated by Euclidean distance between the vectors <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, because</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%5Cleq+2+%7C%7C+xx%5ET+-+yy%5ET+%7C%7C%5E2_F+%5Cleq+2+%5Ccdot+%28%7C%7Cx%7C%7C+%2B+%7C%7Cy%7C%7C%29%5E2+%5Ccdot+%7C%7Cx-y%7C%7C%5E2+%3D+8+%7C%7Cx-y%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (d(x,y))^2 \leq 2 || xx^T - yy^T ||^2_F \leq 2 \cdot (||x|| + ||y||)^2 \cdot ||x-y||^2 = 8 ||x-y||^2 " class="latex" title="\displaystyle  (d(x,y))^2 \leq 2 || xx^T - yy^T ||^2_F \leq 2 \cdot (||x|| + ||y||)^2 \cdot ||x-y||^2 = 8 ||x-y||^2 " /></p>
<p> where we used the inequality </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+xx%5ET+-+yy%5ET+%7C%7C_F+%5Cleq+%28+%7C%7Cx+%7C%7C+%2B+%7C%7Cy%7C%7C%29+%5Ccdot+%7C%7Cx-y%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || xx^T - yy^T ||_F \leq ( ||x || + ||y||) \cdot ||x-y|| " class="latex" title="\displaystyle  || xx^T - yy^T ||_F \leq ( ||x || + ||y||) \cdot ||x-y|| " /></p>
<p>
We can conclude that an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-net over the unit Euclidean sphere is also a <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7B8%7D%5Ccdot+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt {8}\cdot \epsilon}" class="latex" title="{\sqrt {8}\cdot \epsilon}" />-net for the metric space <img src="https://s0.wp.com/latex.php?latex=%7B%28T%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(T,d)}" class="latex" title="{(T,d)}" />. For the unit Euclidean sphere there is an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-net of size at most <img src="https://s0.wp.com/latex.php?latex=%7B%283%2F%5Cepsilon%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3/\epsilon)^n}" class="latex" title="{(3/\epsilon)^n}" />. To apply generic chaining, let <img src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_k}" class="latex" title="{T_k}" /> be an arbitrary subset of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{2^k}}" class="latex" title="{2^{2^k}}" /> if <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ek+%3C+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^k &lt; n}" class="latex" title="{2^k &lt; n}" />, and an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-net with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+3%5Ccdot+2%5E%7B-2%5Ek+%2F+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = 3\cdot 2^{-2^k / n}}" class="latex" title="{\epsilon = 3\cdot 2^{-2^k / n}}" /> otherwise. Applying the generic chaining inequality,</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5C+%5Chat+F+%5Cleq+O%281%29+%5Ccdot+%5Csum_k+2%5E%7Bk%2F2%7D+%5Ccdot+%5Csqrt%7B8%7D+%5Ccdot+%5Cmin+%5Cleft%5C%7B+2%2C+3%5Ccdot+2%5E%7B-2%5Ek+%2F+n%7D+%5Cright%5C%7D+%3D+O%28%5Csqrt+n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \sup \ \hat F \leq O(1) \cdot \sum_k 2^{k/2} \cdot \sqrt{8} \cdot \min \left\{ 2, 3\cdot 2^{-2^k / n} \right\} = O(\sqrt n) " class="latex" title="\displaystyle  \mathop{\mathbb E} \sup \ \hat F \leq O(1) \cdot \sum_k 2^{k/2} \cdot \sqrt{8} \cdot \min \left\{ 2, 3\cdot 2^{-2^k / n} \right\} = O(\sqrt n) " /></p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/"><span class="datestr">at May 03, 2020 05:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/067">TR20-067 |  Computational and proof complexity of partial string avoidability | 

	Dmitry Itsykson, 

	Alexander Okhotin, 

	Vsevolod Oparin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The partial string avoidability problem is stated as follows: given a finite set of strings with possible ``holes'' (wildcard symbols), determine whether there exists a two-sided infinite string containing no substrings from this set, assuming that a hole matches every symbol. The problem is known to be NP-hard and in PSPACE, and this paper establishes its PSPACE-completeness. Next, string avoidability over the binary alphabet is interpreted as a version of conjunctive normal form satisfiability problem (SAT), where each clause has infinitely many shifted variants. Non-satisfiability of these formulas can be proved using variants of classical propositional proof systems, augmented with derivation rules for shifting proof lines
(such as clauses, inequalities, polynomials, etc). First, it is proved that there is a particular formula that has a short refutation in Resolution with a shift rule, but requires classical proofs of exponential size At the same time, it is shown that exponential lower bounds for classical proof systems can be translated for their shifted versions. Finally, it is shown that superpolynomial lower bounds on the size of shifted proofs would separate NP from PSPACE; a connection to lower bounds on circuit complexity is also established.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/067"><span class="datestr">at May 03, 2020 11:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/066">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/066">TR20-066 |  Quantum Implications of Huang&amp;#39;s Sensitivity Theorem | 

	Scott Aaronson, 

	Shalev Ben-David, 

	Robin Kothari, 

	Avishay Tal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function $f$, the deterministic query complexity, $D(f)$, is at most quartic in the quantum query complexity, $Q(f)$: $D(f) = O(Q(f)^4)$. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017). We also use the result to resolve the quantum analogue of the Aanderaa-Karp-Rosenberg conjecture. We show that if $f$ is a nontrivial monotone graph property of an $n$-vertex graph specified by its adjacency matrix, then $Q(f) = \Omega(n)$, which is also optimal.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/066"><span class="datestr">at May 03, 2020 11:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5494">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/05/03/math-summer-programs/">Math Summer Programs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The virus is causing some math summer programs to cancel. Surprisingly, this led to something wonderful. Unusually strong undergrads are starting to run their own online summer math programs for high school students. 1. The MORPH program is run by the Harvard math club. It offers a variety of mathematical topics at different levels of […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/05/03/math-summer-programs/"><span class="datestr">at May 03, 2020 01:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.15009">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.15009">From communication complexity to an entanglement spread area law in the ground state of gapped local Hamiltonians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anshu:Anurag.html">Anurag Anshu</a>, Aram W. Harrow, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soleimanifar:Mehdi.html">Mehdi Soleimanifar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.15009">PDF</a><br /><b>Abstract: </b>In this work, we make a connection between two seemingly different problems.
The first problem involves characterizing the properties of entanglement in the
ground state of gapped local Hamiltonians, which is a central topic in quantum
many-body physics. The second problem is on the quantum communication
complexity of testing bipartite states with EPR assistance, a well-known
question in quantum information theory. We construct a communication protocol
for testing (or measuring) the ground state and use its communication
complexity to reveal a new structural property for the ground state
entanglement. This property, known as the entanglement spread, roughly measures
the ratio between the largest and the smallest Schmidt coefficients across a
cut in the ground state. Our main result shows that gapped ground states
possess limited entanglement spread across any cut, exhibiting an "area law"
behavior. Our result quite generally applies to any interaction graph with an
improved bound for the special case of lattices. This entanglement spread area
law includes interaction graphs constructed in [Aharonov et al., FOCS'14] that
violate a generalized area law for the entanglement entropy. Our construction
also provides evidence for a conjecture in physics by Li and Haldane on the
entanglement spectrum of lattice Hamiltonians [Li and Haldane, PRL'08]. On the
technical side, we use recent advances in Hamiltonian simulation algorithms
along with quantum phase estimation to give a new construction for an
approximate ground space projector (AGSP) over arbitrary interaction graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.15009"><span class="datestr">at May 03, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14995">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14995">Using Decision Diagrams to Compactly Represent the State Space for Explicit Model Checking</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Hao.html">Hao Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Andrew.html">Andrew Price</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Myers:Chris.html">Chris Myers</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14995">PDF</a><br /><b>Abstract: </b>The enormous number of states reachable during explicit model checking is the
main bottleneck for scalability. This paper presents approaches of using
decision diagrams to represent very large state space compactly and
efficiently. This is possible for asynchronous systems as two system states
connected by a transition often share many same local portions. Using decision
diagrams can significantly reduce memory demand by not using memory to store
the redundant information among different states. This paper considers
multi-value decision diagrams for this purpose. Additionally, a technique to
reduce the runtime overhead of using these diagrams is also described.
Experimental results and comparison with the state compression method as
implemented in the model checker SPIN show that the approaches presented in
this paper are memory efficient for storing large state space with acceptable
runtime overhead.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14995"><span class="datestr">at May 03, 2020 10:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14931">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14931">The Complexity of Dynamic Data Race Prediction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathur:Umang.html">Umang Mathur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavlogiannis:Andreas.html">Andreas Pavlogiannis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viswanathan:Mahesh.html">Mahesh Viswanathan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14931">PDF</a><br /><b>Abstract: </b>Writing concurrent programs is notoriously hard due to scheduling
non-determinism. The most common concurrency bugs are data races, which are
accesses to a shared resource that can be executed concurrently. Dynamic
data-race prediction is the most standard technique for detecting data races:
given an observed, data-race-free trace $t$, the task is to determine whether
$t$ can be reordered to a trace $t^*$ that exposes a data-race. Although the
problem has received significant practical attention for over three decades,
its complexity has remained elusive. In this work, we address this lacuna,
identifying sources of intractability and conditions under which the problem is
efficiently solvable. Given a trace $t$ of size $n$ over $k$ threads, our main
results are as follows.
</p>
<p>First, we establish a general $O(k\cdot n^{2\cdot (k-1)})$ upper-bound, as
well as an $O(n^k)$ upper-bound when certain parameters of $t$ are constant. In
addition, we show that the problem is NP-hard and even W[1]-hard parameterized
by $k$, and thus unlikely to be fixed-parameter tractable. Second, we study the
problem over acyclic communication topologies, such as server-clients
hierarchies. We establish an $O(k^2\cdot d\cdot n^2\cdot \log n)$ upper-bound,
where $d$ is the number of shared variables accessed in $t$. In addition, we
show that even for traces with $k=2$ threads, the problem has no
$O(n^{2-\epsilon})$ algorithm under Orthogonal Vectors. Since any trace with 2
threads defines an acyclic topology, our upper-bound for this case is optimal
wrt polynomial improvements for up to moderate values of $k$ and $d$. Finally,
we study a distance-bounded version of the problem, where the task is to expose
a data race by a witness trace that is similar to $t$. We develop an algorithm
that works in $O(n)$ time when certain parameters of $t$ are constant.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14931"><span class="datestr">at May 03, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14891">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14891">Fully-Dynamic Coresets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kale:Sagar.html">Sagar Kale</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14891">PDF</a><br /><b>Abstract: </b>With input sizes becoming massive, coresets---small yet representative
summary of the input---are relevant more than ever. A weighted set $C_w$ that
is a subset of the input is an $\varepsilon$-coreset if the cost of any
feasible solution $S$ with respect to $C_w$ is within $[1 {\pm} \varepsilon]$
of the cost of $S$ with respect to the original input. We give a very general
technique to compute coresets in the fully-dynamic setting where input points
can be added or deleted. Given a static $\varepsilon$-coreset algorithm that
runs in time $t(n, \varepsilon, \lambda)$ and computes a coreset of size $s(n,
\varepsilon, \lambda)$, where $n$ is the number of input points and $1
{-}\lambda$ is the success probability, we give a fully-dynamic algorithm that
computes an $\varepsilon$-coreset with worst-case update time $O((\log n) \cdot
t(s(n, \varepsilon/\log n, \lambda/n), \varepsilon/\log n, \lambda/n) )$ (this
bound is stated informally), where the success probability is $1{-}\lambda$.
Our technique is a fully-dynamic analog of the merge-and-reduce technique that
applies to insertion-only setting. Although our space usage is $O(n)$, we work
in the presence of an adaptive adversary.
</p>
<p>As a consequence, we get fully-dynamic $\varepsilon$-coreset algorithms for
$k$-median and $k$-means with worst-case update time
$O(\varepsilon^{-2}k^2\log^5 n \log^3 k)$ and coreset size
$O(\varepsilon^{-2}k\log n \log^2 k)$ ignoring $\log \log n$ and
$\log(1/\varepsilon)$ factors and assuming that $\varepsilon, \lambda =
\Omega(1/$poly$(n))$ (very weak assumptions made to make bounds easy to parse).
These are the first fully-dynamic algorithms for $k$-median and $k$-means with
worst-case update times $O($poly$(k, \log n, \varepsilon^{-1}))$. The best
previous bound for both problems was amortized $O(n\log n)$ by Cohen-Addad et
al. via randomized $O(1)$-coresets in $O(n)$ space.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14891"><span class="datestr">at May 03, 2020 10:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14789">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14789">Twin-width I: tractable FO model checking</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Eun_Jung.html">Eun Jung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomass=eacute=:St=eacute=phan.html">Stéphan Thomassé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watrigant:R=eacute=mi.html">Rémi Watrigant</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14789">PDF</a><br /><b>Abstract: </b>Inspired by a width invariant defined on permutations by Guillemot and Marx
[SODA '14], we introduce the notion of twin-width on graphs and on matrices.
Proper minor-closed classes, bounded rank-width graphs, map graphs, $K_t$-free
unit $d$-dimensional ball graphs, posets with antichains of bounded size, and
proper subclasses of dimension-2 posets all have bounded twin-width. On all
these classes (except map graphs without geometric embedding) we show how to
compute in polynomial time a sequence of $d$-contractions, witness that the
twin-width is at most $d$. We show that FO model checking, that is deciding if
a given first-order formula $\phi$ evaluates to true for a given binary
structure $G$ on a domain $D$, is FPT in $|\phi|$ on classes of bounded
twin-width, provided the witness is given. More precisely, being given a
$d$-contraction sequence for $G$, our algorithm runs in time $f(d,|\phi|) \cdot
|D|$ where $f$ is a computable but non-elementary function. We also prove that
bounded twin-width is preserved by FO interpretations and transductions
(allowing operations such as squaring or complementing a graph). This unifies
and significantly extends the knowledge on fixed-parameter tractability of FO
model checking on non-monotone classes, such as the FPT algorithm on
bounded-width posets by Gajarsk\'y et al. [FOCS '15].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14789"><span class="datestr">at May 03, 2020 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14724">Learning Bayesian Networks Under Sparsity Constraints: A Parameterized Complexity Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=uuml=ttemeier:Niels.html">Niels Grüttemeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14724">PDF</a><br /><b>Abstract: </b>We study the problem of learning the structure of an optimal Bayesian network
$D$ when additional constraints are posed on the DAG $D$ or on its moralized
graph. More precisely, we consider the constraint that the moralized graph can
be transformed to a graph from a sparse graph class $\Pi$ by at most $k$ vertex
deletions. We show that for $\Pi$ being the graphs with maximum degree $1$, an
optimal network can be computed in polynomial time when $k$ is constant,
extending previous work that gave an algorithm with such a running time for
$\Pi$ being the class of edgeless graphs [Korhonen &amp; Parviainen, NIPS 2015]. We
then show that further extensions or improvements are presumably impossible.
For example, we show that when $\Pi$ is the set of graphs with maximum degree
$2$ or when $\Pi$ is the set of graphs in which each component has size at most
three, then learning an optimal network is NP-hard even if $k=0$. Finally, we
show that learning an optimal network with at most $k$ edges in the moralized
graph presumably has no $f(k)\cdot |I|^{\mathcal{O}(1)}$-time algorithm and
that, in contrast, an optimal network with at most $k$ arcs in the DAG $D$ can
be computed in $2^{\mathcal{O}(k)}\cdot
</p>
<p>|I|^{\mathcal{O}(1)}$ time where $|I|$ is the total input size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14724"><span class="datestr">at May 03, 2020 10:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14715">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14715">Randomized Two-Valued Bounded Delay Online Buffer Management</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14715">PDF</a><br /><b>Abstract: </b>In the bounded delay buffer management problem unit size packets arrive
online to be sent over a network link. The objective is to maximize the total
weight of packets sent before their deadline. In this paper we are interested
in the two-valued variant of the problem, where every packet has either low (1)
or high priority weight ($\alpha$ &gt; 1). We show that its randomized competitive
ratio against an oblivious adversary is 1 + ($\alpha$ -- 1)/($\alpha$ 2 +
$\alpha$).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14715"><span class="datestr">at May 03, 2020 10:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14632">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14632">Geometric group testing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berendsohn:Benjamin_Aram.html">Benjamin Aram Berendsohn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozma:L=aacute=szl=oacute=.html">László Kozma</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14632">PDF</a><br /><b>Abstract: </b>Group testing is concerned with identifying $t$ defective items in a set of
$m$ items, where each test reports whether a specific subset of items contains
at least one defective. In non-adaptive group testing, the subsets to be tested
are fixed in advance. By testing multiple items at once, the required number of
tests can be made much smaller than $m$. In fact, for $t \in \mathcal{O}(1)$,
the optimal number of (non-adaptive) tests is known to be $\Theta(\log{m})$.
</p>
<p>In this paper, we consider the problem of non-adaptive group testing in a
geometric setting, where the items are points in $d$-dimensional Euclidean
space and the tests are axis-parallel boxes (hyperrectangles). We present upper
and lower bounds on the required number of tests under this geometric
constraint. In contrast to the general, combinatorial case, the bounds in our
geometric setting are polynomial in $m$. For instance, our results imply that
identifying a defective pair in a set of $m$ points in the plane always
requires $\Omega(m^{4/7})$ rectangle-tests, and there exist configurations of
$m$ points for which $\mathcal{O}(m^{2/3})$ rectangle-tests are sufficient,
whereas to identify a single defective point in the plane, $\Theta(m^{1/2})$
tests are always necessary and sometimes sufficient.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14632"><span class="datestr">at May 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14622">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14622">The Canny-Emiris conjecture for the sparse resultant</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Carlos D'Andrea, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeronimo:Gabriela.html">Gabriela Jeronimo</a>, Martin Sombra <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14622">PDF</a><br /><b>Abstract: </b>We present a product formula for the initial parts of the sparse resultant
associated to an arbitrary family of supports, generalizing a previous result
by Sturmfels. This allows to compute the homogeneities and degrees of the
sparse resultant, and its evaluation at systems of Laurent polynomials with
smaller supports. We obtain a similar product formula for some of the initial
parts of the principal minors of the Sylvester-type square matrix associated to
a mixed subdivision of a polytope. Applying these results, we prove that the
sparse resultant can be computed as the quotient of the determinant of such a
square matrix by a certain principal minor, under suitable hypothesis. This
generalizes the classical Macaulay formula for the homogeneous resultant, and
confirms a conjecture of Canny and~Emiris.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14622"><span class="datestr">at May 03, 2020 10:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14574">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14574">On Solving Cycle Problems with Branch-and-Cut: Extending Shrinking and Exact Subcycle Elimination Separation Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobeaga:Gorka.html">Gorka Kobeaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Merino:Mar=iacute=a.html">María Merino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lozano:Jose_A=.html">Jose A. Lozano</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14574">PDF</a><br /><b>Abstract: </b>In this paper, we extend techniques developed in the context of the
Travelling Salesperson Problem for cycle problems. Particularly, we study the
shrinking of support graphs and the exact algorithms from subcycle elimination
separation problems. The efficient application of the considered techniques has
proved to be essential in the Travelling Salesperson Problem when solving large
size problems by Branch-and-Cut, and this has been the motivation behind this
work. Regarding the shrinking of support graphs, we prove the validity of the
Padberg-Rinaldi general shrinking rules and the Crowder-Padberg subcycle-safe
shrinking rules. Concerning the subcycle separation problems, we extend two
exact separation algorithms, the Dynamic Hong and the Extended
Padberg-Gr\"otschel algorithms, which are shown to be superior to the ones used
so far in the literature of cycle problems.
</p>
<p>The proposed techniques are empirically tested in 24 subcycle elimination
problem instances generated by solving the Orienteering Problem (involving up
to 15112 vertices) with Branch-and-Cut. The experiments suggest the relevance
of the proposed techniques for cycle problems. The obtained average speedup for
the subcycle separation problems in the Orienteering Problem when the proposed
techniques are used together is around 50 times in medium-sized instances and
around 250 times in large-sized instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14574"><span class="datestr">at May 03, 2020 10:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.14473">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.14473">Arc Routing with Time-Dependent Travel Times and Paths</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal:Thibaut.html">Thibaut Vidal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martinelli:Rafael.html">Rafael Martinelli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pham:Tuan_Anh.html">Tuan Anh Pham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=agrave=:Minh_Ho=agrave=ng.html">Minh Hoàng Hà</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.14473">PDF</a><br /><b>Abstract: </b>Vehicle routing algorithms usually reformulate the road network into a
complete graph in which each arc represents the shortest path between two
locations. Studies on time-dependent routing followed this model and therefore
defined the speed functions on the complete graph. We argue that this model is
often inadequate, in particular for arc routing problems involving services on
edges of a road network. To fill this gap, we formally define the
time-dependent capacitated arc routing problem (TDCARP), with travel and
service speed functions given directly at the network level. Under these
assumptions, the quickest path between locations can change over time, leading
to a complex problem that challenges the capabilities of current solution
methods. We introduce effective algorithms for preprocessing quickest paths in
a closed form, efficient data structures for travel time queries during routing
optimization, as well as heuristic and exact solution approaches for the
TDCARP. Our heuristic uses the hybrid genetic search principle with tailored
solution-decoding algorithms and lower bounds for filtering moves. Our
branch-and-price algorithm exploits dedicated pricing routines, heuristic
dominance rules and completion bounds to find optimal solutions for problem
counting up to 75 services. Based on these algorithms, we measure the benefits
of time-dependent routing optimization for different levels of travel-speed
data accuracy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.14473"><span class="datestr">at May 03, 2020 10:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/065">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/065">TR20-065 |  Sharp Threshold Results for Computational Complexity | 

	Lijie Chen, 

	Ce Jin, 

	Ryan Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We establish several ``sharp threshold'' results for computational complexity. For certain tasks, we can prove a resource lower bound of $n^c$ for $c \geq 1$ (or obtain an efficient circuit-analysis algorithm for $n^c$ size), there is strong intuition that a similar result can be proved for larger functions of $n$, yet we can also prove that replacing ``$n^c$'' with ``$n^{c+\varepsilon}$'' in our results, for any $\varepsilon &gt; 0$, would imply a breakthrough $n^{\omega(1)}$ lower bound.  
	  
We first establish such a result for Hardness Magnification. We prove (among other results) that for some $c$, the Minimum Circuit Size Problem for $(\log n)^c$-size circuits on length-$n$ truth tables (${MCSP}[(\log n)^c]$) does not have $n^{2-o(1)}$-size probabilistic formulas. We also prove that an $n^{2+\varepsilon}$ lower bound for ${MCSP}[(\log n)^c]$ (for any $\varepsilon &gt; 0$ and $c \geq 1$) would imply major lower bound results, such as ${NP}$ does not have $n^k$-size formulas for all $k$, and $\#{SAT}$ does not have log-depth circuits.  
Similar results hold for time-bounded Kolmogorov complexity. Note that cubic size lower bounds are known for probabilistic De Morgan formulas (for other functions).  
	  
Next we show a sharp threshold for Quantified Derandomization (QD) of probabilistic formulas.  
(1) For all $\alpha, \varepsilon &gt; 0$, there is a deterministic polynomial-time algorithm that finds satisfying assignments to every probabilistic formula of $n^{2-2\alpha-\varepsilon}$ size with at most $2^{n^{\alpha}}$ falsifying assignments.  
(2) If for some $\alpha, \varepsilon &gt; 0$, there is such an algorithm for probabilistic formulas of $n^{2-\alpha+\varepsilon}$-size and $2^{n^{\alpha}}$ unsatisfying assignments, then a full derandomization of ${NC}^1$ follows: a deterministic poly-time algorithm additively approximating the acceptance probability of any polynomial-size formula. Consequently, ${NP}$ does not have $n^k$-size formulas, for all $k$.  

	  
Finally we show a sharp threshold result for Explicit Obstructions, inspired by Mulmuley's notion of explicit obstructions from GCT. An explicit obstruction against $S(n)$-size formulas is a poly-time algorithm $A$ such that $A(1^n)$ outputs a list $\{(x_i,f(x_i))\}_{i \in [\mathrm{poly}(n)]} \subseteq \{0,1\}^n \times \{0,1\}$, and every $S(n)$-size formula $F$ is inconsistent with the (partially defined) function $f$. We prove that for all $\varepsilon &gt; 0$, there is an explicit obstruction against $n^{2-\varepsilon}$-size formulas, and prove that there is an explicit obstruction against $n^{2+\varepsilon}$-size formulas for some $\varepsilon &gt; 0$ if and only if there is an explicit obstruction against all polynomial-size formulas. This in turn is equivalent to the statement that ${E}$ does not have $2^{o(n)}$-size formulas, a breakthrough in circuit complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/065"><span class="datestr">at May 02, 2020 11:43 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/064">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/064">TR20-064 |  Automating Algebraic Proof Systems is NP-Hard | 

	Mika Göös, 

	Susanna de Rezende, 

	Jakob Nordström, 

	Toniann Pitassi, 

	Robert Robere, 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that algebraic proofs are hard to find: Given an unsatisfiable CNF formula $F$, it is NP-hard to find a refutation of $F$ in the Nullstellensatz, Polynomial Calculus, or Sherali--Adams proof systems in time polynomial in the size of the shortest such refutation. Our work extends, and gives a simplified proof of, the recent breakthrough of Atserias and Muller (FOCS 2019) that established an analogous result for Resolution.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/064"><span class="datestr">at May 02, 2020 11:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4780">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4780">Vaccine challenge trials NOW!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I’ve asked myself again and again over the last few months: why are human challenge trials for covid vaccines not an ethical no-brainer?  What am I missing that all the serious medical experts see?  And what are we waiting for: for 10 million more to die? 20 million?  So it made me feel a little less crazy that <a href="https://www.washingtonpost.com/opinions/2020/04/27/pandemic-ethics-case-experiments-human-volunteers/?arc404=true">the world’s most famous ethicist agrees</a>.</p>



<p>I loved the way James Miller put it on my Facebook:</p>



<blockquote class="wp-block-quote"><p>This is the trolley problem where the fat man wants to jump knowing his chance of death is below 1% and our decision is whether to stop him.</p></blockquote>



<p>Like, suppose someone willingly sacrificed themselves so that doctors could use their body parts to save 10 million people.  We might say: we would’ve lacked the strength to do the same in their place.  We might say: we hope they weren’t pressured or coerced into it.  But after the deed is done, is there anything to call this person but a hero, or even a martyr?  Whatever we feel about the fireman who sacrifices his life in the course of saving 10 kids from a burning building, shouldn’t we feel it about this person a million times over?  And of course, I deliberately made this vastly more extreme than the actual situation faced by young, healthy volunteers in a covid challenge trial, who in all likelihood would recover and be fine.</p>



<p>Regarding the obvious question: so would <em>I</em> volunteer to take an unproved vaccine, followed by a deliberate covid injection?  Sure!  Unfortunately, I might no longer be a candidate: I’m now nearing middle age and pre-diabetic, I help watch two young kids, and I live with two immunocompromised parents.  But on the principle of walking the walk: if it were a vaccine candidate that I considered promising (and there are now several), and if it were practical to isolate me away from home for the requisite time, and if I could actually be of use, then absolutely, jab me.</p>



<p><strong>On a somewhat related note:</strong> Last night I watched the <a href="https://en.wikipedia.org/wiki/Ender%27s_Game_(film)"><em>Ender’s Game</em> movie</a> with my 7-year-old daughter Lily (neither of us had seen it; I’d read the book but only as a kid).  Not surprisingly, the movie was a <em>huge</em> hit with Lily; she’s already begging to see it again.  As for me, my first thought was: what a hackneyed sci-fi premise, that the entire human race is under attack from some alien species, and that all human children grow up in the shadow of that knowledge.  Nothing whatsoever like the real world of 2020!  My second thought was: what a quaint concept, that faced with a threat to humanity, the earth-authorities would immediately respond “quick, we need to find and train and cultivate <em>super-geniuses willing to break the rules</em>, and put them in command!”  Only in the movies, never in real life!  Except in, y’know, WWII, where that mindset was pretty crucial to the Allied victory?  But 75 years later, yes, it reads to us as science fiction.</p>



<p>To inject a tiny note of optimism, I’m hopeful that we <em>will</em> eventually see some fruits of genius commensurate with the threat, whether in the realm of treatments or vaccines or contact-tracing apps or PPE or something else that no one’s thought of yet.  Right now, though, the sad fact is this: as far as I know, the only indisputable work of genius to have arisen in response to the covid crisis has been the <a href="https://twitter.com/steak_umm?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter account for steak-umms</a>.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4780"><span class="datestr">at May 02, 2020 01:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17007">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/">Mathematics of COVID-19</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Its not just <img src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R_0}" class="latex" title="{R_0}" /></em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/220px-sir_francis_galton_by_charles_wellington_furse/" rel="attachment wp-att-17012"><img src="https://rjlipton.files.wordpress.com/2020/05/220px-sir_francis_galton_by_charles_wellington_furse.jpg?w=600" alt="" class="alignright size-full wp-image-17012" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Sir Francis Galton by Charles Wellington Furse ]</font></td>
</tr>
</tbody>
</table>
<p>
Francis Galton is a perfect example of a Victorian era scientist. Sir Galton, he was knighted in 1909, had many roles: a statistician, a sociologist, a psychologist, an anthropologist, a tropical explorer, a geographer, a meteorologist, a geneticist, and an inventor. He coined the phrase “nature versus nurture” and <a href="https://en.wikipedia.org/wiki/Francis_Galton">more</a>.</p>
<p>
Today we trade in jokes for some mathematics of the virus.</p>
<p>
We wish there was something clever we can say about the spread of the virus. But the statistics of the spread are complex. We wish there was something theory can contribute to the fight against the virus. But the front-line is clearly dominated by medicine and biology.</p>
<p>
However there are two areas that are relevant. The first is the math of how fast the virus spreads and the second is how valid are the claims about the virus. The latter is an area where theory could play a role in the future.</p>
<p>
In preparing this discussion we noted that Galton was indeed an inventor. He invented a device to demonstrate the central limit theorem. You probably have seen some <a href="http://www.karlsims.com/marbles/">version</a>. Sometimes called the bean machine, it gives a visual demonstration of the central limit theorem. Other times it is called the Galton board. Perhaps if Galton were alive today he would be on cable news explaining how the virus spreads. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/marble-run/" rel="attachment wp-att-17009"><img width="168" alt="" src="https://rjlipton.files.wordpress.com/2020/05/marble-run.jpg?w=168&amp;h=300" class="aligncenter size-medium wp-image-17009" height="300" /></a></p>
<p>
Galton also had views that are troubling. See <a href="https://www.statisticsviews.com/details/news/11158556/June-2019-issue-of-Significance-just-published.html">this</a> for example. He lived over one hundred years ago, but his views on eugenics are still upsetting. Should we not have featured him? What do you think?</p>
<p></p><h2> Extinction </h2><p></p>
<p></p><p>
The issue is will the terrible virus stop infecting people? Will it become extinct? Or will it at least stop infecting more and more people. The part of math that studies such questions was invented by Galton in 1889 as a model to track family names. We wish we were talking today about family names and not a killer virus. The area he invented is now called <a href="https://en.wikipedia.org/wiki/Branching_process">branching process</a>. </p>
<blockquote><p><b> </b> <em> There was concern amongst the Victorians that aristocratic surnames were becoming extinct. Galton originally posed the question regarding the probability of such an event in an 1873 issue of The Educational Times, and the Reverend Henry Watson replied with a solution. Together, they then wrote an 1874 paper entitled <a href="https://www.jstor.org/stable/2841222?origin=crossref&amp;seq=1#metadata_info_tab_contents">On the probability of the extinction of families</a>. </em>
</p></blockquote>
<p></p><p>
We are interested in branching processes and when they are likely to become extinct. We want the virus to stop infecting people, and become extinct. Or at least stop its explosive growth that is so terrible. See these <a href="https://theconversation.com/how-to-flatten-the-curve-of-coronavirus-a-mathematician-explains-133514">comments</a>, for more information. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/fig/" rel="attachment wp-att-17010"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/fig.png?w=300&amp;h=225" class="aligncenter size-medium wp-image-17010" height="225" /></a></p>
<p>
</p><p></p><h2> On Average </h2><p></p>
<p></p><p>
The contagiousness of a disease is described by its “reproduction rate” or the average number of people infected by one infectious person in a population without immunity. You might also hear this number referred to as the <img src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R_0}" class="latex" title="{R_0}" /> value. When it is less than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, the disease does not become an pandemic. This process is called a <i>branching process</i>. In order to tell if a branching process will eventually become extinct we need more than <img src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R_0}" class="latex" title="{R_0}" />. That is we need to understand more than the average number of descendants. Let’s see why.</p>
<p>
Consider a process <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> that creates <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> descendants with probability <img src="https://s0.wp.com/latex.php?latex=%7Ba_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a_{k}}" class="latex" title="{a_{k}}" /> and a process <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> that creates <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> descendants with probability <img src="https://s0.wp.com/latex.php?latex=%7Bb_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b_{k}}" class="latex" title="{b_{k}}" />. The number of average descendants is for <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BA%7D+%3D+a_%7B1%7D+%2B+2a_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots " class="latex" title="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots " /></p>
<p>and for <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BB%7D+%3D+b_%7B1%7D+%2B+2b_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots " class="latex" title="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots " /></p>
<p>Is it always better to have the process with the smaller average? The answer is no.</p>
<p>
Consider the process <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> so that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a_%7B0%7D+%3D+0%2C+a_%7B1%7D+%3D+1%2C+a_%7B2%7D+%3D+%5Cepsilon%2C+a_%7B3%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots " class="latex" title="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots " /></p>
<p>And consider the process <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> so that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_%7B0%7D+%3D+1%2Fn%2C+b_%7B1%7D+%3D+1%2Fn%2C+%5Cdots%2C+b_%7Bn%7D+%3D+1%2Fn%2C+b_%7Bn%2B1%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots " class="latex" title="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots " /></p>
<p>The first average <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu_%7BA%7D+%3D+1+%2B+2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu_{A} = 1 + 2\epsilon}" class="latex" title="{\mu_{A} = 1 + 2\epsilon}" /> and the second average is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2Fn+%2B+2%2Fn+%2B+%5Ccdots+%2B+n%2Fn+%3D+%28n%2B1%29%2F2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. " class="latex" title="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. " /></p>
<p>Clearly the second has a much larger value for <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \ge 2}" class="latex" title="{n \ge 2}" />. But the first will never go extinct and the second can become extinct.</p>
<p>
</p><p></p><h2> It’s In the Variance </h2><p></p>
<p></p><p>
The key difference is that the second process has higher <em>variance</em>. The importance of the variance—as opposed to the mean—is remembered in some ways but seems to be forgotten in others. It is the nub of one of the <a href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/">jokes</a> we included in the previous post:</p>
<blockquote><p><b> </b> <em> There was a statistician who drowned crossing a river—that was only 3 feet deep on average. </em>
</p></blockquote>
<p></p><p>
For an example closer to our point, suppose a third-party candidate <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> entering a race expects to take more votes away from candidate <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> than candidate <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> so as to double the margin that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> expects to lose by. But suppose <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> alters the dynamics of the race so that the standard deviation is quadrupled. Then <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> generally has a better chance of winning under that scenario. If the distribution is normal and the original standard deviation equaled the expected margin, then <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" />‘s chances of winning improve from 16% to over 30%.</p>
<p>
In our case, “winning” means outcomes where the virus dies out, locally and ultimately globally. Such outcomes are needed for opening up. It is not enough to reduce the number of active cases to “<img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />” because the branching started from such a state. Whatever active cases there are must be known and contained as well as <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />. This is the situation currently <a href="https://www.bbc.com/news/world-asia-52436658">claimed</a> in New Zealand.</p>
<p>
</p><p></p><h2> Branching and Uncertainty </h2><p></p>
<p></p><p>
At the other end is the state where the virus is not contained but branching stops because of saturation. When the proportion of targeted descendants who have already had the virus and are (we hope) immune is greater than the branching factor, then the expectation takes over from the variance as a determinant of stoppage. This proportion is what is meant by “herd immunity” and is estimated to be 60–70% for this virus.</p>
<p>
What we feel is the central mystery is whether there are enough undetected cases to bring that point even possibly in range. Randomized testing in the New York area has found nearly a 25% rate of antibodies. Analogous <a href="https://www.nytimes.com/2020/04/21/health/coronavirus-antibodies-california.html">tests</a> in less-affected California and in other countries have found under 10% positive rates, however. Those results are subject to uncertainty in the representativeness of the samples.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We had planned on saying something about checking if reported data about the virus is correct, or is it faked. With so much at stake it seems smart to insist on data being verified. More on that in the future.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/"><span class="datestr">at May 01, 2020 09:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5535989522381894451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/05/predicting-virus.html">Predicting the Virus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As a complexity theorist I often find myself far more intrigued in what we cannot compute than what we can. <div><br /></div><div>In 2009 I <a href="https://blog.computationalcomplexity.org/2009/06/failure-of-social-networks.html">posted on some predictions of the spread of the H1N1 virus</a> which turned out to be off by two orders of magnitude. I wrote "I always worry that bad predictions from scientists make it harder to have the public trust us when we really need them to." Now we need them to.</div><div><br /></div><div>We find ourselves bombarded with predictions from a variety of experts and even larger variety of mathematicians, computer scientists, physicists, engineers, economists and others who try to make their own predictions with no earlier experience in epidemiology. Many of these models give different predictions and even the best have proven significantly different than reality. We keep coming back to the George Box quote "All models are wrong, but some are useful."</div><div><br /></div><div>So why do these models have so much trouble? The standard complaint of inaccurate and inconsistently collected data certainly holds. And if a prediction changes our behavior, we cannot fault the predictor for not continuing to be accurate.</div><div><br /></div><div>There's another issue. You often here of a single event having a dramatic effect in a region--a soccer game in Italy, a funeral in Georgia, a Bar Mitzvah in New York. These events ricocheted, people infected attended other events that infected others. This becomes a complex process that simple network models can never get right. Plenty of soccer games, funerals and Bar Mitzvahs didn't spread the virus. If a region has hadn't a large number of cases and deaths is it because they did the right thing or just got lucky. Probably something in between but that makes it hard to generalize and learn from experience. We do know that less events means less infection but beyond that is less clear.</div><div><br /></div><div>As countries and states decide how to open up and universities decide how to handle the fall semester, we need to rely on some sort of predictive models and the public's trust in them to move forward. We can't count on the accuracy of any model but which models are useful? We don't have much time to figure it out.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/05/predicting-virus.html"><span class="datestr">at May 01, 2020 02:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=3460">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/gradient-flows/">Effortless optimization through gradient flows</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month <a href="https://francisbach.com/computer-aided-analyses/">post</a>, <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a> explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient descent. This will be done using vanishing step-sizes that lead to <em>gradient flows</em>.</p>



<h2>Gradient as local information</h2>



<p class="justify-text">The intuitive principle behind gradient descent is the quest for <em>local</em> descent. We thus need to characterize the local behavior of the function we aim to optimize. This is what gradients are for.</p>



<p class="justify-text">In this blog post, I will consider minimizing a function \(f\) over \(\mathbb{R}^d\). Assuming \(f\) is differentiable, a first order Taylor expansion of \(f\) around a point \(x\) leads to $$f(x+\delta) = f(x) + \nabla f(x) ^\top \delta + o(\| \delta\|),$$ for any norm \(\| \cdot \|\) on \(\mathbb{R}^d\), where \(\nabla f(x) \in \mathbb{R}^d\) is  the gradient of \(f\) at \(x\), composed of partial derivatives of \(f\). Therefore, around \(x\), \(f\) is approximately affine.</p>



<p class="justify-text">Since we have a local affine approximation around \(x\), we can look for the direction of steepest descent, that is, the unit norm vector \(u \in \mathbb{R}^d\) such that \(f\) decays the most along \(u\), that is such that $$  u^\top \nabla f(x)$$ is minimized. This steepest descent direction depends on the choice of norm (assuming that the gradient is not zero at \(x\)).</p>



<p class="justify-text">For the \(\ell_2\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_2 = 1\), leads to $$ \displaystyle u = \ – \frac{\nabla f(x)}{ \| \nabla f(x) \|_2},$$ that is the steepest descent is along the negative gradient (see an illustration below). In this blog post I will only focus on this steepest descent direction. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="428" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/gradient_contours-1024x358.png" class="wp-image-3542" height="149" />Function \(f\) represented through its contour lines for values 1, 2, 3, 4 and 5. Negative gradient \(– \nabla f(x)\) as the steepest descent direction at point \(x\), which is orthogonal to the contour lines.</figure></div>



<p class="justify-text">As as side note, for the \(\ell_1\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_1 = 1\), leads to $$u \in\  – \arg\max_{ v \in \{-e_1,\, e_1,\, -e_2,\, e_2,\dots,\, -e_d,\, e_d \}} v^\top \nabla f(x),$$ where \(e_i\) is the \(i\)-th canonical basis vector of \(\mathbb{R}^d\). Here the steepest descent is along a coordinate axis (along the positive or negative side), and this leads to various forms of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate descent</a> (this will probably be a topic for another post). </p>



<p class="justify-text">Given that the negative gradient leads to the steepest descent direction (for the Euclidean norm), it is natural to use this as a direction for an iterative algorithm, an idea that dates back to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> in 1847 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">1</a>] (see the nice summary by Claude Le Maréchal [<a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">2</a>]).</p>



<h2>From gradient descent to gradient flows</h2>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is the most classical iterative algorithm to minimize differentiable functions. It takes the form $$x_{n+1} = x_{n} \, – \gamma \nabla f(x_{n})$$ at iteration \(n\), where \(\gamma &gt; 0 \) is a step-size.  </p>



<p class="justify-text">Gradient descent comes in many flavors, steepest, stochastic, pre-conditioned, conjugate, proximal, projected, accelerated, etc. There are lots of papers and books [e.g., 3, 4, 5] analyzing it in various settings.</p>



<p class="justify-text">In this post, to simplify its analysis and setting the stage for later posts, I will present the gradient flow, which is essentially the limit of gradient descent when the step-size \(\gamma\) tends to zero.</p>



<p class="justify-text">More precisely, this is obtained by considering that our iterates \(x_n\) are sampled at each multiple of \(\gamma\), from a function \(X: \mathbb{R}_+ \to \mathbb{R}^d\), as $$x_n = X(n\gamma).$$ We can then use a piecewise affine interpolation to define a function defined on all points. We then have for \(t = n\gamma\), $$X(t + \gamma) = x_{n+1} =x_{n} \, – \gamma \nabla f(x_{n}) = X(t)\, – \gamma \nabla f(X(t)).$$ Dividing by \(\gamma\), we get $$ \frac{1}{\gamma} \big[ X(t + \gamma) \, – X(t) \big] = \, – \nabla f(X(t)).$$</p>



<p class="justify-text">When \(\gamma\) tends to zero (and with simple additional regularity assumptions), the left hand side tends to the derivative of \(X\) at \(t\), and thus the function \(X\) tends to the solution of the following <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equation</a> $$ \dot{X}(t) = \ – \nabla f (X(t)).$$ See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="348" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow.gif" class="wp-image-3479" height="271" />Gradient descent (with piece-wise affine interpolation between iterates) vs. gradient flow on the same time scale for a logistic regression problem.</figure></div>



<p class="justify-text">Studying the gradient flow in lieu of the gradient descent recursions comes with pros and cons.</p>



<p class="justify-text"><strong>Simplified analyses</strong>. The gradient flow has no step-size, so all the traditional annoying issues regarding the choice of step-size, with line-search, constant, decreasing or with a weird schedule are unnecessary. Moreover, the use of differential calculus makes proving properties really simple (see examples below). We can thus focus on the essence of the algorithm rather than on technicalities.</p>



<p class="justify-text"><strong>From (continuous) flow to actual (discrete) algorithms</strong>. A flow cannot be run on a computer at is is a continuous-time object. The traditional discretization is the <a href="https://en.wikipedia.org/wiki/Euler_method">Euler method</a>, that exactly replaces the flow by a piecewise-affine interpolation of the gradient descent iterates, where as shown above, we see \(x_n\) as \(X(n\gamma)\), where \(\gamma\) is the time increment between two samples. Four interesting observations:</p>



<ul class="justify-text"><li><em>No direct proof transfer</em> : While Euler discretization always provides an algorithm, the generic convergence proofs do not allow to transfer immediately continuous-time proofs to convergence results for the discrete analysis. A key difficulty is to set-up the step-size \(\gamma\). However, the analysis can often be mimicked, i.e., similar <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a> can be used (see examples below).</li><li><em>Proximal algorithms</em> : Faced with non-continuous gradient functions, the <em>forward</em> version of Euler discretization \(x_{n+1} = x_{n} – \gamma \nabla f(x_{n})\) can be replaced by the <em>backward</em> version $$x_{n+1} = x_{n} \, –  \gamma \nabla f(x_{n+1}),$$ which is only implicit as it can be solved by minimizing $$ f(x) + \frac{1}{2\gamma}\|x-x_{n}\|_2^2,$$ thus leading to the <a href="https://fr.wikipedia.org/wiki/Algorithme_proximal_(optimisation)">proximal point algorithm</a>. Forward-backward schemes can also be recovered when \(f\) is the sum of a smooth and a non-smooth term.</li><li><em>Stochastic gradient descent</em> : There are two ways to deal with stochastic gradient descent, leading to two very different continuous limits. Adding independent and identically distributed (for simplicity) zero-mean noise \(\varepsilon_n\) to the gradient leads to the recursion $$x_{n+1} = x_{n} – \gamma \big[ \nabla f(x_{n}) + \varepsilon_n\big] = x_{n}\, – \gamma \nabla f(x_{n}) \,- \gamma \varepsilon_n,$$ where the noise is multiplied by the step-size \(\gamma\). Surprisingly, taking the limit when \(\gamma\) tends to zero leads to the deterministic gradient flow equation. A more detailed argument is presented at the end of post, but the main hand-waving reason is that the noise contribution vanishes because it is multiplied by the step-size. Note that this limiting behavior is consistent with a convergence to a minimizer of \(f\).</li><li><em>Convergence to a Langevin diffusion</em> : When instead the noise is added with magnitude proportional to the square root \(\sqrt{2 \gamma}\) of the step-size (which is asymptotically larger than \(\gamma\)), when \(\gamma\) tends to zero, and if the covariance of the noise is identity, we converge to a <a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion process</a> which is the solution of a <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">stochastic differential equation</a>: $$ dX(t) = \ – \nabla f(X(t)) + \sqrt{2} dB(t),$$ where \(B\) is a standard <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>. Moreover, as \(t\) tends to infinity, \(X(t)\) happens to tend in distribution to a random variable with density proportional to \(\exp( – f(x) )\). See more details at the end of the post and in [6]. The difference in behavior is illustrated below.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="359" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_SGD-3.gif" class="wp-image-3527" height="318" /> Comparison of flow and diffusion, for the same small \(\gamma\). The flow is deterministic and converges to a stationary point of \(f\) (here the global minimum), while the diffusion is stochastic and converges to a distribution (which is typically not a point mass)</figure></div>



<h2>Properties of gradient flows</h2>



<p class="justify-text">The gradient flow $$ \dot{X}(t) = \ – \nabla f (X(t)) $$ is well-defined for a wide variety of conditions on the function \(f\). The most classical ones are <a href="https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem">Lipschitz-continuity</a> or semi-convexity [<a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">7</a>].</p>



<p class="justify-text">The most obvious property is that the function decreases along the flow; in other words, \(f(X(t))\) is decreasing, which is a simple consequence of $$ \frac{d}{dt} f(X(t)) =  \nabla f(X(t))^\top \frac{dX(t)}{dt} =\  – \| \nabla f (X(t) )\|_2^2 \leqslant 0.$$</p>



<p class="justify-text">If \(f\) is bounded from below, then \(f(X(t))\) will always converge (as a non-increasing function which is bounded from below, see <a href="https://en.wikipedia.org/wiki/Monotone_convergence_theorem">here</a>). However, in general, \(X(t)\) may not always converge without any further assumptions, e.g., it may oscillate forever. This is however rare and there are a variety of sufficient conditions for convergence of gradient flows, that date back to Lojasiewicz [8], and are based on “Lojasiewicz inequalities” that state that for \(y\) and \(x\) close enough, \(|f(x) – f(y)|^{1-\theta} \leqslant C \| \nabla f(x)\|\) for some \(C &gt; 0 \) and \(\theta \in (0,1)\). These are satisfied for “sub-analytical functions”, that include most functions one can imagine [<a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">9</a>].</p>



<p class="justify-text">Once \(X(t)\) converges to some \(X(\infty) \in \mathbb{R}^d\), assuming \(\nabla f\) is continuous, we must have \(\nabla f(X(\infty))=0\), that is, \(X(\infty)\) is a stationary point of \(f\). Among all stationary points (that can be local minima, local maxima, or saddle-points), the one to which \(X(t)\) converges to depends on \(X(0)\).</p>



<p class="justify-text">Given any stationary point, one can look at the set of initializations that lead to it. Typically, only local minima are stable, that is, the attraction basins of other stationary points has typically zero Lebesgue measure (see, e.g., [<a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">10</a>]). See examples below. </p>



<p class="justify-text">We start with a simple function defined on the two-dimensional plane, with several local minima and saddle-points.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="511" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/plot_non_convex-1.png" class="wp-image-3490" height="401" />Various gradient flows trajectories, starting from green points and ending in black points. Note the proximity of the three top starting points, all ending in different local minima. See the motion below.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="519" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_noncvx-1.gif" class="wp-image-3492" height="500" />Various gradient flows trajectories, in motion! All flows share the same time scale. Some seem “slower” than others (because the gradient norm is small).</figure></div>



<p class="justify-text">Before moving on, I cannot resist presenting a “real” two-dimensional example that probably all skiers, hikers, and cyclists with some form of mathematical abilities have thought of, the topographic map. Here is an example below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="458" alt="" src="https://francisbach.com/wp-content/uploads/2020/04/glandon_croix_de_fer-1-1024x1024.jpg" class="wp-image-3751" height="458" />Extract from <a href="https://www.geoportail.gouv.fr/">IGN</a> topographic map, around <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a> (French Alps).</figure></div>



<p class="justify-text">Given the topographic map, how would gradient descent or gradient flow perform? Clearly, this corresponds to a non convex function, but it is quite well-behaved, as following water flows will typically lead to sea level. I chose two starting points famous to cyclists, <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a>, and ran gradient descent with a small step-size (to approximate the gradient flow), without noise (left) and with noise (right), on the topographic map (thanks to <a href="http://recherche.ign.fr/labos/matis/cv.php?nom=Landrieu">Loïc Landrieu</a> for the data extraction).</p>



<figure class="wp-block-image size-large"><img src="https://francisbach.com/wp-content/uploads/2020/04/flows_final_square_small-1024x394.png" alt="" class="wp-image-3753" /></figure>



<p class="justify-text">Without noise, the descent from la Croix de Fer ends up getting stuck quickly in a local minimum, while the one from Glandon goes down to the valley, but then is not able to follow the almost flat slope. When noise is added, the two flows go a bit lower, highlighting the benefits of noise to escape local minima.</p>



<h2>Gradient flows for optimization and machine learning</h2>



<p class="justify-text">There are (at least) two key questions in optimization and machine learning related to gradient flows: </p>



<ul class="justify-text"><li>When can we have global guarantees for convergence? That is, can we make sure that we choose an initialization point well enough to get the the global optimum <em>without knowing where the global optimum is</em>. A key difficulty is that the volume of the attraction basin of the global optimum can be made arbitrarily small, even for infinitely differentiable functions (imagine a function equal to zero everywhere except on a small ball where it is negative).</li><li>How fast can we get there? “there” can be a stationary point or a global optimum. This is an important question as mere convergence in the limit may be arbitrarily slow [<a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">11</a>].</li></ul>



<p class="justify-text">An important class of function is <a href="https://en.wikipedia.org/wiki/Convex_function">convex functions</a>, where everything works out very well. We will study them below. Other functions will be studied in future posts.</p>



<h2>Convex functions</h2>



<p class="justify-text">We now assume that the function \(f\) is <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> and differentiable. Within machine learning, this corresponds to objective functions encountered for supervised learning which are based on empirical risk minimization with a prediction function which is linearly parameterized, such as <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>.</p>



<p class="justify-text">There are various definitions of convexity, which are based on global properties (the function is always “below its chords”, or it is always “above its tangents”) or local properties (the Hessian is always positive semi-definite). The one which we need here is to be above its tangents, that is, for any \(x, y \in \mathbb{R}^d\), $$f(x) \geqslant f(y)  + \nabla f(y)^\top ( x \, – y).$$ Applying this to any stationary point \(y\) such that \(\nabla f(y)=0\) shows that for all \(x\), \(f(x) \geqslant f(y)\), that is, \(y\) is a global minimizer of \(f\). This is the classical benefit of convexity: no need to worry about local minima.</p>



<p class="justify-text">Another property we will need is the Lojasiewicz inequality, which is in particular satisfied when \(f\) is \(\mu\)-<a href="https://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions">strongly convex</a> (that is, \(f – \frac{1}{2} \| \cdot \|_2^2\) is convex): $$ f(x) \ – f(x_\ast) \leqslant \frac{1}{2 \mu} \| \nabla f (x)\|^2$$ for any minimizer \(x_\ast\) of \(f\) and any \(x\). This property allows to go from a bound on the gradient norm to a bound on function values.</p>



<p class="justify-text">We then obtain the convergence rate <em>in one line</em> as follows (see more details in [<a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">12</a>]): $$ \frac{d}{dt} \big[ f(X(t))\ – f(x_\ast) \big] =\  \nabla f(X(t))^\top \dot{X}(t) =  \ – \| \nabla f(X(t))\|_2^2 \leqslant \ – 2\mu  \big[ f(X(t)) \ – f(x_\ast) \big]$$ using the Lojasiewicz inequality above, leading to by simple integration of the derivative of \(\log \big[ f(X(t)) \ – f(x_\ast) \big]\): $$f(X(t)) \ – f(x_\ast) \leqslant \exp( – 2\mu t ) \big[ f(X(0))\  – f(x_\ast) \big], $$ that is, the convergence is exponential and the characteristic time is proportional to \(1/\mu\).</p>



<p class="justify-text">The gradient flow gives the main insight (exponential convergence); and applying the result above to \(t = \gamma n\), we seem to recover the traditional rate proportional to \(\exp( – \gamma \mu n)\); HOWEVER, this is only true asymptotically for \(\gamma\) tending to zero, and proving a result for gradient descent requires extra steps to deal with having a constant step-size. This requires typically \(\gamma \leqslant 1/L\), where \(L\) is the smoothness constant of \(f\), and the simplest proof happens to use the same structure (see [<a href="https://arxiv.org/pdf/1608.04636">13</a>] and references therein, as well as [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">14</a>]).</p>



<p class="justify-text">Without strong convexity, we have, using the tangent property at \(X(t)\) and \(x_\ast\): $$ \frac{d}{dt}\big[   \| X(t)\ – x_\ast \|^2 \big] = \ –   2 ( X(t) \ – x_\ast )^\top \nabla f(X(t)) \leqslant \ – 2 \big[ f(X(t)) \ – f(x_\ast) \big],$$  leading to, by integrating from \(0\) to \(t\), and using the monotonicity of \(f(X(t))\): $$  f(X(t)) \ – f(x_\ast) \leqslant \frac{1}{t} \int_0^t \big[ f(X(u)) \ – f(x_\ast) \big] du \leqslant \frac{1}{2t} \| X(0) \ – x_\ast \|^2 \ – \frac{1}{2t} \| X(t) \ – x_\ast \|^2.$$ We recover the usual rates in \(O(1/n)\), with \(t = \gamma n\), with the same caveat as above (the step-size needs to be bounded).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I covered the basic aspects of gradient flows, in particular their relationships with various forms of gradient descent, and their use in obtaining simple convergence justifications. Next months, I will cover extensions of the analyses above, in particular in terms of (1) acceleration for convex functions, where several flows and discretizations are interesting beyond the gradient flow and Euler method [12, 15], and (2) another class of functions which includes non-convex functions as encountered when learning with neural networks [16].</p>



<h2>References</h2>



<p class="justify-text">[1] Augustin Louis Cauchy. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">Méthode générale pour la résolution des systèmes d’équations simultanées</a>. Compte Rendu à l’Académie des Sciences, 25:536–538, 1847.<br />[2] Claude Lemaréchal. <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">Cauchy and the Gradient Method</a>. <em>Documenta Mathematica</em>, <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/vol-ismp.html">Extra Volume: Optimization Stories</a>, 251–254, 2012.<br />[3] Yurii Nesterov. <em>Introductory lectures on convex optimization: A basic course</em> (Vol. 87). Springer Science &amp; Business Media, 2013.<br />[4] Dimitri P. Bertsekas, <em>Nonlinear programming</em>. Athena Scientific, 1999.<br />[5] Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.<br />[6] Arnak S. Dalalyan. <a href="https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/rssb.12183">Theoretical guarantees for approximate sampling from smooth and log‐concave densities</a>. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 79(3), 651-676, 2017.<br />[7] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br />[8] Stanislaw Lojasiewicz. Sur les trajectoires du gradient d’une fonction analytique. <em>Seminari di Geometria</em>, 1983:115–117, 1982.<br />[9] Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. <a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">A nonsmooth Morse–Sard theorem for subanalytic functions</a>. <em>Journal of Mathematical Analysis and Applications</em>, 321(2):729–740, 2006.<br />[10] Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht. <a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">Gradient descent only converges to minimizers</a>. <em>Conference on learning theory</em>, 1246-1257, 2016.<br />[11] Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, Aarti Singh. <a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">Gradient descent can take exponential time to escape saddle points</a>. <em>Advances in neural information processing systems</em>, 1067-1077, 2017.<br />[12] Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d’Aspremont,. <a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">Integration methods and optimization algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 1109-1118, 2017.<br />[13] Hamed Karimi, Julie Nutini, Mark Schmidt. <a href="https://arxiv.org/pdf/1608.04636">Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition</a>. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 795-811, 2016.<br />[14] Boris T. Polyak. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">Gradient methods for minimizing functionals</a>. <em>Zh. Vychisl. Mat. Mat. Fiz.</em>, 3(4):643–653, 1963. <br />[15] Weijie Su, Stephen Boyd, Emmanuel J. Candès. <a href="http://www.jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1), 5312-5354, 2017.<br />[16] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the global convergence of gradient descent for over-parameterized models using optimal transport</a>. <em>Advances in Neural Information Processing Systems</em>, 3036-3046, 2018.</p>



<h2>Limits of stochastic gradient descent for vanishing step-sizes</h2>



<p class="justify-text"><strong>Convergence to gradient flow. </strong>We consider fixed times \(t = n \gamma \) and \(s = m \gamma\), and we let \(\gamma\) tend to zero, with thus \(m\) and \(n\) tending to infinity. Starting from the recursion $$x_{n+1} = x_{n}\, – \gamma \nabla f(x_{n})\  – \gamma \varepsilon_n,$$ we get the following by applying it \(m\) times: $$X(t+s) \ – X(t) = x_{n+m}-x_n = \ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\  – \gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ The term \(\displaystyle \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\) converges to \(\displaystyle \int_{t}^{t+s}\!\!\! \nabla f(X(u)) du\), while the term \(\gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}\) has zero expectation and variance equal to \(\gamma^2 m = \gamma s \) times the variance of each \(\varepsilon_{k+n}\), and thus it tends to zero (since \(\gamma\) tends to zero). Thus, in the limit, $$X(t+s)\  – X(t) = \ – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du,$$ which is equivalent to the gradient flow equation.</p>



<p class="justify-text"><strong>Convergence to diffusion.</strong> We consider the recursion $$x_{n+1} = x_{n}\,  – \gamma \nabla f(x_{n}) + \sqrt{2\gamma} \varepsilon_n.$$ With the same argument as above, we now get $$X(t+s) \ – X(t) = x_{n+m}-x_n =\ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\ – \sqrt{2\gamma} \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ Now the second term has zero mean but a variance proportional to \(2s\) (<em>which does not go to zero when \(\gamma\) goes to zero</em>). We can then use when \(m\) tends to infinity the <a href="https://en.wikipedia.org/wiki/Wiener_process#Wiener_process_as_a_limit_of_random_walk">limit of the sum of independent variables as a Wiener process</a>, to get $$X(t+s)\ – X(t) =\  – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du + \sqrt{2} \big[ B(t+s)-B(t) \big].$$ The <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion#Invariant_measures">limiting distribution</a> of \(X(t)\) happens to be the so-called <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs</a> distribution, with density \(\exp(-f(x))\) (the factor of \(\sqrt{2}\) was added to avoid an extra constant factor in the Gibbs distribution). More on this in a future post.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/gradient-flows/"><span class="datestr">at May 01, 2020 05:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=428">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/04/30/tcs-talk-wednesday-may-6-nathan-klein-university-of-washington/">TCS+ talk: Wednesday, May 6 — Nathan Klein, University of Washington</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Nathan Klein</strong> from the University of Washington will speak about “<em>An improved approximation algorithm for TSP in the half integral case” (abstract below).</em></p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br />
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a>suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: A classic result from Christofides in the 70s tells us that a fast algorithm for the traveling salesperson problem (TSP) exists which returns a solution at most 3/2 times worse than the optimal. Since then, however, no better approximation algorithm has been found. In this talk, I will give an overview of research towards the goal of beating 3/2 and will present the first sub-3/2 approximation algorithm for the special case of “half integral” TSP instances. These instances have received significant attention in part due to a conjecture from Schalekamp, Williamson and van Zuylen that they attain the integrality gap of the subtour polytope. If this conjecture is true, our work shows that the integrality gap of the polytope is bounded away from 3/2, giving hope for an improved approximation for the general case. This presentation is of joint work with Anna Karlin and Shayan Oveis Gharan.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/04/30/tcs-talk-wednesday-may-6-nathan-klein-university-of-washington/"><span class="datestr">at May 01, 2020 03:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/04/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/04/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>Goodbye Insta (<a href="https://mathstodon.xyz/@11011110/104009624456399772"></a>). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07630">Four pages are indeed necessary for planar graphs</a> (<a href="https://mathstodon.xyz/@11011110/104016197893535979"></a>). At STOC 1986, Yannakakis proved that planar graphs have 4-page <a href="https://en.wikipedia.org/wiki/Book_embedding">book embeddings</a>, and announced an example requiring 4 pages, but never published the example. Finally now Bekos et al. have provided detailed constructions for planar graphs requiring 4 pages. Still lost in limbo: Unger’s claim from 1992 that testing 3-page embeddability with fixed vertex ordering is polynomial.</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2020-04-16/uc-reeling-under-staggering-coronavirus-costs-the-worst-impacts-all-at-once">Universities are starting to see the costs of the lockdown</a> (<a href="https://mathstodon.xyz/@11011110/104023169399231809"></a>), in lost revenue from students and medical centers and extra expenses from the transition to remote learning — the linked story is on the University of California, but other universities are likely in similar or worse shape. So far my campus has not announced any specific cuts but colleagues predict that a hiring freeze, at least, is likely to come.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.13777">Subgraph densities in a surface</a> (<a href="https://mathstodon.xyz/@11011110/104035284995793475"></a>). In 1993 I published a paper “<a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-JGT-93.pdf">Connectivity, graph minors, and subgraph multiplicity</a>” showing that a planar graph  can have at most linearly many copies in larger planar graphs if and only if  is 3-connected. I thought it was long-forgotten, but now Huynh, Joret and Wood have generalized it in two ways: other surfaces than the plane, and other exponents than one in the number of copies.</p>
  </li>
  <li>
    <p><a href="https://github.com/microsoft/STL/pull/724">Fix boyer_moore_searcher with the Rytter correction</a> (<a href="https://mathstodon.xyz/@11011110/104040417854587667"></a>, <a href="https://news.ycombinator.com/item?id=22895932">via</a>). A 40-year-old bugfix to an even older linear-time string matching algorithm finally makes it to production code, with an admonishment that this should have been mentioned in more recent explanations of the algorithm such as <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm">Wikipedia’s</a> (since added).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/">To cheer you up in difficult times II: Mysterious matching news</a> (<a href="https://mathstodon.xyz/@11011110/104046659532923035"></a>). Gil Kalai blogs about two recent papers: one by Gal Beniamini and Noam Nisan on <a href="https://arxiv.org/abs/2001.07642">a polynomial that is one for bipartite graphs with perfect matchings and zero otherwise</a> and one by UCI colleagues Vijay Vazirani and Thorben Tröbst <a href="https://arxiv.org/abs/2003.08917">extending it to test whether a subgraph of a weighted complete bipartite graph contains a minimum-weight perfect matching</a>.</p>
  </li>
  <li>
    <p><a href="https://inkscape.org/release/inkscape-1.0rc1/">Inkscape 1.0 (release candidate) now runs natively on OS X</a> (<a href="https://mathstodon.xyz/@11011110/104052350948366314"></a>, <a href="https://news.ycombinator.com/item?id=22855357">via</a>). I still haven’t tried it, and the discussion suggests it still needs some tuning. But it seems pretty popular in free-software circles, and it’s good to know that there are free vector drawing programs out there that can compete with the (expensive) one I use, Adobe Illustrator.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir <a href="https://mathstodon.xyz/@antoinechambertloir/104018917625130123">asks for an explanation of the Schläfli graph’s Hamiltonicity</a>, or more specifically how <a href="https://en.wikipedia.org/wiki/Schl%C3%A4fli_graph">the Wikipedia article’s</a> infobox illustration can be related to more standard constructions of the graph.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2020/04/the-big-lock-down-math-off-match-5/">The eggbox puzzle</a> (<a href="https://mathstodon.xyz/@11011110/104063021972727214"></a>). <em>The Aperiodical</em> has been posting “Big Lock-Down Math-Off” posts, double-headers with a vote for which “made you say Aha! the loudest”. I chose this one with James Munro’s eggbox puzzle because the illustrations made me say Aha! This state space is a <a href="https://en.wikipedia.org/wiki/Median_graph">median graph</a>!</p>

    <p>To find the median of three -egg placements, put the th egg in the median of its three positions. In fact, it’s a distributive lattice where the meet of two placements is to move each egg to the rightmost of its two positions and the join is to move each egg to the leftmost of the two. Of course that doesn’t help much in solving the actual puzzle…</p>
  </li>
  <li>
    <p><a href="https://listserv.umd.edu/cgi-bin/wa?A2=ind2004&amp;L=CAST10&amp;P=3191">Another MDPI journal editorial board resigns</a> (<a href="https://mathstodon.xyz/@11011110/104068456940158118"></a>; 
“another” because of the 2018 <em>Nutrients</em> mass resignation). The topic appears to be related to chemical engineering. The resigning editor-in-chief describes the reason: “MDPI has stated that it will not modify the current plan for rapid, quota-driven growth, while the Editorial Board will not compromise its overarching goals of publication quality and scholarly contribution.”</p>
  </li>
  <li>
    <p>I had been using Wunderlist for to-do lists of review/submit deadlines, a shared family grocery list, etc but <a href="https://11011110.github.io/blog/2020/03/15/stay-home-linkage.html">as I posted earlier</a>, it is being strangled by new owner Microsoft to push you to their other thing. So after comparing other cross-platform shareable to-do-list apps I chose <a href="https://todoist.com/">todoist</a> because of its similar workflow and <a href="https://todoist.com/import/wunderlist">Wunderlist import feature</a> (<a href="https://mathstodon.xyz/@11011110/104074471851344995"></a>). Close second was Tom Hull’s suggestion, Trello. Not as close: Microsoft.</p>
  </li>
  <li>
    <p><a href="http://statnet.org/COVID-JustOneFriend/">Can’t I please just visit one friend?</a> (<a href="https://mathstodon.xyz/@11011110/104085917246234853"></a>) Or, how graph drawing helps us understand the importance of maintaining strict isolation.</p>
  </li>
  <li>
    <p><a href="https://www.icann.org/news/blog/icann-board-withholds-consent-for-a-change-of-control-of-the-public-interest-registry-pir">ICANN blocks .org sale</a> (<a href="https://mathstodon.xyz/@11011110/104091255847507444"></a>, <a href="https://news.ycombinator.com/item?id=23038637">via</a>). This is very good news, and follows onto their <a href="https://www.theregister.co.uk/2020/04/17/icann_california_org_sale_delay/">delay of the sale a couple of weeks ago after the intervention of the California attorney general</a>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/04/30/linkage.html"><span class="datestr">at April 30, 2020 09:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-2580044341116607459">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2020/04/an-interview-with-rob-van-glabbeek.html">An interview with Rob van Glabbeek, CONCUR Test-of-Time Award recipient</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This post is devoted to the third interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi</a>, and <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html">here</a> for the interview with <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a>.) I asked <a href="http://theory.stanford.edu/~rvg/">Rob van Glabbeek</a> (Data61, CSIRO, Sydney, Australia) a few questions via email and I am happy share his answers with readers of this blog below. Rob's words brought me back to the time when the CONCUR conference started and our field of concurrency theory was roughly a decade old. I hope that his interesting account and opinions will inspire young researchers in concurrency theory of all ages.<br /><br />Luca: You receive one of the two CONCUR ToT Awards for the period  1990-1993 for your companion papers on "The Linear Time-Branching Time  Spectrum", published at CONCUR 1990 and 1993. Could you tell us briefly  what spurred you to  embark in the encyclopaedic study of process semantics you developed in  those two papers and what were your main sources of inspiration?<br /><br />Rob: My PhD supervisors, Jan Willem Klop and Jan Bergstra, were examining<br />bisimulation semantics, but also failures semantics, in collaboration<br />with Ernst-Ruediger Olderog, and at some point, together with Jos Baeten,<br />wrote a paper on ready-trace semantics to deal with a priority operator.<br />I had a need to see these activities, and also those of Tony Hoare and<br />Robin Milner, as revealing different aspects of the same greater whole,<br />rather than as isolated research efforts. This led naturally to an<br />abstraction of their work in which the crucial difference was the semantic<br />equivalence employed. Ordering these semantics in a lattice was never<br />optional for me, but necessary to see the larger picture.  It may be in<br />my nature to relate different solutions to similar problems I encounter<br />by placing them in lattices.<br /><span class="im"></span> <br />Luca: Did you imagine at the time that the papers on "The Linear Time-Branching Time Spectrum" would have so much impact? <br /><div><br /></div><div>Rob: I didn't think much about impact in those days. I wrote the papers<br />because I felt the need to do so, and given that, could just as well<br />publish them. Had I made an estimate, I would have guessed that my<br />papers would have impact, as I imagined that things that I find<br />necessary to know would interest some others too. But I would have<br />underestimated the amount of impact, in part because I had grossly<br />underestimated the size of the research community eventually working<br />on related matters.<span class="im"><br /></span></div><div><br /></div><div>Luca: In your papers, you present characterizations of process semantics in  relational form, via testing scenarios, via modal logics and in terms of  (conditional) equational axiomatizations. Of all those  characterizations, which ones do you think have been most useful in  future work? To your mind, what are the most  interesting or unexpected uses in the  literature of the notions and techniques you developed in the "The  Linear Time-Branching Time Spectrum"? </div><div> </div><div>Rob: I think that all these characterizations together shed the light on<br />process semantics that makes them understandable and useful.<br />Relational characterizations, i.e., of (bi)simulations, are often the<br />most useful to grasp what kind of relation one has, and this is essential<br />for any use. But modal characterizations are intimately linked with<br />model checking, which is one of the more successful verification tools.<br />Equational characterizations also find practical use in verification,<br />e.g., in normalization. Yet, they are most useful in illustrating<br />the essential difference between process semantics, and thus form a guide<br />in choosing the right one for an application. To me, the testing scenarios<br />are the most useful in weighing how realistic certain semantic<br />identifications are in certain application contexts.</div><div><span class="im"></span> </div><div>Luca: How much of your later work has  built on your award-winning papers? What follow-up results of yours are  you most proud of and why?</div><div> </div><div>Rob: About one third, I think.</div><div><ul><li>My work on action refinement, together with Ursula Goltz, extends theclassification of process semantics in a direction orthogonal to the linear time - branching time spectrum. I am proud of this work mostly because, for a distinct class of applications, action refinement is great criterion to tell which semantics equivalences one ought to prefer over others. These process semantics have also been used in later work on distributability with Ursula and with Jens-Wolfhard Schicke Uffmann. Distributability tells us which distributed systems can be cast as sequential systems cooperating asynchronously, and which fundamentally cannot.</li><li>I wrote several papers on Structural Operational Semantics, many jointly with Wan Fokkink, aiming to obtain congruence results for semantic equivalences. These works carefully employed the modal characterizations from my spectrum papers. I consider this work important because congruence properties are essential in verification, as we need compositionality to avoid state explosions. </li><li>I used a version of failure simulation semantics from my second spectrum paper in work with Yuxin Deng, Matthew Hennessy, Carroll Morgan and Chenyi Zhang, characterizing may- and must testing equivalences for probabilistic nondeterministic processes. Probability and nondeterminism interact in many applications, and I think that may- and must testing are the right tools to single out the essence of their semantics.</li></ul></div><div><span class="im"><br /></span> </div><div>Luca: If I remember correctly, your master thesis was in pure mathematics. Why  did you decide to move to a PhD in computer science? What was it like  to work as a PhD student at CWI in the late 1980s? How did you start  your collaborations with Jos Baeten and fellow PhD student Frits  Vaandrager, amongst others? </div><div> </div><div>Rob: I was interested in mathematical logic already since high school,<br />after winning a book on the foundations of logic set theory in a<br />mathematics Olympiad.  I found the material in my logic course,<br />thought at the University of Leiden by Jan Bergstra, fascinating, and<br />when doing a related exam at CWI, where Jan had moved to at that<br />time, and he asked me to do a PhD on related subjects, I say no reason<br />not to. But he had to convince me first that my work would not involve<br />any computer science, as my student advisor at The University of<br />Leiden, Professor Claas, had told us that this was not a real<br />science. Fortunately, Jan had no problem passing that hurdle.<br /><br />I was the third member of the team of Jan Bergstra and Jan Willem<br />Klop, after Jos Baeten had been recruited a few months earlier, but in<br />a more senior position. Frits arrived a month later than me. As we<br />were exploring similar subjects, and shared an office, it was natural<br />to collaborate. As these collaborations worked out very well, they<br />became quite strong.<br /><br />CWI was a marvelous environment in the late 1980s. Our group gradually<br />grew beyond a dozen members, but there remained a much greater<br />cohesion then I have seen anywhere else. We had weekly process<br />algebra meetings, where each of us shared our recent ideas with others<br />in the group. At those meetings, if one member presented a proof, the<br />entire audience followed it step by step, contributing meaningfully<br />where needed, and not a single error went unnoticed.</div><div><span class="im"><br /></span> </div><div>Luca: Already as PhD student, you took up leadership roles within the  community. For instance, I remember attending a workshop on "Combining  Compositionality and Concurrency" <span>in March 1988,</span> which you  co-organized with Ursula Goltz and Ernst-Ruediger Olderog. Would you  recommend to a PhD student that he/she become involved in that kind of  activities early on in their career? Do you think that doing so had a  positive effect on your future career?  </div><div> </div><div>Rob: Generally I recommend this. Although I co-organized this workshop<br />because the circumstances were right for it, and the need great; not<br />because I felt it necessary to organize things.  My advice to PhD<br />students would not be specifically to add a leadership component to<br />their CV regardless of the circumstances, but instead do take such an<br />opportunity when the time is right, and not be shy because of lack of<br />experience.</div><div> </div><div>This workshop definitively had a positive effect on my future career.<br />When finishing my PhD I inquired at Stanford and MIT for a possible<br />post-doc position, and the good impressions I had left at this<br />workshop were a factor in getting offers from both institutes.<br />In fact, Vaughan Pratt, who was deeply impressed with work I had<br />presented at this workshop, convinced me to apply straight away for an<br />assistant professorship, and this led to my job at Stanford.   </div><div><span class="im"><br /></span> </div><div>Luca: I still have vivid memories of your talk at the first CONCUR conference  in 1990 in Amsterdam, where you introduced notions of black-box  experiments on processes using an actual box and beer-glass mats. You  have attended and contributed to many of the editions of the conference  since then. To your mind, how has the focus of CONCUR changed since its  first edition in 1990? </div><div> </div><div>Rob: I think the focus on foundations has shifted somewhat to more applied<br />topics, and the scope of "concurrency theory" has broadened significantly.<br />Still, many topics from the earlier CONCURs are still celebrated today.</div><div><span class="im"><br /></span> </div><div>Luca: The last forty years have seen a huge amount of work on process algebra  and process calculi. However, the emphasis on that field of research  within concurrency theory seems to have diminished over the last few  years, even in Europe. What advice would you give to a young researcher  interested in working  on process calculi today? </div><div> </div><div>Rob: Until 10 years ago, many people I met in industry voiced the opinion<br />that formals methods in general, and process algebra in particular,<br />has been tried in the previous century, and had been found to not to<br />scale to tackle practical problems. But more recently this opinion is<br />on the way out, in part because it became clear that the kind of<br />problems solved by formal methods are much more pressing then originally<br />believed, and in part because many of the applications that were<br />deemed to hard in the past, are now being addressed quite adequately.<br />For these reasons, I think work on process calculi is still a good bet<br />for a PhD study, although a real-world application motivating the<br />theory studied is perhaps harder needed than in the past, and toy<br />examples just to illustrate the theory are not always enough.</div><div><span class="im"><br /></span> </div><div>Luca: What are the research topics that currently excite you the most?</div><div> </div><div>Rob: I am very excited about showing liveness properties, in Lamport's<br />sense, telling that a modeled system eventually achieves a desirable outcome.<br />And even more in creating process algebraic verification frameworks<br />that are in principle able to do this. I have come to believe that<br />traditional approaches popular in process algebra and temporal logic<br />can only deal with liveness properties when making fairness<br />assumptions that are too strong and lead to unwarranted conclusions.<br />Together with Peter Hoefner I have been advocating a weaker concept of<br />fairness, that we call justness, that is much more suitable as a basis<br />for formulating liveness properties. Embedding this concept into the<br />conceptual foundations of process algebra and concurrency theory, in<br />such a way that it can be effectively used in verification, is for a me<br />a challenging task, involving many exciting open questions.<br /><br />A vaguely related subject that excites me since the end of last year,<br />is the extension of standard process algebra with a time-out operator,<br />while still abstaining from quantifying time. I believe that such an<br />extension reaches exactly the right level of expressiveness necessary<br />for many applications.<br /><br />Both topics also relate with the impossibility of correctly expressing<br />expressing mutual exclusion in standard process algebras, a discovery<br />that called forth many philosophical questions, such as under which<br />assumptions on our hardware is mutual exclusion, when following the<br />formal definitions of Dijkstra and others, even theoretically implementable.</div><div> </div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2020/04/an-interview-with-rob-van-glabbeek.html"><span class="datestr">at April 30, 2020 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7481748881751465174">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html">A Guest Blog on the Pandemic's affect on disability students</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>I asked my Grad Ramsey Theory class to email me about whatever thoughts they have on the pandemic that they want to share with the world, with the intend of making some of them into a blog post. I thought there would be several short thoughts for one post. And I may still do that post. But I got a FANTASTIC long answer from one Emily Mae Kaplitz. Normally I would ask to shorten or edit a guest post, but I didn't do that here since that might make it less authentic.</div><div><br /></div><div>Here is Emily Kaplitz's email (with her enthusiastic permission)</div><div><br /></div><div>--------------------------------------------------------------------------------------</div><div><br /></div><div>Ok so this might be super ranty, (It definitely is.) but I think it is super important to bring up in a blog post written by an academic that will be probably read by other academics. </div><div><br /></div><div>The students that are being most affected by this pandemic with online learning are disability students. As a disability student, we carefully cultivate the way that we learn best based off of years of trial and error. This is harder than anything else, we have to face in our lifetime. Most of the time disability students are left on the back burner and that statement is so much more prevalent right now. My friends brother is autistic. He is struggling so much right now because he is at home. Disability students learn what environment works best for them and at home is usually not the best place. We have to split our lives into different boxes that each have different tools to help us get our brains to focus and work well when we need them too. Disability students will rely on everything being planned out, so that they can succeed. Teachers and professors cannot understand the stress and strain that having to work at home puts on the student. Every time I go to another school, it is a struggle to figure out what new thing I need to add into the mix and what old thing I need to throw away. It's exhausting, but when I go from one school to another I at least know that the basics are the same. I sit in a classroom, the professors lecturer, and then I do work at home that is assigned to me. Changing to online changes that dynamic so much. A professor cannot see when a student is visibly struggling with a topic because we'll all behind computers. A neurotypical person might ask, "well why don't you just ask a question? Why don't you just let the professor know that you don't understand". Let me answer that simply. If all your life you've been silenced because of something that you cannot control, is your first reaction to speak out or to stay silent. It is so hard for disability students to ask a question after we've been labeled the dumb kid. Every time we ask a question, we always have the thought of: is this going to make me sound stupid. We've worked so hard to eliminate that word from our vocabulary and from others who will throw that word back at us. Disability students are being left in the hands of their parents and teachers/professors who do not understand us and our needs even if they try to or want to. It is so hard for us to explain what our normal is because we don't live your normal and therefore don't know the difference. Many disability students have their confidence slashed the moment they enter a classroom and realize that they are not like the other kids. Even more so because they don't understand why they aren't. Disability students are one of the most hard-working individuals when we have a cheerleader to cheer us on because it's hard. It's harder than anything anyone has to do. Because no one listens to you when you are stupid and no one cares for you if you're not easy to care for unless they are given a specific reason to. Fighting a losing battle every day is awful. Now imagine all of your weapons that you have carefully crafted over the years have been taken away and you are left defenseless. While we have things like ADS that are supposed to help support us, it's not enough. Just like putting a Band-Aid on an open infected wound will not be enough. Now more than ever we need to learn from this as academics. We need to learn that helping disability students does not only help disability students. It helps all students because all students learn differently. All students if given the chance can excel at any field that we put them in. We just have to figure out the best way to get that student to shine. That is one of the reasons why I am a PhD student right now. I saw in the tutoring center at my undergrad how many students came to me with so much frustration about something they are doing in class. Both students with disabilities and without. These students are constantly apologizing because they don't understand something. In one session by just changing the way that we talk about a subject the student was able to get it in less time than the professor taught it. I've had students come to me after an exam and tell me that the only reason they got the grade that they did was because in their head was my voice coaching them on a subject. We are not teaching optimally. We are teaching the way that it has been done for years and years and years and that is not the best way to teach. It might be the best way to teach the strongest links but really the link that matters the most is the weakest link that will snap under pressure because you can't pull a tractor with a broken link. Disability students think differently. Imagine how many impossible problems we can solve when we have people that think differently. But that's just my two cents as a disability student who is struggling and sees other disability students struggling every day. And really just wants to help all students succeed.</div><div><br /></div><div>I blame any misspellings, grammar errors, and run on sentences on my speech to text and text to speech. This was a long email and if we were on tumblr, I would post a potato at the end. Since we aren't, I will leave this email with this. Thank you for taking the time to read this rant. Even if you don't include this in your blog post, I believe one person reading this has made the difference.</div><div><br /></div><div>Thanks!</div><div><br /></div><div>Emily Mae Kaplitz</div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html"><span class="datestr">at April 29, 2020 01:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/063">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/063">TR20-063 |  On the Existence of Algebraically Natural Proofs | 

	Prerona Chatterjee, 

	Mrinal Kumar, 

	C Ramya, 

	Ramprasad Saptharishi, 

	Anamay Tengse</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For every constant c &gt; 0, we show that there is a family {P_{N,c}} of polynomials whose degree and algebraic circuit complexity are polynomially bounded in the number of variables, and that satisfies the following properties:
* For every family {f_n} of polynomials in VP, where f_n is an n variate polynomial of degree at most n^c with bounded integer coefficients and for N = \binom{n^c + n}{n}, P_{N,c} -vanishes- on the coefficient vector of f_n.
* There exists a family {h_n} of polynomials where h_n is an n variate polynomial of degree at most n^c with bounded integer coefficients such that for N = \binom{n^c + n}{n}, P_{N,c} -does not vanish- on the coefficient vector of h_n.

In other words, there are efficiently computable defining equations for polynomials in VP that have small integer coefficients.
In fact, we also prove an analogous statement for the seemingly larger class VNP. Thus, in this setting of polynomials with small integer coefficients, this provides evidence -against- a natural proof like barrier for proving algebraic circuit lower bounds, a framework for which was proposed in the works of Forbes, Shpilka and Volk (2018), and Grochow, Kumar, Saks and Saraf (2017).

Our proofs are elementary and rely on the existence of (non-explicit) hitting sets for VP (and VNP) to show that there are efficiently constructible, low degree defining equations for these classes, and also extend to finite fields of small size.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/063"><span class="datestr">at April 29, 2020 01:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
