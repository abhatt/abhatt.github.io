<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc ‚Äì QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs ‚Äì Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I‚Äôm a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Caf√©: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 07, 2019 06:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/067">TR19-067 |  Sign rank vs Discrepancy | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Sign-rank and discrepancy are two central notions in communication complexity. The seminal work of  Babai, Frankl, and Simon from 1986  initiated an active line of research that investigates  the gap between these two notions.
In this article, we establish the strongest possible separation  by constructing a Boolean matrix whose sign-rank is only $3$, and yet its discrepancy is  $2^{-\Omega(n)}$. We note that every matrix of sign-rank $2$ has discrepancy $n^{-O(1)}$.
Our result in particular implies that there are Boolean functions with $O(1)$ unbounded error randomized communication complexity while having $\Omega(n)$ weakly unbounded error randomized communication complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/067"><span class="datestr">at May 07, 2019 09:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15840">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/">The Network Coding Conjecture Is Powerful</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>More hard Boolean functions</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg"><img src="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg?w=197&amp;h=162" alt="" width="197" class="alignright wp-image-15841" height="162" /></a></p>
<p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent <a href="https://arxiv.org/abs/1902.10935">paper</a> which we just <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">discussed</a>. </p>
<p>
Today Ken and I will update our discussion. </p>
<p>
Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL is:</p>
<blockquote><p><b>Theorem 1</b> <em> If the NCC is true, then every Boolean circuit that computes the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function has size <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" />. </em>
</p></blockquote>
<p></p><p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function is: Given an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit number <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and a number <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \le k \le n}" class="latex" title="{1 \le k \le n}" />, compute the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k}}" class="latex" title="{2^{k}}" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Ctimes+2%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x \times 2^{k}. " class="latex" title="\displaystyle  x \times 2^{k}. " /></p>
<p>This is a special case of the integer multiplication problem. In symbols it maps <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B0%5Ek+x+0%5E%7Bn-k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^k x 0^{n-k}}" class="latex" title="{0^k x 0^{n-k}}" />, as in our photo above. </p>
<p>
Our point, however, is not about integer multiplication. Nor even about NCC‚Äîno knowledge of it will be needed today, so read on even if you are not aware of NCC. No. Our point is that a whole lot of other Boolean functions would inherit the same <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> circuit lower bound as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. And several aspects of that seem troubling.</p>
<p>
</p><p></p><h2> Some Worry </h2><p></p>
<p></p><p>
We are impressed by the AFKL paper but also worried. Proving a super-linear lower bound in the unrestricted Boolean complexity model has long been considered a difficult problem. Maybe a hopeless problem. Yes they are proving it not for a single-output function; they are proving it for a multiple-output function. Still I thought that it seems too good to be correct. Even worse, assuming NCC they also resolve other open problems in complexity theory. I am worried.</p>
<p>
What we suggest is to catalog and study the consequences of their results. If we find that their results lead to a contradiction, then there was something to be worried about. Or perhaps it would mean that NCC is false. If we find no contradiction, then everything we discover is also a consequence of NCC. Either way we learn more.</p>
<p>
</p><p></p><h2> AFKL Functions </h2><p></p>
<p></p><p>
Let‚Äôs call a Boolean function an <i>AFKL function</i> provided it has Boolean circuit complexity <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> if the NCC is true. Thanks to AFKL, we now know that integer multiplication is an AFKL function. I started to think about: What functions are in this class? Here are some examples: </p>
<ul>
<li>
Integer multiplication <p></p>
</li><li>
Integer multiplication by a power of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> <p></p>
</li><li>
Discrete convolution <p></p>
</li><li>
Sparse convolution
</li></ul>
<p>
We describe the last three next. We show they have linear size-preserving reductions from the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function.</p>
<p>
</p><p></p><h2> The Flip Function </h2><p></p>
<p></p><p>
Define <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%3A+%5C%7B0%2C1%5C%7D%5E%7Bn%7D+%5Crightarrow+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" class="latex" title="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BFLIP%7D_%7Bn%7D%280%5E%7Bk%7D1%5Calpha%29%3D+1%5Calpha+0%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " class="latex" title="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \ge 1}" class="latex" title="{k \ge 1}" />. For any input not of this form, let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> be <img src="https://s0.wp.com/latex.php?latex=%7B0%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^{n}}" class="latex" title="{0^{n}}" />.</p>
<blockquote><p><b>Theorem 2</b> <em> The Boolean function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is an AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,k}" class="latex" title="{x,k}" /> be the input to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x| = n}" class="latex" title="{|x| = n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \leq n}" class="latex" title="{k \leq n}" /> in binary. In linear size we can test <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 0}" class="latex" title="{k = 0}" />, when there is nothing to do, so presume <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \geq 1}" class="latex" title="{k \geq 1}" />. The first step is to create </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1}. " class="latex" title="\displaystyle  0^{n-k}10^{k-1}. " /></p>
<p>This is just binary-to-unary conversion and has linear-size circuits‚Äîas in multiplex decoding and as remarked by AFKL. This becomes the first <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits of an application of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> to the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit string </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D+x.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1} x. " class="latex" title="\displaystyle  0^{n-k}10^{k-1} x. " /></p>
<p>It yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++10%5E%7Bk-1%7D+x+0%5E%7Bn-k%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  10^{k-1} x 0^{n-k}. " class="latex" title="\displaystyle  10^{k-1} x 0^{n-k}. " /></p>
<p>Changing the first bit to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> then leaves the desired output of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The point is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is a super-simple function. It just moves the initial block of <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />‚Äòs in a string to the end. It is amazing that this function should have only non-linear, indeed <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />-sized, circuits. </p>
<p>
This also means that Ken‚Äôs <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">function</a>, which takes <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%2C2%5C%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in \{0,1,2\}^*}" class="latex" title="{x \in \{0,1,2\}^*}" /> and moves all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />s to the end of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, is hard even in the special cases where all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />‚Äòs are at the front. What‚Äôs strange is that Ken proves his function equivalent to another special case where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x|}" class="latex" title="{|x|}" /> is even and exactly half the characters are <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. This latter case is one in which <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> is easy, but the two cases are separate. All this is touch-and-go enough to compound our ‚Äúworry.‚Äù</p>
<p>
</p><p></p><h2> The Sparse Convolution Function </h2><p></p>
<p></p><p>
The following is also an AFKL function. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi%3D1%7D%5E%7Bn-l%7D+w_%7Bi%7D+%5Cwedge+x_%7Bi%2Bl%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " class="latex" title="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bl%3D1%2C%5Cdots%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{l=1,\dots,n}" class="latex" title="{l=1,\dots,n}" /> where an empty OR is defined to be <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. This can even further be restricted to the case where exactly one of the <img src="https://s0.wp.com/latex.php?latex=%7Bw_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_{i}}" class="latex" title="{w_{i}}" /> are <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and the rest are <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. Call this the <i>sparse convolution function</i>.</p>
<blockquote><p><b>Theorem 3</b> <em> The sparse convolution is a monotone AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  We will give a sketch of why this is true. Define 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi+%3D+1%7D%5E%7Bn-l%7D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D+%5Cwedge+x_%7Bi%2Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " class="latex" title="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " /></p>
<p>It is not hard to show that this yields the FLIP function. We can reduce computing it to a convolution of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{i}}" class="latex" title="{x_{i}}" />‚Äòs and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28i%29+%3D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " class="latex" title="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " /></p>
<p>The key is to note that exactly one <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> will be non-zero, and so the convolution is sparse. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The sparse convolution function raises an interesting question: Are the methods for sparse FFT useful here? The lower bound for AFKL functions suggests that they are not applicable. </p>
<p>
</p><p></p><h2> Is the NCC-Boolean Connection New? </h2><p></p>
<p></p><p>
The subtitle of our <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">post</a> marveled that a core-theory advance on circuits for multiplication had come via the practical side of throughput in computer networks. AFKL deserve plaudits for linking two communities. We should mention that one theoretician we both know well, Mark Braverman, with his students Sumegha Garg and Ariel Schvartzman at Princeton, <a href="https://arxiv.org/pdf/1608.06545.pdf">proved</a> a fact about NCC that is relevant to this discussion:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="BGS"></a> Either NCC is false, or bit-operations save a whole <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7B%5COmega%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{(\log n)^{\Omega(1)}}" class="latex" title="{(\log n)^{\Omega(1)}}" /> factor in the network size. </em>
</p></blockquote>
<p></p><p>
Even this paper, however, does not address lower bounds on Boolean circuits. The only prior link between NCC and Boolean complexity is a 2007 <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v14i1r44">paper</a> by S√∏ren Riis, which is cited by AFKL, and has a 2011 <a href="http://emis.impa.br/EMIS/journals/EJC/Volume_18/PDF/v18i1p192.pdf">followup</a> by Demetres Christofides and Klas Markstr√∂m. The paper by Riis has a new ‚Äúguessing game‚Äù on graphs and a demonstration that a lower-bound conjecture of Leslie Valiant needs to be rescued by dividing by a <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> factor. Theorem <a href="https://rjlipton.wordpress.com/feed/#BGS">4</a>, however, seems to say that no such <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> shading can apply to NCC.</p>
<p>
When we ask Google ‚Äúnetwork coding conjecture Boolean circuit lower bounds‚Äù (without quotes), the first page shows AFKL, our posts, and this 2014 <a href="https://people.csail.mit.edu/rrw/ccc14-survey.pdf">survey</a> by Ryan Williams‚Äîwhich mentions neural networks but not NCC. On the next page of hits we see Riis and the followup paper but nothing else that seems directly relevant. Nor does appending `-multiplication‚Äô help screen out AFKL and our posts.</p>
<p>
There is said to be empirical evidence for NCC. We wonder, however, whether that has reached the intensity of thought about circuit lower bounds. We say this because the implications from NCC make three giant steps:</p>
<ol>
<li>
Not only does it assert a super-linear circuit lower bound (okay, for a function with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> output bits), but‚Ä¶ <p></p>
</li><li>
‚Ä¶it asserts <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />‚Ä¶ <p></p>
</li><li>
‚Ä¶for functions that are easily in Turing machine linear time.
</li></ol>
<p>
So one side of our worry is whether NCC can actually shed light on so many fundamental issues from complexity theory, more than absorbing light. At the very least, AFKL have re-stimulated interest in all of these issues. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> hard? Is NCC true? What other Boolean functions are AFKL functions? What about other consequences of the NCC to complexity theory?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/"><span class="datestr">at May 07, 2019 03:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kintali.wordpress.com/?p=1235">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kintali.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/">Preventing future college admissions scandals using Blockchain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="graf graf--p graf-after--h3" id="5bf6">The recent¬†<a href="https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener" target="_blank">college admissions scandal</a>¬†resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‚ÄòOperation Varsity Blues‚Äô uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases like ‚Äúbroken admissions system‚Äù, ‚Äúrich can buy their way into college‚Äù, etc etc.</p>
<p class="graf graf--p graf-after--p" id="4b6a">There must be several other cases of fraud that go unnoticed on a daily basis. It‚Äôs in human nature to shortcut the rules, collude and cheat to achieve one‚Äôs own short-term goals, hoping to get away with one‚Äôs fraudulent actions. Let‚Äôs discuss how to use Blockchain technology and create a rigorous system to prevent some aspects of such scandals in future.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="b66e"><strong class="markup--strong markup--p-strong">Problems</strong>: Fake athletic certificates and phony athletic profile. Taking photos of students on a stationary rowing machine. Photoshopping students‚Äô face on another athlete‚Äôs photo.</p>
<p class="graf graf--p graf-after--p" id="ad90"><strong class="markup--strong markup--p-strong">Solution</strong>: A genuine high-school athlete achieves his/her athletic credentials during a four year period. Getting a genuine athletic certificate involves achieving several intermediate goals. For example, if you achieved a black belt in karate in your 11th grade, you must have progressed through beginner level (with white, yellow and orange belts), intermediate level (with green, blue, purple, brown and red belts) and then reached the advanced level (with a black belt).¬†<a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">TrueCerts</a>¬†technology allows sports coaching centers to create certificates for each of these intermediate athletic achievements, sign them using their private keys and post only the SHA-256 hash of the certificate on a public blockchain, thus immutably time-stamping each achievement at the specific day/time the athlete achieved it. The athletic photos can be time-stamped similarly.</p>
<p class="graf graf--p graf-after--p" id="2c0c">This process achieves the seemingly impossible combination of¬†<strong class="markup--strong markup--p-strong">security, privacy and transparency¬†</strong>!! Security is achieved by the asymmetric key encryption. Privacy is achieved by the one-way nature of the cryptographic hash function¬†<a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">SHA 256</a>. Transparency is achieved by the fact that anybody with the access to the sports certificate (upon student‚Äôs consent) can compute the SHA 256 hash of the certificate and verify that it is stored on a public blockchain validly signed by the private key of the issuer (sports coaching center). This eliminates the fraudulent behavior of creating a bunch of fake credentials and photos, all at once, in a brief period of time. Using a fairly decentralized public blockchain is very important here.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="c253"><strong class="markup--strong markup--p-strong">Problem</strong>: Bribing coaches to accept certain students in their sports teams or issue fake credentials.</p>
<p class="graf graf--p graf-after--p" id="6290"><strong class="markup--strong markup--p-strong">Solution</strong>: This problem can be solved by having several people (perhaps five coaches, some administrative assistants, one principal, one vice-principal, etc) in the organization collectively responsible (using multi-signature wallets and m-of-n signatures) to issue credentials. Each of the involved person is accountable for every issued certificate.</p>
<p class="graf graf--p graf-after--p" id="eb99">Bribing one coach is easy. Bribing ten people is hard. People often hesitate bribing multiple people. Collusion becomes increasingly hard when you increase the size of the group involved. When there are more people involved, there is a higher chance that at least one of them is honest (and brave) to overcome the pressure of the others and blow the whistle.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="0d42"><strong class="markup--strong markup--p-strong">Problem</strong>: Fake college entrance exam (SAT, ACT) test scores</p>
<p class="graf graf--p graf-after--p" id="fe6e"><strong class="markup--strong markup--p-strong">Solution</strong>: Current paper-based test score issuance and verification system is too time-consuming and error-prone. These credentials can be easily faked or tampered with. An ideal solution involves creating a digital certificate, validly signed (or multi-signed) by the issuer and time-stamped with a¬†<a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SHA 256</a>¬†hash on a public blockchain. See¬†<a href="https://medium.com/@truecerts/dr-kintalis-motivation-behind-developing-truecerts-platform-aba93f4d290a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">my previous post</a>¬†about preventing fraud in academic transcripts and making the entire system efficient.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="b957"><strong class="markup--strong markup--p-strong">Problem</strong>: Other students taking entrance tests on your behalf.</p>
<p class="graf graf--p graf-after--p" id="0e07"><strong class="markup--strong markup--p-strong">Solution</strong>: This is a problem of verifying the identity of the student taking the test. The current system of using paper-based credentials is broken. It‚Äôs easy to create fake driver‚Äôs license, passports etc. At¬†<a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>, we have created an automated multi-step identity verification that combines your standard KYC identity procedures, utility bills and more importantly biometrics. We define identity as a several data points achieved over a period of time, not just one piece of paper. It‚Äôs very hard to cheat all of these steps. If you have a look-alike twin then consider yourself lucky¬†<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="üôÇ" style="height: 1em;" class="wp-smiley" /></p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="a16a"><strong class="markup--strong markup--p-strong">Partially solvable problems</strong>: Bribing officials to change student‚Äôs answers in paper-based exam can be solved to certain extent by using a completely computer-based exam. Bribing proctors to tell the answers to a student (during the test) can be solved with computer-vision-based cheating detection software. Some photoshopped images can be detected using image analysis techniques.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="4d0f"><strong class="markup--strong markup--p-strong">Hard to solve problems</strong>: (1) Getting a fake medical certificate that your kid requires an isolated room to take the test and then bribing the proctor to tell him/her all the answers during the test. (2) Laundering bribes using a non-profit entity. Phew‚Ä¶. These things actually happened during this college admissions scandal. As the famous saying goes ‚Äúa person is capable of as much atrocity as he/she has imagination‚Äù.</p>
<p>¬†</p>
<p class="graf graf--p graf-after--p" id="5ad0">In summary, combining the existing technologies we can solve several of the above mentioned problems and simultaneously achieve¬†<strong class="markup--strong markup--p-strong">security, privacy and transparency</strong>. The main goal here is to make the bad people‚Äôs job as difficult as possible and simultaneously making the good people‚Äôs job very efficient.</p>
<p class="graf graf--p graf-after--p" id="a4f5">If you want to know more about how we (at¬†<a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>) are using Blockchain and other technologies to prevent fraud and corruption in broad range of areas,¬†<a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">contact us</a>.</p>
<p class="graf graf--p graf-after--p" id="e889">Stay tuned for my next post about a huge list of real-world fraud and corruption stories that can be prevented rigorously by using cutting-edge technologies.</p>
<p><img src="https://kintali.files.wordpress.com/2019/05/truecertstrustsimplified.png?w=660" alt="TrueCertsTrustSimplified" class="alignnone size-full wp-image-1236" /></p></div>







<p class="date">
by kintali <a href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/"><span class="datestr">at May 07, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2431469456247088523">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html">Thoughts on the recent Jeopardy streak (SPOILERS)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
James Holzhauer¬† has won 22 consecutive games of Jeopardy and has made around 1.6 million dollars. Nice work if you can get it. Here are some thoughts no this<br />
<br />
1) Before James H the records for number of consecutive games was, and still is, Ken Jennings winning 74 in a row, and second place was 20. I was surprised that Ken was that much better than the competition.<br />
<br />
2) Before James H the record for amount of money in normal play (not extra from, say, tournament of champions or losing to a computer) was around 400,000. I was surprised that Ken was that much better than the competition.<br />
<br />
3) James is obliterating the records for most wins in a single game. He holds the top 12 records for this.¬† This is due to his betting A LOT on the daily doubles and the final jeop, as well as of course answering so many questions right.<br />
<br />
4) One reason players in Jeopardy don't have long streaks is fatigue. The actually play<br />
5 games a day, two days of the week.¬† James H is getting a break since he has two weeks off now since they will soon have the Teachers Tournament. This could work either way--- he gets a break or he loses being in-the-zone.<br />
<br />
5) James strategy is:<br />
<br />
a) Begin with the harder (and more lucrative) questions.<br />
<br />
b) Bet A LOT on the daily doubles (which are more likely to be in the more lucrative questions) and almost always go into final jeop with more than twice your opponent (He failed to do this only once.)<br />
<br />
c) Bet A LOT on Final Jeop- though not enough so that if you lose you lose the game. I think he's gotten every Final Jeop question right.<br />
<br />
For more on his strategy see this article by Oliver Roeder in Nate Silvers Blog:¬†<a href="https://fivethirtyeight.com/features/the-man-who-solved-jeopardy/">here</a><br />
<br />
6) I tend to think of this as being a high-risk, high-reward strategy and thus it is unlikely he will beat Ken Jennings, but every time he wins that thought seems sillier and sillier. While we are here, how likely is it that someone will beat Ken Jennings? In an article before all of this Ben Morrison in Nate Silvers Blog wrote that it was quite likely SOMEONE would break Ken J's record,¬† see¬†<a href="https://fivethirtyeight.com/features/ken-jennings-has-nothing-on-joe-dimaggio/">here</a>.<br />
<br />
7) OKAY, how does James H compare to Ken J? According to Oliver Roeder in Nate Silvers Blog,<br />
<a href="https://fivethirtyeight.com/features/the-battle-for-jeopardy-supremacy/">here</a>, they are similar in terms of percent of questions answered right, but James H bets so much more (bets better?) which is why he is getting so much money. I'll be curious to see a head-to-head contest at some point. But to the issue at hand, they don't give James H that good a chance to break Ken J's record.<br />
<br />
8) Jeop used to have a¬† 5-game limit. Maybe that was a good idea- its not that interesting seeing the same person with the same strategy win 22 in a row. Also, the short-talk-with-Alex T-- James is running out of interesting things to say. I wonder what Alex did with Ken J after 50 games.<br />
``So Ken, I hear you're good at Jeopardy''<br />
<br />
9) Misc: Ken J was the inspiration for IBM to do Watson.<br />
<br />
10) Will future players use James Strategy? Note that you have to be REALLY GOOD in the first place for it to help you. Maybe a modified version where you go for the lucrative questions and bet a lot on Daily Doubles (more than people have done in the past) when its an area you know really well (I'll take Ramsey Theory for $2000.)<br />
<br />
11) I used to DVR and watch Jeop but didn't mind if I was a few behind. Now I have to stay on top of it so articles like those pointed to above don't give me a spoiler.<br />
<br />
12) My prediction: He will beat Ken Jenning for money but not for number-of-games. I have no real confidence in these predictions.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html"><span class="datestr">at May 07, 2019 12:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/">Answer to TYI 37: Arithmetic Progressions in 3D Brownian Motion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Consider a Brownian motion in three dimensional space. <a href="https://gilkalai.wordpress.com/2019/03/07/test-your-intuition-or-simply-guess-37-arithmetic-progressions-for-brownian-motion-in-space/">We asked (TYI 37)</a> What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, <img src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1,x_2, x_t" class="latex" title="x_1,x_2, x_t" />, so that all <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2B1%7D-x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{i+1}-x_i" class="latex" title="x_{i+1}-x_i" /> are equal.)</p>
<p>Here is what you voted for</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png?w=640" alt="" class="alignnone size-full wp-image-17423" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Final-results</strong></span></p>
<p>Analysis of the poll results:¬† Almost surely 2 is the winner with 30.14% of the 209 votes, and almost surely infinity (28.71%) comes close at second place. In the¬† third place is¬† almost surely 3 (14.83%),¬† and then comes positive probability for each integer (13.4%), almost surely 5 (5.26%),¬† almost surely 6 (2.87%), and¬† almost surely 4 (2.39%).</p>
<h2>Test your political intuition: which coalition is going to be formed?</h2>
<p>Almost surely 2 (briefly AS2) and almost surely infinity (ASI) can form a government¬† with no need for a larger coalition. But they represent two political extremes. Is AS3 politically closer to AS2 or to ASI? ‚Äúk with probability p_k for every k&gt;2‚Äù (briefly, COM) represent a complicated political massage. Is it closer to AS2 or to ASI?</p>
<a name="pd_a_10312542"></a>
<div style="display: inline-block;" class="PDS_Poll" id="PDI_container10312542"></div>
<div id="PD_superContainer"></div>
<noscript>&lt;a href="https://polldaddy.com/poll/10312542"&gt;Take Our Poll&lt;/a&gt;</noscript>
<p>You are most encouraged to participated in the new political poll for some coalitions that were offered and make a comment on your thoughts on which coalition will be formed.¬† This could be a lovely discussion. (See the old posts on <a href="https://gilkalai.wordpress.com/2009/02/16/which-coalition/">which coalition</a> <a href="https://gilkalai.wordpress.com/2009/02/17/which-coalition-to-form-2/">will be formed</a>.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-br.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-br.png?w=131&amp;h=300" alt="" width="131" class="alignnone size-medium wp-image-17424" height="300" /></a> <a href="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png?w=133&amp;h=300" alt="" width="133" class="alignnone size-medium wp-image-17425" height="300" /> </a><a href="https://gilkalai.files.wordpress.com/2019/05/poll189.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll189.png?w=127&amp;h=300" alt="" width="127" class="alignnone size-medium wp-image-17427" height="300" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Partial results. It was exciting to see how the standing of the answers changed in the process of counting the votes.</strong></span></p>
<p>And the correct answer is: <span id="more-17422"></span></p>
<h2><strong>5 (FIVE)</strong></h2>
<p>See the paper:</p>
<p class="title mathjax">Itai Benjamini and Gady Kozma: <a href="https://arxiv.org/abs/1810.10077">Arithmetic progressions in the trace of Brownian motion in space</a></p>
<p>Comments on the mathematics are welcome too!</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/"><span class="datestr">at May 06, 2019 09:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4240">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Online Optimization Post 3: Follow the Regularized Leader</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its analysis have statistical physics interpretations, but even this observation brings up more questions than it answers. The Gibbs distribution, for example, does put more weight on lower-energy states, and so it makes sense in an optimization setting, but to get good approximations one wants to use lower temperatures, while the distributions used by the multiplicative weights algorithms have temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\epsilon}" class="latex" title="{2\epsilon}" /> is the final ‚Äúamortized‚Äù regret bound, so that one uses, quite counterintuitively, higher temperatures for better approximations. </p>
<p>
Furthermore, it is not clear how we would generalize the ideas of multiplicative weights to the case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is anything other than the set of distributions.</p>
<p>
Today we discuss the <em>‚ÄúFollow the Regularized Leader‚Äù</em> method, which provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will then see how we can ‚Äúdiscover‚Äù the definition and analysis of multiplicative weights, and how to ‚Äúdiscover‚Äù another online algorithm which can be seen as a generalization of projected gradient descent (that is, one can derive the projected gradient descent algorithm and its analysis from this other online algorithm).</p>
<p>
<span id="more-4240"></span></p>
<p>
</p><p><b>1. Follow The Regularized Leader </b></p>
<p></p><p>
We will first state some results in full generality, making no assumptions on the set <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of feasible solutions or on the set of loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" title="{f_t : K \rightarrow {\mathbb R}}" /> encountered by the algorithm at each step.</p>
<p>
Let us try to define an online optimization algorithm from scratch. The solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> proposed by the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> can only depend on the previous cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_{t-1}}" class="latex" title="{f_1,\ldots,f_{t-1}}" />; how should it depend on it? If the offline optimal solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is consistently better than all others at each time step, then we would like <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be that solution, so we want <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be a solution that would have worked well in the previous steps. The most extreme way of implementing this idea is the <em>Follow the Leader</em> algorithm (abbreviated FTL), in which we set the solution at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /></p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> to be the best solution for the previous steps. (Note that the algorithm does not prescribe what solution to use at step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1}" class="latex" title="{t=1}" />.)</p>
<p>
It is possible for FTL to perform very badly. Consider for example the ‚Äúexperts‚Äù setting in which we analyzed multiplicative weights: the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta}" class="latex" title="{\Delta}" /> of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\ldots,n\}}" class="latex" title="{\{1,\ldots,n\}}" />, and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Csum_i+%5Cell_t%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \sum_i \ell_t(i) x(i)}" class="latex" title="{f_t(x) = \sum_i \ell_t(i) x(i)}" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+%5Cell_t%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0\leq \ell_t(i) \leq 1}" class="latex" title="{0\leq \ell_t(i) \leq 1}" />. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> and that <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%280.5%2C.0.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (0.5,.0.5)}" class="latex" title="{x_1 = (0.5,.0.5)}" />. Then a possible run of the algorithm could be: </p>
<ol>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%28.5%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (.5,.5)}" class="latex" title="{x_1 = (.5,.5)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1+%3D+%280%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_1 = (0,.5)}" class="latex" title="{\ell_1 = (0,.5)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2 = (1,0)}" class="latex" title="{x_2 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2 = (1,0)}" class="latex" title="{\ell_2 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_3 = (0,1)}" class="latex" title="{x_3 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_3 = (0,1)}" class="latex" title="{\ell_3 = (0,1)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_4 = (1,0)}" class="latex" title="{x_4 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_4 = (1,0)}" class="latex" title="{\ell_4 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_5 = (0,1)}" class="latex" title="{x_5 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_5 = (0,1)}" class="latex" title="{\ell_5 = (0,1)}" />
</li></ol>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cvdots&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \vdots" class="latex" title="\displaystyle \vdots" /></p>
<p>
In which, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm suffers a loss of <img src="https://s0.wp.com/latex.php?latex=%7BT-+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T- O(1)}" class="latex" title="{T- O(1)}" /> while the offline optimum is <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2 + O(1)}" class="latex" title="{T/2 + O(1)}" />. Thus, the regret is about <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2}" class="latex" title="{T/2}" />, which compares very unfavorably to the <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt T)}" class="latex" title="{O(\sqrt T)}" /> regret of the multiplicative weight algorithm. For general <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, a similar example shows that the regret of FTL can be as high as about <img src="https://s0.wp.com/latex.php?latex=%7BT%5Ccdot+%5Cleft%28+1-+%5Cfrac+1n+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T\cdot \left( 1- \frac 1n \right)}" class="latex" title="{T\cdot \left( 1- \frac 1n \right)}" />.</p>
<p>
In the above bad example, the algorithm keeps ‚Äúoverfitting‚Äù to the past history: if an expert is a bit better than the others, the algorithm puts all its probability mass on that expert, and the algorithm keeps changing its mind at every step. Interestingly, this is the only failure mode of the algorithm.</p>
<blockquote><p><b>Theorem 1 (Analysis of FTL)</b> <em> For any sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_t}" class="latex" title="{f_1,\ldots,f_t}" /> and any number of time steps <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, the FTL algorithm satisfies the regret bound </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t+%29+-+f_t%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " /></p>
</em><p><em> </em></p></blockquote>
<p> So that if the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> are Lipschitz with respect to a distance function on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, then the only way for the regret to be large is for <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to typically be far, in that distance, from <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t+1}}" class="latex" title="{x_{t+1}}" />.</p>
<p>
<em>Proof:</em>  Recalling the definition of regret, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " class="latex" title="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " /></p>
<p> the theorem is equivalent to <a name="ftl.analysis"></a></p><a name="ftl.analysis">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" class="latex" title="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" /></p>
</a><p><a name="ftl.analysis"></a> We will prove <a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a> by induction. The base case <img src="https://s0.wp.com/latex.php?latex=%7BT%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T=1}" class="latex" title="{T=1}" /> is just the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2}" class="latex" title="{x_2}" />. Assuming that $latex {<a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a>}&amp;fg=000000$ is true up to <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+f_%7BT%2B1%7D+%28x_%7BT%2B2%7D%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7BT%2B2%7D%29+%3D+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " class="latex" title="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " /></p>
<p> where the middle step follows from the use of the inductive assumption, which gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7BT%2B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " class="latex" title="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The above example and analysis suggest that we should modify FTL in such a way that the choices of the algorithm don‚Äôt change too much from step to step, and that the solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> should be a compromise between optimizing with respect to previous cost functions and not changing too much from step to step.</p>
<p>
In order to do this, we introduce a new function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" />, called a <em>regularizer</em> (more on it later), and, at each step, we compute the solution</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> This algorithm is called <em>Follow the Regularized Leader</em> or FTRL. Typically, the function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is chosen to be strictly convex and to take values that are rather big in magnitude. Then <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> will be the unique minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and, at each subsequent step, <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> will be selected in a way to balance the pull toward the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and the pull toward the FTL solution <img src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_k+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\arg\min_{x\in K} \sum_k f_k(x)}" class="latex" title="{\arg\min_{x\in K} \sum_k f_k(x)}" />. In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is large in magnitude compared to each <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" />, the solution will not change too much from step to step.</p>
<p>
We have the following analysis that makes no assumptions on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, on the cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> and on the regularizer (not even that the regularizer is convex).</p>
<blockquote><p><b>Theorem 2 (Analysis of FTRL)</b> <em> For every sequence of cost functions and every regularizer function, the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps of the FTRL algorithm is bounded as follows: for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_%7Bt%7D%29+-+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+R%28x%29+-+R%28x_1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" /></p>
<p> where </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%28x%29+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " class="latex" title="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let us run for <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the FTRL algorithm with regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" />, and call <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,\ldots,x_T}" class="latex" title="{x_1,\ldots,x_T}" /> the solutions computed by the FTL algorithm. </p>
<p>
Now consider the following mental experiment: we run the FTL algorithm for <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" /> steps, with the sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7BR%2Cf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R,f_1,\ldots,f_t}" class="latex" title="{R,f_1,\ldots,f_t}" />, and we use <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> as a first solution. Then we see that the solutions computed by the FTL algorithm will be precisely <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_1%2Cx_2%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,x_1,x_2,\ldots,x_T}" class="latex" title="{x_1,x_1,x_2,\ldots,x_T}" />. The regret bound for FTL implies that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />,</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x_1%29+-+R%28x%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_t%28x%29+%5Cleq+R%28x_1%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " class="latex" title="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Having established these results, the general recipe to solve an online optimization problem will be to find a regularizer function <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> such that the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> ‚Äúpulls away from‚Äù solutions that would make the FTL algorithm overfit, and such that there is a good balance between how big <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> gets over <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> (because we pay <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%5E%2A%29+-+R%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x^*) - R(x_1)}" class="latex" title="{R(x^*) - R(x_1)}" /> in the regret, where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum) and how stable is the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" class="latex" title="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> varies.</p>
<p>
</p><p><b>2. Negative-Entropy Regularization </b></p>
<p></p><p>
Let us consider again the ‚Äúexperts‚Äù setting, that is, the online optimization setup in which <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2C+n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots, n\}}" class="latex" title="{\{ 1,\ldots, n\}}" /> and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+%5Cell_t+%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i \ell_t (i) x(i)}" class="latex" title="{f_t (x) = \sum_i \ell_t (i) x(i)}" /> with bounded coefficients.</p>
<p>
The example we showed above showed that FTL will tend to put all the probability mass on one expert. We would like to choose a regularizer that fights this tendency by penalizing ‚Äúconcentrated‚Äù distributions and favoring ‚Äúspread-out‚Äù distributions. This observation might trigger the thought that the <em>entropy</em> of a distribution is a good measure of how concentrated or spread out it is, although the entropy is actually higher for spread-out distribution and smaller for concentrated ones. So we will use as a regularizer <em>minus the entropy</em>, multiplied by an appropriate scaling factor: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> (Entropy is usually defined using logarithms in base 2, but using natural logarithms will make it cleaner to take derivatives, and it only affects the constant factor <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" />.) With this choice of regularizer, we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%5CDelta%7D+%5C+%5C+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> To compute the minimum of the above function we will use the method of Lagrange multipliers. Specialized to our setting, the method of Lagrange multiplier states that if we want to solve the constrained minimization problem </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx+%3A+%5C+a%5ETx+%3D+b+%7D+%5C+%5C+f%28x+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " class="latex" title="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " /></p>
<p> we introduce a new parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> and define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%5Clambda+%28x%29+%3A%3D+f%28x%29+%2B+%5Clambda+%5Ccdot+%28a%5ET+x+-+b+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " class="latex" title="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " /></p>
<p> Then it is possible to prove that if <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a feasible minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />, then there is at least a value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%5E%2A%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x^*) = 0}" class="latex" title="{\nabla f_\lambda (x^*) = 0}" />, that is, such that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a stable point of <img src="https://s0.wp.com/latex.php?latex=%7Bf_%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_\lambda}" class="latex" title="{f_\lambda}" />. So one can proceed by finding all <img src="https://s0.wp.com/latex.php?latex=%7Bx%2C%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,\lambda}" class="latex" title="{x,\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x) = 0}" class="latex" title="{\nabla f_\lambda (x) = 0}" /> and then filtering out the values of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5ET+x+%5Cneq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a^T x \neq b}" class="latex" title="{a^T x \neq b}" />, and finally looking at which of the remaining <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> minimizes <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />.</p>
<p>
Ignoring for a moment the non-negativity constraints, the constraint <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in \Delta}" class="latex" title="{x\in \Delta}" /> reduces to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_i x_i = 1}" class="latex" title="{\sum_i x_i = 1}" />, so we have to consider the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+%5Ccdot+%5Cleft%28+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle+-+1%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " /></p>
<p> The partial derivative of the above expression with respect to <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i}" class="latex" title="{x_i}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+1+%2B+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " /></p>
<p> If we want the gradient to be zero then we want all the above expressions to be zero, which translates to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%7B%5Crm+exp%7D+%5Cleft%28+-1+-+%5Cfrac+%5Clambda+c+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " class="latex" title="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " /></p>
<p> There is only one value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> that makes the above solution a probability distribution, and the corresponding solution is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%5Cfrac+%7B%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+%7D+%7B%5Csum_j+%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28j%29+%5Cright%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " class="latex" title="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " /></p>
<p> Notice that this is exactly the solution computed by the multiplicative weights algorithm, if we choose <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = 1/\epsilon}" class="latex" title="{c = 1/\epsilon}" />. So we have ‚Äúrediscovered‚Äù the multiplicative weights algorithm and we have also ‚Äúexplained‚Äù what it does: at every step it balances the goals of finding a solution that is good for the past and that has large entropy.</p>
<p>
Now it remains to bound, at each time step, </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x_t%29+-+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " class="latex" title="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " /></p>
<p> For this, it is convenient to return to the notation that we used in describing the multiplicative weights algorithm, that is, it is convenient to work with the weights defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_1%28i%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_1(i) = 1" class="latex" title="\displaystyle  w_1(i) = 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_%7Bt%2B1%7D+%28i%29+%3D+w_t+%28i%29+%5Ccdot+e%5E%7B+%5Cell_t+%28i%29+%2F+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" class="latex" title="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" /></p>
<p> so that, at each time step </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7Bw_t%28i%29%7D%7B%5Csum_j+w_t+%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " class="latex" title="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " /></p>
<p> We are assuming <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cell_t+%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 \leq \ell_t (i) \leq 1}" class="latex" title="{0 \leq \ell_t (i) \leq 1}" />, so the weights are non-increasing with time. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%28i%29+%3D+%5Cfrac+%7Bw_%7Bt%2B1%7D+%28i%29+%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%3D+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%5Cgeq+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%7D+%28j%29+%7D+%5Cgeq+x_t%28i%29+%5Ccdot+e%5E%7B-1%2Fc%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " class="latex" title="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " /></p>
<p> For every <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c \geq 1}" class="latex" title="{c \geq 1}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-1%2Fc%7D+%5Cgeq+1+-+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-1/c} \geq 1 - 1/c}" class="latex" title="{e^{-1/c} \geq 1 - 1/c}" />, so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%7D%28i%29+-+x_%7Bt%2B1%7D%28i%29+%5Cleq+%5Cfrac+1c+x_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " class="latex" title="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%5Csum_i+%5Cell_t%28i%29+%5Ccdot+%5Cfrac+1c+x_t%28i%29+%5Cleq+%5Cfrac+1c+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " class="latex" title="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " /></p>
<p> Putting it all together, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cfrac+Tc+%2B+c+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " class="latex" title="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " /></p>
<p> Choosing <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B%5Cln+n%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {\ln n}}}" class="latex" title="{c = \sqrt{\frac T {\ln n}}}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
<p> Thus, we have reconstructed the analysis of the multiplicative weights algorithm.</p>
<p>
Interestingly, the analysis that we derived today is not exactly identical to the one from the post on multiplicative weights. There, we derived the bound</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t%5E2+%28i%29+x_t+%28i%29+%5C+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " /></p>
<p> while here, setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = 1/c}" class="latex" title="{\epsilon = 1/c}" />, we derived </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t+%28i%29+x_t%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+-+%5Cfrac+1+%5Cepsilon+H%28x%5E%2A%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum and <img src="https://s0.wp.com/latex.php?latex=%7BH%28x%29+%3D+%5Csum_i+x_i+%5Cln+%5Cfrac+1+%7Bx_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" class="latex" title="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" /> is the entropy function (computed using natural logarithms). </p>
<p>
</p><p><b>3. L2 Regularization </b></p>
<p></p><p>
Now that we have a general method, let us apply it to a new context: suppose that, as before, our cost functions are linear, but let <img src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K = {\mathbb R}^n}" class="latex" title="{K = {\mathbb R}^n}" />. With linear cost functions and no bound on the size of solutions, it will not be possible to talk about regret with respect to the offline optimum, because the offline optimum will always be <img src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-\infty}" class="latex" title="{-\infty}" />, but it will be possible to talk about regret with respect to a particular offline solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, which will already lead to interesting consequences.</p>
<p>
What regularizer should we use? In reasoning about regularizers, it can be helpful to think about what would go wrong if we use FTL, and then considering what regularizer would successfully ‚Äúpull away‚Äù from the bad solutions found by FTL. In this context of linear loss functions and unbounded solutions, FTL will pick an infinitely big solution at each step, or, to be more precise, the ‚Äúmax‚Äù in the definition of FTL is undefined. To fight this tendency of FTL to go off to infinity, it makes sense for the regularizer to be a measure of how big a solution is. Since we are going to have to compute derivatives, it is good to use a measure of ‚Äúbigness‚Äù with a nice gradient, and <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{||x ||^2}" class="latex" title="{||x ||^2}" /> is a natural choice. So, for a scale parameter <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> to be optimized later, our regularizer will be </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%7C%7C+x%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c || x||^2 " class="latex" title="\displaystyle  R(x) := c || x||^2 " /></p>
<p> This tells us that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_+1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_ 1 = {\bf 0} " class="latex" title="\displaystyle  x_ 1 = {\bf 0} " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cx%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Clangle+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " /></p>
<p> The function that we are minimizing in the above expression is convex, so we just have to compute the gradient and set it to zero </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2c+x+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " class="latex" title="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+-+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " class="latex" title="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " /></p>
<p> Which can be also expressed as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%7B%5Cbf+0%7D%3B+%5C+%5C+%5C+x_%7Bt%2B1%7D+%3D+x_t+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " class="latex" title="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " /></p>
<p> This makes perfect sense because, in the ‚Äúexperts‚Äù interpretation, we want to penalize the experts that performed badly in the past. Here we have no constraints on our allocations, so we simply decrease (additively this time, not multiplicatively) the allocation to the experts that caused a higher loss.</p>
<p>
To compute the regret bound, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%3D+%5Cleft%5Clangle+%5Cell_t+%2C+%5Cfrac+1+%7B2c%7D+%5Cell_t+%5Cright%5Crangle+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t%7C%7C%5E2+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " class="latex" title="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " /></p>
<p> and so the regret with respect to a solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+R%28x%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%7C%7C+x%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " class="latex" title="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " /></p>
<p> If we know a bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+t%3A+%5C+%5C+%7C%7C+%5Cell_t+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall t: \ \ || \ell_t || \leq L " class="latex" title="\displaystyle  \forall t: \ \ || \ell_t || \leq L " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x|| \leq D " class="latex" title="\displaystyle  || x|| \leq D " /></p>
<p> then we can optimize <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B2D%5E2+L%5E2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {2D^2 L^2}}}" class="latex" title="{c = \sqrt{\frac T {2D^2 L^2}}}" /> and we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D+L+%5Csqrt%7B+2+T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " /></p>
<p>
</p><p><b>  3.1. Dealing with Constraints </b></p>
<p></p><p>
Consider now the case in which the loss functions are linear and <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is an arbitrary convex set. Using the same regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) = c || x||^2}" class="latex" title="{R(x) = c || x||^2}" /> we have the algorithm </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " class="latex" title="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+c+%7C%7Cx+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " /></p>
<p> How can we solve the above constrained optimization problem? A very helpful observation is that we can first solve the unconstrained optimization and then project on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, that is we can proceed as follows: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cy+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " class="latex" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5CPi_K+%28y_%7Bt%2B1%7D+%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p> and we claim that we always have <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D+%3D+x_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1} = x_{t+1}}" class="latex" title="{x'_{t+1} = x_{t+1}}" />. The fact that we can reduce a regularized constrained optimization problem to an unconstrained problem and a projection is part of a broader theory that we will describe in a later post. For now, we will limit to prove the equivalence in this specific setting. First of all, we already have an expression for <img src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_{t+1}}" class="latex" title="{y_{t+1}}" />, namely </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " class="latex" title="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " /></p>
<p> Now the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1}}" class="latex" title="{x'_{t+1}}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+%2B+%7C%7C+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft%5Clangle+x+%2C+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx%7C%7C%5E2+-+%5Csum_%7Bk%3D1%7D%5Et+%5Cleft%5Clangle+x+%2C+%5Cell_t+%5Cright%5Crangle+%3D+x_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " class="latex" title="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " /></p>
<p>
In order to bound the regret, we have to compute </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_t%28x_%7Bt%2B1%7D+%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%7C%7C+%5Cell_t+%7C%7C+%5Ccdot+%7C%7Cx_t+-+x_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " class="latex" title="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " /></p>
<p> and since L2 projections cannot increase L2 distances, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_t+-+x_%7Bt%2B1%7D+%7C%7C+%5Cleq+%7C%7C+y_t+-+y_%7Bt%2B1%7D+%7C%7C+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " class="latex" title="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " /></p>
<p>
So the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+%7C%7Cx%5E%2A%7C%7C%5E2+-+c%7C%7C+x_1%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is an upper bound to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{x\in K} || x||}" class="latex" title="{\max_{x\in K} || x||}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> is an upper bound to the norm <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+%5Cell_t+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| \ell_t ||}" class="latex" title="{|| \ell_t ||}" /> of all the loss vectors, then</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+D%5E2+%2B+%5Cfrac+1+%7B2c%7D+T+L%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " /></p>
<p> which can be optimized to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+DL+%5Csqrt+%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " class="latex" title="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " /></p>
<p>
</p><p><b>  3.2. Deriving the Analysis of Gradient Descent </b></p>
<p></p><p>
Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: K \rightarrow {\mathbb R}}" class="latex" title="{g: K \rightarrow {\mathbb R}}" /> is a convex function whose gradient <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla g}" class="latex" title="{\nabla g}" /> is well defined at all points in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, and that we are interested in minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />. Then a way to reduce this problem to online optimization would be to use the function <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as loss function at each step. Then the offline optimum would be the minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and achieving small regret means that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1T+%5Csum_t+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1T \sum_t g(x_t)}" class="latex" title="{\frac 1T \sum_t g(x_t)}" /> is close to the minimum of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and so the best <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer.</p>
<p>
Unfortunately, this is not a very helpful idea, because if we ran an FTRL algorithm against an adversary that keeps proposing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost function at each step then we would have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29+%2B+t+%5Ccdot+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " /></p>
<p> which, for large <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, is essentially the same problem as minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, so we have basically reduced the problem of minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to itself.</p>
<p>
Indeed, the power of the FTRL algorithm is that the algorithm does well even though it does not know the cost function, and if we keep using the same cost function at each step we are not making a good use of its power. Now, suppose that we use cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> such that </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+x%5Cin+K+%5C+%5C+f_t%28x%29+%5Cleq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\forall x\in K \ \ f_t(x) \leq g(x)}" class="latex" title="{\forall x\in K \ \ f_t(x) \leq g(x)}" />
</li></ul>
<p> Then, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%3D+%7B%5Crm+Regret%7D_T+%2B+%5Cmin%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cleq+%7B%5Crm+Regret%7D_T+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> meaning </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so one of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer. Indeed, using convexity, we also have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright%29+%5Cleq+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so the average of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is also an approximate minimizer. From the point of view of exploiting FTRL do to minimize <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> as above work just as well as presenting <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost functions at each step.</p>
<p>
How do we find cost functions that satisfy the above two properties and for which the FTRL algorithm is easy to implement? The idea is to let <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> be the linear approximation of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+g%28x_t%29+%2B+%5Clangle+%5Cnabla+g+%28x_t%29%2C+x+-+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " class="latex" title="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " /></p>
<p> The <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" /> condition is immediate, and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%5Cgeq+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x) \geq f_t (x) " class="latex" title="\displaystyle  g(x) \geq f_t (x) " /></p>
<p> is a consequence of the convexity of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />.</p>
<p>
The cost functions that we have defined are affine functions, that is, each of them equals a constant plus a linear function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x%29+%3D+%5Cleft%28+g%28x_t%29+-+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x_t%5Crangle+%5Cright%29+%2B+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " class="latex" title="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " /></p>
<p>
Adding a constant term to a cost function does not change the iteration of FTRL, and does not change the regret (because the same term is added both to the solution found by the algorithm and to the offline optimum), so the algorithm is just initialized with</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_1 = {\bf 0} " class="latex" title="\displaystyle  y_1 = {\bf 0} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5CPi_K%28%7B%5Cbf+0%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " class="latex" title="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " /></p>
<p> and then continues with the update rules </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3Dy_t+-%5Cfrac+1+%7B2c%7D+%5Cnabla+g+%28x_t%29+%5Cmbox%7B+for+%7D+t+%5Cgeq+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" class="latex" title="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5CPi_K%28y_%7Bt%2B1%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+%5Cmbox%7B+for+%7D+t+%5Cgeq+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " class="latex" title="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " /></p>
<p> which is just projected gradient descent.</p>
<p>
If we have known upper bounds </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+%5Cnabla+g%28x%29+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " class="latex" title="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+x+%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || x || \leq D " class="latex" title="\displaystyle  \forall x \in K \ \ || x || \leq D " /></p>
<p> then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1+T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright+%29+%5Cleq+DL+%5Ccdot+%5Csqrt%7B%5Cfrac+2+T%7D+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> which means that to achieve additive error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> it is enough to proceed for <img src="https://s0.wp.com/latex.php?latex=%7B2D%5E2L%5E2+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2D^2L^2 / \epsilon^2}" class="latex" title="{2D^2L^2 / \epsilon^2}" /> steps. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"><span class="datestr">at May 06, 2019 02:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/">E-ink on the move</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;">Today I was overjoyed to notice that the <a href="https://www.mbta.com/projects/solar-powered-e-ink-signs">MBTA is installing e-ink signs</a>. I didn‚Äôt know about this when <a href="https://emanueleviola.wordpress.com/2019/03/07/a-dream-come-true-sort-of-e-ink-monitors/">I wrote in the previous post</a> that the market for e-ink monitors will be huge.</p>
<p style="text-align: justify;">I was actually about to report more on my experience, and by another standard coincidence today a reader asks:</p>
<blockquote><p>Some time have passed, is your evaluation the same? Did you come across any unexpected difficulties?</p></blockquote>
<p style="text-align: justify;">Well, I wrote a <a href="http://www.ccs.neu.edu/home/viola/papers/tm.pdf">paper</a> entirely in e-ink. But I regret to admit that towards the end of the semester I got really busy with the usual end-of-Spring matters at the university, and I switched back to my back-lit 30-inch Dell monitor.¬† I had to interact with a number of computer systems where I could not easily change font size (the story of my life), and where color tended to matter, and I felt that the new monitor was slowing me down.¬† I haven‚Äôt switched back to the e-ink monitor yet, partly because I am still recovering from the burst.</p>
<p style="text-align: justify;">However I look forward to using the e-ink monitor more during this summer, especially outdoors.¬† Here the fact that it‚Äôs usb powered will be essential.¬† In the MBTA project they use solar power which I think is really cool and makes me think of bringing my monitor to the secluded off-the-grid cabin in Maine I don‚Äôt have.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/"><span class="datestr">at May 06, 2019 12:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/adv/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/adv/">Adversarial Examples Are Not Bugs, They Are Features</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://arxiv.org/abs/1905.02175" class="bbutton">
<i class="fas fa-file-pdf"></i>
¬† ¬† Read the paper
</a>
<a style="float: right;" href="http://git.io/adv-datasets" class="bbutton">
<i class="fab fa-github"></i>
¬†¬† Download the datasets
</a></p>

<p>Over the past few years, adversarial examples ‚Äì or inputs that have been slightly perturbed by an adversary to cause unintended behavior in machine learning systems ‚Äì have received significant attention in the machine learning community (for more background, read our introduction to adversarial examples <a href="https://gradientscience.org/intro_adversarial">here</a>). There has been much work on training models that are not vulnerable to adversarial examples (in previous posts, we discussed methods for training robust models: <a href="https://gradientscience.org/robust_opt_pt1/">part 1</a>, <a href="https://gradientscience.org/robust_opt_pt2/">part 2</a>, but all this research does not really confront the fundamental question: <i>why</i> do these adversarial examples arise in the first place?</p>

<p>So far, the prevailing view has been that adversarial examples stem from ‚Äúquirks‚Äù of the models that will eventually disappear once we make enough progress towards better training algorithms and larger scale data collection. Common views include adversarial examples being either a consequence of the input space being high-dimensional (e.g. <a href="https://arxiv.org/abs/1801.02774">here</a>) or attributed to finite-sample phenomena (e.g. <a href="https://arxiv.org/abs/1608.07690">here</a> or <a href="https://arxiv.org/abs/1804.11285">here</a>).</p>

<p>Today we will discuss our <a href="https://gradientscience.org/adv.pdf">recent work</a> that provides a new perspective on the reasons for adversarial examples arise. However, before we dive into the details, let us first tell you a short story:</p>

<h2 id="a-planet-called-erm">A Planet called <i>Erm</i></h2>

<p>Our tale begins on <i><span class="sc">Erm</span></i>, a distant planet inhabited by members of an ancient alien race known as <i>Nets</i>. The Nets are a strange species; each individual‚Äôs place in the social hierarchy is determined by their ability to classify bizarre 32-by-32 pixel images (which are meaningless to the Nets) into ten completely arbitrary categories. These images are drawn from a top-secret dataset, <span class="sc">See-Far</span>‚Äîoutside of looking at those curious pixelated images, the Nets live their lives totally blind.</p>

<p>As Nets grow older and wiser they begin to discover more and more tell-tale <i>patterns</i> in <span class="sc">See-Far</span>. Each new pattern that an alien discovers helps them classify the dataset even more accurately. Due to the immense societal value of increased classification accuracy, the aliens have names for the most predictive image patterns:</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/toogit.png" alt="A Toogit, highly indicative of a '1' image." style="width: 30%;" /></p>
<div class="footnote">
        A TOOGIT, highly indicative of a "1" image.
        Nets are extremely sensitive to TOOGITs.
</div>

<p>The most powerful aliens were remarkably adept at spotting patterns, and thus were extremely sensitive to their presence in <span class="sc">See-Far</span> images.</p>

<p>Somehow (perhaps looking for <span class="sc">See-Far</span> classification tips), some of the aliens obtain access to a <i>human-written</i> machine learning paper. One figure in
particular caught the aliens‚Äô eye:</p>

<p>
<img src="https://gradientscience.org/images/featsnotbugs/bagaboop.png" alt="An quote-unquote adversarial example?" />
</p><div class="footnote">
An "adversarial example"?
</div>
<p></p>

<p>The figure was relatively simple, they thought: on the left was a ‚Äú2‚Äù, in the middle there was a GAB pattern, which was known to indicate ‚Äú4‚Äù‚Äîunsurprisingly, adding a GAB to the image on the left resulted in a new image, <i>which looked (to the Nets) exactly like an image corresponding to the ‚Äú4‚Äù category</i>.</p>

<p>The Nets could not understand why, according to the paper, the original and final images, being completely different, should be identically classified. Confused, the Nets flipped on through the manuscript, wondering what other useful patterns humans were oblivious to$\ldots$</p>

<h2 id="what-we-can-learn-from-erm">What we can learn from <i>Erm</i></h2>

<p>As the names may suggest, this story is not merely one of aliens and their curious social constructs: the way Nets develop is reminiscent of how we train machine learning models. In particular, we maximize accuracy without incorporating much prior context about classified classes, the physical world, or other human-related concepts. The result in the story is that the aliens are able to realize that what humans think of as meaningless adversarial perturbation are actually patterns crucial to <span class="sc">See-Far</span> classification. The tale of the Nets should thus make us wonder:</p>

<p><i>Are adversarial perturbations really unnatural and meaningless?</i></p>

<h3 id="a-simple-experiment">A simple experiment</h3>
<p>To investigate this issue, let us first perform a simple experiment:</p>

<ul>
  <li>We start with an image from the training set of a standard dataset (e.g. CIFAR10):</li>
</ul>

<p><img src="https://gradientscience.org/images/featsnotbugs/train.png" alt="An image from the training set" style="width: 50%;" /></p>

<ul>
  <li>We synthesize a targeted adversarial example (on a standard pre-trained model) from each (x, y) towards the ‚Äúnext‚Äù class y+1 (or 0, if y is the last class):</li>
</ul>

<p><img src="https://gradientscience.org/images/featsnotbugs/adv.png" alt="An adversarial perturabtion to the next class" /></p>

<ul>
  <li>We then construct a new training set, by labeling these adversarial examples with their corresponding target class:</li>
</ul>

<p><img src="https://gradientscience.org/images/featsnotbugs/newtrain.png" alt="A new training set based on the perturbation" style="width: 35%;" /></p>

<p>Now, the resulting training set is imperceptibly perturbed from the original, but the labels have been changed‚Äîas such, it looks <i>completely</i> mislabeled to a human. In fact, the mislabelings are even consistent with a ‚Äúpermuted‚Äù hypothesis (i.e. every dog is labeled as a cat, every cat as a bird, etc.).</p>

<p>We train a new classifier (not necessarily with the same architecture as the first) on the ‚Äúmislabeled dataset.‚Äù How will this classifier perform on the  <em>original (unaltered) test set</em> (i.e. the standard CIFAR-10 test set)?</p>

<p>Remarkably, we find that the resulting classifier actually has moderate accuracy (e.g. 44% for CIFAR)! This is despite the fact that training inputs are associated with their ‚Äútrue‚Äù labels <em>solely through imperceptible perturbations</em>, and are associated with a different (now incorrect) label matching through <i>all</i> visible features.</p>

<p>What‚Äôs going on here?</p>

<h2 id="our-conceptual-model-for-adversarial-examples">Our Conceptual Model for Adversarial Examples</h2>

<p>The experiment we just described establishes adversarial perturbations of standard models as patterns predictive of the target class in a <i>well-generalizing</i> sense. That is, adversarial perturbations in the training set alone allowed moderately accurate predictions on the test set. In this light, one might wonder: perhaps these patterns are <i>not</i> fundamentally different from what humans use to classify images (e.g. ears, whiskers, snouts)! This is precisely our hypothesis‚Äîthere exist a variety of features of the input that are predictive of the label, and only some of these are perceptible to humans.</p>

<p>More precisely, we posit that predictive features of the data can be split into ‚Äúrobust‚Äù and ‚Äúnon-robust‚Äù features. Robust features correspond to patterns that are predictive of the true label <em>even when adversarially perturbed</em> (e.g. the presence of ‚Äúfur‚Äù in an image) under some pre-defined (and crucially human-defined) perturbation set, e.g. the $\ell_2$ ball. Conversely, non-robust features correspond to patterns that while predictive, can be ‚Äúflipped‚Äù by an adversary within a pre-defined perturbation set to be indicate a wrong class (see <a href="https://gradientscience.org/adv.pdf">our paper</a> for a formal definition).</p>

<p>Since we always only consider perturbation sets that do not affect human classification performance,  we expect humans to rely solely on robust features. However, when the goal is maximizing (standard) test-set accuracy, non-robust features can be just as useful as robust ones‚Äìin fact, the two types of features are completely interchangeable. This is illustrated in the following figure:</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/model.png" alt="" /></p>

<p>From this perspective, our experiment describes something quite simple. In the original training set, both the robust and non-robust features of the input are predictive of the label. When we make a small adversarial perturbation, we cannot significantly affect the robust features (essentially by definition), but  we can still flip <i>non-robust features</i>. For instance, every dog image now retains the robust features of a dog (and thus appears to us to be a dog), but has non-robust features of a cat. After the training set is relabelled, we make it so that the robust features actually point in the <i>wrong direction</i> (i.e. the pictures with robust ‚Äúdog‚Äù features are labeled as cats) and hence only the non-robust features actually provide correct guidance for generalization.</p>

<p>In summary, both robust and non-robust features are predictive on the training set, but <i>only non-robust features will yield generalization to the original test set</i>:</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/diagram.png" alt="" /></p>

<p>Thus, the fact that models trained on this dataset actually generalize to the standard test set indicates that (a) non-robust features exist and are sufficient for good generalization, and (b) deep neural networks indeed rely on these non-robust features, even in the presence of predictive robust features.</p>

<h2 id="do-robust-models-learn-robust-features">Do Robust Models Learn Robust Features?</h2>

<p>Our experiments establish that adversarial perturbations are not meaningless artifacts but actually correspond directly to perturbing features that are crucial to generalization. At the same time, our blog posts about adversarial examples (<a href="https://gradientscience.org/robust1">here</a> and <a href="https://gradientscience.org/robust2">here</a>) showed that by using robust optimization, we can get models that are more robust to adversarial examples.</p>

<p>So a natural question to ask is: can we verify that robust models actually rely on <i>robust</i> features? To test this, we establish a methodology for restricting (in a best-effort sense) inputs to only the features a model is sensitive to (for deep neural networks, features correspond to the penultimate layer activations). Using this method, we create a new training set that is restricted to only contain the features that an already-trained robust model utilizes:</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/frog.png" alt="" /></p>

<p>We then train a model on the resulting dataset  <em>without</em> adversarial training and find that the resulting model has non-trivial accuracy <em>and</em> robustness! This is in stark contrast to training on the standard training set which leads to models that are accurate yet completely brittle.</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/CIFAR_res.png" alt="" /></p>
<div class="footnote">
Standard and robust accuracy, tested on CIFAR-10 test set ($\mathcal{D}$). Training: <br />
<strong>Left</strong>: training normally on CIFAR-10 ($\mathcal{D}$) <br />
<strong>Middle</strong>: training adversarially on CIFAR-10 ($\mathcal{D}$) <br />
<strong>Right</strong>: training normally on constructed dataset ($\widehat{\mathcal{D}}_R$)
</div>

<p>Our results thus indicate that robustness (and in turn non-robustness) can in fact arise as a property of the dataset itself. In particular, when we remove non-robust features from the original training set, we can get robust models just via standard (non-adversarial) training. This is further evidence that adversarial examples arise as a result of non-robust features and are not necessarily tied to the standard training framework.</p>

<h2 id="transferability">Transferability</h2>

<p>An immediate consequence of this change in perspective is that the <i>transferability</i> of adversarial examples (the thus-far mysterious phenomenon that perturbations for one model are often adversarial for others) no longer requires a separate explanation. Specifically, now that we view adversarial vulnerability as a direct product of the <i>features derived from the dataset</i> (as opposed to quirks in the training of individual models), we would expect similarly expressive models to be able to find and use these features to improve their classification accuracy too.</p>

<p>To further explore this idea, we study how the tendency of different architectures to learning similar non-robust features relates to the transferability of adversarial examples between them:</p>

<p><img src="https://gradientscience.org/images/featsnotbugs/transfer.png" alt="" />
In the above, we generate the dataset that we described in our very first experiment (a training set of adversarial examples labeled with the target class), using a ResNet-50 to construct the adversarial examples. We can think of the resulting dataset as having all of the ResNet-50‚Äôs non-robust features ‚Äúflipped‚Äù to the target class. We then train the five architectures shown above on this dataset, and record their generalization performance on the true test set: this corresponds to how well the architecture is able to generalize using only the non-robust features from a ResNet-50.</p>

<p>When we analyze the results we see that, as our new view of adversarial examples suggests, models‚Äô ability to pick up the non-robust features introduced by the ResNet-50 correlates extremely well with the adversarial transferability from ResNet-50 to standard models of each architecture.</p>

<h2 id="implications">Implications</h2>

<p>Our discussion and experiments establish adversarial examples as a purely human-centric phenomenon. From the perspective of classification performance there is no reason for a model to prefer robust over non-robust features. After all, the notion of robustness is human-specified. Hence, if we want our models to rely mostly on robust features we need to explicitly account for that by incorporating priors into architecture or training process. From that perspective, adversarial training (and more broadly robust optimization) can be thought of as a tool to incorporate desired invariances into the learned model. For example, robust training can be viewed as attempting to undermine the predictiveness of non-robust features by constantly ‚Äúflipping‚Äù them, and thus steering the trained model away from relying on them.</p>

<p>At the same time, the reliance of standard models on non-robust (and hence incomprehensible to humans) features needs to be accounted for when designing interpretability methods. In particular, any ‚Äúexplanation‚Äù of a standardly trained model‚Äôs prediction should either highlight such non-robust features (and hence not be fully human-meaningful) or hide them (and hence not be fully faithful to the model‚Äôs decision process). Therefore, if we want interpretability methods that are both human-meaningful and faithful, resorting only to post-training processing is fundamentally insufficient: one needs to intervene <i>at</i> training time.</p>

<h2 id="more-in-the-paper">More in the Paper</h2>

<p>In <a href="https://gradientscience.org/adv.pdf">our paper</a>, we also describe a precise framework for discussing robust and non-robust features, further experiments corroborating our hypothesis, and a theoretical model under which we can study the dynamics of robust training in the presence of non-robust features.</p></div>







<p class="date">
<a href="http://gradientscience.org/adv/"><span class="datestr">at May 06, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01282">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01282">Learning Some Popular Gaussian Graphical Models without Condition Number Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jonathan Kelner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koehler:Frederic.html">Frederic Koehler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meka:Raghu.html">Raghu Meka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01282">PDF</a><br /><b>Abstract: </b>Gaussian Graphical Models (GGMs) have wide-ranging applications in machine
learning and the natural and social sciences. In most of the settings in which
they are applied, the number of observed samples is much smaller than the
dimension and they are assumed to be sparse. While there are a variety of
algorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph
structure with a logarithmic number of samples, they assume various conditions
that require the precision matrix to be in some sense well-conditioned.
</p>
<p>Here we give the first polynomial-time algorithms for learning attractive
GGMs and walk-summable GGMs with a logarithmic number of samples without any
such assumptions. In particular, our algorithms can tolerate strong
dependencies among the variables. We complement our results with experiments
showing that many existing algorithms fail even in some simple settings where
there are long dependency chains, whereas ours do not.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01282"><span class="datestr">at May 06, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01254">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01254">RLE edit distance in near optimal time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clifford:Rapha=euml=l.html">Rapha√´l Clifford</a>, Pawe≈Ç Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Daniel_P=.html">Daniel P. Martin</a>, Przemys≈Çaw Uzna≈Ñski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01254">PDF</a><br /><b>Abstract: </b>We show that the edit distance between two run-length encoded strings of
compressed lengths $m$ and $n$ respectively, can be computed in
$\mathcal{O}(mn\log(mn))$ time. This improves the previous record by a factor
of $\mathcal{O}(n/\log(mn))$. The running time of our algorithm is within
subpolynomial factors of being optimal, subject to the standard SETH-hardness
assumption. This effectively closes a line of algorithmic research first
started in 1993.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01254"><span class="datestr">at May 06, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01216">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01216">Fully Dynamic Single-Source Reachability in Practice: An Experimental Study</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanauer:Kathrin.html">Kathrin Hanauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulz:Christian.html">Christian Schulz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01216">PDF</a><br /><b>Abstract: </b>Given a directed graph and a source vertex, the fully dynamic single-source
reachability problem is to maintain the set of vertices that are reachable from
the given vertex, subject to edge deletions and insertions. While there has
been theoretical work on this problem, showing both linear conditional lower
bounds for the fully dynamic problem and insertions-only and deletions-only
upper bounds beating these conditional lower bounds, there has been no
experimental study that compares the performance of fully dynamic reachability
algorithms in practice. Previous experimental studies in this area concentrated
only on the more general all-pairs reachability or transitive closure problem
and did not use real-world dynamic graphs.
</p>
<p>In this paper, we bridge this gap by empirically studying an extensive set of
algorithms for the single-source reachability problem in the fully dynamic
setting. In particular, we design several fully dynamic variants of well-known
approaches to obtain and maintain reachability information with respect to a
distinguished source. Moreover, we extend the existing insertions-only or
deletions-only upper bounds into fully dynamic algorithms. Even though the
worst-case time per operation of all the fully dynamic algorithms we evaluate
is at least linear in the number of edges in the graph (as is to be expected
given the conditional lower bounds) we show in our extensive experimental
evaluation that their performance differs greatly, both on random as well as on
real-world instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01216"><span class="datestr">at May 06, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01185">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01185">Most vital segment barriers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kostitsyna:Irina.html">Irina Kostitsyna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=ouml=ffler:Maarten.html">Maarten L√∂ffler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Polishchuk:Valentin.html">Valentin Polishchuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Staals:Frank.html">Frank Staals</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01185">PDF</a><br /><b>Abstract: </b>We study continuous analogues of "vitality" for discrete network flows/paths,
and consider problems related to placing segment barriers that have highest
impact on a flow/path in a polygonal domain. This extends the graph-theoretic
notion of "most vital arcs" for flows/paths to geometric environments. We give
hardness results and efficient algorithms for various versions of the problem,
(almost) completely separating hard and polynomially-solvable cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01185"><span class="datestr">at May 06, 2019 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01134">Positive-Instance Driven Dynamic Programming for Graph Searching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannach:Max.html">Max Bannach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berndt:Sebastian.html">Sebastian Berndt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01134">PDF</a><br /><b>Abstract: </b>Research on the similarity of a graph to being a tree - called the treewidth
of the graph - has seen an enormous rise within the last decade, but a
practically fast algorithm for this task has been discovered only recently by
Tamaki (ESA 2017). It is based on dynamic programming and makes use of the fact
that the number of positive subinstances is typically substantially smaller
than the number of all subinstances. Algorithms producing only such
subinstances are called positive-instance driven (PID). We give an alternative
and intuitive view on this algorithm from the perspective of the corresponding
configuration graphs in certain two-player games. This allows us to develop
PID-algorithms for a wide range of important graph parameters such as
treewidth, pathwidth, and treedepth. We analyse the worst case behaviour of the
approach on some well-known graph classes and perform an experimental
evaluation on real world and random graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01134"><span class="datestr">at May 06, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01029">Range closest-pair search in higher dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rahul:Saladi.html">Saladi Rahul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Jie.html">Jie Xue</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01029">PDF</a><br /><b>Abstract: </b>Range closest-pair (RCP) search is a range-search variant of the classical
closest-pair problem, which aims to store a given set $S$ of points into some
space-efficient data structure such that when a query range $Q$ is specified,
the closest pair in $S \cap Q$ can be reported quickly. RCP search has received
attention over years, but the primary focus was only on $\mathbb{R}^2$. In this
paper, we study RCP search in higher dimensions. We give the first nontrivial
RCP data structures for orthogonal, simplex, halfspace, and ball queries in
$\mathbb{R}^d$ for any constant $d$. Furthermore, we prove a conditional lower
bound for orthogonal RCP search for $d \geq 3$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01029"><span class="datestr">at May 06, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01019">Adversarial Training with Voronoi Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khoury:Marc.html">Marc Khoury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hadfield=Menell:Dylan.html">Dylan Hadfield-Menell</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01019">PDF</a><br /><b>Abstract: </b>Adversarial examples are a pervasive phenomenon of machine learning models
where seemingly imperceptible perturbations to the input lead to
misclassifications for otherwise statistically accurate models. We propose a
geometric framework, drawing on tools from the manifold reconstruction
literature, to analyze the high-dimensional geometry of adversarial examples.
In particular, we highlight the importance of codimension: for low-dimensional
data manifolds embedded in high-dimensional space there are many directions off
the manifold in which an adversary could construct adversarial examples.
Adversarial examples are a natural consequence of learning a decision boundary
that classifies the low-dimensional data manifold well, but classifies points
near the manifold incorrectly. Using our geometric framework we prove that
adversarial training is sample inefficient, and show sufficient sampling
conditions under which nearest neighbor classifiers and ball-based adversarial
training are robust. Finally we introduce adversarial training with Voronoi
constraints, which replaces the norm ball constraint with the Voronoi cell for
each point in the training set. We show that adversarial training with Voronoi
constraints produces robust models which significantly improve over the
state-of-the-art on MNIST and are competitive on CIFAR-10.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01019"><span class="datestr">at May 06, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.00973">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.00973">Benchmark Instances and Branch-and-Cut Algorithm for the Hashiwokakero Puzzle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coelho:Leandro_C=.html">Leandro C. Coelho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laporte:Gilbert.html">Gilbert Laporte</a>, Arinei Lindbeck, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal:Thibaut.html">Thibaut Vidal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00973">PDF</a><br /><b>Abstract: </b>Hashiwokakero, or simply Hashi, is a Japanese single-player puzzle played on
a rectangular grid with no standard size. Some cells of the grid contain a
circle, called island, with a number inside it ranging from one to eight. The
remaining positions of the grid are empty. The player must connect all of the
islands by drawing a series of horizontal or vertical bridges between them,
respecting a series of rules: the number of bridges incident to an island
equals the number indicated in the circle, at most two bridges are incident to
any side of an island, bridges cannot cross each other or pass through islands,
and each island must eventually be reachable from any other island. In this
paper, we present some complexity results and relationships between Hashi and
well-known graph theory problems. We give a formulation of the problem by means
of an integer linear mathematical programming model, and apply a branch-and-cut
algorithm to solve the model in which connectivity constraints are dynamically
generated. We also develop a puzzle generator. Our experiments on 1440 Hashi
puzzles show that the algorithm can consistently solve hard puzzles with up to
400 islands.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.00973"><span class="datestr">at May 06, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.00948">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.00948">Submodular Streaming in All its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kazemi:Ehsan.html">Ehsan Kazemi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitrovic:Marko.html">Marko Mitrovic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zadimoghaddam:Morteza.html">Morteza Zadimoghaddam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lattanzi:Silvio.html">Silvio Lattanzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karbasi:Amin.html">Amin Karbasi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00948">PDF</a><br /><b>Abstract: </b>Streaming algorithms are generally judged by the quality of their solution,
memory footprint, and computational complexity. In this paper, we study the
problem of maximizing a monotone submodular function in the streaming setting
with a cardinality constraint $k$. We first propose Sieve-Streaming++, which
requires just one pass over the data, keeps only $O(k)$ elements and achieves
the tight $(1/2)$-approximation guarantee. The best previously known streaming
algorithms either achieve a suboptimal $(1/4)$-approximation with $\Theta(k)$
memory or the optimal $(1/2)$-approximation with $O(k\log k)$ memory. Next, we
show that by buffering a small fraction of the stream and applying a careful
filtering procedure, one can heavily reduce the number of adaptive
computational rounds, thus substantially lowering the computational complexity
of Sieve-Streaming++. We then generalize our results to the more challenging
multi-source streaming setting. We show how one can achieve the tight
$(1/2)$-approximation guarantee with $O(k)$ shared memory while minimizing not
only the required rounds of computations but also the total number of
communicated bits. Finally, we demonstrate the efficiency of our algorithms on
real-world data summarization tasks for multi-source streams of tweets and of
YouTube videos.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.00948"><span class="datestr">at May 06, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1116">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1116">News for April 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>After a quite slow month of March, things sped up quite significantly in April: six different papers, ranging from graph testing to function testing to quantum distribution testing!</p>



<p><strong>Update (05/04): </strong>We missed one. Seven!</p>



<p><strong>Junta correlation is testable</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/1904.04216">arXiv</a>).  Junta testing really has seen <a href="https://ptreview.sublinear.info/?s=junta">a lot of attention and progress</a> over the pass couple years! In this new work, the focus is on the <em>tolerant</em> version of (Boolean) junta testing, where the goal is to decide whether a given function \(f\colon \{-1,1\}^n\to \{-1,1\}\) is close to some \(k\)-junta, or far from any such simple function. The current paper improves on previous work of Blais, Canonne, Eden, Levi, and Ron, which showed how to distinguish between \(\varepsilon\)-close to \(k\)-junta and \(16\varepsilon\)-far from \(k\)-junta in \(O_{k,\varepsilon}(1)\) queries, in two ways: (i) the gap-factor of 16 can now be made arbitrarily small, yielding the first <em>fully</em> tolerant non-trivial junta tester; and (ii) the new algorithm also identifies the underlying ‚Äúcore junta‚Äù (up to permutation of the coordinates). Besides the other results contained in the paper (such as results for a relaxation of the tolerant question akin to that of Blais et al.), a key aspect of this paper is to go beyond the use of (set) influence as a proxy for distance to junta-ness which underlied all previous work, thus potentially opening a new avenue towards get fully tolerant, \(\mathrm{poly}(k,1/\varepsilon)\)-query junta testing algorithms.</p>



<p><strong>Testing Tensor Products</strong>, by Irit Dinur and Konstantin Golubev (<a href="https://arxiv.org/abs/1904.12747">arXiv</a>). In this paper, the authors consider the following testing question: given query access to a Boolean function \(f\colon [n]^d \to \mathbb{F}_2\), test whether \(f\) is of the form \(f(x) = f_1(x_1)+\dots f_d(x_d)\) (equivalently, this is the task of testing whether a \(d\)-dimensional tensor with \(\pm 1\)  is of rank \(1\)). They provide two different proximity-oblivious testers (POT) for this task: a \(4\)-query one, reminiscent and building upon the BLR linearity test; as well as a \((d+1)\)-query POT <em>(which they dub ‚ÄúShapka Test,‚Äù one can assume for their love of warm and comfortable hats)</em>, whose analysis is significantly simpler.</p>



<p>Changing direction, the next paper we saw is concerned with quantum testing of (regular, good ol‚Äô classical) probability distributions:</p>



<p><strong>Quantum Algorithms for Classical Probability Distributions</strong>, by Aleksandrs Belovs (<a href="https://arxiv.org/abs/1904.02192">arXiv</a>). This work on quantum distribution testing considers 4 of the existing models of access to samples from an unknown probability distribution, analyzes their relationship, and argues their merits. Further, the autho establishes that for the simple hypothesis testing question of distinguishing between two (known, fixed) probability distributions \(p,q\), in all four models the optimal sample complexity is proportional to the inverse of the Hellinger distance between  \(p\) and \(q\)‚Äîin contrast to the classical setting, where it is known to be proportional to the <em>squared</em> inverse of this distance.</p>



<p>Turning to graphs, the next work considers clustering of bounded-degree graphs, a topic we <a href="https://ptreview.sublinear.info/?p=1075">recently discussed as well</a>:</p>



<p><strong>Robust Clustering Oracle and Local Reconstructor of Cluster Structure of Graphs</strong>, by Pan Peng (<a href="https://arxiv.org/abs/1904.09710">arXiv</a>). Consider the following noisy model: an unknown bounded-degree graph (of maximum degree \(d\)) \(G\) which is \((k, \phi_{\rm in},\phi_{\rm out})\)-clusterable (i.e., clusterable in at most \(k\) clusters of inner and outer conductances bounded by \(\phi_{\rm in},\phi_{\rm out}\)) is adversarially modified in at most \($\varepsilon d n\) edges between clusters. This noise model, introduced in this work, leads to the natural questions of ‚Äúrecovering the underlying graph \(G\)‚Äù: this is what the author tackles, by designing sublinear-time <em>local clustering oracles</em> and <em>local reconstruction algorithms</em>  (local filters) in this setting. Further, in view of the noise model reminiscent of property testing in the bounded-degree graph setting, connections to testing clusterability are discussed; the implications of the results for testing clusterability are discussed in Section 1.4.</p>



<p><strong>A Faster Local Algorithm for Detecting Bounded-Size Cuts with Applications to Higher-Connectivity Problems</strong>, by Sebastian Forster and Liu Yang (<a href="https://arxiv.org/abs/1904.08382">arXiv</a>). The authors focus on the problem of finding an edge (or vertex) cut in a given graph from a <em>local</em> viewpoint, i.e., in the setting of local computation algorithms, and provide a slew of results in detecting bounded-size cuts. This in turn has direct implications for testing \(k\)-edge and \(k\)-vertex connectivity, directed and undirected, in both the bounded-degree and general (unbounded degree) models, improving on or subsuming the current state-of-the-art across the board  <em>(see Section 1.2.4 of the paper, and the very helpful Tables 3 and 4, for a summary of the improvements)</em>.</p>



<p><strong>Random walks and forbidden minors II: A \(\mathrm{poly}(d\varepsilon^‚àí1)\)-query tester for minor-closed properties of bounded degree graphs</strong>, by Akash Kumar, C. Seshadhri, and Andrew Stolman (<a href="https://eccc.weizmann.ac.il/report/2019/046/">ECCC</a>). Following their <a href="https://ptreview.sublinear.info/?p=999">breakthrough from last May</a>, the authors are at it again! Leveraging a random-walk approach, they resolve the following open question in testing bounded-degree graph: <em>is there a \(\mathrm{poly}(1/\varepsilon)\)-query tester for planarity (and, more generality, for minor-closed graph properties)? </em>The previous state-of-the-art, due to Levi and Ron, had a quasipolynomial dependence on \(1/\varepsilon\).<br />Without spoiling too much: the answer to the open question is <em>yes‚Äî</em> and further the authors settle it by showing an even more general result on testing \(H\)-freeness (for any constant-size graph \(H\)).</p>



<p><strong>Update (05/04): </strong></p>



<p><strong>Testing Unateness Nearly Optimally</strong>, by Xi Chen and Erik Waingarten (<a href="https://arxiv.org/abs/1904.05309">arXiv</a>). Recall that a function \(f\colon \{0,1\}^n\to \{0,1\}\) is said to be unate if there exists some \(s\in\{0,1\}^n\) such that \(f(\cdot \oplus s)\) is monotone; i.e., if \(f\) is either non-increasing or non-decreasing in each coordinate. Testing unateness has seen a surge of interest over the past year or so; this work essentially settles the question, up to polylogarithmic factors in \(n\) (and the exact dependence on \(\varepsilon\)). Namely, the authors present and analyze an \(\tilde{O}(n^{2/3}/\varepsilon^2)\)-query adaptive tester for unateness, which nearly matches the \(\tilde{\Omega}(n^{2/3})\)-query lower bound for adaptive testers previously established by Chen, Waingarten, and Xie.  </p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1116"><span class="datestr">at May 04, 2019 06:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4184">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4184">On the scientific accuracy of ‚ÄúAvengers: Endgame‚Äù</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>[BY REQUEST: SPOILERS FOLLOW]</p>



<p>Today Ben Lindbergh, a writer for <em>The Ringer</em>, put out <a href="https://www.theringer.com/movies/2019/5/3/18527776/marvel-avengers-endgame-time-travel-david-deutsch-proposition-scott-aaronson">an article</a> about the scientific plausibility (!) of the time-travel sequences in the new ‚ÄúAvengers‚Äù movie.  The article relied on two interviewees:</p>



<p>(1) David Deutsch, who confirmed that he has no idea what the ‚ÄúDeutsch proposition‚Äù mentioned by Tony Stark refers to but declined to comment further, and</p>



<p>(2) some quantum computing dude from UT Austin who had no similar scruples about spouting off on the movie.</p>



<p>To be clear, the UT Austin dude hadn‚Äôt even <em>seen</em> the movie, or any of the previous ‚ÄúAvengers‚Äù movies for that matter!  He just watched the clips dealing with time travel.  Yet Lindbergh still saw fit to introduce him as ‚Äúa real-life [Tony] Stark without the vast fortune and fancy suit.‚Äù  Hey, I‚Äôll take it.</p>



<p>Anyway, if you‚Äôve seen the movie, and/or you know Deutsch‚Äôs causal consistency proposal for quantum closed timelike curves, and you can do better than I did at trying to reconcile the two, feel free to take a stab in the comments.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4184"><span class="datestr">at May 03, 2019 07:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4182">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4182">A small post</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>I really liked <a href="https://venturebeat.com/2019/04/21/quantum-computing-is-a-marathon-not-a-sprint/?fbclid=IwAR2WHdke1ppFQFE4ZfYswa06lXe-sUM2PZ2neLmLOhFTrJYHY3iXNesGHb8">this article by Chris Monroe</a>, of the University of Maryland and IonQ, entitled ‚ÄúQuantum computing is a marathon not a sprint.‚Äù  The crazier expectations get in this field‚Äîand right now they‚Äôre <em>really</em> crazy, believe me‚Äîthe more it needs to be said.</li><li>In a <a href="https://cacm.acm.org/magazines/2019/5/236426-quantum-hype-and-quantum-skepticism/fulltext?mobile=false">piece for <em>Communications of the ACM</em></a>, Moshe Vardi came out as a ‚Äúquantum computing skeptic.‚Äù  But it turns out what he means by that is not that he knows a reason why QC is impossible in principle, but simply that it‚Äôs often overhyped and that it will be hard to establish a viable quantum computing industry.  By that standard, I‚Äôm a ‚ÄúQC skeptic‚Äù as well!  But then what does that make Gil Kalai or Michel Dyakonov?</li><li>Friend-of-the-blog Bram Cohen asked me to link to <a href="https://www.chia.net/2019/04/04/chia-network-announces-second-vdf-competition-with-in-total-prize-money.en.html">this second-round competition</a> for Verifiable Delay Functions, sponsored by his company Chia.  Apparently the first link I provided actually mattered in sending serious entrants their way.</li><li>Blogging, it turns out, is really, really hard when (a) your life has become a pile of real-world obligations stretching out to infinity, <em>and also</em> (b) the Internet has become a war zone, with anything you say quote-mined by people looking to embarrass you.  But don‚Äôt worry, I‚Äôll have more to say soon.  In the meantime, doesn‚Äôt anyone have more questions about the research papers discussed in the previous post?  Y‚Äôknow, NEEXP in MIP*?  SBP versus QMA?  Gentle measurement of quantum states and differential privacy turning out to be almost the same subject?</li></ol></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4182"><span class="datestr">at May 03, 2019 07:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15827">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/">Sedgewick Wins An Award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>An award for educational writing</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/sedgewick_1183631/" rel="attachment wp-att-15828"><img width="200" alt="" class="alignright  wp-image-15828" src="https://rjlipton.files.wordpress.com/2019/05/sedgewick_1183631.jpeg?w=200" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ ACM ]</font></td>
</tr>
</tbody>
</table>
<p>
Robert Sedgewick is the 2018 recipient of the ACM Outstanding Educator Award. </p>
<p>
Today we congratulate Bob on this wonderful honor.<br />
<span id="more-15827"></span></p>
<p>
The award is named after Karl Karlstrom. Years ago, he was an editor at the publishing house Prentice-Hall. To convey why the award was named for him, it may suffice to quote one nugget. This is ‚ÄúFortune <a href="http://motd.ambians.com/quotes.php/name/linux_computers/toc_id/1-1-4/s/340">341</a>‚Äù from the old <tt>motd</tt> (message of the day) program which gave some humor or wisdom when you logged into UNIX/Linux:</p>
<blockquote><p><b> </b> <em> ‚ÄúI have travelled the length and breadth of this country, and have talked with the best people in business administration. I can assure you on the highest authority that data processing is a fad and won‚Äôt last out the year.‚Äù</em></p><em>
</em><p><em>
‚Äî Editor in charge of business books at Prentice-Hall publishers, responding to Karl V. Karlstrom (a junior editor who had recommended a manuscript on the new science of data processing), c. 1957 </em>
</p></blockquote>
<p>Karl K. had a knack for being right, you see.</p>
<p>
</p><p></p><h2> Karl Stories </h2><p></p>
<p></p><p>
I recall Karl fondly. We mostly interacted when we were both attending some theory conference. I often found myself talking to him over a drink while we sat in a hotel bar. This was back, ages ago, when I did drink a beer or two. Karl was one we could count on to amuse and also‚Äîmost importantly‚Äîpick up the bar tab. He had an expense account. The IEEE ‚ÄúComputer Pioneers‚Äù <a href="https://history.computer.org/pioneers/karlstrom.html">site</a> says this about him:</p>
<blockquote><p><b> </b> <em> Early computer science textbook editor who put Prentice-Hall in the forefront, but who lost heart when he learned that the best textbook criteria are short words, big type, wide margins, and colored illustrations. ACM named its education award after him. </em>
</p></blockquote>
<p>
</p><p></p><h2> Bob Stories </h2><p></p>
<p></p><p>
The ACM award may be named for Karlstrom, but I suspect that many of the awardees, including Bob, never had the pleasure of meeting Karlstrom. Too bad. </p>
<p>
I believe we all know why Bob was selected to get this award. He has done some wonderful work in many aspects of education. He is best known for his series of Algorithms textbooks. I thought it might be fun to recall a couple of Sedgewick stories that have nothing to do with his main work.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>A big result</i>. One day Bob grabbed me and told me that he had a wonderful result. This is when I was still at Princeton. I asked what was the breakthrough? He explained:</p>
<blockquote><p><b> </b> <em> I now can do arrows really well. Really. </em>
</p></blockquote>
<p></p><p>
What? He explained that TeX and LaTeX did not do arrows well. This refers, of course, to arrows as in directed graphs or flow diagrams. Bob uses lots of diagrams, with lots of arrows, in his textbooks. He had worked hard to get a postscript hack that made arrows look great. Thus he could typeset an arrow so it looked perfect even when it touched another object. I listened and was unsure what to make of his claim. Was he losing it? He then showed me a print-out of some of his arrows. I have to say they really did look quite good.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>A secret result</i>. Bob and I worked for a while on a front-end to TeX we called <i>notech</i>. The concept was to have the absolute minimum of commands, and have the notech system figure out what you mean. For example, in an earlier system for typesetting from Bell Labs, called <a href="https://en.wikipedia.org/wiki/Troff">Troff</a>, a new paragraph was marked by the command <img src="https://s0.wp.com/latex.php?latex=%7B.PP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{.PP}" class="latex" title="{.PP}" />. Thus</p>
<p>
This is part of a paragraph. .PP And this is the start of the next paragraph. </p>
<p>
This is ugly and TeX‚Äôs idea is much better. As you probably know the start of a new paragraph is marked by a blank line. No ugly command like <img src="https://s0.wp.com/latex.php?latex=%7B.PP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{.PP}" class="latex" title="{.PP}" />.</p>
<p>
What Bob and I did was to try and take this idea as far as possible. The system notech tried to guess line breaks, math displays, tables, verbatim for C code, text displays, and much more. It did this with out using commands for as much as possible. I used the system for years for all my papers and memos and notes. Eventually, I gave it up and switched to LaTeX like every one else.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>A public result</i>. Bob also worked with me and my team in the 1980‚Äôs on systems for designing VLSI chips. One such paper was joint with Jacobo Valdes, Gopalakrishnan Vijayan, and Stephen North: <a href="https://dl.acm.org/citation.cfm?id=809246">VLSI Layout as Programming</a>. The trouble with this and related work is that it never took off; it never had as much impact as we thought it would. Oh well.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We wish Bob the best. May he be awarded many other prizes.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/"><span class="datestr">at May 03, 2019 04:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/066">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/066">TR19-066 |  A Lower Bound for Sampling Disjoint Sets | 

	Thomas Watson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose Alice and Bob each start with private randomness and no other input, and they wish to engage in a protocol in which Alice ends up with a set $x\subseteq[n]$ and Bob ends up with a set $y\subseteq[n]$, such that $(x,y)$ is uniformly distributed over all pairs of disjoint sets. We prove that for some constant $\varepsilon&gt;0$, this requires $\Omega(n)$ communication even to get within statistical distance $\varepsilon$ of the target distribution. Previously, Ambainis, Schulman, Ta-Shma, Vazirani, and Wigderson (FOCS 1998) proved that $\Omega(\sqrt{n})$ communication is required to get within $\varepsilon$ of the uniform distribution over all pairs of disjoint sets of size $\sqrt{n}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/066"><span class="datestr">at May 03, 2019 12:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/02/playing-model-trains">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html">Playing with model trains and calling it graph theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>You‚Äôve probably played with model trains, for instance with something like the <a href="https://en.wikipedia.org/wiki/Brio_(company)">Brio</a> set shown below.<sup id="fnref:fn"><a href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html#fn:fn" class="footnote">1</a></sup> And if you‚Äôve built a layout with a model train set, you may well have wondered: is it possible for my train to use all the parts of my track?</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/brio-33133.jpg" alt="Brio train set" /></p>

<p>For instance, in the layout shown in this image, if your train starts on the far right, moving downward, it will be stuck in a loop that it can never escape. There are no choice points where the train can switch to another track until it returns to the Y at the right, moving in the same direction. On the other hand, if you allow yourself to reverse the train, it can reverse back through the other entrance to the Y and reach the rest of the track. It‚Äôs also possible for a long-enough train to block itself, preventing it from escaping certain parts of the track that a short train could negotiate more easily.</p>

<p>My newest preprint, ‚ÄúReconfiguring Undirected Paths‚Äù (with Demaine, Hesterberg, Jain, Lubiw, Uehara, and Uno, <a href="https://arxiv.org/abs/1905.00518">arXiv:1905.00518</a>), considers an abstract model for such problems, in which the train track is modeled as an undirected graph and the train is a simple path in the graph. You can slide the train by adding an edge to one end of the path and removing an edge from the other end; we don‚Äôt distinguish which end of the train is which, so it can slide in both directions. The vertices of the graph model points where you can choose which of several directions to slide the train. Because it‚Äôs an undirected graph, these are like the three-way and four-way junctions in the middle of the image (allowing the train to enter and exit along any pair of track segments) rather than the Y junctions at the far right (where a train that enters at one of the two top edges of the Y has to exit the bottom).</p>

<p>For instance, in a  grid graph, the different positions of a length- path and the ways that one position can shift into another can be visualized as the state space shown below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/path-reconfig-states.svg" alt="PSPACE-hardness reduction for path reconfiguration" /></p>

<p>Testing whether a long train can slide from one position to another turns out to be PSPACE-complete, even on graphs of bounded bandwidth, by a reduction from <a href="https://en.wikipedia.org/wiki/Nondeterministic_constraint_logic">nondeterministic constraint logic</a>. Here‚Äôs an example of an NCL problem transformed by our reduction into a path-sliding problem:</p>

<p style="text-align: center;"><img width="80%" alt="PSPACE-hardness reduction for path reconfiguration" src="https://11011110.github.io/blog/assets/2019/path-reconfig-redux.svg" /></p>

<p>Our main results are a <a href="https://11011110.github.io/blog/2019/05/02/Parameterized complexity">fixed-parameter tractable algorithm</a> parameterized by train length (so it‚Äôs fast for short trains) and a linear time algorithm when the graph is a tree. Both cases are based on the same intuition, that the problem becomes easier if we can maneuver the train onto a long enough path. For the parameterized version, if the graph has a path twice as long as the train that can be reached from the starting position of the train, and another long path that can reach the ending position, then we can maneuver the train onto the first long path, send it on an express route directly from the first long path to the second one, and then maneuver it from there into its final position. On the other hand, until we find these long paths, we can restrict our attention to a subgraph with no long paths; this implies that it has bounded <a href="https://11011110.github.io/blog/2019/05/02/Tree-depth">tree-depth</a> and makes searching within the subgraph easy. The linear time tree algorithm similarly involves a lot of back-and-forth maneuvering of the train to free up longer and longer segments of it until the whole train is freed to move from the start to the goal.</p>

<p>A shorter version of our paper will appear at <a href="http://wads.org/">WADS</a> this summer.
While it was in submission to WADS, a related preprint appeared on arXiv: ‚ÄúThe Parameterized Complexity of Motion Planning for Snake-Like Robots‚Äù, by Gupta, Sa‚Äôar, and Zehavi (<a href="https://arxiv.org/abs/1903.02445">arXiv:1903.02445</a>). They show that for a graph-theoretic model of the <a href="https://en.wikipedia.org/wiki/Snake_(video_game_genre)">Snake video game</a>, getting the snake from one position to another is fixed-parameter tractable in the length of the snake. For this problem, snakes are again paths in graphs, but they can move only in one direction, and the techniques they use to prove fixed-parameter tractability involve sparsifying the state space instead of maneuvering into long paths. <a href="https://11011110.github.io/blog/2018/08/06/congratulations-dr-gupta.html">Sid Gupta was my student</a> at UCI before taking his current postdoc in Israel, but I haven‚Äôt talked to him about this, so I think their work must be independent and its appearance at about the same time a coincidence.</p>

<div class="footnotes">
  <ol>
    <li id="fn:fn">
      <p>Searching on tineye finds that this image was on Amazon in 2008. Presumably it was supplied to them by Brio?¬†<a href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html#fnref:fn" class="reversefootnote">‚Ü©</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102029697142872437">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html"><span class="datestr">at May 02, 2019 07:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3204953698007493895">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html">The Next Chapter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator">
<a style="margin-left: 1em; margin-right: 1em;" href="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s1600/COS_stacked_blk_red.jpg"><img width="320" src="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s320/COS_stacked_blk_red.jpg" border="0" height="95" /></a></div>
<div>
<br /></div>
I've <a href="https://news.iit.edu/stories/2019/05/lance-fortnow-designated-new-college-science-dean">accepted a position</a> as Dean of the <a href="https://science.iit.edu/">College of Science</a> at the Illinois Institute of Technology in Chicago starting in August. It's an exciting opportunity to really build up the sciences and computing in the city that I have spent the bulk of my academic career and grew to love.<br />
<div>
<br /></div>
<div>
I had a fantastic time at Georgia Tech over the last seven years working with an incredible faculty, staff and students in the School of Computer Science. This is a special place and I enjoyed watching the school, the institute and the City of Atlanta grow and evolve.<br />
<br />
After I <a href="https://twitter.com/fortnow/status/1123644799825907712">tweeted</a> the news yesterday, Bill Cook reminded me that<br />
<blockquote class="tr_bq">
Illinois Tech was the long-time home of Karl Menger, the first person to pose the problem of determining the complexity of the TSP. Now you can settle it!</blockquote>
I wouldn't bet on my settling the complexity of traveling salesman even if I didn't have a college to run. But it goes to remind us that wherever you go in life, P and NP will be right there waiting for you.¬†</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html"><span class="datestr">at May 02, 2019 12:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=12881">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/">The last paper of Catherine R√©nyi and Alfr√©d R√©nyi: Counting k-Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A <em>k</em>-tree is a graph obtained as follows: A clique with <em>k</em>¬†vertices is a <em>k</em>-tree. A <em>k</em>-tree with <em>n+1</em> vertices is obtained from a <em>k</em>-tree with n-vertices by adding a new vertex and connecting it to all vertices of a <em>¬†k</em>-clique. There is a <a href="https://www.sciencedirect.com/science/article/pii/S0021980069801201">beautiful formula</a> by Beineke and Pippert (1969) for the number of <em>k</em>-trees with <em>n</em> labelled vertices. Their number is</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7Bk%7D%7D%28k%28n-k%29%2B1%29%5E%7Bn-k-2%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}." class="latex" title="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}." /></p>
<p>If we count <strong>rooted</strong> <em>k</em>-trees where the root is a <em>k</em>-clique the formula becomes somewhat simpler.</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%28k%28n-k%29%2B1%29%5E%7Bn-k-1%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(k(n-k)+1)^{n-k-1}." class="latex" title="(k(n-k)+1)^{n-k-1}." /></p>
<p>In 1972, when I was a teenage undergraduate student I was very interested in various extensions of Cayley‚Äôs formula for counting labeled trees. I thought about the question of finding a <a href="https://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence">Pr√ºfer code</a> for<em> k</em>-trees and¬† how to extend the results by Beineke and¬† Pippert when¬† for every clique of size <img src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k-1" class="latex" title="k-1" /> in the <em>k</em>-tree we specify its ‚Äúdegree‚Äù, namely, the number of <em>k</em>-cliques containing it. (I will come back to the mathematics at the end of the post.) I thank Miki Simonovits for the photos and description and very helpful comments.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg"><img src="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg?w=640&amp;h=434" alt="" width="640" class="alignnone size-full wp-image-17134" height="434" /></a></p>
<p><strong><span style="color: #ff0000;">Above, Kat√≥ Renyi, Paul Turan, Vera S√≥s, and Paul Erd≈ës ; below Kat√≥, Vera, and Lea Sch√∂nheim. Pictures: Jochanan (Janos) Sch√∂nheim.</span></strong></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg"><img src="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg?w=640&amp;h=447" alt="" width="640" class="alignnone size-full wp-image-17137" height="447" /></a></p>
<p>¬†</p>
<p>¬†</p>
<p><a href="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg"><img src="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg?w=640" alt="renyi-turan-erdos" class="alignnone size-full wp-image-12883" /></a></p>
<p><strong><span style="color: #ff0000;">From right, R√©nyi, Tur<b>√°</b>n and Erd≈ës and Gr√§tzer.¬†</span></strong></p>
<p>¬†</p>
<p>While I was working on enumeration of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-trees I came across¬† a paper by Catherine R√©nyi and Alfr√©d R√©nyi that did everything I intended to do and quite a bit more.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr5.png"><img src="https://gilkalai.files.wordpress.com/2019/04/rr5.png?w=640&amp;h=260" alt="" width="640" class="alignnone size-large wp-image-17340" height="260" /></a></p>
<p>What caught my eye was a heartbreaking footnote: when the paper was completed Catherine R√©nyi was no longer alive.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr2r.png"><img src="https://gilkalai.files.wordpress.com/2019/04/rr2r.png?w=640&amp;h=172" alt="" width="640" class="alignnone size-large wp-image-17341" height="172" /></a></p>
<p>The proceedings where the paper appeared were of a conference in combinatorics in Hungary in 1969. This was the first international conference in combinatorics that took place in Hungary.¬† The list of speakers consists of the best combinatorialists in the world and many young people including Laci Lovasz, Laci Babai, Endre Szemeredi, and many more who since then have become world-class¬† scientists.</p>
<p>Years later Vera S√≥s told me the story of Alfr√©d R√©nyi‚Äôs lecture at this conference, the first international conference in combinatorics that took place in Hungary:¬† ‚ÄúKat√≥ died on August 23, on the day of arrival of the conference on ‚ÄúCombinatorial Theory and its Applications‚Äù (Balatonfured, August 24-29). Alfr√©d Renyi gave his talk (with the same title as the paper) on August 27 and his talk was longer than initially scheduled.¬† They proved the results in the paper just the week before the conference. The paper appeared in the proceedings¬† of the conference.‚Äù</p>
<p>Alfr√©d Renyi was one of the organizers of the conference and also served as one of the editors of the proceedings of the conference, which appeared in 1970. A few months after the conference, on February 1, 1970 Alfr√©d R√©nyi ¬†died of a violent illness. The proceedings are dedicated to the memory of Catherine R√©nyi and Alfr√©d R√©nyi.</p>
<p>¬†</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg"><img src="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg?w=640&amp;h=444" alt="" width="640" class="alignnone size-full wp-image-17135" height="444" /></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg"><img src="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg?w=640&amp;h=445" alt="" width="640" class="alignnone size-full wp-image-17136" height="445" /></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png"><img src="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png?w=640&amp;h=426" alt="" width="640" class="alignnone size-full wp-image-17138" height="426" /></a></p>
<p>Two pictures showing Alfr√©d and Catherine R√©nyi and a picture of Alfred R√©nyi and Paul Erd≈ës.</p>
<p>¬†</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/srrhe.png"><img src="https://gilkalai.files.wordpress.com/2019/04/srrhe.png?w=640&amp;h=498" alt="" width="640" class="alignnone size-full wp-image-17315" height="498" /></a></p>
<p>Repeating a picture from last-week <a href="https://gilkalai.wordpress.com/2019/04/21/the-random-matrix-and-more/">post</a>. From left: S√°ndor Szalai,¬† Catherine R√©nyi, Alfr√©d R√©nyi, Andr√°s Hajnal and Paul Erd≈ës (Matrahaza)</p>
<p>Going back to my story. I was 17 at the time and naturally I wondered if counting trees and similar things is what I want to do in my life. Shortly afterwards I went to the army. Without belittling the excitement of the army I quickly reached the conclusion that I prefer to count trees and to do similar things. My first result as a PhD student was another high dimensional extension of Cayley‚Äôs formula (mentioned in <a href="https://gilkalai.wordpress.com/2008/06/10/hellys-theorem-hypertrees-and-strange-enumeration-i/">this post</a> and a few subsequent posts).¬† The question of how to generalize both formulas for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-trees and for my hypertrees is still an open problem. We know the objects we want to count,¬† we know what the outcome should be, and we know that we can cheat and use weighted counting, but still I don‚Äôt know how to make it work.</p>
<p>Some more comments on k-trees:</p>
<ol>
<li>Regarding the degree sequences for k-trees. You cannot specify the actual (k-2)-faces because those (in fact just the graph) determines the k-tree completely. So you need to count rooted k-trees and to specify the (k-2)-faces in terms of how they ‚Äúgrew‚Äù from the root.</li>
<li>The case that all degrees are 1 and 2 that correspond to paths for ordinary trees and to triangulating polygons with diagonals for 2-trees are precisely the stacked (k-1)-dimensional polytopes. This is a special case of the Renyi &amp; Renyi formula that was also <a href="https://link.springer.com/article/10.1007%2FBF02330563?LI=true">¬†found, with a different proof,</a> by Beineke and Pippert.</li>
<li>It is¬† unlikely that there would be a matrix-tree formula for k-trees since telling¬† if a graph contains a 2-tree ¬†is known to be NP complete. See <a href="https://mathoverflow.net/questions/281848/spanning-k-trees">this MO question</a>. Maybe some matrix-tree formulas are available when we start with special classes of graphs.</li>
<li>Regarding the general objects ‚Äì those are simplicial complexes that are Cohen-Macaulay and their dual (blocker) is also Cohen-Macaulay.</li>
</ol>
<p>This post is just about a single paper of Catherine R√©nyi and Alfr√©d R√©nyi mainly through my eyes from 45 years ago. Catherine R√©nyi‚Äôs ¬†main interest originally was¬† Number theory, she was a student of Tur√†n, and soon she became¬† interested in the theory of Complex Analytic Functions. Alfr√©d R√©nyi was a student of Frigyes Riesz and he is known for many contributions in number theory, graph theory and combinatorics and primarily in probability theory.¬† Alfr√©d R√©nyi wrote several papers about enumeration of trees, and this joint paper was Catherine R√©nyi ‚Äòs first paper on this topic.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/"><span class="datestr">at May 01, 2019 02:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/065">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/065">TR19-065 |  Derandomization from Algebraic Hardness: Treading the Borders | 

	Mrinal Kumar, 

	Ramprasad Saptharishi, 

	Noam Solomon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A hitting-set generator (HSG) is a polynomial map $Gen:\mathbb{F}^k \to \mathbb{F}^n$ such that for all $n$-variate polynomials $Q$ of small enough circuit size and degree, if $Q$ is non-zero, then $Q\circ Gen$ is non-zero. In this paper, we give a new construction of  such a HSG assuming that we have an explicit polynomial of sufficient hardness in the sense of approximative or border complexity.  Formally, we prove the following result over any characteristic zero field $\mathbb{F}$:

Suppose $P(z_1,\ldots, z_k)$ is an explicit $k$-variate degree $d$ polynomial that is not in the border of circuits of size $s$. Then, there is an explicit hitting-set generator $Gen_P:\mathbb{F}^{2k} \rightarrow \mathbb{F}^n$ such that  every non-zero $n$-variate degree $D$ polynomial $Q(x_1, x_2, \ldots, x_n)$ in the border of size $s'$ circuits satisfies $Q \neq 0 \Rightarrow Q \circ Gen_P \neq 0$, provided $n^{10k}d D s' \leq s$. 

This is the first HSG in the algebraic setting that yields a complete derandomization of polynomial identity testing (PIT) for general circuits from a suitable algebraic hardness assumption.

As a direct consequence, we show that even a slightly non-trivial explicit construction of hitting sets for polynomials in the border of constant-variate circuits implies a deterministic polynomial time algorithm for PIT. More precisely, we prove the following theorem:

Let $\delta &gt; 0$ be any constant and $k$ be a large enough constant. Suppose, for every $s \geq k$, there is an explicit hitting set of size $s^{k-\delta}$ for all degree $s$ polynomials in the border of $k$-variate size $s$ algebraic circuits. Then, there is an explicit hitting set of size $poly(s)$ for the border $s$-variate algebraic circuits of size $s$ and degree $s$. 

Unlike the prior constructions of such maps [NW94, KI04, AGS18, KST19], our construction is purely algebraic and does not rely on the notion of combinatorial designs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/065"><span class="datestr">at May 01, 2019 05:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/04/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/04/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/04/good-article-terrible-headline.html">Good article, terrible headline</a> (<a href="https://mathstodon.xyz/@11011110/101938798669973189"></a>). Bill Gasarch rants about several recent instances of clickbaity, inaccurate, and overhyped media coverage of theoretical computer science topics. I suspect the answer to his question ‚Äúis it just our field?‚Äù is no.</p>
  </li>
  <li>
    <p><a href="https://www.vox.com/science-and-health/2019/4/16/18311194/black-hole-katie-bouman-trolls">Vox on the sexist backlash against astronomer Katie Bouman</a> (<a href="https://mathstodon.xyz/@11011110/101942756338391262"></a>, <a href="https://www.cnn.com/2019/04/12/us/andrew-chael-katie-bouman-black-hole-image-trnd/index.html">see also</a>), of black hole image fame, after she was cast by the media in the ‚Äúlone genius‚Äù role typically reserved for men and untypical of how science actually happens.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2019/04/mathematical-sign-language-interview-with-dr-jess-boland/">Mathematical sign language</a> (<a href="https://mathstodon.xyz/@11011110/101950143529837988"></a>). Hearing-impaired eletrical engineering researcher Jess Boland discovered that weren‚Äôt enough technical terms in British Sign Language to cover the mathematics she uses in her work, so she‚Äôs been creating new ones as well as promoting the ones BSL already had. Katie Steckles interviews her for <em>The Aperiodical</em>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1804.05452">Regular polygon surfaces</a> (<a href="https://mathstodon.xyz/@11011110/101955536664219652"></a>). Ian Alevy answers <a href="http://cs.smith.edu/~jorourke/TOPP/P72.html#Problem.72">Problem 72 of The Open Problems Project</a>: every topological sphere made of regular pentagons can be constructed by gluing regular dodecahedra together. You can also <a href="https://momath.org/mathmonday/the-paragons-system/">glue dodecahedra to get higher-genus surfaces</a>, but Alevy‚Äôs theorem doesn‚Äôt apply, so we don‚Äôt know whether all higher-genus regular-pentagon surfaces are formed that way.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/04/12/czech-president-blocks-professorships-academic-critics">Czech president Milo≈° Zeman ‚Äúhas repeatedly used presidential powers to block the professorships of political opponents‚Äù</a> (<a href="https://mathstodon.xyz/@11011110/101965701030220573"></a>). Charles University is now suing to allow their promotions to go through.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.08845">Planar point sets determine many pairwise crossing segments</a> (<a href="https://mathstodon.xyz/@11011110/101968467896290245"></a>). J√°nos Pach, Natan Rubin, and G√°bor Tardos make significant progress on  whether every<br />
 points in the plane have a large matching where all edges cross each other. A 1994 paper by Paul Erd≈ës and half a dozen others only managed to prove this for ‚Äúlarge‚Äù meaning . The new paper proves a much stronger bound,  (Ryan Williams‚Äô favorite function).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2019/04/21/on2-in-createprocess/">Why asymptotics matters</a> (<a href="https://mathstodon.xyz/@11011110/101970781407484011"></a>, <a href="https://news.ycombinator.com/item?id=19716673">via</a>): because if you don‚Äôt pay attention to it you get problems like this slow quadratic-time process creation bug in Windows 10.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/101982200745112808">Snap cube puzzle</a>. The cubes have one peg and five holes; how many ways can you snap them into a connected  block with no pegs showing? See link in discussion thread for spoilers.</p>
  </li>
  <li>
    <p>There‚Äôs lots of reasons to be unenthusiastic about newly-official-presidential-candidate Biden involving multiple instances of poor treatment of African-Americans and women, but here‚Äôs another more techy reason: <a href="https://www.pastemagazine.com/articles/2019/04/biden-to-attend-fundraiser-hosted-by-comcast-blue.html">his first major fundraiser as a candidate closely involves anti-net-neutrality lobbyists from Comcast</a> (<a href="https://mathstodon.xyz/@11011110/101987719804605064"></a>).</p>
  </li>
  <li>
    <p><a href="https://adsabs.github.io/blog/transition-reminder">SAO/NASA Astrophysics Data System updates its user interface</a> (<a href="https://mathstodon.xyz/@11011110/101996739440443058"></a>). <a href="http://adsabs.harvard.edu/">The ADS</a> is a useful database of papers in astronomy and related fields. From comments on their post, the new UI is very slow. It is <a href="http://adsabs.github.io/help/faq/">unusable without JavaScript</a>. And it <a href="https://en.wikipedia.org/wiki/Special:Diff/892128592">‚Äúsends the users‚Äô personal identifying information to at least 5 third-party companies‚Äù</a>. This is progress?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/329910/440">I ask for a reference for an easy fact about divisibility representations of partial orders</a> (<a href="https://mathstodon.xyz/@11011110/102002516978139958"></a>). The MathOverflow community isn‚Äôt very helpful, preferring instead to simultaneously complain that it‚Äôs too trivial and explain why it‚Äôs true to me as if I didn‚Äôt already say in my question that I thought it was trivial.`</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@henryseg/101975738950740643">Cannon-Thurston maps for veering triangulations</a>, whatever those are. Henry Segerman posts some pretty pictures from his joint work with David Bachman and Saul Schleimer.</p>
  </li>
  <li>
    <p><a href="https://www.mathunion.org/fileadmin/CWM/Initiatives/CWMNewsletter1.pdf">Newsletter of the IMU Committee for Women</a> (<a href="https://aperiodical.com/2019/04/imu-committee-for-women-in-mathematics-now-has-a-newsletter/">via</a>). Includes an interview with Marie-Francoise Roy and the announcement of the book <em>World Women in Mathematics 2018</em>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Network coding yields lower bounds</a> (<a href="https://mathstodon.xyz/@11011110/102018096543192991"></a>). Lipton and Regan report on <a href="https://arxiv.org/abs/1902.10935">a new paper by Afshani, Freksen, Kamma, and Larsen</a> on lower bounds for multiplication. If algorithmically opening and recombining network messages never improves fractional flow, then  circuit size for multiplication is optimal. But the same lower bound holds for simpler bit-shifting operations, so it‚Äôs not clear how it could extend from circuits to bignum algorithms.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/04/30/linkage.html"><span class="datestr">at April 30, 2019 11:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/064">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/064">TR19-064 |  Randomness and Intractability in Kolmogorov Complexity | 

	Igor Carboni Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce randomized time-bounded Kolmogorov complexity (rKt), a natural extension of Levin's notion of Kolmogorov complexity from 1984. A string w of low rKt complexity can be decompressed from a short representation via a time-bounded algorithm that outputs w with high probability. 

This complexity measure gives rise to a decision problem over strings: MrKtP (The Minimum rKt Problem). We explore ideas from pseudorandomness to prove that MrKtP and its variants cannot be solved in randomized quasi-polynomial time. This exhibits a natural string compression problem that is provably intractable, even for randomized computations. Our techniques also imply that there is no n^{1-eps}-approximate algorithm for MrKtP running in randomized quasi-polynomial time. 

Complementing this lower bound, we observe connections between rKt, the power of randomness in computing, and circuit complexity. In particular, we present the first hardness magnification theorem for a natural problem that is unconditionally hard against a strong model of computation.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/064"><span class="datestr">at April 30, 2019 06:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15814">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Network Coding Yields Lower Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Practice leads theory</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg"><img src="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg?w=123&amp;h=162" alt="" width="123" class="alignright wp-image-15816" height="162" /></a><p></p><p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new <a href="https://arxiv.org/abs/1902.10935">paper</a> titled ‚ÄúLower Bounds for Multiplication via Network Coding‚Äù. </p><p>
Today we will talk about how practical computing played a role in this theory research.</p><p>
The authors (AFKL) state this:</p><blockquote><p><b> </b> <em> In this work, we prove that if a central conjecture in the area of network coding is true, then any constant degree boolean circuit for multiplication must have size <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" />, thus <s>almost</s> completely settling the complexity of multiplication circuits. </em>
</p></blockquote><p><span id="more-15814"></span></p><p></p><p>
We added the strikeout because of the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n \log n)}" class="latex" title="{O(n \log n)}" /> upper bound that we discussed recently <a href="https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/">here</a>.</p><p>
AFKL have conditionally solved a long standing open problem: ‚ÄúHow hard is it to multiply two <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit numbers?‚Äù Their proof shows that a conjecture from practice implies a circuit lower bound. This is rare: using a conjecture from practice, to solve a complexity open problem. We have used conjectures from many parts of mathematics, and from some parts of physics, to make progress, but drawing on experience with practical networking is strikingly fresh. </p><p>
</p><p></p><h2> Integer Multiplication </h2><p></p><p></p><p>
The authors AFKL explain the history of the multiplication problem. We knew some of the story, but not all the delicious details.</p><blockquote><p><b> </b> <em> In 1960, Andrey Kolmogorov conjectured that the thousands of years old <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^{2})}" class="latex" title="{O(n^{2})}" />-time algorithm is optimal and he arranged a seminar at Moscow State University with the goal of proving this conjecture. However only a week into the seminar, the student Anatoly Karatsuba came up with an <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B%5Clog_%7B2%7D3%7D%29+%5Capprox+O%28n%5E%7B1.585%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" class="latex" title="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" /> time algorithm. The algorithm was presented at the next seminar meeting and the seminar was terminated. </em>
</p></blockquote><p></p><p>
Ken and I wish we could have Kolmogorov‚Äôs luck, in one of our seminars. Partly because it would advance knowledge; partly because it would let us out of teaching. Sweet.</p><p>
The main result of AFKL is:</p><blockquote><p><b>Theorem 1</b> <em><a name="NC2mult"></a> Assuming the Network Conjecture, every general boolean circuit that computes the product of two <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit integers has size order at least <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" />. </em>
</p></blockquote><p></p><p>
This says that the boolean complexity of multiplication is super-linear. No restriction of a bounded depth, no restriction on the operations allowed, no restrictions at all. Given our non-existent lower bounds this is remarkable. If it was unconditional, it would be a terrific result. But it still is a strong one. </p><p>
We will next explain what the Network Coding Conjecture (NCC) is. </p><p>
</p><p></p><h2> Network Coding </h2><p></p><p></p><p>
One of the basic papers was authored by Rudolf Ahlswede, Ning Cai, Shuo-Yen Li, and Raymond Yeung <a href="http://www.inf.fu-berlin.de/lehre/WS11/Wireless/papers/CodAhlswede00.pdf">here</a>. The paper has close to ten thousand citations, which would be amazing for a theory paper.</p><p>
In basic networks each node can receive and send messages to and from other nodes. They can only move messages around‚Äîthey are not allowed to peer into a message. The concept of <i>network coding</i> is to allow nodes also to decode and encode messages. Nodes can peer into messages and create new ones. The goal, of course, is to decrease the time required to transmit information through the network.</p><p>
The following example combines figures from a 2004 <a href="http://www.eecg.utoronto.ca/~bli/papers/allerton04.pdf">paper</a> by Zongpeng Li and Baochun Li which formulated the NCC. At left is a situation where two senders, <img src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1}" class="latex" title="{S_1}" /> with an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit message <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> with an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit message <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />, wish to transmit to respective receivers <img src="https://s0.wp.com/latex.php?latex=%7BT_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_1}" class="latex" title="{T_1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BT_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_2}" class="latex" title="{T_2}" />. The network‚Äôs links are one-way as shown, with two intermediate nodes <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, and each link can carry <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits at any one time.</p><p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png"><img src="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png?w=500&amp;h=115" alt="" width="500" class="aligncenter wp-image-15815" height="115" /></a></p><p></p><p><br />
If <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> are black-boxes that must be kept entire, there is no way to solve this in three time steps. But if the nodes can read messages and do lightweight computations, then the middle diagram gives a viable solution. Node <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> reads <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> and on-the-fly transmits their bitwise exclusive-or to node <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. Node <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> relays this to each receiver, who has also received the other party‚Äôs message directly. The receivers can each do a final exclusive-or to recover the messages intended for them. </p><p>
The ability to look inside messages seems powerful, and there are networks where it helps even more dramatically. Incidentally, as <a href="https://en.wikipedia.org/wiki/Linear_network_coding">noted</a> by Wikipedia, the exclusive-or trick was anticipated in a 1978 <a href="https://ieeexplore.ieee.org/document/1455117">paper</a> showing how the two senders can exchange their messages <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> by relaying them to a satellite which transmits <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Coplus+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a \oplus b}" class="latex" title="{a \oplus b}" /> back to each.</p><p>
</p><p></p><h2> The Conjecture </h2><p></p><p></p><p>
However, there is another solution if the links are bi-directional and messages can be broken in half. Sender <img src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1}" class="latex" title="{S_1}" /> simply routes half of <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> one way around the network and the other half the other way. Sender <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> does similarly. This is shown at far right. Each link never has more than <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits of total load and the three-step elapsed time is the same. Moreover, the link from <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is not needed. This is just an undirected network commodity flow with fractional units.</p><p>
In fact, <i>no</i> example is known in an undirected network where encoding beats fractional routing. That is, the known network encoding rate is just the flow rate of the network. The network coding (NCC) conjecture is informally:</p><blockquote><p><b> </b> <em> <i>The coding rate is never better than the flow rate in undirected graphs</i>. </em>
</p></blockquote><p></p><p>
The paper by Li and Li gave formal details and several equivalent statements. Quoting them:</p><blockquote><p><b> </b> <em> For undirected networks with integral routing, there still exist configurations that are feasible with network coding but infeasible with routing only. For undirected networks with fractional routing, we show that the potential of network coding to help increase throughput in a capacitied network is equivalent to the potential of network coding to increase bandwidth efficiency in an uncapacitied network. We conjecture that these benefits are non-existent. </em>
</p></blockquote><p>
</p><p></p><h2> Good and Bad News </h2><p></p><p></p><p>
What has become of the NCC in the fifteen years since? Here‚Äôs how Ken and I see it:</p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The Good News</i>: The NCC helps solve long-standing open problems. Since this conjecture is widely believed this is impressive. Besides integer multiplication, NCC has been used to prove other lower bounds. For example, Larsen working with Alireza Farhadi, Mohammad Hajiaghayi, and Elaine Shi used it to <a href="https://arxiv.org/abs/1811.01313">prove</a> lower bounds on sorting with external memory. </p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The Bad News</i>: The NCC helps solve long-standing open problems. This suggests that this conjecture could be deep and hard to resolve. The boolean complexity of integer multiplication is a long standing open question. Since the NCC leads to a non-linear lower bound, perhaps proving this conjecture could be hopeless.</p><p>
I have mixed feelings about these lower bound results. They are impressive and shed light on hard open problems. But I wonder if the NCC could be wrong. There is a long <a href="https://rjlipton.wordpress.com/2010/06/19/guessing-the-truth/">history</a> in complexity theory where guesses of the form: </p><blockquote><p><b> </b> <em> <i>The obvious algorithm is optimal</i> </em>
</p></blockquote><p>have failed. The situation strikes us as resembling that of the (Strong) Exponential Time Hypothesis, in ways we <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">discussed</a> four years ago. </p><p>
</p><p></p><h2> How the New Paper Works </h2><p></p><p></p><p>
The authors AFKL did not know that an <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n\log n)}" class="latex" title="{O(n\log n)}" /> upper bound had been proved for integer multiplication when they posted their paper. They did, however, prove a stronger version of Theorem (<a href="https://rjlipton.wordpress.com/feed/#NC2mult">1</a>) for a problem with a known <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" /> upper bound. This is to create circuits with <img src="https://s0.wp.com/latex.php?latex=%7Bn+%2B+%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n + \log_2 n}" class="latex" title="{n + \log_2 n}" /> input gates and <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" /> output gates that given <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and a binary number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cleq+n+%3D+%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell \leq n = |x|}" class="latex" title="{\ell \leq n = |x|}" /> output the string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> whose bits <img src="https://s0.wp.com/latex.php?latex=%7Bn-%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n-\ell}" class="latex" title="{n-\ell}" /> through <img src="https://s0.wp.com/latex.php?latex=%7B2n-%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n-\ell-1}" class="latex" title="{2n-\ell-1}" /> equal <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, with other bits <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. A conditional lower bound on this <i>shift</i> task implies the same for multiplication, since the shift is the same as multiplication by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^\ell}" class="latex" title="{2^\ell}" />. </p><blockquote><p><b>Theorem 2</b> <em><a name="NC2shift"></a> Assuming the NCC, circuits for the shift task need size order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" />. </em>
</p></blockquote><p></p><p>
The proof is disarmingly elementary: The input <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> gives <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> ‚Äúsenders‚Äù and each value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> creates a different set of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> ‚Äúreceivers.‚Äù With a circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> fixed, they show one can fix a shift <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_0}" class="latex" title="{\ell_0}" /> so that the average distance from sender to receiver in an undirected multi-commodity flow is <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(\log n)}" class="latex" title="{\Omega(\log n)}" />, giving <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" /> total flow. If <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> achieves smaller size, then it represents a counterexample to the NCC. </p><p>
Pretty neat‚Äîthis is half a page in the paper. The paper proves more intricate results relating to conjectures by Les Valiant about Boolean circuits of bounded fan-in and <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\log n)}" class="latex" title="{O(\log n)}" /> depth that compute permutations and their reduction to depth-3 circuits of unbounded fan-in. This also may extend to the sorting/shifting problem Ken wrote about long ago in a guest <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">post</a> for Lance Fortnow and Bill Gasarch‚Äôs blog.</p><p>
</p><p></p><h2> Open Problems </h2><p></p><p></p><p>
Is the NCC true? Can it be proved for some interesting classes of graphs? I believe it is known for tiny size graphs of at most six nodes. What about, for example, planar graphs?</p><p>
[inserted ‚Äúconditionally‚Äù before ‚Äúsolved‚Äù in intro]<br />
[Fixed AFKL typos]</p><table class="image alignright">











































</table></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/"><span class="datestr">at April 30, 2019 01:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/policy_gradients_pt3">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/policy_gradients_pt3">A Closer Look at Deep Policy Gradients (Part 3&amp;#58; Landscapes and Trust Regions)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This post is the last of a three part series about our recent paper: ‚Äú<a href="https://arxiv.org/abs/1811.02553">Are Deep
Policy Gradient Algorithms Truly Policy Gradient
Algorithms?</a>‚Äù Today, we will analyze agents‚Äô
reward landscapes as well as try to understand to what extent, and by what mechanisms,
our agents enforce so-called <i>trust regions</i>.</p>

<p>First, a quick recap (it‚Äôs been a while!):</p>
<ul>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a>, we outlined the
RL framework and introduced policy gradient algorithms. We saw that
auxiliary optimizations hidden in the implementation details of RL algorithms
drastically impact performance. These findings highlighted the need for a more
fine-grained analysis of how algorithms really operate.</p>
  </li>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt2">second post</a>, we zoomed in on
three algorithms: trust region policy optimization (TRPO), proximal policy
optimization (PPO), and an algorithm we called ‚ÄúPPO-M.‚Äù PPO-M is the core PPO
algorithm exactly as described in the <a href="https://arxiv.org/abs/1707.06347">original
paper</a>, without any of the auxiliary
optimizations. Using these methods as a test-bed, we studied two core
primitives of the policy gradient framework: gradient estimation and value
prediction.</p>
  </li>
</ul>

<p>Our discussion today begins where we left off in our second post. Recall that
last time we studied the variance of the gradient estimates our algorithms use
to maximize rewards. We found (among other things) that, despite high variance, algorithm steps were
still (very slightly) correlated with the actual, ‚Äútrue‚Äù gradient of the reward.
However, how good was this true gradient to begin with? After all, a fundamental
assumption of the whole policy gradient framework is that our gradient steps actually point in a direction (in policy
parameter space) that increases the reward. Is this indeed so in practice?</p>

<h2 id="optimization-landscapes">Optimization Landscapes</h2>
<p>Recall from our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a> that
policy gradient methods treat reward maximization as a zeroth-order optimization
problem. That is, they maximize the objective by applying first order methods
with finite sample gradient estimates of the form:</p>



<p>Here,  represents the cumulative reward of a trajectory , where
 is sampled from the distribution of trajectories induced by the current
policy . We let  represent an easily computable
function of  that is an unbiased estimator of the gradient of the reward
(seen on the left hand side)‚Äîfor more details see <a href="https://gradientscience.org/policy_gradients_pt1/#rl-with-policy-gradients">our previous post</a>.
Finally, we denote by  the number of trajectories used to estimate the
gradient.</p>

<p>An important point is, however, that instead of following the gradients of the cumulative reward (as
suggested by the above equation), the algorithms we analyze actually use a
<i>surrogate reward</i> at each step. The surrogate reward is a function of the
collected trajectories that is meant to locally approximate the true reward,
while providing other benefits such as easier computation and a smoother
optimization landscape. Consequently, at each step these algorithms maximize the
surrogate rewards  instead of following the gradient of the
true reward .</p>

<p>A natural question to ask is: <i>do steps maximizing the surrogate reward
consistently increase policy returns?</i> To answer this, we will use
<i>landscape plots</i> as a tool for visualizing the landscape of returns
around a given policy :</p>

<p><img src="https://gradientscience.org/images/rl/labeled_landscape.jpg" alt="Labeled diagram of optimization landscape plot" /></p>

<p>Here, for each point  in the plot,  and  specify a policy
 parameterized by</p>



<p>where  is the step computed by the studied algorithm. (Note that
we include the random Gaussian  direction to visualize how
‚Äúimportant‚Äù the step direction is compared to a random baseline). The  axis
corresponds to the return attained by the policy , which we
denote by .</p>

<p>Now, when we make this plot for a random  (corresponding to a randomly
initialized policy network), everything looks as expected:</p>

<p><img src="https://gradientscience.org/images/rl/step_0_landscape.jpg" alt="Landscape for randomly initialized network" /></p>

<p>That is, the return increases significantly more quickly along the step direction than in
the random direction. However, repeating these landscape experiments at later
iterations, we find a more surprising picture: going in the step direction
actually <i>decreases</i> the average cumulative reward obtained by the resulting
agent!</p>

<p><img src="https://gradientscience.org/images/rl/landscape_step_150_300_450.jpg" alt="Landscape for trained networks" /></p>

<p>So what exactly is going on here? The steps computed by these algorithms are
estimates of the gradient of the reward, so it is unexpected that the reward
plateaus (or in some cases decreases!) along this direction.</p>

<p>We find that the
answer lies in a <i>misalignment</i> of the true reward and the surrogate. Specifically, while
the steps taken do correspond to an improvement in the surrogate problem, they
do <i>not</i> always correspond to a similar improvement in the true return.
Here are the same graphs as above shown again, this time with the corresponding
optimization landscapes of the <i>surrogate loss</i>
<sup><a href="https://gradientscience.org/feed.xml#footnote1">1</a></sup>:</p>
<div class="footnote">
<sup><a id="footnote1">1</a></sup>All of the landscapes we plot in this post are for PPO; we have (similar)
results for TRPO in <a href="https://arxiv.org/abs/1811.02553">our paper</a>.
</div>

<p><img src="https://gradientscience.org/images/rl/surrogate_vs_real_landscape.jpg" alt="Surrogate vs real landscape" /></p>






<p>To make matters worse, we find that in the low sample regime that policy
gradient methods actually operate in, it is hard to even <i>discern</i> directions of
improvement in the true reward landscape. (In all the plots above,
we use orders of magnitude more samples than an agent would ever see in practice
at a single step.) In the plot below, we visualize reward landscapes while
varying the number of samples used to estimate the expected return of a policy
:</p>

<p><img src="https://gradientscience.org/images/rl/landscape_conc.jpg" alt="Surrogate vs real landscape" /></p>

<p>In contrast to the smooth landscape we see on the right and in the plots above,
the reward landscape actually accessible to the model is jagged and poorly behaved.
This landscape makes it thus near-impossible for an agent to distinguish between good
and bad points in its relevant sample regime, even when the true underlying
landscape is fairly well-behaved!</p>

<p>Overall, our investigation into the optimization landscape of policy gradient algorithms
reveals that (a) the surrogate reward function is often misaligned with the
underlying true return of the policy, and (b) in the relevant sample regime, it
is hard to distinguish between ‚Äúgood‚Äù steps and ‚Äúbad‚Äù steps, even when looking at the true reward
landscape. As always, however, <i>none of this stops the agents from training
and continually improving reward in the average sense</i>. This raises some
key questions about the landscape of policy optimization:</p>

<ul>
  <li>Given that the function we actually optimize is so often misaligned with the
underlying rewards, how is it that agents continually improve?</li>
  <li>Can we explain or link the local behaviour we observe in the landscape with a
more global view of policy optimization?</li>
  <li>How do we ensure that the reward landscape is navigable? And, more generally,
what is the best way to navigate it?</li>
</ul>

<h2 id="trust-regions">Trust Regions</h2>
<p>Let us now turn our attention to another important notion in the popular policy gradient algorithms: that of the <i>trust region</i>. 
Recall that a convenient way to think about our training process is to view it as a series of policy parameter iterates:</p>



<p>An important aspect of this process is ensuring that the steps we take don‚Äôt
lead us outside of the region (of parameter space) where the samples we
collected are informative. Intuitively, if we collect samples at a given set of
policy parameters , there is no reason to expect that these samples
should tell us about the performance of a new set of parameters that is far away
from .</p>

<p>Thus, in order to ensure that gradient steps are predictive, classical
algorithms like the 
<a href="http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">conservative policy update</a> 
employ update schemes that constrain the probability distributions induced by 
successive policy parameters.  The <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a> 
in particular showed 
that one can 
guarantee monotonic policy improvement with each step by solving a surrogate problem of the following form:</p>



<p>The second, ‚Äúpenalty‚Äù term in the above objective, referred to as the <i>trust region</i> penalty, is a
critical component of modern policy gradient algorithms. TRPO, one of the
algorithms we study, proposes a relaxation of \eqref{eq:klpen} that
instead imposes a hard constraint on the
<i>mean</i> KL divergence<sup><a href="https://gradientscience.org/feed.xml#footnote2">2</a></sup>
 (estimated using the empirical samples we obtain):</p>



<div class="footnote">
<sup><a id="footnote2">2</a></sup>It's worth noting that 
<a href="https://arxiv.org/abs/1705.10528">a
recent paper</a> showed that under some conditions, mean KL is actually
sufficient.
</div>

<p>In other words, we try to ensure that the <i>average</i> distance between 
conditional probability distributions is small. 
Finally, PPO approximates the mean KL bound of TRPO by attempting to
constrain a <i>ratio</i> between successive conditional probability
distributions, instead of the KL divergence. The exact mechanism for
enforcing this is shown in the 
box below. Intuitively, however, what PPO does is just throw away
(i.e. get no gradient signal from) the
rewards incurred from any state-action pair such that:</p>



<p>where  is a user-chosen hyperparameter.</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input type="checkbox" id="ac-1" name="accordion-1" />
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"></span>¬†<strong>The PPO update step</strong> (Click to expand)</label>
<article class="small">
<i>Note that the following is only for interested readers and is unessential 
for reading the rest of the blog post.</i> <br /> <br />

The exact update used by PPO is as follows, where $\widehat{A}_\pi$ is
the <a href="https://arxiv.org/abs/1506.02438">generalized advantage 
estimate</a>:
$$
\begin{array}{c}{\max _{\theta} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi}\left[\min \left(\operatorname{clip}\left(\rho_{t}, 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{\pi}\left(s_{t}, a_{t}\right), \rho_{t} \widehat{A}_{\pi}\left(s_{t}, a_{t}\right)\right)\right]} \\ {\text{where }\ \ \rho_{t}=\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi\left(a_{t} | s_{t}\right)}}\end{array}
$$
As described in the main text, this intuitively corresponds to throwing
away (i.e. getting no gradient signal) from state-action pairs where the
ratio of conditional probabilities between successive policies is too high.
</article>
</div>
</div>
</section>
<p><br /></p>

<p>To recap, there is a theoretically motivated algorithm \eqref{eq:klpen} which
constrains maximum KL. This motivates TRPO‚Äôs bound on
mean KL in \eqref{eq:trpotrust}, which in turn motivates the 
ratio-based bound of PPO (shown in the box). This chain of approximations might 
lead us to ask: <i>how well do these algorithms actually maintain trust regions</i>?</p>

<p>We first plot the mean KL divergence between successive policies for each
algorithm:
<img src="https://gradientscience.org/images/rl/meankl_trust.jpg" style="width: 50%;" />
<br /></p>

<p>TRPO seems to constrain this very well
<a href="https://gradientscience.org/feed.xml#footnote3"><sup>3</sup></a>
! On the other hand, our two
varieties of the PPO algorithm paint a drastically different picture. Recall that we decided to
separately study two versions of PPO: PPO (based on a state-of-the-art implementation), 
and PPO-M, which we defined to be the core PPO algorithm without auxiliary optimizations. 
PPO <i>with</i> optimizations does quite well at maintaining a KL trust region, 
but PPO-M does not. This is unexpected: PPO‚Äôs main mechanism for maintaining the
trust region (the ratio clipping) is present in both methods‚Äîthe
differences are only in auxiliary optimizations such as Adam learning rate annealing or
orthogonal initialization. As such, it is unclear exactly which mechanisms 
in PPO are responsible for maintaining the mean KL constraint.</p>

<div class="footnote">
<a id="footnote3"><sup>3</sup></a>
Note that this is somewhat unsurprising, since TRPO constrains this
directly in its optimization.
</div>

<p>In fact, we find that PPO‚Äôs inability to maintain a KL-based trust region 
is not entirely due to the looseness of its relaxation; it turns out that 
PPO does not even successfully enforce its <i>own</i> ratio-based trust region.
Below, we plot the maximum ratio \eqref{eq:ratio} between successive
policies for the three algorithms in question:</p>

<p><img src="https://gradientscience.org/images/rl/maxratio_trust.jpg" style="width: 50%;" />
<br /></p>

<p>In the above, the dotted line represents , corresponding to the 
bound in \eqref{eq:ratio}‚Äîit looks like the max ratio is not kept at all! And once again, 
simply adding the auxiliary, code-level optimizations to PPO-M
yields <i>better</i> trust region
enforcement, despite the main clipping mechanism staying the same.
Indeed, it turns out that the
way that PPO enforces the ratio trust region does not actually keep the ratios
from becoming too large or too small. In fact, in our paper we show that there
are <i>infinite</i> optima of the optimization problem PPO solves to find each
step and only <i>one</i> of them enforces the intended trust region bound.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>Deep reinforcement learning algorithms are rooted in a well-grounded framework
of classical RL, and have shown great promise in practice. However, as we‚Äôve
found in our three-part investigation, this framework often falls a little short
of explaining the behavior of these algorithms in practice.</p>

<p>Beyond just being disconcerting, this disconnect impedes our understanding of
why these algorithms succeed (or fail). It also poses a major barrier to
addressing key challenges facing deep RL, such as widespread brittleness and
poor reproducibility (as has been observed by our 
<a href="https://gradientscience.org/policy_gradients_pt1">study in part one</a> and
many others, e.g., [<a href="https://arxiv.org/abs/1709.06560">1</a>,
<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">2</a>,
<a href="http://amid.fish/reproducing-deep-rl">3</a>]).</p>

<p>To close this gap, we need to either develop methods that adhere more closely to
theory, or build theory that can capture what makes existing policy gradient
methods successful. In both cases, the first step is to precisely pinpoint where
theory and practice diverge. Even more broadly, our findings suggest that
developing a deep RL toolkit that is truly robust and reliable will require
moving beyond the current benchmark-driven evaluation model, to a more
fine-grained understanding of deep RL algorithms.</p></div>







<p class="date">
<a href="http://gradientscience.org/policy_gradients_pt3"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6025884855508893027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">x3   + y3 + z3 = 33  has a solution in Z. And its big!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the following problem:<br />
<br />
Given k, a natural number, determine if there exists x,y,z INTEGERS such that x<sup>3</sup>+y<sup>3</sup>+z<sup>3</sup>=k.<br />
<br />
It is not obvious that this problem is decidable (I think it is but have not been able to find an exact statement to that affect; however, if it was not solvable, I would know that, hence it is solvable. If you know a ref give it in the comments.)<br />
<br />
<br />
If k‚â° 4,5 mod 9  then mod arguments easily show there is no solution.¬†<a href="https://arxiv.org/pdf/1604.07746.pdf">Huisman</a>¬†showed that if k‚â§ 1000, k‚â°1,2,3,6,7,8 mod 9 and max(|x|,|y|,|z|) ‚â§ 10<sup>15</sup> and k is NOT one of<br />
<br />
33, 42, 114, 165, 390, 579, 627, 633, 732, 795, 906, 921, 975<br />
<br />
then there was a solution. For those on the list it was unknown.<br />
<br />
Recently¬†<a href="https://people.maths.bris.ac.uk/~maarb/papers/cubesv1.pdf">Booker</a>¬†(not Cory Booker, the candidate for prez, but Andrew Booker who I assume is a math-computer science person and is not running for prez) showed that<br />
<br />
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =33<br />
<br />
DOES have a solution in INTEGERS. It is<br />
<br />
x= 8,866,128,975,287,528<br />
<br />
y=-8,778,405,442,862,239<br />
<br />
z=-2,736,111,468,807,040<br />
<br />
<br />
does that make us more likely or less likely to think that<br />
<br />
x<sup>3</sup>¬†+ y<sup>3</sup>¬†+ z<sup>3</sup>¬†=42<br />
<br />
has a solution? How about =114, etc, the others on the list?<br />
<br />
Rather than say what I think is true (I have no idea) here is what I HOPE is true: that the resolution of these problems leads to some mathematics of interest.<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html"><span class="datestr">at April 29, 2019 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/063">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/063">TR19-063 |  Efficient Black-Box Identity Testing for Free Group Algebra | 

	Abhranil Chatterjee, 

	Vikraman Arvind, 

	Partha Mukhopadhyay, 

	Rajit Datta</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Hrube≈° and Wigderson [HW14] initiated the study of
  noncommutative arithmetic circuits with division computing a
  noncommutative rational function in the free skew field, and
  raised the question of rational identity testing. It is now known
  that the problem can be solved in deterministic polynomial time in
  the white-box model for noncommutative formulas with
  inverses, and in randomized polynomial time in the black-box
  model [GGOW16, IQS18, DM18], where the running time is
  polynomial in the size of the formula. 

  The complexity of identity testing of noncommutative rational
  functions remains open in general (when the formula size
  is not polynomially bounded). We solve the problem for a natural
  special case. We consider polynomial expressions in the free group
  algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots, x_n\}$, a
  subclass of rational expressions of inversion height one. Our main
  results are the following.

1. Given a degree $d$ expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized $\text{poly}(n,d)$ algorithm to check
  whether $f$ is an identically zero expression or not. We obtain this
  by generalizing the Amitsur-Levitzki theorem [AL50] to
  $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a deterministic identity testing algorithm (and even
  an expression reconstruction algorithm) that is polynomial time in
  the sparsity of the input expression.

2. Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at most
  $D$, and sparsity $s$, as black-box, we can check whether $f$ is
  identically zero or not in randomized $\text{poly}(n,\log s, \log D)$
  time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/063"><span class="datestr">at April 28, 2019 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">New York Area Theory Day (Spring 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 10, 2019 Columbia University http://www.cs.columbia.edu/theory/s19-tday.html The New York Area Theory Day, co-organized by Columbia, IBM, and NYU, is a semi-annual conference aiming to bring together researchers in the New York Metropolitan area. It usually features a few hour long talks on recent advances in theoretical computer science. The speakers this time are Sepehr Assadi, ‚Ä¶ <a href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/" class="more-link">Continue reading <span class="screen-reader-text">New York Area Theory Day (Spring¬†2019)</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/"><span class="datestr">at April 26, 2019 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Online Optimization Post 2: Constructing Pseudorandom Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an ‚Äúalmost pairwise independent‚Äù pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a ‚Äúbuilder‚Äù against an ‚Äúinspector,‚Äù in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the ‚Äúinspector‚Äù is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the ‚Äúoffline optimum‚Äù that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the ‚Äúbuilder‚Äù that runs the algorithm and the ‚Äúinspector‚Äù who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the ‚Äúdual view‚Äù of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" title="{{\cal C} = \{ C_1,\ldots,C_N \}}" /> is a collection of boolean functions <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" />, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S\subseteq \{ 0,1 \}^n}" class="latex" title="{S\subseteq \{ 0,1 \}^n}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i \in \cal C}" class="latex" title="{C_i \in \cal C}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " /></p>
<p> That is, sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, which we can do with <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log_2 |S|}" class="latex" title="{\log_2 |S|}" /> random bits, is as good as sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" />, which requires <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits, as far as the functions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28%28%5Clog+N%29%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O((\log N)/\epsilon^2)}" class="latex" title="{O((\log N)/\epsilon^2)}" />, so that we can sample from it using only <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" title="{\log\log N + 2\log \frac 1 \epsilon + O(1)}" /> random bits.</p>
<p>
We will prove this result (while also providing an ‚Äúalgorithm‚Äù for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"></span></p>
<p>
First of all, possibly by changing <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2N}" class="latex" title="{2N}" />, we may assume that for every function <img src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C \in {\cal C}}" class="latex" title="{C \in {\cal C}}" /> the function <img src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-C}" class="latex" title="{1-C}" /> is also in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " /></p>
<p>
We will make up an ‚Äúexperts‚Äù setup in which there is an expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> for each function <img src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i}" class="latex" title="{C_i}" />. Thus, the algorithm, at each step, comes up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> over the functions, which we can think of as a ‚Äúprobabilistic function.‚Äù At time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, the adversary chooses a string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t \in \{ 0,1 \}^n}" class="latex" title="{s_t \in \{ 0,1 \}^n}" /> and defines the cost function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " /></p>
<p> where the adversary chooses an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) \geq 0}" class="latex" title="{f_t(x_t) \geq 0}" />. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> is always possible;
</li><li> That the cost function is of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" title="{f_t(x) = \langle \ell_t , x\rangle}" />, where the loss vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\ell_t (i) | \leq 1}" class="latex" title="{|\ell_t (i) | \leq 1}" />, so that the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq 2 \sqrt{T \ln N}}" class="latex" title="{\leq 2 \sqrt{T \ln N}}" />;
</li><li> That the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_1,\ldots,s_T}" class="latex" title="{s_1,\ldots,s_T}" /> of choices by the adversary determines a <img src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" title="{2\sqrt {\frac {\ln N}{T}}}" />-pseudorandom multiset for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />, and, in particular, we get an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom multiset of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" title="{4 \frac {\ln N}{\epsilon^2}}" />
</li></ol>
<p> For the first point, note that for a random <img src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s \sim \{ 0,1 \}^n}" class="latex" title="{s \sim \{ 0,1 \}^n}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " /></p>
<p> so there is an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " /></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " /></p>
<p> so the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " /></p>
<p> which, after dividing by <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p> Consider now the application of constructing a small-support distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" /> that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-almost-pairwise-independent, meaning that if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> is a random string sampled according to this distribution, then, for every <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j}" class="latex" title="{i,j}" />, the marginal <img src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(s_i,s_j)}" class="latex" title="{(s_i,s_j)}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^2}" class="latex" title="{\{ 0,1 \}^2}" /> in total variation distance. This is the same thing as asking for a small-support distribution that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for all functions <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" /> that depend on only two input variables. There are only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} \ln n)}" class="latex" title="{O(\epsilon^{-2} \ln n)}" />, meaning that the distribution can be sampled using <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" /> random bits. Furthermore the algorithm can be implemented to run in time <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{O(1)} / \epsilon^2}" class="latex" title="{n^{O(1)} / \epsilon^2}" />. The only tricky step is how to find the string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> at each step. For a string <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, the loss <img src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f (x_t)}" class="latex" title="{f (x_t)}" /> obtained by choosing <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> as the ‚Äúreference string‚Äù is a polynomial of degree 2 in the bits of <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, so we can find a no-worse-than-average <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> of the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-th string in the sample space can be generated in time <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\log n)^{O(1)}}" class="latex" title="{(\log n)^{O(1)}}" />. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma‚Äôs construction of small-bias generators we would not get the correct dependency on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />. This framework can ‚Äúderandomize Chernoff bounds‚Äù in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of ‚Äúpessimistic estimators‚Äù described in the Motwani-Raghavan book on randomized algorithms. </p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"><span class="datestr">at April 26, 2019 01:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=351">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/">TCS+ talk: Wednesday, May 1st ‚Äî Chris Peikert, University of Michigan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about ‚Äú<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>‚Äù (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat‚ÄîShamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on ‚Äúexotic‚Äù assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> circuits, for any polynomially bounded <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible ‚Äúmodes,‚Äù yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" target="_blank" rel="noopener">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/"><span class="datestr">at April 25, 2019 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1084">Call for Participation: HALG 2019 (Highlights of Algorithms)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>-------------------------------------------------------------------<br />
4rd Highlights of Algorithms conference (HALG 2019)<br />
Copenhagen, June 14-16, 2019<br />
<a href="http://highlightsofalgorithms.org/" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br />
highlights of recent developments in algorithms and for discussing<br />
potential further advances in this area. The conference will provide a<br />
broad picture of the latest research in algorithms through a series of<br />
invited talks, as well as the possibility for all researchers and<br />
students to present their recent results through a series of short<br />
talks and poster presentations. Attending the Highlights of Algorithms<br />
conference will also be an opportunity for networking and meeting<br />
leading researchers in algorithms.<br />
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br />
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br />
accepted short contributions can be found at:<br />
<a href="http://2018.highlightsofalgorithms.org/programme" target="_blank" rel="noopener noreferrer">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br />
¬† ¬† ¬†<a href="http://highlightsofalgorithms.org/registration" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/registration</a><br />
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br />
- academic rate (incl. postdocs): 160‚Ç¨<br />
- student rate: 115‚Ç¨</p>
<p>Regular registration will be 50‚Ç¨ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. √òrsted Institute of the<br />
University of Copenhagen.<br />
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br />
Monika Henzinger (University of Vienna)<br />
Thomas Vidick (California Institute of Technology)<br />
Laszlo Vegh (London School of Economics)<br />
James Lee (University of Washington)<br />
Timothy Chan (University of Illinois at Urbana-Champaign)<br />
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br />
Martin Grohe (RWTH Aachen University)<br />
Josh Alman (MIT)<br />
Nima Anari (Stanford University)<br />
Michal Kouck√Ω (Charles University)<br />
Naveen Garg (IIT Delhi)<br />
Vera Traub (University of Bonn)<br />
Rico Zenklusen (ETH Zurich)<br />
Shayan Oveis Gharan (University of Washington)<br />
Greg Bodwin (MIT)<br />
Cliff Stein (Columbia University)<br />
Sungjin Im (University of California at Merced)<br />
C. Seshadhriy (University of California, Santa Cruz)<br />
Shay Moran (Technion)<br />
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br />
Sebastien Bubeck (Microsoft Research, Redmond)<br />
Sushant Sachdeva (University of Toronto)<br />
Kunal Talwar (Google Brain)<br />
Moses Charikar (Stanford University)<br />
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1084"><span class="datestr">at April 25, 2019 12:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8078353386966881451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html">Geo-Centric Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br />
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (L√°szl√≥ Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk.¬†</li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html"><span class="datestr">at April 25, 2019 12:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Are Natural Mathematical Problems Bad Problems?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One unique aspect of the conference ‚ÄúVisions in Mathematics Towards 2000‚Äù (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.¬† In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to¬† one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p></p>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that¬† the densest packing problem in <img src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R^3" class="latex" title="R^3" /> is not interesting.¬† In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes‚Äôs earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" alt="" width="640" class="alignnone size-full wp-image-17390" height="361" /></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov‚Äôs proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.¬† He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and¬† G√ºnter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5>
<p><span id="more-17389"></span></p>
<p>encore</p>
<p>¬†</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/v1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/v1.png?w=300&amp;h=188" alt="" width="300" class="alignnone size-medium wp-image-17401" height="188" /></a> ¬†<a href="https://gilkalai.files.wordpress.com/2019/04/v2.png"><img src="https://gilkalai.files.wordpress.com/2019/04/v2.png?w=300&amp;h=210" alt="" width="300" class="alignnone size-medium wp-image-17402" height="210" />¬†¬†</a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/n1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/n1.png?w=300&amp;h=227" alt="" width="300" class="alignnone size-medium wp-image-17403" height="227" /></a> <a href="https://gilkalai.files.wordpress.com/2019/04/n2.png"><img src="https://gilkalai.files.wordpress.com/2019/04/n2.png?w=214&amp;h=300" alt="" width="214" class="alignnone size-medium wp-image-17404" height="300" /></a></p>
<p><span style="color: #ff0000;">Vitali Milman attacked the solution of the 4CT as ‚Äúbad‚Äùand Segei Novikov disagreed and referred to the proof as ‚Äúgreat‚Äù.¬† </span></p>
<p>¬†</p>
<p>¬†</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/"><span class="datestr">at April 25, 2019 09:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4236">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Online Optimization Post 1: Multiplicative Weights</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> self-described experts. At each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,2,\ldots}" class="latex" title="{t=1,2,\ldots}" />, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" title="{x_t = (x_t(1),\ldots,x_t(n))}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) \geq 0}" class="latex" title="{x_t (i) \geq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" title="{\sum_{i=1}^n x_t(i)=1}" />. After the algorithm makes this choice, it is revealed that following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> leads to loss <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t (i)}" class="latex" title="{\ell_t (i)}" />, so that the expected loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}" />. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm ‚Äúregrets‚Äù that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " /></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" title="{\Delta \subseteq {\mathbb R}^n}" /> of probability distributions over the sample space <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots,n\}}" class="latex" title="{\{ 1,\ldots,n\}}" /> and in which the loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x)}" class="latex" title="{f_t (x)}" /> are linear functions of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" title="{f_t (x) = \sum_i x(i) \ell_t (i)}" />. In order to bound the regret, we also have to bound the ‚Äúmagnitude‚Äù of the loss functions, so in the following we will assume that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and all <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t (i) | \leq 1}" class="latex" title="{| \ell_t (i) | \leq 1}" />, and otherwise we can scale everything by a known upper bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{t,i} |\ell_t |}" class="latex" title="{\max_{t,i} |\ell_t |}" />.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> a vector of <em>weights</em> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" title="{w_t = (w_t(1),\ldots,w_t(n))}" /> which is initialized as <img src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_1 := (1,\ldots,1)}" class="latex" title="{w_1 := (1,\ldots,1)}" />. The algorithm performs the following operations at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" />
</li></ul>
<p>
That is, the weight of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" />, and the probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t(i)}" class="latex" title="{x_t(i)}" /> of following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is proportional to the weight. The parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t(i) | \leq 1}" class="latex" title="{| \ell_t(i) | \leq 1}" />, for every <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" title="{0 &lt; \epsilon &lt; 1/2}" />, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " /></p>
<p> In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T &gt; 4 \ln n}" class="latex" title="{T &gt; 4 \ln n}" />, by setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}" /> we achieve a regret bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<span id="more-4236"></span></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, define the quantity</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " /></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, and that if <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " /></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^*}" class="latex" title="{L^*}" /> is large)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> be an index such that <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" title="{L^* = \sum_{t=1}^T \ell_t (j)}" />. Then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t^2}" class="latex" title="{\ell_t^2}" /> is the vector whose <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />-th coordinate is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left( \ell_t (i)\right)^2 }" class="latex" title="{\left( \ell_t (i)\right)^2 }" /> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Since we know that <img src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_1 = n}" class="latex" title="{W_1 = n}" />, it is enough to prove that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,\ldots, T}" class="latex" title="{t=1,\ldots, T}" />, we have <a name="eq.lemma.two"></a></p><a name="eq.lemma.two">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.lemma.two"></a> And we see that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " /></p>
<p> where we used the definitions of our quantities and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-z} \leq 1-z+z^2}" class="latex" title="{e^{-z} \leq 1-z+z^2}" /> for <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1/2}" class="latex" title="{|z| \leq 1/2}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Using the fact that <img src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-z \leq e^{-z}}" class="latex" title="{1-z \leq e^{-z}}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1}" class="latex" title="{|z| \leq 1}" />, the above lemmas can be restated as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " /></p>
<p> which together imply </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " /></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let‚Äôs call the loss caused by expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> after <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps the <em>energy</em> of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " /></p>
<p> Note that we have defined it in such a way that the algorithm knows <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. Our offline optimum is the energy of the lowest energy expert at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />, that, is, the energy of the <em>ground state</em> at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />. When we have a collection of numbers <img src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(1),\ldots, E_t(n)}" class="latex" title="{E_t(1),\ldots, E_t(n)}" />, a nice lower bound to their minimum is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> which is true for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" />. The right-hand side above is the <em>free energy</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1 \epsilon}" class="latex" title="{\frac 1 \epsilon}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. This seems like the kind of expression that we could use to bound the offline optimum, so let‚Äôs give it a name </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> are the losses of the experts at times <img src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1,\ldots,t-1}" class="latex" title="{1,\ldots,t-1}" />. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on experts that have done well in the first <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps, that is <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on ‚Äúlower-energy‚Äù experts. When we have a system in which, at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, state <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> has energy <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" />, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " /></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " /></p>
<p> So far we have ‚Äúrediscovered‚Äù our multiplicative weights algorithm, and the quantity <img src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_t}" class="latex" title="{W_t}" /> that we had in our analysis gets interpreted as the partition function <img src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_t}" class="latex" title="{Z_t}" />. The fact that <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{T+1}}" class="latex" title="{\Phi_{T+1}}" /> bounds the offline optimum suggests that we should use <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_t}" class="latex" title="{\Phi_t}" /> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " /></p>
<p> which telescopes to give </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> Recalling that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " /></p>
<p> we have again </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"><span class="datestr">at April 25, 2019 06:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15798">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/">Why Check A Proof?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="G√∂del‚Äôs Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Why check another‚Äôs proof?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" alt="" width="300" class="alignright size-medium wp-image-15809" height="203" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=600" alt="" class="aligncenter size-full wp-image-15804" /></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.<br />
<span id="more-15798"></span></p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where ‚Äúproof‚Äù is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on ‚Äúwhy check proofs‚Äù you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead‚Äôs three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p></p><h2> Why Check A Proof? </h2><p></p>
<p></p><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person‚Äôs proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p></p><h3> Bob has to </h3><p></p>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p></p><h3> Bob wants to </h3><p></p>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice‚Äôs theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should‚Äîif possible‚Äìexplain that her proof of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses something new. Proofs that are ‚Äújust technical inductions‚Äù are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction‚Ä¶</i> This is not a good idea. </p>
<p>
</p><p></p><h3> Bob needs to </h3><p></p>
<p>This happens when Bob has some ‚Äúskin‚Äù in the game. A classic situation is when Bob has an earlier result that is affected by Alice‚Äôs new theorem. If <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is stronger than Bob‚Äôs previous result, then he is motivated to check her theorem. Or if <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p></p><h2> Skin in the Game </h2><p></p>
<p></p><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m,n)}" class="latex" title="{B(m,n)}" /> be the class of groups that are generated by <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> elements and all elements in the group satisfy, <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{n} = 1}" class="latex" title="{x^{n} = 1}" />. Sergei Adian and Pyotr Novikov proved that <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m, n)}" class="latex" title="{B(m, n)}" /> is infinite for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> odd, <img src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{n \ge 4381}}" class="latex" title="{{n \ge 4381}}" /> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton‚Äôs proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton‚Äôs 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" alt="" width="199" class="aligncenter size-medium wp-image-15802" height="300" /></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of ‚Äúskin in the game‚Äù. He writes about an enumeration problem of counting how many curves lie on a certain manifold‚Äîa century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  317,206,375. " class="latex" title="\displaystyle  317,206,375. " /></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2,682,549,425. " class="latex" title="\displaystyle  2,682,549,425. " /></p>
<p>Somewhat a different count‚Äînot even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p></p><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p></p>
<p></p><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger‚Äôs page, including its use of ‚Äúshowed‚Äù:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, proves a new variant of the Union Theorem of McCreight and Meyer for <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p></p><p>
Woeginger gives a link to Hauptmann‚Äôs <a href="http://arxiv.org/abs/1602.04781">paper</a>, ‚ÄúOn Alternation and the Union Theorem,‚Äù and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann‚Äôs idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, ‚ÄúHence the assumption <img src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P = \Sigma_{2}^{p}}" class="latex" title="{P = \Sigma_{2}^{p}}" /> cannot hold.‚Äù We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p></p><h2> A Resolved Claim </h2><p></p>
<p></p><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\neq}" class="latex" title="{\neq}" /> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short‚Äîthe crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24‚Äì48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum‚Äôs attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann‚Äôs paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/"><span class="datestr">at April 24, 2019 02:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
