<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 04, 2019 05:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/135">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/135">TR19-135 |  Doubly-Efficient Pseudo-Deterministic Proofs | 

	Dhiraj Holden, 

	Shafi Goldwasser, 

	Michel Goemans</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In [20] Goldwasser, Grossman and Holden  introduced pseudo-deterministic interactive proofs for search problems where a powerful prover can convince a probabilistic polynomial time verifier that a solution to a search problem is canonical.  They studied  search problems for which polynomial time algorithms are not known and for which many solutions are possible. They showed that whereas there exists a constant round pseudo deterministic proof for graph isomorphism where the canonical solution is the lexicographically smallest isomorphism, the existence of pseudo-deterministic interactive proofs for NP-hard problems would imply the collapse of the polynomial time hierarchy.

In this paper, we turn our attention to studying  doubly-efficient pseudo-deterministic proofs for polynomial time search problems:  pseudo-deterministic proofs with the extra requirement that the prover runtime is polynomial  and the verifier runtime to verify that a solution is canonical is significantly lower  than the complexity of finding any solution, canonical or otherwise. Naturally this question is particularly interesting for search problems for which a lower bound on its worst case complexity  is known or has been widely conjectured. 

We show doubly-efficient pseudo-deterministic algorithms for a host of natural problems whose complexity has long been conjectured. In particular:

We show a doubly efficient pseudo-deterministic proof for linear programming where the canonical solution which the prover will provide is  the lexicographically greatest optimal solution for the LP. To this end, we show how through perturbing the linear program and strong duality this solution can be both  computed efficiently by the prover, and verified by the verifier.  
The time of the verifier is $O(d^2 )$ for a linear program with integer data and at most $d$ variables and constraints, whereas the time to solve such linear program is $\tilde{O}(d^{\omega} )$ by randomized algorithms [11] for $\omega$  the exponent for fast matrix multiplication .


We show a doubly efficient pseudo-deterministic proof for 3-SUM and problems reducible to 3-SUM where the prover is a $O(n^2)$ time algorithm and the verifier takes time $\tilde{O}(n^{1.5})$. 


We show a doubly-efficient pseudo-deterministic proof for the hitting set problem} where the verifier runs in time $\tilde{O}(m)$ and the prover runs in time $\tilde{O}(m^2)$ where $ m = \sum_{S \in \mathcal{S}} |S| + \sum_{T \in \mathcal{T}} |T|$ for inputs  collections of sets $\mathcal{S}, \mathcal{T}$.

We show a doubly-efficient pseudo-deterministic proof for the Zero Weight Triangle problem where the verifier runs in time $\tilde{O}(n^{2 + \omega/3})$ and the prover runs in randomized time $\tilde{O}(n^3)$. The Zero Weight Triangle problem is equivalent to the All-Pairs Shortest Path problem, a well-studied problem that is the foundation of many hardness results in graph algorithms [39,38], under sub-cubic reductions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/135"><span class="datestr">at October 04, 2019 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/134">TR19-134 |  Finding monotone patterns in sublinear time | 

	Omri Ben-Eliezer, 

	Clement Canonne, 

	Shoham Letzter, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the problem of finding monotone subsequences in an array from the viewpoint of sublinear algorithms. For fixed $k \in \mathbb{N}$ and $\varepsilon &gt; 0$, we show that the non-adaptive query complexity of finding a length-$k$ monotone subsequence of $f \colon [n] \to \mathbb{R}$, assuming that $f$ is $\varepsilon$-far from free of such subsequences, is $\Theta((\log n)^{\lfloor \log_2 k \rfloor})$. Prior to our work, the best algorithm for this problem, due to Newman, Rabinovich, Rajendraprasad, and Sohler (2017), made $(\log n)^{O(k^2)}$ non-adaptive queries; and the only lower bound known, of $\Omega(\log n)$ queries for the case $k = 2$, followed from that on testing monotonicity due to Erg\"un, Kannan, Kumar, Rubinfeld, and Viswanathan (2000) and Fischer (2004).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/134"><span class="datestr">at October 04, 2019 03:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/">Postdoc at University of Warwick, UK (apply by December 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for a postdoc position hosted by Tom Gur at the University of Warwick, United Kingdom, in the fields of sublinear-time algorithms, coding theory, interactive and probabilistically checkable proofs, and complexity theory.</p>
<p>Please contact Tom Gur directly with your CV. The start date is flexible.</p>
<p>Website: <a href="https://www.dcs.warwick.ac.uk/~tomgur/">https://www.dcs.warwick.ac.uk/~tomgur/</a><br />
Email: tom.gur@warwick.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/04/postdoc-at-university-of-warwick-uk-apply-by-december-1-2019/"><span class="datestr">at October 04, 2019 01:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3425">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/10/04/highlights-beyond-ec-call-for-nominations/">Highlights Beyond EC – Call for Nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="http://ec20.sigecom.org/">21st ACM Conference on Economics and Computation</a> (EC’20) will host a special <strong>plenary session</strong> highlighting some of the best work in economics and computation that appears in conferences and journals other than EC, or mature working papers. The intention of this session is to expose EC attendees to related work just beyond the boundary of their current awareness. We seek nominations for papers in Economics and Computation that have made breakthrough advances, opened up new horizons for research, made interesting connections between different scientific areas, or had significant impact on practice. Examples of relevant conferences and journals include STOC/FOCS/SODA/ITCS, AAAI/IJCAI/AAMAS, NIPS/ICML/COLT, WWW/KDD, AER/Econometrica/JPE/QJE/RESTUD/TE/AEJ Micro/JET/GEB, and Math of OR/Management Science/Operations Research.</p>
<p><strong>Who can nominate? </strong>This call is open to everyone (self-nominations are also allowed), but we particularly encourage members of PCs or editorial boards in various venues to submit nominations.</p>
<p><strong>Deadline:</strong> December 23, 2019.</p>
<p><strong>Nomination format:</strong> Nominations should be emailed to <a href="mailto:HighlightsBeyondEC@gmail.com">HighlightsBeyondEC2020@gmail.com</a>, and should include:</p>
<ul>
<li>Paper title and author names.</li>
<li>Publication venue or online working version. Preference will be given to papers that have appeared in a related conference or journal within the past two years, or have a working version circulated within the past two years.</li>
<li>A short (2-3 paragraph) justification letter, explaining the significance of the paper.</li>
<li>Names of 1-3 experts on the area of the paper.</li>
</ul>
<p><strong>Committee members:</strong></p>
<ul>
<li><strong>Michal Feldman </strong>(Tel Aviv University)</li>
<li><strong>Hervé Moulin</strong> (University of Glasgow)</li>
<li><strong>Michael Wellman </strong>(University of Michigan)</li>
<li><strong>Adam Wierman </strong>(California Institute of Technology)</li>
</ul></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2019/10/04/highlights-beyond-ec-call-for-nominations/"><span class="datestr">at October 04, 2019 06:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1192">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1192">News for Sept 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><s>Five</s> Six papers this month: results on testing separations, linearity testing in \(\mathbb{R}^n\), testing for regular languages, graph property testing, topological property testing, and Boolean rank. </p>



<p><strong>Hard properties with (very) short PCPPs and their applications</strong>, by Omri Ben-Eliezer, Eldar Fischer, Amit Levi, and Ron D. Rothblum (<a href="https://eccc.weizmann.ac.il/report/2019/088/">arXiv</a>). Probably, the most significant takeaway from this work is a (largest possible) separation between standard and tolerant property testing. PCPPs (Probabilistically Checkable Proofs of Proximity) are the “NP” variant of property testing, where the tester is aided by a proof string. Consider property \(\mathcal{P}\). If \(x \in \mathcal{P}\), there must be a proof string that makes the tester accept (with probability 1). If \(x\) is far from \(\mathcal{P}\) (in the usual property testing sense), for any proof string, the tester must reject with sufficiently high probability. PCPPs have played a role in the classical constructions of PCPs, but have also found uses in getting a better understanding of property testing itself. And this paper shows how PCPP constructions can be used to get property testing separations. The main result in this paper is a property \(\mathcal{P}\) that (basically) requires \(\Omega(n)\) queries to “property test”, but has a PCPP system where the proof length is \(\widetilde{O}(n)\). (\(n\) is the input length.) The main construction uses collections of random linear codes. Significantly, these constructions show a strong separation between standard vs tolerant property testing, and standard vs erasure-resilient property testing. (The latter is a recent variant by <a href="https://epubs.siam.org/doi/abs/10.1137/16M1075661?journalCode=smjcat">Dixit et al</a>, where certain parts of the input are hidden from the tester.) There is a property that is testable in a constant number of queries, but requires \(\widetilde{\Omega}(n)\) queries to test tolerantly (for any non-trivial choice of parameters). An analogous result holds for erasure-resilient testing.</p>



<p><strong>Distribution-Free Testing of Linear Functions on R^n</strong>, by Noah Fleming and Yuichi Yoshida (<a href="https://arxiv.org/abs/1909.03391">arXiv</a>). Linearity testing is arguably <em>the</em> canonical problem in property testing, yet there is still much to be learned about it. This paper considers functions \(f: \mathbb{R}^n \to \mathbb{R}\), and the <em>distribution-free setting</em>. (In this setting, distance is measured according is an unknown distribution \(\mathcal{D}\) over the input, and the tester can access samples from this distribution. For \(\mathbb{R}^n\), the “standard” distribution would the \(n\)-dimensional Gaussian.) The main result is that linearity testing can be done in the distribution-free setting with \(\widetilde{O}(1/\varepsilon)\) queries, assuming that the distribution is continuous. The primary technical tool, an interesting result in its own right, is that additivity \((f(x+y) = f(x) + f(y))\) can be tested in \(\widetilde{O}(1/\varepsilon)\) queries. The significance of the testing result is cemented by an \(\Omega(n)\) lower bound for sample-based testers.</p>



<p><strong>Sliding window property testing for regular languages</strong> by Moses Ganardi, Danny Hucke, Markus Lohrey, Tatiana Starikovskaya (<a href="https://arxiv.org/abs/1909.10261">arXiv</a>). Fix a regular language \(\mathcal{R}\). Consider the streaming model, and the basic question of recognizing whether the string (being streamed) is in \(\mathcal{R}\). Simple, you will say! Run the DFA recognizing \(\mathcal{R}\) in constant space. Now, suppose there is a sliding window length of \(n\). The aim is to determine if the past \(n\) symbols (the “active window”) form a string in \(\mathcal{R}\). Suprisingly (at least to me), there is a full characterization of the space required for randomized algorithms, and (depending on \(\mathcal{R}\)), it is either \(\Theta(1)\), \(\Theta(\log\log n)\), \(\Theta(\log n)\), or \(\Theta(n)\).  In the interest of beating these lower bounds, suppose we wish to property test on the active window. It turns out the answer is quite nuanced. There are deterministic \(O(\log n)\)-space testers and randomized two-sided \(O(1/\varepsilon)\)-space testers for all regular languages. For randomized one-sided testers, there are multiple possibilities for the optimal space complexity, and there is a full characterization of these regular languages.</p>



<p><strong>A characterization of graph properties testable for general planar graphs with one-sided error (It’s all about forbidden subgraphs)</strong> by Artur Czumaj and Christian Sohler (<a href="https://arxiv.org/pdf/1909.10647.pdf">arXiv</a>). Property testing of sparse graphs has been receiving more attention, but most results focus on the bounded degree setting. Unfortunately, many of these results break quite dramatically on sparse graphs with unbounded degrees. This paper focuses on property testing, within the class of unbounded degree planar graphs. (Meaning, the input is always assumed to be planar.) The results achieve a significant goal: as the title suggests, there is a complete characterization of properties that are constant-query testable with one-sided error. The easier part is in showing that all such properties can be reduced to testing \(H\)-freeness. The harder (remarkable) result is \(H\)-freeness can be tested in general planar queries with constant queries. (This is non-trivial even for triangle-freeness.) And, as is easy to conjecture but hard to prove, these results carry over for all minor-closed families.  As a small indication of the challenge, most testers for bounded-degree graphs work by doing constant depth BFSes. When high degree vertices are present, this method fails, and we really need new ideas to deal with such graphs.</p>



<p><strong>Near Coverings and Cosystolic Expansion – an example of topological property testing</strong> by Irit Dinur and Roy Meshulam (<a href="https://eccc.weizmann.ac.il/report/2019/126/">ECCC</a>). In most algebraic settings, property testing results can be seen as local to global theorems. When do local constraints on a large object imply a global condition? This paper gives a topological instantiation of this phenomenon. We need to define the <em>cover</em> of a simplicial complex \(X\). For concreteness, think of a 2D simplicial complex \(X\), which is a hypergraph with hyperedges of size at most 3, where subsets of hyperedges are also present. A 2-cover is a simplicial complex \(X’\) with the following property. It has two copies of each vertex of \(X\). Each hyperedge of \(X\) must have two “corresponding” disjoint copies in \(X’\). Let the copies of vertex \(v\) be \(v_0, v_1\). Then, for every hyperedge (say) \((u,v,w)\) of \(X\), there must be two disjoint hyperedges in \(X’\) involving copies of the corresponding vertices. One can consider the property testing twist: if the neighborhoods of “most” vertices \(v\) in \(X\) satisfy these condition (with respect to the neighborhoods of the copies of \(v\) in \(X’\)), then is \(X’\) close to being a cover of \(X\)? Indeed, this paper proves that such a “property testing condition” holds iff \(X\) is a high-dimensional expander.</p>



<p><strong>Property testing of the Boolean and binary rank</strong> by Michal Parnas, Dana Ron, and Adi Shraibman (<a href="https://arxiv.org/abs/1908.11632">arXiv</a>). The Boolean rank of a matrix \(M\) is a fundamental quantity that appears in many lower bound constructions. (Recall that an \(n \times m\) Boolean matrix \(M\) has a rank \(r\) if \(M\) can be expressed as \(X \cdot Y\), where \(X \in \mathbb{F}_2^{n \times d}\) and \(Y \in \mathbb{F}_2^{d \times m}\).) In the real-valued setting, results show that one can property test rank in \(poly(d/\varepsilon)\) queries. This paper proves an analogous result for the Boolean rank. There is a surprise element here: over reals, the rank can be computed in polynomial time, and many of the geometric intuitions can be brought over to the property testing problem. On the other hand, the Boolean rank is NP-hard to compute exactly, yet we can still get a tester with \(poly(d)\) query complexity. The paper also gives results for <em>binary rank</em>. For the binary rank, we require the component matrices \(X, Y\) to be Boolean, but algebraic operations are over the reals. In the case, the tester has query complexity \(2^{2d}\) (with varying dependencies on \(\varepsilon\) for adaptive/non-adaptive testers). The intriguing open problem is whether \(poly(d)\)-query testers exist for binary rank.</p>



<p> </p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1192"><span class="datestr">at October 04, 2019 05:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01565">On partisan bias in redistricting: computational complexity meets the science of gerrymandering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Tanima.html">Tanima Chatterjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/DasGupta:Bhaskar.html">Bhaskar DasGupta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01565">PDF</a><br /><b>Abstract: </b>The topic of this paper is "gerrymandering", namely the curse of deliberate
creations of district maps with highly asymmetric electoral outcomes to
disenfranchise voters, and it has a long legal history. Measuring and
eliminating gerrymandering has enormous implications to sustain the backbone of
democratic principles of a society. Although there is no dearth of legal briefs
involving gerrymandering over many years, it is only more recently that
mathematicians and applied computational researchers have started to
investigate this topic. However, it has received relatively little attention so
far from the computational complexity researchers dealing with theoretical
analysis of computational complexity issues, such as computational hardness,
approximability issues, etc. There could be many reasons for this, such as
descriptions of these problem non-CS non-math (often legal or political)
journals that theoretical CS (TCS) people usually do not follow, or the lack of
coverage of these topics in TCS publication venues. One of our modest goals in
writing this article is to improve upon this situation by stimulating further
interactions between the gerrymandering and TCS researchers. To this effect,
our main contributions are twofold: (1) we provide formalization of several
models, related concepts, and corresponding problem statements using TCS
frameworks from the descriptions of these problems as available in existing
non-TCS (perhaps legal) venues, and (2) we also provide computational
complexity analysis of some versions of these problems, leaving other versions
for future research.
</p>
<p>The goal of writing this article is not to have the final word on
gerrymandering, but to introduce a series of concepts, models and problems to
the TCS community and to show that science of gerrymandering involves an
intriguing set of partitioning problems involving geometric and combinatorial
optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01565"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01552">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01552">Extensions of the Algorithmic Lovasz Local Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolmogorov:Vladimir.html">Vladimir Kolmogorov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01552">PDF</a><br /><b>Abstract: </b>We consider recent formulations of the algorithmic Lovasz Local Lemma by
Achlioptas-Iliopoulos-Kolmogorov [2] and by Achlioptas-Iliopoulos-Sinclair [3].
These papers analyze a random walk algorithm for finding objects that avoid
undesired "bad events" (or "flaws"), and prove that under certain conditions
the algorithm is guaranteed to find a "flawless" object quickly. We show that
conditions proposed in these papers are incomparable, and introduce a new
family of conditions that includes those in [2, 3] as special cases.
</p>
<p>Secondly, we extend our previous notion of "commutativity" in [15] to this
more general setting, and prove that it allows to use an arbitrary strategy for
selecting the next flaw to address. In the special case of primary flaws we
prove a stronger property: the flaw selection strategy does not affect at all
the expected number of steps until termination, and also does not affect the
distribution induced by the algorithm upon termination. This applies, in
particular, to the single-clause backtracking algorithm for constraint
satisfaction problems considered in [3].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01552"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01492">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01492">A Grid-based Approach for Convexity Analysis of a Density-based Cluster</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Sayyed-Ahmad Naghavi-Nozad, Seyed-Mojtaba Banaei, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saberi:Mohsen.html">Mohsen Saberi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01492">PDF</a><br /><b>Abstract: </b>This paper presents a novel geometrical approach to investigate the convexity
of a density-based cluster. Our approach is grid-based and we are about to
calibrate the value space of the cluster. However, the cluster objects are
coming from an infinite distribution, their number is finite, and thus, the
regarding shape will not be sharp. Therefore, we establish the precision of the
grid properly in a way that, the reliable approximate boundaries of the cluster
are founded. After that, regarding the simple notion of convex sets and
midpoint convexity, we investigate whether or not the density-based cluster is
convex. Moreover, our experiments on synthetic datasets demonstrate the
desirable performance of our method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01492"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01357">Recognizing the Tractability in Big Data Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Xiangyu.html">Xiangyu Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jianzhong.html">Jianzhong Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miao:Dongjing.html">Dongjing Miao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xianmin.html">Xianmin Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01357">PDF</a><br /><b>Abstract: </b>Due to the limitation on computational power of existing computers, the
polynomial time does not works for identifying the tractable problems in big
data computing. This paper adopts the sublinear time as the new tractable
standard to recognize the tractability in big data computing, and the
random-access Turing machine is used as the computational model to characterize
the problems that are tractable on big data. First, two pure-tractable classes
are first proposed. One is the class $\mathrm{PL}$ consisting of the problems
that can be solved in polylogarithmic time by a RATM. The another one is the
class $\mathrm{ST}$ including all the problems that can be solved in sublinear
time by a RATM. The structure of the two pure-tractable classes is deeply
investigated and they are proved $\mathrm{PL^i} \subsetneq \mathrm{PL^{i+1}}$
and $\mathrm{PL} \subsetneq \mathrm{ST}$. Then, two pseudo-tractable classes,
$\mathrm{PTR}$ and $\mathrm{PTE}$, are proposed. $\mathrm{PTR}$ consists of all
the problems that can solved by a RATM in sublinear time after a PTIME
preprocessing by reducing the size of input dataset. $\mathrm{PTE}$ includes
all the problems that can solved by a RATM in sublinear time after a PTIME
preprocessing by extending the size of input dataset. The relations among the
two pseudo-tractable classes and other complexity classes are investigated and
they are proved that $\mathrm{PT} \subseteq \mathrm{P}$, $\sqcap'\mathrm{T^0_Q}
\subsetneq \mathrm{PTR^0_Q}$ and $\mathrm{PT_P} = \mathrm{P}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01357"><span class="datestr">at October 04, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01331">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01331">Optimal Joint Subcarrier and Power Allocation in NOMA is Strongly NP-Hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salaun:Lou.html">Lou Salaun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chung_Shue.html">Chung Shue Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coupechoux:Marceau.html">Marceau Coupechoux</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01331">PDF</a><br /><b>Abstract: </b>Non-orthogonal multiple access (NOMA) is a promising radio access technology
for 5G. It allows several users to transmit on the same frequency and time
resource by performing power-domain multiplexing. At the receiver side,
successive interference cancellation (SIC) is applied to mitigate interference
among the multiplexed signals. In this way, NOMA can outperform orthogonal
multiple access schemes used in conventional cellular networks in terms of
spectral efficiency and allows more simultaneous users. This paper investigates
the computational complexity of joint subcarrier and power allocation problems
in multi-carrier NOMA systems. We prove that these problems are strongly
NP-hard for a large class of objective functions, namely the weighted
generalized means of the individual data rates. This class covers the popular
weighted sum-rate, proportional fairness, harmonic mean and max-min fairness
utilities. Our results show that the optimal power and subcarrier allocation
cannot be computed in polynomial time in the general case, unless P = NP.
Nevertheless, we present some tractable special cases and we show that they can
be solved efficiently.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01331"><span class="datestr">at October 04, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01327">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01327">Privately detecting changes in unknown distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cummings:Rachel.html">Rachel Cummings</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krehbiel:Sara.html">Sara Krehbiel</a>, Yuliia Lut, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Wanrong.html">Wanrong Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01327">PDF</a><br /><b>Abstract: </b>The change-point detection problem seeks to identify distributional changes
in streams of data. Increasingly, tools for change-point detection are applied
in settings where data may be highly sensitive and formal privacy guarantees
are required, such as identifying disease outbreaks based on hospital records,
or IoT devices detecting activity within a home. Differential privacy has
emerged as a powerful technique for enabling data analysis while preventing
information leakage about individuals. Much of the prior work on change-point
detection (including the only private algorithms for this problem) requires
complete knowledge of the pre-change and post-change distributions. However,
this assumption is not realistic for many practical applications of interest.
This work develops differentially private algorithms for solving the
change-point problem when the data distributions are unknown. Additionally, the
data may be sampled from distributions that change smoothly over time, rather
than fixed pre-change and post-change distributions. We apply our algorithms to
detect changes in the linear trends of such data streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01327"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01317">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01317">Orbit Computation for Atomically Generated Subgroups of Isometries of $\mathbb{Z}^n$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Haizi.html">Haizi Yu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mineyev:Igor.html">Igor Mineyev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Varshney:Lav_R=.html">Lav R. Varshney</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01317">PDF</a><br /><b>Abstract: </b>Isometries and their induced symmetries are ubiquitous in the world. Taking a
computational perspective, this paper considers isometries of $\mathbb{Z}^n$
(since values are discrete in digital computers), and tackles the problem of
orbit computation under various isometry subgroup actions on $\mathbb{Z}^n$.
Rather than just conceptually, we aim for a practical algorithm that can
partition any finite subset of $\mathbb{Z}^n$ based on the orbit relation. In
this paper, instead of all subgroups of isometries, we focus on a special class
of subgroups, namely atomically generated subgroups. This newly introduced
notion is key to inheriting the semidirect-product structure from the whole
group of isometries, and in turn, the semidirect-product structure is key to
our proposed algorithm for efficient orbit computation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01317"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01296">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01296">Best-first Search Algorithm for Non-convex Sparse Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakaue:Shinsaku.html">Shinsaku Sakaue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marumo:Naoki.html">Naoki Marumo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01296">PDF</a><br /><b>Abstract: </b>Non-convex sparse minimization (NSM), or $\ell_0$-constrained minimization of
convex loss functions, is an important optimization problem that has many
machine learning applications. NSM is generally NP-hard, and so to exactly
solve NSM is almost impossible in polynomial time. As regards the case of
quadratic objective functions, exact algorithms based on quadratic
mixed-integer programming (MIP) have been studied, but no existing exact
methods can handle more general objective functions including Huber and
logistic losses; this is unfortunate since those functions are prevalent in
practice. In this paper, we consider NSM with $\ell_2$-regularized convex
objective functions and develop an algorithm by leveraging the efficiency of
best-first search (BFS). Our BFS can compute solutions with objective errors at
most $\Delta\ge0$, where $\Delta$ is a controllable hyper-parameter that
balances the trade-off between the guarantee of objective errors and
computation cost. Experiments demonstrate that our BFS is useful for solving
moderate-size NSM instances with non-quadratic objectives and that BFS is also
faster than the MIP-based method when applied to quadratic objectives.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01296"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01293">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01293">A Fast Exponential Time Algorithm for Max Hamming Distance X3SAT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoi:Gordon.html">Gordon Hoi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Sanjay.html">Sanjay Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephan:Frank.html">Frank Stephan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01293">PDF</a><br /><b>Abstract: </b>X3SAT is the problem of whether one can satisfy a given set of clauses with
up to three literals such that in every clause, exactly one literal is true and
the others are false. A related question is to determine the maximal Hamming
distance between two solutions of the instance. Dahll\"of provided an algorithm
for Maximum Hamming Distance XSAT, which is more complicated than the same
problem for X3SAT, with a runtime of $O(1.8348^n)$; Fu, Zhou and Yin considered
Maximum Hamming Distance for X3SAT and found for this problem an algorithm with
runtime $O(1.6760^n)$. In this paper, we propose an algorithm in $O(1.3298^n)$
time to solve the Max Hamming Distance X3SAT problem; the algorithm actually
counts for each $k$ the number of pairs of solutions which have Hamming
Distance $k$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01293"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01251">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01251">Search problems in algebraic complexity, GCT, and hardness of generator for invariant rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makam:Visu.html">Visu Makam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Rafael.html">Rafael Oliveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01251">PDF</a><br /><b>Abstract: </b>We consider the problem of outputting succinct encodings of lists of
generators for invariant rings. Mulmuley conjectured that there are always
polynomial sized such encodings for all invariant rings. We provide simple
examples that disprove this conjecture (under standard complexity assumptions).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01251"><span class="datestr">at October 04, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01147">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01147">Path and Ancestor Queries on Trees with Multidimensional Weight Vectors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Meng.html">Meng He</a>, Serikzhan Kazi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01147">PDF</a><br /><b>Abstract: </b>We consider an ordinal tree $T$ on $n$ nodes, with each node assigned a
</p>
<p>$d$-dimensional weight vector $\pnt{w} \in \{1,2,\ldots,n\}^d,$ where $d \in
\mathbb{N}$ is a constant.
</p>
<p>We study path queries as generalizations of well-known {\textit{orthogonal
range queries}}, with one of the dimensions being tree topology rather than a
linear order. Since in our definitions $d$ only represents the number of
dimensions of the weight vector without taking the tree topology into account,
a path query in a tree with $d$-dimensional weight vectors generalize the
corresponding $(d+1)$-dimensional orthogonal range query.
</p>
<p>We solve {\textit{ancestor dominance reporting}} problem as a direct
generalization of dominance reporting problem, %in time $\O((\lg^{d-1}
n)/(\lg\lg n)^{d-2}+k)$ in time $\O(\lg^{d-1}{n}+k)$ %and space of $\O(n(\lg
n)^{d-1}/(\lg \lg n)^{d-2})$ words, and space of $\O(n\lg^{d-2}n)$ words, where
$k$ is the size of the output, for $d \geq 2.$
</p>
<p>We also achieve a tradeoff of $\O(n\lg^{d-2+\eps}{n})$ words of space, with
query time of $\O((\lg^{d-1} n)/(\lg\lg n)^{d-2}+k),$ for the same problem,
when $d \geq 3.$
</p>
<p>We solve {\textit{path successor problem}} in $\O(n\lg^{d-1}{n})$ words of
space and time $\O(\lg^{d-1+\eps}{n})$ for $d \geq 1$ and an arbitrary constant
$\eps &gt; 0.$ We propose a solution to {\textit{path counting problem}}, with
$\O(n(\lg{n}/\lg\lg{n})^{d-1})$ words of space and $\O((\lg{n}/\lg\lg{n})^{d})$
query time, for $d \geq 1.$
</p>
<p>Finally, we solve {\textit{path reporting problem}} in
$\O(n\lg^{d-1+\eps}{n})$ words of space and
$\O((\lg^{d-1}{n})/(\lg\lg{n})^{d-2}+k)$ query time, for $d \geq 2.$
</p>
<p>These results match or nearly match the best tradeoffs of the respective
range queries. We are also the first to solve path successor even for $d = 1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01147"><span class="datestr">at October 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3615744836127152440">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html">Quantum Supremacy: A Guest Post by Abhinav Deshpande</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I am delighted to introduce you to Abhinav Deshpande, who is a graduate student at the University of Maryland, studying Quantum Computing. This will be a guest post on the rumors of the recent Google breakthrough on Quantum Supremacy. For other blog posts on this exciting rumor, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott Aaronson's post</a>, <a href="https://www.scottaaronson.com/blog/?p=4342">Scott Aaronson's second post on it</a>, <a href="https://www.quantamagazine.org/john-preskill-explains-quantum-supremacy-20191002/">John Preskill's quanta article</a>, <a href="https://blog.computationalcomplexity.org/2019/09/quantum-supremacy.html">Fortnow's post</a>,<br />
and there may be others.<br />
<br />
Guest post by Abhinav:<br />
<br />
I (Abhinav) thank Bill Fefferman for help with this post, and Bill Gasarch for inviting me to do a guest post.<br />
<br />
<br />
<b>The quest towards quantum computational supremacy</b><br />
<br />
September saw some huge news in the area of quantum computing, with rumours that the Google AI Lab has achieved a milestone known as 'quantum computational supremacy', also termed 'quantum supremacy' or 'quantum advantage' by some authors. Today, we examine what this term means, the most promising approach towards achieving this milestone, and the best complexity-theoretic evidence we have so far against classical simulability of quantum mechanics. We will not be commenting on details of the purported paper since there is no official announcement or claim from the authors so far.<br />
<br />
<b>What it means</b><br />
<br />
First off, the field of quantum computational supremacy arose from trying to formally understand the differences in the power of classical and quantum computers. A complexity theorist would view this goal as trying to give evidence to separate the complexity classes BPP and BQP. However, it turns out that one can gain more traction from considering the sampling analogues of these classes, SampBPP and SampBQP.  These are classes of distributions that can be efficiently sampled on classical and quantum computers, respectively. Given a quantum circuit U on n qubits, one may define an associated probability distribution over 2^n outcomes as follows: apply U to the fiducial initial state |000...0&gt; and measure the resulting state in the computational basis. This produces a distribution D_U.<br />
<br />
A suitable way to define the task of simulating the quantum circuit is as follows<b style="font-style: italic;">:</b><br />
<br />
Input: Description of a quantum circuit U acting on n qubits.<br />
<br />
Output: A sample from the probability distribution D_U obtained by measuring U|000...0&gt; in the computational basis.<br />
<br />
One of the early works in this field was that of <a href="https://arxiv.org/abs/quant-ph/0205133">Terhal and DiVincenzo</a>, which first considered the complexity of sampling from a distribution (weak simulation) as opposed to that of calculating the exact probability of a certain outcome (strong simulation). Weak simulation is arguably the more natural notion of simulating a quantum system, since in general, we cannot feasibly compute the probability of a certain outcome even if we can simulate the quantum circuit. Subsequent works by <a href="https://arxiv.org/abs/1011.3245">Aaronson and Arkhipov</a>, and by <a href="https://arxiv.org/abs/1005.1407">Bremner, Jozsa, and Shepherd</a> established that if there is a classically efficient weak simulator for different classes of quantum circuits, the polynomial hierarchy collapses to the third level.<br />
<br />
<br />
So far, we have only considered the question of exactly sampling from the distribution D_U. However, any realistic experiment is necessarily noisy, and a more natural problem is to sample from a distribution that is not exactly D_U but from any distribution D_O that is ε-close in a suitable distance measure, say the variation distance.<br />
<br />
The aforementioned work by Aaronson and Arkhipov was the first to consider this problem, and they made progress towards showing that a special class of quantum circuits (linear optical circuits) is classically hard to approximately simulate in the sense above. The task of sampling from the output of linear optical circuits is known as boson sampling. At the   time, it was the best available way to show that quantum computers  may solve some problems that are far beyond the reach of classical computers.<br />
<br />
Even granting that the PH doesn't collapse, one still needs to make an additional conjecture to establish that boson sampling is not classically simulable.  The conjecture is that additively approximating the output probabilities of a random linear optical quantum circuit is #P-hard.  The reason this may be true is that output probabilities of random linear optical quantum circuits are Permanents of a Gaussian random matrix, and the Permanent is as hard to compute on a random matrix as it is on a worst-case matrix. Therefore, the only missing link is to go from average-case hardness of exact computation to average-case hardness of an additive estimation. In addition, if we make a second conjecture known as the "anti-concentration" conjecture, we can show that this additive estimation is non-trivial: it suffices to give us a good multiplicative estimation with high probability.<br />
<br />
So that's what quantum computational supremacy is about: we have a computational task that is efficiently solvable with quantum computers, but which would collapse the polynomial hierarchy if done by a classical computer (assuming certain other conjectures are true). One may substitute "collapse of the polynomial hierarchy" with stronger conjectures and incur a corresponding tradeoff in the likelihood of the conjecture being true.<br />
<br />
<b>Random circuit sampling</b><br />
<br />
In 2016,<a href="https://arxiv.org/abs/1608.00263"> Boixo et al</a>. proposed to replace the class of quantum circuits for which some hardness results were known (commuting circuits and boson sampling) by random circuits of sufficient depth on a 2D grid of qubits having nearest-neighbour interactions. Concretely, the proposed experiment would be to apply random unitaries from a specified set on n qubits arranged on a 2D grid for sufficient depth, and then sample from the resulting distribution. The two-qubit unitaries in the set are restricted to act between nearest neighbours, respecting the geometric This task is called random circuit sampling (RCS).<br />
<br />
At the time, the level of evidence for the hardness of this scheme was not yet the same as the linear optical scheme. However, given the theoretical and experimental interest in the idea of demonstrating a quantum speedup over classical computers, subsequent works by<a href="https://arxiv.org/abs/1803.04402"> Bouland, Fefferman, Nirkhe and Vazirani</a>, and <a href="https://arxiv.org/abs/1809.06957">Harrow and Mehraban</a> bridged this gap (the relevant work by <a href="https://arxiv.org/abs/1612.05903">Aaronson and Chen</a> will be discussed in the following section). Harrow and Mehraban proved anticoncentration for random circuits. In particular, they showed that a 2-dimensional grid of n qubits achieve anticoncentration in depth O(\sqrt{n}), improving upon earlier results with higher depth due to <a href="https://arxiv.org/abs/1208.0692">Brandao, Harrow and Horodeck</a>i. Bouland et al. proved the same supporting evidence for RCS as that for boson sampling, namely a worst-to-average-case reduction for exactly computing most output probabilities, even without the permanent structure possessed by linear optical quantum circuits.<br />
<br />
<b>Verification</b><br />
<br />
So far, we have not discussed the elephant in the room: of verifying that the output distribution supported on 2^n outcomes. It turns out that there are concrete lower bounds such as those due to Valiant and Valiant, showing that verifying whether an empirical distribution is close to a target distribution is impossible if one has few samples.<br />
<br />
Boixo et al. proposed a way of certifying the fidelity of the purported simulation. Their key observation was to note that if their experimental system is well modelled by a noise model called global depolarising noise, estimating the output fidelity is possible with relatively few outcomes. Under global depolarising noise with fidelity f, the noisy distribution takes the form D_N = f D_U + (1-f) I, where I is the uniform distribution over the 2^n outcomes. Together with another empirical observation about the statistics of output probabilities of the ideal distribution D_U, they argued that computing the following cross-entropy score would serve as a good estimator of the fidelity:<br />
<br />
f ~ H(I, D_U) - H(D_exp, D_U), where H(D_A,D_B) is the cross-entropy between the two distributions: H(D_A, D_B) = -\sum_i p_A log (p_B).<br />
<br />
The proposal here was to experimentally collect several samples from D_exp, classically compute using brute-force the probabilities of these outcomes in the distribution D_U, and estimate the cross-entropy using this information. If the test outputs a high score for a computation on sufficiently many qubits and depth, the claim is that quantum supremacy has been achieved.<br />
<br />
Aaronson and Chen gave alternative form of evidence for the hardness of scoring well on a test that aims to certify quantum supremacy similar to the manner above. This sidesteps the issue of whether a test similar to the one above does indeed certify the fidelity. The specific problem considered was "Heavy Output Generation" (HOG), the problem of outputting strings that have higher than median probability in the output distribution. Aaronson and Chen linked the hardness of HOG to a closely related problem called "QUATH", and conjectured that QUATH is hard for classical computers.<br />
<br />
<b>Open questions</b><br />
<br />
Assuming the Google team has performed the impressive feat of both running the experiment outlined before and classically computing the probabilities of the relevant outcomes to see a high score on their cross-entropy test, I discuss the remaining positions a skeptic might take regarding the claim about quantum supremacy.<br />
<br />
"The current evidence of classical hardness of random circuit sampling is not sufficient to conclude that the task is hard". Assuming that the skeptic believes that the polynomial hierarchy does not collapse, a remaining possibility is that there is no worst-to-average-case reduction for the problem of *approximating* most output probabilities, which kills the proof technique of Aaronson and Arkhipov to show hardness of approximate sampling.<br />
<br />
"The cross-entropy proposal does not certify the fidelity." Boixo et al. gave numerical evidence and other arguments for this statement, based on the observation that the noise is of the global depolarising form. A skeptic may argue that the assumption of global depolarising noise is a strong one.<br />
<br />
"The QUATH problem is not classically hard." In order to give evidence for the hardness of QUATH, Aaronson and Chen examined the best existing algorithms for this problem and also gave a new algorithm that nevertheless do not solve QUATH with the required parameters.<br />
<br />
It would be great if the community could work towards strengthening the evidence we already have for this task to be hard, either phrased as a sampling experiment or together with the verification test.<br />
<br />
Finally, I think this is an exciting time for quantum computing and to witness this landmark event. It may not be the first probe of an experiment that is "hard" to classically simulate, since there are many quantum experiments that are beyond the reach of current classical simulations, but the inherent programmability and control present in the experimental system is what enables the tools of complexity theory to be applied to the problem. A thought that fascinates me is the idea that we may be exploring quantum mechanics in a regime never probed this carefully before, the "high complexity regime" of quantum mechanics. One imagines there are important lessons in physics here.<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/10/quantum-supremacy-guest-post-by-abhinav.html"><span class="datestr">at October 03, 2019 07:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=18199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/">Noisy quantum circuits: how do we know that we have robust experimental outcomes at all? (And do we care?)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">a recent post we discussed Google’s claim of achieving “quantum supremacy” and my reasons to think that these claims will not stand.</a> (See also <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/#comment-61043">this comment</a> for necessary requirements from a quantum supremacy experiment.) This debate gives a good opportunity to discuss some conceptual issues regarding sampling, probability distributions, statistics, and computational complexity. This time we will discuss <span style="color: #ff0000;">chaotic behavior vs. robust experimental outcomes.</span></p>
<p>On unrelated matter, I just heard Shachar Lovett’s very beautiful TCS+ lecture on the sunflower conjecture (<a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">see this post</a> on the Alweiss, Lovett, Wu, and Zhang’s breakthrough). You can see the lecture and many others on the <a href="https://www.youtube.com/user/TCSplusSeminars/videos">TCS+ you tube channel</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2019/10/cern-slide-30c.png?w=640&amp;h=449" class="alignnone size-full wp-image-18240" height="449" /></a></p>
<p style="text-align: center;"><span style="color: #ff0000;"><span style="color: #993366;">Slide 30 from my August, ’19 CERN lecture: predictions of near-term experiments. (Here is the</span> <a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">full powerpoint presentation</a><span style="color: #993366;">.) In this post we mainly</span> <strong>discuss</strong> <strong>point b) about chaotic behavior. </strong><span style="color: #800080;">See also <a href="https://arxiv.org/abs/1908.02499">my paper: The argument against quantum computers</a>.</span></span></p>
<p>Consider an experiment aimed for establishing quantum supremacy: your quantum computer produced a sample <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> which is a 0-1 string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> from a certain distribution <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />. The research assumption is that <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />  is close enough to a fixed distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> (<img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> accounts for the computing process and the noise) which is very hard to be demonstrated on a classical computer. By looking at a large number of samples you can perform a statistical test on the samples to verify that they were (approximately) sampled from <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, or at least that they were sampled from a probability distribution that is very hard to be computed on a classical computer!</p>
<p>But, is it possible that all the distributions <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" />‘s are very different? Namely that each sample is taken from a completely different distribution? More formally, is it possible  that under a correct modeling of the device for two different samples <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_j" class="latex" title="x_j" />, <img src="https://s0.wp.com/latex.php?latex=D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_i" class="latex" title="D_i" /> has a very small correlation with <img src="https://s0.wp.com/latex.php?latex=D_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_j" class="latex" title="D_j" />? In this case we say that the experiment outcomes are <strong>not robust</strong> and that the situation is <strong>chaotic</strong>.</p>
<p>Here are a couple of questions that I propose to think about:</p>
<ul>
<li>How do we test robustness?</li>
<li>Do the supremacy experiments require that the experiment is robust?</li>
<li>If, after many samples, you reach a probability distribution that require exponential time on a classical computer should you worry about the question whether the experiment is robust?</li>
<li><span style="color: #0000ff;">Do the 10,000,000 samples for the Google 53-qubit experiment represent a robust sampling experiment?</span></li>
</ul>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/"><span class="datestr">at October 03, 2019 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/">Faculty position at University of Minnesota-Twin Cities (apply by November 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science &amp; Engineering at the University of Minnesota-Twin Cities is hiring to fill multiple tenure-track positions at the assistant professor level, although higher levels of appointments may be considered when commensurate with experience and accomplishments. One of the areas of interest is theoretical computer science.</p>
<p>Website: <a href="https://www.cs.umn.edu/news/cse-now-hiring-new-faculty">https://www.cs.umn.edu/news/cse-now-hiring-new-faculty</a><br />
Email: csciadmin@umn.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/03/faculty-position-at-university-of-minnesota-twin-cities-apply-by-november-1-2019/"><span class="datestr">at October 03, 2019 06:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/">Faculty at Virginia Tech (apply by December 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="http://careers.pageuppeople.com/968/cw/en-us/job/510994">http://careers.pageuppeople.com/968/cw/en-us/job/510994</a></p>
<p>Website: <a href="http://www.cs.vt.edu/">http://www.cs.vt.edu/</a><br />
Email: facdev@cs.vt.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/10/03/faculty-at-virginia-tech-apply-by-december-31-2019/"><span class="datestr">at October 03, 2019 02:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/10/03/NTK/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/10/03/NTK/">Ultra-Wide Deep Nets and Neural Tangent Kernel (NTK)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(Crossposted <a href="https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/">at CMU ML</a>.)</p>

<p>Traditional wisdom in machine learning holds that there is a careful trade-off between training error and generalization gap. There is a “sweet spot” for the model complexity such that the model (i) is big enough to achieve reasonably good training error, and (ii) is small enough so that the generalization gap - the difference between test error and training error - can be controlled. A smaller model would give a larger training error, while making the model bigger would result in a larger generalization gap, both leading to larger test errors. This is described by the classical U-shaped curve for the test error when the model complexity varies (see Figure 1(a)).</p>

<p>However, it is common nowadays to use highly complex over-parameterized models like deep neural networks. These models are usually trained to achieve near zero error on the training data, and yet they still have remarkable performance on test data. <a href="https://arxiv.org/abs/1812.11118">Belkin et al. (2018)</a> characterized this phenomenon by a “double descent” curve which extends the classical U-shaped curve. It was observed that, as one increases the model complexity past the point where it can perfectly fits the training data (i.e., <em>interpolation</em> regime is reached), test error continues to drop! Interestingly, the best test error is often achieved by the largest model, which goes against the classical intuition about the “sweet spot.” The following figure from <a href="https://arxiv.org/abs/1812.11118">Belkin et al. (2018)</a> illustrates this phenomenon.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/belkinfig.jpg" style="width: 700px;" />
<br />
<b>Figure 1.</b> Effect of increased model complexity on generalization: traditional belief vs actual practice. 
</div>
<p><br /></p>

<p>Consequently one suspects that the training algorithms used in deep learning - (stochastic) gradient descent and its variants - somehow implicitly constrain the complexity of trained networks (i.e., “true number” of parameters), thus leading to a small generalization gap.</p>

<p>Since larger models often give better performance in practice, one may naturally wonder:</p>

<blockquote>
  <p>How does an infinitely wide net perform?</p>
</blockquote>

<p>The answer to this question corresponds to the right end of Figure 1(b). This blog post is about a model that has attracted a lot of attention in the past year:  deep learning in the regime where the width - namely, the number of channels in convolutional filters, or the number of neurons in fully-connected internal layers - goes to infinity. At first glance this approach may seem hopeless for both practitioners and theorists: all the computing power in the world is insufficient to train an infinite network, and theorists already have their hands full trying to figure out finite ones. But in math/physics there is a tradition of deriving insights into questions by studying them in the infinite limit, and indeed here too the infinite limit becomes easier for theory.</p>

<p>Experts may recall the connection between infinitely wide neural networks and kernel methods from 25 years ago by <a href="https://www.cs.toronto.edu/~radford/pin.abstract.html">Neal (1994)</a> as well as the recent extensions by <a href="https://openreview.net/forum?id=B1EA-M-0Z">Lee et al. (2018)</a> and <a href="https://arxiv.org/abs/1804.11271">Matthews et al. (2018)</a>. These kernels correspond to infinitely wide deep networks whose all parameters are chosen randomly, and <em>only the top (classification) layer is trained</em> by gradient descent. Specifically, if $f(\theta,x)$ denotes the output of the network on input $x$ where $\theta$ denotes the parameters in the network, and $\mathcal{W}$ is an initialization distribution over $\theta$ (usually Gaussian with proper scaling), then the corresponding kernel is

where $x,x’$ are two inputs.</p>

<p>What about the more usual scenario when <em>all layers are trained</em>? Recently, <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a> first observed that this is also related to a kernel named <em>neural tangent kernel (NTK)</em>, which has the form
</p>

<p>The key difference between the NTK and previously proposed kernels is that the NTK is defined through the inner product between the gradients of the network outputs with respect to the network parameters. This gradient arises from the use of the gradient descent algorithm. Roughly speaking, the following conclusion can be made for a sufficiently wide deep neural network trained by gradient descent:</p>

<blockquote>
  <p>A properly randomly initialized <strong>sufficiently wide</strong> deep neural network <strong>trained by gradient descent</strong> with infinitesimal step size (a.k.a. gradient flow) is <strong>equivalent to a kernel regression predictor</strong> with a <strong>deterministic</strong> kernel called <em>neural tangent kernel (NTK)</em>.</p>
</blockquote>

<p>This was more or less established in the original paper of <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a>, but they required the width of every layer to go to infinity in a sequential order. In <a href="https://arxiv.org/abs/1904.11955">our recent paper</a> with Sanjeev Arora, Zhiyuan Li, Ruslan Salakhutdinov and Ruosong Wang, we improve this result to the non-asymptotic setting where the width of every layer only needs to be greater than a certain finite threshold.</p>

<p>In the rest of this post we will first explain how NTK arises and the idea behind the proof of the equivalence between wide neural networks and NTKs. Then we will present experimental results showing how well infinitely wide neural networks perform in practice.</p>

<h2 id="how-does-neural-tangent-kernel-arise">How Does Neural Tangent Kernel Arise?</h2>

<p>Now we describe how training an ultra-wide fully-connected neural network leads to kernel regression with respect to the NTK. A more detailed treatment is given in <a href="https://arxiv.org/abs/1904.11955">our paper</a>. We first specify our setup. We consider the standard supervised learning setting, in which we are given $n$ training data points ${(x_i,y_i)}_{i=1}^n \subset \mathbb{R}^{d}\times\mathbb{R}$ drawn from some underlying distribution and wish to find a function that given the input $x$ predicts the label $y$ well on the data distribution. We consider a fully-connected neural network defined by $f(\theta, x)$, where $\theta$ is the collection of all the parameters in the network and $x$ is the input. For simplicity we only consider neural network with a single output, i.e., $f(\theta, x) \in \mathbb{R}$, but the generalization to multiple outputs is straightforward.</p>

<p>We consider training the neural network by minimizing the quadratic loss over training data:

Gradient descent with infinitesimally small learning rate (a.k.a. gradient flow) is applied on this loss function $\ell(\theta)$:                                                                               
where $\theta(t)$ denotes the parameters at time $t$.</p>

<p>Let us define some useful notation. Denote $u_i = f(\theta, x_i)$, which is the network’s output on $x_i$. We let $u=(u_1, \ldots, u_n)^\top \in \mathbb{R}^n$ be the collection of the network outputs on all training inputs. We use the time index $t$ for all variables that depend on time, e.g. $u_i(t), u(t)$, etc. With this notation the training objective can be conveniently written as $\ell(\theta) = \frac12 |u-y|_2^2$.</p>

<p>Using simple differentiation, one can obtain the dynamics of $u(t)$ as follows: (see <a href="https://arxiv.org/abs/1904.11955">our paper</a> for a proof)​

where $H(t)$ is an $n\times n$ positive semidefinite matrix whose $(i, j)$-th entry is $\left\langle \frac{\partial f(\theta(t), x_i)}{\partial\theta}, \frac{\partial f(\theta(t), x_j)}{\partial\theta} \right\rangle$.</p>

<p>Note that $H(t)$ is the <em>kernel matrix</em> of the following (time-varying) kernel $ker_t(\cdot,\cdot)$ evaluated on the training data:

In this kernel an input $x$ is mapped to a feature vector $\phi_t(x) = \frac{\partial f(\theta(t), x)}{\partial\theta}$ defined through the gradient of the network output with respect to the parameters at time $t$.</p>

<p>###The Large Width Limit</p>

<p>Up to this point we haven’t used the property that the neural network is very wide. The formula for the evolution of $u(t)$ is valid in general. In the large width limit, it turns out that the time-varying kernel $ker_t(\cdot,\cdot)$ is (with high probability) always close to a <em>deterministic</em> fixed kernel $ker_{\mathsf{NTK}}(\cdot,\cdot)$, which is the <strong>neural tangent kernel (NTK)</strong>. This property is proved in two steps, both requiring the large width assumption:</p>

<ol>
  <li>
    <p><strong>Step 1: Convergence to the NTK at random initialization.</strong> Suppose that the network parameters at initialization ($t=0$), $\theta(0)$, are i.i.d. Gaussian. Then under proper scaling, for any pair of inputs $x, x’$, it can be shown that the random variable $ker_0(x,x’)$, which depends on the random initialization $\theta(0)$, converges in probability to the deterministic value $ker_{\mathsf{NTK}}(x,x’)$, in the large width limit.</p>

    <p>(Technically speaking, there is a subtlety about how to define the large width limit. <a href="https://arxiv.org/pdf/1806.07572.pdf">Jacot et al. (2018)</a> gave a proof for the sequential limit where the width of every layer goes to infinity one by one. Later <a href="https://arxiv.org/abs/1902.04760">Yang (2019)</a> considered a setting where all widths go to infinity at the same rate. <a href="https://arxiv.org/abs/1904.11955">Our paper</a> improves them to the non-asymptotic setting, where we only require all layer widths to be larger than a finite threshold, which is the weakest notion of limit.)</p>
  </li>
  <li>
    <p><strong>Step 2: Stability of the kernel during training.</strong> Furthermore, the kernel <em>barely changes</em> during training, i.e., $ker_t(x,x’) \approx ker_0(x,x’)$ for all $t$. The reason behind this is that the weights do not move much during training, namely $\frac{|\theta(t) - \theta(0)|}{|\theta(0)|} \to 0$ as width $\to\infty$. Intuitively, when the network is sufficiently wide, each individual weight only needs to move a tiny amount in order to have a non-negligible change in the network output. This turns out to be true when the network is trained by gradient descent.</p>
  </li>
</ol>

<p>Combining the above two steps, we conclude that for any two inputs $x, x’$, with high probability we have

As we have seen, the dynamics of gradient descent is closely related to the time-varying kernel $ker_t(\cdot,\cdot)$. Now that we know that $ker_t(\cdot,\cdot)$ is essentially the same as the NTK, with a few more steps, we can eventually establish the equivalence between trained neural network and NTK: the final learned neural network at time $t=\infty$, denoted by $f_{\mathsf{NN}}(x) = f(\theta(\infty), x)$, is equivalent to the <em>kernel regression</em> solution with respect to the NTK. Namely, for any input $x$ we have

where $ker_{\mathsf{NTK}}(x, X) = (ker_{\mathsf{NTK}}(x, x_1), \ldots, ker_{\mathsf{NTK}}(x, x_n))^\top \in \mathbb{R}^n$, and $ker_{\mathsf{NTK}}(X, X) $ is an $n\times n$ matrix whose $(i, j)$-th entry is $ker_{\mathsf{NTK}}(x_i, x_j)$.</p>

<p>(In order to not have a bias term in the kernel regression solution we also assume that the network output at initialization is small: $f(\theta(0), x)\approx0$; this can be ensured by e.g. scaling down the initialization magnitude by a large constant, or replicating a network with opposite signs on the top layer at initialization.)</p>

<h2 id="how-well-do-infinitely-wide-neural-networks-perform-in-practice">How Well Do Infinitely Wide Neural Networks Perform in Practice?</h2>

<p>Having established this equivalence, we can now address the question of how well infinitely wide neural networks perform in practice — we can just evaluate the kernel regression predictors using the NTKs! We test NTKs on a standard image classification dataset, CIFAR-10. Note that for image datasets, one needs to use convolutional neural networks (CNNs) to achieve good performance. Therefore, we derive an extension of NTK, <em>convolutional neural tangent kernels (CNTKs)</em> and test their performance on CIFAR-10. In the table below, we report the classification accuracies of different CNNs and CNTKs:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/cntk_acc.jpeg" style="width: 700px;" />
<br />
</div>

<p>Here CNN-Vs are vanilla practically-wide CNNs (without pooling), and CNTK-Vs are their NTK counterparts. We also test CNNs with global average pooling (GAP), denotes above as CNN-GAPs, and their NTK counterparts, CNTK-GAPs. For all experiments, we turn off batch normalization, data augmentation, etc., and only use SGD to train CNNs (for CNTKs, we use the closed-form formula of kernel regression).</p>

<p>We find that CNTKs are actually very power kernels. The best kernel we find, 11-layer CNTK with GAP, achieves 77.43% classification accuracy on CIFAR-10. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher than methods reported by <a href="https://openreview.net/forum?id=B1g30j0qF7">Novak et al. (2019)</a>. The CNTKs also perform similarly to their CNN counterparts. This means that ultra-wide CNNs can achieve reasonable test performance on CIFAR-10.</p>

<p>It is also interesting to see that the global average pooling operation can significantly increase the classification accuracy for both CNNs and CNTKs. From this observation, we suspect that many techniques that improve the performance of neural networks are in some sense universal, i.e., these techniques might benefit kernel methods as well.</p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>

<p>Understanding the surprisingly good performance of over-parameterized deep neural networks is definitely a challenging theoretical question. Now, at least we have a better understanding of a class of ultra-wide neural networks: they are captured by neural tangent kernels! A hurdle that remains is that the classic generalization theory for kernels is still incapable of giving realistic bounds for generalization. But at least we now know that better understanding of kernels can lead to better understanding of deep nets.</p>

<p>Another fruitful direction is to “translate” different architectures/tricks of neural networks to kernels and to check their practical performance. We have found that global average pooling can significantly boost the performance of kernels, so we hope other tricks like batch normalization, dropout, max-pooling, etc. can also benefit kernels. Similarly, one can try to translate other architectures like recurrent neural networks, graph neural networks, and transformers, to kernels as well.</p>

<p>Our study also shows that there is a performance gap between infinitely wide networks and finite ones. How to explain this gap is an important theoretical question.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/10/03/NTK/"><span class="datestr">at October 03, 2019 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4476874159248982333">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2019/10/harvard-admissions-lawsuit-decision-out.html">Harvard Admissions Lawsuit Decision Out</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As someone who reads a significant number of court documents and decisions (I still do expert witness work), I can recommend for your reading pleasure the <a href="https://admissionscase.harvard.edu/files/adm-case/files/2019-10-30_dkt_672_findings_of_fact_and_conclusions_of_law.pdf">very recent decision on the Harvard admissions case</a>.  For those who want a sense of how Harvard admissions works, you will get a good summary of the information that came out during the trial.  For those who want to see a well-written court decision, in my opinion, this is a good example.  (Whether you agree with the decision or not, you should find the decision well written;  it lays out the issues and challenges in determining the decision clearly, and similarly explains the reasons for the ultimate conclusion clearly.)  And for those who care about the actual underlying issues of discrimination and affirmative action, I think the document provides a lot of food for thought, with a depth beyond what you'll see in the  news coverage.</div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2019/10/harvard-admissions-lawsuit-decision-out.html"><span class="datestr">at October 03, 2019 05:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4342">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4342">From quantum supremacy to classical fallacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Maybe I should hope that people <em>never</em> learn to distinguish for themselves which claimed breakthroughs in building new forms of computation are obviously serious, and which ones are obviously silly.  For as long as they don’t, this blog will always serve at least one purpose.  People will cite it, tweet it, invoke its “authority,” even while from my point of view, I’m offering nothing more intellectually special than my toddler does when he calls out “moo-moo cow! baa-baa sheep!” as we pass them on the road.</p>



<p>But that’s too pessimistic.  Sure, most readers <em>must</em> more-or-less already know what I’ll say about each thing: that <a href="https://www.scottaaronson.com/blog/?p=4317">Google’s quantum supremacy claim</a> is serious, that <a href="https://www.scottaaronson.com/blog/?p=2212">memcomputing to solve NP-complete problems</a> is not, etc.  Even so, I’ve heard from many readers that this blog was at least helpful for double-checking their initial impressions, and for making <a href="https://www.scottaaronson.com/blog/?p=2410">common knowledge</a> what before had merely been known to many.  I’m fine for it to continue serving those roles.</p>



<p>Last week, even as I dealt with fallout from Google’s quantum supremacy leak, I also got several people asking me to comment on a <em>Nature</em> paper entitled <a href="https://www-nature-com.ezproxy.lib.utexas.edu/articles/s41586-019-1557-9">Integer factorization using stochastic magnetic tunnel junctions</a> (warning: paywalled).  See also <a href="https://www.purdue.edu/newsroom/releases/2019/Q3/poor-mans-qubit-can-solve-quantum-problems-without-going-quantum.html">here</a> for a university press release.</p>



<p>The authors report building a new kind of computer based on asynchronously updated “p-bits” (probabilistic bits).  A p-bit is “a robust, classical entity fluctuating in time between 0 and 1, which interacts with other p-bits … using principles inspired by neural networks.”  They build a device with 8 p-bits, and use it to factor integers up to 945.  They present this as another “unconventional computation scheme” alongside quantum computing, and as a “potentially scalable hardware approach to the difficult problems of optimization and sampling.”</p>



<p>A <a href="https://www.nature.com/articles/d41586-019-02742-x">commentary accompanying the </a><em><a href="https://www.nature.com/articles/d41586-019-02742-x">Nature</a></em><a href="https://www.nature.com/articles/d41586-019-02742-x"> paper</a> goes much further still—claiming that the new factoring approach, “if improved, could threaten data encryption,” and that resources should now be diverted from quantum computing to this promising new idea, one with the advantages of requiring no refrigeration or maintenance of delicate entangled states.  (It should’ve added: and how big a number has Shor’s algorithm factored anyway, 21?  Compared to 945, that’s peanuts!)</p>



<p>Since I couldn’t figure out a gentler way to say this, here goes: it’s <strong>astounding</strong> that this paper and commentary made it into <em>Nature</em> in the form that they did.  Juxtaposing Google’s sampling achievement with p-bits, as several of my Facebook friends did last week, is juxtaposing the Wright brothers with some guy bouncing around on a pogo stick.</p>



<p>If you were looking forward to watching me dismantle the p-bit claims, I’m afraid you might be disappointed: the task is over almost the moment it begins.  <strong>“p-bit” devices can’t scalably outperform classical computers, for the simple reason that they <font color="red">are</font> classical computers.</strong>  A little unusual in their architecture, but still well-covered by the classical <a href="https://www.scottaaronson.com/talks/bernays2.ppt">Extended Church-Turing Thesis</a>.  Just like with the <a href="https://en.wikipedia.org/wiki/Adiabatic_quantum_computation">quantum adiabatic algorithm</a>, an energy penalty is applied to coax the p-bits into running a local optimization algorithm: that is, making random local moves that preferentially decrease the number of violated constraints.  Except here, because the whole evolution is classical, there doesn’t seem to be even the <em>pretense</em> that anything is happening that a laptop with a random-number generator couldn’t straightforwardly simulate.  In terms of <a href="https://www.nytimes.com/2019/10/02/opinion/impeachment-trump-nixon.html">this editorial</a>, if adiabatic quantum computing is Richard Nixon—hiding its lack of observed speedups behind subtle arguments about tunneling and spectral gaps—then p-bit computing is Trump.</p>



<p>Even so, I wouldn’t be writing this post if you opened the paper and it immediately said, in effect, “look, <em>we know</em>.  You’re thinking that this is just yet another stochastic local optimization method, which could clearly be simulated efficiently on a conventional computer, thereby putting it into a different conceptual universe from quantum computing.  You’re thinking that factoring an n-bit integer will self-evidently take exp(n) time by this method, as compared to exp(n<sup>1/3</sup>) for the <a href="https://en.wikipedia.org/wiki/General_number_field_sieve">Number Field Sieve</a>, and that no crypto is in even remote danger from this.  But here’s why you should still be interested in our p-bit model: because of other advantages X, Y, and Z.”  Alas, in vain one searches the whole paper, <em>and</em> the lengthy supplementary material, <em>and</em> the commentary, for any acknowledgment of the pachyderm in the pagoda.  Not an asymptotic runtime scaling in sight.  Quantum computing is there, but stripped of the theoretical framework that gives it its purpose.</p>



<p>That silence, in the pages of <em>Nature</em>—<em>that’s</em> the part that convinced me that, while on the negative side this blog seems to have accomplished nothing for the world in 14 years of existence, on the positive side it will likely have a role for decades to come.</p>



<p><strong>Update:</strong> See a <a href="https://www.scottaaronson.com/blog/?p=4342#comment-1820670">response in the comments</a>, which I appreciated, from Kerem Cansari (one of the authors of the paper), and <a href="https://www.scottaaronson.com/blog/?p=4342#comment-1820674">my response to the response</a>.</p>



<p><strong>(Partly) Unrelated Announcement #1:</strong> My new postdoc, <a href="https://andrearocchetto.github.io/">Andrea Rocchetto</a>, had the neat idea of compiling a <a href="https://quantumfactsheet.github.io/">Quantum Computing Fact Sheet</a>: a quick “Cliffs Notes” for journalists, policymakers, and others looking to get the basics right.  The fact sheet might grow in the future, but in the meantime, check it out!  Or at a more popular level, try the <a href="https://quantumatlas.umd.edu/">Quantum Atlas</a> made by folks at the University of Maryland.</p>



<p><strong>Unrelated Announcement #2:</strong> Daniel Wichs asked me to give a shout-out to a new <a href="https://itcrypto.github.io/">Conference on Information-Theoretic Cryptography</a>, to be held June 17-19 in Boston.</p>



<p><strong>Third Announcement:</strong> Several friends asked me to share that <a href="https://peterwittek.com/">Prof. Peter Wittek</a>, quantum computing researcher at the University of Toronto, has <a href="https://www.theglobeandmail.com/canada/article-renowned-ai-expert-university-of-toronto-prof-missing-after-avalanche/?fbclid=IwAR0FTnzQxRL79-oo43xjKaNEA7Oe1rA8A2yVjvhrgodxG1wJzhfhJZt9oJw">gone missing</a> in the Himalayas.  Needless to say we hope for his safe return.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4342"><span class="datestr">at October 03, 2019 03:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01099">Parameterized complexity of edge-coloured and signed graph homomorphism problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foucaud:Florent.html">Florent Foucaud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hocquard:Herv=eacute=.html">Hervé Hocquard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lajou:Dimitri.html">Dimitri Lajou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitsou:Valia.html">Valia Mitsou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pierron:Th=eacute=o.html">Théo Pierron</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01099">PDF</a><br /><b>Abstract: </b>We study the complexity of graph modification problems for homomorphism-based
properties of edge-coloured graphs. A homomorphism from an edge-coloured graph
$G$ to an edge-coloured graph $H$ is a vertex-mapping from $G$ to $H$ that
preserves adjacencies and edge-colours. We consider the property of having a
homomorphism to a fixed edge-coloured graph $H$. Given an edge-coloured graph
$G$, can we perform $k$ graph operations so that the resulting graph has a
homomorphism to $H$? The operations we consider are vertex-deletion,
edge-deletion and switching (an operation that permutes the colours of the
edges incident to a given vertex). Switching plays an important role in the
theory of signed graphs, that are $2$-edge-coloured graphs whose colours are
$+$ and $-$. We denote the corresponding problems (parameterized by $k$) by
VERTEX DELETION $H$-COLOURING, EDGE DELETION $H$-COLOURING and SWITCHING
$H$-COLOURING. These generalise $H$-COLOURING (where one has to decide if an
input graph admits a homomorphism to $H$). Our main focus is when $H$ has order
at most $2$, a case that includes standard problems such as VERTEX COVER, ODD
CYCLE TRANSVERSAL and EDGE BIPARTIZATION. For such a graph $H$, we give a
P/NP-complete complexity dichotomy for all three studied problems. Then, we
address their parameterized complexity. We show that all VERTEX DELETION
$H$-COLOURING and EDGE DELETION $H$-COLOURING problems for such $H$ are FPT.
This is in contrast with the fact that already for some $H$ of order~$3$,
unless P=NP, none of the three considered problems is in XP. We show that the
situation is different for SWITCHING $H$-COLOURING: there are three
$2$-edge-coloured graphs $H$ of order $2$ for which this is W-hard, and
assuming the ETH, admits no algorithm in time $f(k)n^{o(k)}$ for inputs of size
$n$. For the other cases, SWITCHING $H$-COLOURING is FPT.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01099"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01095">The Multivariate Schwartz-Zippel Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>M. Levent Doğan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erg=uuml=r:Alperen_A=.html">Alperen A. Ergür</a>, Jake D. Mundo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsigaridas:Elias.html">Elias Tsigaridas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01095">PDF</a><br /><b>Abstract: </b>We show that, except for a special family of polynomials -that we call
{\lambda}-reducible-, a natural generalization of
Schwartz-Zippel-DeMillo-Lipton lemma holds. Moreover, we develop a symbolic
algorithm to detect {\lambda}-reducibility. Our work is motivated by and has
applications in combinatorial geometry. Along the way we also present a
multivariate generalization of Combinatorial Nullstellensatz, which might be of
independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01095"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01082">Subexponential-time algorithms for finding large induced sparse subgraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Novotn=aacute=:Jana.html">Jana Novotná</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okrasa:Karolina.html">Karolina Okrasa</a>, Michał Pilipczuk, Paweł Rzążewski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leeuwen:Erik_Jan_van.html">Erik Jan van Leeuwen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walczak:Bartosz.html">Bartosz Walczak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01082">PDF</a><br /><b>Abstract: </b>Let $\mathcal{C}$ and $\mathcal{D}$ be hereditary graph classes. Consider the
following problem: given a graph $G\in\mathcal{D}$, find a largest, in terms of
the number of vertices, induced subgraph of $G$ that belongs to $\mathcal{C}$.
We prove that it can be solved in $2^{o(n)}$ time, where $n$ is the number of
vertices of $G$, if the following conditions are satisfied:
</p>
<p>* the graphs in $\mathcal{C}$ are sparse, i.e., they have linearly many edges
in terms of the number of vertices;
</p>
<p>* the graphs in $\mathcal{D}$ admit balanced separators of size governed by
their density, e.g., $\mathcal{O}(\Delta)$ or $\mathcal{O}(\sqrt{m})$, where
$\Delta$ and $m$ denote the maximum degree and the number of edges,
respectively; and
</p>
<p>* the considered problem admits a single-exponential fixed-parameter
algorithm when parameterized by the treewidth of the input graph.
</p>
<p>This leads, for example, to the following corollaries for specific classes
$\mathcal{C}$ and $\mathcal{D}$:
</p>
<p>* a largest induced forest in a $P_t$-free graph can be found in
$2^{\tilde{\mathcal{O}}(n^{2/3})}$ time, for every fixed $t$; and
</p>
<p>* a largest induced planar graph in a string graph can be found in
$2^{\tilde{\mathcal{O}}(n^{3/4})}$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01082"><span class="datestr">at October 03, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01073">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01073">Online Geometric Discrepancy for Stochastic Arrivals with Applications to Envy Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulkarni:Janardhan.html">Janardhan Kulkarni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01073">PDF</a><br /><b>Abstract: </b>Consider a unit interval $[0,1]$ in which $n$ points arrive one-by-one
independently and uniformly at random. On arrival of a point, the problem is to
immediately and irrevocably color it in $\{+1,-1\}$ while ensuring that every
interval $[a,b] \subseteq [0,1]$ is nearly-balanced. We define
\emph{discrepancy} as the largest imbalance of any interval during the entire
process. If all the arriving points were known upfront then we can color them
alternately to achieve a discrepancy of $1$. What is the minimum possible
expected discrepancy when we color the points online?
</p>
<p>We show that the discrepancy of the above problem is sub-polynomial in $n$
and that no algorithm can achieve a constant discrepancy. This is a substantial
improvement over the trivial random coloring that only gets an
$\widetilde{O}(\sqrt n)$ discrepancy. We then obtain similar results for a
natural generalization of this problem to $2$-dimensions where the points
arrive uniformly at random in a unit square. This generalization allows us to
improve recent results of Benade et al.\cite{BenadeKPP-EC18} for the online
envy minimization problem when the arrivals are stochastic.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01073"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01071">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01071">Computing the largest bond of a graph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Gabriel L. Duarte, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedrosa:Lehilton_L=_C=.html">Lehilton L. C. Pedrosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schouery:Rafael_C=_S=.html">Rafael C. S. Schouery</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Souza:U=eacute=verton_S=.html">Uéverton S. Souza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01071">PDF</a><br /><b>Abstract: </b>A bond of a graph $G$ is an inclusion-wise minimal disconnecting set of $G$,
i.e., bonds are cut-sets that determine cuts $[S,V\setminus S]$ of $G$ such
that $G[S]$ and $G[V\setminus S]$ are both connected. Given $s,t\in V(G)$, an
$st$-bond of $G$ is a bond whose removal disconnects $s$ and $t$. Contrasting
with the large number of studies related to maximum cuts, there are very few
results regarding the largest bond of general graphs. In this paper, we aim to
reduce this gap on the complexity of computing the largest bond and the largest
$st$-bond of a graph. Although cuts and bonds are similar, we remark that
computing the largest bond of a graph tends to be harder than computing its
maximum cut. We show that {\sc Largest Bond} remains NP-hard even for planar
bipartite graphs, and it does not admit a constant-factor approximation
algorithm, unless $P = NP$. We also show that {\sc Largest Bond} and {\sc
Largest $st$-Bond} on graphs of clique-width $w$ cannot be solved in time
$f(w)\times n^{o(w)}$ unless the Exponential Time Hypothesis fails, but they
can be solved in time $f(w)\times n^{O(w)}$. In addition, we show that both
problems are fixed-parameter tractable when parameterized by the size of the
solution, but they do not admit polynomial kernels unless NP $\subseteq$
coNP/poly.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01071"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.01047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.01047">TE-ETH: Lower Bounds for QBFs of Bounded Treewidth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fichte:Johannes_Klaus.html">Johannes Klaus Fichte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hecher:Markus.html">Markus Hecher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pfandler:Andreas.html">Andreas Pfandler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.01047">PDF</a><br /><b>Abstract: </b>The problem of deciding the validity (QSAT) of quantified Boolean formulas
(QBF) is a vivid research area in both theory and practice. In the field of
parameterized algorithmics, the well-studied graph measure treewidth turned out
to be a successful parameter. A well-known result by Chen in parameterized
complexity is that QSAT when parameterized by the treewidth of the primal graph
of the input formula together with the quantifier depth of the formula is
fixed-parameter tractable. More precisely, the runtime of such an algorithm is
polynomial in the formula size and exponential in the treewidth, where the
exponential function in the treewidth is a tower, whose height is the
quantifier depth. A natural question is whether one can significantly improve
these results and decrease the tower while assuming the Exponential Time
Hypothesis (ETH). In the last years, there has been a growing interest in the
quest of establishing lower bounds under ETH, showing mostly problem-specific
lower bounds up to the third level of the polynomial hierarchy. Still, an
important question is to settle this as general as possible and to cover the
whole polynomial hierarchy. In this work, we show lower bounds based on the ETH
for arbitrary QBFs parameterized by treewidth (and quantifier depth). More
formally, we establish lower bounds for QSAT and treewidth, namely, that under
ETH there cannot be an algorithm that solves QSAT of quantifier depth i in
runtime significantly better than i-fold exponential in the treewidth and
polynomial in the input size. In doing so, we provide a versatile reduction
technique to compress treewidth that encodes the essence of dynamic programming
on arbitrary tree decompositions. Further, we describe a general methodology
for a more fine-grained analysis of problems parameterized by treewidth that
are at higher levels of the polynomial hierarchy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.01047"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00994">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00994">Doubly-Efficient Pseudo-Deterministic Proofs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Michel Goemans, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldwasser:Shafi.html">Shafi Goldwasser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holden:Dhiraj.html">Dhiraj Holden</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00994">PDF</a><br /><b>Abstract: </b>In [20] Goldwasser, Grossman and Holden introduced pseudo-deterministic
interactive proofs for search problems where a powerful prover can convince a
probabilistic polynomial time verifier that a solution to a search problem is
canonical. They studied search problems for which polynomial time algorithms
are not known and for which many solutions are possible. They showed that
whereas there exists a constant round pseudo deterministic proof for graph
isomorphism where the canonical solution is the lexicographically smallest
isomorphism, the existence of pseudo-deterministic interactive proofs for
NP-hard problems would imply the collapse of the polynomial time hierarchy.
</p>
<p>In this paper, we turn our attention to studying doubly-efficient
pseudo-deterministic proofs for polynomial time search problems:
pseudo-deterministic proofs with the extra requirement that the prover runtime
is polynomial and the verifier runtime to verify that a solution is canonical
is significantly lower than the complexity of finding any solution, canonical
or otherwise. Naturally this question is particularly interesting for search
problems for which a lower bound on its worst case complexity is known or has
been widely conjectured.
</p>
<p>We show doubly-efficient pseudo-deterministic algorithms for a host of
natural problems whose complexity has long been conjectured. In particular:
</p>
<p>We show a doubly efficient pseudo-deterministic proof for linear programming
where the canonical solution which the prover will provide is the
lexicographically greatest optimal solution for the LP.
</p>
<p>We show a doubly efficient pseudo-deterministic proof for 3-SUM and problems
reducible to 3-SUM.
</p>
<p>We show a doubly-efficient pseudo-deterministic proof for the hitting set
problem.
</p>
<p>We show a doubly-efficient pseudo-deterministic proof for the Zero Weight
Triangle problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00994"><span class="datestr">at October 03, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00960">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00960">A Framework for Differential Calculus on Persistence Barcodes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leygonie:Jacob.html">Jacob Leygonie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oudot:Steve.html">Steve Oudot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tillmann:Ulrike.html">Ulrike Tillmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00960">PDF</a><br /><b>Abstract: </b>We define notions of differentiability for maps from and to the space of
persistence barcodes. Inspired by the theory of diffeological spaces, the
proposed framework uses lifts to the space of ordered barcodes, from which
derivatives can be computed. The two derived notions of differentiability
(respectively from and to the space of barcodes) combine together naturally to
produce a chain rule that enables the use of gradient descent for objective
functions factoring through the space of barcodes. We illustrate the
versatility of this framework by showing how it can be used to analyze the
smoothness of various parametrized families of filtrations arising in
topological data analysis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00960"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00941">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00941">A Self-contained Analysis of the Lempel-Ziv Compression Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sudan:Madhu.html">Madhu Sudan</a>, David Xiang <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00941">PDF</a><br /><b>Abstract: </b>This article gives a self-contained analysis of the performance of the
Lempel-Ziv compression algorithm on (hidden) Markovian sources. Specifically we
include a full proof of the assertion that the compression rate approaches the
entropy rate of the chain being compressed.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00941"><span class="datestr">at October 03, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00901">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00901">Sublinear Algorithms for Gap Edit Distance</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldenberg:Elazar.html">Elazar Goldenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saha:Barna.html">Barna Saha</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00901">PDF</a><br /><b>Abstract: </b>The edit distance is a way of quantifying how similar two strings are to one
another by counting the minimum number of character insertions, deletions, and
substitutions required to transform one string into the other. A simple dynamic
programming computes the edit distance between two strings of length $n$ in
$O(n^2)$ time, and a more sophisticated algorithm runs in time $O(n+t^2)$ when
the edit distance is $t$ [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit
of obtaining faster running time, the last couple of decades have seen a flurry
of research on approximating edit distance, including polylogarithmic
approximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010],
and a constant-factor approximation in subquadratic time [Chakrabarty, Das,
Goldenberg, Kouck\'y and Saks, FOCS 2018].
</p>
<p>We study sublinear-time algorithms for small edit distance, which was
investigated extensively because of its numerous applications. Our main result
is an algorithm for distinguishing whether the edit distance is at most $t$ or
at least $t^2$ (the quadratic gap problem) in time
$\tilde{O}(\frac{n}{t}+t^3)$. This time bound is sublinear roughly for all $t$
in $[\omega(1), o(n^{1/3})]$, which was not known before. The best previous
algorithms solve this problem in sublinear time only for $t=\omega(n^{1/3})$
[Andoni and Onak, STOC 2009].
</p>
<p>Our algorithm is based on a new approach that adaptively switches between
uniform sampling and reading contiguous blocks of the input strings. In
contrast, all previous algorithms choose which coordinates to query
non-adaptively. Moreover, it can be extended to solve the $t$ vs
$t^{2-\epsilon}$ gap problem in time $\tilde{O}(\frac{n}{t^{1-\epsilon}}+t^3)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00901"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00868">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00868">Advice Complexity of Adaptive Priority Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyar:Joan.html">Joan Boyar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kim_S=.html">Kim S. Larsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pankratov:Denis.html">Denis Pankratov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00868">PDF</a><br /><b>Abstract: </b>The priority model was introduced by Borodin, Rackoff and Nielsen to capture
"greedy-like" algorithms. Motivated, in part, by the success of advice
complexity in the area of online algorithm, recently Borodin et al. have
extended the fixed priority model to include an advice tape oracle. They also
developed a reduction-based framework for proving lower bounds against this
rather powerful model. In this paper, we extend the advice tape model further
to the arguably more useful adaptive priority algorithms. We show how to modify
the reduction-based framework in order for it to apply against the more
powerful adaptive priority algorithms. In the process, we manage to simplify
the proof that the framework works, and we strengthen all the lower bounds by a
factor of 2. As a motivation of an adaptive priority model with advice, we
present a purely combinatorial adaptive priority algorithm with advice for the
minimum vertex cover problem on graphs of maximum degree 3. Our algorithm
achieves optimality and uses at most 15n/46 bits of advice. This advice is
provably shorter than what can be achieved by online algorithms with advice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00868"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00831">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00831">On the Hardness of Set Disjointness and Set Intersection with Bounded Universe</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldstein:Isaac.html">Isaac Goldstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lewenstein:Moshe.html">Moshe Lewenstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00831">PDF</a><br /><b>Abstract: </b>In the SetDisjointness problem, a collection of $m$ sets $S_1,S_2,...,S_m$
from some universe $U$ is preprocessed in order to answer queries on the
emptiness of the intersection of some two query sets from the collection. In
the SetIntersection variant, all the elements in the intersection of the query
sets are required to be reported. These are two fundamental problems that were
considered in several papers from both the upper bound and lower bound
perspective.
</p>
<p>Several conditional lower bounds for these problems were proven for the
tradeoff between preprocessing and query time or the tradeoff between space and
query time. Moreover, there are several unconditional hardness results for
these problems in some specific computational models. The fundamental nature of
the SetDisjointness and SetIntersection problems makes them useful for proving
the conditional hardness of other problems from various areas. However, the
universe of the elements in the sets may be very large, which may cause the
reduction to some other problems to be inefficient and therefore it is not
useful for proving their conditional hardness.
</p>
<p>In this paper, we prove the conditional hardness of SetDisjointness and
SetIntersection with bounded universe. This conditional hardness is shown for
both the interplay between preprocessing and query time and the interplay
between space and query time. Moreover, we present several applications of
these new conditional lower bounds. These applications demonstrates the
strength of our new conditional lower bounds as they exploit the limited
universe size. We believe that this new framework of conditional lower bounds
with bounded universe can be useful for further significant applications.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00831"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00788">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00788">Streaming Balanced Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Peilin.html">Peilin Zhong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00788">PDF</a><br /><b>Abstract: </b>Clustering of data points in metric space is among the most fundamental
problems in computer science with plenty of applications in data mining,
information retrieval and machine learning. Due to the necessity of clustering
of large datasets, several streaming algorithms have been developed for
different variants of clustering problems such as $k$-median and $k$-means
problems. However, despite the importance of the context, the current
understanding of balanced clustering (or more generally capacitated clustering)
in the streaming setting is very limited. The only previously known streaming
approximation algorithm for capacitated clustering requires three passes and
only handles insertions.
</p>
<p>In this work, we develop \emph{the first single pass streaming algorithm} for
a general class of clustering problems that includes capacitated $k$-median and
capacitated $k$-means in Euclidean space, using only poly$( k d \log \Delta)$
space, where $k$ is the number of clusters, $d$ is the dimension and $\Delta$
is the maximum relative range of a coordinate. (Note that $d\log \Delta$ is the
space required to represent one point.) This algorithm only violates the
capacity constraint by a $1+\epsilon$ factor. Interestingly, unlike the
previous algorithm, our algorithm handles both insertions and deletions of
points. To provide this result we define a decomposition of the space via some
curved half-spaces. We used this decomposition to design a strong coreset of
size poly$( k d \log \Delta)$ for balanced clustering. Then, we show that this
coreset is implementable in the streaming and distributed settings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00788"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00777">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00777">Prime Clocks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fiske:Michael_Stephen.html">Michael Stephen Fiske</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00777">PDF</a><br /><b>Abstract: </b>Physical implementations of digital computers began in the latter half of the
1930's and were first constructed from various forms of logic gates. Based on
the prime numbers, we introduce prime clocks and prime clock sums, where the
clocks utilize time and act as computational primitives instead of gates. The
prime clocks generate an infinite abelian group, where for each n, there is a
finite subgroup S such that for each Boolean function f : {0, 1}^n --&gt; {0, 1},
there exists a finite prime clock sum in S that can represent and compute f. A
parallelizable algorithm, implemented with a finite prime clock sum, is
provided that computes f. In contrast, the negation, conjunction, and
disjunction operations generate a Boolean algebra. In terms of computation,
Boolean circuits computed with logic gates NOT, AND, OR have a depth. This
means that a completely parallel computation of Boolean functions is not
possible with these gates. Overall, some new connections between number theory,
Boolean functions and computation are established.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00777"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00773">Approximating the Geometric Edit Distance</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fox:Kyle.html">Kyle Fox</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xinyi.html">Xinyi Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00773">PDF</a><br /><b>Abstract: </b>Edit distance is a measurement of similarity between two sequences such as
strings, point sequences, or polygonal curves. Many matching problems from a
variety of areas, such as signal analysis, bioinformatics, etc., need to be
solved in a geometric space. Therefore, the geometric edit distance (GED) has
been studied. In this paper, we describe the first strictly sublinear
approximate near-linear time algorithm for computing the GED of two point
sequences in constant dimensional Euclidean space. Specifically, we present a
randomized (O(n\log^2n)) time (O(\sqrt n))-approximation algorithm. Then, we
generalize our result to give a randomized $\alpha$-approximation algorithm for
any $\alpha\in [1, \sqrt{n}]$, running in time $\tilde{O}(n^2/\alpha^2)$. Both
algorithms are Monte Carlo and return approximately optimal solutions with high
probability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00773"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00724">A Pre-defined Sparse Kernel Based Convolutionfor Deep CNNs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kundu:Souvik.html">Souvik Kundu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prakash:Saurav.html">Saurav Prakash</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akrami:Haleh.html">Haleh Akrami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beerel:Peter_A=.html">Peter A. Beerel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chugg:Keith_M=.html">Keith M. Chugg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00724">PDF</a><br /><b>Abstract: </b>The high demand for computational and storage resources severely impede the
deployment of deep convolutional neural networks (CNNs) in limited-resource
devices. Recent CNN architectures have proposed reduced complexity versions
(e.g. SuffleNet and MobileNet) but at the cost of modest decreases inaccuracy.
This paper proposes pSConv, a pre-defined sparse 2D kernel-based convolution,
which promises significant improvements in the trade-off between complexity and
accuracy for both CNN training and inference. To explore the potential of this
approach, we have experimented with two widely accepted datasets, CIFAR-10 and
Tiny ImageNet, in sparse variants of both the ResNet18 and VGG16 architectures.
Our approach shows a parameter count reduction of up to 4.24x with modest
degradation in classification accuracy relative to that of standard CNNs. Our
approach outperforms a popular variant of ShuffleNet using a variant of
ResNet18 with pSConv having 3x3 kernels with only four of nine elements not
fixed at zero. In particular, the parameter count is reduced by 1.7x for
CIFAR-10 and 2.29x for Tiny ImageNet with an increased accuracy of ~4%.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00724"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1910.00704">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1910.00704">Spherical k-Nearest Neighbors Interpolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Philippe Trempe <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00704">PDF</a><br /><b>Abstract: </b>Geospatial interpolation is a challenging task due to real world data often
being sparse, heterogeneous and inconsistent. For that matter, this work
presents SkNNI, a spherical interpolation algorithm capable of working with
such challenging geospatial data. This work also presents NDDNISD an accurate
and efficient interpolation function for SkNNI which shines due to its spatial
awareness in terms of proximity and distribution of observation neighbors.
SkNNI's open source implementation is also discussed and illustrated with a
simple usage example.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1910.00704"><span class="datestr">at October 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
