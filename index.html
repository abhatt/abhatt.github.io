<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at September 23, 2020 08:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10709">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10709">Fast Black-Box Quantum State Preparation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bausch:Johannes.html">Johannes Bausch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10709">PDF</a><br /><b>Abstract: </b>Quantum state preparation is an important ingredient for other higher-level
quantum algorithms, such as Hamiltonian simulation, or for loading
distributions into a quantum device to be used e.g. in the context of
optimization tasks such as machine learning. Starting with a generic "black
box" method devised by Grover in 2000, which employs amplitude amplification to
load coefficients calculated by an oracle, there has been a long series of
results and improvements with various additional conditions on the amplitudes
to be loaded, culminating in Sanders et al.'s work which avoids almost all
arithmetic during the preparation stage. In this work, we improve upon this
routine in two aspects: we reduce the required qubit overhead from $g$ to
$\log_2(g)$ in the bit precision $g$ (at a cost of slightly increasing the
count of non-Clifford operations), and show how various sets of $N$
coefficients can be loaded significantly faster than in $O(\sqrt N)$ rounds of
amplitude amplification - up to only $O(1)$ many - by bootstrapping the
procedure with an optimised initial state.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10709"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10677">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10677">On the Mysteries of MAX NAE-SAT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Neng.html">Neng Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potechin:Aaron.html">Aaron Potechin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zwick:Uri.html">Uri Zwick</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10677">PDF</a><br /><b>Abstract: </b>MAX NAE-SAT is a natural optimization problem, closely related to its
better-known relative MAX SAT. The approximability status of MAX NAE-SAT is
almost completely understood if all clauses have the same size $k$, for some
$k\ge 2$. We refer to this problem as MAX NAE-$\{k\}$-SAT. For $k=2$, it is
essentially the celebrated MAX CUT problem. For $k=3$, it is related to the MAX
CUT problem in graphs that can be fractionally covered by triangles. For $k\ge
4$, it is known that an approximation ratio of $1-\frac{1}{2^{k-1}}$, obtained
by choosing a random assignment, is optimal, assuming $P\ne NP$. For every
$k\ge 2$, an approximation ratio of at least $\frac{7}{8}$ can be obtained for
MAX NAE-$\{k\}$-SAT. There was some hope, therefore, that there is also a
$\frac{7}{8}$-approximation algorithm for MAX NAE-SAT, where clauses of all
sizes are allowed simultaneously.
</p>
<p>Our main result is that there is no $\frac{7}{8}$-approximation algorithm for
MAX NAE-SAT, assuming the unique games conjecture (UGC). In fact, even for
almost satisfiable instances of MAX NAE-$\{3,5\}$-SAT (i.e., MAX NAE-SAT where
all clauses have size $3$ or $5$), the best approximation ratio that can be
achieved, assuming UGC, is at most $\frac{3(\sqrt{21}-4)}{2}\approx 0.8739$.
Using calculus of variations, we extend the analysis of O'Donnell and Wu for
MAX CUT to MAX NAE-$\{3\}$-SAT. We obtain an optimal algorithm, assuming UGC,
for MAX NAE-$\{3\}$-SAT, slightly improving on previous algorithms. The
approximation ratio of the new algorithm is $\approx 0.9089$.
</p>
<p>We complement our theoretical results with some experimental results. We
describe an approximation algorithm for almost satisfiable instances of MAX
NAE-$\{3,5\}$-SAT with a conjectured approximation ratio of 0.8728, and an
approximation algorithm for almost satisfiable instances of MAX NAE-SAT with a
conjectured approximation ratio of 0.8698.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10677"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10502">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10502">Computing $L(p,1)$-Labeling with Combined Parameters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, Kazuma Kawai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ono:Hirotaka.html">Hirotaka Ono</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10502">PDF</a><br /><b>Abstract: </b>Given a graph, an $L(p,1)$-labeling of the graph is an assignment $f$ from
the vertex set to the set of nonnegative integers such that for any pair of
vertices $(u,v),|f (u) - f (v)| \ge p$ if $u$ and $v$ are adjacent, and $f(u)
\neq f(v)$ if $u$ and $v$ are at distance $2$. The \textsc{$L(p,1)$-labeling}
problem is to minimize the span of $f$ (i.e.,$\max_{u\in V}(f(u)) - \min_{u\in
V}(f(u))+1$). It is known to be NP-hard even for graphs of maximum degree $3$
or graphs with tree-width 2, whereas it is fixed-parameter tractable with
respect to vertex cover number. Since vertex cover number is a kind of the
strongest parameter, there is a large gap between tractability and
intractability from the viewpoint of parameterization. To fill up the gap, in
this paper, we propose new fixed-parameter algorithms for
\textsc{$L(p,1)$-Labeling} by the twin cover number plus the maximum clique
size and by the tree-width plus the maximum degree. These algorithms reduce the
gap in terms of several combinations of parameters.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10502"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10408">Control dynamics using quantum memory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roget:Mathieu.html">Mathieu Roget</a>, Basile Herzog, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molfetta:Giuseppe_Di.html">Giuseppe Di Molfetta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10408">PDF</a><br /><b>Abstract: </b>We propose a new quantum numerical scheme to control the dynamics of a
quantum walker in a two dimensional space-time grid. More specifically, we show
how, introducing a quantum memory for each of the spatial grid, this result can
be achieved simply by acting on the initial state of the whole system, and
therefore can be exactly controlled once for all. As example we prove
analytically how to encode in the initial state any arbitrary walker's mean
trajectory and variance. This brings significantly closer the possibility of
implementing dynamically interesting physics models on medium term quantum
devices, and introduces a new direction in simulating aspects of quantum field
theories (QFTs), notably on curved manifold.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10408"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10353">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10353">Discriminating Codes in Geometric Setups</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Sanjana.html">Sanjana Dey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foucaud:Florent.html">Florent Foucaud</a>, Subhas C Nandy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sen:Arunabha.html">Arunabha Sen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10353">PDF</a><br /><b>Abstract: </b>We study two geometric variations of the discriminating code problem. In the
\emph{discrete version}, a finite set of points $P$ and a finite set of objects
$S$ are given in $\mathbb{R}^d$. The objective is to choose a subset $S^*
\subseteq S$ of minimum cardinality such that the subsets $S_i^* \subseteq S^*$
covering $p_i$, satisfy $S_i^*\neq \emptyset$ for each $i=1,2,\ldots, n$, and
$S_i^* \neq S_j^*$ for each pair $(i,j)$, $i \neq j$. In the \emph{continuous
version}, the solution set $S^*$ can be chosen freely among a (potentially
infinite) class of allowed geometric objects.
</p>
<p>In the 1-dimensional case ($d=1$), the points are placed on some fixed-line
$L$, and the objects in $S$ and $S^*$ are finite sub-segments of $L$ (called
intervals). We show that the discrete version of this problem is NP-complete.
This is somewhat surprising as the continuous version is known to be
polynomial-time solvable. This is also in contrast with most geometric covering
problems, which are usually polynomial-time solvable in 1D. We then design a
polynomial-time $2$-approximation algorithm for the 1-dimensional discrete
case. We also design a PTAS for both discrete and continuous cases when the
intervals are all required to have the same length.
</p>
<p>We then study the 2-dimensional case ($d=2$) for axis-parallel unit square
objects. We show that both continuous and discrete versions are NP-hard, and
design polynomial-time approximation algorithms with factors $4+\epsilon$ and
$32+\epsilon$, respectively (for every fixed $\epsilon&gt;0$).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10353"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10255">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10255">A Low-Level Index for Distributed Logic Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Thomas Prokosch <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10255">PDF</a><br /><b>Abstract: </b>A distributed logic programming language with support for meta-programming
and stream processing offers a variety of interesting research problems, such
as: How can a versatile and stable data structure for the indexing of a large
number of expressions be implemented with simple low-level data structures? Can
low-level programming help to reduce the number of occur checks in Robinson's
unification algorithm? This article gives the answers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10255"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10217">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10217">A Faster Interior Point Method for Semidefinite Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kathuria:Tarun.html">Tarun Kathuria</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Padmanabhan:Swati.html">Swati Padmanabhan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10217">PDF</a><br /><b>Abstract: </b>Semidefinite programs (SDPs) are a fundamental class of optimization problems
with important recent applications in approximation algorithms, quantum
complexity, robust learning, algorithmic rounding, and adversarial deep
learning. This paper presents a faster interior point method to solve generic
SDPs with variable size $n \times n$ and $m$ constraints in time \begin{align*}
\widetilde{O}(\sqrt{n}( mn^2 + m^\omega + n^\omega) \log(1 / \epsilon) ),
\end{align*} where $\omega$ is the exponent of matrix multiplication and
$\epsilon$ is the relative accuracy. In the predominant case of $m \geq n$, our
runtime outperforms that of the previous fastest SDP solver, which is based on
the cutting plane method of Jiang, Lee, Song, and Wong [JLSW20].
</p>
<p>Our algorithm's runtime can be naturally interpreted as follows:
$\widetilde{O}(\sqrt{n} \log (1/\epsilon))$ is the number of iterations needed
for our interior point method, $mn^2$ is the input size, and $m^\omega +
n^\omega$ is the time to invert the Hessian and slack matrix in each iteration.
These constitute natural barriers to further improving the runtime of interior
point methods for solving generic SDPs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10217"><span class="datestr">at September 23, 2020 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10160">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10160">On rooted $k$-connectivity problems in quasi-bipartite digraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nutov:Zeev.html">Zeev Nutov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10160">PDF</a><br /><b>Abstract: </b>We consider the directed Rooted Subset $k$-Edge-Connectivity problem: given a
set $T \subseteq V$ of terminals in a digraph $G=(V+r,E)$ with edge costs and
an integer $k$, find a min-cost subgraph of $G$ that contains $k$ edge disjoint
$rt$-paths for all $t \in T$. The case when every edge of positive cost has
head in $T$ admits a polynomial time algorithm due to Frank, and the case when
all positive cost edges are incident to $r$ is equivalent to the $k$-Multicover
problem. Recently, [Chan et al. APPROX20] obtained ratio $O(\ln k \ln |T|)$ for
quasi-bipartite instances, when every edge in $G$ has an end in $T+r$. We give
a simple proof for the same ratio for a more general problem of covering an
arbitrary $T$-intersecting supermodular set function by a minimum cost edge
set, and for the case when only every positive cost edge has an end in $T+r$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10160"><span class="datestr">at September 23, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.10088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.10088">On the Theory of Modern Quantum Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jacob Biamonte <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.10088">PDF</a><br /><b>Abstract: </b>This dissertation unites variational computation with results and techniques
appearing in the theory of ground state computation. It should be readable by
graduate students.
</p>
<p>The topics covered include: Ising model reductions, stochastic versus quantum
processes on graphs, quantum gates and circuits as tensor networks, variational
quantum algorithms and Hamiltonian gadgets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.10088"><span class="datestr">at September 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17612">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/09/22/puzzle-reviews-by-a-puzzle-writer/">Puzzle Reviews by a Puzzle Writer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Not puzzling reviews</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/09/jason-1.jpeg"><img width="140" alt="" src="https://rjlipton.files.wordpress.com/2020/09/jason-1.jpeg?w=140&amp;h=160" class="alignright wp-image-17615" height="160" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Princeton University Press <a href="https://press.princeton.edu/our-authors/rosenhouse-jason">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Jason Rosenhouse is professor in the Department of Mathematics at James Madison University. His research focuses on algebraic graph theory and analytic number theory involving exponential sums.  The former includes a neat <a href="https://www.researchgate.net/publication/220620901_Expansion_Properties_Of_Levi_Graphs">paper</a> on expansion properties of a family of graphs associated to block designs, with two undergraduates among its authors.  But besides his “real” research, he has written a number of books on puzzles such as <i><a href="https://www.amazon.com/s?k=Jason+Rosenhouse&amp;i=stripbooks&amp;ref=nb_sb_noss_2">The Monty Hall Problem</a>: The Remarkable Story of Math’s Most Contentious Brain Teaser</i>. Soon his book <i><a href="https://www.amazon.co.uk/Games-Your-Mind-History-Puzzles/dp/0691174075">Games for Your Mind</a>: The History and Future of Logic Puzzles</i> is to be published.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/09/revbook-1.png"><img width="103" alt="" src="https://rjlipton.files.wordpress.com/2020/09/revbook-1.png?w=103&amp;h=150" class="aligncenter size-thumbnail wp-image-17626" height="150" /></a></p>
<p>
Today Ken and I thought we would highlight his recent review of a book on math puzzles.</p>
<p>
I have mixed feelings about puzzles. I like them, and am happy when I can understand their solution. I am even happier when I can solve them. I sometimes feel that I should spend my limited brain cycles on “real” problems. But puzzles are fun. </p>
<p>
Rosenhouse’s <a href="https://www.ams.org/journals/notices/202009/rnoti-p1382.pdf">review</a> is in the recent <em>Notices of the AMS</em> on the book <i><a href="https://bookstore.ams.org/prb-36">Bicycles or Unicycles</a>: A Collection of Intriguing Mathematical Puzzles</i>. This book, the “Bicycle Book,” is authored by Daniel Velleman and Stan Wagon.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/09/maabook.jpg"><img width="101" alt="" src="https://rjlipton.files.wordpress.com/2020/09/maabook.jpg?w=101&amp;h=145" class="aligncenter wp-image-17617" height="145" /></a></p>
<p>
Their book is a collection of <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+5+%5Ctimes+7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 \times 5 \times 7}" class="latex" title="{3 \times 5 \times 7}" /> mathematical puzzles. Rosenhouse likes their book, which means a lot coming from an author of so many puzzle books himself. </p>
<p>
</p><p></p><h2> A Cool Problem </h2><p></p>
<p></p><p>
Rosenhouse presents this problem from the Bicycle Book. </p>
<blockquote><p><b> </b> <em> You are playing solitaire in the first quadrant of the Cartesian plane, the lower corner of which is shown in Figure 1. You begin with a single checker on square a1. On each turn, a legal move consists of removing one checker from the board and then placing two new checkers in the cells immediately above and to the right of the original checker. If either of those two cells is occupied, then the move is illegal, and a different checker must be selected for removal. </em>
</p></blockquote>
<p></p><p>
<a href="https://rjlipton.files.wordpress.com/2020/09/solitairepuzzle.png"><img src="https://rjlipton.files.wordpress.com/2020/09/solitairepuzzle.png?w=600" alt="" class="aligncenter size-full wp-image-17618" /></a></p>
<p>
Show that you can never make all of the <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 \times 3}" class="latex" title="{3 \times 3}" /> lower-left squares empty. This is a complexity question. You describe a computation and assert that certain states cannot be reached. The challenge is two-fold: </p>
<ol>
<li>
The computation is nondeterministic. There can be more than one next state. <p></p>
</li><li>
The computation can reach infinitely many states. The task is to prove that no reachable state has the lower nine squares empty.
</li></ol>
<p>
</p><p></p><h2> A Cool Solution </h2><p></p>
<p></p><p>
I must admit I read the solution before I tried to solve the puzzle. I did find an alternative solution. It was not as clever as the one from the book. Let’s look at that solution first. </p>
<p>
The idea is to assign <i>magic</i> values to each square on the checkerboard. The value of a state is the sum over all the values of squares with a checker. We need these to hold: </p>
<ol>
<li>
The value of the initial square is <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. <p></p>
</li><li>
The value of a move leaves the total sum over all the checkers the same. <p></p>
</li><li>
The value of the squares <b>not</b> in the lower <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 \times 3}" class="latex" title="{3 \times 3}" /> is less than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />.
</li></ol>
<p>Then there can never be a reachable state that avoids all the lower <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 \times 3}" class="latex" title="{3 \times 3}" />. How can we do this? Assign the values as shown below. </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bccccl%7D+%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5C%5C+1%2F8+%26+1%2F16+%26+1%2F32+%26+1%2F64+%26+%5Ccdots%5C%5C+1%2F4+%26+1%2F8+%26+1%2F16+%26+1%2F32+%26+%5Ccdots%5C%5C+1%2F2+%26+1%2F4+%26+1%2F8+%26+1%2F16+%26+%5Ccdots%5C%5C+1+%26+1%2F2+%26+1%2F4+%26+1%2F8+%26+%5Ccdots+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{ccccl} \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\ 1/8 &amp; 1/16 &amp; 1/32 &amp; 1/64 &amp; \cdots\\ 1/4 &amp; 1/8 &amp; 1/16 &amp; 1/32 &amp; \cdots\\ 1/2 &amp; 1/4 &amp; 1/8 &amp; 1/16 &amp; \cdots\\ 1 &amp; 1/2 &amp; 1/4 &amp; 1/8 &amp; \cdots \end{array} " class="latex" title="\displaystyle  \begin{array}{ccccl} \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\ 1/8 &amp; 1/16 &amp; 1/32 &amp; 1/64 &amp; \cdots\\ 1/4 &amp; 1/8 &amp; 1/16 &amp; 1/32 &amp; \cdots\\ 1/2 &amp; 1/4 &amp; 1/8 &amp; 1/16 &amp; \cdots\\ 1 &amp; 1/2 &amp; 1/4 &amp; 1/8 &amp; \cdots \end{array} " /></p>
<p>
Ken remembers, as a teenager, seeing this puzzle in a collection by the master Martin Gardner, with the same proof. Ken thought of it again when considering problems in physics and combinatorics that involve defining an appropriate potential function as the first step. </p>
<p>
</p><p></p><h2> An Uncool Solution </h2><p></p>
<p></p><p>
Let <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> be the lower-right <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 \times 3}" class="latex" title="{3 \times 3}" /> corner board. Label the positions as usual with <img src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(i,j)}" class="latex" title="{(i,j)}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j}" class="latex" title="{i,j}" /> both are in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,2,3\}}" class="latex" title="{\{1,2,3\}}" />.</p>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t)}" class="latex" title="{N(t)}" /> be the number of checkers in <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. Of course <img src="https://s0.wp.com/latex.php?latex=%7BN%280%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(0)=1}" class="latex" title="{N(0)=1}" /> and the checker is at <img src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1,1)}" class="latex" title="{(1,1)}" />.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v1.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/09/config33v1.jpg?w=150&amp;h=107" class="aligncenter wp-image-17621" height="107" /></a></p>
<p>
Suppose by way of contradiction that it is possible to make <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> empty. </p>
<p>
Our proof uses that the transition from <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t)&gt;0}" class="latex" title="{N(t)&gt;0}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%2B1%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t+1)=0}" class="latex" title="{N(t+1)=0}" /> requires that <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t)=1}" class="latex" title="{N(t)=1}" />. That is <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t)=2}" class="latex" title="{N(t)=2}" /> or even <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> is impossible. The rule cannot remove two or more checkers from <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> in one move. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v2.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/09/config33v2.jpg?w=150&amp;h=97" class="aligncenter wp-image-17622" height="97" /></a></p>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t)=1}" class="latex" title="{N(t)=1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BN%28t%2B1%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N(t+1)=0}" class="latex" title="{N(t+1)=0}" />. So where is the checker? A simple case analysis shows it must be at <img src="https://s0.wp.com/latex.php?latex=%7B%283%2C3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3,3)}" class="latex" title="{(3,3)}" />. So now we know the last placement. But how did we get to this position? It is easy to see that it had to be previously at <img src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3,2)}" class="latex" title="{(3,2)}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%282%2C3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(2,3)}" class="latex" title="{(2,3)}" />. By symmetry we can assume was <img src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3,2)}" class="latex" title="{(3,2)}" />. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v3.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/09/config33v3.jpg?w=150&amp;h=98" class="aligncenter wp-image-17623" height="98" /></a></p>
<p>
Our goal to show that we cannot place one checker at <img src="https://s0.wp.com/latex.php?latex=%7B%283%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3,2)}" class="latex" title="{(3,2)}" /> and no other in <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. A little analysis shows that it must be the case that the previous state was one checker at <img src="https://s0.wp.com/latex.php?latex=%7B%283%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(3,1)}" class="latex" title="{(3,1)}" />. But it is impossible to place a checker there and avoid having more checkers. This yields a contradiction. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/09/config33v4.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/09/config33v4.jpg?w=150&amp;h=97" class="aligncenter wp-image-17624" height="97" /></a></p>
<p></p><h2> Another Solution </h2><p></p>
<p></p><p>
We could use finite state automata theory to supply another solution. The obvious issue is the full game is played on an infinite checkerboard. But we can use a standard trick to reduce the state space to a finite one. Imagine we play the game on just <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. When we have a move that creates checkers outside of <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> just throw them away. It is simple to see that no move can place checkers inside <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. Thus if we cannot empty <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> in this finite version, then there is no way in the full game. </p>
<p>
Now the state space is bounded by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B9%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{9}}" class="latex" title="{2^{9}}" />: each of the nine squares can have a checker or not. We know the initial state and we know the final state. So we can run a finite state search algorithm and decide the answer.</p>
<p>
The value of this solution is that it could handle more complex rules and larger squares. Well at least those within reason. </p>
<p>
</p><p></p><h2> Other Puzzles </h2><p></p>
<p></p><p>
Rosenhouse covers nine other puzzles in his review. In our meta review of his review we will cover just two more. </p>
<p>
The third puzzle in his review comes from the challenge to prove that each matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BC_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{C_n\}}" class="latex" title="{\{C_n\}}" /> has determinant <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. This is a puzzle because the matrices <img src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_{n}}" class="latex" title="{C_{n}}" /> look like they could have some strange determinant, one that even varies with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. The trick is to show that there are other families <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BA_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{A_n\}}" class="latex" title="{\{A_n\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BB_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{B_n\}}" class="latex" title="{\{B_n\}}" /> of matrices, in which each matrix has determinant <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C_n+%3D+A_n+B_n.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C_n = A_n B_n. " class="latex" title="\displaystyle  C_n = A_n B_n. " /></p>
<p>Of course this immediately proves that <img src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_{n}}" class="latex" title="{C_{n}}" /> also have determinant <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. The challenge is kind of a factorization problem. </p>
<p>
Another puzzle is to prove that a number <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> is prime if and only if there is exactly one pair of positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bm%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m,n}" class="latex" title="{m,n}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1%7D%7Bm%7D+-+%5Cfrac%7B1%7D%7Bn%7D+%3D+%5Cfrac%7B1%7D%7Bp%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{1}{m} - \frac{1}{n} = \frac{1}{p}. " class="latex" title="\displaystyle  \frac{1}{m} - \frac{1}{n} = \frac{1}{p}. " /></p>
<p>This seems to be surprising in two ways: First who could think of this? Second who could think of this? Okay it should be why is it true? Indeed Rosenhouse says that the proof is complex. </p>
<p>
Rosenhouse adds that most puzzles in this book are less “bite-sized” than the ones typically posed by the master Gardner. This certainly goes for the title puzzle about whether a bicycle can possibly move along a curve—other than a straight line—that was made by a unicycle. It requires a foray into differential equations.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
My “uncool solution” was left somewhat incomplete. Do you see how to complete the analysis?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/09/22/puzzle-reviews-by-a-puzzle-writer/"><span class="datestr">at September 22, 2020 10:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/144">TR20-144 |  Toward Probabilistic Checking against Non-Signaling Strategies with Constant Locality | 

	Mohammad Jahanara, 

	Sajin Koroth, 

	Igor Shinkar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Non-signaling strategies are a generalization of quantum strategies that have been studied in physics over the past three decades. Recently, they have found applications in theoretical computer science, including to proving inapproximability results for linear programming and to constructing protocols for delegating computation. A central tool for these applications is probabilistically checkable proof (PCPs) systems that are sound against non-signaling strategies.

In this paper we show, assuming a certain geometrical hypothesis about noise robustness of non-signaling proofs (or, equivalently, about robustness to noise of solutions to the Sherali-Adams linear program), that a slight variant of the parallel repetition of the exponential-length constant-query PCP construction due to Arora et al. (JACM 1998) is sound against non-signaling strategies with constant locality.

Our proof relies on the analysis of the linearity test and agreement test (also known as the direct product test) in the non-signaling setting.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/144"><span class="datestr">at September 22, 2020 11:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09743">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09743">Improving on Best-of-Many-Christofides for $T$-tours</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traub:Vera.html">Vera Traub</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09743">PDF</a><br /><b>Abstract: </b>The $T$-tour problem is a natural generalization of TSP and Path TSP. Given a
graph $G=(V,E)$, edge cost $c: E \to \mathbb{R}_{\ge 0}$, and an even
cardinality set $T\subseteq V$, we want to compute a minimum-cost $T$-join
connecting all vertices of $G$ (and possibly containing parallel edges).
</p>
<p>In this paper we give an $\frac{11}{7}$-approximation for the $T$-tour
problem and show that the integrality ratio of the standard LP relaxation is at
most $\frac{11}{7}$. Despite much progress for the special case Path TSP, for
general $T$-tours this is the first improvement on Seb\H{o}'s analysis of the
Best-of-Many-Christofides algorithm (Seb\H{o} [2013]).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09743"><span class="datestr">at September 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09678">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09678">Efficiently Computing Maximum Flows in Scale-Free Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, Tobias Friedrich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weyand:Christopher.html">Christopher Weyand</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09678">PDF</a><br /><b>Abstract: </b>We study the maximum-flow/minimum-cut problem on scale-free networks, i.e.,
graphs whose degree distribution follows a power-law. We propose a simple
algorithm that capitalizes on the fact that often only a small fraction of such
a network is relevant for the flow. At its core, our algorithm augments
Dinitz's algorithm with a balanced bidirectional search. Our experiments on a
scale-free random network model indicate sublinear run time. On scale-free
real-world networks, we outperform the commonly used highest-label Push-Relabel
implementation by up to two orders of magnitude. Compared to Dinitz's original
algorithm, our modifications reduce the search space, e.g., by a factor of 275
on an autonomous systems graph.
</p>
<p>Beyond these good run times, our algorithm has an additional advantage
compared to Push-Relabel. The latter computes a preflow, which makes the
extraction of a minimum cut potentially more difficult. This is relevant, for
example, for the computation of Gomory-Hu trees. On a social network with 70000
nodes, our algorithm computes the Gomory-Hu tree in 3 seconds compared to 12
minutes when using Push-Relabel.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09678"><span class="datestr">at September 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09646">A Novel Method for Inference of Acyclic Chemical Compounds with Bounded Branch-height Based on Artificial Neural Networks and Integer Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Azam:Naveed_Ahmed.html">Naveed Ahmed Azam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Jianshen.html">Jianshen Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Yanming.html">Yanming Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Yu.html">Yu Shi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shurbevski:Aleksandar.html">Aleksandar Shurbevski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Liang.html">Liang Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nagamochi:Hiroshi.html">Hiroshi Nagamochi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akutsu:Tatsuya.html">Tatsuya Akutsu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09646">PDF</a><br /><b>Abstract: </b>Analysis of chemical graphs is a major research topic in computational
molecular biology due to its potential applications to drug design. One
approach is inverse quantitative structure activity/property relationship
(inverse QSAR/QSPR) analysis, which is to infer chemical structures from given
chemical activities/properties. Recently, a framework has been proposed for
inverse QSAR/QSPR using artificial neural networks (ANN) and mixed integer
linear programming (MILP). This method consists of a prediction phase and an
inverse prediction phase. In the first phase, a feature vector $f(G)$ of a
chemical graph $G$ is introduced and a prediction function $\psi$ on a chemical
property $\pi$ is constructed with an ANN. In the second phase, given a target
value $y^*$ of property $\pi$, a feature vector $x^*$ is inferred by solving an
MILP formulated from the trained ANN so that $\psi(x^*)$ is close to $y^*$ and
then a set of chemical structures $G^*$ such that $f(G^*)= x^*$ is enumerated
by a graph search algorithm. The framework has been applied to the case of
chemical compounds with cycle index up to 2. The computational results
conducted on instances with $n$ non-hydrogen atoms show that a feature vector
$x^*$ can be inferred for up to around $n=40$ whereas graphs $G^*$ can be
enumerated for up to $n=15$. When applied to the case of chemical acyclic
graphs, the maximum computable diameter of $G^*$ was around up to around 8. We
introduce a new characterization of graph structure, "branch-height," based on
which an MILP formulation and a graph search algorithm are designed for
chemical acyclic graphs. The results of computational experiments using
properties such as octanol/water partition coefficient, boiling point and heat
of combustion suggest that the proposed method can infer chemical acyclic
graphs $G^*$ with $n=50$ and diameter 30.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09646"><span class="datestr">at September 22, 2020 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09645">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09645">The Complexity Landscape of Distributed Locally Checkable Problems on Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chang:Yi=Jun.html">Yi-Jun Chang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09645">PDF</a><br /><b>Abstract: </b>Recent research revealed the existence of gaps in the complexity landscape of
locally checkable labeling (LCL) problems in the LOCAL model of distributed
computing. For example, the deterministic round complexity of any LCL problem
on bounded-degree graphs is either $O(\log^\ast n)$ or $\Omega(\log n)$ [Chang,
Kopelowitz, and Pettie, FOCS 2016]. The complexity landscape of LCL problems is
now quite well-understood, but a few questions remain open.
</p>
<p>For bounded-degree trees, there is an LCL problem with round complexity
$\Theta(n^{1/k})$ for each positive integer $k$ [Chang and Pettie, FOCS 2017].
It is conjectured that no LCL problem has round complexity $o(n^{1/(k-1)})$ and
$\omega(n^{1/k})$ on bounded-degree trees. As of now, only the case of $k = 2$
has been proved [Balliu et al., DISC 2018].
</p>
<p>In this paper, we show that for LCL problems on bounded-degree trees, there
is indeed a gap between $\Theta(n^{1/(k-1)})$ and $\Theta(n^{1/k})$ for each $k
\geq 2$. Our proof is constructive in the sense that it offers a sequential
algorithm that decides which side of the gap a given LCL problem belongs to. We
also show that it is EXPTIME-hard to distinguish between $\Theta(1)$-round and
$\Theta(n)$-round LCL problems on bounded-degree trees. This improves upon a
previous PSPACE-hardness result [Balliu et al., PODC 2019].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09645"><span class="datestr">at September 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09623">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09623">The Complexity of Constrained Min-Max Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daskalakis:Constantinos.html">Constantinos Daskalakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skoulakis:Stratis.html">Stratis Skoulakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09623">PDF</a><br /><b>Abstract: </b>Despite its important applications in Machine Learning, min-max optimization
of nonconvex-nonconcave objectives remains elusive. Not only are there no known
first-order methods converging even to approximate local min-max points, but
the computational complexity of identifying them is also poorly understood. In
this paper, we provide a characterization of the computational complexity of
the problem, as well as of the limitations of first-order methods in
constrained min-max optimization problems with nonconvex-nonconcave objectives
and linear constraints.
</p>
<p>As a warm-up, we show that, even when the objective is a Lipschitz and smooth
differentiable function, deciding whether a min-max point exists, in fact even
deciding whether an approximate min-max point exists, is NP-hard. More
importantly, we show that an approximate local min-max point of large enough
approximation is guaranteed to exist, but finding one such point is
PPAD-complete. The same is true of computing an approximate fixed point of
Gradient Descent/Ascent.
</p>
<p>An important byproduct of our proof is to establish an unconditional hardness
result in the Nemirovsky-Yudin model. We show that, given oracle access to some
function $f : P \to [-1, 1]$ and its gradient $\nabla f$, where $P \subseteq
[0, 1]^d$ is a known convex polytope, every algorithm that finds a
$\varepsilon$-approximate local min-max point needs to make a number of queries
that is exponential in at least one of $1/\varepsilon$, $L$, $G$, or $d$, where
$L$ and $G$ are respectively the smoothness and Lipschitzness of $f$ and $d$ is
the dimension. This comes in sharp contrast to minimization problems, where
finding approximate local minima in the same setting can be done with Projected
Gradient Descent using $O(L/\varepsilon)$ many queries. Our result is the first
to show an exponential separation between these two fundamental optimization
problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09623"><span class="datestr">at September 22, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09605">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09605">Distributed Algorithms for Matching in Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Oussama Hanguir, Clifford Stein <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09605">PDF</a><br /><b>Abstract: </b>$ $We study the $d$-Uniform Hypergraph Matching ($d$-UHM) problem: given an
$n$-vertex hypergraph $G$ where every hyperedge is of size $d$, find a maximum
cardinality set of disjoint hyperedges. For $d\geq3$, the problem of finding
the maximum matching is NP-complete, and was one of Karp's 21
$\mathcal{NP}$-complete problems. In this paper we are interested in the
problem of finding matchings in hypergraphs in the massively parallel
computation (MPC) model that is a common abstraction of MapReduce-style
computation. In this model, we present the first three parallel algorithms for
$d$-Uniform Hypergraph Matching, and we analyse them in terms of resources such
as memory usage, rounds of communication needed, and approximation ratio. The
highlights include:
</p>
<p>$\bullet$ A $O(\log n)$-round $d$-approximation algorithm that uses $O(nd)$
space per machine.
</p>
<p>$\bullet$ A $3$-round, $O(d^2)$-approximation algorithm that uses
$\tilde{O}(\sqrt{nm})$ space per machine.
</p>
<p>$\bullet$ A $3$-round algorithm that computes a subgraph containing a
$(d-1+\frac{1}{d})^2$-approximation, using $\tilde{O}(\sqrt{nm})$ space per
machine for linear hypergraphs, and $\tilde{O}(n\sqrt{nm})$ in general.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09605"><span class="datestr">at September 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09604">On Distributed Differential Privacy and Counting Distinct Elements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lijie.html">Lijie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghazi:Badih.html">Badih Ghazi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ravi.html">Ravi Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manurangsi:Pasin.html">Pasin Manurangsi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09604">PDF</a><br /><b>Abstract: </b>We study the setup where each of $n$ users holds an element from a discrete
set, and the goal is to count the number of distinct elements across all users,
under the constraint of $(\epsilon, \delta)$-differentially privacy:
</p>
<p>- In the non-interactive local setting, we prove that the additive error of
any protocol is $\Omega(n)$ for any constant $\epsilon$ and for any $\delta$
inverse polynomial in $n$.
</p>
<p>- In the single-message shuffle setting, we prove a lower bound of
$\Omega(n)$ on the error for any constant $\epsilon$ and for some $\delta$
inverse quasi-polynomial in $n$. We do so by building on the moment-matching
method from the literature on distribution estimation.
</p>
<p>- In the multi-message shuffle setting, we give a protocol with at most one
message per user in expectation and with an error of $\tilde{O}(\sqrt(n))$ for
any constant $\epsilon$ and for any $\delta$ inverse polynomial in $n$. Our
protocol is also robustly shuffle private, and our error of $\sqrt(n)$ matches
a known lower bound for such protocols.
</p>
<p>Our proof technique relies on a new notion, that we call dominated protocols,
and which can also be used to obtain the first non-trivial lower bounds against
multi-message shuffle protocols for the well-studied problems of selection and
learning parity.
</p>
<p>Our first lower bound for estimating the number of distinct elements provides
the first $\omega(\sqrt(n))$ separation between global sensitivity and error in
local differential privacy, thus answering an open question of Vadhan (2017).
We also provide a simple construction that gives $\tilde{\Omega}(n)$ separation
between global sensitivity and error in two-party differential privacy, thereby
answering an open question of McGregor et al. (2011).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09604"><span class="datestr">at September 22, 2020 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09480">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09480">A General Framework for the Security Analysis of Blockchain Protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lewis=Pye:Andrew.html">Andrew Lewis-Pye</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09480">PDF</a><br /><b>Abstract: </b>Blockchain protocols differ in fundamental ways, including the mechanics of
selecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and
the method to establish consensus (e.g., longest chain rules vs. Byzantine
fault-tolerant (BFT) inspired protocols). These fundamental differences have
hindered "apples-to-apples" comparisons between different categories of
blockchain protocols and, in turn, the development of theory to formally
discuss their relative merits.
</p>
<p>This paper presents a parsimonious abstraction sufficient for capturing and
comparing properties of many well-known permissionless blockchain protocols,
simultaneously capturing essential properties of both proof-of-work (PoW) and
proof-of-stake (PoS) protocols, and of both longest-chain-type and BFT-type
protocols. Our framework blackboxes the precise mechanics of the user selection
process, allowing us to isolate the properties of the selection process that
are significant for protocol design.
</p>
<p>We demonstrate the utility of our general framework with several concrete
results:
</p>
<p>1. We prove a CAP-type impossibility theorem asserting that liveness with an
unknown level of participation rules out security in a partially synchronous
setting.
</p>
<p>2. Delving deeper into the partially synchronous setting, we prove that a
necessary and sufficient condition for security is the production of
"certificates," meaning stand-alone proofs of block confirmation.
</p>
<p>3. Restricting to synchronous settings, we prove that typical protocols with
a known level of participation (including longest chain-type PoS protocols) can
be adapted to provide certificates, but those with an unknown level of
participation cannot.
</p>
<p>4. Finally, we use our framework to articulate a modular two-step approach to
blockchain security analysis that effectively reduces the permissionless case
to the permissioned case.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09480"><span class="datestr">at September 22, 2020 11:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09460">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09460">Recent Progress on Matrix Rigidity -- A Survey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramya:C=.html">C. Ramya</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09460">PDF</a><br /><b>Abstract: </b>The concept of matrix rigidity was introduced by Valiant(independently by
Grigoriev) in the context of computing linear transformations. A matrix is
rigid if it is far(in terms of Hamming distance) from any matrix of low rank.
Although we know rigid matrices exist, obtaining explicit constructions of
rigid matrices have remained a long-standing open question. This decade has
seen tremendous progress towards understanding matrix rigidity. In the past,
several matrices such as Hadamard matrices and Fourier matrices were
conjectured to be rigid. Very recently, many of these matrices were shown to
have low rigidity. Further, several explicit constructions of rigid matrices in
classes such as $E$ and $P^{NP}$ were obtained recently. Among other things,
matrix rigidity has found striking connections to areas as disparate as
communication complexity, data structure lower bounds and error-correcting
codes. In this survey, we present a selected set of results that highlight
recent progress on matrix rigidity and its remarkable connections to other
areas in theoretical computer science.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09460"><span class="datestr">at September 22, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09442">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09442">TADOC: Text Analytics Directly on Compression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Feng.html">Feng Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhai:Jidong.html">Jidong Zhai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Xipeng.html">Xipeng Shen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Dalin.html">Dalin Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zheng.html">Zheng Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutlu:Onur.html">Onur Mutlu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wenguang.html">Wenguang Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Xiaoyong.html">Xiaoyong Du</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09442">PDF</a><br /><b>Abstract: </b>This article provides a comprehensive description of Text Analytics Directly
on Compression (TADOC), which enables direct document analytics on compressed
textual data. The article explains the concept of TADOC and the challenges to
its effective realizations. Additionally, a series of guidelines and technical
solutions that effectively address those challenges, including the adoption of
a hierarchical compression method and a set of novel algorithms and data
structure designs, are presented. Experiments on six data analytics tasks of
various complexities show that TADOC can save 90.8% storage space and 87.9%
memory usage, while halving data processing times.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09442"><span class="datestr">at September 22, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.09288">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.09288">On combinatorial optimization for dominating sets (literature survey, new models)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levin:Mark_Sh=.html">Mark Sh. Levin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.09288">PDF</a><br /><b>Abstract: </b>The paper focuses on some versions of connected dominating set problems:
basic problems and multicriteria problems. A literature survey on basic problem
formulations and solving approaches is presented. The basic connected
dominating set problems are illustrated by simplifyed numerical examples. New
integer programming formulations of dominating set problems (with multiset
estimates) are suggested.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.09288"><span class="datestr">at September 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/143">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/143">TR20-143 |  Characterizing Average-Case Complexity of PH by Worst-Case Meta-Complexity | 

	Shuichi Hirahara</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We exactly characterize the average-case complexity of the polynomial-time hierarchy (PH) by the worst-case (meta-)complexity of GapMINKT(PH), i.e., an approximation version of the problem of determining if a given string can be compressed to a short PH-oracle efficient program.  Specifically, we establish the following equivalence:

  DistPH is contained in AvgP (i.e., PH is easy on average) if and only if GapMINKT(PH) is in P.

In fact, our equivalence is significantly broad: A number of statements on several fundamental notions of complexity theory, such as errorless and one-sided-error average-case complexity, sublinear-time-bounded and polynomial-time-bounded Kolmogorov complexity, and PH-computable hitting set generators, are all shown to be equivalent.

Our equivalence provides fundamentally new proof techniques for analyzing average-case complexity through the lens of *meta-complexity* of time-bounded Kolmogorov complexity and resolves, as immediate corollaries, questions of equivalence among different notions of average-case complexity of PH: low success versus high success probabilities (i.e., a hardness amplification theorem for DistPH against uniform algorithms) and errorless versus one-sided-error average-case complexity of PH.

Our results are based on a sequence of new technical results that further develops the proof techniques of the author's previous work on the non-black-box worst-case to average-case reduction and unexpected hardness results for Kolmogorov complexity (FOCS'18, CCC'20, ITCS'20, STOC'20).  Among other things, we prove the following.

  1.  If GapMINKT(NP) is in P, then P = BPP.
  At the core of the proof is a new black-box hitting set generator construction whose reconstruction algorithm uses few random bits, which also improves the approximation quality of the non-black-box worst-case to average-case reduction without using a pseudorandom generator.

  2.  If GapMINKT(PH) is in P, then DistPH is contained in AvgBPP = AvgP.

  3.  If MINKT(PH) is easy on a 1/poly(n)-fraction of inputs, then GapMINKT(PH) is in P.
  This improves the error tolerance of the previous non-black-box worst-case to average-case reduction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/143"><span class="datestr">at September 21, 2020 10:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4972">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4972">Agent 3203.7: Guest post by Eliezer Yudkowsky</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In his day, Agent 3203.7 had stopped people from trying to kill Adolf Hitler, Richard Nixon, and even, in the case of one unusually thoughtful assassin, Henry David Thoreau. But this was a new one on him.</p>



<p>“So…” drawled the seventh version of Agent 3203. His prosthetic hand crushed the simple 21st-century gun into fused metal and dropped it. “You traveled to the past in order to kill… of all people… Donald Trump. Care to explain why?”</p>



<p>The time-traveller’s eyes looked wild. Crazed. Nothing unusual. “How can you ask me that? You’re a time-traveler too! You know what he does!”</p>



<p>That was a surprising level of ignorance even for a 21st-century jumper. “Different timelines, kid. Some are pretty obscure. What the heck did Trump do in yours that’s worth taking your one shot at time travel to assassinate him of all people?”</p>



<p>“He’s destroying my world!”</p>



<p>Agent 3203.7 took a good look at where Donald Trump was pridefully addressing the unveiling of the Trump Taj Mahal in New Jersey, then took another good look at the errant time-traveler. “Destroying it how, exactly? Did Trump turn mad scientist in your timeline?”</p>



<p>“He’s President of the United States!”</p>



<p>Agent 3203.7 took another long stare at his new prisoner. He was apparently serious. “How did Trump become President in your timeline? Strangely advanced technology, subliminal messaging?”</p>



<p>“He was elected in the usual way,” the prisoner said bitterly.</p>



<p>Agent 3203.7 shook his head in amazement. Talk about shooting the messenger. “Kid, I doubt Trump was your timeline’s main problem.”</p>



<p><em>(thanks to Eliezer for giving me permission to reprint here)</em></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4972"><span class="datestr">at September 21, 2020 04:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3655236890828727429">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/baseball-can-go-on-forever-it-doesnt.html">Baseball can go on forever, it doesn't just seem that way</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Most games have some way to make sure they cannot go on forever.</p><p>1) Chess: I had thought there was a 50-move rule and a 3-times-same-position rule, but its a byte more complicated than that, see <a href="https://en.wikipedia.org/wiki/Draw_(chess)">here</a>. There is also a chess clock. Suffice to say, Chess can never on forever (though it may seem like it does). </p><p>2) NIM: Eventually all of the stones are gone. There may be more complicated versions where you can add some stones, but in those versions I suspect that there is some parameter that goes to 0.</p><p>3) Basketball, Football, Hockey, Soccer: These all have a clock so they are time limited. For overtime there are also rules that make sure the game cannot go on forever. Or maybe its just very rare: what if the Superb Owl (spelled that way to avoid lawsuits, see <a href="https://www.vox.com/the-goods/2019/1/31/18202037/super-bowl-53-ads-trademark-the-big-game-2019">here</a>) is tied 0-0 at the end of the four quarters and goes into overtime and... nobody scores... ever. Could the game go on forever or would the referees declare it a tie? In the regular season there are ties, but in the in the superb owl? Actually this may be more a problem in the playoffs since you need to determine who goes to the next round.</p><p>4) Take your favorite game. I would bet dollars to doughnuts (what an odd phrase---see <a href="https://en.wiktionary.org/wiki/bet_a_dollar_to_a_doughnut">here</a> for more about the phrase) that there is some mechanism to make sure the game ends. An exception that Darling pointed out to me: If in Gin Rummy both players are terrible then the game can go on forever. This is probably true for other games as well and actually makes the question into two questions (a) will a game terminate no matter what the players do, and (b) (not sure how to formalize) will a game terminate if both players are trying to win and are making reasonable moves.</p><p>You may have noticed that in item 3 I left out Baseball. There is no clock in baseball. So one way the game can go on forever is to have a tie and extra innings and nobody scores. I think the umpire has the authority to call it a tie. (Incidentally, the shortened baseball season has a new extra inning rule---each inning starts with a runner on second. See <a href="https://www.mlb.com/news/reasons-new-extra-innings-rule-is-good">here</a>,) When Lance read an earlier version of this post he pointed me to 5 ways a game can go on forever, not counting the example I have later in this post. <a href="https://cs.nyu.edu/~gottlieb/tr/back-issues/1990s/1992/1-jan-scanned.pdf">Here</a> is where Lance found the question and answer (look on the first page under Speed Department for the question, and the very end of the second page for the answer). I also did my own writuep with more details, see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/baseballforever.pdf">here</a>.  Also of interest (though not if you were actually at the game this happened), the record for number of times a player has a foul with 2 strikes is 16, see <a href="https://www.businessinsider.com/brandon-belts-record-at-bat-pop-fly-2018-4">here</a>. </p><p> However, I came across an  example more obscure than any of those. </p><p>Here is what happened (and you can see the video of it <a href="https://www.youtube.com/watch?v=yDyCRTlKllk">here</a>, though it really starts about a minute into it. Keep reading- it looks like its another post, but its part of this post: </p><div style="border-bottom-style: solid; border-color: rgb(222, 224, 225); border-width: 1px; color: #282829; direction: ltr; font-size: 15px; padding: 8px 16px;" class="q-box qu-borderBottom qu-px--medium qu-py--small"><div style="direction: ltr; display: flex;" class="q-flex qu-justifyContent--space-between"><div style="direction: ltr; display: flex;" class="q-flex qu-alignItems--center"><div style="color: #636466; direction: ltr; font-size: 13px; margin-left: 8px;" class="q-text qu-fontSize--small qu-ml--small qu-color--gray">From your Digest</div></div></div></div><div style="color: #282829; direction: ltr; font-size: 15px;" class="q-box"><div style="direction: ltr;" class="q-box"><div style="direction: ltr; padding: 16px 16px 4px;" class="q-box qu-pt--medium qu-pb--tiny"><div style="direction: ltr;" class="q-box"><div style="direction: ltr;" class="q-box"><div style="direction: ltr;" class="q-box"><div style="direction: ltr;" class="q-box"><div style="direction: ltr; display: flex;" class="q-flex"><div style="direction: ltr; margin-bottom: 8px; padding-right: 24px; width: 546px;" class="q-box qu-mb--small qu-pr--large"><div style="direction: ltr;" class="q-box spacing_log_answer_header"><div style="direction: ltr; display: flex; width: 522px;" class="q-flex"><div style="direction: ltr; display: inline-flex; margin-right: 8px;" class="q-inlineFlex qu-mr--small qu-alignItems--center"><div style="direction: ltr; display: inline-block;" class="q-box qu-display--inline-block"><div style="direction: ltr; display: inline-block;" class="q-box qu-display--inline-block"><div style="direction: ltr; display: inline-block;" class="q-relative qu-display--inline-block"><div style="direction: ltr; display: inline-block;" class="q-box qu-display--inline-block"><a href="https://www.quora.com/profile/Zev-Steinhardt" class="q-box qu-display--inline-flex qu-color--gray_dark qu-cursor--pointer qu-hover--textDecoration--underline" target="_blank"><div style="direction: ltr; display: inline-flex;" class="q-inlineFlex qu-flex--none"><div style="direction: ltr; display: inline-flex;" class="q-inlineFlex"><div class="q-inlineFlex qu-overflow--hidden qu-borderRadius--circle qu-borderWidth--retinaOverride"><div style="background-color: white; border-radius: 100%; direction: ltr;" class="q-box qu-bg--white__ignore_dark_mode qu-borderRadius--circle"></div><img src="https://qph.fs.quoracdn.net/main-thumb-138599745-200-pbrgkfnbxdzyttabmtnmavtcwavrcktv.jpeg" class="q-image qu-display--block qu-size--36 qu-minWidth--36" /><div class="q-box qu-borderRadius--circle qu-borderAll Photo___StyledBox-sc-1x7c6d3-1 djSgZk"></div></div></div></div></a></div></div></div></div></div><div class="q-box qu-flex--auto"><div style="direction: ltr; display: flex;" class="q-flex qu-flexWrap--wrap"><div style="direction: ltr;" class="q-box"><div style="direction: ltr; font-size: 13px; font-weight: bold;" class="q-text qu-bold qu-color--gray_dark qu-fontSize--small qu-passColorToLinks"><div style="direction: ltr; display: inline;" class="q-box qu-display--inline"><div style="direction: ltr; display: inline;" class="q-box qu-display--inline"><div style="direction: ltr; display: inline;" class="q-relative qu-display--inline"><div style="direction: ltr; display: inline;" class="q-box qu-display--inline"><a href="https://www.quora.com/profile/Zev-Steinhardt" class="q-box qu-color--gray_dark qu-cursor--pointer qu-hover--textDecoration--underline" target="_blank">Zev Steinhardt</a></div></div></div></div></div></div><span style="color: #636466; direction: ltr; font-size: 13px; margin-left: 4px; margin-right: 4px;" class="q-text qu-mx--tiny qu-color--gray qu-fontSize--small">·</span><div class="q-text qu-color--gray qu-fontSize--small qu-passColorToLinks qu-truncateLines--1"><a href="https://www.quora.com/Has-a-play-ever-happened-in-baseball-that-was-so-out-of-the-ordinary-that-no-written-umpiring-rule-at-the-time-covered-it/answer/Zev-Steinhardt" class="q-box qu-cursor--pointer qu-hover--textDecoration--underline" target="_top">July 9, 2019</a></div></div><div style="direction: ltr; display: flex; margin-top: 2px;" class="q-flex qu-flexWrap--wrap"><div class="q-text qu-truncateLines--2 qu-color--gray qu-passColorToLinks qu-fontSize--small">Studied at <span class="TopicName___StyledSpan-t3tegb-0 crUglW">Pace University</span></div></div></div></div></div></div><div style="direction: ltr; margin-left: auto; padding-left: 4px;" class="q-box qu-pl--tiny"><div style="direction: ltr; height: 18px; width: 18px;" class="q-relative qu-size--18"><div class="q-absolute"><div style="direction: ltr; display: inline-block;" class="q-box qu-display--inline-block"><div style="direction: ltr;" class="q-relative"><div class="q-click-wrapper qu-active--bg--darken qu-active--textDecoration--none qu-focus--bg--darken qu-focus--textDecoration--none qu-borderRadius--pill qu-whiteSpace--nowrap qu-display--inline-block qu-tapHighlight--white qu-textAlign--center qu-cursor--pointer qu-hover--bg--darken qu-hover--textDecoration--none" tabindex="0"><div style="direction: ltr; display: flex;" class="q-flex qu-alignItems--center qu-justifyContent--center"><div style="direction: ltr; display: flex;" class="q-relative qu-display--flex qu-alignItems--center"><span style="direction: ltr; display: inline-block; height: 24px; line-height: 0; vertical-align: text-bottom; width: 24px;" class="q-inlineBlock qu-verticalAlign--text-bottom" name="SmallClose"><span class="CssComponent__CssInlineComponent-sc-1oskqb9-1 Icon___StyledCssInlineComponent-sc-11tmcw7-0 eXDwse"></span></span></div></div></div></div></div></div></div></div></div><div style="direction: ltr; display: flex;" class="q-flex"></div><div style="direction: ltr; display: flex; margin-bottom: 4px;" class="q-flex qu-mb--tiny"><div style="direction: ltr; font-size: 16px; font-weight: bold; line-height: 1.4;" class="q-text qu-bold qu-color--gray_dark_dim qu-passColorToLinks qu-userSelect--text qu-lineHeight--regular"><span class="CssComponent__CssInlineComponent-sc-1oskqb9-1 TitleText___StyledCssInlineComponent-sc-1hpb63h-0 jPnwvF"><a href="https://www.quora.com/Has-a-play-ever-happened-in-baseball-that-was-so-out-of-the-ordinary-that-no-written-umpiring-rule-at-the-time-covered-it" class="q-box qu-cursor--pointer qu-hover--textDecoration--underline" target="_blank"><div style="direction: ltr; display: flex;" class="q-flex qu-flexDirection--row"><div style="direction: ltr; display: inline;" class="q-inline qu-flexWrap--wrap"><div style="direction: ltr;" class="q-text puppeteer_test_question_title"><span style="direction: ltr;" class="q-box qu-userSelect--text">Has a play ever happened in baseball that was so out of the ordinary that no written umpiring rule at the time covered it?</span></div></div></div></a></span></div></div><div style="direction: ltr;" class="q-relative spacing_log_answer_content"><div style="direction: ltr;" class="q-text"><span style="direction: ltr;" class="q-box qu-userSelect--text"><p style="direction: ltr; margin: 0px 0px 1em; padding: 0px;" class="q-text qu-display--block">Back in 2008, the Yankees drafted a pitcher named Pat Venditte. What made Venditte unusual is that he can throw with both hands. In other words, he’s a switch pitcher. When he was drafted, he was assigned to the Staten Island Yankees, a low A ball team.</p><p style="direction: ltr; margin: 0px 0px 1em; padding: 0px;" class="q-text qu-display--block">In his first game (against the Mets farm team, the Brooklyn Cyclones), Venditte came in to pitch. After getting the first two batters out and giving up a single, he then faced Ralph Henriquez, was a switch hitter. What happened next resembled an Abbott and Costello comedy routine. Venditte would put the glove on one hand (he had a specially made glove that could be worn on either hand) and Henriquez would then step across the plate to bat from the other side. Venditte would then switch his glove hand again and Henriquez went back to the other side.</p><p style="direction: ltr; margin: 0px 0px 1em; padding: 0px;" class="q-text qu-display--block">Eventually, after much discussion, the umpires ruled that Henriquez would have to choose a batting side first, before Venditte had to commit. Henriquez was mad and, after he struck out, he slammed the bat against the ground in frustration.</p><p style="direction: ltr; margin: 0px 0px 1em; padding: 0px;" class="q-text qu-display--block">The umpires were, in essence, winging it, because there was no rule to cover the situation. Eventually, the higher ups in baseball did write a rule to cover the situation — the opposite of the umpires’ decision.</p></span></div></div></div></div></div></div></div></div></div><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/baseball-can-go-on-forever-it-doesnt.html"><span class="datestr">at September 20, 2020 06:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/142">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/142">TR20-142 |  Relaxed Locally Correctable Codes with Improved Parameters | 

	Vahid Reza Asadi, 

	Igor Shinkar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Locally decodable codes (LDCs) are error-correcting codes $C : \Sigma^k \to \Sigma^n$ that admit a local decoding algorithm that recovers each individual bit of the message by querying only a few bits from a noisy codeword. An important question in this line of research is to understand the optimal trade-off between the query complexity of LDCs and their block length. Despite importance of these objects, the best known constructions of constant query LDCs have super-polynomial length, and there is a significant gap between the best constructions and the known lower bounds in terms of the block length.

For many applications it suffices to consider the weaker notion of relaxed LDCs (RLDCs), which allows the local decoding algorithm to abort if by querying a few bits it detects that the input is not a codeword. This relaxation turned out to allow decoding algorithms with constant query complexity for codes with almost linear length. Specifically, [Ben+06] constructed an $O(q)$-query RLDC that encodes a message of length $k$ using a codeword of block length $n = O(k^{1+1/\sqrt{q}})$.

In this work we improve the parameters of [Ben+06] by constructing an $O(q)$-query RLDC that encodes a message of length $k$ using a codeword of block length $O(k^{1+1/{q}})$. This construction matches (up to a multiplicative constant factor) the lower bounds of [KT00; Woo07] for constant query LDCs, thus making progress toward understanding the gap between LDCs and RLDCs in the constant query regime.

In fact, our construction extends to the stronger notion of relaxed locally correctable codes (RLCCs), introduced in [GRR18], where given a noisy codeword the correcting algorithm either recovers each individual bit of the codeword by only reading a small part of the input, or aborts if the input is detected to be corrupt.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/142"><span class="datestr">at September 20, 2020 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1244">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">Announcing a short course in Paris</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This coming academic year I am on sabbatical, in Paris. It’s certainly a funny year to be on sabbatical. (It’s a funny year to be doing anything, isn’t it? Or is “funny” not the appropriate word…Yet I can’t find any other way to look at it that doesn’t send me straight into the abyss. So, let it be “funny”—knowing that, no, I’m not actually laughing right now.) On the one hand, I am lucky to have escaped the incessant debates on the format of teaching, how many people per square foot are allowed in each building on campus, what distance I should stay from my students were I to attempt to meet them in person, and so many other similar decisions that have come to take up a larger and larger fraction of our professional lives (not to mention of course the incommensurate challenges that many are facing at the personal and familial level). On the other hand, the situation makes it much harder to meet others and engage in new collaborations, one of the goals of my sabbatical. I’ll see how it plays out; I’ll be sure to write more on this blog as time progresses.</p>



<p>During the sabbatical I am being hosted successively by different French institutions. For the first 6 months I had the good fortune of being awarded a “chair” from the “<a href="https://www.sciencesmaths-paris.fr/en/">Fondation Sciences Mathématiques de Paris</a>” (FSMP), a private foundation which supports, in very general terms, the development of the mathematics community in Paris, from the organization of general-public conferences to the support of research collaborations. My only formal obligation during these 6 months is to give 20 hours of lecture on a theme of my choosing. The goal that I elected for the course is provide an in-depth introduction to two major works in quantum complexity and cryptography of the past few years: first, Mahavev’s 2018 result on <a href="https://arxiv.org/abs/1804.01082">classical verification of quantum computation</a> (a result for which I already shared my enthusiasm <a href="https://mycqstate.wordpress.com/2018/08/06/the-cryptographic-leash/">here</a>); second, my result <a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> with Ji, Natarajan, Wright and Yuen on the power of quantum multi-prover interactive proof systems, which I mentioned in the <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">previous post</a>, and its consequences. For more about the course, including a tentative breakdown of lectures and some resources, see the <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">course webpage</a>. </p>



<p>While at the time of writing the course is still scheduled to start as an in-person meeting (to take place in a very large layered amphitheater with ample space for social distancing), there is no telling how the situation, and regulations, will evolve in the near future. To accommodate participants who are unable or prefer not to travel in person, all lectures starting with the first one will be recorded. In addition I will post course materials, including lecture notes, <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">here</a>. The purpose of this post is to advertise the course: participants from everywhere are welcome to watch the recorded videos, read the notes, and write to me with any questions in suggestions. In particular I plan to outsource the proof-reading of the notes via overleaf and I welcome any participant’s interest in helping with that; draft notes for the first lecture are already available <a href="https://www.overleaf.com/2293291658twkjfbtctsdb">here</a>. Anyone is welcome to make direct corrections, or add inline comments pointing to issues that may need my attention.</p>



<p>The program that I chose is ambitious, and we will see how far we get along. My goal is to start slow, so as to remain inclusive with respect to varying backgrounds in computer science, mathematics or physics. At first I will give complete definitions, state and prove simple lemmas, etc., in order to establish common language. As time progresses I expect that things will become a little more high-level, less self-contained, and more technical. Depending on your background and interests, you may find the first few lectures, or the last few ones, more interesting. Teaching the course will certainly be beneficial for me because I believe that there is a strong unity behind the two works I chose to present. I hope to make that unity apparent by presenting them together. Moreover, both works introduce new techniques that leave many avenues open; I hope that a “clean” presentation will help me, and others, build on them. </p>



<p>A side benefit of an “un-necessary” course such as this one is that it contributes to bringing a certain community together. (By “un-necessary” I mean that the course will not be required for any curriculum; if it did not take place, as long as it was replaced by other research-level activities its absence would not be felt.) COIVD-19 unfortunately turns that opportunity into a challenge. It is because of it that I insist–regulations allowing– on having the course take place in person: as much as we are getting used to Zoom, and as well as it may be working as a replacement for many aspects of our interactive lives, from in-person classes to conferences to research collaborations, a scientific event such as this one, with sustained involvement by a small set of participants coming from distant backgrounds, is probably one of the more challenging ones to make work online. I hope it doesn’t come to that. Even if it does, one of the lessons learned from the Spring 2020 semester on quantum computing at the Simons Institute in Berkeley, which was interrupted half-ways due to the pandemic, is that having an initial in-person phase was of great help to cement future online interactions. So, I hope that I am able to lecture on Tuesday; after that, we will see.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/"><span class="datestr">at September 20, 2020 03:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5573">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/09/19/combinatorial-journals-are-changing/">Combinatorial Journals are Changing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I used to ask most combinatorialists I met for their opinion about the level of various journals. With this feedback, I compiled a rough journal ranking for combinatorics papers (for personal use). This was a very educational experience for me as a new combinatorialist. I learned that different people have rather different opinions. For example, […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/09/19/combinatorial-journals-are-changing/"><span class="datestr">at September 19, 2020 09:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2020/09/19/beyondlogconvavesampling/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2020/09/19/beyondlogconvavesampling/">Beyond log-concave sampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As the growing number of posts on this blog would suggest, recent years have seen a lot of progress in understanding optimization beyond convexity. However, optimization is only one of the basic algorithmic primitives in machine learning — it’s used by most forms of risk minimization and model fitting. Another important primitive is sampling, which is used by most forms of inference (i.e. answering probabilistic queries of a learned model).</p>

<p>It turns out that there is a natural analogue of convexity for sampling — <em>log-concavity</em>. Paralleling the state of affairs in optimization, we have a variety of (provably efficient) algorithms for sampling from log-concave distributions, under a variety of access models to the distribution. Log-concavity, however, is very restrictive and cannot model common properties of distributions we frequently wish to sample from in machine learning applications, for example multi-modality and manifold structure in the level sets, which is what we’ll focus on in this and the upcoming post.</p>

<p>Unlike non-convex optimization, the field of sampling beyond log-concavity is very nascent. In this post, we will survey the basic tools and difficulties for sampling beyond log-concavity. In the next post, we will survey recent progress in this direction, in particular with respect to handling multi-modality and manifold structure in the level sets, covering the papers <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski and <a href="https://arxiv.org/abs/2002.05576">Fast convergence for Langevin diffusion with matrix manifold structure</a> by Ankur Moitra and Andrej Risteski.</p>

<h1 id="formalizing-the-sampling-problem">Formalizing the sampling problem</h1>

<p>The formulation of the sampling problem we will consider is as follows:</p>

<blockquote>
  <p><strong>Problem</strong>: Sample from a distribution $p(x) \propto e^{-f(x)}$ given black-box access to $f$ and $\nabla f$.</p>
</blockquote>

<p>This formalization subsumes a lot of inference tasks involving different kinds of probabilistic models. We give several common examples:</p>

<p><em>1.Posterior inference</em>: Suppose our data is generated from a model with <em>unknown</em> parameters $\theta$ , such that the data-generation process is given by $p(x \mid \theta)$ and we have a prior $p(\theta)$ over the model parameters. Then the <em>posterior distribution</em> $p(\theta \mid x)$ , by Bayes’s Rule, is given by</p>

\[p(\theta \mid x) = \frac{p(x \mid \theta)p(x)}{p(x)}\propto p(x \mid \theta)p(\theta).\]

<p>A canonical example of this is a <em>noisy inference task</em> where a signal (parametrized by $\theta$ ) is perturbed by noise (as specified by $p(x \mid \theta)$ ).</p>

<p><em>2.Posteriors in latent-variable models</em>: If the data-generation process has a <em>latent (hidden) variable</em> $h$ associated to each data point, such that $h$ has a <em>known</em> prior $p(h)$ and a <em>known</em> conditional $p_\theta(x \mid h)$ , then again by Bayes’s rule, we have</p>

\[p_\theta(h \mid x) = \frac{p_\theta(x \mid h)p_\theta(h)}{p_\theta(x)}\propto p_\theta(x \mid h)p_\theta(h).\]

<p>In typical latent-variable models, $p_\theta(x \mid h)$ and $p_\theta(h)$ have a simple parametric form, which makes it easy to evaluate $p_\theta(x \mid h)p_\theta(h)$ . Some examples of latent-variable models are mixture models (where $h$ encodes which component a sample came from), topic models (where $h$ denote the topic proportions in a document), and noisy-OR networks (and latent-variable Bayesian belief networks).</p>

<p><em>3.Sampling from energy models</em>: in energy models, the distribution of the data is parametrized as $p(x) \propto \exp(-E(x))$ for some <em>energy</em> function $E(x)$ which is smaller on points in the data distribution. Recent works by <a href="https://arxiv.org/abs/1907.05600">(Song, Ermon 2019)</a> and <a href="https://arxiv.org/abs/1903.08689">(Du, Mordatch 2019)</a> have scaled up the training of these models on images so that the visual quality of the samples they produce is comparable to that of more popular generative models like GANs and flow models.</p>

<p>The “exponential form” $e^{-f(x)}$ is also helpful in making an analogy to optimization. Namely, if we sample from $p(x)\propto e^{-f(x)}$, a particular point $x$ is more likely to be sampled if $f(x)$ is small. The key difference between with optimization is that while in optimization, we only want to get to the minimum, in sampling, we want to pick points with the correct probabilities.</p>

<h1 id="comparison-with-optimization">Comparison with optimization</h1>

<p>The computational hardness landscape for our sampling problem parallels the one for black-box optimization, in which the goal is to find the minimum of a function $f$, given value/gradient oracle access. When $f$ is <em>convex</em>, there is a unique local minimum, so that local search algorithms like <em>gradient descent</em> are efficient. When $f$ is non-convex, gradient descent can get trapped in potentially poor local minima, and in the worst case, an exponential number of queries is needed.</p>

<p>Similarly, for sampling, when $p$ is <em>log-concave</em>, the distribution is unimodal and a Markov Chain which is a close relative of gradient descent — <em>Langevin Monte Carlo</em> —  is efficient. When $p$ is non-log-concave, Langevin Monte Carlo can get trapped in one of many modes, and and exponential number of queries may also be needed.</p>

<blockquote>
  <p>A distribution $p(x)\propto e^{-f(x)}$ is <strong>log-concave</strong> if $f(x) = -\log p(x)$ is convex. It is $\alpha$-strongly log-concave if $f(x)$ is $\alpha$-strongly convex.</p>
</blockquote>

<p>However, such worst-case hardness rarely stop practitioners from trying to solve the non-convex optimization or non-log-concave sampling problems which are ubiquitous in modern machine learning. Often they manage to do so with great success - for instance, in training deep neural networks, gradient descent and its relatives perform quite well. Similarly, Langevin Monte Carlo and its relatives can do quite well on non-log-concave problems, though they sometimes need to be aided by temperature heuristics and other tricks.</p>

<p>As theorists, we’d like to develop theory that will lead to a better understanding of why and when these heuristics work. Just like we’ve done for optimization, we need to be guided both by hardness results and relevant structure of real-world problems in this endeavour.</p>

<p>The following table summarizes the comparisons we have come up with:</p>

<p><img src="http://www.andrew.cmu.edu/user/aristesk/table_opt.jpg" alt="" /></p>

<p>Before we move on to non-log-concave distributions, though, we need to understand the basic algorithm for sampling and its guarantees for log-concave distributions.</p>

<h1 id="langevin-monte-carlo">Langevin Monte Carlo</h1>

<p>Just as gradient descent is the canonical algorithm for optimization, <em>Langevin Monte Carlo</em> (LMC) is the canonical algorithm for our sampling problem. In a nutshell, it is gradient descent that also injects Gaussian noise:</p>

\[\text{Gradient descent:}\quad 
x_{t+\eta} = x_t - \eta \nabla f(x_t)\]

\[\text{Langevin Monte Carlo:}\quad
x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I)\]

<p>Both of these processes can be considered as discretizations of a continuous process. For gradient descent, the limit is an <em>ordinary differential equation</em>, and for Langevin Monte Carlo a <em>stochastic differential equation</em>:</p>

\[\text{Gradient flow:} \quad dx_t = -\nabla f(x_t) dt\]

\[\text{Langevin diffusion:} \quad dx_t = -\nabla f(x_t) dt + \sqrt{2} dB_t\]

<p>where $B_t$ denotes Brownian motion of the appropriate dimension.</p>

<p>The crucial property of the above stochastic differential equation is that under fairly mild assumptions on $f$, the stationary distribution is $p(x) \propto e^{-f(x)}$. (If you’re more comfortable with optimization, note that while gradient descent generally converges to (local) minima, the Gaussian noise term prevents LMC from converging to a single point - rather, it converges to a <em>stationary distribution</em>. See animation below.)</p>

<p><img src="http://www.andrew.cmu.edu/user/aristesk/gd_ld_animated.gif" alt="" /></p>

<p>Langevin Monte Carlo fits in the <em>Markov Chain Monte Carlo</em> (MCMC) paradigm: design a random walk, so that the stationary distribution is the desired distribution. “Mixing” means getting close to the stationary distribution, and rapid mixing means this happens quickly.</p>

<p>Like in optimization, Langevin Monte Carlo is the most “basic” algorithm: for example, one can incorporate “acceleration” and obtain <em>underdamped</em> Langevin, or use the physics-inspired Hamiltonian Monte Carlo.</p>

<h1 id="tools-for-bounding-mixing-time-challenges-beyond-log-concavity">Tools for bounding mixing time, challenges beyond log-concavity</h1>

<p>To illustrate the difficulty in moving beyond log-concavity, we’ll describe the tools that are used to prove fast mixing for log-concave distributions, and where they fall short for non-log-concave distributions.</p>

<p>We will do this by an analogy to how we analyze random walks on graphs. One common way to prove rapid mixing of a random walk on a graph is to show the Laplacian has a spectral gap (equivalently, the transition matrix has a gap between the largest and next-to-largest eigenvalue). The analogue of this for Langevin diffusion is showing a <em>Poincaré inequality</em>. (A spectral gap of $1/C$ corresponds to Poincaré constant of $C$.)</p>

<blockquote>
  <p>We say that $p(x)$ satisfies a <strong>Poincaré inequality</strong> with constant $C$ if for all functions $g$ on $\mathbb R^d$ (such that $g$ and $\nabla g$ are square-integrable with respect to $p$),</p>
  <div> $$\text{Var}_p(g) \le C \int_{\mathbb R^d} ||\nabla g(x)||^2 p(x)\,dx.$$ </div>
</blockquote>

<p>A small constant $C$ implies fast mixing in $\chi^2$ divergence, which implies fast mixing in total variation distance. More precisely, the mixing time for Langevin diffusion is on the order of $C$. We note that other functional inequalities imply mixing with respect to other measures (such as log-Sobolev inequalities for KL divergence).</p>

<p>While it may not be obvious what the Poincaré inequality has to do with a spectral gap, it turns out that we can think of the right-hand side as a quadratic form involving the <em>infinitesimal generator</em> of Langevin process, which functions as the continuous analogue of a Laplacian for a graph random walk.</p>

<p>The following table shows the analogy: we can put the discrete and continuous processes on the same footing by defining a quadratic form called the Dirichlet form from the Laplacian or infinitesimal generator.</p>

<p><img src="http://www.andrew.cmu.edu/user/aristesk/table_mixing.jpg" alt="" /></p>

<p>To see how the Poincaré inequality represents a spectral gap in the discrete case, we write it in a more explicit form in a familiar special case: a lazy random walk (i.e. a random walk that with probability $1/2$ stays in the current vertex, and with probability $1/2$ goes to a random neighbor) on a regular graph with $n$ vertices. In this case, $p$ is the uniform distribution, and $v_1=\mathbf 1,\ldots, v_n$ are the eigenvectors of $A$ with eigenvalues $1=\lambda_1\ge \lambda_2\ge \cdots \ge \lambda_n\ge 0$; normalize $v_1,\ldots, v_n$ so they have unit norm with respect to $p$, i.e. $\Vert v_i\Vert_p^2=\frac 1n\sum_j v_{ij}^2=1$.</p>

<p>Writing $g= \sum_i a_i v_i$, since $v_2,\ldots, v_n$ are orthogonal to $v_1=\mathbf 1$, we have $\langle g, \mathbf 1\rangle_p =  a_1$, so</p>

\[\text{Var}_p(g) = \frac{1}{n}(\sum_i  g_i^2) - a_1^2 = \sum_{i=2}^n a_i^2\]

<p>Furthermore, we have</p>

\[\langle g, Lg \rangle_p = \langle \sum_i a_iv_i, (I- A)(\sum_i a_iv_i)\rangle_p=  \sum_{i=2}^n a_i^2(1-\lambda_i)\]

<p>These coefficients are all at most $1-\lambda_2$, i.e. the <em>spectral gap</em>, so</p>

\[\langle g, Lg \rangle_p \ge (1-\lambda_2)\text{Var}_p(g),\]

<p>which shows the Poincaré inequality with constant $(1-\lambda_2)^{-1}$.</p>

<p>A classic theorem establishes a Poincaré inequality for (strongly) log-concave distributions.</p>

<blockquote>
  <p><strong>Theorem (Bakry, Emery 1985)</strong>: If $p(x)$ is $\alpha$-strongly log-concave, then $p(x)$ satisfies a Poincaré inequality with constant $\frac1{\alpha}$.</p>
</blockquote>

<p>Hence, for strongly-log-concave distributions, Langevin diffusion mixes rapidly. To complete the picture, a line of recent works, starting with <a href="https://arxiv.org/abs/1412.7392">(Dalalyan 2014)</a> have established bounds for discretization error to obtain algorithmic guarantees for Langevin Monte Carlo.</p>

<p>However, guarantees break down when we don’t assume log-concavity. Generically, algorithms for sampling depend <em>exponentially</em> on the ambient dimension $d$, or on the “size” of the non-log-concave region (e.g., the distance between modes of the distribution). In terms of their dependence on $d$, they are not doing much better than if we split space into cells and sample each according to its probability, similar to “grid search” for optimization. This is unsurprising: we can’t hope for better guarantees without structural assumptions.</p>

<p>Toward this end, in the next blog post we will consider two kinds of structure that allow efficient sampling:</p>

<ol>
  <li>Simple multimodal distributions, such as a mixture of gaussians with equal variance.</li>
  <li>Manifold structure, arising from symmetries in the level sets of the distribution.</li>
</ol></div>







<p class="date">
<a href="http://offconvex.github.io/2020/09/19/beyondlogconvavesampling/"><span class="datestr">at September 19, 2020 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/">Living with Asynchrony: Bracha's Reliable Broadcast</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this series of posts, we explore what can be done in the Asynchronous model. This model seems challenging because the adversary can delay messages by any bounded time. By the end of this series, you will see that almost everything that can be done in synchrony can be obtained...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/"><span class="datestr">at September 19, 2020 01:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=457">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/09/18/tcs-talk-wednesday-september-23-fotis-iliopoulos-princeton-and-ias/">TCS+ talk: Wednesday, September 23 — Fotis Iliopoulos, Princeton and IAS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, September 23th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Fotis Iliopoulos</strong> from Princeton and IAS will speak about “<em>Stochastic Local Search and the Lovász Local Lemma</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: The Lovasz Local Lemma (LLL) is a powerful tool in probabilistic combinatorics which can be used to establish the existence of objects that satisfy certain properties. The breakthrough of Moser and Tardos (who recently received the Godel Prize for their work) and follow-up works revealed that the LLL has intimate connections with a class of stochastic local search algorithms for finding such desirable objects.<br /><br />In this talk, I will survey this line of work through the perspective of recent unifying results, and also talk about recent applications to solving pseudo-random constraint satisfaction problems.</p></blockquote>



<p></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/09/18/tcs-talk-wednesday-september-23-fotis-iliopoulos-princeton-and-ias/"><span class="datestr">at September 18, 2020 04:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/private-pac/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/private-pac/">Differentially Private PAC Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The study of differentially private PAC learning runs all the way from
its introduction in 2008 <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> to a best paper award at the
Symposium on Foundations of Computer Science (FOCS) this year <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>.
In this post, we’ll recap the history of this line of work, aiming for
enough detail for a rough understanding of the results and methods.</p>

<p>Before we get to the “what” and “how” of private PAC learning, it’s
worth thinking about the “why”. One motivation for this line of work is
that it neatly captures a fundamental question: does privacy in machine
learning come at a price? Machine learning is now sufficiently
successful and widespread for this question to have real import. But to
even start to address this question, we need a formalization of machine
learning that allows us to reason about possible trade-offs in a
rigorous way. Statistical learning theory, and its computational
formalization as PAC learning, provide one such clean and well-studied
model. We can therefore use PAC learning as a testbed whose insights we
might carry to other less idealized forms of learning.</p>

<p>With this motivation in mind, the rest of this post is structured as
follows. The first section covers the basics of the PAC model, and
subsequent sections gradually build up a chronology of results. When
possible, we give short sketches of the accompanying techniques.</p>

<h1 id="pac-learning">PAC Learning</h1>

<p>We’ll start with a brief overview of PAC learning absent any privacy
restrictions. Readers familiar with PAC learning can probably skip this
section while noting that</p>

<ol>
  <li>
    <p>(the cardinality version of) Occam’s razor is a baseline learner
using \(O(\log|\mathcal{H}|)\) samples,</p>
  </li>
  <li>
    <p>VC dimension characterizes non-private PAC learning,</p>
  </li>
  <li>
    <p>we’ll focus on the sample complexity of realizable PAC learning,</p>
  </li>
  <li>
    <p>we’ll usually omit dependencies on accuracy and success probability
parameters, and</p>
  </li>
  <li>
    <p>we’ll usually ignore computational efficiency.</p>
  </li>
</ol>

<p>For readers needing a refresher on PAC learning, the basic element of
the “probably approximately correct” (PAC) framework <a href="https://dl.acm.org/doi/10.1145/1968.1972" title="Leslie G Valiant. A theory of the learnable. Communications of the ACM, 1984"><strong>[Val84]</strong></a> is a
<em>hypothesis</em>. Each hypothesis is a function
\(h \colon \mathcal{X}\to \{-1,1\}\) mapping <em>examples</em> from some space
\(\mathcal{X}\) to binary labels. A collection of hypotheses is a
<em>hypothesis class</em> \(\mathcal{H}\), e.g., thresholds (a.k.a. perceptrons),
rectangles, conjunctions, and so on. In the <em>realizable</em> setting, a
learner receives examples drawn from some unknown distribution and
labeled by an unknown \(h^\ast \in \mathcal{H}\). The learner’s goal is to
with high probability (“probably”) output a hypothesis that mostly
matches the labels of \(h^\ast\) on future examples from the unknown example
distribution (“approximately correct”). In the <em>agnostic</em> setting,
examples are not necessarily labeled by any \(h
\in \mathcal{H}\), and the goal is only to output a hypothesis that
approximates the best error of any hypothesis from \(\mathcal{H}\). As
mentioned above, we focus on the realizable setting unless otherwise
specified. In the <em>proper</em> setting, the learner must output a hypothesis
from \(\mathcal{H}\) itself. In the <em>improper</em> setting, this requirement
is removed.</p>

<p>In general, we say an algorithm \((\alpha,\beta)\)-PAC learns
\(\mathcal{H}\) with sample complexity \(n\) if \(n\) samples are sufficient
to with probability at least \(1-\beta\) obtain error at most \(\alpha\)
over new examples from the distribution. For the purposes of this post,
we generally omit these dependencies on \(\alpha\) and \(\beta\), as they
typically vary little or not at all when switching between non-private
and private PAC learning.</p>

<p>Fortunately, we always have a simple baseline learner based on empirical
risk minimization: given a set of labeled examples, iterate over all
hypotheses \(h \in \mathcal{H}\), check how many of the labeled examples
each \(h\) mislabels, and output a hypothesis that mislabels the fewest
examples. Using this learner, which is sometimes called “Occam’s razor,”
\(O(\log|\mathcal{H}|)\) samples suffice to PAC learn \(\mathcal{H}\).</p>

<p>At the same time, \(|\mathcal{H}|\) is a pretty coarse measure of
hypothesis class complexity, as it would immediately rule out learning
any infinite hypothesis class (of which there are many). Thus, as you
might expect, we can do better. We do so using <em>VC dimension</em>.
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is the size of the largest
possible collection of examples such that, for every labeling of the
examples, \(\mathcal{H}\) contains a hypothesis with that labeling. With
VC dimension, we can essentially swap \(\log|\mathcal{H}|\) with
\(\mathsf{VCD}\left(\mathcal{H}\right)\) in the Occam’s razor bound and
PAC learn with \(O(\mathsf{VCD}\left(\mathcal{H}\right))\) samples. In
fact, the “Fundamental Theorem of Statistical Learning” says that PAC
learnability (realizable or agnostic) is equivalent to finite VC
dimension. In this sense, \(\mathsf{VCD}\left(\mathcal{H}\right)\) is a
good measure of how hard it is to PAC learn \(\mathcal{H}\). As a
motivating example that will re-appear later, note that for the
hypothesis class of 1-dimensional thresholds over \(T\) points,
\(\log |\mathcal{H}| = \log T\), while
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is only 1.</p>

<p><img width="400" style="margin: auto; display: block;" alt="Example: a one-dimensional threshold function" src="https://differentialprivacy.org/images/thresh.png" />
An illustration of 1-dimensional thresholds. A given threshold is determined by some point \(x^\ast \in [T]\): any example \(x \leq x^\ast\) receives label \(-1\), and any example \(x &gt; x^\ast\) receives label 1.</p>

<h1 id="a-simple-private-pac-learner">A Simple Private PAC Learner</h1>

<p>It is straightforward to add a differential privacy constraint to the
PAC framework: the hypothesis output by the learner must be a
differentially private function of the labeled examples
\((x_1, y_1), \ldots, (x_n, y_n)\). That is, changing any one of the
examples — even to one with an inconsistent label — must not affect
the distribution over hypotheses output by the learner by too much.</p>

<p>Since we haven’t talked about any other PAC learner, we may as well
start with the empirical risk minimization-style Occam’s razor discussed
in the previous section, which simply selects a hypothesis that
minimizes empirical error. A private version becomes easy if we view
this algorithm in the right light. All it is doing is assigning a score
to each possible output (the hypothesis’ empirical error) and outputting
one with the best (lowest) score. This makes it a good candidate for
privatization by the <em>exponential mechanism</em> <a href="https://dl.acm.org/doi/10.1109/FOCS.2007.41" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>.</p>

<p>Recall that the exponential mechanism uses a scoring function over
outputs to release better outputs with higher probability, subject to
the privacy constraint. More formally, the exponential mechanism
requires a scoring function \(u(X,h)\) mapping (database, output) pairs to
real-valued scores and then selects a given output \(h\) with probability
proportional to \(\exp\left(\tfrac{\varepsilon
u(X,h)}{2\Delta(u)}\right)\). Thus a lower \(\varepsilon\) (stricter
privacy requirement) and larger \(\Delta(u) := \sup_h \sup_{X \sim X’} u(X,h) - u(X’,h) \) (scoring function more sensitive to changing one element in the database \(X\) to make \(X’\)) both lead to a more uniform (more
private) output distribution.</p>

<p>Fortunately for our PAC learning setting, empirical error is not a very
sensitive scoring function: changing one sample only changes empirical
error by 1. We can therefore use (negative) empirical error as our
scoring function \(u(X,h)\), apply the exponential mechanism, and get a
“private Occam’s razor.” This was exactly what Kasiviswanathan, Lee,
Nissim, Raskhodnikova, and Smith <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> did when they introduced
differentially private PAC learning in 2008. The resulting sample
complexity bounds differ from the generic Occam’s razor only by an
\(\varepsilon\) factor in the denominator, and
\(O(\log|\mathcal{H}|/\varepsilon)\) samples suffice to privately PAC
learn \(\mathcal{H}\).</p>

<p>Of course, our experience with non-private PAC learning suggests that we
shouldn’t be satisfied with this \(\log
|\mathcal{H}|\) dependence. Maybe VC dimension characterizes private PAC
learning, too?</p>

<h1 id="characterizing-pure-private-pac-learning">Characterizing Pure Private PAC Learning</h1>

<p>As it turns out, answering this question will take some time. We start
with a partial negative answer. Specifically, we’ll see a class with VC
dimension 1 and (a restricted form of) private sample complexity
arbitrarily larger than 1. We’ll also cover the first in a line of
characterization results for private PAC learning.</p>

<p>We first consider learners that satisfy <em>pure</em> privacy. Recall that pure
\((\varepsilon,0)\)-differential privacy forces output distributions that
may only differ by a certain \(e^\varepsilon\) multiplicative factor (like
the exponential mechanism above). The strictly weaker notion of
approximate \((\varepsilon,\delta)\)-differential privacy also allows a
small additive \(\delta\) factor. Second, we restrict ourselves to
<em>proper</em> learners, which may only output hypotheses from the learned
class \(\mathcal{H}\).</p>

<p>With these assumptions in place, in 2010, Beimel, Kasiviswanathan, and
Nissim <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> studied a hypothesis class called \(\mathsf{Point}_d\).
\(\mathsf{Point}_d\) consists of \(2^d\) hypotheses, one for each vector in
\(\{0,1\}^d\). Taking the set of examples \(\mathcal{X}\) to be \(\{0,1\}^d\)
as well, we define each hypothesis in \(\mathsf{Point}_d\) to label only
its associated vector as 1, and the remaining \(2^d-1\) examples as
-1. <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> showed that the hypothesis class \(\mathsf{Point}_d\) requires
\(\Omega(d)\) samples for proper pure private PAC learning. In contrast,
\(\mathsf{VCD}\left(\mathsf{Point}_d\right) = 1\), so this \(\Omega(d)\)
lower bound shows us that VC dimension does <em>not</em> characterize proper
pure private PAC learning.</p>

<p>This result uses the classic “packing” lower bound method, which powers
many lower bounds for pure differential privacy. The general packing
method is to first construct a large collection of databases which are
all “close enough” to each other but nonetheless all have different
“good” outputs. Once we have such a collection, we use <em>group privacy</em>.
Group privacy is a corollary of differential privacy that requires
databases differing in \(k\) elements to have \(k\varepsilon\)-close output
distributions. Because of group privacy, if we start with a collection
of databases that are close together, then the output distributions for
any two databases in the collection cannot be too different. This
creates a tension: utility forces the algorithm to produce different
output distributions for different databases, but privacy forces
similarity. The packing argument comes down to arguing that, unless the
databases are large, privacy wins out, and when privacy wins out then
there is some database where the algorithm probably produces a bad
output.</p>

<p>For \(\mathsf{Point}_d\), we sketch the resulting argument as follows.
Suppose we have an \(\varepsilon\)-private PAC learner that uses \(m\)
samples. Then we can define a collection of different databases of size
\(m\), one for each hypothesis in \(\mathsf{Point}_d\). By group privacy,
the output distribution for our private PAC learner changes by at most
\(e^{m\varepsilon}\) between any two of the databases in this collection.
Thus we can pick any \(h \in \mathsf{Point}_d\) and know that the
probability of outputting the wrong hypothesis is at least roughly
\(2^d \cdot e^{-m\varepsilon}\). Since we need this probability to be
small, rearranging implies \(m =
\Omega(d/\varepsilon)\).</p>

<p><a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> then contrasted this result with an <em>improper</em> pure private PAC
learner. This learner applies the exponential mechanism to a class
\(\mathsf{Point}_d’\) of hypotheses derived from \(\mathsf{Point}_d\) —
but <em>not</em> necessarily a subset of \(\mathsf{Point}_d\) — gives an
improper pure private PAC learner with sample complexity \(O(\log
d)\). Since this learner is improper, it circumvents the “one database
per hypothesis” step of the packing lower bound. Moreover, <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> gave a
still more involved improper pure private PAC learner requiring only
\(O(1)\) samples. This separates proper pure private PAC learning from
improper pure private PAC learning. In contrast, the sample complexities
of proper and improper PAC learning absent privacy are the same up to
logarithmic factors in \(\alpha\) and \(\beta\).</p>

<p>In 2013, Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> proved a more general
result. They gave the first characterization of pure (improper) private
PAC learning by defining a new hypothesis class measure called the
<em>representation dimension</em>, \(\mathsf{REPD}\left(\mathcal{H}\right)\).
Roughly, the representation dimension considers the collection of all
distributions \(\mathcal{D}\) over sets of hypotheses, not necessarily
from \(\mathcal{H}\), that “cover” \(\mathcal{H}\). By “cover,” we mean that
for any \(h
\in \mathcal{H}\), with high probability a set drawn from covering
distribution \(\mathcal{D}\) includes a hypothesis that mostly produces
labels that agree with \(h\). With this collection of distributions
defined, \(\mathsf{REPD}\left(\mathcal{H}\right)\) is the minimum over all
such covering distributions of the logarithm of the size of the largest
set in its support. Thus a hypothesis class that can be covered by a
distribution over small sets of hypotheses will have a small
representation dimension. With the notion of representation dimension in
hand, <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> gave the following result:</p>

<blockquote>
  <p><strong>Theorem 1</strong> (<a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Theta(\mathsf{REPD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Representation dimension may seem like a strange definition, but a
sketch of the proof of this result helps illustrate the connection to
private learning. Recall from our private Occam’s razor, and the
improper pure private PAC learner above, that if we can find a good and
relatively small set of hypotheses to choose from, then we can apply the
exponential mechanism and call it a day. It is exactly this kind of
“good set of hypotheses” that representation dimension aims to capture.
A little more formally, given an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we know there is some covering
distribution whose largest hypothesis set is not too big. That means we
can construct a learner that draws a hypothesis set from this covering
distribution and applies the exponential mechanism to it. Just as we
picked up a \(\log|\mathcal{H}|\) sample complexity dependence using
private Occam’s razor, since \(\mathsf{REPD}\left(\mathcal{H}\right)\)
measures the logarithm of the size of the largest hypothesis set in the
support, this pure private learner picks up a
\(\mathsf{REPD}\left(\mathcal{H}\right)\) sample complexity dependence
here. This gives us one direction of
Theorem 1.</p>

<p>This logic works in the other direction as well. To go from a pure
private PAC learner with sample complexity \(m\) to an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we return to the group privacy
trick used by <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a>. Suppose we fix a database of size \(m\) and pass it
to the learner. By group privacy and the learner’s accuracy guarantee,
if we fix some concept \(c\), the learner has probability at least roughly
\(e^{-m}\) of outputting a hypothesis that mostly agrees with \(c\). Thus if
we repeat this process roughly \(e^{m}\) times, we probably get at least
one hypothesis that mostly agrees with \(c\). In other words, this
repeated calling of the learner on the arbitrary database yields a
covering distribution for \(\mathcal{H}\). Since we called the learner
approximately \(e^m\) times, the logarithm of this is \(m\), and we get our
upper bound on \(\mathsf{REPD}\left(\mathcal{H}\right)\).</p>

<p>To recap, we now know that proper pure private PAC learning is strictly
harder than improper pure private PAC learning, which is characterized
by representation dimension. A picture sums it up. Note the dotted line,
since we don’t yet have any evidence separating finite representation
dimension and finite VC dimension.</p>

<p><img width="400" style="margin: auto; display: block;" alt="Landscape of Private PAC, take 1" src="https://differentialprivacy.org/images/private_pac_1.png" /></p>

<h1 id="separating-pure-and-approximate-private-pac-learning">Separating Pure and Approximate Private PAC Learning</h1>

<p>So far, we’ve focused only on pure privacy. In this section, we move on
to the first separations between pure and approximate private PAC
learning, as well as the first connection between private learning and
<em>online</em> learning.</p>

<p>Our source is a pair of interconnected papers from around 2014. Among
other things, Feldman and Xiao <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a> introduced <em>Littlestone
dimension</em> to private PAC learning. By connecting representation
dimension to results from communication complexity to Littlestone
dimension, they proved the following:</p>

<blockquote>
  <p><strong>Theorem 2</strong> (<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Omega(\mathsf{LD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Littlestone dimension \(\mathsf{LD}\left(\mathcal{H}\right)\) is, roughly,
the maximum number of mistakes an adversary can force an <em>online</em>
PAC-learning algorithm to make <a href="https://link.springer.com/article/10.1023/A:1022869011914" title="Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 1988"><strong>[Lit88]</strong></a>. We always have
\(\mathsf{VCD}\left(\mathcal{H}\right) \leq \mathsf{LD}\left(\mathcal{H}\right) \leq \log|\mathcal{H}|\),
but these inequalities can be strict. For example, denoting by
\(\mathsf{Thresh_T}\) the class of thresholds over \(\{1, 2, \ldots,
T\}\), since an adversary can force \(\Theta(\log T)\) wrong answers from
an online learner binary searching over \(\{1,2, \ldots, T\}\),
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right) = \Omega(\log T)\). In
contrast, \(\mathsf{VCD}\left(\mathsf{Thresh_T}\right) = 1\).</p>

<p>At first glance it’s not obvious what
Theorem 2 adds over
Theorem 1. After all,
Theorem 1 gives an equivalence, not just a lower bound. One
advantage of
Theorem 2 is that Littlestone dimension is a known
quantity that has already been studied in its own right. We can now
import results like the lower bound on
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right)\), whereas bounds on
\(\mathsf{REPD}\left(\cdot\right)\) are not common. A second advantage is
that Littlestone dimension conceptually connects private learning and
online learning: we now know that pure private PAC learning is no easier
than online PAC learning.</p>

<p>A second paper by Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a> contrasted this
\(\Omega(\log T)\) lower bound for pure private learning of thresholds
with a \(2^{O(\log^\ast T)}\) upper bound for <em>approximate</em> private PAC
learning \(\mathsf{Thresh_T}\). Here \(\log^\ast\) denotes the very
slow-growing iterated logarithm, the number of times we must take the
logarithm of the argument to bring it \(\leq 1\). (We’re not kidding about
“very slow-growing” either:
\(\log^\ast(\text{number of atoms in universe}) \approx
4\).) With Feldman and Xiao’s result, this separates pure private PAC
learning from approximate private PAC learning. It also shows that
representation dimension does <em>not</em> characterize approximate private PAC
learning.</p>

<p>At the same time, Feldman and Xiao observed that the connection between
pure private PAC learning and Littlestone dimension is imperfect. Again
borrowing results from communication complexity, they observed that the
hypothesis class \(\mathsf{Line_p}\) (which we won’t define here) has
\(\mathsf{LD}\left(\mathsf{Line_p}\right) = 2\) but
\(\mathsf{REPD}\left(\mathsf{Line_p}\right)
= \Theta(\log(p))\). In contrast, they showed that an <em>approximate</em>
private PAC learner can learn \(\mathsf{Line_p}\) using
\(O\left(\tfrac{\log(1/\beta)}{\alpha}\right)\) samples. Since this
entails no dependence on \(p\) at all, it improves the separation between
pure and approximate private PAC learning given by <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a>.</p>

<p>Let’s pause to recap what’s happened so far. We learned in the last
section that representation dimension characterizes pure private PAC
learning <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>. We learned in this section that Littlestone dimension
gives lower bounds for pure private PAC learning but, as shown by
\(\mathsf{Line_p}\), these bounds are sometimes quite loose <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>.
\(\mathsf{Thresh_T}\) shows that representation dimension does not
characterize approximate private PAC learning <strong>[<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014">FX14</a>
; <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013">BNS13b</a>]</strong>, and we
still have no privacy-specific lower bounds for approximate private
learners. So the picture now looks like this:</p>

<p><img width="400" style="margin: auto; display: block;" alt="Landscape of Private PAC, take 2" src="https://differentialprivacy.org/images/private_pac_2.png" /></p>

<p>In particular, we might still find that VC dimension characterizes
approximate private PAC learning!</p>

<h1 id="lower-bounds-for-approximate-private-pac-learning">Lower Bounds for Approximate Private PAC Learning</h1>

<p>We now dash this hope. In 2015, Bun, Nissim, Stemmer, and
Vadhan <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> gave the first nontrivial lower bound for approximate
private PAC learning. They showed that learning \(\mathsf{Thresh_T}\) has
<em>proper</em> approximate private sample complexity \(\Omega(\log^\ast(T))\) and
\(O(2^{\log^\ast(T)})\).</p>

<p>We’ll at least try to give some intuition for the presence of \(\log^\ast\)
in the lower bound. Informally, the lower bound relies on an inductive
construction of a sequence of hard problems for databases of size
\(n=1, 2,
\ldots\). The \(k^{th}\) hard problem relies on a distribution over
databases of size \(k\) whose data universe is of of size exponential in
the size of the data universe for the \((k-1)^{th}\) distribution. The
base case is the uniform distribution over the two singleton databases
\(\{0\}\) and \(\{1\}\), and they show how to inductively construct
successive problems such that a solution for the \(k^{th}\) problem
implies a solution for the \((k-1)^{th}\) problem. Unraveling the
recursive relationship between the problem domain sizes implies a
general lower bound of roughly \(\log^\ast|X|\) for domain \(X\).</p>

<p>The inclusion of \(\log^\ast\) makes this is an extremely mild lower bound.
However, \(\log^\ast(T)\) can still be arbitrarily larger than 1, so this is
the first definitive evidence that proper approximate privacy introduces
a cost over non-private PAC learning.</p>

<p>In 2018, Alon, Livni, Malliaris, and Moran <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a> extended this
\(\Omega(\log^\ast T)\) lower bound for \(\mathsf{Thresh_T}\) to <em>improper</em>
approximate privacy. More generally, they gave concrete evidence for the
importance of thresholds, which have played a seemingly outsize role in
the work so far. They did so by relating a class’ Littlestone dimension
to its ability to “contain” thresholds. Here, we say \(\mathcal{H}\)
“contains” \(m\) thresholds if there exist \(m\) (unlabeled) examples
\(x_1,\ldots,x_m\) and hypotheses \(h_1, \ldots, h_m \in \mathcal{H}\) such
that the hypotheses “behave like” thresholds on the \(m\) examples, i.e., 
\(h_i(x_j) = 1 \Leftrightarrow j \geq
i\). With this language, they imported a result from model theory to show
that any hypothesis class \(\mathcal{H}\) contains
\(\log(\mathsf{LD}\left(\mathcal{H}\right))\) thresholds. This implies
that learning \(\mathcal{H}\) is at least as hard as learning
\(\mathsf{Thresh_T}\) with
\(T = \log(\mathsf{LD}\left(\mathcal{H}\right))\). Since
\(\log^\ast(\log(\mathsf{LD}\left(\mathcal{H}\right)))
= \Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\), combining these
two results puts the following limit on private PAC learning:</p>

<blockquote>
  <p><strong>Theorem 3</strong> (<a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\).</p>
</blockquote>

<p>Littlestone dimension characterizes online PAC learning, so we now know
that online PAC learnability is necessary for private PAC learnability.
Sufficiency, however, remains an open question. This produces the
following picture, where the dotted line captures the question of
sufficiency.</p>

<p><img width="400" style="margin: auto; display: block;" alt="Landscape of Private PAC, take 3" src="https://differentialprivacy.org/images/private_pac_3.png" /></p>

<h1 id="characterizing-approximate-private-pac-learning">Characterizing Approximate Private PAC Learning</h1>

<p>Spurred by this question, several advances in private PAC learning have
appeared in the last year. First, Gonen, Hazan, and Moran strengthened
Theorem 3 by giving a constructive method for converting
<em>pure</em> private learners to online learners <a href="https://arxiv.org/abs/1905.11311" title="Alon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. NeurIPS 2019"><strong>[GHM19]</strong></a>. Their result
reaches back to the 2013 characterization of pure private learning in
terms of representation dimension by using the covering distribution to
generate a collection of “experts” for online learning. Again revisiting
\(\mathsf{Thresh_T}\), Kaplan, Ligett, Mansour, Naor, and
Stemmer <a href="https://arxiv.org/abs/1911.10137" title="Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. COLT 2020"><strong>[KLMNS20]</strong></a> significantly reduced the \(O(2^{\log^\ast(T)})\) upper
bound of <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> to just \(O((\log^\ast(T))^{1.5})\). And Alon, Beimel,
Moran, and Stemmer <a href="https://arxiv.org/abs/2003.04509" title="Noga Alon, Amos Beimel, Shay Moran, and Uri Stemmer. Closure properties for private classification and online prediction. COLT 2020"><strong>[ABMS20]</strong></a> justified this post’s focus on realizable
private PAC learning by giving a transformation from a realizable
approximate private PAC learner to an agnostic one at the cost of
slightly worse privacy and sample complexity. This built on an earlier
transformation that only applied to <em>proper</em> learners <a href="https://arxiv.org/abs/1407.2662" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled examples. SODA 2015"><strong>[BNS15]</strong></a>.</p>

<p>Finally, Bun, Livni, and Moran <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> answered the open question posed
by <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>:</p>

<blockquote>
  <p><strong>Theorem 4</strong> (<a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\).</p>
</blockquote>

<p>To prove this, <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> introduced the notion of a <em>globally stable</em>
learner and showed how to convert an online learner to a globally stable
learner to a private learner. Thus, combined with the result of <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>,
we now know that the sample complexity of private PAC learning any
\(\mathcal{H}\) is at least
\(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\) and at most
\(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\). In this sense, online
learnability characterizes private learnability.</p>

<p><img width="400" style="margin: auto; display: block;" alt="Landscape of Private PAC, final take" src="https://differentialprivacy.org/images/private_pac_4.png" /></p>

<p>Narrowing the gap between the lower and upper bounds above is an open
question. Note that we cannot hope to close the gap completely. For the
lower bound, the current \(\mathsf{Thresh_T}\) upper bound implies that no
general lower bound can be stronger than
\(\Omega((\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))^{1.5})\). For the
upper bound, there exist hypotheses classes \(\mathcal{H}\) with
\(\mathsf{VCD}\left(\mathcal{H}\right) = \mathsf{LD}\left(\mathcal{H}\right)\)
(e.g., \(\mathsf{VCD}\left(\mathsf{Point}_d\right) = \mathsf{LD}\left(\mathsf{Point}_d\right)= 1\)), so since non-private PAC learning requires
\(\Omega(\mathsf{VCD}\left(\mathcal{H}\right))\) samples, the best
possible private PAC learning upper bound is
\(O(\mathsf{LD}\left(\mathcal{H}\right))\). Nevertheless, proving either
bound remains open.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This concludes our post, and with it our discussion of this fundamental
question: the price of privacy in machine learning. We now know that in
the PAC model, proper pure private learning, improper pure private
learning, approximate private learning, and non-private learning are all
strongly separated. By the connection to Littlestone dimension, we also
know that approximate private learnability is equivalent to online
learnability. However, many questions about computational efficiency and
tight sample complexity bounds remain open.</p>

<p>As mentioned in the introduction, we focused on the clean yet widely
studied and influential model of PAC learning. Having characterized how
privacy enters the picture in PAC learning, we can hopefully convey this
understanding to other models of learning, and now approach these
questions from a rigorous and grounded point of view.</p>

<p>Congratulations to Mark Bun, Roi Livni, and Shay Moran on their best
paper award — and to the many individuals who paved the way before
them!</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to Kareem Amin and Clément Canonne for helpful feedback while
writing this post.</p></div>







<p class="date">
by Matthew Joseph <a href="https://differentialprivacy.org/private-pac/"><span class="datestr">at September 16, 2020 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/141">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/141">TR20-141 |  Candidate Tree Codes via Pascal Determinant Cubes | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Anand Kumar Narayanan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Tree codes are combinatorial structures introduced by Schulman (STOC 1993) as key ingredients in interactive coding schemes. Asymptotically-good tree codes are long known to exist, yet their explicit construction remains a notoriously hard open problem. Even proposing a plausible construction, without the burden of proof, is difficult and the defining tree code property requires structure that remains elusive. To the best of our knowledge, only one candidate appears in the literature, due to Moore and Schulman (ITCS 2014).

We put forth a new candidate for an explicit asymptotically-good tree code. Our construction is an extension of the vanishing rate tree code by Cohen-Haeupler-Schulman (STOC 2018) combined with a vanishing distance tree code by Gelles et al. (SODA 2016). The correctness of our construction relies on a conjecture that we introduce on certain Pascal determinants indexed by the points of the Boolean hypercube. We furnish evidence supporting our conjecture through numerical computation, combinatorial arguments from planar path graphs and based on well-studied heuristics from arithmetic geometry.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/141"><span class="datestr">at September 16, 2020 05:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/140">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/140">TR20-140 |  Optimal Testing of Discrete Distributions with High Probability | 

	Ilias Diakonikolas, 

	Themis Gouleakis, 

	Daniel Kane, 

	John Peebles, 

	Eric Price</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the problem of testing discrete distributions with a focus on the high probability regime.
Specifically, given samples from one or more discrete distributions, a property $\mathcal{P}$, and 
parameters $0&lt; \epsilon, \delta &lt;1$, we want to distinguish {\em with probability at least $1-\delta$}
whether these distributions satisfy $\mathcal{P}$ or are $\epsilon$-far from $\mathcal{P}$
in total variation distance. Most prior work in distribution testing studied the constant confidence case 
(corresponding to $\delta = \Omega(1)$), and provided sample-optimal testers for a range of properties.
While one can always boost the confidence probability of any such tester by black-box amplification, 
this generic boosting method typically leads to sub-optimal sample bounds.

Here we study the following broad question: For a given property $\mathcal{P}$, can we {\em characterize} 
the sample complexity of testing $\mathcal{P}$ as a function of all relevant problem parameters, 
including the error probability $\delta$? Prior to this work, uniformity testing was the only statistical task
whose sample complexity had been characterized in this setting. As our main results,
we provide the first algorithms for closeness and independence testing that are sample-optimal, within 
constant factors, as a function of all relevant parameters. We also show matching
information-theoretic lower bounds on the sample complexity of these problems.
Our techniques naturally extend to give optimal testers for  related problems. To illustrate the generality of our methods, 
we give optimal algorithms for testing collections of distributions and testing closeness with unequal sized samples.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/140"><span class="datestr">at September 16, 2020 05:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4962">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4962">In a world like this one, take every ally you can get</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 17):</span></strong> Several people, here and elsewhere, wrote to tell me that while they completely agreed with my strategic and moral stance in this post, they think that it’s the ads of <a href="https://www.youtube.com/channel/UC03-Q9vq-JyiStTnqasADVg">Republican Voters Against Trump</a>, rather than the Lincoln Project, that have been most effective in changing Trump supporters’ minds.  So please consider <a href="https://rvat.org/donate/">donating to RVAT</a> instead or in addition!  In fact, what the hell, I’ll match donations to RVAT up to $1000.</p>



<p></p><hr /><p></p>



<p>For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work.  Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless covid deaths, and an election in 48 days that <em>if I didn’t know such things were impossible in America</em> would seem <a href="https://www.washingtonpost.com/health/2020/09/14/michael-caputo-coronavirus-cdc/">likely</a> to produce a terrifying standoff as a despot and millions of his armed loyalists refuse to cede control.  Meanwhile, catalysts for my relatively functional periods have included teaching my undergrad <a href="https://www.scottaaronson.com/qclec.pdf">quantum information class</a>, Zoom calls with my students, <a href="https://www.nature.com/articles/s41550-020-1174-4">life on Venus?!?</a> (my guess is no, but almost entirely due to priors), learning new math (fulfilling a decades-old goal, I’m finally working my way through Paul Cohen’s celebrated <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">proof</a> of the independence of the Continuum Hypothesis—more about that later!).</p>



<p>Of course, when you feel crushed by the weight of the world’s horribleness, it improves your mood to be able even just to prick the horribleness with a pin.  So I was gratified that, in response to a <a href="https://www.scottaaronson.com/blog/?p=4942">previous post</a>, <em>Shtetl-Optimized</em> readers contributed at least $3,000, the first $2,000 of which I matched, mostly to the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> but a little to the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>.</p>



<p>Alas, a <a href="https://www.scottaaronson.com/blog/?p=4942#comment-1856922">commenter</a> was unhappy with the latter:</p>



<blockquote class="wp-block-quote"><p>Lincoln Project? Really? … Pushing the Overton window rightward during a worldwide fascist dawn isn’t good. I have trouble understanding why even extremely smart people have trouble with this sort of thing.</p></blockquote>



<p>Since this is actually important, I’d like to spend the rest of this post responding to it.</p>



<p>For me it’s simple.</p>



<p>What’s the goal right now?  To defeat Trump.  In the US right now, that’s the prerequisite to <strong>every other</strong> sane political goal.</p>



<p>What will it take to achieve that goal? Turnout, energizing the base, defending the election process … but also, if possible, <em>persuading a sliver of Trump supporters in swing states to switch sides</em>, or at least vote third party or abstain.</p>



<p>Who is actually effective at that goal?  Well, no one knows for sure.  But while I thought the Biden campaign had some semi-decent ads, the Lincoln Project’s best stuff seems <a href="https://www.youtube.com/watch?v=2uLJkpH__os">better</a> to me, just savagely good.</p>



<p><em>Why</em> are they effective?  The answer seems obvious: for the same reason why a jilted ex is a more dangerous foe than a stranger.  If <em>anyone</em> understood how to deprogram a Republican from the Trump cult, who would it be: Alexandria Ocasio-Cortez, or a fellow Republican who successfully broke from the cult?</p>



<p>Do I agree with the Lincoln Republicans about most of the “normal” issues that Americans once argued about?  Not at all.  Do I hold them, in part, morally responsible for creating the preconditions to the current nightmare?  Certainly.</p>



<p>And should any of that cause me to boycott them? Not in my moral universe.  If Churchill and FDR could team up with Stalin, then surely we in the Resistance can temporarily ally ourselves with the rare Republicans who chose their stated principles over power when tested—their very rarity attesting to the nontriviality of their choice.</p>



<p>To my mind, turning one’s back on would-be allies, in a conflict whose stakes obviously overshadow what’s bad about those allies, is simultaneously one of the dumbest <em>and</em> the ugliest things that human beings can do.  It abandons reason for moral purity and ends up achieving neither.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4962"><span class="datestr">at September 16, 2020 07:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-report-new-discovery-about-the-dodecahedron-20200831/">Closed quasigeodesics on the dodecahedron</a> (<a href="https://mathstodon.xyz/@11011110/104785420838924796">\(\mathbb{M}\)</a>), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, <a href="https://arxiv.org/abs/1811.04131">arXiv:1811.04131</a>, <a href="https://doi.org/10.1080/10586458.2020.1712564">doi:10.1080/10586458.2020.1712564</a>. I saw this on Numberphile a few months back (video linked in article) but now it’s on <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://blog.graphicine.com/lorenz-stoer-geometric-landscapes/">Lorenz Stöer’s geometric landscapes</a> (<a href="https://mathstodon.xyz/@11011110/104799765760054680">\(\mathbb{M}\)</a>). <a href="https://11011110.github.io/blog/2014/09/30/linkage-for-end.html">In 2014 I linked a different page</a> with a few of Stöer’s 16th-century proto-surrealist combinations of landscape and geometry, but they were black and white. This one has more of them, in color.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ideal_polyhedron">Ideal polyhedron</a>, a polyhedron in hyperbolic space with all vertices at infinity, and <a href="https://en.wikipedia.org/wiki/Sylvester%E2%80%93Gallai_theorem">Sylvester–Gallai theorem</a>, that every finite set of points in the Euclidean plane has a line that either passes through all of them or through exactly two of them. Both newly promoted to Good Article status on Wikipedia (<a href="https://mathstodon.xyz/@11011110/104803212564257211">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://merveilles.town/@neauoire/104779168858836970">Escherian wiener-dog Cerberus fetches three impossible things</a>.</p>
  </li>
  <li>
    <p>Flamebait post of the day: <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">Why mathematicians should stop naming things after each other</a> (<a href="https://mathstodon.xyz/@11011110/104813883920721252">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24385389">via</a>).  For once the via-link discussion is worth reading (main point: the alternative, using common English words to describe specialized technical concepts, can be even more confusing).</p>
  </li>
  <li>
    <p>Early Renaissance painter Piero della Francesca was also an accomplished mathematician, and his book on polyhedra, <em>De quinque corporibus regularibus</em> (subject of a <a href="https://en.wikipedia.org/wiki/De_quinque_corporibus_regularibus">new Wikipedia article</a>; <a href="https://mathstodon.xyz/@11011110/104820183749646319">\(\mathbb{M}\)</a>) has an interesting history that deserves to be better known. Rediscovery of the mathematics of Archimedes! “First full-blown case of plagiarism in the history of mathematics” (by Luca Pacioli, in Divina proportione)! Maybe owned by John Dee! Long lost and found centuries later in the Vatican Library!</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2020/08/30/moonlighting/">Peter Cameron gives a nice roundup of two recent online conferences on group theory and combinatorics</a> (<a href="https://mathstodon.xyz/@11011110/104833470202099899">\(\mathbb{M}\)</a>) that he attended more-or-less simultaneously, something that would have been impossible for physical conferences. The parts on synchronizing automata and twin-width particularly caught my attention as stuff I should look up and find out more about.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/RodBogart/status/455123609195802624">An hourglass that demonstrates Archimedes’ theorem that the volume of a cylinder is the sum of the volumes of its inscribed sphere and cone</a> (<a href="https://mathstodon.xyz/@mjd/104836143207567957">\(\mathbb{M}\)</a>), from Rod Bogart’s twitter feed.</p>
  </li>
  <li>
    <p>The <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">hexagon-minimizing simple bipartite polyhedra of my recent blog post</a> make nice shapes when converted to <a href="https://arxiv.org/abs/0912.0537">simple orthogonal polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104842837266010559">\(\mathbb{M}\)</a>): a squared-off amphitheater with L-shaped terraces of increasing length as they rise, or a diagonal staircase with congruent L-shaped steps. In each case the outer \(2n\)-gon is the underside of the polygon and the inner cycles are the horizontal faces.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/orthogonal-eberhard.svg" alt="Hexagon-minimizing simple bipartite polyhedra represented as simple orthogonal polyhedra" /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11933">Open is not forever: a study of vanished open access journals</a> (<a href="https://mathstodon.xyz/@11011110/104850663521092974">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24422593">via</a>, <a href="https://www.sciencemag.org/news/2020/09/dozens-scientific-journals-have-vanished-internet-and-no-one-preserved-them">via</a>). This study shows the need for systematic archiving and redundant copying of online open journals, but I suspect that the problem for small hand-run print-based journals without much library pickup might be much worse.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2018/10/a-prickly-structure-made-of-70000-reusable-hexapod-particles/">A prickly structure made of 70,000 reusable hexapod particles</a> (<a href="https://mathstodon.xyz/@11011110/104856358909259046">\(\mathbb{M}\)</a>). Sort of like those <a href="https://en.wikipedia.org/wiki/Tetrapod_(structure)">seawalls they build by jumbling together giant concrete caltrops</a>, only with pieces that are not quite so big and with usable spaces left void within it. Sometimes the article says “hexapod” and sometimes “decapod”; the pictures appear to show structures that mix two different kinds of particle.</p>
  </li>
  <li>
    <p><em><a href="http://math.sfsu.edu/beck/ct/board.php">Combinatorial Theory</a></em> (<a href="https://mathstodon.xyz/@bremner/104859257534118058">\(\mathbb{M}\)</a>, <a href="https://twitter.com/wtgowers/status/1305253478047068160">see also</a>), a new open-access combinatorics journal formed from the mass resignation of the Elsevier <em>JCTA</em> editorial board.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2020/09/14/us/caputo-virus.html">Trump officials are now telling their supporters to buy guns and ammunition to use against scientists for being anti-Trump</a>. <a href="https://thehill.com/policy/healthcare/516319-top-hhs-official-accuses-scientists-of-plotting-against-trump-tells">No, seriously</a> (<a href="https://mathstodon.xyz/@11011110/104865069277930004">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><em><a href="https://www.cambridge.org/us/academic/subjects/mathematics/recreational-mathematics/origametry-mathematical-methods-paper-folding">Origametry: Mathematical Methods in Paper Folding</a></em> (<a href="https://mathstodon.xyz/@11011110/104870325812873444">\(\mathbb{M}\)</a>), new book coming out October 31 by Tom Hull. I haven’t seen anything more than the blurb linked here and the <a href="https://books.google.com/books?id=LdX7DwAAQBAJ">limited preview on Google Books</a>, but it looks interesting and worth waiting for.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/15/linkage.html"><span class="datestr">at September 15, 2020 10:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17600">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/">Ken Regan Turned 61</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>Happy birthday to Ken</em><br />
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/collage-2/" rel="attachment wp-att-17602"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2020/09/collage.jpg?w=600&amp;h=450" class="aligncenter size-full wp-image-17602" height="450" /></a></td>
</tr>
</tbody>
</table>
<p>Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled <i>On the separation of complexity classes</i>. He was the PhD student of Dominic Welsh who was a student of John Hammersley.</p>
<p>Today I would like to wish Ken a happy birthday.</p>
<p><span id="more-17600"></span></p>
<p>He is now 61 years young. I hope you will join me and wish him many more birthdays. His age is <a href="https://en.wikipedia.org/wiki/61_(number)">special</a> for many reasons:</p>
<ul>
<li>It is a twin prime.</li>
<li>It is equal to <img src="https://s0.wp.com/latex.php?latex=%7B5%5E%7B2%7D+%2B+6%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{5^{2} + 6^{2}}" class="latex" title="{5^{2} + 6^{2}}" />.</li>
<li>It is the ninth Mersenne prime: <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B61%7D+-+1+%3D+2%2C305%2C843%2C009%2C213%2C693%2C951%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{61} - 1 = 2,305,843,009,213,693,951}" class="latex" title="{2^{61} - 1 = 2,305,843,009,213,693,951}" />.</li>
</ul>
<p>There are three big <b>I’s</b> in his life. Let’s talk about two of them.</p>
<h2>Interest in Cricket</h2>
<p>Ken loves sports in general and especially cricket. Last Sunday he told me he watched his Bills win their first NFL game while he watched a cricket match. I have no idea how cricket works, but here is Ken’s <a href="https://cse.buffalo.edu/~regan/Writing/CricketBaseball.html">explanation</a>: <i>Are Cricket and Baseball sister games?</i></p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/cr1/" rel="attachment wp-att-17603"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2020/09/cr1.jpg?w=600&amp;h=450" class="aligncenter size-full wp-image-17603" height="450" /></a></td>
</tr>
</tbody>
</table>
<ul>
<li>In a baseball game you see pitchers on the field.<br />
In a cricket match you see fielders on the pitch.</li>
<li>In baseball, a bad delivery is called a “Ball”.<br />
In cricket, it’s a “No Ball”.</li>
<li>In baseball, if a batter carries his bat, he’s out.<br />
In cricket, the batsmen always carry their bat, and an opening batsman who “carries his bat” is never out.</li>
<li>In baseball, an innings is called a half-inning.<br />
In cricket, an inning is called an innings.</li>
<li>In baseball, a batter hit by a pitched ball gets a free pass to First Base.<br />
In cricket, such a batter can be Out Leg Before Wicket.</li>
<li>In baseball, if a ball is caught over the boundary, “yer out!”<br />
In cricket, you score 6 runs.</li>
<li>In baseball, when a batter “walks”, he gets a free pass to first and is not out.<br />
In cricket, it means the batsman declares himself out before the umpire has a chance to make the call. This classic show of sportsmanship is considered unsportsmanlike in baseball.<p></p>
<h2>Interest in Chess</h2>
<p>When the chess world wants to know if someone has cheated, they call Ken. He is an international chess master, and has worked on stopping cheating for years. It is important these days, since most tournaments are now online. And cheating is easier when no one is directly able to watch you. Ken is busy.</p>
<p>Let’s look at the cheating problem. Suppose that Alice and Bob are playing an online game of chess. Alice makes her own moves, but she wonders if Bob could be cheating. He could be using advice from another “player”, Sally. There are several points:</p>
<ol>
<li>Sally is a stronger player than anyone—she can easily beat Alice and Bob.</li>
<li>Sally not only says “here is my move”—she will sometimes give several good moves.</li>
<li>Sally is a program that is deterministic—given a position she gives the same answer.The issue for Ken is: When Alice played Bob did Bob make the moves or did he consult Sally?</li>
</ol>
<p>There are many complexities:</p>
<ol>
<li>What if Bob agreed with all Sally’s moves? Then he certainly did cheat.</li>
<li>What if Bob was just lucky and played above his strength? Then he did not cheat.</li>
<li>What if Bob used Sally for some positions but not others? Then he did cheat, but it may be hard to be sure.</li>
<li>And so on.</li>
</ol>
<p>What Ken has done is create both a theory and programs to determine whether Bob did indeed cheat. I find the general problem of telling if one cheats online at chess to be fascinating. See us <a href="https://rjlipton.wordpress.com/2014/06/18/the-problem-of-catching-chess-cheaters/">before</a> for more details and also see <a href="http://www.uschess.org/index.php/June/How-To-Catch-A-Chess-Cheater-Ken-Regan-Finds-Moves-Out-Of-Mind.html">this</a>.</p>
<h2>Open Problems</h2>
<p>Ken is one of the nicest people I know. Hope he has many more birthdays and many more twin primes.</p></li>
</ul></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/"><span class="datestr">at September 15, 2020 04:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/">Postdoc position in Theoretical Computer Science/Foundations of AI at Aarhus University, Denmark (apply by October 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark.</p>
<p>Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record.</p>
<p>Website: <a href="https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/">https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/</a><br />
Email: iannis@cs.au.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/"><span class="datestr">at September 14, 2020 05:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6364597152143042105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html">An interesting serendipitous number</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Last seek I blogged about two math problems of interest to me <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>.</p><p>One of them two people posted answers, which was great since I didn't know how to solve them and now I do. Yeah! I blogged about that <a href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html">here</a>.</p><p><br /></p><p>The other problem got no comments, so I suppose it was of interest to me but not others. I was interested in it because the story behind it is interesting, and the answer is interesting.</p><p><br /></p><p>it is from the paper </p><p>An interesting and serendipitous number by John Ewing and Ciprian Foias, which is a chapter in the wonderful book </p><p>Finite vs Infinite: Contributions to an eternal dilemma</p><p>Here is the story, I paraphrase the article (I'll give pointers  later).</p><p>In the mid 1970's a student asked Ciprian about the following math-competition problem:</p><p>x(1)&gt;0    x(n+1) =  (1 + (1/x(n)))^n. For which x(1) does x(n) --&gt; infinity?</p><p>It turned out this was a misprint. The actual problem was</p><p>x(1)&gt;0  x(n+1)=(1+(1/x(n))^{x(n)}. For which x(1) does x(n) --&gt; infinity.</p><p><br /></p><p>The actual math-comp problem  (with exp x(n)) is fairly easy (I leave it to you.) But this left the misprinted problem (with exp n).  Crispian proved that there is exactly ONE x(1) such that x(n)--&gt; infinity. </p><p>Its approx 1.187... and may be trans.</p><p><br /></p><p>I find the story and the result interesting, but the proof is to long for a blog post.</p><p>I tried to find the article online and could not. A colleague found the following:</p><p><br /></p><p>A preview of the start of the article <a href="https://link.springer.com/chapter/10.1007/978-1-4471-0751-4_8">here</a></p><p>Wikipedia Page on the that number, called the Foias constant, <a href="https://en.wikipedia.org/wiki/Foias_constant">here</a></p><p>Mathworld page on that number <a href="https://mathworld.wolfram.com/FoiasConstant.html">here</a></p><p>Most of the article but skips two pages <a href="https://books.google.com/books?id=Bjb0BwAAQBAJ&amp;pg=PA119&amp;lpg=PA119&amp;dq=serendipitous+number+John+Ewing+and+Ciprian+Foias&amp;source=bl&amp;ots=4tn1sk3XEA&amp;sig=ACfU3U3GM9VtlyWxTjq302E5Uf7Tmr49Hw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiewLSF9OjrAhVkmXIEHRYEAMA4ChDoATAAegQICRAB#v=onepage&amp;q=serendipitous%20number%20John%20Ewing%20and%20Ciprian%20Foias&amp;f=false">here</a></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html"><span class="datestr">at September 14, 2020 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/">Broadcast from Agreement and Agreement from Broadcast</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post, we highlight the connection between Broadcast and Agreement in the synchronous model. Broadcast and Agreement: How can you implement one from the other? We defined Agreement and Broadcast in a previous post, here is a recap: Agreement A set of $n$ nodes where each node $i$ has...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/"><span class="datestr">at September 14, 2020 02:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
