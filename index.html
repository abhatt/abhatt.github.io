<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="http://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 20, 2021 01:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/146">TR21-146 |  Sample-Based Proofs of Proximity | 

	Guy Goldberg, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose we have random sampling access to a huge object, such as a graph or a database.
Namely, we can observe the values of \emph{random} locations in the object, say random records in the database or random edges in the graph.
We cannot, however, query locations of our choice. Can we verify complex properties of the object using only this restricted sampling access?

In this work, we initiate the study of \emph{sample-based} proof systems, where the verifier is extremely constrained; Given an input, the verifier can only obtain samples of uniformly random and i.i.d. locations in the input string, together with the values at those locations. The goal is verifying complex properties in sublinear time, using only this restricted access.
Following the literature on Property Testing and on Interactive Proofs of Proximity (IPPs), we seek proof systems where the verifier accepts every input that has the property, and with high probability rejects every input that is \emph{far} from the property.

We study both interactive and non-interactive sample-based proof systems, showing:

	- On the positive side, our main result is that rich families of properties / languages have sub-linear sample-based interactive proofs of proximity (SIPPs).
	We show that every language in $\mathcal{NC}$ has a SIPP, where the sample and communication complexities, as well as the verifier's running time, are $\widetilde{O}(\sqrt{n})$, and with polylog(n) communication rounds.
	We also show that every language that can be computed in polynomial-time and bounded-polynomial space has a SIPP, where the sample and communication complexities of the protocol, as well as the verifier's running time are roughly $\sqrt{n}$, and with a constant number of rounds.
	
	This is achieved by constructing a reduction protocol from SIPPs to IPPs.
	With the aid of an untrusted prover, this reduction enables a restricted, sample-based verifier to simulate an execution of a (query-based) IPP, even though it cannot query the input.
	Applying the reduction to known query-based IPPs yields SIPPs for the families described above.
	
	- We show that every language with an adequate (query-based) property tester has a 1-round SIPP with \emph{constant} sample complexity and logarithmic communication complexity.
	One such language is equality testing, for which we give an explicit and simple SIPP.
	
	- On the negative side, we show that \emph{interaction} can be essential:
	we prove that there is no \emph{non}-interactive sample-based proof of proximity for equality testing.
	
	- Finally, we prove that \emph{private coins} can dramatically increase the power of SIPPs.
	We show a strong separation between the power of public-coin SIPPs and private-coin SIPPs for Equality Testing.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/146"><span class="datestr">at October 19, 2021 08:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/">Some Statistical Gamma Fun</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Nothing takes place in the world whose meaning is not that of some maximum or minimum. — Leonhard Euler</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/kaspermueller/" rel="attachment wp-att-19221"><img width="125" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/KasperMueller-150x150.jpg?resize=125%2C125&amp;ssl=1" class="alignright wp-image-19221" height="125" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cantor’s Paradise <a href="https://www.cantorsparadise.com/inventing-mathematics-a33cc9d2732b">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Kasper Müller is a mathematics and data science writer for <a href="https://medium.com/">Medium</a>, where he contributes primarily to the blogs <a href="https://www.cantorsparadise.com/">Cantor’s Paradise</a> and <a href="https://towardsdatascience.com/">Towards Data Science</a>. He wrote a nice <a href="https://www.cantorsparadise.com/the-beautiful-gamma-function-and-the-genius-who-discovered-it-8778437565dc">article</a> last April titled, “The Beautiful Gamma Function and the Genius Who Discovered It.”</p>
<p>
Today we discuss the relevance of the gamma function to statistics and use statistics to suggest a new kind of estimate for it.</p>
<p>
The “Genius” that Müller refers to is Leonhard Euler. Euler proved that for all integers <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 0}" class="latex" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%21+%3D+%5Cint_0%5E1+%28-%5Cln+s%29%5En+%3D+%5Cint_0%5E%5Cinfty+t%5En+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n! = \int_0^1 (-\ln s)^n = \int_0^\infty t^n e^{-t} dt, " class="latex" /></p>
<p>where the latter equation uses the substitution <img src="https://s0.wp.com/latex.php?latex=%7Bs+%3D+e%5E%7B-t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s = e^{-t}}" class="latex" />. The right-hand side produces a value for any complex number <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+x+%2B+iy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = x + iy}" class="latex" /> in place of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> provided <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3E+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x &gt; -1}" class="latex" />. This leads to the formal definition </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28z%29+%3D+%5Cint_0%5E%5Cinfty+t%5E%7Bz-1%7D+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma(z) = \int_0^\infty t^{z-1} e^{-t} dt, " class="latex" /></p>
<p>whose analytic extension is defined everywhere except for <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+0%2C+-1%2C+-2%2C+-3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = 0, -1, -2, -3,\dots}" class="latex" />. Because <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> has no zeroes, its reciprocal is an entire function. One neat value is <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" />. We will be mainly concerned with ratios of two values of <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" />.</p>
<p>
</p><p></p><h2> What is Gamma For? </h2><p></p>
<p></p><p>
For all <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> except the non-positive integers, <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> obeys the formula </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+1)}{\Gamma(z)} = z. " class="latex" /></p>
<p>Of course, this follows from <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29+%3D+%28z-1%29%21%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z) = (z-1)!}" class="latex" /> for positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" />. Also </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%29%7D+%3D+%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%2B1%29%7D%5Ccdot%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+%28z%2B1%29z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+2)}{\Gamma(z)} = \frac{\Gamma(z+2)}{\Gamma(z+1)}\cdot\frac{\Gamma(z+1)}{\Gamma(z)} = (z+1)z. " class="latex" /></p>
<p>In general, for all <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a &gt; 0}" class="latex" />, <a name="approx"></a></p><a name="approx">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2Ba%29%7D%7B%5CGamma%28z%29%7D+%5Csim+z%5Ea+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+a)}{\Gamma(z)} \sim z^a \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="approx"></a> but there is a discrepancy. This and the lack of a simple explicit formula for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> at all have always made the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function seem opaque to me. Two notable values are <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B3%7D%7B2%7D%29+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2}}" class="latex" />.</p>
<p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function is not even the only uniformly continuous interpolation of the factorial function. It is the unique one whose logarithm is a convex function. This is the first of many reasons given in Müller’s article for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> to be <em>salient</em> and beautiful, culminating in its relation to the Riemann zeta function given by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28%5Cfrac%7Bs%7D%7B2%7D%29%5Czeta%28s%29%7D%7B%5Cpi%5E%7Bs%2F2%7D%7D+%3D+%5Cfrac%7B%5CGamma%28%5Cfrac%7B1-s%7D%7B2%7D%29%5Czeta%281-s%29%7D%7B%5Cpi%5E%7B%281-s%29%2F2%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(\frac{s}{2})\zeta(s)}{\pi^{s/2}} = \frac{\Gamma(\frac{1-s}{2})\zeta(1-s)}{\pi^{(1-s)/2}}. " class="latex" /></p>
<p>Yet the log-convex uniqueness was proved only 99 years ago, and none of these tell me at a flash what the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function <b>is</b>. </p>
<p>
What is the simplest label for its corner of the sky? The leading example is the formula for the volume of a sphere of radius <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> dimensions: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++V_n+%3D+%5Cfrac%7B%5Cpi%5E%7Bn%2F2%7D%7D%7B%5CGamma%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7Dr%5En.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  V_n = \frac{\pi^{n/2}}{\Gamma(n + \frac{1}{2})}r^n. " class="latex" /></p>
<p>But I wonder whether a different application is more fundamental. Since we are dealing with <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a = \frac{1}{2}}" class="latex" /> already here, let us define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%3D+%5Cfrac%7B%5CGamma%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D%7B%5CGamma%28z%29%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) = \frac{\Gamma(z+\frac{1}{2})}{\Gamma(z)}. " class="latex" /></p>
<p>Noting <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%29+%5Csim+z%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z) \sim z^{1/2}}" class="latex" /> via (<a href="https://rjlipton.wpcomstaging.com/feed/#approx">1</a>), this is a tweak of the square-root function. Here are some values of it:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5CGamma_%7B1%2F2%7D%281%29+%26%3D%26+%5Cfrac%7B%5CGamma%281.5%29%7D%7B%5CGamma%281%29%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%2F2%7D%7B1%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%5C%5C+%5CGamma_%7B1%2F2%7D%282%29+%26%3D%26+%5Cfrac%7B%5CGamma%282.5%29%7D%7B%5CGamma%282%29%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%2F4%7D%7B1%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%7D%7B4%7D%5C%5C+%5CGamma_%7B1%2F2%7D%283%29+%26%3D%26+%5Cfrac%7B%5CGamma%283.5%29%7D%7B%5CGamma%283%29%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%2F8%7D%7B2%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%7D%7B16%7D%5C%5C+%5CGamma_%7B1%2F2%7D%284%29+%26%3D%26+%5Cfrac%7B%5CGamma%284.5%29%7D%7B%5CGamma%284%29%7D+%3D+%5Cfrac%7B105%5Csqrt%7B%5Cpi%7D%2F16%7D%7B6%7D+%3D+%5Cfrac%7B35%5Csqrt%7B%5Cpi%7D%7D%7B32%7D%5C%5C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  \Gamma_{1/2}(1) &amp;=&amp; \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}/2}{1} = \frac{\sqrt{\pi}}{2}\\ \Gamma_{1/2}(2) &amp;=&amp; \frac{\Gamma(2.5)}{\Gamma(2)} = \frac{3\sqrt{\pi}/4}{1} = \frac{3\sqrt{\pi}}{4}\\ \Gamma_{1/2}(3) &amp;=&amp; \frac{\Gamma(3.5)}{\Gamma(3)} = \frac{15\sqrt{\pi}/8}{2} = \frac{15\sqrt{\pi}}{16}\\ \Gamma_{1/2}(4) &amp;=&amp; \frac{\Gamma(4.5)}{\Gamma(4)} = \frac{105\sqrt{\pi}/16}{6} = \frac{35\sqrt{\pi}}{32}\\ \end{array} " class="latex" /></p>
<p>
Here is the significance:</p>
<blockquote><p><b> </b> <em> For integer <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 1}" class="latex" />, the expected Euclidean norm of a vector of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent samples from the standard Gaussian distribution is <a name="gammahalf"></a></em></p><em><a name="gammahalf">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csqrt%7B2%7D%5Ccdot%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sqrt{2}\cdot\Gamma_{1/2}(\frac{n}{2}). \ \ \ \ \ (2)" class="latex" /></p>
</a></em><p><em><a name="gammahalf"></a> </em>
</p></blockquote>
<p></p><p>
That’s it: Gamma gives the norm of Gaussians. The norm is of order <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{n}}" class="latex" /> but not exactly. The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function gives it <em>exactly</em>.</p>
<p>
</p><p></p><h2> An Inferior But Curious Estimate </h2><p></p>
<p></p><p>
The norm of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent Gaussians is called the <a href="https://en.wikipedia.org/wiki/Chi_distribution">chi distribution</a>. Its square is the better-known <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>. This idea is used in the statistical <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared</a> test, but what follows is simpler.</p>
<p>
We let <img src="https://s0.wp.com/latex.php?latex=%7BX%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X^2}" class="latex" /> stand for the square norm divided by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> stands for the Euclidean norm divided by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. From (<a href="https://rjlipton.wpcomstaging.com/feed/#gammahalf">2</a>) we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BX%5D+%3D+%5Csqrt%7B%5Cfrac%7B2%7D%7Bn%7D%7D%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E[X] = \sqrt{\frac{2}{n}}\Gamma_{1/2}(\frac{n}{2}). " class="latex" /></p>
<p>
We will estimate <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> a different way and use that to estimate <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}}" class="latex" />. First we note that since the vector entries <img src="https://s0.wp.com/latex.php?latex=%7Bz_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_i}" class="latex" /> are independent and normally distributed, we have the exact values</p>
<p><br /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++E%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+E%5Bz_i%5E2%5D+%3D+1%5C%5C+Var%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%5E2%7D+%5Csum_%7Bi%3D1%7D%5En+Var%5Bz_i%5E2%5D+%3D+%5Cfrac%7B1%7D%7Bn%5E2%7D%5Csum_%7Bi%3D1%7D%5En+%28E%5Bz_i%5E4%5D+-+E%5Bz_i%5D%5E2%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%283+-+1%29+%3D+%5Cfrac%7B2%7D%7Bn%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  E[X^2] &amp;=&amp; \frac{1}{n}\sum_{i=1}^n E[z_i^2] = 1\\ Var[X^2] &amp;=&amp; \frac{1}{n^2} \sum_{i=1}^n Var[z_i^2] = \frac{1}{n^2}\sum_{i=1}^n (E[z_i^4] - E[z_i]^2) = \frac{1}{n}(3 - 1) = \frac{2}{n}. \end{array} " class="latex" /></p>
<p></p><p><br />
Since we have <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X^2]}" class="latex" />, computing either <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> suffices to get the other, by the relation <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+E%5BX%5E2%5D+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = E[X^2] - E[X]^2}" class="latex" />. Our also having <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X^2]}" class="latex" /> enables estimating <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> via the <a href="http://www.phidot.org/software/mark/docs/book/pdf/app_2.pdf">delta</a> <a href="https://en.wikipedia.org/wiki/Delta_method">method</a>, in a particular form I noticed <a href="https://stats.stackexchange.com/questions/10337/operatornamevarx2-if-operatornamevarx-sigma2/383603">here</a>. The derivation requires no special properties of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />: <a name="est2"></a></p><a name="est2">
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Var%5BX%5E2%5D+%5Capprox+4E%5BX%5D%5E2+Var%5BX%5D+-+Var%5BX%5D%5E2.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Var[X^2] \approx 4E[X]^2 Var[X] - Var[X]^2. \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="est2"></a></p><p><br />
For our particular <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+1+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = 1 - E[X]^2}" class="latex" />, this yields a quadratic equation in <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y = E[X]^2}" class="latex" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D+%5Cdoteq+4y%281+-+y%29+-+%281-y%29%5E2%2C+%5Cquad%5Ctext%7Bso%7D%5Cquad+5y%5E2+-+6y+%2B+1+%2B+%5Cfrac%7B2%7D%7Bn%7D+%3D+0+%5Cquad%5Ctext%7Bso%7D%5Cquad+y+%3D+%5Cfrac%7B6+%2B+%5Csqrt%7B16+-+%5Cfrac%7B40%7D%7Bn%7D%7D%7D%7B10%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n} \doteq 4y(1 - y) - (1-y)^2, \quad\text{so}\quad 5y^2 - 6y + 1 + \frac{2}{n} = 0 \quad\text{so}\quad y = \frac{6 + \sqrt{16 - \frac{40}{n}}}{10}. " class="latex" /></p>
<p>This yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D%5CGamma_%7B1%2F2%7D%5E2%28%5Cfrac%7Bn%7D%7B2%7D%29+%5Cdoteq+%5Cfrac%7B3%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4%7D%7B25%7D+-+%5Cfrac%7B2%7D%7B5n%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n}\Gamma_{1/2}^2(\frac{n}{2}) \doteq \frac{3}{5} + \sqrt{\frac{4}{25} - \frac{2}{5n}}. " class="latex" /></p>
<p>
Changing variables to <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+%5Cfrac%7Bn%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = \frac{n}{2}}" class="latex" /> and rearranging, we get the estimate </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%5Cdoteq+%5Csqrt%7B%5Cfrac%7B3z%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4z%5E2%7D%7B25%7D+-+%5Cfrac%7Bz%7D%7B5%7D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) \doteq \sqrt{\frac{3z}{5} + \sqrt{\frac{4z^2}{25} - \frac{z}{5}}}. " class="latex" /></p>
<p>
It has been traditional to estimate what we would call <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z+\frac{1}{2})}" class="latex" /> instead, so putting <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+z+%2B+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = z + \frac{1}{2}}" class="latex" /> we finally get:</p>
<p>
<a name="estimate"></a></p><a name="estimate">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28x%2B1%29%7D%7B%5CGamma%28x%2B%5Cfrac%7B1%7D%7B2%7D%29%7D+%5Csim+%5Csqrt%7B0.6x+%2B+0.3+%2B+0.2%5Csqrt%7B8x%5E2+-+2x+-+1.5%7D%7D+%7E.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(x+1)}{\Gamma(x+\frac{1}{2})} \sim \sqrt{0.6x + 0.3 + 0.2\sqrt{8x^2 - 2x - 1.5}} ~. \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="estimate"></a></p>
<p>
As an estimate, this is barely competitive with the simple <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bx+%2B+0.25%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{x + 0.25}}" class="latex" /> and far inferior to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%5E2+%2B+0.5x+%2B+0.125%29%5E%7B1%2F4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  (x^2 + 0.5x + 0.125)^{1/4}, " class="latex" /></p>
<p>which is the first of several estimates of the form <img src="https://s0.wp.com/latex.php?latex=%7Bp_k%28x%29%5E%7B1%2F2k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p_k(x)^{1/2k}}" class="latex" /> <a href="https://www.sciencedirect.com/science/article/pii/S0895717710001317">given</a> by Cristinel Mortici in 2010. But it is curious that we got a formula with nested radicals and non-dyadic coefficients from a simple statistical estimate. It makes us wonder whether formulas with nested radicals can be tuned for greater accuracy, and whether this might knock back to statistical estimation.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can vectors of Gaussian variables be leveraged to say further interesting things about the gamma function and its applications? What are your favorite properties of the gamma function?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/"><span class="datestr">at October 19, 2021 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/145">TR21-145 |  Revisiting a Lower Bound on the Redundancy of Linear Batch Codes | 

	Omar Alrabiah, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent work of Li and Wootters (2021) shows a redundancy lower bound of $\Omega(\sqrt{Nk})$ for systematic linear $k$-batch codes of block length $N$ by looking at the $O(k)$ tensor power of the dual code. In this note, we present an alternate proof of their result via a linear independence argument on a collection of polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/145"><span class="datestr">at October 19, 2021 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=102">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/">Thursday Oct 21st — Maxim Raginsky from UIUC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, Oct</strong> <strong>21</strong>st at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong><a href="http://maxim.ece.illinois.edu/" target="_blank" rel="noreferrer noopener">Maxim Raginsky </a></strong>from<strong> University of Illinois, Urbana-Champaign</strong> will speak about “Neural SDEs: Deep Generative Models in the Diffusion Limit”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: In deep generative models, the latent variable is generated by a time-inhomogeneous Markov chain, where at each time step we pass the current state through a parametric nonlinear map, such as a feedforward neural net, and add a small independent Gaussian perturbation. In this talk, based on joint work with Belinda Tzen, I will discuss the diffusion limit of such models, where we increase the number of layers while sending the step size and the noise variance to zero. I will first provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. Then I will show how we can quantify the expressiveness of diffusion-based generative models. Specifically, I will prove that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback-Leibler divergence to the target distribution. Finally, I will briefly discuss a scheme for unbiased, finite-variance simulation in such models. This scheme can be implemented as a deep generative model with a random number of layers.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/"><span class="datestr">at October 19, 2021 02:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08677">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08677">Algorithmic Thresholds for Refuting Random Polynomial Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hsieh:Jun=Ting.html">Jun-Ting Hsieh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh_K=.html">Pravesh K. Kothari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08677">PDF</a><br /><b>Abstract: </b>Consider a system of $m$ polynomial equations $\{p_i(x) = b_i\}_{i \leq m}$
of degree $D\geq 2$ in $n$-dimensional variable $x \in \mathbb{R}^n$ such that
each coefficient of every $p_i$ and $b_i$s are chosen at random and
independently from some continuous distribution. We study the basic question of
determining the smallest $m$ -- the algorithmic threshold -- for which
efficient algorithms can find refutations (i.e. certificates of
unsatisfiability) for such systems. This setting generalizes problems such as
refuting random SAT instances, low-rank matrix sensing and certifying
pseudo-randomness of Goldreich's candidate generators and generalizations.
</p>
<p>We show that for every $d \in \mathbb{N}$, the $(n+m)^{O(d)}$-time canonical
sum-of-squares (SoS) relaxation refutes such a system with high probability
whenever $m \geq O(n) \cdot (\frac{n}{d})^{D-1}$. We prove a lower bound in the
restricted low-degree polynomial model of computation which suggests that this
trade-off between SoS degree and the number of equations is nearly tight for
all $d$. We also confirm the predictions of this lower bound in a limited
setting by showing a lower bound on the canonical degree-$4$ sum-of-squares
relaxation for refuting random quadratic polynomials. Together, our results
provide evidence for an algorithmic threshold for the problem at $m \gtrsim
\widetilde{O}(n) \cdot n^{(1-\delta)(D-1)}$ for $2^{n^{\delta}}$-time
algorithms for all $\delta$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08677"><span class="datestr">at October 19, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08669">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08669">Constructing Many Faces in Arrangements of Lines and Segments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Haitao.html">Haitao Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08669">PDF</a><br /><b>Abstract: </b>We present new algorithms for computing many faces in arrangements of lines
and segments. Given a set $S$ of $n$ lines (resp., segments) and a set $P$ of
$m$ points in the plane, the problem is to compute the faces of the
arrangements of $S$ that contain at least one point of $P$. For the line case,
we give a deterministic algorithm of $O(m^{2/3}n^{2/3}\log^{2/3}
(n/\sqrt{m})+(m+n)\log n)$ time. This improves the previously best
deterministic algorithm [Agarwal, 1990] by a factor of $\log^{2.22}n$ and
improves the previously best randomized algorithm [Agarwal, Matou\v{s}ek, and
Schwarzkopf, 1998] by a factor of $\log^{1/3}n$ in certain cases (e.g., when
$m=\Theta(n)$). For the segment case, we present a deterministic algorithm of
$O(n^{2/3}m^{2/3}\log n+\tau(n\alpha^2(n)+n\log m+m)\log n)$ time, where
$\tau=\min\{\log m,\log (n/\sqrt{m})\}$ and $\alpha(n)$ is the inverse
Ackermann function. This improves the previously best deterministic algorithm
[Agarwal, 1990] by a factor of $\log^{2.11}n$ and improves the previously best
randomized algorithm [Agarwal, Matou\v{s}ek, and Schwarzkopf, 1998] by a factor
of $\log n$ in certain cases (e.g., when $m=\Theta(n)$). We also give a
randomized algorithm of $O(m^{2/3}K^{1/3}\log n+\tau(n\alpha(n)+n\log m+m)\log
n\log K)$ expected time, where $K$ is the number of intersections of all
segments of $S$. In addition, we consider the query version of the problem,
that is, preprocess $S$ to compute the face of the arrangement of $S$ that
contains any query point. We present new results that improve the previous work
for both the line and the segment cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08669"><span class="datestr">at October 19, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08483">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08483">Streaming Decision Trees and Forests</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Haoyin.html">Haoyin Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Jayanta.html">Jayanta Dey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panda:Sambit.html">Sambit Panda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vogelstein:Joshua_T=.html">Joshua T. Vogelstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08483">PDF</a><br /><b>Abstract: </b>Machine learning has successfully leveraged modern data and provided
computational solutions to innumerable real-world problems, including physical
and biomedical discoveries. Currently, estimators could handle both scenarios
with all samples available and situations requiring continuous updates.
However, there is still room for improvement on streaming algorithms based on
batch decision trees and random forests, which are the leading methods in batch
data tasks. In this paper, we explore the simplest partial fitting algorithm to
extend batch trees and test our models: stream decision tree (SDT) and stream
decision forest (SDF) on three classification tasks of varying complexities.
For reference, both existing streaming trees (Hoeffding trees and Mondrian
forests) and batch estimators are included in the experiments. In all three
tasks, SDF consistently produces high accuracy, whereas existing estimators
encounter space restraints and accuracy fluctuations. Thus, our streaming trees
and forests show great potential for further improvements, which are good
candidates for solving problems like distribution drift and transfer learning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08483"><span class="datestr">at October 19, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08451">Sum-of-Squares Geometry Processing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marschner:Zo=euml=.html">Zoë Marschner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Paul.html">Paul Zhang</a>, David Palmer, Justin Solomon <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08451">PDF</a><br /><b>Abstract: </b>Geometry processing presents a variety of difficult numerical problems, each
seeming to require its own tailored solution. This breadth is largely due to
the expansive list of geometric primitives, e.g., splines, triangles, and
hexahedra, joined with an ever-expanding variety of objectives one might want
to achieve with them. With the recent increase in attention toward higher-order
surfaces, we can expect a variety of challenges porting existing solutions that
work on triangle meshes to work on these more complex geometry types. In this
paper, we present a framework for solving many core geometry processing
problems on higher-order surfaces. We achieve this goal through sum-of-squares
optimization, which transforms nonlinear polynomial optimization problems into
sequences of convex problems whose complexity is captured by a single degree
parameter. This allows us to solve a suite of problems on higher-order
surfaces, such as continuous collision detection and closest point queries on
curved patches, with only minor changes between formulations and geometries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08451"><span class="datestr">at October 19, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08354">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08354">Faster Modular Composition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neiger:Vincent.html">Vincent Neiger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salvy:Bruno.html">Bruno Salvy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schost:=Eacute=ric.html">Éric Schost</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Villard:Gilles.html">Gilles Villard</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08354">PDF</a><br /><b>Abstract: </b>A new Las Vegas algorithm is presented for the composition of two polynomials
modulo a third one, over an arbitrary field. When the degrees of these
polynomials are bounded by $n$, the algorithm uses $O(n^{1.43})$ field
operations, breaking through the $3/2$ barrier in the exponent for the first
time. The previous fastest algebraic algorithms, due to Brent and Kung in 1978,
require $O(n^{1.63})$ field operations in general, and ${n^{3/2+o(1)}}$ field
operations in the particular case of power series over a field of large enough
characteristic. If using cubic-time matrix multiplication, the new algorithm
runs in ${n^{5/3+o(1)}}$ operations, while previous ones run in $O(n^2)$
operations.
</p>
<p>Our approach relies on the computation of a matrix of algebraic relations
that is typically of small size. Randomization is used to reduce arbitrary
input to this favorable situation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08354"><span class="datestr">at October 19, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08335">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08335">Trigger Hunting with a Topological Prior for Trojan Detection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xiaoling.html">Xiaoling Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Xiao.html">Xiao Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cogswell:Michael.html">Michael Cogswell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yao:Yi.html">Yi Yao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jha:Susmit.html">Susmit Jha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chao.html">Chao Chen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08335">PDF</a><br /><b>Abstract: </b>Despite their success and popularity, deep neural networks (DNNs) are
vulnerable when facing backdoor attacks. This impedes their wider adoption,
especially in mission critical applications. This paper tackles the problem of
Trojan detection, namely, identifying Trojaned models -- models trained with
poisoned data. One popular approach is reverse engineering, i.e., recovering
the triggers on a clean image by manipulating the model's prediction. One major
challenge of reverse engineering approach is the enormous search space of
triggers. To this end, we propose innovative priors such as diversity and
topological simplicity to not only increase the chances of finding the
appropriate triggers but also improve the quality of the found triggers.
Moreover, by encouraging a diverse set of trigger candidates, our method can
perform effectively in cases with unknown target labels. We demonstrate that
these priors can significantly improve the quality of the recovered triggers,
resulting in substantially improved Trojan detection accuracy as validated on
both synthetic and publicly available TrojAI benchmarks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08335"><span class="datestr">at October 19, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.08325">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.08325">Minor Embedding in Broken Chimera and Pegasus Graphs is NP-complete</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lobe:Elisabeth.html">Elisabeth Lobe</a>, Annette Lutz <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.08325">PDF</a><br /><b>Abstract: </b>The embedding is an essential step when calculating on the D-Wave machine. In
this work we show the hardness of the embedding problem for both types of
existing hardware, represented by the Chimera and the Pegasus graphs,
containing unavailable qubits. We construct certain broken Chimera graphs,
where it is hard to find a Hamiltonian cycle. As the Hamiltonian cycle problem
is a special case of the embedding problem, this proves the general complexity
result for the Chimera graphs. By exploiting the subgraph relation between the
Chimera and the Pegasus graphs, the proof is then further extended to the
Pegasus graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.08325"><span class="datestr">at October 19, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">Theory Group Postdoc at UC Berkeley (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Postdoc inquiries will be viewable by all our group’s faculty. Individual faculty may then reach out in the case of matched interests. Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line.</p>
<p>Website: <a href="http://theory.cs.berkeley.edu/">http://theory.cs.berkeley.edu/</a><br />
Email: tcs-postdoc-inquiries@lists.eecs.berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/"><span class="datestr">at October 18, 2021 09:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/">Multiple Open Tenure-Track Faculty Positions in Computer Science and Engineering at The Ohio State University (apply by October 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science and Engineering at The Ohio State University invites applications for tenure-track faculty appointments at all ranks and in all research areas. Theory and algorithms is one of the areas with particular emphasis in this faculty search. Review of applications will continue until the positions are filled.</p>
<p>Website: <a href="https://www.cse.ohio-state.edu/faculty-recruiting/tenuredtenure-track-faculty-positions">https://www.cse.ohio-state.edu/faculty-recruiting/tenuredtenure-track-faculty-positions</a><br />
Email: rountev.1@osu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/17/multiple-open-tenure-track-faculty-positions-in-computer-science-and-engineering-at-the-ohio-state-university-apply-by-october-31-2021/"><span class="datestr">at October 17, 2021 09:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2010994775517576408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html">Is MATH Ready for P=NP? Is Alexandra Fahrenthold Ready for P=NP?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>(This post was inspired by Harry Lewis emailing me about his granddaughter.)</div><div><br /></div><br /><div style="clear: both; text-align: center;" class="separator"><div style="clear: both; text-align: center;" class="separator"><a style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;" href="https://1.bp.blogspot.com/-EaezFO7EQKg/YWbi3-_1hKI/AAAAAAAB-8o/S76g-orqsg48R9MHQqPf1SZi8M7GFchswCLcBGAsYHQ/s4032/pnp.jpg"><img src="https://1.bp.blogspot.com/-EaezFO7EQKg/YWbi3-_1hKI/AAAAAAAB-8o/S76g-orqsg48R9MHQqPf1SZi8M7GFchswCLcBGAsYHQ/s320/pnp.jpg" border="0" width="240" height="320" /></a></div><a style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-xlbZstxaM_0/YWbibmPgZWI/AAAAAAAB-8Y/TDBhMXf7D2sQ8ZV3M3tlyWYeaJhNzC5dACLcBGAsYHQ/s4032/singing.jpg"><img src="https://1.bp.blogspot.com/-xlbZstxaM_0/YWbibmPgZWI/AAAAAAAB-8Y/TDBhMXf7D2sQ8ZV3M3tlyWYeaJhNzC5dACLcBGAsYHQ/s320/singing.jpg" border="0" width="240" height="320" /></a><br /></div><div style="clear: both; text-align: center;" class="separator"><br /></div><div>Harry Lewis's grand daughter Alexandra Fahrenthold (see both pictures) wants information</div><div>on how to claim the Millennial prize, so she will be ready.</div><div><br /></div><div>This raises the question: How likely is it that Alexandra will resolve P vs NP (or perhaps some other Millennium problem if she wants to rebel against her grandfather)?</div><div><br /></div><div>And more seriously:</div><div><br /></div><div>1) Have we made progress on P vs NP? (I think not.)</div><div>(By <i>we</i>  I mean the community, not <i>Harry and I </i>or <i>Harry and I and Alexandra</i>,</div><div>for which the answer is a more definite NO.)</div><div><br /></div><div>2) If not then why not?</div><div><br /></div><div>3) How does this compare historically to other open problems in Math?</div><div><br /></div><div>We will refer to progress made in solving an open problem, though that is a tricky notion since only after a problem is solved can you look back and say what was progress.  One might also count subcases (e.g., n=4 case of FLT) as progress even if they don't help lead to the final proof. I quote a letter from Harry Lewis to me upon reading a first draft of this post:</div><div><blockquote><i>The one larger point I would suggest adding is to add my operational definition of progress: Progress is being made on a problem if, when the solution is published, it will cite work being published today. Of course that is “operational” only after the fact. Demillo Lipton Perlis at the end have a nice riff on this. The alchemists thought they were making progress on turning lead to gold but they weren’t, even though we know that was actually a solvable problem. Likewise jumping off of higher and higher buildings was not making progress toward heavier than air flight.</i></blockquote></div><div><br /></div><div>---------------------------------------------------------</div><div><br /></div><div><br /></div><div><div>1) Have we made progress on P vs NP?</div><div><br /></div><div>a) I tell my students that we have made progress on ruling out certain techniques.</div><div>They laugh at that, at which point I decide to<i> not </i>tell them that my PhD thesis was about that sort of thing (oracles). I could say</div><div><br /></div><div><i>Once you know what's not going to work you can concentrate one what is going to work.</i></div><div><br /></div><div>But that sounds hollow since very few people are working on techniques that</div><div>might work (The Geometric Complexity Program, see <a href="https://en.wikipedia.org/wiki/Geometric_complexity_theory">here</a>, is the only exception I know of.)</div><div><br /></div><div>b) Are there any partial results? Ryan Williams showed that SAT (and also counting mod versions of it) cannot be done in time n^c and space n^{o(1)} where c is 2cos(2pi/7) (see <a href="https://link.springer.com/content/pdf/10.1007/s00037-008-0248-y.pdf">here</a>).  That is the only concrete lower bound on SAT that I know of.  Is it progress? Sam Buss and Ryan Williams later showed (see <a href="https://link.springer.com/content/pdf/10.1007/s00037-015-0104-9.pdf">here</a>) that, using current techniques, this cannot be improved. If that inspires new techniques that push it further, that would be great. So it is progress? Hard to know now. </div><div><br /></div><div>c) There are some circuit lower bounds. One can debate if this is progress.</div><div>It will be a much better informed debate once the problem is solved.</div><div><br /></div><div>So I would say VERY LITTLE PROGRESS.</div><div><br /></div><div>------------------------------------------------</div><div>2) If not then why not?</div><div><br /></div><div>a) It's only been open for 50 years. A drop in the mathematical bucket.</div><div><i>Counterargument</i>: 50 years of 20th and 21st century mathematics is A LOT.</div><div><br /></div><div>b) Sociology: The academic computer science conference-model induces us to get out a paper in time for the next conference deadline, and not think deeply about a problem.  Carl Smith thought that P vs NP would be solved by the son of a member of the communist party in the USSR (when there was a USSR) who did not have the pressure to get tenure and grants and such. He may be right.</div><div><i>Counterargumen</i>t: there are some (1) mavericks who buck the system, and (2) people like Carl's son-of-a-party-member who are allowed to think deeply for years.</div><div><br /></div><div>c) It's just dang hard! That's the real question. Paul Erdos said of the Collatz Conjecture:</div><div>       <i> Mathematics may not be ready for such problems.</i></div><div>Is that true of P vs NP as well?</div><div><br /></div></div><div><div><br /></div><div>----------------------------------</div><div>3) History and Philosophy.</div><div>(In college I once took the following four courses in one semester: History of Philosophy, Philosophy of History, Philosophy of Philosophy, History of History.)</div><div><br /></div><div>Let's look at problems that were open and then solved:</div><div><br /></div><div>a) The Three Greek Problems of Antiquity: Squaring the circle (given a circle, construct a square with the same area), doubling the cube (given a line that is the edge of cube, construct another line that is the edge of a cube with twice the volume), trisecting an angle (given an angle, construct two lines whose angle is 1/3 of the given angle), with a straightedge and compass. (When I first heard of this problem I wondered how knowing what direction was North would help trisect an angle.) Posed in roughly 400BC. Not clear what <i>posed</i> means in this context. Did the ask for a construction OR did they ask for EITHER a construction OR a proof that there wasn't one?</div><div><br /></div><div>This might be the closest analogy to P vs NP: At the time the problem was stated</div><div> <i>MATHEMATICS WAS NOT READY FOR SUCH PROBLEMS. </i></div><div>It took lots of new math, a better notation, and a different way of looking at numbers, to show that they  could not be done: Pierre Wantzel--doubling the cube (1837),Pierre Wantzel--trisection (1837), Lindemann-Weierstrass--squaring the circle (1882).</div><div>NOTE: Some sources list a fourth problem: constructing every regular polygon. Pierre Watnzel proved, in 1837, that a regular n-gon is constructible iff n is the product of a power of 2 and distinct Fermat  primes. (Why isn't Wantzel better known?) </div><div><br /></div><div>b) Fermat's Last Theorem. Given its origin, not quite clear when it was posed but 1640's seems fair. This could not be solved when it was posed (On an episode of Dr. Who they claim that Fermat had a simple proof. Note that Dr. Who is fictional and their PhD (if they has one) is probably not in mathematics.) </div><div><i>MATHEMATICS WAS NOT READY FOR SUCH PROBLEMS</i>, </div><div>but not as much as the three Greek problems. Very steady progress on it, see  <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/progressflt.pdf">here</a>. One of the real milestone was connecting it to other problems in Math. And then Wiles proved it in the 1990's. While the solution was a surprise when it happened it was not that much of a surprise.</div><div><br /></div><div>QUESTION: Is P vs NP more similar to Greek3 or to FLT? </div><div><br /></div><div>c) Peano Arithmetic (and similar systems) are incomplete. Hilbert's 2nd problem (1900) asked to show the axioms of PA were consistent. Godel (1931) showed this could not be done.  Moreover, there are TRUE statements about numbers that PA cannot prove. I think people mostly thought PA was complete so one of the innovations was to think it was incomplete.  </div><div><i>MATHEMATICS WAS READY FOR SUCH PROBLEMS </i></div><div>but it took the boldness to think PA was incomplete to solve it.  The math needed was known when Hilbert posed the problem. But of course, how to put it together was still quite a challenge.</div></div><div><br /></div><div><div><br /></div><div>d) The Continuum Hypothesis, CH, is that there is no cardinality between N and R. Cantor in 1878 asked for a proof that CH was true. It was Hilbert's first problem in 1900.</div><div>When Hilbert posed this problem in 1900</div><div><i>MATHEMATICS WAS NOT QUITE  READY FOR SUCH PROBLEMS.</i></div><div>The math to solve it wasn't quite there, but wasn't so far off (of course, that's in hindsight). Godel's model L (1940) was brilliant, though Lowenhiem-Skolem had constructed models.  A model of set theory that was defined by levels was, I think, though of by Russell (though in a very diff way than L). When Cohen did a model where CH is false (1963) he invented forcing for Set Theory, though forcing had already been used in Recursion theory (The Kleene-Post construction of intermediary Turing degrees.)</div><div><br /></div><div>e) Hilbert's tenth problem (1900): Find an algorithm that will, given a poly in many variables over Z, determine if it has a solution in Z.</div><div><i>MATHEMATICS WAS ALMOST READY FOR SUCH PROBLEMS.</i></div><div>I turns out that there is no such algorithm. Similar to CH: Once it was thought that it was unsolvable, the proof that it was unsolvable just took a few decades. However, it did need  the definition of computable to be pinned down.  Davis-Putnam-Robinson outlined what was needed in the 1950's,and Matiyasevich finished it in 1970.  While it required just the right combination of ideas, and lots of cleverness, the math needed was known.</div><div>CAVEAT: There are many restrictions of H10 that are still open. My favorite: is the following solvable: given k, does x^3 + y^3 + z^3 = k have a solution in Z? (See my blog post on this problem <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">here</a>.) For a survey of what is known about subcases see (1) my paper <a href="http://export.arxiv.org/pdf/2104.07220">here</a>, though it is has been argued that I am looking at the wrong subcases (see my blog post on this <a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">here</a>), and (2) Bogdan Grechuk's paper <a href="https://arxiv.org/abs/2108.08705">here</a></div><div>CAVEAT: Matiyasevich has suggested that Hilbert really meant to ask about equations and solutions over  Q. That problem is still open. If it is unsolvable, that might be proven reasonably soon. If it is solvable, then</div><div><i>MATHEMATICS IS NOT READY FOR SUCH PROBLEMS.</i></div><div><br /></div><div>f) The four color theorem. Posed in 1852 by Francis Guthrie, proven in 1976. Haken, Appel, and Koch (more on that last name later) did do some very impressive math to set the problem up, and the computer program to finish it off. When the problem was posed (1852) the computing power was not up to the task. So </div><div><i>COMPUTER SCIENCE WAS NOT READY FOR SUCH PROBLEMS.</i></div><div>Could the ideas to set it up have been done earlier? Maybe, but not much earlier. The result is often attributed to Haken and Appel, but actually there are two papers, and Koch is an author on the second one. Note that (1) Robertson, Sanders, Seymour, Thomas had a simpler, though still computer proof (1996), and (2) Werner Gonthier formalized the proof inside the Coq proof assistant in 2005.</div><div>CAVEAT: An open problem that is hard to state precisely is to come up with a non-computer proof.</div></div><div>CAVEAT: There is a non-computer proof that every planar graph is 4.5-colorable, see my blog post in this <a href="https://blog.computationalcomplexity.org/search?q=chromatic+number">here</a>. (No, this is not a joke. If it was I would make if funnier and claim there is a non-computer proof that every planar graph is 4 + 1/e colorable.)</div><div><br /></div><div><div>g) Poincare Conjecture. Conjectured in 1904 and solved in 2002. To bad---if it was solved in 2004 it would be exactly 100 years. There was some progress on this all along so I don't know which step was <i>the hard one </i>though probably they were all hard. This one is harder for me to speculate on. When it was solved and Darling wanted to know why it was worth $1,000,000 I told her that it says <i>if something</i> <i>tastes and smells and feels like a sphere, its a sphere</i>. She was unimpressed.  But back to our story:  in hindsight,</div><div><i>MATH WAS READY FOR SUCH PROBLEMS</i></div><div> since there was steady progress. I think of NOT READY as meaning NO progress, NO plan.</div><div><br /></div><div>h) The Erdos Distance Problem: Show that for any n points in the plane the number of distinct distances is Omega(n/\sqrt{log n}). Not quite solved, but a big milestone was Gutz and Katz proof of Omega(n/log n). For that result</div><div><i>MATH WAS READY FOR SUCH PROBLEMS</i></div><div><i>Steady progress</i>:  see the Wikipedia entry <a href="https://en.wikipedia.org/wiki/Erd%C5%91s_distinct_distances_problem#:~:text=Erd%C5%91s%20distinct%20distances%20problem%20From%20Wikipedia%2C%20the%20free,by%20Larry%20Guth%20and%20Nets%20Katz%20in%202015.">here</a>. What's of interest to us is that there was a barrier result of Omega(n^{8/9}) by Ruzsa (apparently unpublished) that said the techniques being used could not do better-- so people, in short order, found new techniques.  Here is hoping that happens with P vs NP.</div><div><br /></div><div>--------------------------------------------------------------------------------</div><div>Let's look at problems that are open and unsolved.</div><div><br /></div><div>a) Collatz Conjecture (also called the 3x+1 conjecture). I asked</div><div>Jeff Lagarias, who is an expert on the problem:</div><div><br /></div><div><i>Is it true? When will it be resolved?</i> He said <i>Yes</i> and <i>Never</i>.</div><div><br /></div><div>I once heard there has been NO progress on this problem, though I later  heard that Terry Tao has made some progress. In any case, not much progress has been made. Maybe Erdos was right.</div><div><br /></div><div>QUESTION: Why does my spell checker think that<i> Collatz</i> is not a word? </div><div><br /></div><div>b) Small Ramsey Numbers. I asked Stanislaw Radziszowski, who is an expert on Small Ramsey Numbers (he has a dynamic survey on small Ramsey numbers <a href="https://www.combinatorics.org/files/Surveys/ds1/ds1v15-2017.pdf">here</a>) </div><div><br /></div><div><i>What is R(5)?  When will we know? </i>He said <i>43</i> and <i>Never</i>.</div><div><br /></div><div>Worse than being hard, I don't think any nice math has come out of trying to find R(5,5). Too bad. The coloring that gives the lower bound for R(4) and some (perhaps all) of the R(i,j) where i,j\le 4 can be derived from group theory. YEAH! But then connections to interesting math just... stopped. For now? Forever? Joel Spencer told me this is an example of the <i>law</i> <i>of small numbers</i>: patterns that hold for small numbers stop holding when the numbers get too big. (I've seen other things  called <i>the law of small numbers</i> as well.) </div></div><div><div><i>MATH MAY NEVER BE READY FOR SUCH PROBLEMS</i> </div><div>If no interesting math comes out of the attempt to find the exact values of the Ramsey Numbers, then it is not a good problem. </div><div><br /></div><div><i>Note:  </i>The conversations about Collatz and R(5) were within 10 minutes of each other. Depressing day!</div><div><br /></div><div>c) The Twin Primes Conjecture. Sieve methods have been used to get partial result. YEAH! Yitang Zhang showed there exists infinite x such that x and x + 70million (something like that are prime. YEAH. Its been gotten down to x, x+246 and with various assumptions x,x+12 or x, x+6). YEAH! but Sieve methods are known to NOT be able to prove the  conjecture. Dang it!</div><div><i>DO NOT KNOW IF MATH IS READY FOR SUCH PROBLEMS</i>.</div><div>I think people are kind of stuck here. Much like P vs NP, though at least they have some partial results. By contrast, with regard to P vs NP we don't even have that (unless you count Ryan's lower bound on SAT---maybe you do).</div><div><br /></div><div><i>Note</i>: I found that information <a href="https://www.britannica.com/science/twin-prime-conjecture">here</a> which seems to be an Encyclopedia Britannica  website. I would have thought that, with the web and Wikipedia, they would be out of business. Good for them to still be in business! </div><div><br /></div><div>d) I am not qualified to write about any of the Millennium prizes except P vs NP (am I even qualified for that?)  so I ask my readers to leave opinions (informed or not) about, for which of them, </div><div><i>MATH IS NOT READY FOR SUCH PROBLEMS</i></div><div>One of the people who worked on the Riemann Hypothesis said: </div><div><br /></div><div><i>I do not recommend spending half your life on the Riemann Hypothesis. </i></div><div><i><br /></i></div><div>That raises a different question: When do you give up? (topic for a different blog post). </div><div><i><br /></i></div><div>e) I am also not qualified to write about the Hilbert Problems where are still unsolved. Note that some of them are not well enough defined  to ever be resolved (H6: Make Physics rigorous) and some are either solved or unsolved depending on who you ask (H4: Construct ALL metrics where lines are geodesics-- <i>surely,</i> he didn't mean ALL metrics. Probably right, but  stop calling me <i>Shirley</i>!) For a byte more about Hilbert's problems, including a few paragraphs on H4,  see my reviews of two books on them, <a href="http://www.cs.umd.edu/~gasarch/bookrev/44-4.pdf">here</a>. Same as the last item- if you have an opinion (informed or not) about, for which of them that are though to be sort-of open, is math ready for them, leave a comment. </div><div><br /></div><div>CODA: Alexandra will be working on Collatz this summer!</div><div>Let's wish her luck!</div></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html"><span class="datestr">at October 17, 2021 07:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/144">TR21-144 |  Towards Uniform Certification in QBF | 

	Leroy Chew, 

	Friedrich Slivovsky</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We pioneer a new technique that allows us to prove a multitude of previously open simulations in QBF proof complexity. In particular, we show that extended QBF Frege p-simulates clausal proof systems such as IR-Calculus, IRM-Calculus, Long-Distance Q-Resolution, and Merge Resolution.
  These results are obtained by taking a technique of Beyersdorff et al. (JACM 2020) that turns strategy extraction into simulation and combining it with new local strategy extraction arguments.



This approach leads to simulations that are carried out mainly in propositional logic, with minimal use of the QBF rules. Our proofs therefore provide a new, largely propositional interpretation of the simulated systems.
We argue that these results strengthen the case for uniform certification in QBF solving, since many QBF proof systems now fall into place underneath extended QBF Frege.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/144"><span class="datestr">at October 17, 2021 03:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/copriors/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/copriors/">Combining Diverse Feature Priors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2110.08220" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/madrylab/copriors" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br /></p>

<p><i>
    In <a href="https://arxiv.org/abs/2110.08220">our new paper</a>, we take a closer look at the design space of so-called feature priors—i.e., priors that bias the features that a model relies on. We show that models trained with diverse sets of such priors have less overlapping failure modes and can correct each other’s mistakes.
</i></p>

<p>At the core of deep learning’s success is its ability to automatically learn useful features from raw data, e.g., image pixels. Consequently, the set of such learned features has a large impact on the deep learning model’s ability to generalize, especially when the deployment conditions deviate from the training environment. For example, a model that relies heavily on the background of an image may have trouble recognizing a cow on a beach if during training it was mostly encountering cows on grass.</p>

<p>So, how can one control the features that a model relies on? This is often accomplished by changing the model’s architecture or training methodology. For example, by choosing to use a convolutional neural network a model designer biases that model towards learning a hierarchy of spatial features. Similarly, by employing data augmentation during training, one biases the model towards features that are invariant to the particular augmentations used. In fact, researchers have recently explored explicitly manipulating the set of features that a model learns, via, for example, <a href="https://arxiv.org/abs/1811.12231">suppressing texture information</a> by training on stylized inputs or by <a href="https://arxiv.org/abs/1706.06083">training with worst-case input perturbations</a> to <a href="https://arxiv.org/abs/1906.00945">avoid brittle features</a>.</p>

<p>Now, all of these design choices can be thought of as imposing feature priors, i.e., biasing a model towards learning a particular type of features. The question thus becomes: how can we explore and leverage this space of feature priors in a more systematic and purposeful manner?</p>

<h2 id="feature-priors-as-distinct-perspectives-on-data">Feature Priors as Distinct Perspectives on Data</h2>
<p>The core idea is to view different feature priors as distinct perspectives on the data. That is, since different sets of features are likely to generalize differently across inputs, by considering multiple such sets in tandem we can obtain a more holistic (and thus, hopefully, reliable) view on our data.</p>

<p><strong>Setting up our case study:</strong> Let us focus our attention on two concrete feature priors: shape and texture. These two priors arise naturally in the context of image classification and will serve as the basis of our study. We impose these priors through deliberate construction of the dataset and architecture:</p>

<ul>
  <li><strong>Shape-based priors</strong>:  We remove texture information from the images with the help of an edge detection algorithm. We use for this purpose two canonical edge detection algorithms from the computer vision literature: <a href="https://en.wikipedia.org/wiki/Canny_edge_detector">Canny</a> and <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel</a>.</li>
  <li><strong>Texture-based priors</strong>: We use a variant of the <a href="https://arxiv.org/abs/1904.00760">BagNet</a> model, which limits the model’s receptive field to prevent the model from relying on global structures like shape.</li>
</ul>

<div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/original.png" />
            <span style="text-align: center;">Original</span>
        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/sobel.png" />
            <span style="text-align: center;">Sobel</span>

        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/canny.png" />
            <span style="text-align: center;">Canny</span>
        </div>
    </div>
    <div class="quarterblock">
        <div class="block">
            <img src="https://gradientscience.org/assets/copriors/bagnet.png" />
            <span style="text-align: center;">BagNet</span>

        </div>
    </div>
</div>

<div class="footnote">
Visualizing different feature priors. Sobel and Canny suppress texture through edge detection while BagNet suppresses shape by limiting the receptive field.
</div>

<p>Intuitively, these priors should correspond to very different sets of features. But are the views offered by these priors truly complementary? A simple way to measure this is to quantify the overlap of the failure modes of the models trained with the respective priors. Specifically, we measure the correlation of predicting the correct label for each pair of such models. We perform this analysis on a subset of CIFAR-10.</p>

<p><img src="https://gradientscience.org/assets/copriors/cifar_corr_table.png" class="bigimg" id="pipeline" /></p>
<div class="footnote"> Correlation of predictions between pairs of models with different priors on a subset of CIFAR-10. The shape-biased and texture-biased models have the least correlated predictions.
</div>

<p>It looks like these results match our intuition! Indeed, models corresponding to the same feature priors (but different random initialization) are relatively well correlated with each other. This also includes the case when we use two <em>different</em> shape biases. On the other hand, when we consider a pairing of a shape-biased model and a texture-biased model the predictions are the least correlated, that is, they make different mistakes on the test set.</p>

<h2 id="combining-feature-priors">Combining feature priors</h2>

<p>Since shape- and texture-biased models make different types of mistakes, can we leverage their diversity to improve our predictions?</p>

<h3 id="ensembles">Ensembles</h3>
<p>A natural way to examine that question is combining these models in an ensemble. This ensemble, for a given test input, evaluates both models on that input and then outputs the one prediction that is assigned the highest probability by the respective model. It turns out that, indeed, such an ensemble is significantly more accurate when we combine in this way models with different priors (as opposed to combining two models trained with the same prior, but with different initializations). Clearly, prediction diversity matters!</p>

<div id="anno"> 
<canvas width="100%" id="ensemble_bar" height="35%"></canvas>
</div>
<div class="footnote"> The maximum accuracy achieved when using a single model, an ensemble with two models with the same prior, and an ensemble with two models with different priors on the CIFAR-10 subset. 
</div>

<h3 id="co-training">Co-Training</h3>
<p>So far, we demonstrated that models with diverse feature priors have less overlapping failure modes, and can be combined via ensembling for improved generalization performance. However, is that all? In particular, are there ways of incorporating prior diversity during training (as opposed to ensembling post hoc) in order to improve the learning process itself?</p>

<p>To answer this question, we focus on <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.664.3543">self-training</a>, a methodology often employed when the labeled data is insufficient to learn a well-generalizing model alone, but a large pool of unlabeled data is available. The key idea in self-training is to use a model trained on the labeled data to produce “pseudo-labels” for the unlabeled data and then use these labels for further training. This setting is particularly challenging since: (a) the (original) labelled data points are typically too few to learn reliable prediction rules, and (b) any incorrect prediction rules learned will be reinforced via pseudo-labelling (so-called <a href="https://arxiv.org/abs/1908.02983">“confirmation bias”</a>).</p>

<p>From this perspective, our goal is to jointly train models with different feature priors to mitigate the propagation of such incorrect prediction rules. We will do so by leveraging the well-established framework of <a href="https://www.cs.cmu.edu/~avrim/Papers/cotrain.pdf">co-training</a>, a framework designed for learning from data with multiple independent sets of features. In the context of our study, we can instantiate this framework as follows.</p>

<p>First, we train one model for each prior using the labeled data. Then, we use each of these models to pseudo-label the unlabelled data and add the examples which are assigned the highest predicted probability to a joint pseudo-labelled data pool. We then use these examples to train both models further and keep repeating this process until we eventually use all the unlabeled data for training. In the end, we combine these models into a single classifier by training a standard model from scratch on the combined pseudo-labels.</p>

<p>The intuition behind this process is that by jointly training models which rely on different features, these models can learn from each other’s predictions. If one model produces incorrect pseudo-labels, we can hope that the other model will correct them by relying on alternative prediction rules.</p>

<p>So, how well does this approach work? To evaluate it, we extract from the CIFAR-10 dataset a small, labeled part (100 examples per class) and treat the rest of this dataset unlabeled. We then compare how different training methods—specifically, self-training a model with a single prior, and co-training models with different priors—perform in this setting. (For an additional baseline, we also consider ensembling two such models with different priors together.)</p>

<div id="anno"> 
<canvas width="100%" id="summary_bar" height="35%"></canvas>
</div>
<div class="footnote"> Test accuracy of models in pre-trained, self-trained, and co-trained settings. We consider: a single model alone, combining multiple models with the same prior, and combining models with diverse priors.  
</div>

<p>Overall, we find that co-training with shape- and texture-based priors can significantly improve the test accuracy of the final model compared to self-training with any of the priors alone. In fact, co-training models with diverse priors also improves significantly upon simply combining self-trained models in an ensemble. So these models are indeed able to take advantage of each other’s predictions during training.</p>

<h2 id="priors-and-spurious-correlations">Priors and Spurious Correlations</h2>
<p>So far, we were focused on a setting where the training and test data were all sourced from the same distribution. However, a major challenge for the real-world model deployment are spurious correlations: associations which are predictive on the training data but not valid for the actual task. For example, an image classification model may predict <a href="https://gradientscience.org/background/">an object’s class based on its location</a>, or <a href="https://www.medrxiv.org/content/10.1101/2020.09.13.20193565v2.full">rely on artifacts of the data collection process</a>.</p>

<p>How can we train models that avoid picking up such spurious correlations? For this problem to be tractable in the first place, we need to rely on some information beyond the training data. Here, we will assume that we have access to an unlabelled dataset where this correlation does not hold (e.g., cows do not always appear on grass and thus the correlation “grass”-&gt;”cow” is not always predictive). This is a rather mild assumption in settings where we can easily collect unlabelled data from a variety of sources—if a correlation is spurious, it is less likely to be uniformly present.</p>

<p>As a concrete example, let us consider a simple gender classification task based on the CelebA dataset. In this task, we will introduce a spurious correlation into the labeled data by only collecting photographs of blond females and non-blond males. This makes hair color a good predictor of gender for the labeled dataset, but will not generalize well beyond that as such correlation does not hold in the real world.</p>

<p>Our goal here will be to harness an unbiased, yet unlabelled dataset, to learn a model that avoids this correlation. We will again attempt to do so by co-training models with diverse feature priors: shape and texture. Notice that since the spurious correlation is color-based, shape-biased models are likely to ignore it. As a result, we anticipate that the prediction of the shape-biased and texture-biased models will differ on inputs where hair color disagrees with gender. Thus, during co-training, these models are intuitively providing each other with counter-examples and are thus likely to steer each other away from incorrect prediction rules.</p>

<div id="anno"> 
<canvas width="100%" id="spurious_bar" height="35%"></canvas>
</div>
<div class="footnote"> Accuracy of models with different feature priors on the (unbiased) CelebA test set. We consider the setting of using only the (biased) labeled data, as well as self-training and co-training using the (unbiased) unlabeled dataset. (For co-training, the combined model is a standard model trained on the combined pseudo-labels of the co-trained Canny and BagNet models.) 
</div>

<p>We find that this is indeed the case! When we co-train a texture-biased model with a shape-biased one, the texture-biased model improves substantially, relying less on the hair color. Moreover, the shape-biased model also improves through co-training. This indicates that even though the texture-biased model relies heavily on the spurious correlation, it also captures non-spurious associations that, through pseudo-labeling, are useful for the shape-based model too.</p>

<h2 id="outlook-exploring-the-design-space-of-priors">Outlook: Exploring the Design Space of Priors</h2>
<p>In this post, we described how models trained with diverse feature priors can be leveraged during training to learn more reliable prediction rules (e.g., in the presence of spurious correlations). However, we view our exploration as only the first step in systematically exploring the design space of feature priors. We believe that this direction will yield an important building block of reliable training and deployment pipelines.</p></div>







<p class="date">
<a href="https://gradientscience.org/copriors/"><span class="datestr">at October 17, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/">The Ideal State Machine Model: Multiple Clients and Linearizability</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduced state machines and state machine replication in an earlier post. In this post, we elaborate on the exact notion of safety and liveness that can be obtained by an ideal state machine when there are multiple clients. First, these definitions highlight the challenges of serving multiple clients. Second,...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/"><span class="datestr">at October 16, 2021 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>I was interested to see a familiar-looking graph drawing as one of the answers to <a href="https://fractalkitty.com/2021/10/02/mathober-2021-begins/">the prompt “multiplicity” for the first entry in Mathober 2021</a> (<a href="https://mathstodon.xyz/@11011110/107030908468774953">\(\mathbb{M}\)</a>). It’s a multigraph formed by a triangle with tripled edges, and looks a lot like <a href="https://commons.wikimedia.org/wiki/File:Multigraph-edge-coloring.svg">the drawing I made</a> for <a href="https://en.wikipedia.org/wiki/Shannon_multigraph">the Wikipedia Shannon multigraph article</a>, prettied up by making an infinitely recessing sequence of these drawings rather than just one. Good choice for multiplicity.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2021/10/03/to-cheer-you-up-in-difficult-times-32-annika-heckels-guest-post-how-does-the-chromatic-number-of-a-random-graph-vary/">Non-concentration of the chromatic number of random graphs</a> (<a href="https://mathstodon.xyz/@11011110/107040518830792599">\(\mathbb{M}\)</a>). Uniformly random graphs, \(G(n,1/2)\) in the Erdős–Rényi–Gilbert model, turn out to have chromatic numbers that, for infinitely many \(n\), are spread over roughly \(\sqrt{n}\) values. But there are weird fluctuations so that, conjecturally, for some \(n\) the concentration is much tighter, more like \(n^{1/4}\).</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/10/04/tenure-under-threat-georgia">University System of Georgia moves to gut tenure</a> (<a href="https://mathstodon.xyz/@11011110/107047579890149970">\(\mathbb{M}\)</a>). The proposed new policy includes procedures for removal of tenure under certain enumerated grounds, including failure to perform their jobs (this is pretty normal) but then adds a massive escape clause in which the board of regents can remove tenured faculty at any time as long as their reason for doing so is not one of the enumerated ones.</p>
  </li>
  <li>
    <p><a href="http://hyperbolic-crochet.blogspot.com/2010/07/story-about-origins-of-model-of.html">The first physical models of the hyperbolic plane, made in 1868 by Beltrami</a> (<a href="https://mathstodon.xyz/@11011110/107052024179327595">\(\mathbb{M}\)</a>, <a href="http://www.open.ac.uk/blogs/is/?p=731">via</a>), blog post by Daina Taimiņa from 2010. Maybe you could make something like this by wrapping and stretching a disk of wet paper in a roll around a pseudosphere (https://en.wikipedia.org/wiki/Pseudosphere)? The rolled-up photo of Beltrami’s model suggests that he did that. The via link shows this as a tangent to a story about triangulated polygons, frieze patterns, and the Farey tessellation.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=QFj-hF8XDQ0">Why do bees make rhombic dodecahedrons</a> (<a href="https://mathstodon.xyz/@11011110/107058516442920053">\(\mathbb{M}\)</a>)? Nice video from Matt Parker (Stand-up Maths) on why bees usually end the cells of their honeycombs with rhombic dodecahedron faces, why this isn’t the optimal solution to fitting two layers of cells together (in terms of minimum wax usage), and why it isn’t reasonable to expect bees to find exact optima for this problem. If I have to quibble with something, though, it’s his plural. It’s not wrong, but see <a href="https://books.google.com/ngrams/graph?content=dodecahedrons%2Cdodecahedra">Google ngrams</a>.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-prove-melting-ice-stays-smooth-20211006/">Mathematicians prove melting ice stays smooth</a> (<a href="https://mathstodon.xyz/@11011110/107064697896988128">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Stefan_problem">see also</a>). The headline is a little overstated: you’re probably familiar with thin necks of ice melting to sharp points at the instant they separate. But these singularities are instantaneous: mathematical models of ice stay smooth for all but a measure-zero set of times. Original paper: “<a href="https://arxiv.org/abs/2103.13379">The singular set in the Stefan problem</a>”, Alessio Figalli, Xavier Ros-Oton, and Joaquim Serra.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@JordiGH/107061930434927745">Discussion of the recent meme telling programmers and mathematicians that summation notation and for loops are the same thing</a>. They’re not quite the same, though: summations don’t have an order of evaluation. But which is easier for people who don’t already know what they are to search and find out about? And why do programmers get angry at non-programming notational conventions?</p>
  </li>
  <li>
    <p><a href="http://eugeniacheng.com/wp-content/uploads/2017/02/cheng-morality.pdf">Mathematics, morally</a> (<a href="https://mathstodon.xyz/@11011110/107078314739030894">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=28816050">via</a>), Eugenia Cheng, 2004. Somehow I hadn’t run across this  before. It argues that much philosophy of mathematics is irrelevant to practice (“You can’t tell from somebody’s mathematics if they are a fictionalist, a rationalist, a platonist, a realist, an operationalist, a logicist, a formalist, structuralist, nominalist, intuitionist.”) and instead considerations of the right way of handling certain topics are more central.</p>
  </li>
  <li>
    <p>The SIGACT Committee for the Advancement of Theoretical Computer Science is collecting information on women in theoretical computer science (<a href="https://mathstodon.xyz/@11011110/107084102339408082">\(\mathbb{M}\)</a>). If this is you, please see <a href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/">their announcement</a> for details of how to be counted.</p>
  </li>
  <li>
    <p><a href="https://pratt.duke.edu/about/news/rudin-squirrel-award">Cynthia Rudin wins major award with silly name</a> (<a href="https://mathstodon.xyz/@11011110/107089557928657710">\(\mathbb{M}\)</a>), for her work on machine learning systems that learn to predict behavior using simple, interpretable, and transparent formulas.</p>
  </li>
  <li>
    <p>According to the <a href="https://www.siam.org/conferences/cm/conference/soda22">SODA web site</a>, SIAM has decided that their conferences will be hybrid through July (<a href="https://mathstodon.xyz/@11011110/107090556997921608">\(\mathbb{M}\)</a>). So if (like me) you wanted to participate in SODA/SOSA/ALENEX/APOCS, but were worried about planning a trip to Virginia with another predicted winter wave of Covid, now you can stay home and conference safely. Or, if you feel hybrid conferences are problematic and organizers should do one or the other but not both, now you have another reason to be annoyed.</p>
  </li>
  <li>
    <p>Rarely is the question asked: <a href="https://blogs.ams.org/beyondreviews/2021/10/14/are-math-papers-getting-longer/">Are math papers getting longer?</a> (<a href="https://mathstodon.xyz/@11011110/107104144047349269">\(\mathbb{M}\)</a>). Following earlier work by Nick Trefethen, Edward Dunne provides some data suggesting that (for certain journals, at least, and not the ones with page limits) the answer is yes. I’m not convinced by the suggested explanation that it’s because they are taking more effort to explain “connections with other work”, though: is that really a big enough fraction of most papers?</p>
  </li>
  <li>
    <p>I haven’t been using my office desktop Mac much because I haven’t been into my office much, so it took me a while to pay attention to the fact that much of its networking had recently broken. <a href="https://eclecticlight.co/2021/09/21/el-capitan-and-older-mac-os-x-are-about-to-have-a-security-certificate-problem/">Here’s why</a> (<a href="https://mathstodon.xyz/@11011110/107107828546240163">\(\mathbb{M}\)</a>). It was still running OS X El Capitan (10.11.6) and a crucial top-level certificate expired. The machine is too old (late 2009) for the latest OS X but it looks like I can and should upgrade to High Sierra, 10.13. So much for getting anything else accomplished today…</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/15/linkage.html"><span class="datestr">at October 15, 2021 03:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7816227323646004961">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html">A Young Person's Game?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>When László Babai first announced his graph isomorphism in quasipolynomial time result, I <a href="https://blog.computationalcomplexity.org/2015/11/a-primer-on-graph-isomorphism.html">wrote</a></p><blockquote><p>We think of theory as a young person's game, most of the big breakthroughs coming from researchers early in their careers. Babai is 65, having just won the Knuth Prize for his lifetime work on interactive proofs, group algorithms and communication complexity. Babai uses his extensive knowledge of combinatorics and group theory to get his algorithm. No young researcher could have had the knowledge base or maturity to be able to put the pieces together the way that Babai did.</p></blockquote><p>Babai's proof is an exceptional story, but it is exceptional. Most CS theorists have done their best work early in their career. I got myself into a <a href="https://twitter.com/fortnow/status/1447681436513882112">twitter discussion</a> on the topic. For me, I'm proud of the research I did through my forties, but I'll always be best known, research wise, for my work on interactive proofs around 1990. It would be hard to run a scientific study to determine cause and effect but here are some reasons, based on my own experiences, on why we don't see research dominated by the senior people in theory.</p><p></p><b>The field changes - </b>Computation complexity has moved from a computational-based discipline to one now dominated by combinatorics, algebra and analysis. I'm not complaining, a field should evolve over time but it plays less to my strengths. It's hard to teach this old dog new tricks.<div><b>The fruit hanged lower - </b>there were important problems with easier proofs available then not available now<div><b>Responsibilities </b>- You have fewer as a PhD student, postdoc or assistant professor.</div><div><b>Family - </b>becomes more of a focus.</div><div><b>Taking on new jobs - </b>Many academics, though not all, take on administrative roles at their university or , or leave academics completely. </div><div><b>The young people have the new ideas </b>- And older people get settled in their ways</div><div><b>The thrill is gone or at least decays - </b>Your first theorem, your first talk, your first conference paper gives you a level of excitement that's hard to match.</div><div><b>Existentialism - </b>The realization that while computing has no doubt changed the world, my research, for the most part, hasn't.</div><div><b>Cognitive Decline</b> - Probably the most controversial but for me I find it hard to focus on problems like I used to. Back in the day I prided myself on knowing all the proofs of my theorems, now I can't even remember the theorems.</div><div><br /></div><div>Honestly there is just nothing wrong with taking on new roles, writing books, surveys and blogs, focusing on teaching and mentorship and service and leaving the great research to the next generation.</div><div><p></p></div></div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html"><span class="datestr">at October 15, 2021 01:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/">Multiple faculty positions in theory of computing at York University (apply by November 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>York University in Toronto, Canada is inviting applications for five tenured or tenure-track positions in the theory of computing or data science (both broadly interpreted). The review of applications will begin on November 15, and full applications are due by November 30.</p>
<p>Website: <a href="https://lassonde.yorku.ca/about/careers/faculty-recruitment">https://lassonde.yorku.ca/about/careers/faculty-recruitment</a><br />
Email: eecsjoin@yorku.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/15/multiple-faculty-positions-in-theory-of-computing-at-york-university-apply-by-november-30-2021/"><span class="datestr">at October 15, 2021 01:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-10-15-Nakamoto-Consensus/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-10-15-Nakamoto-Consensus/">Nakamoto's Longest-Chain Wins Protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post, we will cover Nakamoto’s Consensus protocol presented in the Bitcoin whitepaper. There are a lot of posts and videos that explain this seminal and simple protocol. Our goal in this post will be to intuitively piece out the need for different aspects of the protocol, such as...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-10-15-Nakamoto-Consensus/"><span class="datestr">at October 15, 2021 04:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/15/rqs-postdoctoral-fellowship-at-university-of-maryland-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/15/rqs-postdoctoral-fellowship-at-university-of-maryland-apply-by-december-1-2021/">RQS Postdoctoral Fellowship at University of Maryland (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The NSF Quantum Leap Challenge Institute for Robust Quantum Simulation (<a href="https://rqs.umd.edu">https://rqs.umd.edu</a>) seeks exceptional candidates for the RQS Postdoctoral Fellowship. Applications should be submitted through AcademicJobsOnline at <a href="https://academicjobsonline.org/ajo/jobs/19910">https://academicjobsonline.org/ajo/jobs/19910</a>.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/19910">https://academicjobsonline.org/ajo/jobs/19910</a><br />
Email: quics-coordinator@umiacs.umd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/15/rqs-postdoctoral-fellowship-at-university-of-maryland-apply-by-december-1-2021/"><span class="datestr">at October 15, 2021 12:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/14/assistant-professor-of-computer-science-at-california-state-university-east-bay-apply-by-october-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/14/assistant-professor-of-computer-science-at-california-state-university-east-bay-apply-by-october-31-2021/">Assistant Professor of Computer Science at California State University East Bay (apply by October 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at CSUEB invites applications for 2 tenure-track appointments as Assistant Professor in Computer Science (considering all areas of computer science, capable of teaching in emerging areas) starting Fall 2022.</p>
<p>Website: <a href="http://www.csueastbay.edu/oaa/jobs/csueb.html">http://www.csueastbay.edu/oaa/jobs/csueb.html</a><br />
Email: cs@csueastbay.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/14/assistant-professor-of-computer-science-at-california-state-university-east-bay-apply-by-october-31-2021/"><span class="datestr">at October 14, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/smoothing/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/smoothing/">Certified Patch Robustness Via Smoothed Vision Transformers (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://gradientscience.org/certivited.pdf" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/madrylab/smoothed-vit" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>In a series of two blog posts, we dive into how to build practical certified defenses against adversarial patches.
In Part I below, we give an overview of smoothing-based defenses, a popular class of certified defenses, and walk through a specific example of using de-randomized smoothing to defend against adversarial patch attacks.
In <a href="https://gradientscience.org/smoothed-vit">Part II</a>, we discuss our latest work that demonstrates how using vision transformers with de-randomized smoothing (smoothed ViTs) can significantly improve these defenses. Our approach not only provides better certified defenses against adversarial patches but also maintains standard accuracy and inference times at levels comparable to that of regular (non-robust) models. Thanks to these improvements, smoothed ViTs become a truly viable alternative to regular models.
</i></p>

<div class="container">
    <button style="float: left; width: 25%; margin-left: 150px;" class="rbutton">❮</button>
    <button style="float: right; width: 25%; margin-right: 150px;" class="rbutton">❯</button>
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide12.png" class="mySlides" />
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide7.png" class="mySlides" />
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide8.png" class="mySlides" />
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide9.png" class="mySlides" />
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide10.png" class="mySlides" />
  <img src="https://gradientscience.org/assets/certified-vit/schematic/Slide11.png" class="mySlides" />
</div>



<p>Adversarial examples are small modifications of inputs that can induce machine learning models to misclassify those inputs. These attacks can be roughly categorized into two broad classes: small, synthetic perturbations that are visually indistinguishable from the original input, and physically realizable attacks that can break deployed machine learning systems in the wild. One popular attack in the latter category, known as adversarial patches, maliciously perturbs a bounded region in the image (typically a square).</p>

<div>
    <div class="halfblock">
        <div class="block">
            <span style="text-align: center;">Clean image</span>
            <img src="https://gradientscience.org/assets/certified-vit/images/threshold_illustration_schematic/duck_8.png" />
            <span style="text-align: center; font-weight: 400;">Predicted class: Duck</span>
        </div>
    </div>
    <div class="halfblock">
        <div class="block">
            <span style="text-align: center;">Attacked image</span>
            <img src="https://gradientscience.org/assets/certified-vit/images/threshold_illustration_schematic/duck_attacked_largepatch_8.png" />
            <span style="text-align: center; font-weight: 400;">Predicted class: Boat</span>
        </div>
    </div>
</div>

<div class="footnote">
An example of an adversarial patch attack. A square patch is added to the image in order to cause the classifier to misclassify the image as something else, such as a boat instead of a duck. 
</div>

<p>Adversarial patches can be used to undermine a variety of vision-based tasks—in particular, have been used to <a href="https://arxiv.org/abs/1712.09665">deceive image classifiers</a>, <a href="https://arxiv.org/abs/1906.11897">manipulate object detectors</a>, and <a href="https://arxiv.org/abs/1910.10053">disrupt optical flow estimation</a>. Such attacks tend to be relatively easy to implement too as they just require, e.g., <a href="https://arxiv.org/abs/1707.08945">printing adversarial stickers</a>, <a href="https://arxiv.org/abs/1910.14667">creating adversarial pieces of clothing</a>,or <a href="https://dl.acm.org/doi/pdf/10.1145/2976749.2978392">manufacturing adversarial glasses</a>.</p>

<p>While several heuristic defenses have been proposed to protect against adversarial patches, some of these defenses were shown to <a href="https://arxiv.org/abs/2003.06693">not be fully effective</a>. This is unfortunately <a href="https://arxiv.org/abs/2002.08347">a fairly common theme in adversarial examples research</a>.</p>

<p>Certified defenses attempt to alleviate such empirical evaluation problems by providing robustness guarantees that are provable. However, certified guarantees tend to be modest and come at a steep cost. For instance, <a href="https://arxiv.org/abs/2002.10733">recently proposed certified defenses against adversarial patches</a> can guarantee only 14%  robust accuracy on ImageNet. Furthermore, certified models are often substantially slower (by even 2-4 orders of magnitude) and significantly less accurate (in terms of standard accuracy) than non-robust models, which severely limits their practicality.</p>

<h2 id="smoothing-models-for-provable-robustness">Smoothing models for provable robustness</h2>
<p>We begin our discussion of certified defenses with an overview of smoothing, a general strategy for creating models that are provably robust to adversarial attacks. They revolve around the idea of deriving a (robust) prediction by aggregating many  variations  of the original input (e.g., ones that correspond to <a href="https://arxiv.org/abs/1902.02918">adding Gaussian noise</a>, <a href="https://arxiv.org/abs/1911.09272">masking parts of the input</a>, or <a href="https://arxiv.org/abs/2007.08450">augmenting it with generative models</a>). The underlying intuition is that while it might be easy for an adversary to attack a single input, it is much harder for them to simultaneously attack all of the input variations. Thus, instead of directly classifying the original input, we classify each of such variations of that input and then output the most frequently predicted class—if the number of variations that agree with this class is high enough then the prediction is guaranteed to be robust.</p>

<p>Let’s dive into the details! To create a smoothed classifier, we change the typical  forward pass of our classifier (often referred to as the base classifier) into a three-step procedure:</p>

<ol>
  <li>Construct variations of the input.</li>
  <li>Classify all input variations individually.</li>
  <li>Output the most frequently predicted class.</li>
</ol>

<p>Now, once we have such a smoothed classifier, how can we certify its robustness? The key assumption for certification is that an adversary can only affect a limited number of input variations. If this is true, certification amounts to a simple counting problem: if the difference between the first- and second-most frequently predicted classes is larger than what an adversary could ever change via their manipulation, then the prediction is guaranteed to be robust! Importantly, for this approach to work well, it is key that these variations remain informative of the correct class while remaining hard for the adversary to perturb many of them simultaneously. This gives rise to a natural trade-off between robustness and accuracy.</p>

<p>By choosing different types of input variations, smoothing techniques can defend against different types of adversarial attacks. For example, smoothing classifiers with random additive noise can provide certified defenses against adversarial <a href="https://arxiv.org/abs/1902.02918">L2</a> and <a href="https://arxiv.org/abs/2002.08118">L1</a> attacks. On the other hand, smoothing over “pixel flows” can provide <a href="https://arxiv.org/abs/1910.10783">certified defenses against Wasserstein perturbations</a>. One can even “smooth” the entire pipeline of both training and inference (treating it as a single module), to get robustness to <a href="https://arxiv.org/abs/2002.03018">data poisoning attacks</a>. In general, if we can construct a set of input variations so that the adversary can only affect some fraction of them, we can use smoothing to defend against the corresponding attacks!</p>

<h2 id="derandomized-smoothing-a-smoothing-defense-against-patch-attacks">Derandomized Smoothing: A Smoothing Defense against Patch Attacks</h2>

<p>How can we use smoothing to defend against adversarial patches specifically? Recall that to be able to create a smoothing defense, we need to come up with a way to generate variations of an image that limit the effect of an adversarial patch and still be indicative of the correct class label. Here, we exploit a key property of adversarial patches: they are limited to a small, contiguous region of the input image.</p>

<p>Specifically, for each variation, we mask out all of the image except for a small region. In most of these variations, the adversarial patch will be fully masked, and thus cannot affect the model’s prediction by construction. This technique was originally introduced in a certified defense known as <a href="https://arxiv.org/abs/2002.10733">derandomized smoothing (Levine &amp; Feizi, 2020)</a>. Typically, we leave a column of the image unmasked as shown in the figure below, otherwise known as a column ablation (check out the original paper for some other ablation types).</p>

<div id="ablation_figure">
    <div>
        <div class="seventhblock">
        </div>
    </div>
</div>

<div class="footnote">
    Example of column ablations for an input image. Most of the image is masked out except for a column of the image. An adversarial patch can only affect a column ablation if the patch itself is located within the ablation, thus limiting the effect of an adversary. Click on the image to see other column ablations for the same image. 
</div>

<div class="seventhblock token_column"></div><img src="https://gradientscience.org/' + imgUrl + '" />

<h2 id="certified-guarantee-against-patch-attacks">Certified guarantee against patch attacks</h2>

<p>Now that we have our smoothed classifier, how can we prove that the prediction is robust to patch attacks? Since a patch is of limited size, an adversary can only affect the few image variations that physically overlap with the patch. Consequently, if the margin of the winning prediction is large enough, then we can guarantee robustness to adversarial patches.</p>

<div>
    <div>
        <div class="thirdblock">
            <div class="rbutton block clicked sc patch_button" id="no_patch">No patch</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block sc patch_button" id="small_patch">32x32 patch</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block sc patch_button" id="large_patch">64x64 patch</div>
        </div>
    </div>
    <div>
        <div class="twofifthsblock">
            <div class="block full_div">
                <span style="text-align: center;" id="full_label"></span>
                <img src="https://gradientscience.org/feed.xml" id="full_image" />
            </div>
        </div>
        <div class="threefifthsblock">
            <div class="block">
                <span style="text-align: center;">Classified column ablations</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Duck</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Boat</span>
            </div>
            <div class="eigthblock">
                <div class="block ablation_div">
                    <img src="https://gradientscience.org/feed.xml" class="ablation_column" />
                </div>
                <span style="text-align: center;" class="ablation_label">Finch</span>
            </div>
        </div>
    </div>
    <div class="block" id="ablation_container">
        <canvas width="400" id="majority_vote" height="200"></canvas>
    </div>
</div>



<div class="footnote">
A simplified demonstration of the derandomized smoothing defense for patch robustness. Here, an image of a duck is split into column ablations (our choice of input variation), each of which are classified individually. Since 6 out of 8 column ablations are classified as duck, the smoothed model predicts that this is a duck. A 32x32 patch is only large enough to affect at most 2 column ablations, which is not enough to change the prediction. Thus, this prediction is certifiably robust to 32x32 patches. In contrast, a 64x64 patch can affect up to 3 column ablations and can potentially flip the most frequent class to boat, and so the model is not certifiably robust to 64x64 patches. 
</div>

<p>To make this more precise, let $\Delta$ be the number of ablations that a patch could simultaneously affect (in the worst case). If the winning margin is at least $2\Delta$, then the smoothing defense guarantees that the predicted class is robust to this patch size. The fraction of examples which meet this certification threshold is typically referred to as certified accuracy.</p>

<div class="footnote">
 For adversarial patch attacks of size m x m, and column ablation size $b$, we have $\Delta=m+b-1$
</div>

<h2 id="challenges-for-certified-patch-defenses">Challenges for Certified Patch Defenses</h2>

<p>Although certified patch defenses can provide guarantees against adversarial attacks, like many other certified defenses, they face several major challenges that limit their practicality:</p>

<ol>
  <li>They provide relatively modest guarantees, and can only guarantee robustness for a relatively small fraction of inputs and/or only a small patch size.</li>
  <li>The standard accuracy is typically much lower than that of a standard, non-robust model, forcing on the practitioners an unfavorable “trade-offs” between robustness and accuracy.</li>
  <li>Inference times tend to be several orders of magnitude larger than that of a standard, non-robust model, making certified defenses difficult to deploy in real-time settings.</li>
</ol>

<p>In the <a href="https://gradientscience.org/smoothed-vit">next post</a>, we discuss our latest work on making certified defenses much more practical. Specifically, we show how leveraging vision transformers (ViT) enables better handling of input variations and significant improvement of the margins for certification. With some straightforward modifications to the ViT architecture, we are also able to develop certifiably robust models that not only outperform previous certified defenses but also have practical inference times and standard accuracies.</p></div>







<p class="date">
<a href="https://gradientscience.org/smoothing/"><span class="datestr">at October 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/smoothed-vit/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/smoothed-vit/">Certified Patch Robustness Via Smoothed Vision Transformers (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://gradientscience.org/certivited.pdf" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/madrylab/smoothed-vit" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>In our <a href="https://gradientscience.org/smoothing">previous post</a>, we gave an overview of smoothing-based approaches to certified defenses, and described a certified patch defense known as de-randomized smoothing. In our latest work, we show how to leverage vision transformers (ViTs) to significantly improve such certified patch defenses along all possible axes: standard accuracy, certified accuracy, and speed. In fact, with some straightforward modifications, our certified ViTs not only enable significantly better certified robustness but they also maintain standard accuracy and inference times that are comparable to that of regular (non-robust) models. This progress marks a substantial step forward for certified defenses as a practical alternative to regular models. 
</i></p>

<div class="container">
  <img src="https://gradientscience.org/assets/certified-vit/schematic/vit_schematic.png" class="mySlides" />
</div>
<div class="footnote">
     <b>An overview of the Smoothed ViT.</b>
</div>

<p>The main appeal of certified defense is their ironclad guarantees: a certified result leaves no room for doubt and is not prone to blind-spots of empirical evaluation. However, practitioners care not only about certified performance: the accuracy and the inference speed of the model are equally important factors too. Unfortunately, nearly all certified defenses come at a severe price: (1) the standard accuracy plummets in comparison to a standard model and/or (2) inference time is orders of magnitude slower than that of a standard model.</p>

<div class="footnote">
A standard ResNet-50 can achieve about 76% accuracy on the ImageNet benchmark, and takes less than 1 second to make a prediction on a typical GPU. In contrast, top-performing certified defenses such as <a href="https://arxiv.org/abs/2002.10733">Levine &amp; Feizi (2020)</a> reports 44% standard accuracy and takes 150 seconds to make a prediction on similar hardware. 
</div>

<p>This trade-off between robustness and performance can be untenable even for safety-critical applications. For example, object detection systems deployed in autonomous vehicles need to recognize obstacles and signs quickly in real time. Consequently, in addition to being robust, these systems also need to be fast and accurate.</p>

<p>To approach this tradeoff between robustness and accuracy, we take a closer look at certified defenses from an architectural perspective. Specifically, smoothing defenses use a backbone architecture that is typically implemented using classic convolutional networks, the default choice across much of computer vision. But are these architectures really the best tool for the job?</p>

<p>In our work, we show that when moving from standard models to certified defenses, it may be worth it to rethink the core architecture behind them. Specifically, we find that smoothing-based patch defenses can drastically improve when using vision transformers (ViTs) instead of convolutional networks as their backbone. With some additional adjustments to the transformer architecture, we not only achieve superior certified accuracy to prior work but also reach standard accuracies and inference times comparable to a standard, non-robust ResNet!</p>

<h2 id="rethinking-smoothing-backbones">Rethinking smoothing backbones</h2>

<p>Recall from the <a href="https://gradientscience.org/smoothing">previous post</a> that smoothing defenses use a base classifier to make predictions on many variations of the original input. For patch defenses, this amounts to using the base classifier to classify column ablations (i.e., masked images that only reveal a small column of the original image). Consequently, the accuracy of the base classifier at classifying columns of an image directly translates into the performance of the certified defense.</p>

<p>In the light of this observation, it is crucial to realize that convolutional networks typically process complete (i.e., not masked) images. So, this type of architecture might not actually be best suited for classifying the highly masked images used in smoothing defenses.</p>

<p>Why might we expect convolutions to be suboptimal for masked images? CNNs slide filters across the image and grow the receptive field across layers of the network. This large receptive field, while usually being a strength of CNNs for standard settings, forces the network to process a large amount of masked pixels when processing ablated images. Since these pixels carry no information, the network must learn to detect and ignore these masked regions (in addition to performing unnecessary computation).</p>

<h2 id="vision-transformers-for-ablated-images">Vision transformers for ablated images</h2>

<p>Unlike CNNs, vision transformers (ViTs) use attention modules instead of convolutions as their main building block. Crucially, attention modules can ignore masked regions instead of processing them.</p>

<p>To test out this intuition, we measured the accuracy of various convolutional and ViT architectures at classifying column ablations. It turns out that ViTs are significantly better at this task—for example, on ImageNet, a ViT-S has 12% higher accuracy at classifying column ablations than a similarly sized ResNet-50.</p>

<div>
    <div>
        <div class="thirdblock">
            <div class="rbutton block sc ablation_button" id="ablation_small">Small</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_medium">Medium</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block sc ablation_button" id="ablation_large">Large</div>
        </div>
    </div>
    <div>
        <canvas width="400" id="ablation_acc" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Ablation accuracy of ViTs vs. ResNets:</b> A comparison of the accuracy of ViTs with similarly sized ResNets at classifying image ablations.  
</div>



<p>These improvements in ablation accuracy directly boosts both the certified and standard accuracies for the smoothed model. For example, the increase in ablation accuracy improves  the certified accuracy of a smoothed ViT-S by 13% (as compared to a similarly sized ResNet-50) and the standard accuracy by 12%.</p>

<p>With some additional adjustments to the certification procedure <a href="https://gradientscience.org/certivited.pdf">described in our paper</a>, our largest certifiably robust vision transformer can reach 73% standard accuracy—only 3% off from the standard accuracy of a regular, non-robust ResNet-50, and almost 30% higher than previous versions of de-randomized smoothing!</p>

<div>
    <div>
        <div class="thirdblock">
            <div class="rbutton block sc cert_button" id="cert_small">Small</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block clicked sc cert_button" id="cert_medium">Medium</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block sc cert_button" id="cert_large">Large</div>
        </div>
    </div>
    <div>
        <canvas width="400" id="cert_acc" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Certified accuracy of ViTs vs. ResNets:</b> A comparison of the certified accuracy of smoothed ViTs/ResNets when using different ablation sizes.  
</div>



<div>
    <div>
        <div class="thirdblock">
            <div class="rbutton block sc standard_button" id="standard_small">Small</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block clicked sc standard_button" id="standard_medium">Medium</div>
        </div>
        <div class="thirdblock">
            <div class="rbutton block sc standard_button" id="standard_large">Large</div>
        </div>
    </div>
    <div>
        <canvas width="400" id="standard_acc" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Standard accuracy of ViTs vs. ResNets:</b> A comparison of the standard accuracy of smoothed ViTs/ResNets when using different ablation sizes.  
</div>



<h2 id="modifying-vits-to-speed-up-inference-time">Modifying ViTs to Speed up Inference Time</h2>

<p>Also, recall that a smoothed classifier tends to be significantly slower than a standard model because it needs to classify a large number of input variations for each input image. Indeed, <a href="https://arxiv.org/abs/1902.02918">traditional randomized smoothing approaches</a> have used upwards of 100,000 variations, resulting in a forward pass that is 4-5 orders of magnitude slower than a standard model. These costs are infeasible in scenarios where decisions need to be made in a fraction of a second, such as autonomous driving. As <a href="https://arxiv.org/abs/2002.10733">Levine &amp; Feizi</a> have shown, for patch defenses, a move towards <em>deterministic</em> smoothing (as opposed to randomized) can result in a great speed up (by reducing the number of variations needed to be classified). However, the resulting solution is still two orders of magnitude slower than standard models.</p>

<p>In order to speed up inference (much) further, we leverage the token-based nature of ViTs to gracefully handle image ablations. Specifically, note that ViTs process images as tokens, where each token represents a contiguous patch of the input image. Since the runtime of a ViT is proportional to the number of tokens, a natural approach to speed up inference is to simply drop the masked out tokens, as shown in the following figure:</p>

<div id="token_figure">
    <div>
        <div class="seventhblock">
        </div>
    </div>
</div>
<div class="footnote">
    Vision transformers split images into patches, which are processed by the transformer architecture. When processing column ablated images, we can simply pass only the tokens in the ablation to significantly reduce the computational complexity of smoothing. 
</div>
<div class="seventhblock token_column"></div><img src="https://gradientscience.org/' + imgUrl + '" />

<p>We modify the ViT architecture to drop fully masked tokens in such a manner. As expected, this significantly speeds up the inference time of the smoothed model! Indeed, with this optimization, our smoothed ViT-S is faster than a smoothed ResNet-50 by an order of magnitude. In our <a href="https://gradientscience.org/certivited.pdf">paper</a>, we discuss how additional improvements to the certification procedure can make our ViT architectures <i>another</i> order of magnitude faster. In total, our fastest smoothed ViT is actually comparable in terms of inference speed to a standard, non-robust ResNet.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Smoothed ViTs present a simple but effective way to achieve state-of-the art certified accuracy. These models are also realistically deployable, as they maintain similar accuracy and inference speed as non-robust models. The key here is the vision transformer’s inherent ability to quickly and accurately process image ablations. These advancements are thus a step towards certified defenses that can be practically deployed in real-world scenarios.</p></div>







<p class="date">
<a href="https://gradientscience.org/smoothed-vit/"><span class="datestr">at October 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/13/relevant-neighbors">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/13/relevant-neighbors.html">Relevant neighbors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I have a new preprint, <a href="https://arxiv.org/abs/2110.06163">“Finding Relevant Points for Nearest-Neighbor Classification”, arXiv:2110.06163</a>, to appear in January at the <a href="https://www.siam.org/conferences/cm/conference/sosa22">SIAM Symposium on Simplicity in Algorithms (SOSA22)</a>. It’s about points in Euclidean spaces of dimension three or more, but I thought it would make a good warm-up to discuss here the one-dimensional version of the same problem, solved (together with the 2d version) by Bremner, Demaine, Erickson, Iacono, Langerman, Morin, and Toussaint in their paper <a href="http://dx.doi.org/10.1007/s00454-004-1152-0">“Output-sensitive algorithms for computing nearest-neighbour decision boundaries”, <em>Discrete Comput. Geom.</em> 2005</a>.</p>

<p>So in this problem, you have a collection of real-valued data points with known discrete classifications (say, a finite set of colors), and you want to guess the color of new points whose color is not already given. Nearest neighbor classification means simply that, to guess the color of \(x\), you find the closest known point \(y\) and guess that \(x\) has the same color as \(y\). One easy way to do this would be to store the known points in a sorted list and use binary search. There’s lots more to say about this (for instance its use in combination with <a href="https://en.wikipedia.org/wiki/Random_projection">random projections</a> for high-dimensional approximate nearest neighbors) but for today I want to focus on the size of this sorted list. We can store a list that is potentially much smaller, but always produces the same results, by keeping only points that have a neighbor with a different classification.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/1d-nnc.svg" alt="One-dimensional nearest neighbor classification on a full data set and the data set trimmed to its relevant points" /></p>

<p>These points with differently-classified neighbors are the “relevant points” of the preprint title. Another way of describing them is that a point is relevant if deleting it would change the classification of some other (unknown) points in space that might later be queried. Among the set of decision boundaries, the ones separating parts of space with different classifications are the ones defined by relevant points. So if we store a nearest-neighbor data structure with only relevant points, we will get the same answer as if we store all known points. But because we’re storing fewer points, it will take less memory and less time (especially if the reduced memory allows it to fit into cache).</p>

<p>If you have the points given to you in sorted order, then it’s easy enough to scan through them in that order, keeping track of which pairs have different classifications, and produce a filtered sequence of the relevant ones. Here it is in Python (with apologies for the low-contrast syntax coloring):</p>

<figure class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">relevant</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">classify</span><span class="p">):</span>
    <span class="s">"""Filter points whose classification differs from a neighbor.
    Arguments are a sequence of points, and a function to classify
    each point. The return value is an iterator for the filtered sequence."""</span>
    <span class="n">prevpoint</span> <span class="o">=</span> <span class="n">prevclass</span> <span class="o">=</span> <span class="n">prevlisted</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
        <span class="n">xclass</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prevlisted</span> <span class="o">!=</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">xclass</span> <span class="o">!=</span> <span class="n">prevclass</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prevlisted</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">prevpoint</span>
            <span class="k">yield</span> <span class="n">x</span>
            <span class="n">prevlisted</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prevlisted</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">prevpoint</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">prevclass</span> <span class="o">=</span> <span class="n">xclass</span></code></pre></figure>

<p>However, as Bremner et al observed, sorting an input to make it usable by this scan does more work than necessary, because we don’t need the sorted ordering of all the other points between the relevant ones. Instead, we can use an idea resembling <a href="https://en.wikipedia.org/wiki/Quickselect">quickselect</a>, where we modify the quicksort algorithm to stop recursing in subproblems where sorting is unnecessary. For finding relevant points, these subproblems are the homogeneous ones, in which all points have the same classification as each other. Bremner et al combined this idea with a version of quicksort that always partitions its subproblems at the exact median, in order to achieve a good worst-case time bound, but if we’re happy with expected analysis we can use the same random-pivoting idea as the more usual form of quicksort:</p>

<figure class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">quickrelevant</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">classify</span><span class="p">):</span>
    <span class="s">"""Same output as relevant(sorted(list(seq)),classify).
    We assume the input sequence is sortable and has no repeat values."""</span>

    <span class="k">def</span> <span class="nf">supersequence</span><span class="p">():</span>
        <span class="s">"""Generate sorted supersequence of relevant points by quicksort-like
        recursive subdivision, stopping at homogeneous subproblems.
        We include the endpoints of each subproblem, even though some might
        not be relevant, so the results should be cleaned up using relevant().
        We use an explicit stack to handle the recursion, avoiding the need
        to pass yielded outputs back through a call stack."""</span>

        <span class="n">liststack</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">seq</span><span class="p">)]</span>
        <span class="k">while</span> <span class="n">liststack</span><span class="p">:</span>
            <span class="n">L</span> <span class="o">=</span> <span class="n">liststack</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
        
            <span class="c1"># Base cases of recursion: lists of zero or one item
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">L</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">continue</span>
            
            <span class="c1"># Test whether L is homogeneous
</span>            <span class="c1"># and if so only generate its extreme values
</span>            <span class="n">homogeneous</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">firstclass</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">firstclass</span> <span class="o">!=</span> <span class="n">classify</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                    <span class="n">homogeneous</span> <span class="o">=</span> <span class="bp">False</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">homogeneous</span><span class="p">:</span>
                <span class="k">yield</span> <span class="nb">min</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
                <span class="k">yield</span> <span class="nb">max</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
                <span class="k">continue</span>
            
            <span class="c1"># Divide and conquer with random pivot
</span>            <span class="n">pivot</span> <span class="o">=</span> <span class="n">L</span><span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">))]</span>
            <span class="n">low</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">high</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">L</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">pivot</span><span class="p">:</span>
                    <span class="n">low</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">high</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">liststack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">high</span><span class="p">)</span>
            <span class="n">liststack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">low</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">relevant</span><span class="p">(</span><span class="n">supersequence</span><span class="p">(),</span><span class="n">classify</span><span class="p">)</span></code></pre></figure>

<p>The time complexity of this algorithm can be analyzed much like <a href="https://11011110.github.io/blog/2007/10/09/blum-style-analysis-of.html">the analysis of quickselect</a>, by observing that the time is proportional to the number of comparisons with pivots, computing the probability that each possible comparison happens, and summing. In quicksort, two distinct elements at distance \(d\) from each other in the final sorted output are compared whenever one of them is the first to be chosen as a pivot in the interval between them, which happens with probability exactly \(2/(d+1)\). In quickrelevant, this same probability holds for pairs that are separated by one of the decision boundaries. But pairs of elements that are within the same homogeneous block are less likely to be compared, because of the possibility that the recursion will stop before it separates or compares them.</p>

<p>If a pair of elements lies within a single block, has distance \(d\) separating them, and is $e$ units from both ends of the block, then it will only be compared if one of the two elements is first to be chosen in at least one of the two intervals of length \(d+e\) extending from the two elements towards an end of their block. This happens with probability at most \(4/(d+e)\), because there are two ways of choosing which element to pick first as the pivot and two ways of choosing which extended interval it is first in.</p>

<p>If we sum up these probabilities, for pairs involving a single element that is \(e\) units from its nearest block boundary among a set of \(n\) elements, we get \(O\bigl(\log(n/e)\bigr)\) as the expected contribution to the total time for that one element. If we sum the contributions from the elements within a block, for a block of length \(\ell_i\), we get a total expected contribution from that block of \(O\bigl(\ell_i\log(n/\ell_i)\bigr)\). And if we have \(k\) relevant points and \(O(k)\) blocks, and we sum over all blocks, the total time is maximized when all the blocks are of equal size, \(\Theta(n/k)\), for which we get total time \(O(n\log k)\).</p>

<p>For quicksort and quickselect, it’s possible to be more careful in the analysis, derive exact formulas for the probability of making each comparison, and from them get an analysis of the expected number of comparisons that does not use \(O\)-notation for its leading term; see my linked post on the quickselect analysis. Probably it’s possible here too but it looks messier. Maybe it would make a good undergraduate research project. One thing to be careful of is that the comparisons are not over in the homogeneous case; finding the min and max simultaneously, in a block of length \(\ell_i\), takes roughly \(3\ell_i/2\) comparisons. But that should only be a lower-order term compared to the \(O(n\log k)\) leading term of the analysis.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107098438066916443">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/13/relevant-neighbors.html"><span class="datestr">at October 13, 2021 11:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/13/postdoc-in-theory-of-machine-learning-at-harvard-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/13/postdoc-in-theory-of-machine-learning-at-harvard-university-apply-by-december-1-2021/">Postdoc in Theory of Machine Learning at Harvard University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for exceptional junior scientists to work collaboratively with a group of faculty from Computer Science (Boaz Barak), Statistics (Lucas Janson), Electrical Engineering (Demba Ba) and Applied Mathematics (Cengiz Pehlevan) towards a theory of representations in artificial and natural systems.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/10730">https://academicpositions.harvard.edu/postings/10730</a><br />
Email: theory-postdoc-apply@seas.harvard.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/13/postdoc-in-theory-of-machine-learning-at-harvard-university-apply-by-december-1-2021/"><span class="datestr">at October 13, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/13/rabin-postdoc-at-harvard-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/13/rabin-postdoc-at-harvard-university-apply-by-december-1-2021/">Rabin Postdoc at Harvard University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for junior scientists in theoretical computer science, broadly construed.<br />
The normal duration of the Rabin Fellowship is two years. Rabin Fellows will receive a generous salary as well as an allocation for research and travel expenses.<br />
While interaction with Harvard faculty, students, and visitors is encouraged, Rabin Fellows are free to pursue their own interests.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/10730">https://academicpositions.harvard.edu/postings/10730</a><br />
Email: theory-postdoc-apply@seas.harvard.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/13/rabin-postdoc-at-harvard-university-apply-by-december-1-2021/"><span class="datestr">at October 13, 2021 10:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/143">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/143">TR21-143 |  Hitting Sets For Regular Branching Programs | 

	Edward Pyne</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We construct an explicit $\varepsilon$-hitting set generator (HSG) for regular ordered branching programs of length $n$ and $\textit{unbounded width}$ with a single accept state that has seed length
    \[O(\log(n)(\log\log(n)+\log(1/\varepsilon))).\]
    Previously, the best known seed length for regular branching programs of width $w$ with a single accept state was by Braverman, Rao, Raz and Yehudayoff (FOCS 2010, SICOMP 2014) and Hoza Pyne and Vadhan (ITCS 2021), which gave
    \[O(\log(n)(\log\log(n)+\min\{\log(w),\log(n)\}+\log(1/\varepsilon))).\]
    We also give a simple co-HSG for the model with optimal seed length $O(\log n)$.
    
    For the more restricted model of $\textit{permutation}$ branching programs, Hoza Pyne and Vadhan (ITCS 2021) constructed a PRG with seed length matching our HSG, and then Pyne and Vadhan (CCC 2021) developed an error-reduction procedure that gave an HSG (in fact a ``weighted PRG'') with seed length $\widetilde{O}(\log(n)\sqrt{\log(n/\varepsilon)}+\log(1/\varepsilon)).$ We show that if an analogous error-reduction result could be obtained for our HSG, there is an explicit HSG for general ordered branching programs of width $w=n$ with seed length $\widetilde{O}(\log^{3/2}n)$, improving on the $O(\log^2n)$ seed length of Nisan (Combinatorica 1992).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/143"><span class="datestr">at October 13, 2021 03:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/10/13/ideal-mini-workshop-on-statistical-and-computational-aspects-of-robustness-in-high-dimensional-estimation/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/10/13/ideal-mini-workshop-on-statistical-and-computational-aspects-of-robustness-in-high-dimensional-estimation/">IDEAL mini-workshop on “Statistical and Computational Aspects of Robustness in High-dimensional Estimation”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
October 19, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-statistical-and-computational-aspects-of-robustness-in-high-dimensional-estimation/ Registration deadline: October 18, 2021 Today’s data pose unprecedented challenges to statisticians. It may be incomplete, corrupted or exposed to some unknown source of contamination or adversarial attack. Robustness is one of the revived concepts in statistics and machine learning that can accommodate such complexity and glean useful information from … <a href="https://cstheory-events.org/2021/10/13/ideal-mini-workshop-on-statistical-and-computational-aspects-of-robustness-in-high-dimensional-estimation/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “Statistical and Computational Aspects of Robustness in High-dimensional Estimation”</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/10/13/ideal-mini-workshop-on-statistical-and-computational-aspects-of-robustness-in-high-dimensional-estimation/"><span class="datestr">at October 13, 2021 03:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/10/13/prediction/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/10/13/prediction/">Machine learning is not nonparametric statistics.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Many times in my career, I’ve been told by respected statisticians that machine learning is nothing more than nonparametric statistics. The longer I work in this field, the more I think this view is both misleading and unhelpful. Not only can I never get a consistent definition of what “nonparametric” means, but the jump from statistics to machine learning is considerably larger than most expect. Statistics is an important tool for understanding machine learning and randomness is valuable for machine learning algorithm design, but there is considerably more to machine learning than what we learn in elementary statistics.</p>

<p>Machine learning at its core is the art and science of <em>prediction</em>. By prediction, I mean the general problem of leveraging regularity of natural processes to guess the outcome of yet unseen events. As before, we can formalize the prediction problem by assuming a population of $N$ individuals with a variety of attributes. Suppose each individual has an associated variable $X$ and $Y$. The goal of prediction is to guess the value of $Y$ from $X$ that minimizes some error metric.</p>

<p>A classic prediction problem aims to find a function that makes the fewest number of incorrect predictions across the population.  Think of this function like a computer program that takes $X$ as input and outputs a prediction of $Y$. For a fixed prediction function, we can sum up all of the errors made on the population. If we divide by the size of the population, this is the mean error rate of the function.</p>

<p>A particularly important prediction problem is classification. In classification, the attribute $Y$ takes only two values: the input $X$ could be some demographic details about a person, and $Y$ would be whether or not that person was taller than 6 feet. The input $X$ could be an image, and $Y$ could be whether or not the image contains a cat. Or the input could be a set of laboratory results about a patient, and $Y$ could be whether or not the patient is afflicted by a disease. Classification is the simplest and most common prediction problem, one that forms the basis of most contemporary machine learning systems.</p>

<p>For classification problems, it is relatively straightforward to compute the best error rate achievable. First, for every possible value of the attribute $X$, collect the subgroup of individuals of the population with that value. Then, the best assignment for the prediction function is the one that correctly labels the majority of this subgroup. For example, in our height example, we could take all women, aged 30, born in the United States, and reside in California. Then the optimal label for this group would be decided based on whether there are more people in the group who are taller than 6 feet or not. (Answer: no).</p>

<p>This minimum error rule is intuitive and simple, but computing the rule exactly requires examining the entire population. What can we do if we work from a subsample? Just as was the case in experiment design, we’d like to be able to design good prediction functions from a small sample of the population so we don’t have to inspect all individuals. For a <em>fixed</em> function, we could use the same law-of-large-numbers approximations to estimate the best decision. That is, if we decide in advance upon a prediction function, we could estimate the percentage of mistakes on the population by gathering a random sample and computing the proportion of mistakes on this subset. Then we could apply a standard confidence interval analysis to extrapolate to the population.</p>

<p>However, what if we’d like to find a good predictor on the population using only a set of examples sampled from the population. We immediately run into an issue: to find the best prediction function, we needed to observe all possible values of $X$. What if we’d like to make predictions about an individual with a set of attributes that was not observed in our sample?</p>

<p>How can we build accurate population-level predictors from small subsamples? In order to solve this problem, we must make some assumptions about the relationship between predictions at related, but different values of $X$. We can restrict our attention to a set of functions that respect regularity properties that we think the predictions should have. Then, with a subsample from the population, we find the function that minimizes the error on the sample and obeys the prescribed regularity properties.</p>

<p>This optimization procedure is called <em>”empirical risk minimization”</em>  and is the core predictive algorithm of machine learning. Indeed, for all of the talk about neuromorphic deep networks with fancy widgets, most of what machine learning does is try to find computer programs that make good predictions on the data we have collected and that respect some sort of rudimentary knowledge that we have about the broader population.</p>

<p>The flexibility in defining what “knowledge” or “regularity” means complicates the solution of such empirical risk minimization problems. What does the right set of functions look like? There are three immediate concerns:</p>

<ol>
  <li>
    <p>What is the right <em>representation</em>? The set needs to contain enough functions to well approximate the true population prediction function. There are a variety of ways to express complex functions, and each expression has its own benefits and drawbacks.</p>
  </li>
  <li>
    <p>The set of functions needs to be simple to search over, so we don’t have to evaluate every function in our set as this would be too time consuming. Efficient search for high quality solutions is called <em>optimization</em>.</p>
  </li>
  <li>
    <p>How will the predictor <em>generalize</em> to the broader population? The functions cannot be too complex or else they will fail to capture the regularity and smoothness of the prediction problem (estimating functions of too high complexity is colloquially called “overfitting”).</p>
  </li>
</ol>

<p>Balancing representation, optimization, and generalization gets complicated quickly, and this is why we have a gigantic academic and industrial field devoted to the problem.</p>

<p>I’m repeating myself at this point, but I again want to pound my fist on the table and reiterate that nothing in our development here requires that the relationship between the variables $X$ and $Y$ be probabilistic. Statistical models are often the starting point of discussion in machine learning, but such models are just a convenient way to describe populations and their proportions. Prediction can be analyzed in terms of a deterministic population, and, just as we discussed in the case of randomized experiments, randomness can be introduced as a means of sampling the population to determine trends. Even generalization, which is usually studied as a statistical phenomenon, can be analyzed in terms of the randomness of the sampling procedure with no probabilistic modeling of the population.</p>

<p>On the other hand, some sort of knowledge about the population is necessary. The more we know about how prediction varies based on changes in the covariates, the better a predictor we can build. Engineering such prior knowledge into appropriate function classes and optimization algorithms form the art and science of contemporary machine learning.</p>

<p>This discussion highlights that while we <em>can</em> view prediction through the lens of statistical sampling, pigeonholing it as simply “nonparametric statistics” does not do the subject justice. While the <a href="https://www.argmin.net/2021/09/28/rct/">jump from mean estimation to causal RCTs is small</a>, the jump from mean estimation to prediction is much less immediate. And in machine learning practice, the intuitions from statistics often don’t apply. For example, conventional wisdom from statistics tells us that evaluating multiple models on the same data set amounts to multiple hypothesis testing, and will lead to overfitting on the test set. However, <a href="https://arxiv.org/abs/1902.10811">there</a> <a href="https://papers.nips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning">is</a> <a href="https://arxiv.org/abs/1906.02168">more</a> <a href="https://arxiv.org/abs/2004.14444">and</a> <a href="https://proceedings.mlr.press/v119/shankar20c.html">more</a> <a href="https://arxiv.org/abs/1905.10498">evidence</a> that using a train-test split leads does not lead to overfitting. Instead, the phenomena we see is that dataset benchmarks can remain useful for decades. Another common refrain from statistics is that model complexity must be explicitly constrained in order to extrapolate to new data, but this also  <a href="https://cacm.acm.org/magazines/2021/3/250713-understanding-deep-learning-still-requires-rethinking-generalization/fulltext">does not seem to apply at all to machine learning practice</a>.</p>

<p>Prediction predates probability and statistics by centuries. As Moritz and I chronicle in the introduction to <a href="http://mlstory.org">Patterns, Predictions, and Actions</a> astronomers were using pattern matching to predict celestial motions, and the astronomer Edmund Halley realized that similar techniques could be used to predict life expectancy when pricing annuities. Moreover, even though modern machine learning embraced contemporary developments in statistics by Neyman, Pearson, and Wald, the tools quickly grew more sophisticated and separate from core statistical practice. In the next post, I’ll discuss an early example of this divergence between machine learning and statistics, describing some of the theoretical understanding of the Perceptron in the 1960s and how its analysis was decidedly different from the theory advanced by statisticians.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/10/13/prediction/"><span class="datestr">at October 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/12/the-khot-naor-approximation-algorithm-for-3-xor/">The Khot-Naor Approximation Algorithm for 3-XOR</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today I would like to discuss the Khot-Naor approximation algorithm for the 3-XOR problem, and an open question related to it.</p>
<p><span id="more-4566"></span></p>
<p>In 3-XOR, we have a system of linear equations modulo 2, with three variables per equation, that might look something like</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bll%7D+x_1+%2B+x_2+%2B+x_4+%26+%5Cequiv+0+%5Cpmod+2%5C%5C+x_1+%2B+x_5+%2B+x_6+%26+%5Cequiv+1+%5Cpmod+2%5C%5C+x_2+%2B+x_3+%2B+x_4+%26+%5Cequiv+1+%5Cpmod+2%5C%5C+x_5+%2B+x_3+%2B+x_6+%26+%5Cequiv+1+%5Cpmod+2+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{ll} x_1 + x_2 + x_4 &amp; \equiv 0 \pmod 2\\ x_1 + x_5 + x_6 &amp; \equiv 1 \pmod 2\\ x_2 + x_3 + x_4 &amp; \equiv 1 \pmod 2\\ x_5 + x_3 + x_6 &amp; \equiv 1 \pmod 2 \end{array} " class="latex" /></p>
<p>The above system is not satisfiable (if we add up the left-hand sides we get 0, if we add up the right-hand sides we get 1), but it is possible to satisfy <img src="https://s0.wp.com/latex.php?latex=%7B3%2F4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3/4}" class="latex" /> of the equations, for example by setting all the variables to 1. In Max 3-XOR problem (which we will simply refer to as “3-XOR” from now on), given a system of equations mod 2 with three variables per equation, we want to find an assignment that satisfies as many equations as possible.</p>
<p>Either setting all the variables to zero or setting all the variables to one will satisfy half of the equations, and the interesting question is how much better than 1/2 it is possible to do on a given instance. <a href="https://epubs.siam.org/doi/abs/10.1137/070691140">Khot and Naor</a> provide an algorithm that, given an instance in which it is possible to satisfy an <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2 + \epsilon}" class="latex" /> fraction of equations, finds a solution that satisfies at least <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2+%2B+%5Cepsilon+%5Ccdot+O+%5Cleft%28+%5Csqrt%7B%5Cfrac+%7B%5Clog+n%7D+n%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2 + \epsilon \cdot O \left( \sqrt{\frac {\log n} n} \right)}" class="latex" /> fraction of equations, where <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is the number of variables. The algorithm is randomized, it runs in polynomial time, and it succeeds with high probability. I believe that it is still the state of the art in terms of worst-case approximation guarantee.</p>
<p>Like the approximation algorithm for sparsest cut in Abelian Cayley graphs implied by the result of Bauer et al. that was the subject of the last two posts, the result of Khot and Naor <em>does not</em> prove a bound on the integrality gap of a relaxation of the problem.</p>
<p>I will describe the Khot-Naor algorithm and describe how it manages to use convex optimization to provide an approximation algorithm, but without establishing an integrality gap bound. I thank my student Lucas Pesenti for explaining the algorithm to me and for thinking together about this problem.</p>
<p>If our 3-XOR instance has <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m}" class="latex" /> equations and <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> variables, then the problem of maximizing the number of satisfied equations can be rewritten as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+m2+%2B+%5Cmax_%7Bx+%5Cin+%5C%7B+-1+%2C+%2B1+%5C%7D%5En%7D+%5C+%5Csum_%7Bi%2Cj%2Ck%7D+c_%7Bi%2Cj%2Ck%7D+x_i+x_j+x_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac m2 + \max_{x \in \{ -1 , +1 \}^n} \ \sum_{i,j,k} c_{i,j,k} x_i x_j x_k " class="latex" /></p>
<p> so that our goal is to approximate the combinatorial optimization problem</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7Bx+%5Cin+%5C%7B+-1+%2C+%2B1+%5C%7D%5En%7D+%5C+%5Csum_%7Bi%2Cj%2Ck%7D+c_%7Bi%2Cj%2Ck%7D+x_i+x_j+x_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \max_{x \in \{ -1 , +1 \}^n} \ \sum_{i,j,k} c_{i,j,k} x_i x_j x_k " class="latex" /></p>
<p> Up to a constant factor loss in the approximation guarantee, Khot and Naor show that the above is equivalent to <a name="tensornorm"></a></p>
<p><a name="tensornorm"></a></p><a name="tensornorm">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cmax_%7Bx%2Cy%2Cz+%5Cin+%5C%7B+-1+%2C+%2B1+%5C%7D%5En%7D+%5C+%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_i+y_j+z_k+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \max_{x,y,z \in \{ -1 , +1 \}^n} \ \sum_{i,j,k} T_{i,j,k} x_i y_j z_k \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="tensornorm"></a><a name="tensornorm"></a> where <img src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi%2Cj%2Ck%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T_{i,j,k}}" class="latex" /> is a symmetric 3-tensor with entries in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+-1%2C0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ -1,0,1\}}" class="latex" /> and with <img src="https://s0.wp.com/latex.php?latex=%7BO%28m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(m)}" class="latex" /> non-zero entries.</p>
<p>Before continuing, let us recall that if <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> is an <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \times n}" class="latex" /> matrix, then its <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_\infty}" class="latex" />-to-<img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_1}" class="latex" /> operator norm has the characterization</p>
<p><a name="inftoone"></a></p>
<p><a name="inftoone"></a></p><a name="inftoone">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M+%7C%7C_%7B%5Cinfty+%5Crightarrow+1+%7D+%3D+%5Cmax_%7Bx%2Cy+%5Cin+%5B-1%2C1%5D%5En%7D+x%5ET+M+y+%3D+%5Cmax_%7Bx%2Cy+%5Cin+%5C%7B+-1%2C1+%5C%7D%5En%7D+x%5ET+M+y+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || M ||_{\infty \rightarrow 1 } = \max_{x,y \in [-1,1]^n} x^T M y = \max_{x,y \in \{ -1,1 \}^n} x^T M y \ \ \ \ \ (2)" class="latex" /></p>
</a><p><a name="inftoone"></a><a name="inftoone"></a></p>
<p>We could also define the “Grothendieck norm” <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+M+%7C%7C_%7BGrot%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|| M ||_{Grot}}" class="latex" /> of a matrix as the following natural semidefinite programming relaxation of the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_\infty}" class="latex" />-to-<img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_1}" class="latex" /> norm:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Blll%7D+%5Cmax+%26+%5Csum_%7Bi%2Cj%7D+M_%7Bi%2Cj%7D+%5Clangle+x_i+%2C+y_j%5Crangle+%5C%5C+%7B%5Crm+s.t.%7D%5C%5C+%26+%7C%7C+x_i+%7C%7C%5E2+%3D+1+%26+i+%3D+1%2C%5Cldots%2Cn%5C%5C+%26+%7C%7Cy_j+%7C%7C%5E2+%3D+1+%26+i+%3D+j%2C%5Cldots%2Cn+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{lll} \max &amp; \sum_{i,j} M_{i,j} \langle x_i , y_j\rangle \\ {\rm s.t.}\\ &amp; || x_i ||^2 = 1 &amp; i = 1,\ldots,n\\ &amp; ||y_j ||^2 = 1 &amp; i = j,\ldots,n \end{array} " class="latex" /></p>
<p> where the <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_i}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7By_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y_j}" class="latex" /> are arbitrary vectors. The Grothendieck inequality is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M%7C%7C_%7B%5Cinfty+%5Crightarrow+1+%7D+%5Cleq+%7C%7C+M+%7C%7C_%7BGrot%7D+%5Cleq+O%281%29+%5Ccdot+%7C%7C+M+%7C%7C_%7B%5Cinfty+%5Crightarrow+1+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || M||_{\infty \rightarrow 1 } \leq || M ||_{Grot} \leq O(1) \cdot || M ||_{\infty \rightarrow 1 }" class="latex" /></p>
<p> where the <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(1)}" class="latex" /> is an absolute constant, known to be less than <img src="https://s0.wp.com/latex.php?latex=%7B1.8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.8}" class="latex" />. Furthermore, the above inequality has a constructive proof, and it leads to a polynomial time constant factor approximation for the problem of finding values <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_i}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7By_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y_i}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pm 1}" class="latex" /> that maximize <a href="https://lucatrevisan.wordpress.com/feed/#inftoone">(2)</a>.</p>
<p>Basically, we can see problem <a href="https://lucatrevisan.wordpress.com/feed/#tensornorm">(1)</a> as the natural generalization of <a href="https://lucatrevisan.wordpress.com/feed/#inftoone">(2)</a> to tensors, and one would like to see a semidefinite programming relaxation of <a href="https://lucatrevisan.wordpress.com/feed/#tensornorm">(1)</a> achieving something resembling the Grothendieck inequality, but with a loss of something like <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\tilde O(\sqrt n)}" class="latex" />. As I mentioned above, this remains an open question, as far as I know.</p>
<p>The idea of Khot and Naor is the following. Suppose that we are given an instance <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of problem <a href="https://lucatrevisan.wordpress.com/feed/#tensornorm">(1)</a>, and suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%2Cy%5E%2A%2Cz%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^*,y^*,z^*}" class="latex" /> is an optimal solution, and let us call</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cepsilon+%3D+%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x%5E%2A_i+y%5E%2A_j+z%5E%2A_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \epsilon = \sum_{i,j,k} T_{i,j,k} x^*_i y^*_j z^*_k " class="latex" /></p>
<p> the value of the optimum (the algorithm will not need to know or guess <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" />).</p>
<p>The key step is now to see that if we pick a <em>random</em> <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B-1%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \in \{-1,1\}^n}" class="latex" />, there is at least a <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/n^{O(1)}}" class="latex" /> probability that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_i+y%5E%2A_j+z%5E%2A_k+%5Cgeq+%5Cepsilon+%5Ccdot+%5COmega+%5Cleft%28%5Csqrt%7B%5Cfrac%7B%5Clog+n%7D%7Bn%7D%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{i,j,k} T_{i,j,k} x_i y^*_j z^*_k \geq \epsilon \cdot \Omega \left(\sqrt{\frac{\log n}{n}} \right) " class="latex" /></p>
<p> This is a bit difficult, but it is really easy to see that with <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/n^{O(1)}}" class="latex" /> probability we have <a name="main"></a></p>
<p><a name="main"></a></p><a name="main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_i+y%5E%2A_j+z%5E%2A_k+%5Cgeq+%5Cepsilon+%5Ccdot+%5COmega+%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \sum_{i,j,k} T_{i,j,k} x_i y^*_j z^*_k \geq \epsilon \cdot \Omega \left(\sqrt{\frac{1}{n}} \right) \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="main"></a><a name="main"></a> and we can do that by seeing that by defining a vector <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bw_i+%3D+%5Csum_%7Bj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+y%5E%2A_j+z%5E%2A_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i = \sum_{j,k} T_{i,j,k} y^*_j z^*_k}" class="latex" />, so that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x%2C+w+%5Crangle+%3D+%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_i+y%5E%2A_j+z%5E%2A_k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \langle x, w \rangle = \sum_{i,j,k} T_{i,j,k} x_i y^*_j z^*_k" class="latex" /></p>
<p> So we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x%5E%2A%2Cw+%5Crangle+%3D+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \langle x^*,w \rangle = \epsilon " class="latex" /></p>
<p> which, using Cauchy-Schwarz, gives</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+w%7C%7C%5E2+%5Cgeq+%5Cepsilon%5E2+%2Fn+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || w||^2 \geq \epsilon^2 /n " class="latex" /></p>
<p> Now, for a random <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Csim+%5C%7B+-1%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\sim \{ -1,1\}^n}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Clangle+x%2Cw+%5Crangle%5E2+%3D+%7C%7C+w%7C%7C%5E2+%5Cgeq+%5Cepsilon%5E2+%2Fn+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \langle x,w \rangle^2 = || w||^2 \geq \epsilon^2 /n " class="latex" /></p>
<p> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Clangle+x%2Cw+%5Crangle%5E4+%5Cleq+n%5E%7BO%281%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} \langle x,w \rangle^4 \leq n^{O(1)} " class="latex" /></p>
<p> so by Paley–Zygmund we have, let’s say</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+P%7D+%5Cleft%5B+%5Clangle+x%2Cw+%5Crangle%5E2+%5Cgeq+%5Cfrac+%7B%5Cepsilon%5E2%7D%7B2n%7D+%5Cright%5D+%5Cgeq+%5Cfrac+1+%7Bn%5E%7BO%281%29%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb P} \left[ \langle x,w \rangle^2 \geq \frac {\epsilon^2}{2n} \right] \geq \frac 1 {n^{O(1)}} " class="latex" /></p>
<p> which, together with the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w}" class="latex" /> and the fact that the distribution of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+x%2C+w%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\langle x, w\rangle}" class="latex" /> is symmetric around zero, gives us the claim <a href="https://lucatrevisan.wordpress.com/feed/#main">(3)</a>.</p>
<p>Now suppose that we have been lucky and that we have found such an <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. We define the matrix</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_%7Bj%2Ck%7D+%3D+%5Csum_i+T_%7Bi%2Cj%2Ck%7D+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_{j,k} = \sum_i T_{i,j,k} x_i " class="latex" /></p>
<p> and we see that our claim can be written as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y%5E%7B%2AT%7D+X+z%5E%2A+%5Cgeq+%5Cepsilon+%5Ccdot+%5COmega+%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  y^{*T} X z^* \geq \epsilon \cdot \Omega \left(\sqrt{\frac{1}{n}} \right) " class="latex" /></p>
<p> At this point we just apply the algorithm implied by the Grothendieck inequality to the matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />, and we find <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+-1%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ -1,1 \}^n}" class="latex" /> such that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y%5ET+X+z+%5Cgeq+%5Cepsilon+%5Ccdot+%5COmega+%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D+%5Cright%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  y^T X z \geq \epsilon \cdot \Omega \left(\sqrt{\frac{1}{n}} \right)" class="latex" /></p>
<p> meaning that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_i+y_j+z_k+%5Cgeq+%5Cepsilon+%5Ccdot+%5COmega+%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{i,j,k} T_{i,j,k} x_i y_j z_k \geq \epsilon \cdot \Omega \left(\sqrt{\frac{1}{n}}\right) " class="latex" /></p>
<p>Summarizing, our algorithm is to pick a random vector <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Csim+%5C%7B-1%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\sim \{-1,1\}}" class="latex" /> and to find a constant-factor approximation for the problem <a name="xfixed"></a></p>
<p><a name="xfixed"></a></p><a name="xfixed">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cmax_%7By%2Cz+%5Cin+%5C%7B-1%2C1%5C%7D%5En%7D+%5C+%5Csum_%7Bi%2Cj%2Ck%7D+T_%7Bi%2Cj%2Ck%7D+x_iy_jz_k+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \max_{y,z \in \{-1,1\}^n} \ \sum_{i,j,k} T_{i,j,k} x_iy_jz_k \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="xfixed"></a><a name="xfixed"></a> using semidefinite programming. We do that <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{O(1)}}" class="latex" /> times, and take the best solution.</p>
<p>The analysis can be turned into an upper bound certificate in the following way. For the (suboptimal) analysis using Paley-Zygmund, we only need the entries of the random <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> to be 4-wise independent, and there are distributions on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B-1%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{-1,1\}^n}" class="latex" /> where the entries are unbiased and 4-wise independent, and such that the sample space is of size polynomial in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. Thus, one could write an SDP relaxation of <a href="https://lucatrevisan.wordpress.com/feed/#xfixed">(4)</a> for each <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> in the support of such a distribution, and then take the maximum of these SDPs, multiply it by <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt n)}" class="latex" />, and it would be a certified upper bound. Such an upper bound, however, would not come from a relaxation of the 3-XOR problem, and I find it really strange that it is not clear how to turn these ideas into a proof that, say, the standard degree-4 sum-of-squares semidefinite programming relaxation of 3-XOR has an integrality gap at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\tilde O(\sqrt n)}" class="latex" />.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/12/the-khot-naor-approximation-algorithm-for-3-xor/"><span class="datestr">at October 12, 2021 12:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/11/lecturer-in-optimisation-at-queen-mary-university-of-london-apply-by-november-3-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/11/lecturer-in-optimisation-at-queen-mary-university-of-london-apply-by-november-3-2021/">Lecturer in Optimisation at Queen Mary University of London (apply by November 3, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The School of Mathematical Sciences at Queen Mary University of London is seeking to appoint a permanent faculty member in the area of optimisation, with a focus on applying rigorous research methods to address emerging problems in the general field of combinatorial optimisation. We especially welcome applicants who would complement existing expertise in the School’s Combinatorics group.</p>
<p>Website: <a href="https://webapps2.is.qmul.ac.uk/jobs/job.action?jobID=5954">https://webapps2.is.qmul.ac.uk/jobs/job.action?jobID=5954</a><br />
Email: r.johnson@qmul.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/11/lecturer-in-optimisation-at-queen-mary-university-of-london-apply-by-november-3-2021/"><span class="datestr">at October 11, 2021 04:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/11/faculty-at-williams-college-apply-by-november-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/11/faculty-at-williams-college-apply-by-november-15-2021/">Faculty at Williams College (apply by November 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Williams College invites applications for two faculty positions beginning July 1,2022. One is a tenure-track position at the rank of assistant professor with a three-year initial term. The other is an open rank position with a preference for more advanced candidates. That position will have terms commensurate with prior experience.</p>
<p>Website: <a href="https://apply.interfolio.com/91229">https://apply.interfolio.com/91229</a><br />
Email: hiring@cs.williams.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/11/faculty-at-williams-college-apply-by-november-15-2021/"><span class="datestr">at October 11, 2021 02:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4680987113771618016">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/i-have-book-out-on-muffins-you-prob.html">I have a book out on muffins (you prob already know that)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i>Lance</i>: How come you haven't blogged on your muffin book? You've blogged about two books by Harry Lewis (see <a href="https://blog.computationalcomplexity.org/2021/10/how-have-computers-changed-society.html">here</a> and <a href="https://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html">here</a>) one book by the lesswrong community (see <a href="https://blog.computationalcomplexity.org/2021/09/review-of-blog-book-based-on-less-wrong.html">here</a>), and you even did a mashup of a post by two different Scott A's (see <a href="https://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html">here</a>),  but not on your own work.</p><p><i>Bill</i>: I thought I did a post on my muffin book.</p><p><i>Lance</i>: No. You have blogged <i>about </i>the muffin problem, and sometimes you <i>mention</i> either the book or the problem in passing, but you haven't had a post that says</p><p><i><b>HEY, I wrote a book!</b></i></p><p>And this is all the more strange since you asked me to have the book on our blog page. </p><p><i>Bill: </i>(Searches blog with keyword muffin and finds no ref to muffin book). Well pierce my ears and call be drafty! I have not posted on the muffin book! Do you recall my thoughts on when to tell people you are working on a book?</p><p><i>Lance</i>: No</p><p><i>Bill</i>:  I had a college roommate who was an aspiring science fiction writer who told me there are two kinds of people: Those who talk about writing a book, and those who write a book. I have adapted this to:</p><p><i><b>Do not tell people you are writing a book until you are picking out the cover art.</b></i></p><p><i>Lance: </i>I posted about my book when I hadn't even decided on <a href="https://blog.computationalcomplexity.org/2012/06/name-my-book.html">the title</a>. But your cover art is picked out (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>).  And, by the way, its very nice, though it makes me hungry. So I think you can begin talking about the book.</p><p><i>Bill</i>: Indeed! I will!</p><p>------------------------------------------------------------------------------------</p><p><b>Hey I have a book! (See <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a> to buy it on amazon.) </b></p><p><b>Title: Mathematical Muffin Morsels: Nobody Wants a Small Piece</b></p><p><b>by Gasarch, Metz, Prinz, Smolyak</b></p><p><b>(The other authors were undergraduates when we wrote the book. Prinz and Smolyak are now grad students in CS, Metz is in Finance.) </b></p><p><b>Origin: </b></p><p><a href="https://en.wikipedia.org/wiki/Martin_Gardner">Martin Gardner</a> wrote a Mathematics Recreational column for Scientific American for many years, starting in 1956 and ending in the early 1980s. For many STEM people of my generation (Using my <a href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html">fake birthday</a> of Oct 1, 1960, I am 62 years old) Martin Gardner's columns were both an inspiration and an early exposure to mathematics. His columns also made the line between Mathematical Recreation and so-called serious mathematics thin or nonexistent. (See <a href="http://www.cs.umd.edu/~gasarch/bookrev/FRED/gardner.pdf">here</a> for a review of Martin Gardner in the 21st century, a book about the kind of math Gardner wrote of. The book makes a mockery of the distinction between recreational and serious mathematics.) He passed away in 2010 at the age of 95.</p><p>There is a gathering in his honor that is hold roughly every 2 years, called <a href="https://www.gathering4gardner.org/">Gathering For Gardner</a>. (It was cancelled in Spring 2020 and Spring 2021 because of COVID- though its in Atlanta where the CDC is, so they could have had it as an experiment and told the CDC the results). You have to be invited to goto it. I got an invite for 2016 from my contact at World Scientific who published my previous book, <i>Problems with a Point: Exploring Math and Computer Science co-authored with Clyde Kruskal </i> (I had two blogs on it, <a href="https://blog.computationalcomplexity.org/2019/02/problems-with-point-exploring-math-and.html">here</a> and <a href="https://blog.computationalcomplexity.org/2019/04/problems-with-point-not-plug-just-some.html">here</a>, and you can buy it on amazon <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279974">here</a>.) I did three posts on G4G-2016 (<a href="https://blog.computationalcomplexity.org/2016/04/some-short-bits-from-gathering-for.html">here</a>, <a href="https://blog.computationalcomplexity.org/2016/05/some-more-bits-from-gathering-for.html">here</a>, and <a href="https://blog.computationalcomplexity.org/search?q=Gathering">here</a>).</p><p>Aside from seeing some great talks that I understood and liked, I also picked up a pamphlet titled:</p><p><b>The Julia Robinson Math Festival</b></p><p><b>A Sample of Mathematical Puzzles</b></p><p><b>Compiled By Nancy Blackman</b></p><p>One of the problems, credited to Alan Frank, was</p><p>How can you divide and distribute 5 muffins for 3 students so that everyone gets 5/3 and the smallest piece is as big as possible?</p><p>They had some other values for muffins and students as well. </p><p>I solved the (5,3) problem and the other ones as well. That was fun. </p><p>When I got home I began looking at the problem for m muffins and s students. I let f(m,s) be the biggest smallest piece possible for giving out m muffins to s students. I proved a general theorem, called the <i>Floor-Ceiling theorem</i>, that always gives an upper bound, FC(m,s) on f(m,s). I worked out formulas for </p><p>f(m,1) (trivial), </p><p>f(m,2) (trivial), </p><p>f(m,3) (its always FC(m,3),</p><p> f(m,4) (its always FC(m,4)).</p><p>While working on f(m,5) I found that  f(m,5) was always FC(m,5) EXCEPT for m=11. So what's up with f(11,5)?  </p><p>By the Floor Ceiling theorem f(11,5) \le 11/25. We (at that point several ugrads and HS students had joined the project)  were unable to find a protocol that would show f(11,5)\ge 11/25. Personally I thought there WAS such an protocol but perhaps it was more complicated than the ones we had found (We were finding them by hand using some easy linear algebra.) Perhaps a computer program was needed. We did find a protocol for f(11,5)\ge 13/30, which surely was not optimal. </p><p>While on an Amtrak I began working out the following train of thought: The protocol for f(11,5)\le 11/25 MUST have </p><p>(1) every muffin cut into two pieces,</p><p>(2) 3 students get 4 pieces, </p><p>(3) 2 students get 5 pieces. </p><p>While working on getting a protocol for f(11,5)\le 11/25 with these properties I found that... <i>there could be no such protocol</i>! Then by reworking what I did I found that f(11,5)\le 13/30. So it was done! and we had a new technique, which we call <i>The Half Method. </i>To see the full proof see my slides <a href="http://www.cs.umd.edu/~gasarch/MUFFINS/muffintalkGen.pdf">here</a></p><p>The story above is typical: We get f(m,k) for all 1\le k\le SOMETHING, we get stuck, and then we find ANOTHER technique to show upper bounds (which in this case are limits on how well we can do). This happened about 8 times depending on how you count.  After a while we realized that this could not just be an article, this was a book! World Scienfiic agreed to publish it, and its out now.</p><p>Misc Notes</p><p>1) I got a conference paper out of it, in the Fun with Algorithms Conference, with some of the co-authors on the book, and some other people. <a href="https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=8806">here is the conf paper</a>.</p><p>2) Early on we realized that f(m,s) = (m/s)f(s,m) so we only had to look at the m&gt;s case.</p><p>3) The fact that f(m,s) exists and is rational is not obvious, but is true. In fact, f(m,s) can be found by a mixed-int program. </p><p>4) Late on in the process I found that there was a by-invite-only math newsgroup that had discussed the problem, and in fact was where Alan Frank first posted it. I obtained their materials and found that they had already shown f(m,s)=(m/s)f(s,m) and also that the answer is always rational and exists. Aside from that our results did not overlap.</p><p>5) Even later in the process Scott Huddleston emailed me (out of the blue) that he had a program that solved the muffin problem quickly. I was skeptical at first, but he did indeed have a whole new way to look at the problem and his code was very fast (I had Jacob Prinz, one of the co-authors on the book, recode it). Later Richard Chatwin (see <a href="https://arxiv.org/abs/1907.08726">here</a>) seems to have proven that Scott's method always works. The approach of Scott and Richard is where to go if you want to do serious further research on Muffins. My book is where you want to go if you want to learn some easy and fun math (a HS student could read it). </p><p>6) I co-authored a column with Scott H, Erik Metz, Jacob Prinz on Muffins, featuring his technique, in Lane's complexity column, <a href="http://www.cs.umd.edu/~gasarch/papers/sigmuffins.pdf">here</a>.</p><p>7) I had an REU student, Stephanie Warman, write a muffin package based on the book.</p><p>8) I gave a talk an invited talk on The Muffin Problem  at a Joint AMS-MAA meeting. </p><p>9) I gave a talk at Gathering for Gardner 2018 on The Muffin Problem. </p><p>10) I often give talks on it to groups of High School students.</p><p>11) When I teach Discrete Math Honors I talk about it and assign problems on it- it really is part of the course. As such its a good way to reinforce the pigeon hole principle. </p><p>12) I contacted Alan Frank about my work. We arranged to meet at an MIT combinatorics seminar where I was to give a talk on muffins. He brought 11 muffins, with 1 cut (1/2,1/2), 2 cut (14/30,16/30),</p><p>and 8 cut (13/30,17/30) so that the 11 of us could each get 11/5 with smallest piece 13/30. </p><p>13) Coda: </p><p>Why did I keep working on this problem?  I kept working on it because I kept hitting barriers and (with co-authors) breaking them with new techniques that were interesting.  If early on a barrier was not breakable then I would have stopped. If (say) Floor-ceiling solved everything than I might have gotten a paper out of  this, but surely not a book.</p><p>Lesson for all of us: look around you! Its not clear what is going to inspire a project!</p><p>Lasting effect: I am reluctant to throw out old math magazines and pamphlets since you never know when one will lead to a book.</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/i-have-book-out-on-muffins-you-prob.html"><span class="datestr">at October 11, 2021 01:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5868">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5868">Gaussian BosonSampling, higher-order correlations, and spoofing: An update</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In my <a href="https://www.scottaaronson.com/blog/?p=5859">last post</a>, I wrote (among other things) about an ongoing scientific debate between the group of Chaoyang Lu at USTC in China, which over the past year has been doing experiments that seek to demonstrate quantum supremacy via Gaussian BosonSampling; and the group of Sergio Boixo at Google, which had a <a href="https://arxiv.org/abs/2109.11525">recent paper</a> on a polynomial-time classical algorithm to sample approximately from the same distributions.  I reported the facts as I understood them at the time.  Since then, though, a long call with the Google team gave me a new and different understanding, and I feel duty-bound to share that here.</p>



<p>A week ago, I considered it obvious that if, using a classical spoofer, you could beat the USTC experiment on a metric like total variation distance from the ideal distribution, then you would’ve completely destroyed USTC’s claim of quantum supremacy.  The reason I believed <em>that</em>, in turn, is a proposition that I hadn’t given a name but needs one, so let me call it <strong>Hypothesis H</strong>:</p>



<blockquote class="wp-block-quote"><p>The only way a classical algorithm to spoof BosonSampling can possibly do well in total variation distance, is by correctly reproducing the high-order correlations (correlations among the occupation numbers of large numbers of modes) — because that’s where the complexity of BosonSampling lies (if it lies anywhere).</p></blockquote>



<p>Hypothesis H had important downstream consequences.  Google’s algorithm, by the Google team’s own admission, does not reproduce the high-order correlations.  Furthermore, because of limitations on both samples and classical computation time, Google’s paper calculates the total variation distance from the ideal distribution only on the marginal distribution on roughly 14 out of 144 modes.  On that marginal distribution, Google’s algorithm does do better than the experiment in total variation distance.  Google presents a claimed extrapolation to the full 144 modes, but eyeballing the graphs, it was far from clear to me what would happen: like, maybe the spoofing algorithm would continue to win, but maybe the experiment would turn around and win; who knows?</p>



<p>Chaoyang, meanwhile, made a clear prediction that the experiment would turn around and win, because of</p>



<ol><li>the experiment’s success in reproducing the high-order correlations,</li><li>the admitted failure of Google’s algorithm in reproducing the high-order correlations, and</li><li>the seeming impossibility of doing well on BosonSampling <em>without</em> reproducing the high-order correlations (Hypothesis H).</li></ol>



<p>Given everything my experience told me about the central importance of high-order correlations for BosonSampling, I was inclined to agree with Chaoyang.</p>



<p>Now for the kicker: it seems that Hypothesis H is false.  A classical spoofer could beat a BosonSampling experiment on total variation distance from the ideal distribution, without even bothering to reproduce the high-order correlations correctly.</p>



<p>This is true because of a combination of two facts about the existing noisy BosonSampling experiments.  The first fact is that the contribution from the order-k correlations falls off like 1/exp(k).  The second fact is that, due to calibration errors and the like, the experiments already show significant deviations from the ideal distribution on the order-1 and order-2 correlations.</p>



<p>Put these facts together and what do you find?  Well, suppose your classical spoofing algorithm takes care to get the low-order contributions to the distribution exactly right.  Just for that reason alone, it could already win over a noisy BosonSampling experiment, as judged by benchmarks like total variation distance from the ideal distribution, or for that matter linear cross-entropy.  Yes, the experiment will beat the classical simulation on the higher-order correlations.  But because those higher-order correlations are exponentially attenuated anyway, they won’t be enough to make up the difference.  The experiment’s lack of perfection on the low-order correlations will swamp everything else.</p>



<p>Granted, I still don’t know for sure that this <em>is</em> what happens — that depends on whether I believe Sergio or Chaoyang about the extrapolation of the variation distance to the full 144 modes (my own eyeballs having failed to render a verdict!).  But I now see that it’s logically possible, maybe even plausible.</p>



<p>So, let’s imagine for the sake of argument that Google’s simulation wins on variation distance, even though the experiment wins on the high-order correlations.  In that case, what would be our verdict: would USTC have achieved quantum supremacy via BosonSampling, or not?</p>



<p>It’s clear what each side could say.</p>



<p>Google could say: by a metric that Scott Aaronson, the coinventor of BosonSampling, thought was perfectly adequate as late as last week — namely, total variation distance from the ideal distribution — we won.  We achieved lower variation distance than USTC’s experiment, and we did it using a fast classical algorithm.  End of discussion.  No moving the goalposts after the fact.</p>



<p>Google could even add: BosonSampling is a <em>sampling</em> task; it’s right there in the name!  The only purpose of any benchmark — whether Linear XEB or high-order correlation — is to give evidence about whether you are or aren’t sampling from a distribution close to the ideal one.  But that means that, if you accept that we <em>are</em> doing the latter better than the experiment, then there’s nothing more to argue about.</p>



<p>USTC could respond: even if Scott Aaronson <em>is</em> the coinventor of BosonSampling, he’s extremely far from an infallible oracle.  In the case at hand, his lack of appreciation for the sources of error in realistic experiments caused him to fixate inappropriately on variation distance as the success criterion.  If you want to see the quantum advantage in our system, you have to deliberately subtract off the low-order correlations and look at the high-order correlations.</p>



<p>USTC could add: from the very beginning, the whole point of quantum supremacy experiments was to demonstrate a clear speedup on <em>some</em> benchmark — we never particularly cared which one!  That horse is out of the barn as soon as we’re talking about quantum supremacy at all — something the Google group, which itself reported the first quantum supremacy experiment in Fall 2019, again for a completely artificial benchmark — knows as well as anyone else.  (The Google team even has experience with adjusting benchmarks: when, for example, <a href="https://arxiv.org/abs/2103.03074">Pan and Zhang</a> pointed out that Linear XEB as originally specified is pretty easy to spoof for random 2D circuits, the most cogent rejoinder was: OK, fine then, add an extra check that the returned samples are sufficiently different from one another, which kills Pan and Zhang’s spoofing strategy.)  In that case, then, why isn’t a benchmark tailored to the high-order correlations as good as variation distance or linear cross-entropy or any other benchmark?</p>



<p>Both positions are reasonable and have merit — though I confess to somewhat greater sympathy for the one that appeals to my doofosity rather than my supposed infallibility!</p>



<p>OK, but suppose, again for the sake of argument, that we accepted the second position, and we said that USTC gets to declare quantum supremacy as long as its experiment does better than any known classical simulation at reproducing the high-order correlations.  We’d still face the question: does the USTC experiment, in fact, do better on that metric?  It would be awkward if, having won the right to change the rules in its favor, USTC still lost even under the new rules.</p>



<p>Sergio tells me that USTC directly reported experimental data only for up to order-7 correlations, and at least individually, the order-7 correlations are easy to reproduce on a laptop (although <em>sampling</em> in a way that reproduces the order-7 correlations might still be hard—a point that Chaoyang confirms, and where further research would be great).  OK, but USTC also reported that their experiment seems to reproduce up to order-19 correlations.  And order-19 correlations, the Google team agrees, are hard to sample consistently with on a classical computer by any currently known algorithm.</p>



<p>So then, why don’t we have direct data for the order-19 correlations?  The trouble is simply that it would’ve taken USTC an astronomical amount of computation time.  So instead, they relied on a statistical extrapolation from the observed strength of the lower-order correlations — there we go again with the extrapolations!  Of course, if we’re going to let Google rest its case on an extrapolation, then maybe it’s only sporting to let USTC do the same.</p>



<p>You might wonder: why didn’t we have to worry about any of this stuff with the <em>other</em> path to quantum supremacy, the one via random circuit sampling with superconducting qubits?  The reason is that, with random circuit sampling, all the correlations except the highest-order ones are completely trivial — or, to say it another way, the reduced state of any small number of output qubits is exponentially close to the maximally mixed state.  This is a real difference between BosonSampling and random circuit sampling—and even 5-6 years ago, we knew that this represented an advantage for random circuit sampling, although I now have a deeper appreciation for just how great of an advantage it is.  For it means that, with random circuit sampling, it’s easier to place a “sword in the stone”: to say, for example, <em>here</em> is the Linear XEB score achieved by the trivial classical algorithm that outputs random bits, and lo, our experiment achieves a higher score, and lo, we challenge anyone to invent a fast classical spoofing method that achieves a similarly high score.</p>



<p>With BosonSampling, by contrast, we have various metrics with which to judge performance, but so far, for none of those metrics do we have a plausible hypothesis that says “<em>here’s</em> the best that any polynomial-time classical algorithm can possibly hope to do, and it’s completely plausible that even a noisy current or planned BosonSampling experiment can do better than that.”</p>



<p>In the end, then, I come back to the exact same three goals I would’ve recommended a week ago for the future of quantum supremacy experiments, but with all of them now even more acutely important than before:</p>



<ol><li>Experimentally, to increase the fidelity of the devices (with BosonSampling, for example, to observe a larger contribution from the high-order correlations) — a much more urgent goal, from the standpoint of evading classical spoofing algorithms, than further increasing the dimensionality of the Hilbert space.</li><li>Theoretically, to design better ways to verify the results of sampling-based quantum supremacy experiments classically — ideally, even ways that could be applied via polynomial-time tests.</li><li>For Gaussian BosonSampling in particular, to get a better understanding of the plausible limits of classical spoofing algorithms, and exactly how good a noisy device needs to be before it exceeds those limits.</li></ol>



<p>Thanks so much to Sergio Boixo and Ben Villalonga for the conversation, and to Chaoyang Lu and Jelmer Renema for comments on this post.  Needless to say, any remaining errors are my own.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5868"><span class="datestr">at October 10, 2021 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1343">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/">Soliciting information about Women in TCS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CATCS is compiling a list of <em>Women in Theoretical Computer Science (WinTCS)</em>, hoping to facilitate engagement and opportunities for Women in TCS in the future. </p>



<p>We cordially invite women (broadly defined as anyone who self-identifies as a woman) as well as gender non-conforming TCS researchers to participate the following simple <a href="https://docs.google.com/forms/d/e/1FAIpQLSc2LcI0mtUvyKgl34OqbxDVpu0zbYs0fiLmU_5jr2qHybCfMQ/viewform?usp=sf_link" target="_blank" rel="noreferrer noopener">google-form survey</a> to provide your information. </p>



<p>Please submit your information before <strong>Oct. 31, 2021</strong>. After that, information can still be provided through the CATCS website, and will be added to the list monthly. <br />**Please feel free to share this solicitation broadly within your networks.**</p>



<p>If you have any questions regarding this survey, please feel free to contact us at <a target="_blank" rel="noreferrer noopener">goldner@bu.edu</a> or <a target="_blank" rel="noreferrer noopener">yusuwang@ucsd.edu</a>.  </p>



<p>– Kira Goldner and Yusu Wang (on behalf of the SIGACT CATCS)</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2021/10/08/soliciting-information-about-women-in-tcs/"><span class="datestr">at October 08, 2021 09:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5384653288710765756">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/c-is-for-cookie-and-thats-good-enough.html">C++ is for Cookie and That's Good Enough for Me</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Potbelly, a local sandwich chain, made me an offer I couldn't refuse: change my password and earn a free (and quite tasty) oatmeal chocolate chip cookie. A free cookie is a great motivator, and checking that this wasn't some clever phishing attack, changed my password and got my cookie. Not sure why Potbelly wanted me to change my password but happy to take their cookie.</p><p>Potbelly likely didn't make this offer to everyone so what if you want a cookie?</p><p></p><ol style="text-align: left;"><li>Use an app to get a cookie delivered.</li><li>Visit a specialty cookie store.</li><li>Go to your local supermarket and pick up a package of <a href="https://www.marianos.com/p/chips-ahoy-original-chocolate-chip-cookies-family-size/0004400003338">Chip's Ahoy</a>.</li><li>Buy some pre-made <a href="https://www.marianos.com/p/pillsbury-ready-to-bake-chocolate-chip-cookie-dough/0001800081778">cookie dough</a> and put it in the oven.</li><li>Buy some <a href="https://www.marianos.com/p/betty-crocker-chocolate-chip-cookie-mix/0001600030650">cookie mix</a>, add ingredients and bake.</li><li>Find a <a href="https://www.bettycrocker.com/recipes/ultimate-chocolate-chip-cookies/77c14e03-d8b0-4844-846d-f19304f61c57">cookie recipe</a>, buy the ingredients and get cooking</li><li>Get fresh ingredients direct from a farm stand</li><li>Grow and gather your own ingredients, ala <a href="https://shop.scholastic.com/teachers-ecommerce/teacher/books/pancakes-pancakes-9780545653619.html">Pancakes Pancakes</a></li></ol><div>In machine learning we seem to be heading into a similar set of choices</div><div><ol style="text-align: left;"><li>Not even realize you are using machine learning, such as recommendations on Netflix or Facebook.</li><li>Using ML implicitly, like talking to Alexa</li><li>Using pre-trained ML through an app, like Google Translate</li><li>Using pre-trained ML through an API</li><li>Using a model like GPT-3 with an appropriate prompt</li><li>Use an easily trained model like <a href="https://aws.amazon.com/fraud-detector/">Amazon Fraud Detector</a></li><li>An integrated machine learning environment like <a href="https://aws.amazon.com/sagemaker/">Sagemaker</a></li><li>Use pre-built ML tools like TensorFlow or PyTorch</li><li>Code up your own ML algorithms in C++</li><li>Build your own hardware and software</li></ol><div>and probably missing a few options.</div><div><br /></div><div>When you want cookies or learning, do you buy it prepackaged or do you roll your own? And when people offer it to you for free, how wary should you be?</div></div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/c-is-for-cookie-and-thats-good-enough.html"><span class="datestr">at October 08, 2021 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/08/arv-on-abelian-cayley-graphs/">ARV on Abelian Cayley Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Continuing from <a href="https://lucatrevisan.wordpress.com/2021/10/07/buser-inequalities-in-graphs/">the previous post</a>, we are going to prove the following result: let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> be a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Cayley graph of an Abelian group, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> be the normalized edge expansion of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BARV%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{ARV(G)}" class="latex" /> be the value of the ARV semidefinite programming relaxation of sparsest cut on <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> (we will define it below), and <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G)}" class="latex" /> be the second smallest normalized Laplacian eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Then we have <a name="main"></a></p>
<p><a name="main"></a></p><a name="main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_2+%28G%29+%5Cleq+O%28d%29+%5Ccdot+%28ARV+%28G%29%29%5E2+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_2 (G) \leq O(d) \cdot (ARV (G))^2 \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="main"></a><a name="main"></a> which, together with the fact that <img src="https://s0.wp.com/latex.php?latex=%7BARV%28G%29+%5Cleq+2+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{ARV(G) \leq 2 \phi(G)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29+%5Cleq+%5Csqrt%7B2+%5Clambda_2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G) \leq \sqrt{2 \lambda_2}}" class="latex" />, implies the Buser inequality <a name="buser"></a></p>
<p><a name="buser"></a></p><a name="buser">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_2+%28G%29+%5Cleq+O%28d%29+%5Ccdot+%5Cphi%5E2+%28G%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_2 (G) \leq O(d) \cdot \phi^2 (G) \ \ \ \ \ (2)" class="latex" /></p>
</a><p><a name="buser"></a><a name="buser"></a> and the approximation bound <a name="arvapx"></a></p>
<p><a name="arvapx"></a></p><a name="arvapx">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cphi%28G%29+%5Cleq+O%28%5Csqrt+d%29+%5Ccdot+ARV%28G%29+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \phi(G) \leq O(\sqrt d) \cdot ARV(G) \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="arvapx"></a><a name="arvapx"></a> The proof of <a href="https://lucatrevisan.wordpress.com/feed/#main">(1)</a>, due to Shayan Oveis Gharan and myself, is very similar to the proof by Bauer et al. of <a href="https://lucatrevisan.wordpress.com/feed/#buser">(2)</a>.</p>
<p><span id="more-4564"></span></p>
<p><b>1. Ideas </b></p>
<p>For a positive integer parameter <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, call <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> the multigraph whose adjacency matrix is <img src="https://s0.wp.com/latex.php?latex=%7BA%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> is the adjacency matrix of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. So <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d^t}" class="latex" />-regular graph, and each edge in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> corresponds to a length-<img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> walk in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Our proof boils down to showing <a name="rw"></a></p>
<p><a name="rw"></a></p><a name="rw">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++ARV%28G%5Et%29+%5Cleq+O%28%5Csqrt%7Bdt%7D%29+%5Ccdot+ARV%28G%29+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   ARV(G^t) \leq O(\sqrt{dt}) \cdot ARV(G) \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="rw"></a><a name="rw"></a> which gives <a href="https://lucatrevisan.wordpress.com/feed/#main">(1)</a> after we note that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%5Et%29+%3D+1+-+%281+-+%5Clambda_2%28G%29%29%5Et+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G^t) = 1 - (1 - \lambda_2(G))^t " class="latex" /></p>
<p> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ARV%28G%5Et%29+%5Cgeq+%5Clambda_2+%28G%5Et%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  ARV(G^t) \geq \lambda_2 (G^t) " class="latex" /></p>
<p> and we combine the above inequalities with <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3A%3D+1%2F%5Clambda_2+%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t := 1/\lambda_2 (G)}" class="latex" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5COmega%281%29+%5Cleq+1+-+%281+-+%5Clambda_2%28G%29%29%5Et+%3D+%5Clambda_2+%28G%5Et%29+%5Cleq+ARV%28G%5Et%29+%5Cleq+O%28%5Csqrt%7Bdt%7D%29+%5Ccdot+ARV%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Omega(1) \leq 1 - (1 - \lambda_2(G))^t = \lambda_2 (G^t) \leq ARV(G^t) \leq O(\sqrt{dt}) \cdot ARV(G) " class="latex" /></p>
<p> The reader will see that our argument could also prove (roughly as done by Bauer et al.) that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%5Et%29+%5Cleq+O%28%5Csqrt+d%29+%5Ccdot+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G^t) \leq O(\sqrt d) \cdot \phi(G)}" class="latex" />, simply by reasoning about distributions of cuts instead of reasoning about ARV solutions, which would give <a href="https://lucatrevisan.wordpress.com/feed/#buser">(2)</a> more directly. By reasoning about ARV solutions, however, we are also able to establish <a href="https://lucatrevisan.wordpress.com/feed/#arvapx">(3)</a>, which we think is independently interesting.</p>
<p>It remains to prove <a href="https://lucatrevisan.wordpress.com/feed/#rw">(4)</a>. I will provide a completely self-contained proof, including the definition of Cayley graphs and of the ARV relaxation, but, first, here is a summary for the reader already familiar with this material: we take an optimal solution of ARV for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, and bound how well it does for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" />. We need to understand how much bigger is the fraction of edges cut by the solution in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> compared to the fraction of edges cut in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />; a random edge in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is obtained by randomly sampling <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> generators and adding them together, which is roughly like sampling <img src="https://s0.wp.com/latex.php?latex=%7Bt%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t/d}" class="latex" /> times each generator, each time with a random sign. Because of cancellations, we expect that the sum of these <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> random generators can be obtained by summing roughly <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bt%2Fd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{t/d}}" class="latex" /> copies of each of the <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> generators, or <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Btd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{td}}" class="latex" /> generators in total. So a random edge of <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> corresponds roughly to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Btd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{td}}" class="latex" /> edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, and this is by how much, at most, the fraction of cut edges can grow.</p>
<p><b>2. Definitions </b></p>
<p>Now we present more details and definition. Recall that if <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> is a group, for which we use additive notation, and <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> is a set or multiset of group elements, then the Cayley graph <img src="https://s0.wp.com/latex.php?latex=%7BCay%28%5CGamma%2CS%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Cay(\Gamma,S)}" class="latex" /> is the graph that has a vertex for every group element and an edge <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cx%2Bs%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x,x+s)}" class="latex" /> for every group element <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in \Gamma}" class="latex" /> and every element <img src="https://s0.wp.com/latex.php?latex=%7Bs%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s\in S}" class="latex" />. We restrict ourselves to undirected graphs, so we will always assume that if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> is an element of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7B-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-s}" class="latex" /> is also an element of <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> (with the same multiplicity, if <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> if a multiset). Note that the resulting undirected graph is <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S|}" class="latex" />-regular.</p>
<p>Several families of graphs can be seen to be Cayley graphs, including cycles, cliques, balanced complete bipartite graphs, hypercubes, toruses, and so on. All the above examples are actually Cayley graphs of <em>Abelian</em> groups. Several interesting families of graphs, for example several families of expanders, are Cayley graphs of non-Abelian groups, but the result of this post will apply only to Abelian groups.</p>
<p>To define the ARV relaxation, let us take it slowly and start from the definition of the sparsest cut problem. The edge expansion problem <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> is closely related to the <em>sparsest cut</em> problem, which can be defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csigma%28G%29+%3A%3D+%5Cmin_%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5EV%7D+%5C+%5C+%5Cfrac%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sigma(G) := \min_{x \in \{0,1\}^V} \ \ \frac{x^T L_G x}{x^T L_K x } " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7BL_G+%3D+I+-+A%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_G = I - A/d}" class="latex" /> is the normalized Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BL_K+%3D+I+-+J%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_K = I - J/n}" class="latex" /> is the normalized Laplacian of the clique on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> vertices. We wrote it this way to emphasize the similarity with the computation of the second smallest normalized Laplacian eigenvalue, which can be written as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%3D+%5Cmin_%7Bx+%5Cin+R%5EV+%3A+%5C+x+%5Cperp+%7B%5Cbf+1%7D%7D+%5C+%5C+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ETx+%7D+%3D+%5Cmin_%7Bx%5Cin+R%5EV%7D+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) = \min_{x \in R^V : \ x \perp {\bf 1}} \ \ \frac {x^T L_G x}{x^Tx } = \min_{x\in R^V} \frac {x^T L_G x}{x^T L_K x} " class="latex" /></p>
<p> where the second equality is perhaps not obvious but is easily proved (all the solutions <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cperp+%7B%5Cbf+1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\perp {\bf 1}}" class="latex" /> have the same cost function on both sides, and the last expression is shift invariant, so there is no loss in optimizing over all <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5EV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb R}^V}" class="latex" /> or just in the space <img src="https://s0.wp.com/latex.php?latex=%7B%5Cperp+%7B%5Cbf+1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\perp {\bf 1}}" class="latex" />). We see that the computation of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is just a relaxation of the sparsest cut problem, and so we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2%28G%29+%5Cleq+%5Csigma%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2(G) \leq \sigma(G) " class="latex" /></p>
<p> We can write the sparsest cut problem in a less algebraic version as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csigma%28G%29+%3D+%5Cmin_%7BS%5Csubseteq+V%7D+%5C+%5C+%5Cfrac%7B+cut%28S%29%7D%7B%5Cfrac+dn+%5Ccdot+%7CV%7C+%5Ccdot+%7CV-S%7C+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sigma(G) = \min_{S\subseteq V} \ \ \frac{ cut(S)}{\frac dn \cdot |V| \cdot |V-S| } " class="latex" /></p>
<p> and recall that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%3D+%5Cmin_%7BS%5Csubseteq+V%3A+%7CS%7C+%5Cleq+%5Cfrac+%7B%7CV%7C%7D+2%7D+%5C+%5C+%5Cfrac%7B+cut%28S%29%7D%7Bd+%7CS%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) = \min_{S\subseteq V: |S| \leq \frac {|V|} 2} \ \ \frac{ cut(S)}{d |S|} " class="latex" /></p>
<p> The cost function for <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> does not change if we switch <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BV-S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V-S}" class="latex" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> could be defined equivalently as an optimization problem over subsets <img src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+V%3A+%7CS%7C+%5Cleq+%7CV%7C%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S\subseteq V: |S| \leq |V|/2}" class="latex" />, and at this point <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\phi(G)}" class="latex" /> are the same problem except for an extra factor of <img src="https://s0.wp.com/latex.php?latex=%7B%7CV-S%7C%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|V-S|/n}" class="latex" /> in the denominator of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma(G)}" class="latex" />. Such a factor is always between <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, so we have:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi%28G%29+%5Cleq+%5Csigma%28G%29%5Cleq+2+%5Cphi%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \phi(G) \leq \sigma(G)\leq 2 \phi(G) " class="latex" /></p>
<p> (Note that all these definitions have given us a particularly convoluted proof that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29+%5Cleq+2+%5Cphi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G) \leq 2 \phi(G)}" class="latex" />.)</p>
<p>Yet another way to characterize <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2}" class="latex" /> is as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%3D+%5Cmin_%7Bx%5Cin+R%5EV%7D+%5Cfrac+%7Bx%5ET+L_G+x%7D%7Bx%5ET+L_K+x%7D+%3D+%5Cmin_%7BX+%5Csucceq+%7B%5Cbf+0%7D%7D+%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) = \min_{x\in R^V} \frac {x^T L_G x}{x^T L_K x} = \min_{X \succeq {\bf 0}} \frac{L_G \bullet X}{L_K\bullet X} " class="latex" /></p>
<p> Where <img src="https://s0.wp.com/latex.php?latex=%7BA%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A\bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" /> is the Frobenius inner product between matrices. This is also something that is not obvious but that it is not difficult to prove, the main point being that if we write a PSD matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_ix_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_i x_ix_i^T}" class="latex" />, then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+%3D+%5Cfrac%7B%5Csum_i+x_i%5ET+L_G+x_i%7D%7B%5Csum_i+x_i%5ET+L_Kx_i%7D+%5Cgeq+%5Cmin_i+%5Cfrac%7B+x_i%5ET+L_G+x_i%7D%7Bx_i%5ET+L_Kx_i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{L_G \bullet X}{L_K\bullet X} = \frac{\sum_i x_i^T L_G x_i}{\sum_i x_i^T L_Kx_i} \geq \min_i \frac{ x_i^T L_G x_i}{x_i^T L_Kx_i} " class="latex" /></p>
<p> and so there is no loss in passing from an optimization over all PSD matrices versus all rank-1 PSD matrices.</p>
<p>We can rewrite <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2+%5Cmin_%7BX+%5Csucceq+%7B%5Cbf+0%7D%7D+%5Cfrac%7BL_G+%5Cbullet+X%7D%7BL_K%5Cbullet+X%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2 \min_{X \succeq {\bf 0}} \frac{L_G \bullet X}{L_K\bullet X} }" class="latex" /> in terms of the Cholesky decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bllr%7D+%5Cmin+%26+%5Cfrac+%7B%5Csum_%7B%28u%2Cv%29%5Cin+E%7D+%7C%7C+x_u+-+x_v%7C%7C%5E2+%7D%7B%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D+%5C%5C+%5Cmbox%7Bs.t%7D%5C%5C+%26+x_v+%5Cin+%7B%5Cmathbb+R%7D%5Em+%26+%5C+%5C+%5Cforall+v%5Cin+V%5C%5C+%26+m%5Cgeq+1+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{llr} \min &amp; \frac {\sum_{(u,v)\in E} || x_u - x_v||^2 }{\frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 } \\ \mbox{s.t}\\ &amp; x_v \in {\mathbb R}^m &amp; \ \ \forall v\in V\\ &amp; m\geq 1 \end{array} " class="latex" /></p>
<p> where the correspondence between PSD matrices <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and vectors <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> is that we have <img src="https://s0.wp.com/latex.php?latex=%7BX_%7Bu%2Cv%7D+%3D+%5Clangle+x_u%2Cx_v+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_{u,v} = \langle x_u,x_v \rangle}" class="latex" /> (that is, <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> is the Cholesky decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is the Gram matrix of the <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" />). An integral solution of the sparsest cut problem corresponds to choosing rank <img src="https://s0.wp.com/latex.php?latex=%7Bm%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m=1}" class="latex" /> and solutions in which each 1-dimensional <img src="https://s0.wp.com/latex.php?latex=%7Bx_v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_v}" class="latex" /> is either 1 or 0, corresponding on whether <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> is in the set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> or not. The ARV relaxation is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bllr%7D+%5Cmin+%26+%5Cfrac+%7B%5Csum_%7B%28u%2Cv%29%5Cin+E%7D+%7C%7C+x_u+-+x_v%7C%7C%5E2+%7D%7B%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D+%5C%5C+%5Cmbox%7Bs.t%7D%5C%5C+%26+%7C%7Cx_u+-+x_v%7C%7C%5E2+%5Cleq+%7C%7Cx_u+-+x_z%7C%7C%5E2+%2B+%7C%7Cx_z+-+x_v%7C%7C%5E2+%26%5C+%5C+%5Cforall+u%2Cv%2Cz+%5Cin+V%5C%5C+%26+x_v+%5Cin+%7B%5Cmathbb+R%7D%5Em+%26+%5Cforall+v%5Cin+V%5C%5C+%26+m%5Cgeq+1+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{llr} \min &amp; \frac {\sum_{(u,v)\in E} || x_u - x_v||^2 }{\frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 } \\ \mbox{s.t}\\ &amp; ||x_u - x_v||^2 \leq ||x_u - x_z||^2 + ||x_z - x_v||^2 &amp;\ \ \forall u,v,z \in V\\ &amp; x_v \in {\mathbb R}^m &amp; \forall v\in V\\ &amp; m\geq 1 \end{array} " class="latex" /></p>
<p> which is a relaxation of sparsest cut because the “triangle inequality” constraints that we introduced are satisfied by 1-dimensional 0/1 solutions. Thus we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_2+%28G%29+%5Cleq+ARV%28G%29+%5Cleq+%5Cphi%28G%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_2 (G) \leq ARV(G) \leq \phi(G) " class="latex" /></p>
<p><b>3. The Argument </b></p>
<p>Let us take any solution for ARV, and let us symmetrize it so that the symmetrized solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_u+-+x_%7Bu+%2B+s%7D+%7C%7C%5E2+%3D+%7C%7C+x_v+-+x_%7Bv%2Bs%7D+%7C%7C%5E2+%5C+%5C+%5Cforall+s%5Cin+S+%5C+%5Cforall+u%2Cv%5Cin+V+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || x_u - x_{u + s} ||^2 = || x_v - x_{v+s} ||^2 \ \ \forall s\in S \ \forall u,v\in V " class="latex" /></p>
<p> That is, make sure that the contribution of each edge to the numerator of the cost function depends only on the generator that defines the edge, and not on the pair of endpoints.</p>
<p>It is easier to see that this symmetrization is possible if we view our solution as a PSD matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />. In this case, for every group element <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" />, we can let <img src="https://s0.wp.com/latex.php?latex=%7BX_g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_g}" class="latex" /> be the solution (with the same cost) obtained by permuting rows and columns according to the mapping <img src="https://s0.wp.com/latex.php?latex=%7Bv+%5Crightarrow+g%2B+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v \rightarrow g+ v}" class="latex" />; then we can consider the solution <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1n+%5Csum_%7Bg%5Cin+V%7D+X_g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1n \sum_{g\in V} X_g}" class="latex" />, which satisfies the required symmetry condition.</p>
<p>Because of this condition, the cost function applied to <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+%7B+%5Cfrac+n2+%5Ccdot+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2%7D%7B+%5Cfrac+dn+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac { \frac n2 \cdot \sum_{s\in S} || x_s - x_0 ||^2}{ \frac dn \sum_{u &lt; v} ||x_u - x_v ||^2 }" class="latex" /></p>
<p> and the cost function applied to <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x_v+%5C%7D_%7Bv%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ x_v \}_{v\in V}}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+%7B+%5Cfrac+n2+%5Ccdot+%5Csum_%7Bs_1%2C%5Cldots%2Cs_t%5Cin+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_t%7D+-+x_0+%7C%7C%5E2%7D%7B+%5Cfrac+%7Bd%5Et%7D+n+%5Csum_%7Bu+%3C+v%7D+%7C%7Cx_u+-+x_v+%7C%7C%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac { \frac n2 \cdot \sum_{s_1,\ldots,s_t\in S^t} || x_{s_1 + \cdots + s_t} - x_0 ||^2}{ \frac {d^t} n \sum_{u &lt; v} ||x_u - x_v ||^2 }" class="latex" /></p>
<p> meaning that our goal is now simply to prove</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1%7Bd%5Et%7D+%5Csum_%7B%28s_1%2C%5Cldots%2Cs_t%29%5Cin+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+%5Cleq+O%28%5Csqrt+%7Bdt%7D%29+%5Ccdot+%5Cfrac+1d+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac 1{d^t} \sum_{(s_1,\ldots,s_t)\in S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 \leq O(\sqrt {dt}) \cdot \frac 1d \sum_{s\in S} || x_s - x_0 ||^2 " class="latex" /></p>
<p> or, if we take a probabilistic view</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_%7B%28s_1%2C%5Cldots%2Cs_t%29+%5Csim+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+%5Cleq+O%28%5Csqrt+%7Bdt%7D%29+%5Ccdot+%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+%5C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_{(s_1,\ldots,s_t) \sim S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 \leq O(\sqrt {dt}) \cdot \mathop{\mathbb E}_{s\sim S} || x_s - x_0 ||^2 \ " class="latex" /></p>
<p> If we let <img src="https://s0.wp.com/latex.php?latex=%7Bc_s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s}" class="latex" /> be the number of times that generator <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> appears in the sum <img src="https://s0.wp.com/latex.php?latex=%7Bs_1+%2B+%5Cldots+%2B+s_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_1 + \ldots + s_t}" class="latex" />, counting cancellations (so that, if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" /> appears 4 times and <img src="https://s0.wp.com/latex.php?latex=%7B-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-s}" class="latex" /> appears 6 times we let <img src="https://s0.wp.com/latex.php?latex=%7Bc_s+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s = 0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7B-s%7D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_{-s} = 2}" class="latex" />) we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++s_1+%2B+%5Cldots+%2B+s_+t+%3D+%5Csum_%7Bs%5Cin+S%7D+c_s+%5Ccdot+s+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  s_1 + \ldots + s_ t = \sum_{s\in S} c_s \cdot s " class="latex" /></p>
<p> where multiplying an integer by a generator means adding the generator to itself that many times. Using the triangle inequality and the symmetrization we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_%7Bs_1+%2B+%5Cldots+%2B+s_t%7D+-+x_0+%7C%7C%5E2+%5Cleq+%5Csum_%7Bs%5Cin+S%7D+c_s+%5Ccdot+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  || x_{s_1 + \ldots + s_t} - x_0 ||^2 \leq \sum_{s\in S} c_s \cdot || x_s - x_0 ||^2 " class="latex" /></p>
<p> The next observation is that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+c_s+%5Cleq+O%28%5Csqrt+%7Bt%2Fd%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} c_s \leq O(\sqrt {t/d}) " class="latex" /></p>
<p> where the expectation is over the choice of <img src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_1,\ldots,s_t}" class="latex" />. This is because <img src="https://s0.wp.com/latex.php?latex=%7Bc_s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c_s}" class="latex" /> can be seen as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmin+%5C%7B+0+%2C+X_1+%2B+%5Cldots+X_t+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\min \{ 0 , X_1 + \ldots X_t \}}" class="latex" />, where we define <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = +1}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%7Bs_i+%3D+s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_i = s}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = -1}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%7Bs_i+%3D+-s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_i = -s}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX_i+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_i = 0}" class="latex" /> otherwise. We have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+c_s+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%5Cmin+%5C%7B+0+%2C+X_1+%2B+%5Cldots+X_t+%5C%7D+%5Cleq+%5Cmathop%7B%5Cmathbb+E%7D+%7C+X_1+%2B+%5Cldots+%2B+X_t+%7C+%5Cleq+%5Csqrt+%7B%5Cmathop%7B%5Cmathbb+E%7D+%28X_1+%2B+%5Cldots+%2B+X_t%29%5E2%7D+%3D+%5Csqrt%7B2t%2Fd%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathop{\mathbb E} c_s = \mathop{\mathbb E} \min \{ 0 , X_1 + \ldots X_t \} \leq \mathop{\mathbb E} | X_1 + \ldots + X_t | \leq \sqrt {\mathop{\mathbb E} (X_1 + \ldots + X_t)^2} = \sqrt{2t/d} " class="latex" /></p>
<p> Combining everything, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_%7B%28s_1%2C%5Cldots%2Cs_t%29+%5Csim+S%5Et%7D+%7C%7C+x_%7Bs_1+%2B+%5Ccdots+%2B+s_k%7D+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E_{(s_1,\ldots,s_t) \sim S^t} || x_{s_1 + \cdots + s_k} - x_0 ||^2 " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bs%5Cin+S%7D+%5Cmathop%7B%5Cmathbb+E%7D+c_s+%7C%7C+x_s+-+x_0+%7C%7C%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \leq \sum_{s\in S} \mathop{\mathbb E} c_s || x_s - x_0 ||^2" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%5Cleft%28+%5Csqrt+%7B%5Cfrac+td%7D+%5Cright%29+%5Ccdot+%5Csum_%7Bs%5Cin+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \leq O\left( \sqrt {\frac td} \right) \cdot \sum_{s\in S} || x_s - x_0 ||^2" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+O%28%5Csqrt%7Bdt%7D%29+%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+S%7D+%7C%7C+x_s+-+x_0+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = O(\sqrt{dt}) \mathop{\mathbb E}_{s\sim S} || x_s - x_0 ||^2 " class="latex" /></p>
<p>So every solution of ARV for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is also a solution for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> with a cost that increases at most by a <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7Bdt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt {dt})}" class="latex" /> factor, which proves <a href="https://lucatrevisan.wordpress.com/feed/#rw">(4)</a> as promised.</p>
<p><b>4. A Couple of Interesting Questions </b></p>
<p>We showed that ARV has integrality gap at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular Abelian Cayley graph, but we did not demonstrate a rounding algorithm able to achieve an <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt d)}" class="latex" /> approximation ratio.</p>
<p>If we follow the argument, starting from an ARV solution of cost <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" />, choosing <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/d\epsilon^2}" class="latex" /> we see that <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" /> has an ARV solution (the same as before) of cost at most, say, <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" />, and so <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%5Et%29+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G^t) \leq 1/2}" class="latex" />, implying that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_2%28G%29+%5Cleq+O%281%2Ft%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_2(G) \leq O(1/t)}" class="latex" /> and so Fiedler’s algorithm according to a Laplacian eigenvalue finds a cut of sparsity at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7B1%2Ft%7D%29+%3D+O%28%5Csqrt+d+%5Ccdot+%5Cepsilon+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt{1/t}) = O(\sqrt d \cdot \epsilon )}" class="latex" />.</p>
<p>We can also see that if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is the matrix corresponding to an ARV solution of value <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, then one of the eigenvectors <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> must be a test vector of Rayleigh quotient at most <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2}" class="latex" /> for the Laplacian <img src="https://s0.wp.com/latex.php?latex=%7BG%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> is order of <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/d\epsilon^2}" class="latex" />. However it is not clear how to get, from such a vector, a test vector for the Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> of Rayleigh quotient at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7B1%2Ft%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt {1/t} )}" class="latex" /> for the Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, though one such vector should be <img src="https://s0.wp.com/latex.php?latex=%7BA%5Ek+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^k x}" class="latex" /> for a properly chosen <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> in the range <img src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2C+t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1,\ldots, t}" class="latex" />.</p>
<p>If this actually work, then the following, or something like that, would be a rounding algorithm: given a PSD solution <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> of ARV of cost <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" />, consider PSD matrices of the form <img src="https://s0.wp.com/latex.php?latex=%7BA%5Et+X+A%5Et%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A^t X A^t}" class="latex" />, which do not necessarily satisfy the triangle inequalities any more, for <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%2C%5Cldots%2C+1%2Fd%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t = 1,\ldots, 1/d\epsilon^2}" class="latex" />, and try to round them using a random hyperplane. Would this make sense for other classes of graphs?</p>
<p>It is plausible that sparsest cut in Abelian Cayley graphs has actually a constant-factor approximation in polynomial or quasi-polynomial time, and maybe either that ARV achieves constant-factor approximation.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/08/arv-on-abelian-cayley-graphs/"><span class="datestr">at October 08, 2021 02:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19194">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/">Physics Nobel Prize for 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>If I could explain it to the average person, I wouldn’t have been worth the Nobel Prize— Richard Feynman</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/gp/" rel="attachment wp-att-19196"><img width="170" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/gp.png?resize=170%2C170&amp;ssl=1" class="aligncenter wp-image-19196" height="170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite from front-page <a href="https://www.nobelprize.org/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Giorgio Parisi has just been awarded the 2021 Physics Nobel Prize for his work on the disorder in systems of all kinds. At the same time Syukuro Manabe and Klaus Hasselmann won for their joint work on the physical modeling of the Earth’s climate and reliably predicting global warming.</p>
<p>
Today Ken and I look at some aspects of this year’s <a href="https://www.nobelprize.org/prizes/physics/2021/summary/">prizes</a>.<br />
<span id="more-19194"></span></p>
<p>
Small issue: I do believe in human-caused global warming, but shouldn’t the citation read: <i>for reliably predicting the future temperature of the planet</i>? Currently it reads a bit like <i>for reliably predicting the <b>rise</b> in the global stock market</i>, as if the outcome not the model were primary. </p>
<p>
Oh well. Every year there are Nobel prizes awarded for things that have at least some computational aspect. This year’s prize is certainly for something related to computation. One aspect that we are hoping for is Aspect—Alain Aspect. Ken was among <a href="https://www.insidescience.org/news/nine-nobel-prize-predictions-2021">those</a> <a href="https://www.france24.com/en/live-news/20211005-invisibility-cloak-and-quantum-physics-tipped-for-nobel-prize">tipping</a> him, John Clauser, and Anton Zeilinger for this year’s prize—for their work demonstrating quantum entanglement and non-classical experimental outcomes. This work spans substantial areas of quantum computing.</p>
<p>
</p><p></p><h2> General Comments </h2><p></p>
<p></p><p>
The Nobel committee’s technical <a href="https://www.nobelprize.org/uploads/2021/10/sciback_fy_en_21.pdf">review</a> of the prizewinning trio’s accomplishments ends with an expansive comment: </p>
<blockquote><p><b> </b> <em> Clearly this year’s Laureates have made groundbreaking contributions to our understanding of complex physical systems in their broadest sense, from the microscopic to the global. They show that without a proper accounting of disorder, noise and variability, determinism is just an illusion. Indeed, the work recognized here reflects in part the comment ascribed to Richard Feynman (Nobel Laureate 1965), that he “Believed in the primacy of doubt, not as a blemish on our ability to know, but as the essence of knowing.”</em></p><em>
</em><p><em>
Recognizing the work of this troika reflects the importance of understanding that no single prediction of anything can be taken as inviolable truth, and that without soberly probing the origins of variability we cannot understand the behavior of any system. Therefore, only after having considered these origins do we understand that global warming is real and attributable to our own activities, that a vast array of the phenomena we observe in nature emerge from an underlying disorder, and that embracing the noise and uncertainty is an essential step on the road towards predictability. </em>
</p></blockquote>
<p></p><p>
Ken says about the global-warming part of the last sentence that all you need to do is ask a wine grower, of which there are many in the Niagara region. The latitudes at which certain grape strains thrive have been <a href="https://www.economist.com/graphic-detail/2019/11/22/climate-change-is-forcing-winemakers-to-move-further-from-the-equator">shifting</a> <a href="https://www.economist.com/europe/2021/07/15/climate-change-is-affecting-wine-flavours">recently</a> and <a href="https://www.winemag.com/2020/02/03/wine-climate-change/">steadily</a> <a href="https://www.nytimes.com/interactive/2019/10/14/dining/drinks/climate-change-wine.html">north</a>.</p>
<p>
For some general comments by Parisi, see his <a href="https://www.nobelprize.org/prizes/physics/2021/parisi/interview/">interview</a>. He says lots of neat stuff including:</p>
<blockquote><p><b> </b> <em> Well, things that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> Well, my mentor Nicola Cabibbo was usually saying that we should work on a problem only if working on the problem is fun. So, I mean, fun is not very clear what it means, but it’s something that we find deeply interesting, and that we strongly believe that it is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> I mean you won’t find fun in <i>unclear</i> because one gets a new idea of something unexpected and so on. So I tried to work on something that was interesting and which I believed that had some capacity to add something. </em>
</p></blockquote>
<p>
</p><p></p><h2> His Trick </h2><p></p>
<p></p><p>
The Nobel citation says: “One of the many theoretical tools Professor Parisi has used to establish his theory is the so-called ‘replica trick’—a mathematical method which takes a disordered system, replicates it multiple times, and compares how different replicas of the system behave. You can do this, for instance, by compressing marbles in a box, which will form a different configuration each time you make the compression. Over many repetitions, Parisi knew, telling patterns might emerge.” They point to a <a href="https://arxiv.org/abs/1409.2722">paper</a> from talks by Parisi in 2013 also involving Flaviano Morone, Francesco Caltagirone, and Elizabeth Harrison. </p>
<p>
The trick has a Wikipedia <a href="https://en.wikipedia.org/wiki/Replica_trick">page</a>, which says that its crux “is that while the disorder averaging is done assuming <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> to be an integer, to recover the disorder-averaged logarithm one must send <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> continuously to zero. This apparent contradiction at the heart of the replica trick has never been formally resolved, however in all cases where the replica method can be compared with other exact solutions, the methods lead to the same results.” </p>
<p>
The mathematical identity underlying the replica trick is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+Z+%3D+%5Clim_%7Bn%5Crightarrow+0%7D%5Cfrac%7BZ%5En+-+1%7D%7Bn%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \ln Z = \lim_{n\rightarrow 0}\frac{Z^n - 1}{n}. " class="latex" /></p>
<p>The <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z}" class="latex" /> is the <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition function</a> of a physical system or some related thermodynamical measure. The power <img src="https://s0.wp.com/latex.php?latex=%7BZ%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z^n}" class="latex" /> represents <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent copies of the system—if <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is an integer, that is. But instead we treat <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> as a real number going to zero. The formal justification is best seen via a Taylor series expansion: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7BZ%5En+-+1%7D%7Bn%7D+%26%3D%26+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B-1+%2B+e%5E%7Bn+%5Cln+Z%7D%7D%7Bn%7D%5C%5C+%26%3D%26+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B-1+%2B+1+%2B+n+%5Cln+Z+%2B+%5Cfrac%7B1%7D%7B2%21%7D+%28n+%5Cln+Z%29%5E2+%2B+%5Cfrac%7B1%7D%7B3%21%7D+%28n+%5Cln+Z%29%5E3+%2B+%5Cdots%7D%7Bn%7D%5C%5C+%26%3D%26+%5Cln+Z+%2B+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cfrac%7B1%7D%7B2%21%7D+%28n+%5Cln+Z%29%5E2+%2B+%5Cfrac%7B1%7D%7B3%21%7D+%28n+%5Cln+Z%29%5E3+%2B+%5Cdots%7D%7Bn%7D%5C%5C+%26%3D%26+%5Cln+Z.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  \lim_{n \rightarrow 0} \frac{Z^n - 1}{n} &amp;=&amp; \lim_{n \rightarrow 0} \frac{-1 + e^{n \ln Z}}{n}\\ &amp;=&amp; \lim_{n \rightarrow 0} \frac{-1 + 1 + n \ln Z + \frac{1}{2!} (n \ln Z)^2 + \frac{1}{3!} (n \ln Z)^3 + \dots}{n}\\ &amp;=&amp; \ln Z + \lim_{n \rightarrow 0} \frac{\frac{1}{2!} (n \ln Z)^2 + \frac{1}{3!} (n \ln Z)^3 + \dots}{n}\\ &amp;=&amp; \ln Z. \end{array} " class="latex" /></p>
<p>
But on the whole, this strikes me as a silly idea. Suppose that you have a nasty function <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%2C2%2C3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=1,2,3,\dots}" class="latex" />. How do you try to understand the behavior of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" />? Several ideas come to mind: </p>
<ol>
<li>
Try to see how <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> grows as <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \rightarrow \infty}" class="latex" />? <p></p>
</li><li>
Try to get an approximate formula for <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> grows? <p></p>
</li><li>
Try to understand <img src="https://s0.wp.com/latex.php?latex=%7Bf%280%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(0)}" class="latex" />?
</li></ol>
<p>Wait, this is nuts. How can the value <img src="https://s0.wp.com/latex.php?latex=%7Bf%280%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(0)}" class="latex" /> help us understand the limit of 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%281%29%2Cf%282%29%2C%5Cdots%2Cf%281000000%29%2C%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(1),f(2),\dots,f(1000000),\dots " class="latex" /></p>
<p>Indeed.  Another form is that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5Cpartial+Z%5En%7D%7B%5Cpartial+n%7D+%3D+%5Cfrac%7B%5Cpartial+e%5E%7Bn%5Cln+Z%7D%7D%7B%5Cpartial+n%7D+%3D+%28%5Cln+Z%29Z%5En+%5Cto+%5Cln+Z+%5Ctext+%7B+as+%7D+n+%5Cto+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\partial Z^n}{\partial n} = \frac{\partial e^{n\ln Z}}{\partial n} = (\ln Z)Z^n \to \ln Z \text { as } n \to 0. " class="latex" /></p>
<p>
The trick here is that instead of letting <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> go to infinity they set <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> to zero. This is crazy. But it is so crazy that it yields a ton of insight into the behavior for large <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. I wish I could understand this better. Perhaps it could help us with our problems like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" />? </p>
<p>
Indeed, one of Parisi’s main applications is to <a href="https://en.wikipedia.org/wiki/Spin_glass">spin glass</a> models, which are <a href="https://www.quora.com/What-is-the-relationship-between-the-Ising-model-and-spin-glasses">related</a> to the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> and likewise have associated problems that are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-hard. This <a href="https://arxiv.org/abs/1810.05907">applies</a> even to a spin-glass model for which Parisi showed that exact solutions are computable.</p>
<p>
</p><p></p><h2> Manabe and Hasselmann </h2><p></p>
<p></p><p>
The prize for Manabe is well summed by the title to this Italian news <a href="https://www.greenandblue.it/2021/10/06/news/syukuro_suki_manabe_lo_scienziato_che_ha_messo_il_clima_nei_computer-321057100/?ref=drla-1">article</a>, which translates as, “The scientist who put the climate into computers.” The article’s subhead makes a point that deserves further reflection: “Without his models it would have been impossible to do experiments on the climate.” Stated positively and picking up “reliable” from the Nobel citation, the point is:</p>
<blockquote><p><b> </b> <em> Having reliable climate models has made it possible to do experiments on the climate. </em>
</p></blockquote>
<p></p><p>
Now we cannot actually <em>do</em> experiments <em>on</em> the climate—short of setting off a <a href="https://en.wikipedia.org/wiki/Nuclear_winter">nuclear winter</a> or erecting a <a href="https://en.wikipedia.org/wiki/Dyson_sphere">Dyson sphere</a>, maybe. We can observe changes caused by <a href="https://en.wikipedia.org/wiki/El_Nino">El Niño</a> events and shifts in the jet stream, for instance, and try to build the framework of a controlled experiment around them. But that’s all. </p>
<p>
So what is meant is that the models are robust enough, and have proven themselves on predictions at smaller or broader scales, that we can confidently regard computational experiments with them as indicative of real-world outcomes. The article mentions being able to tell what would happen if we made mountains disappear or shuffled the continents around. But in line with what we said in the intro, the logic sounds circular or self-confirming unless its reasonableness is explained more. The Washington Post <a href="https://www.washingtonpost.com/science/2021/10/05/nobel-prize-physics/">article</a> on the prizes stops short of saying that Manabe’s modeling of the effect of atmospheric carbon dioxide is a truly confirmed prediction, but it does say that his 50 years of modeling choices have had an almost 1.000 batting average.</p>
<p>
Hasselmann is hailed for supplying a different piece that promotes confidence and accounting of causality. This is to demonstrate consistent effects of short-term local weather events on longer-term global climate. To those like us not versed in the background, this might again sound circular: didn’t the global configuration cause the local event? One particular effect Hasselmann traced is of weather events on ocean currents. The chain from atmosphere to storm to ocean sub-surface is non-circular. </p>
<p>
Perhaps all this can be put as pithily as John Wheeler’s non-circular explanation of general relativity: “Matter tells space-time how to curve, and curved space-time tells matter how to move.” But until then, we feel a need to hallow a more fundamental story of how computational modeling works and why it is effective.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
One day we will see a more computationally-oriented Nobel Prize, but how soon? Until then, best to all working on theory. You are Laureates in our eyes.</p>
<p>
[added equation with derivatives to “Trick” section]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/10/08/physics-nobel-prize-for-2021/"><span class="datestr">at October 08, 2021 04:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
