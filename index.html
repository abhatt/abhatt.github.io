<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 04, 2019 08:23 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00908">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00908">Phase-based Minimalist Parsing and complexity in non-local dependencies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chesi:Cristiano.html">Cristiano Chesi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00908">PDF</a><br /><b>Abstract: </b>A cognitively plausible parsing algorithm should perform like the human
parser in critical contexts. Here I propose an adaptation of Earley's parsing
algorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that
is able to predict complexity effects in performance. Focusing on self-paced
reading experiments of object clefts sentences (Warren &amp; Gibson 2005) I will
associate to parsing a complexity metric based on cued features to be retrieved
at the verb segment (Feature Retrieval &amp; Encoding Cost, FREC). FREC is
crucially based on the usage of memory predicted by the discussed parsing
algorithm and it correctly fits with the reading time revealed.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00908"><span class="datestr">at June 04, 2019 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00809">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00809">Rpair: Rescaling RePair with Rsync</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gagie:Travis.html">Travis Gagie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/I:Tomohiro.html">Tomohiro I</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakamoto:Hiroshi.html">Hiroshi Sakamoto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takabatake:Yoshimasa.html">Yoshimasa Takabatake</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00809">PDF</a><br /><b>Abstract: </b>Data compression is a powerful tool for managing massive but repetitive
datasets, especially schemes such as grammar-based compression that support
computation over the data without decompressing it. In the best case such a
scheme takes a dataset so big that it must be stored on disk and shrinks it
enough that it can be stored and processed in internal memory. Even then,
however, the scheme is essentially useless unless it can be built on the
original dataset reasonably quickly while keeping the dataset on disk. In this
paper we show how we can preprocess such datasets with context-triggered
piecewise hashing such that afterwards we can apply RePair and other
grammar-based compressors more easily. We first give our algorithm, then show
how a variant of it can be used to approximate the LZ77 parse, then leverage
that to prove theoretical bounds on compression, and finally give experimental
evidence that our approach is competitive in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00809"><span class="datestr">at June 04, 2019 01:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00703">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00703">Parameterised Complexity for Abduction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahmood:Yasir.html">Yasir Mahmood</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meier:Arne.html">Arne Meier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt:Johannes.html">Johannes Schmidt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00703">PDF</a><br /><b>Abstract: </b>Abductive reasoning is a non-monotonic formalism stemming from the work of
Peirce. It describes the process of deriving the most plausible explanations of
known facts. Considering the positive version asking for sets of variables as
explanations, we study, besides asking for existence of the set of
explanations, two explanation size limited variants of this reasoning problem
(less than or equal to, and equal to). In this paper, we present a thorough
classification regarding the parameterised complexity of these problems under a
wealth of different parameterisations. Furthermore, we analyse all possible
Boolean fragments of these problems in the constraint satisfaction approach
with co-clones. Thereby, we complete the parameterised picture started by
Fellows et al. (AAAI 2012), partially building on results of Nordh and
Zanuttini (Artif. Intell. 2008). In this process, we outline a fine-grained
analysis of the inherent intractability of these problems and pinpoint their
tractable parts.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00703"><span class="datestr">at June 04, 2019 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00692">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00692">Betti numbers of unordered configuration spaces of small graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Drummond=Cole:Gabriel_C=.html">Gabriel C. Drummond-Cole</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00692">PDF</a><br /><b>Abstract: </b>The purpose of this document is to provide data about known Betti numbers of
unordered configuration spaces of small graphs in order to guide research and
avoid duplicated effort. It contains information for connected multigraphs
having at most six edges which contain no loops, no bivalent vertices, and no
internal (i.e., non-leaf) bridges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00692"><span class="datestr">at June 04, 2019 01:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00659">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00659">Multistage Vertex Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fluschnik:Till.html">Till Fluschnik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a>, Valentin Rohm, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00659">PDF</a><br /><b>Abstract: </b>Covering all edges of a graph by a minimum number of vertices, this is the
NP-hard Vertex Cover problem, is among the most fundamental algorithmic tasks.
Following a recent trend in studying dynamic and temporal graphs, we initiate
the study of Multistage Vertex Cover. Herein, having a series of graphs with
same vertex set but over time changing edge sets (known as temporal graph
consisting of various layers), the goal is to find for each layer of the
temporal graph a small vertex cover and to guarantee that the two vertex cover
sets between two subsequent layers differ not too much (specified by a given
parameter). We show that, different from classic Vertex Cover and some other
dynamic or temporal variants of it, Multistage Vertex Cover is computationally
hard even in fairly restricted settings. On the positive side, however, we also
spot several fixed-parameter tractability results based on some of the most
natural parameterizations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00659"><span class="datestr">at June 04, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00618">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00618">A Direct $\tilde{O}(1/\epsilon)$ Iteration Parallel Algorithm for Optimal Transport</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jambulapati:Arun.html">Arun Jambulapati</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00618">PDF</a><br /><b>Abstract: </b>Optimal transportation, or computing the Wasserstein or ``earth mover's''
distance between two distributions, is a fundamental primitive which arises in
many learning and statistical settings. We give an algorithm which solves this
problem to additive $\epsilon$ with $\tilde{O}(1/\epsilon)$ parallel depth, and
$\tilde{O}\left(n^2/\epsilon\right)$ work. Barring a breakthrough on a
long-standing algorithmic open problem, this is optimal for first-order
methods. Blanchet et. al. '18, Quanrud '19 obtained similar runtimes through
reductions to positive linear programming and matrix scaling. However, these
reduction-based algorithms use complicated subroutines which may be deemed
impractical due to requiring solvers for second-order iterations (matrix
scaling) or non-parallelizability (positive LP). The fastest practical
algorithms run in time $\tilde{O}(\min(n^2 / \epsilon^2, n^{2.5} / \epsilon))$
(Dvurechensky et. al. '18, Lin et. al. '19). We bridge this gap by providing a
parallel, first-order, $\tilde{O}(1/\epsilon)$ iteration algorithm without
worse dependence on dimension, and provide preliminary experimental evidence
that our algorithm may enjoy improved practical performance. We obtain this
runtime via a primal-dual extragradient method, motivated by recent theoretical
improvements to maximum flow (Sherman '17).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00618"><span class="datestr">at June 04, 2019 01:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00563">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00563">Direct Linear Time Construction of Parameterized Suffix and LCP Arrays for Constant Alphabets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujisato:Noriki.html">Noriki Fujisato</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00563">PDF</a><br /><b>Abstract: </b>We present the first worst-case linear time algorithm that directly computes
the parameterized suffix and LCP arrays for constant sized alphabets. Previous
algorithms either required quadratic time or the parameterized suffix tree to
be built first. More formally, for a string over static alphabet $\Sigma$ and
parameterized alphabet $\Pi$, our algorithm runs in $O(n\pi)$ time and $O(n)$
words of space, where $\pi$ is the number of distinct symbols of $\Pi$ in the
string.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00563"><span class="datestr">at June 04, 2019 01:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00482">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00482">On the Use of Randomness in Local Distributed Graph Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00482">PDF</a><br /><b>Abstract: </b>We attempt to better understand randomization in local distributed graph
algorithms by exploring how randomness is used and what we can gain from it: -
We first ask the question of how much randomness is needed to obtain efficient
randomized algorithms. We show that for all locally checkable problems for
which polylog $n$-time randomized algorithms exist, there are such algorithms
even if either (I) there is a only a single (private) independent random bit in
each polylog $n$-neighborhood of the graph, (II) the (private) bits of
randomness of different nodes are only polylog $n$-wise independent, or (III)
there are only polylog $n$ bits of global shared randomness (and no private
randomness). - Second, we study how much we can improve the error probability
of randomized algorithms. For all locally checkable problems for which polylog
$n$-time randomized algorithms exist, we show that there are such algorithms
that succeed with probability $1-n^{-2^{\varepsilon(\log\log n)^2}}$ and more
generally $T$-round algorithms, for $T\geq$ polylog $n$, that succeed with
probability $1-n^{-2^{\varepsilon\log^2T}}$. We also show that polylog $n$-time
randomized algorithms with success probability $1-2^{-2^{\log^\varepsilon n}}$
for some $\varepsilon&gt;0$ can be derandomized to polylog $n$-time deterministic
algorithms. Both of the directions mentioned above, reducing the amount of
randomness and improving the success probability, can be seen as partial
derandomization of existing randomized algorithms. In all the above cases, we
also show that any significant improvement of our results would lead to a major
breakthrough, as it would imply significantly more efficient deterministic
distributed algorithms for a wide class of problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00482"><span class="datestr">at June 04, 2019 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00476">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00476">Noise reduction using past causal cones in variational quantum algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shehab:Omar.html">Omar Shehab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Isaac_H=.html">Isaac H. Kim</a>, Nhung H. Nguyen, Kevin Landsman, Cinthia H. Alderete, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Daiwei.html">Daiwei Zhu</a>, C. Monroe, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Linke:Norbert_M=.html">Norbert M. Linke</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00476">PDF</a><br /><b>Abstract: </b>We introduce an approach to improve the accuracy and reduce the sample
complexity of near term quantum-classical algorithms. We construct a simpler
initial parameterized quantum state, or ansatz, based on the past causal cone
of each observable, generally yielding fewer qubits and gates. We implement
this protocol on a trapped ion quantum computer and demonstrate improvement in
accuracy and time-to-solution at an arbitrary point in the variational search
space. We report a $\sim 27\%$ improvement in the accuracy of the variational
calculation of the deuteron binding energy and $\sim 40\%$ improvement in the
accuracy of the quantum approximate optimization of the MAXCUT problem applied
to the dragon graph $T_{3,2}$. When the time-to-solution is prioritized over
accuracy, the former requires $\sim 71\%$ fewer measurements and the latter
requires $\sim 78\%$ fewer measurements.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00476"><span class="datestr">at June 04, 2019 01:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00417">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00417">The Number of Minimum $k$-Cuts: Improving the Karger-Stein Bound</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Euiwoong.html">Euiwoong Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00417">PDF</a><br /><b>Abstract: </b>Given an edge-weighted graph, how many minimum $k$-cuts can it have? This is
a fundamental question in the intersection of algorithms, extremal
combinatorics, and graph theory. It is particularly interesting in that the
best known bounds are algorithmic: they stem from algorithms that compute the
minimum $k$-cut.
</p>
<p>In 1994, Karger and Stein obtained a randomized contraction algorithm that
finds a minimum $k$-cut in $O(n^{(2-o(1))k})$ time. It can also enumerate all
such $k$-cuts in the same running time, establishing a corresponding extremal
bound of $O(n^{(2-o(1))k})$. Since then, the algorithmic side of the minimum
$k$-cut problem has seen much progress, leading to a deterministic algorithm
based on a tree packing result of Thorup, which enumerates all minimum $k$-cuts
in the same asymptotic running time, and gives an alternate proof of the
$O(n^{(2-o(1))k})$ bound. However, beating the Karger--Stein bound, even for
computing a single minimum $k$-cut, has remained out of reach.
</p>
<p>In this paper, we give an algorithm to enumerate all minimum $k$-cuts in
$O(n^{(1.981+o(1))k})$ time, breaking the algorithmic and extremal barriers for
enumerating minimum $k$-cuts. To obtain our result, we combine ideas from both
the Karger--Stein and Thorup results, and draw a novel connection between
minimum $k$-cut and extremal set theory. In particular, we give and use tighter
bounds on the size of set systems with bounded dual VC-dimension, which may be
of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00417"><span class="datestr">at June 04, 2019 01:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00339">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00339">Sample-Optimal Low-Rank Approximation of Distance Matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vakilian:Ali.html">Ali Vakilian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Tal.html">Tal Wagner</a>, David Woodruff <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00339">PDF</a><br /><b>Abstract: </b>A distance matrix $A \in \mathbb R^{n \times m}$ represents all pairwise
distances, $A_{ij}=\mathrm{d}(x_i,y_j)$, between two point sets $x_1,...,x_n$
and $y_1,...,y_m$ in an arbitrary metric space $(\mathcal Z, \mathrm{d})$. Such
matrices arise in various computational contexts such as learning image
manifolds, handwriting recognition, and multi-dimensional unfolding.
</p>
<p>In this work we study algorithms for low-rank approximation of distance
matrices. Recent work by Bakshi and Woodruff (NeurIPS 2018) showed it is
possible to compute a rank-$k$ approximation of a distance matrix in time
$O((n+m)^{1+\gamma}) \cdot \mathrm{poly}(k,1/\epsilon)$, where $\epsilon&gt;0$ is
an error parameter and $\gamma&gt;0$ is an arbitrarily small constant. Notably,
their bound is sublinear in the matrix size, which is unachievable for general
matrices.
</p>
<p>We present an algorithm that is both simpler and more efficient. It reads
only $O((n+m) k/\epsilon)$ entries of the input matrix, and has a running time
of $O(n+m) \cdot \mathrm{poly}(k,1/\epsilon)$. We complement the sample
complexity of our algorithm with a matching lower bound on the number of
entries that must be read by any algorithm. We provide experimental results to
validate the approximation quality and running time of our algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00339"><span class="datestr">at June 04, 2019 01:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00326">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00326">Approximate degree, secret sharing, and concentration phenomena</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bogdanov:Andrej.html">Andrej Bogdanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mande:Nikhil_S=.html">Nikhil S. Mande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thaler:Justin.html">Justin Thaler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williamson:Christopher.html">Christopher Williamson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00326">PDF</a><br /><b>Abstract: </b>The $\epsilon$-approximate degree $deg_\epsilon(f)$ of a Boolean function $f$
is the least degree of a real-valued polynomial that approximates $f$ pointwise
to error $\epsilon$. The approximate degree of $f$ is at least $k$ iff there
exists a pair of probability distributions, also known as a dual polynomial,
that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$
with advantage $1 - \epsilon$. Our contributions are:
</p>
<p>We give a simple new construction of a dual polynomial for the AND function,
certifying that $deg_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$. This
construction is the first to extend to the notion of weighted degree, and
yields the first explicit certificate that the $1/3$-approximate degree of any
read-once DNF is $\Omega(\sqrt{n})$.
</p>
<p>We show that any pair of symmetric distributions on $n$-bit strings that are
perfectly $k$-wise indistinguishable are also statistically $K$-wise
indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for
all $k &lt; K &lt; n/64$. This implies that any symmetric function $f$ is a
reconstruction function with constant advantage for a ramp secret sharing
scheme that is secure against size-$K$ coalitions with statistical error
$K^{3/2} \exp(-\Omega(deg_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$
simultaneously. Previous secret sharing schemes required that $K$ be determined
in advance, and only worked for $f=$ AND.
</p>
<p>Our analyses draw new connections between approximate degree and
concentration phenomena.
</p>
<p>As a corollary, we show that for any $d &lt; n/64$, any degree $d$ polynomial
approximating a symmetric function $f$ to error $1/3$ must have $\ell_1$-norm
at least $K^{-3/2} \exp({\Omega(deg_{1/3}(f)^2/d)})$, which we also show to be
tight for any $d &gt; deg_{1/3}(f)$. These upper and lower bounds were also
previously only known in the case $f=$ AND.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00326"><span class="datestr">at June 04, 2019 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00324">Ubiquitous Complexity of Entanglement Spectra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Bin.html">Bin Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yung:Man=Hong.html">Man-Hong Yung</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00324">PDF</a><br /><b>Abstract: </b>In recent years, the entanglement spectra of quantum states have been
identified to be highly valuable for improving our understanding on many
problems in quantum physics, such as classification of topological phases,
symmetry-breaking phases, and eigenstate thermalization, etc. However, it
remains a major challenge to fully characterize the entanglement spectrum of a
given quantum state. An outstanding problem is whether the difficulty is
intrinsically technical or fundamental? Here using the tools in computational
complexity, we perform a rigorous analysis to pin down the counting complexity
of entanglement spectra of (i) states generated by polynomial-time quantum
circuits, (ii) ground states of gapped 5-local Hamiltonians, and (iii)
projected entangled-pair states (PEPS). We prove that despite the state
complexity, the problems of counting the number of sizable elements in the
entanglement spectra all belong to the class $\mathsf{\# P}$-complete, which is
as hard as calculating the partition functions of Ising models. Our result
suggests that the absence of an efficient method for solving the problem is
fundamental in nature, from the point of view of computational complexity
theory.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00324"><span class="datestr">at June 04, 2019 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00298">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00298">Optimal Register Construction in M&amp;M Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hadzilacos:Vassos.html">Vassos Hadzilacos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xing.html">Xing Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Toueg:Sam.html">Sam Toueg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00298">PDF</a><br /><b>Abstract: </b>Motivated by recent distributed systems technology, Aguilera et al.
introduced a hybrid model of distributed computing, called message-and-memory
model or m&amp;m model for short. In this model processes can communicate by
message passing and also by accessing some shared memory. We consider the basic
problem of implementing an atomic single-writer multi-reader (SWMR) register
shared by all the processes in m&amp;m systems. Specifically, for every m&amp;m system,
we give an algorithm that implements such a register in this system and show
that it is optimal in the number of process crashes that it can tolerate. This
generalizes the well-known implementation of an atomic SWMR register in a pure
message-passing system.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00298"><span class="datestr">at June 04, 2019 01:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00294">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00294">On the computational complexity of the probabilistic label tree algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Robert Busa-Fekete, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dembczynski:Krzysztof.html">Krzysztof Dembczynski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovnev:Alexander.html">Alexander Golovnev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jasinska:Kalina.html">Kalina Jasinska</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuznetsov:Mikhail.html">Mikhail Kuznetsov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sviridenko:Maxim.html">Maxim Sviridenko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Chao.html">Chao Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00294">PDF</a><br /><b>Abstract: </b>Label tree-based algorithms are widely used to tackle multi-class and
multi-label problems with a large number of labels. We focus on a particular
subclass of these algorithms that use probabilistic classifiers in the tree
nodes. Examples of such algorithms are hierarchical softmax (HSM), designed for
multi-class classification, and probabilistic label trees (PLTs) that
generalize HSM to multi-label problems. If the tree structure is given,
learning of PLT can be solved with provable regret guaranties [Wydmuch et.al.
2018]. However, to find a tree structure that results in a PLT with a low
training and prediction computational costs as well as low statistical error
seems to be a very challenging problem, not well-understood yet.
</p>
<p>In this paper, we address the problem of finding a tree structure that has
low computational cost. First, we show that finding a tree with optimal
training cost is NP-complete, nevertheless there are some tractable special
cases with either perfect approximation or exact solution that can be obtained
in linear time in terms of the number of labels $m$. For the general case, we
obtain $O(\log m)$ approximation in linear time too. Moreover, we prove an
upper bound on the expected prediction cost expressed in terms of the expected
training cost. We also show that under additional assumptions the prediction
cost of a PLT is $O(\log m)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00294"><span class="datestr">at June 04, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00219">Probabilistic Top-k Dominating Query Monitoring over Multiple Uncertain IoT Data Streams in Edge Computing Environments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lai:Chuan=Chi.html">Chuan-Chi Lai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Tien=Chun.html">Tien-Chun Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Chuan=Ming.html">Chuan-Ming Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Li=Chun.html">Li-Chun Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00219">PDF</a><br /><b>Abstract: </b>Extracting the valuable features and information in Big Data has become one
of the important research issues in Data Science. In most Internet of Things
(IoT) applications, the collected data are uncertain and imprecise due to
sensor device variations or transmission errors. In addition, the sensing data
may change as time evolves. We refer an uncertain data stream as a dataset that
has velocity, veracity, and volume properties simultaneously. This paper
employs the parallelism in edge computing environments to facilitate the top-k
dominating query process over multiple uncertain IoT data streams. The
challenges of this problem include how to quickly update the result for
processing uncertainty and reduce the computation cost as well as provide
highly accurate results. By referring to the related existing papers for
certain data, we provide an effective probabilistic top-k dominating query
process on uncertain data streams, which can be parallelized easily. After
discussing the properties of the proposed approach, we validate our methods
through the complexity analysis and extensive simulated experiments. In
comparison with the existing works, the experimental results indicate that our
method can improve almost 60% computation time, reduce nearly 20% communication
cost between servers, and provide highly accurate results in most scenarios.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00219"><span class="datestr">at June 04, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00211">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00211">Multi-reference factor analysis: low-rank covariance estimation under unknown translations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landa:Boris.html">Boris Landa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shkolnisky:Yoel.html">Yoel Shkolnisky</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00211">PDF</a><br /><b>Abstract: </b>We consider the problem of estimating the covariance matrix of a random
signal observed through unknown translations (modeled by cyclic shifts) and
corrupted by noise. Solving this problem allows to discover low-rank structures
masked by the existence of translations (which act as nuisance parameters),
with direct application to Principal Components Analysis (PCA). We assume that
the underlying signal is of length $L$ and follows a standard factor model with
mean zero and $r$ normally-distributed factors. To recover the covariance
matrix in this case, we propose to employ the second- and fourth-order
shift-invariant moments of the signal known as the $\textit{power spectrum}$
and the $\textit{trispectrum}$. We prove that they are sufficient for
recovering the covariance matrix (under a certain technical condition) when
$r&lt;\sqrt{L}$. Correspondingly, we provide a polynomial-time procedure for
estimating the covariance matrix from many (translated and noisy) observations,
where no explicit knowledge of $r$ is required, and prove the procedure's
statistical consistency. While our results establish that covariance estimation
is possible from the power spectrum and the trispectrum for low-rank covariance
matrices, we prove that this is not the case for full-rank covariance matrices.
We conduct numerical experiments that corroborate our theoretical findings, and
demonstrate the favorable performance of our algorithms in various settings,
including in high levels of noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00211"><span class="datestr">at June 04, 2019 01:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00191">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00191">On problems related to crossing families</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Evans:William.html">William Evans</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saeedi:Noushin.html">Noushin Saeedi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00191">PDF</a><br /><b>Abstract: </b>Given a set of points in the plane, a \emph{crossing family} is a collection
of segments, each joining two of the points, such that every two segments
intersect internally. Aronov et al. [Combinatorica,~14(2):127-134,~1994] proved
that any set of $n$ points contains a crossing family of size
$\Omega(\sqrt{n})$. They also mentioned that there exist point sets whose
maximum crossing family uses at most $\frac{n}{2}$ of the points. We improve
the upper bound on the size of crossing families to $5\lceil \frac{n}{24}
\rceil$. We also introduce a few generalizations of crossing families, and give
several lower and upper bounds on our generalized notions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00191"><span class="datestr">at June 04, 2019 01:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00140">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00140">Fast Algorithm for K-Truss Discovery on Public-Private Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Soroush Ebadian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Xin.html">Xin Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00140">PDF</a><br /><b>Abstract: </b>In public-private graphs, users share one public graph and have their own
private graphs. A private graph consists of personal private contacts that only
can be visible to its owner, e.g., hidden friend lists on Facebook and secret
following on Sina Weibo. However, existing public-private analytic algorithms
have not yet investigated the dense subgraph discovery of k-truss, where each
edge is contained in at least k-2 triangles. This paper aims at finding k-truss
efficiently in public-private graphs. The core of our solution is a novel
algorithm to update k-truss with node insertions. We develop a
classification-based hybrid strategy of node insertions and edge insertions to
incrementally compute k-truss in public-private graphs. Extensive experiments
validate the superiority of our proposed algorithms against state-of-the-art
methods on real-world datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00140"><span class="datestr">at June 04, 2019 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00029">Human-Usable Password Schemas: Beyond Information-Theoretic Security</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosenfeld:Elan.html">Elan Rosenfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh.html">Santosh Vempala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blum:Manuel.html">Manuel Blum</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00029">PDF</a><br /><b>Abstract: </b>Password users frequently employ passwords that are too simple, or they just
reuse passwords for multiple websites. A common complaint is that utilizing
secure passwords is too difficult. One possible solution to this problem is to
use a password schema. Password schemas are deterministic functions which map
challenges (typically the website name) to responses (passwords). Previous work
has been done on developing and analyzing publishable schemas, but these
analyses have been information-theoretic, not complexity-theoretic; they
consider an adversary with infinite computing power.
</p>
<p>We perform an analysis with respect to adversaries having currently
achievable computing capabilities, assessing the realistic practical security
of such schemas. We prove for several specific schemas that a computer is no
worse off than an infinite adversary and that it can successfully extract all
information from leaked challenges and their respective responses, known as
challenge-response pairs. We also show that any schema that hopes to be secure
against adversaries with bounded computation should obscure information in a
very specific way, by introducing many possible constraints with each
challenge-response pair. These surprising results put the analyses of password
schemas on a more solid and practical footing.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00029"><span class="datestr">at June 04, 2019 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.00013">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.00013">Parameterization of tensor network contraction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Bryan O'Gorman <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00013">PDF</a><br /><b>Abstract: </b>We present a conceptually clear and algorithmically useful framework for
parameterizing the costs of tensor network contraction. Our framework is
completely general, applying to tensor networks with arbitrary bond dimensions,
open legs, and hyperedges. The fundamental objects of our framework are rooted
and unrooted contraction trees, which represent classes of contraction orders.
Properties of a contraction tree correspond directly and precisely to the time
and space costs of tensor network contraction. The properties of rooted
contraction trees give the costs of parallelized contraction algorithms. We
show how contraction trees relate to existing tree-like objects in the graph
theory literature, bringing to bear a wide range of graph algorithms and tools
to tensor network contraction. Independent of tensor networks, we show that the
edge congestion of a graph is almost equal to the branchwidth of its line
graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.00013"><span class="datestr">at June 04, 2019 01:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/06/03/trajectories/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/06/03/trajectories/">Is Optimization a Sufficient Language for Understanding Deep Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this Deep Learning era, machine learning usually boils down to defining a suitable objective/cost function for the learning task at hand, and then optimizing this function using some variant of gradient descent (implemented via backpropagation).  Little wonder that hundreds of ML papers each year are devoted to various aspects of optimization. Today I will suggest that if our goal is mathematical understanding of deep learning, then  the optimization viewpoint is potentially insufficient —at least in the conventional view:</p>

<blockquote>
  <p><strong>Conventional View (CV) of Optimization</strong>: Find a solution of minimum possible value of the objective, as fast as possible.</p>
</blockquote>

<p>Note that <em>a priori</em> it is not obvious if all learning should involve optimizing a single objective. Whether or not this is true for the working of the brain is a longstanding open question in neuroscience. Brain components appear to have been repurposed/cobbled together through various accidents of evolution and the whole assemblage may or may not boil down to optimization of an objective. See <a href="https://arxiv.org/pdf/1606.03813.pdf">this survey by Marblestone et al</a>.</p>

<p>I am suggesting that deep learning algorithms also have important properties that are not always reflected in the objective value. Current deep nets, being vastly overparametrized, have multiple optima. They are trained until the objective is almost zero, and training is said to succeed if the optimum (or near-optimum) model thus found also performs well on unseen/held-out data —i.e., <em>generalizes.</em> The value of the objective may imply nothing about generalization (see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>).</p>

<p>Of course experts will now ask: “Wasn’t generalization theory invented precisely for this reason as the “second leg” of machine learning,  where optimization is the first leg?” For instance this theory shows how to add regularizers to the training objective to ensure the solution generalizes. Or that <em>early stopping</em> (i.e., stopping before reaching the optimum) or even adding noise to the gradient (e.g. by playing with batch sizes and learning rates) can be preferable to perfect optimization, even in simple settings such as regression.</p>

<p>However, in practice explicit regularizers  and noising tricks can’t prevent deep nets from attaining low training objective even on data with random labels; see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>. Current generalization theory is designed to give <em>post hoc</em> explanations for why a particular model generalized. It is agnostic about <em>how</em> the solution was obtained, and thus makes few prescriptions —apart from recommending some regularization— for optimization.   (See my earlier <a href="http://www.offconvex.org/2017/12/08/generalization1/">blog post</a>, which explains the distinction between descriptive and prescriptive methods, and  that generalization theory is primarily descriptive.) The fundamental mystery is:</p>

<blockquote>
  <p>Even vanilla gradient descent (GD) is good at finding models with reasonable generalization. Furthermore, methods to speed up gradient descent (e.g., acceleration or adaptive regularization) can sometimes lead to worse generalization.</p>
</blockquote>

<p>In other words, GD has an innate bias towards finding solutions with good generalization. Magic happens along the GD trajectory and is not captured in the objective value per se. We’re reminded of the old adage.</p>

<blockquote>
  <p>The journey matters more than the destination.</p>
</blockquote>

<p>I will illustrate this viewpoint by sketching new  rigorous analyses of gradient descent in two simple but suggestive settings. I  hope more  detailed writeups will appear in future blog posts.</p>

<p>Acknowledgements: My views on this topic were initially shaped by the excellent papers from TTI Chicago group regarding the implicit bias of gradient descent (<a href="https://arxiv.org/pdf/1709.01953.pdf">Behnam Neyshabur’s thesis</a> is a good starting point), and then of course by  various coauthors.</p>

<h2 id="computing-with-infinitely-wide-deep-nets">Computing with Infinitely Wide Deep Nets</h2>

<p>Since overparametrization does not appear to hurt deep nets too much, researchers have wondered what happens in the infinite limit of overparametrization: use a fixed training set such as CIFAR10 to train a classic deep net architecture like AlexNet or VGG19 whose “width” —namely, number of channels in the convolutional filters, and number of nodes in fully connected internal layers—- is allowed to increase to <strong>infinity</strong>. Note that initialization (using sufficiently small Gaussian weights) and training makes sense for any finite width, no matter how large. We assume $\ell_2$ loss at the output.</p>

<p>Understandably, such questions can seem hopeless and pointless: all the computing in the world is insufficient to train an infinite net, and we theorists already have our hands full trying to figure out finite nets.  But sometimes in math/physics one can derive insight into questions by studying them in the infinite limit.  Here where an infinite net is training on a finite dataset like CIFAR10, the number of optima is infinite and we are trying to understand what GD does.</p>

<p>Thanks to insights in recent papers on provable learning by overparametrized deep nets (some of the key papers are: <a href="https://arxiv.org/abs/1811.04918">Allen-Zhou et al 1</a>, <a href="https://arxiv.org/abs/1811.03962">Allen-Zhou et al 2</a> <a href="https://arxiv.org/abs/1811.03804">Du et al</a>, <a href="https://arxiv.org/abs/1811.08888">Zou et al</a>) researchers have realized that a nice limiting structure emerges:</p>

<blockquote>
  <p>As width $\rightarrow \infty$, trajectory approaches the trajectory of GD for a kernel regression problem, where the (fixed) kernel in question is the so-called  <em>Neural Tangent Kernel</em> (NTK). (For convolutional nets the kernel is <em>Convolutional NTK or CNTK.</em> )</p>
</blockquote>

<p>The kernel is implicit in <a href="https://arxiv.org/abs/1810.02054">Du et al</a> and explicitly defined and named by <a href="https://arxiv.org/abs/1806.07572">Jacot et al.</a>.</p>

<p>The definition of this fixed kernel uses the infinite net at its random initialization. For  two inputs $x_i$ and $x_j$ the kernel inner product  $K(x_i, x_j)$  is the inner product of the gradient of the output with respect to the input, evaluated at $x_i$, and $x_j$ respectively. As the net size increases to infinity this kernel inner product can be shown to converge to a limiting value (there is a technicality about how to define the limit, and the series of new papers have improved the formal statement here).</p>

<p>Our <a href="https://arxiv.org/abs/1904.11955">new paper with Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov and Ruosang Wang</a> shows that the CNTK can be efficiently computed via dynamic programming, giving us a way to efficiently compute the answer of the trained net for any desired input,  <em>even though training the infinite net directly is of course computationally infeasible.</em> (Aside: Please do not confuse these new results with some earlier papers which view infinite nets as kernels or Gaussian Processes —see citations and discussion in our paper—  since they correspond to training only the top layer while freezing the lower layers to a random initialization.) Empirically we find that this infinite net (aka kernel regression with respect to the NTK) yields better performance on CIFAR10 than any previously known kernel —not counting kernels that were  hand-tuned or designed by training on image data. For instance we can compute the kernel corresponding to a 10-layer convolutional net (CNN) and obtain 77.4% success rate on CIFAR10.</p>

<h2 id="deep-matrix-factorization-for-solving-matrix-completion">Deep Matrix Factorization for solving Matrix Completion</h2>

<p><a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix completion</a>, motivated by design of recommender systems, is well-studied for over a decade: given $K$ random entries of an unknown matrix, we wish to recover the unseen entries. Solution is not unique in general. But if the unknown matrix is low rank or approximately low rank and satisfies some additional technical assumptions (eg <em>incoherence</em>) then various algorithms can recover the unseen entries approximately or even exactly. A famous algorithm uses convex programming based upon <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">nuclear/trace norm</a>  minimization as follows: find matrix that fits all the known observations and has minimum nuclear norm. It is also possible to rephrase this as a single objective in the form required by the Conventional View as follows where $S$ is the subset of indices of revealed entries,  $\lambda$ is a multiplier:</p>



<p>In case you didn’t know about nuclear norms, you will like the interesting suggestion made by <a href="http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization">Gunasekar et al.</a>: let us forget entirely about the nuclear norm and try to recover the missing entries by  simply training a linear net with two layers via simple gradient descent/backpropagation. This linear net is just a multiplication of two $n\times n $ matrices (you can read about linear deep nets in this <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">earlier blog post by Nadav Cohen</a>) so we obtain the following  where $e_i$ is the vector with all entries $0$ except for $1$ in the $i$th position:</p>



<p>The “data” now correspond to indices $(i, j) \in S$  and since $S$ was chosen randomly among all entries,  “generalization” corresponds exactly to doing well at predicting the remaining entries. Empirically, soving matrix completion this way via deep learning  (i.e., gradient descent to solve for $M_1, M_2$) works as well as the classic algorithm, leading to the following conjecture, which if true would imply that the implicit regularization effect of gradient descent is captured exactly by the nuclear norm.</p>

<blockquote>
  <p>(Conjecture by Gunasekar et al.; Rough Statement) When solving matrix completion as above using a depth-$2$ linear net, the solution obtained is exactly the  one obtained by the nuclear norm minimization method.</p>
</blockquote>

<p>But as you may have already guessed, this turns out to be too simplistic. In <a href="https://arxiv.org/abs/1905.13655">a new paper with Nadav Cohen, Wei Hu and Yuping Luo</a>, we report new experiments suggesting that the above conjecture is false. (I hedge by saying “suggest” because some fine print in the conjecture statement makes it pretty hard to refute definitively.) More interesting, we find that if we overparametrize the problem by further increasing the number of layers from two to $3$ or even higher —which we call Deep Matrix Factorization—then this empirically solves matrix completion even better than nuclear norm minimization. (Note that we’re working in the regime where $S$ is slightly smaller than what it needs to be for nuclear norm algorithm to exactly recover the matrix. Inductive bias is most important precisely in such data-poor settings!) We provide partial analysis for this improved performance of depth $N$ nets by analysing —surprise surprise!—the trajectory of gradient descent and showing how it biases toward finding solutions of low rank. Furthermore our analysis suggests that this bias toward low rank  cannot be captured by nuclear norm or any obvious Schatten quasi-norm of the end-to-end matrix.</p>

<p>NB: Empirically we find that Adam, the celebrated  acceleration method for deep learning, speeds up optimization a lot here as well, but slightly hurts generalization. This also relates to what I said above about the Conventional View.</p>

<h2 id="conclusionstakeways">Conclusions/Takeways</h2>

<p>Though the above settings are simple, I think they suggest that to understand deep learning we have to go beyond the Conventional View of optimization, which focuses only on the value of the objective and the rate of convergence.</p>

<p>(1): Different optimization strategies —GD, SGD, Adam, AdaGrad etc. —-lead to different learning algorithms. They induce different trajectories, which may lead to solutions with different generalization properties.</p>

<p>(2) We need to develop a new vocabulary (and mathematics) to reason about trajectories. This goes beyond the usual “landscape view” of stationary points, gradient norms, Hessian norms, smoothness etc. Caution: trajectories depend on initialization!</p>

<p>(3): I wish I had learnt a few tricks about ODEs/PDEs/Dynamical Systems in college, to be in better shape to reason about trajectories!</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/06/03/trajectories/"><span class="datestr">at June 03, 2019 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/robust_reps/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/robust_reps/">Robustness beyond Security&amp;#58; Representation Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://arxiv.org/abs/1906.00945" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Read the paper
</a>
<a style="float: right;" href="http://git.io/robust-reps" class="bbutton">
<i class="fab fa-github"></i>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/visualization.png" alt="" /></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png" alt="Standard representations are brittle" /></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1" />
	<div style="border-right: 3px white solid;" class="beer-reveal">
	    <img class="slider_img" id="selectedinv2" />
	</div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;" /></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"></div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1" /> 
	    <div style="border-right: 3px white solid;" class="beer-reveal">
		<img class="slider_img" id="man_selected2" /> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source type="video/mp4" id="int_selected">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;" /></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>







<p class="date">
<a href="http://gradientscience.org/robust_reps/"><span class="datestr">at June 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13651">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13651">Principal Fairness: \\ Removing Bias via Projections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anagnostopoulos:Aris.html">Aris Anagnostopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Becchetti:Luca.html">Luca Becchetti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=ouml=hm:Matteo.html">Matteo Böhm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fazzone:Adriano.html">Adriano Fazzone</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leonardi:Stefano.html">Stefano Leonardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Menghini:Cristina.html">Cristina Menghini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schwiegelshohn:Chris.html">Chris Schwiegelshohn</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13651">PDF</a><br /><b>Abstract: </b>Reducing hidden bias in the data and ensuring fairness in algorithmic data
analysis has recently received significant attention. We complement several
recent papers in this line of research by introducing a general method to
reduce bias in the data through random projections in a ``fair'' subspace.
</p>
<p>We apply this method to densest subgraph and $k$-means. For densest subgraph,
our approach based on fair projections allows to recover both theoretically and
empirically an almost optimal, fair, dense subgraph hidden in the input data.
We also show that, under the small set expansion hypothesis, approximating this
problem beyond a factor of $2$ is NP-hard and we show a polynomial time
algorithm with a matching approximation bound. We further apply our method to
$k$-means. In a previous paper, Chierichetti et al.~[NIPS 2017] showed that
problems such as $k$-means can be approximated up to a constant factor while
ensuring that none of two protected class (e.g., gender, ethnicity) is
disparately impacted.
</p>
<p>We show that fair projections generalize the concept of fairlet introduced by
Chierichietti et al. to any number of protected attributes and improve
empirically the quality of the resulting clustering. We also present the first
constant-factor approximation for an arbitrary number of protected attributes
thus settling an open problem recently addressed in several works.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13651"><span class="datestr">at June 03, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13492">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13492">Majorisation-minimisation algorithms for minimising the difference between lattice submodular functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McMeel:Conor.html">Conor McMeel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parpas:Panos.html">Panos Parpas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13492">PDF</a><br /><b>Abstract: </b>We consider the problem of minimising functions represented as a difference
of lattice submodular functions. We propose analogues to the SupSub, SubSup and
ModMod routines for lattice submodular functions. We show that our
majorisation-minimisation algorithms produce iterates that monotonically
decrease, and that we converge to a local minimum. We also extend additive
hardness results, and show that a broad range of functions can be expressed as
the difference of submodular functions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13492"><span class="datestr">at June 03, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13415">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13415">ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stehle:Elias.html">Elias Stehle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jacobsen:Hans=Arno.html">Hans-Arno Jacobsen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13415">PDF</a><br /><b>Abstract: </b>Parsing is essential for a wide range of use cases, such as stream
processing, bulk loading, and in-situ querying of raw data. Yet, the
compute-intense step often constitutes a major bottleneck in the data ingestion
pipeline, since parsing of inputs that require more involved parsing rules is
challenging to parallelise. This work proposes a massively parallel algorithm
for parsing delimiter-separated data formats on GPUs. Other than the
state-of-the-art, the proposed approach does not require an initial sequential
pass over the input to determine a thread's parsing context. That is, how a
thread, beginning somewhere in the middle of the input, should interpret a
certain symbol (e.g., whether to interpret a comma as a delimiter or as part of
a larger string enclosed in double-quotes). Instead of tailoring the approach
to a single format, we are able to perform a massively parallel FSM simulation,
which is more flexible and powerful, supporting more expressive parsing rules
with general applicability. Achieving a parsing rate of as much as 14.2 GB/s,
our experimental evaluation on a GPU with 3584 cores shows that the presented
approach is able to scale to thousands of cores and beyond. With an end-to-end
streaming approach, we are able to exploit the full-duplex capabilities of the
PCIe bus and hide latency from data transfers. Considering the end-to-end
performance, the algorithm parses 4.8 GB in as little as 0.44 seconds,
including data transfers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13415"><span class="datestr">at June 03, 2019 11:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13400">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13400">A Primer on Persistent Homology of Finite Metric Spaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Facundo Memoli, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singhal:Kritika.html">Kritika Singhal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13400">PDF</a><br /><b>Abstract: </b>TDA (topological data analysis) is a relatively new area of research related
to importing classical ideas from topology into the realm of data analysis.
Under the umbrella term TDA, there falls, in particular, the notion of
persistent homology, which can be described in a nutshell, as the study of
scale dependent homological invariants of datasets.
</p>
<p>In these notes, we provide a terse self contained description of the main
ideas behind the construction of persistent homology as an invariant feature of
datasets, and its stability to perturbations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13400"><span class="datestr">at June 03, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13371">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13371">A Weighted Linear Matroid Parity Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iwata:Satoru.html">Satoru Iwata</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yusuke.html">Yusuke Kobayashi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13371">PDF</a><br /><b>Abstract: </b>The matroid parity (or matroid matching) problem, introduced as a common
generalization of matching and matroid intersection problems, is so general
that it requires an exponential number of oracle calls. Nevertheless, Lov\'asz
(1980) showed that this problem admits a min-max formula and a polynomial
algorithm for linearly represented matroids. Since then efficient algorithms
have been developed for the linear matroid parity problem. In this paper, we
present a combinatorial, deterministic, polynomial-time algorithm for the
weighted linear matroid parity problem. The algorithm builds on a polynomial
matrix formulation using Pfaffian and adopts a primal-dual approach based on
the augmenting path algorithm of Gabow and Stallmann (1986) for the unweighted
problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13371"><span class="datestr">at June 03, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13283">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13283">Sum-of-squares meets square loss: Fast rates for agnostic tensor completion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foster:Dylan_J=.html">Dylan J. Foster</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Risteski:Andrej.html">Andrej Risteski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13283">PDF</a><br /><b>Abstract: </b>We study tensor completion in the agnostic setting. In the classical tensor
completion problem, we receive $n$ entries of an unknown rank-$r$ tensor and
wish to exactly complete the remaining entries. In agnostic tensor completion,
we make no assumption on the rank of the unknown tensor, but attempt to predict
unknown entries as well as the best rank-$r$ tensor.
</p>
<p>For agnostic learning of third-order tensors with the square loss, we give
the first polynomial time algorithm that obtains a "fast" (i.e., $O(1/n)$-type)
rate improving over the rate obtained by reduction to matrix completion. Our
prediction error rate to compete with the best $d\times{}d\times{}d$ tensor of
rank-$r$ is $\tilde{O}(r^{2}d^{3/2}/n)$. We also obtain an exact oracle
inequality that trades off estimation and approximation error.
</p>
<p>Our algorithm is based on the degree-six sum-of-squares relaxation of the
tensor nuclear norm. The key feature of our analysis is to show that a certain
characterization for the subgradient of the tensor nuclear norm can be encoded
in the sum-of-squares proof system. This unlocks the standard toolbox for
localization of empirical processes under the square loss, and allows us to
establish restricted eigenvalue-type guarantees for various tensor regression
models, with tensor completion as a special case. The new analysis of the
relaxation complements Barak and Moitra (2016), who gave slow rates for
agnostic tensor completion, and Potechin and Steurer (2017), who gave exact
recovery guarantees for the noiseless setting. Our techniques are
user-friendly, and we anticipate that they will find use elsewhere.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13283"><span class="datestr">at June 03, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13272">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13272">Parallel Algorithm for Non-Monotone DR-Submodular Maximization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ene:Alina.html">Alina Ene</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13272">PDF</a><br /><b>Abstract: </b>In this work, we give a new parallel algorithm for the problem of maximizing
a non-monotone diminishing returns submodular function subject to a cardinality
constraint. For any desired accuracy $\epsilon$, our algorithm achieves a $1/e
- \epsilon$ approximation using $O(\log{n} \log(1/\epsilon) / \epsilon^3)$
parallel rounds of function evaluations. The approximation guarantee nearly
matches the best approximation guarantee known for the problem in the
sequential setting and the number of parallel rounds is nearly-optimal for any
constant $\epsilon$. Previous algorithms achieve worse approximation guarantees
using $\Omega(\log^2{n})$ parallel rounds. Our experimental evaluation suggests
that our algorithm obtains solutions whose objective value nearly matches the
value obtained by the state of the art sequential algorithms, and it
outperforms previous parallel algorithms in number of parallel rounds,
iterations, and solution quality.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13272"><span class="datestr">at June 03, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13246">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13246">Largest Inscribed Rectangles in Geometric Convex Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behroozi:Mehdi.html">Mehdi Behroozi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13246">PDF</a><br /><b>Abstract: </b>We consider the problem of finding inscribed boxes and axis-aligned inscribed
boxes of maximum volume, inside a compact and solid convex set. Our algorithms
are capable of solving these two problems in any such set that can be
represented with finite number of convex inequalities. For the axis-aligned
case, we formulate the problem for higher dimensions and present an exact
optimization algorithm which solves the problem in $\mathcal{O}(d^3+d^2n)$
time, where $d$ is the dimension and $n$ is the number of inequalities defining
the convex set. For the general case, after formulating the problem for higher
dimensions we investigate the traditional 2-dimensional problem, which is in
the literature merely considered for convex polygons, for a broad range of
convex sets. We first present a new exact algorithm that finds the largest
inscribed axis-aligned rectangle in such convex sets for any given direction of
axes in $\mathcal{O}(n)$ time. Using this exact algorithm as a subroutine, we
present an $\epsilon$-approximation algorithm that computes
$(1-\epsilon)$-approximation to the largest inscribed rectangle with
computational complexity of $\mathcal{O}(\epsilon^{-1}n)$. Finally, we show
that how this running time can be improved to $\mathcal{O}(\epsilon^{-1}\log
n)$ with a $\mathcal{O}(\epsilon^{-1}n)$ pre-processing time when the convex
set is a polygon.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13246"><span class="datestr">at June 03, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13229">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13229">Private Hypothesis Selection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bun:Mark.html">Mark Bun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steinke:Thomas.html">Thomas Steinke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Zhiwei_Steven.html">Zhiwei Steven Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13229">PDF</a><br /><b>Abstract: </b>We provide a differentially private algorithm for hypothesis selection. Given
samples from an unknown probability distribution $P$ and a set of $m$
probability distributions $\mathcal{H}$, the goal is to output, in a
$\varepsilon$-differentially private manner, a distribution from $\mathcal{H}$
whose total variation distance to $P$ is comparable to that of the best such
distribution (which we denote by $\alpha$). The sample complexity of our basic
algorithm is $O\left(\frac{\log m}{\alpha^2} + \frac{\log m}{\alpha
\varepsilon}\right)$, representing a minimal cost for privacy when compared to
the non-private algorithm. We also can handle infinite hypothesis classes
$\mathcal{H}$ by relaxing to $(\varepsilon,\delta)$-differential privacy.
</p>
<p>We apply our hypothesis selection algorithm to give learning algorithms for a
number of natural distribution classes, including Gaussians, product
distributions, sums of independent random variables, piecewise polynomials, and
mixture classes. Our hypothesis selection procedure allows us to generically
convert a cover for a class to a learning algorithm, complementing known
learning lower bounds which are in terms of the size of the packing number of
the class. As the covering and packing numbers are often closely related, for
constant $\alpha$, our algorithms achieve the optimal sample complexity for
many classes of interest. Finally, we describe an application to private
distribution-free PAC learning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13229"><span class="datestr">at June 03, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4199">NP-complete Problems and Physics: A 2019 View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr />

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center (TACC), a leading supercomputing facility in North Austin that’s part of the University of Texas, is seeking to hire a Research Scientist focused on quantum computing.  Such a person would be a full participant in our <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin, with plenty of opportunities for collaboration.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting!</a></p>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4199"><span class="datestr">at June 02, 2019 01:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/082">TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/082"><span class="datestr">at June 02, 2019 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/081">TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/081"><span class="datestr">at June 02, 2019 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/080">TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/080"><span class="datestr">at June 01, 2019 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/079">TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/079"><span class="datestr">at June 01, 2019 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/078">TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/078"><span class="datestr">at June 01, 2019 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://plus.maths.org/content/democratic-dilemmas">No maths for Europe</a> (<a href="https://mathstodon.xyz/@11011110/102109693915830408"></a>). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) <a href="https://en.wikipedia.org/wiki/Highest_averages_method">formula for its apportionment</a> of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</p>
  </li>
  <li>
    <p>Prominent cryptographers <a href="https://en.wikipedia.org/wiki/Adi_Shamir">Adi Shamir</a> and <a href="https://en.wikipedia.org/wiki/Ross_J._Anderson">Ross J. Anderson</a> were both <a href="https://www.schneier.com/blog/archives/2019/05/why_are_cryptog.html">denied visas to travel to the US</a> for a conference and a book awards ceremony respectively (<a href="https://mathstodon.xyz/@11011110/102112360619663485"></a>, <a href="https://boingboing.net/2019/05/17/denying-cryptographers-problem.html">see also</a>). Bruce Schneier mentions “two other prominent cryptographers who are in the same boat”. Odd and troubling.</p>
  </li>
  <li>
    <p><a href="https://mathlesstraveled.com/2019/05/09/computing-the-euler-totient-function-part-1/">Three</a> <a href="https://mathlesstraveled.com/2019/05/18/computing-the-euler-totient-function-part-2-seeing-phi-is-multiplicative/">new</a> <a href="https://mathlesstraveled.com/2019/05/27/computing-the-euler-totient-function-part-3-proving-phi-is-multiplicative/">blog posts</a> by Brent Yorgey concern the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> (<a href="https://mathstodon.xyz/@11011110/102118180704402052"></a>). Computing it quickly would break RSA; Brent describes using factoring to do better than brute force. The problem is clearly in , and I think it may be a natural candidate for being -intermediate. Igor Pak (who asked me for -intermediate problems when I recently visited UCLA) <a href="https://cstheory.stackexchange.com/q/43954/95">thinks the prime-counting function may be another</a>, but neither function is very combinatorial. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">In a recent blog post I found a couple of combinatorial candidates</a>, but others would be interesting.</p>
  </li>
  <li>
    <p>The image below (<a href="https://commons.wikimedia.org/wiki/File:Gr%C3%BCnbaum-Rigby_configuration,_vector_graphics.svg">as redrawn by Brammers</a>) is the 
<a href="https://en.wikipedia.org/wiki/Gr%C3%BCnbaum%E2%80%93Rigby_configuration">Grünbaum–Rigby configuration</a> (<a href="https://mathstodon.xyz/@11011110/102119858635464298"></a>) with 21 points and lines, 4 points per line, and 4 lines per point. Klein studied it in the complex projective plane in 1879, but it wasn’t known to have this nice real heptagonal realization until Grünbaum and Rigby (1990). The new Wikipedia article on it was started by “Tomo” (whose real-world identity Wikipedia’s arcane outing rules bar me from disclosing, but he just turned 70, so if you figure it out wish him a happy birthday).</p>

    <p style="text-align: center;"><img width="60%" alt="The Grünbaum–Rigby configuration" src="https://11011110.github.io/blog/assets/2019/grunrig.svg" /></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Garden_of_Eden_(cellular_automaton)">Garden of Eden</a> (<a href="https://mathstodon.xyz/@11011110/102129199678568606"></a>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.pnas.org/content/early/2019/05/20/1902572116. Via https://mathstodon.xyz/@helger/102138884170343694">Ono et al prove that almost all Jensen-Pólya polynomials have only real roots</a> (<a href="https://mathstodon.xyz/@11011110/102143915068349382"></a>, <a href="https://mathstodon.xyz/@helger/102138884170343694">via</a>). The Riemann Hypothesis is equivalent to the statement that they all do. The same thing works for similar families of polynomials associated with partition functions and proves a conjecture of Chen. See also <a href="https://phys.org/news/2019-05-mathematicians-revive-abandoned-approach-riemann.html">a popularized account</a> and <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">Ono’s talk slides</a>.</p>
  </li>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/imu-abacus-medal/">The International Mathematical Union is renaming</a> its <a href="https://en.wikipedia.org/wiki/Nevanlinna_Prize">Nevanlinna Prize</a> to be the IMU Abacus Medal (<a href="https://mathstodon.xyz/@11011110/102149453984232922"></a>). The prize is given every four years for major accomplishments in theoretical computer science. The article doesn’t say why rename but it’s because Nevanlinna was a Nazi sympathizer and collaborator. The prize was named after him in the early 1980s because its funding came from Finland, but Nevanlinna also never had much to do with TCS.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Gasarch on proofreading</a> (<a href="https://mathstodon.xyz/@11011110/102154856271421940"></a>). Just as in programming, there’s always one more bug.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ellen_Fetter">Ellen Fetter</a> and <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)">Margaret Hamilton</a>: <a href="https://www.quantamagazine.org/hidden-heroines-of-chaos-ellen-fetter-and-margaret-hamilton-20190520/">Uncredited collaborators with Edward Lorenz at the birth of chaos theory</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/102163812229252010"></a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2019/05/17/science/math-physics-knitting-matsumoto.html">Elisabetta Matsumoto is studying the mathematics of knitting</a> (<a href="https://mathstodon.xyz/@11011110/102177647389031957"></a>, <a href="https://twitter.com/Sabetta_">via</a>), with the hope that it can lead to new programmable metamaterials.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/news/2019/05/ieee-major-science-publisher-bans-huawei-scientists-reviewing-papers">IEEE bans Huawei employees from reviewing submissions to its journals</a> (<a href="https://mathstodon.xyz/@11011110/102183137967208347"></a>, <a href="https://news.ycombinator.com/item?id=20046771">via</a>), saying it is forced to do so by US government sanctions.</p>
  </li>
  <li>
    <p>The UCI University Club (which in other places might be called a faculty club) is next door to the building I work in, and has a bustling side business hosting weddings. Here’s the view that greeted me as I left the office this evening, looking across their lawn towards the gazebo (<a href="https://mathstodon.xyz/@11011110/102188200347058355"></a>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/uclub/index.html"><img src="https://www.ics.uci.edu/~eppstein/pix/uclub/uclub-m.jpg" alt="UCI University Club lawn" style="border-style: solid; border-color: black;" /></a></p>
  </li>
  <li>
    <p>Line arrangements in architecture (<a href="https://mathstodon.xyz/@11011110/102193430755771327"></a>): the beams of <a href="https://en.wikipedia.org/wiki/Mathematical_Bridge">Cambridge’s Mathematical Bridge</a> form tangent lines to its arch and then extend through and support its trusswork, while another set of radial lines tie the structure together. The bridge just looks like a wood truss bridge in real life but <a href="https://commons.wikimedia.org/wiki/File:Mathematical_Bridge_tangents.jpg">this artificially-colored image</a> makes the underlying structure clearer.</p>

    <p style="text-align: center;"><img width="80%" alt="Cambridge's Mathematical Bridge" style="border-style: solid; border-color: black;" src="https://11011110.github.io/blog/assets/2019/cambridgebridge.jpg" /></p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/31/linkage.html"><span class="datestr">at May 31, 2019 09:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/077">TR19-077 |  Consistency of circuit lower bounds with bounded theories | 

	Jan Bydzovsky, 

	Jan  Krajicek, 

	Igor Carboni Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proving that there are problems in $P^{NP}$ that require boolean circuits of super-linear size is a major frontier in complexity theory. While such lower bounds are known for larger complexity classes, existing results only show that the corresponding problems are hard on infinitely many input lengths. For instance, proving almost-everywhere circuit lower bounds is open even for problems in MAEXP. Giving the notorious difficulty of proving lower bounds that hold for all large input lengths, we ask the following question: 

Can we show that a large set of techniques cannot prove that NP is easy infinitely often? 

Motivated by this and related questions about the interaction between mathematical proofs and computations, we investigate circuit complexity from the perspective of logic.

  Among other results, we prove that for any parameter $k \geq 1$ it is consistent with theory $T$ that computational class $C$ is not contained infinitely often in SIZE$(n^k)$, where $(T, C)$ is one of the pairs:

  $T = T^1_2\;$ and $\;C = P^{NP}$, $\quad T = S^1_2\;$ and $\;C = NP$, $\quad T =~ $PV$\;$ and $C = P$.

  In other words, these theories cannot establish infinitely often circuit upper bounds for the corresponding problems. This is of interest because the weaker theory PV already formalizes sophisticated arguments, such as a proof of the PCP Theorem (Pich, 2015). These consistency statements are unconditional and improve on earlier theorems of Krajicek and Oliveira (2017) and Bydzovsky and Muller (2018) on the consistency of lower bounds with PV.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/077"><span class="datestr">at May 30, 2019 10:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
