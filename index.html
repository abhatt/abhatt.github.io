<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://example.com/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.minimizingregret.com/" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" class="message" title="http status 503">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A> &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at January 02, 2019 02:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-5864407579793524954">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/01/thirty-five-years-of-testing.html">Thirty-five years of "Testing Equivalences for Processes" (TCS edition)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>Thirty-five years ago, <a href="https://www.imtlucca.it/rocco.denicola" target="_blank">Rocco De Nicola</a> and <a href="https://scholar.google.com/citations?user=wIto1LUAAAAJ&amp;hl=en" target="_blank">Matthew Hennessy</a> published the <a href="https://doi.org/10.1016/0304-3975(84)90113-0" target="_blank">archival paper </a>introducing the notions of testing equivalence over concurrent processes in the journal Theoretical Computer Science. These testing equivalences embody in a natural and mathematically elegant way the intuitive idea that two processes should be equated unless they behave differently when subjected to some ‘experiment’ or ‘test’. The origin of this notion of equivalence can be traced back to <a href="https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz" target="_blank">Gottfried Wilhelm Leibniz </a>(1646–1716), who stated that two mathematical objects are equal if there is no test to distinguish them. In the semantics of programming languages, its earliest precursor is, to the best of my  knowledge, the notion of contextual equivalence proposed by Morris in his doctoral dissertation.<br /><br />In general, given a set of processes, a set of tests and a relation between processes and tests that describes when a process passes a test, one can apply<br />Leibniz’s motto and declare two processes to be equivalent if if they pass exactly<br />the same set of tests. In the work of De Nicola and Hennessy, processes are states in some <a href="https://en.wikipedia.org/wiki/Transition_system" target="_blank">labelled transition system</a>. A test is itself a process, which interacts with a concurrent system under observation by hand-shake synchronisation and uses a distinguished action to report success in its observation. Since both processes and tests may be nondeterministic, the interaction between a process and a test may lead to different outcomes depending on how the two systems resolve their nondeterministic choices in the course of a computation. This led De Nicola and Hennessy to define three notions of testing semantics, which are naturally expressed in terms of preorders over processes.<br /><br />In the so-called may semantics, a process q is at least as good as some process p if the set of tests that p may pass is included in the set of tests that q may pass. In may semantics, possible failure under a test is immaterial and therefore nondeterminism is angelic. On the other hand, one may take the view that failure in the testing effort is catastrophic, in the sense that a process that may fail some test is just as bad as one that always fails it. The notion of testing semantics that captures this viewpoint is the so-called must semantics, according to which a process q is at  least as good as some process p if the set of tests that p must pass is included in the set of tests that q must pass. Finally, a third testing preorder over processes is obtained as the intersection of the may and must preorders described above. According to this more refined view of process behaviour, a process that always fails a test is worse than one that may pass that test, which in turn is worse than one that always passes it.<br /><br />De Nicola and Hennessy explored the rich theory of the testing semantics<br />in their seminal TCS paper (see <a href="https://mitpress.mit.edu/books/algebraic-theory-processes" target="_blank">this monograph</a> for a classic, book-length treatment), where each of these semantics is given operational, denotational and axiomatic accounts that are in agreement one with the other. Their ideas and the accompanying technical results have had an enormous impact on further research, as witnessed, among other things, by the 1,640 citations to the TCS paper published in 1984 (source: <a href="https://scholar.google.com/citations?user=Meb6JFkAAAAJ&amp;hl=en" target="_blank">Google Scholar </a>today).<br /><br />Happy 2019 to everyone.<br /><br /><br /><br /></div><div class="commentbar"><p></p><span href="http://processalgebra.blogspot.com/feeds/5864407579793524954/comments/default" class="commentbutton"></span><a href="http://processalgebra.blogspot.com/feeds/5864407579793524954/comments/default"><img src="/images/feed-icon.png" class="commenticon" /> Subscribe to comments</a>  | <a href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=5864407579793524954"><img src="/images/post-icon.png" class="commenticon" /> Post a comment</a></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/01/thirty-five-years-of-testing.html"><span class="datestr">at January 01, 2019 10:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4045">Incompleteness ex machina</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I have a treat with which to impress your friends at New Year’s Eve parties tomorrow night: a <a href="https://www.scottaaronson.com/incompleteness.pdf">rollicking essay</a> graciously contributed by a reader named Sebastian Oberhoff, about a unified and simplified way to prove all of Gödel’s Incompleteness Theorems, as well as Rosser’s Theorem, directly in terms of computer programs.  In particular, this improves over my treatments in <em>Quantum Computing Since Democritus</em> and my <a href="https://www.scottaaronson.com/blog/?p=710">Rosser’s Theorem via Turing machines</a> post.  While there won’t be anything new here for the experts, I loved the style—indeed, it brings back wistful memories of how <em>I</em> used to write, before I accumulated too many imaginary (and non-imaginary) readers tut-tutting at crass jokes over my shoulder.  May 2019 bring us all the time and the courage to express ourselves authentically, even in ways that might be sneered at as incomplete, inconsistent, or unsound.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4045"><span class="datestr">at December 31, 2018 02:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e2022ad3ca27b2200b">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/erickson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://3dpancakes.typepad.com/ernie/2018/12/steal-this-book-1.html">Steal This Book!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://3dpancakes.typepad.com/.a/6a00d83452383469e2022ad3aa81d3200d-popup" class="asset-img-link"><img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e2022ad3aa81d3200d-320wi" alt="BookCover" style="display: block; margin-left: auto; margin-right: auto;" class="asset  asset-image at-xid-6a00d83452383469e2022ad3aa81d3200d img-responsive" title="BookCover" /></a><br />Today I'm <em>finally</em> releasing a final (or more honestly, “final”) pre-publication draft of my <em>Algorithms</em> textbook under a CC-BY license. This 448-page textbook evolved out of a subset of the algorithms lecture notes I've been maintaining for about 20 years.</p>
<p>There are still a few more steps before this becomes an actual paper book—most notably an index—but I wanted to get this out the door this year. I expect to publish the actual paper book in a few weeks; it will also be licensed CC-BY.</p>
<p>Meanwhile, I've set up an issue-tracker on Github where anyone can report errors or provide other feedback.</p>
<p>The book site also includes copies of the lecture notes that I left out of the book (because I wanted a finite book in a finite amount of time), along with a complete archive of old homeworks, exams, lab handouts, and the like.</p>
<p>Enjoy!</p>
<ul>
<li>Official book site: <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/">http://jeffe.cs.illinois.edu/teaching/algorithms/</a></li>
<li>Mnemonic shortcut: <a href="http://algorithms.wtf">http://algorithms.wtf</a></li>
<li><strong>Please report errors:</strong> <a href="https://github.com/jeffgerickson/algorithms">https://github.com/jeffgerickson/algorithms</a></li>
<li>Archival copy: <a href="https://archive.org/details/Algorithms-Jeff-Erickson">https://archive.org/details/Algorithms-Jeff-Erickson</a></li>
</ul></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2018/12/steal-this-book-1.html"><span class="datestr">at December 29, 2018 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e2022ad3aa81e9200d">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/erickson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://3dpancakes.typepad.com/ernie/2018/12/steal-this-book.html">Steal This Book!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://3dpancakes.typepad.com/.a/6a00d83452383469e2022ad3aa81d3200d-popup" class="asset-img-link"><img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e2022ad3aa81d3200d-320wi" alt="BookCover" style="display: block; margin-left: auto; margin-right: auto;" class="asset  asset-image at-xid-6a00d83452383469e2022ad3aa81d3200d img-responsive" title="BookCover" /></a><br />Today I'm <em>finally</em> releasing a final (or more honestly, “final”) pre-publication draft of my <em>Algorithms</em> textbook under a CC-BY license. This 448-page textbook evolved out of a subset of the algorithms lecture notes I've been maintaining for about 20 years.</p>
<p>There are still a few more steps before this becomes an actual paper book—most notably an index—but I wanted to get this out the door this year. I expect to publish the actual paper book in a few weeks; it will also be licensed CC-BY.</p>
<p>Meanwhile, I've set up an issue-tracker on Github where anyone can report errors or provide other feedback.</p>
<p>The book site also includes copies of the lecture notes that I left out of the book (because I wanted a finite book in a finite amount of time), along with a complete archive of old homeworks, exams, lab handouts, and the like.</p>
<p>Enjoy!</p>
<ul>
<li>Official book site: <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/">http://jeffe.cs.illinois.edu/teaching/algorithms/</a></li>
<li>Mnemonic shortcut: <a href="http://algorithms.wtf">http://algorithms.wtf</a></li>
<li><strong>Please report errors:</strong> <a href="https://github.com/jeffgerickson/algorithms">https://github.com/jeffgerickson/algorithms</a></li>
<li>Archival copy: <a href="https://archive.org/details/Algorithms-Jeff-Erickson">https://archive.org/details/Algorithms-Jeff-Erickson</a></li>
</ul></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2018/12/steal-this-book.html"><span class="datestr">at December 29, 2018 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3371">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2018/12/28/sigecom-test-of-time-award/">SIGecom Test of Time Award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div>The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation.</div>
<div></div>
<p></p>
<div>To be eligible, a paper or series of papers must be on a topic in the intersection of economics and computation, including topics in electronic commerce, and must have been first published, in preliminary or final form, in an archival journal or conference proceedings no less than ten years and no more than twenty-five years before the year the award is conferred. Papers for which all authors are deceased at the time the Award Committee makes its decision are not eligible for the award.</div>
<div></div>
<p></p>
<div>The 2019 SIGecom Test of Time Award will be given for papers published no earlier than 1994 and no later than 2009. Nominations are due by February 20th, 2019, and must be made by email to the Award Committee (<a href="mailto:sigecom-awards-tot@acm.org" target="_blank" rel="noopener">sigecom-awards-tot@acm.org</a>) with “ACM SIGecom Test of Time Award” in the subject.</div>
<div></div>
<p></p>
<div>Any member of SIGecom may submit a nomination. Self-nomination is not allowed. Nominations must include the following, preferably in a single PDF file:</div>
<div></div>
<p></p>
<div>1. Bibliographic data for the paper or series of papers demonstrating publication, in preliminary or final form, at least ten years and at most twenty-five years before the award year.</div>
<div></div>
<p></p>
<div>2. An endorsement letter by the nominator of no more than two pages describing the content of the paper or series of papers and the lasting contribution, significance, and impact of the work.</div>
<div></div>
<p></p>
<div>3. The names, email addresses, and affiliations of at least two and at most three other endorsers. Endorsers, like the nominator, may not be authors of the paper or papers under consideration.</div>
<div></div>
<p></p>
<div>4. A one-sentence statement that describes the contribution of the paper or series of papers.</div>
<div></div>
<p></p>
<div>The additional endorsers should send letters directly to the Award Committee (<a href="mailto:sigecom-awards-tot@acm.org" target="_blank" rel="noopener">sigecom-awards-tot@acm.org</a>) by the same deadline. Each letter should specify the relationship of the endorser to nominees and describe, in 500 words or fewer, the lasting contribution, significance, and impact of the paper or papers.</div>
<div></div>
<p></p>
<div>An unsuccessful nomination can be reconsidered for three award cycles, with the option of updating the original nomination to reflect additional impact. Subsequently, a new nomination must be provided. All matters relating to the selection process that are not specified here are left to the discretion of the Award Committee.</div>
<div></div>
<p></p>
<div>The award, conferred annually at the ACM Conference on Economics and Computation, includes a plaque and complimentary conference registration for each winner and an honorarium of $1,000 to be shared among the winners. The award may not be given if the nominations are judged not to meet the standards of the award.</div>
<div></div>
<p></p>
<div>It is expected that at least one of the nominated authors, if selected for the award, will attend the next ACM Conference on Economics and Computation on June 24-28, 2019, in Phoenix, AZ, USA, to accept the award and give a presentation on the work. The award includes complimentary registration but does not cover travel expenses to attend the conference.</div>
<div></div>
<p></p>
<div>The Award Committee welcomes questions from anyone considering or intending to submit a nomination. The Award Committee is happy to provide feedback on informal proposals for potential nominees, should it be needed.</div>
<div></div>
<p></p>
<div>On behalf of the 2019 Award Committee:</div>
<div></div>
<p></p>
<div>Nikhil Devanur</div>
<div>Robert Kleinberg</div>
<div>Tim Roughgarden (Chair)</div>
<div><a href="mailto:sigecom-awards-tot@acm.org" target="_blank" rel="noopener">sigecom-awards-tot@acm.org</a></div></div>







<p class="date">
by timroughgarden <a href="https://agtb.wordpress.com/2018/12/28/sigecom-test-of-time-award/"><span class="datestr">at December 28, 2018 08:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2018/213">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2018/213">TR18-213 |  The Power of Distributed Verifiers in Interactive Proofs | 

	Eylon Yogev, 

	Moni Naor, 

	Merav Parter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://example.com/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We explore the power of interactive proofs with a distributed verifier. In this setting, the verifier consists of $n$ nodes and a graph $G$ that defines their communication pattern. The prover is a single entity that communicates with all nodes by short messages. The goal is to verify that the graph $G$ belongs to some language in a small number of rounds, and with small communication bound, i.e., the proof size.

This interactive model was introduced by Kol, Oshman and Saxena (PODC 2018) as a generalization of non-interactive distributed proofs. They demonstrated the power of interaction in this setting by constructing protocols for problems as Graph Symmetry and Graph Non-Isomorphism -- both of which require proofs of $\Omega(n^2)$-bits without interaction.

In this work, we provide a new general framework for distributed interactive proofs that allows one to translate standard interactive protocols (i.e., with a centralized verifier) to ones where the verifier is distributed with a proof size that depends on the computational complexity of the verification algorithm run by the centralized verifier.
We show the following:

* Every (centralized) computation that can be performed in time $O(n)$ can be translated into three-round distributed interactive protocol with $O(\log n)$ proof size. This implies that many graph problems for sparse graphs have succinct proofs (e.g., testing planarity).

* Every (centralized) computation implemented by either a small space or by uniform NC circuit can be translated into a distributed protocol with $O(1)$ rounds and $O(\log n)$ bits proof size for the low space case and $polylog(n)$ many rounds and proof size for NC.

* We also demonstrate the power of our compilers for problems not captured by the above families. We show that for Graph Non-Isomorphism, one of the striking demonstrations of the power of interaction, there is a 4-round protocol with $O(\log n)$ proof size, improving upon the $O(n \log n)$ proof size of Kol et al.

* For many problems we show how to reduce proof size below the naturally seeming barrier of $\log n$. By employing our RAM compiler, we get a 5-round protocols with proof size $O(\log \log n)$ for a family of problems including Fixed Automorphism, Clique and Leader Election (for the later two problems we actually get $O(1)$ proof size).

* Finally we discuss how to make these proofs non-interactive arguments via random oracles.

Our compilers capture many natural problems and demonstrates the difficultly in showing lower bounds in these regimes.<p></p></div></div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2018/213"><span class="datestr">at December 28, 2018 08:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15551">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2018/12/27/acm-great-results/">ACM Great Results</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>A Puck-ish take on promised technological advances</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/KnechtRuprecht.jpg"><img src="https://rjlipton.files.wordpress.com/2018/12/KnechtRuprecht.jpg?w=189&amp;h=189" alt="" height="189" class="alignright wp-image-15552" width="189" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Wikimedia Commons <a href="https://commons.wikimedia.org/wiki/File:Das_festliche_Jahr_img398_(Ruprecht).jpg">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Knecht Ruprecht accompanies Santa Claus in Germany. He brings gifts to good children but lumps of coal to naughty ones. He is regarded more generally as the German counterpart to England’s Robin Goodfellow, aka. <a href="https://en.wikipedia.org/wiki/Puck_(folklore)">Puck</a>. The Simpsons’ <a href="https://en.wikipedia.org/wiki/Santa's_Little_Helper">dog</a> “Santa’s Little Helper” is named “Knecht Ruprecht” in the show’s German edition.</p>
<p>
Today we do a nice-or-naughty riff on technological gifts suggested by yesterday’s ACM TechNews mailing.</p>
<p>
The ACM mailings highlight the achievements of the whole field: from quantum to everything else. We thought it might be fun to be a bit puckish ourselves and deliver some “coal” to ACM. The stories can be sometimes a bit much. We hope that all involved are in good spirits and accept the “coal” as a holiday-inspired gift—with some echo of the general discussion about naughty-or-nice effects of tech advances.</p>
<p>
</p><p></p><h2> Our Versions of the Stories </h2><p></p>
<p>
</p><p>
Here are some that could be reported in the near future. The originals are <a href="https://technews.acm.org/archives.cfm?fo=2018-12-dec/dec-26-2018.html">here</a>.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Real-Time Readouts of Thinking in Faculty</i>. <br />
Mighty News<br />
December 19, 2018<br />
Researchers from a university consortium have developed an open source system delivering fast, precise neural decoding and real-time readouts of where CS faculty think they are. The neural decoding software decrypts hippocampal spatiotemporal patterns detected from tetrode recordings without requiring spike sorting, an error-prone computational process. Implementing this software on a graphical processing unit (GPU) chip demonstrated a 20- to 50-fold upgrade in decoding and analysis speed over conventional multicore central processing unit (CPU) chips. This builds on work previous done on rats as reported by ACM previously. The lab director says that the CS faculty work presented many challenges beyond that required for rats. The applications—she says—are immense. Faculty currently cannot always tell where they are, and the new system could help them get to classes on time.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>A Robotic Hand Able To Type At Desktop Keyboard At 20 Words Per Minute</i>.<br />
New Yolk Times<br />
December 19, 2018 <br />
Researchers at Can’t-Abridge University have for the first time taught a robotic hand to type on a normal keyboard. The researchers claim that their system can type at rates in excess of 20 words per minute. They say, “this could change the way that computers interact with others.” The system, which now weighs about 500 pounds, could be reduced in size and cost in the future. That the robot sometimes destroys the keys by hitting them too hard continues to be a challenge.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>How AI Spotted Every Solar Panel in the U.S.</i><br />
Pretty Big Solar NewsHour<br />
December 19, 2018<br />
Engineers at the University of St. Anford have located every solar panel in the contiguous U.S. via a network built around a deep learning computer model called Inception. The network completed this task in less than a month, ascertaining that regions with more sun exposure had greater solar panel adoption than areas with less average sunlight. DeepSolar also learned that adoption was higher in locations of increasing average household income. Unbelievable—who would have guessed this?</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>An Amoeba Just Found an Entirely New Way to Write Articles</i>. <br />
ScienceAlarm <br />
December 21, 2018<br />
Researchers at Knockout University in Japan gave an assistant professorship to a “true slime mold” amoeba, and found as the papers-per-year target increased from four to eight, the single-celled organism only needed a linear amount of more time to generate minimum publishable units. This is part of an ongoing project on using lower-level organisms to do research. The project previously used graduate students. The leader of the multiple institution project said that using amoebae could reduce the costs of writing up research by up to 50%. He also said that the amoeba sometimes made various grammar errors, but that the project was attempting to fix this issue.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>A Quantum Computer Just Found an Entirely Old Way to Visit Cities</i>. <br />
ScienceAllure <br />
December 21, 2018<br />
Researchers at TKO University in Japan gave the Traveling Salesman Problem (TPS) to a vast array of noisy astronomical scale quantum (NASQ) processors, and found that as the cities increased from four to eight, the system only needed a linear amount of more time to determine a single reasonable route. This was fresh off its success at factoring numbers higher than 291,311 = 523*557 that it didn’t even <a href="https://en.wikipedia.org/wiki/Integer_factorization_records#Records_for_efforts_by_quantum_computers">know</a> it was factoring. TPS is an optimization problem requiring a computer to look at a list of cities and determine the shortest route in which each city is visited exactly once. The team said their results “may lead to the development of quantum algorithms for problems on as many as ten cities.” </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/SantasLittleHelperOrlando.jpg"><img src="https://rjlipton.files.wordpress.com/2018/12/SantasLittleHelperOrlando.jpg?w=300&amp;h=148" alt="" height="148" class="aligncenter size-medium wp-image-15555" width="300" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2">Modified from <a href="https://www.flickr.com/photos/jared422/11839818825">source</a><br />
</font>
</td>
</tr>
</tbody></table>
<p><img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <i>Programming Proteins to Pair Precisely</i>.<br />
C++ News<br />
December 19, 2018<br />
The <b>std::pair</b> construct in C++ is a common annoyance because human programmers frequently forget its implicit presence when iterating over maps or inserting into sets. This necessitates the re-typing of millions of lines of source code per annum. Absent the development of a robotic hand able to type at a desktop keyboard at 20 words per minute, software companies can improve productivity by optimizing the nutritional intake of programmers. Nanosoft has partnered with CodeURIKA to provide protein-rich drinks worldwide, after a study of electronic sweatshops found that proteins minimize both syntactic and semantic bugs better over the long term than sugars and PEDs. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <i>Room for Improvement? New Hotelier Tests an Algorithmic System</i>.<br />
Wallbanger Street Journal<br />
December 19, 2018<br />
The Lite House hotelier is experimenting with an algorithmic pricing system to set different room rates for guests who arrive in self-driving cars. Once customers book for the first time at a standard rate, they fill out a questionnaire of 200 questions to specify how often they will need the car, how frequently they visit the hotel bar, and other details. The hotelier then activates a key to drive the car into an appropriate space. The optimized use of vertical space and savings from not hiring car valets will enable conference participants who are not staying at the hotel to park there at a rate low enough to include in the conference registration fee. A spokesman said, “Most of the big hotel operating companies are not focused on their conference guests,” while Lite House’s algorithmic rate-setting “is next-generation.”</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <i>Companies Use VR to Train Employees for Difficult Customers</i>.<br />
ESPN Technology Review<br />
December 20, 2018.<br />
Major corporations like Wallstore, ChippedPot, and Horizon are using virtual reality (VR) to prepare employees for potentially difficult situations on the job. For example, Horizon has more than 1,600 stores in the U.S. whose front-line employees participate in a digital scenario in which a customer asks to use the bathroom. In a “Harry Potter-Style Photos for Muggles” twist, researchers have developed software that can animate the central character in a photograph while leaving the rest of the image untouched. Its skeleton can then be animated to create the sense of movement, solving the problem of pose estimation for a limited set of circumstances in which bathroom requests occur. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <i>New Attack Intercepts Keystrokes Via Digital Watches</i>.<br />
TubeNet<br />
December 19, 2018<br />
A team of researchers from Burning Man University has developed a new side-channel attack that exploits the heat generated by people wearing Orange Digital Watches while working on their PCs. Heat amplifies the watches’ ability to detect keystrokes from both hands. Videos known to generate large amounts of heat include comic videos and videos on carpet cleaning. The attack becomes more adept at guessing correct keys as the user gets hotter, as it amasses more key presses from graphic libraries. </p>
<p></p><p><br />
There are some other items, including one particularly chilling, that we chose not to parody.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Will the next year’s advances in AI and other areas of tech be anything like we imagine? Will they bring humanity more gifts than lumps of coal?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2018/12/27/acm-great-results/"><span class="datestr">at December 28, 2018 01:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4043">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4043">Announcements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I’m planning to be in Australia soon—in Melbourne January 4-10 for a friend’s wedding, then in Sydney January 10-11 to meet colleagues and give a talk.  It will be my first trip down under for 12 years (and Dana’s first ever).  If there’s interest, I might be able to do a <em>Shtetl-Optimized</em> meetup in Melbourne the evening of Friday the 4th (or the morning of Saturday the 5th), and/or another one in Sydney the evening of Thursday the 10th.  Email me if you’d go, and then we’ll figure out details.</p>



<p>The <a href="https://www.congress.gov/bill/115th-congress/house-bill/6227/text">National Quantum Initiative Act</a> is now law.  Seeing the photos of Trump signing it, I felt … well, whatever emotions you might imagine I felt.</p>



<p>Frank Verstraete asked me to announce that the University of Vienna is seeking a full professor in quantum algorithms; <a href="https://personalwesen.univie.ac.at/jobs-recruiting/professuren/detail-seite/news/quantum-algorithms-1/">see here</a> for details.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4043"><span class="datestr">at December 27, 2018 08:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2018/212">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2018/212">TR18-212 |  Constructing Faithful Homomorphisms over Fields of Finite Characteristic | 

	Prerona Chatterjee, 

	Ramprasad Saptharishi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://example.com/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the question of algebraic rank or transcendence degree preserving homomorphisms over finite fields. This concept was first introduced by Beecken, Mittmann and Saxena (Information and Computing, 2013), and exploited by them, and Agrawal, Saha, Saptharishi and Saxena (Journal of Computing, 2016) to design algebraic independence based identity tests using the Jacobian criterion over characteristic zero fields. An analogue of such constructions over finite characteristic fields was unknown due to the failure of the Jacobian criterion over finite characteristic fields.
Building on a recent criterion of Pandey, Sinhababu and Saxena (MFCS, 2016), we construct explicit faithful maps for some natural classes of polynomials in the positive characteristic field setting, when a certain parameter called the inseparable degree of the underlying polynomials is bounded (this parameter is always 1 in fields of characteristic zero). This presents the first generalisation of some of the results of Beecken et al. and Agrawal et al. in the positive characteristic setting.<p></p></div></div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2018/212"><span class="datestr">at December 26, 2018 02:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=16645">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2018/12/25/amazing-karim-adiprasito-proved-the-g-conjecture-for-spheres/">Amazing: Karim Adiprasito proved the g-conjecture for spheres!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.files.wordpress.com/2015/01/gilkarim.jpg"><img src="https://gilkalai.files.wordpress.com/2015/01/gilkarim.jpg?w=640&amp;h=853" alt="" height="853" class="alignnone size-full wp-image-12390" width="640" /></a></p>
<p style="text-align: center;"><span style="color: #ff0000;">Karim in his youth with a fan</span></p>
<p>Congratulations, Karim!</p>
<p><strong>Update</strong>: <a href="https://arxiv.org/abs/1812.10454">Here is the link to the paper</a></p>
<p><em>From the arXive, Dec 26, 2018. (Link will be added tomorrow.)</em></p>
<p>COMBINATORIAL LEFSCHETZ THEOREMS BEYOND POSITIVITY</p>
<p>by Karim Adiprasito</p>
<p><strong>Abstract:</strong> Consider a simplicial complex that allows for an embedding into <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb R^d" class="latex" title="\mathbb R^d" />. How many faces of dimension <img src="https://s0.wp.com/latex.php?latex=d%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d/2" class="latex" title="d/2" /> or higher can it have? How dense can they be?</p>
<p>This basic question goes back to Descartes. Using it and other fundamental combinatorial<br />
problems, we will introduce a version of the Kähler package beyond positivity,<br />
allowing us to prove the Lefschetz theorem for toric varieties even when the ample<br />
cone is empty. A particular focus lies on replacing the Hodge-Riemann relations by a<br />
non-degeneracy relation at torus-invariant subspaces, allowing us to state and prove a<br />
generalization of the theorems of Hall and Laman in the setting of toric varieties. Of<br />
the many applications, we highlight two main applications, one because it is the most<br />
well-known, the other because it provided the most guiding light.</p>
<p>(1) We fully characterize the possible face numbers of simplicial spheres, resolving the<br />
so called <em>g</em>-conjecture of McMullen in full generality and generalizing Stanley’s<br />
earlier proof for simplicial polytopes.</p>
<p>(2) We prove that for a simplicial complex <em>K</em> that embeds into <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5E%7B2d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb R^{2d}" class="latex" title="\mathbb R^{2d}" />, the number of <em>d</em>-dimensional simplices exceeds the number of <em>(d − 1)</em>-dimensional simplices by a factor of at most <em>d + 2</em>. This generalizes a result of Descartes, and resolves the Grünbaum-Kalai-Sarkaria conjecture.</p>
<p>_______</p>
<p>(GK:) A few further comments. Probably the <em>g</em>-conjecture for spheres is the single problem I knock my head against the most. It is great to see it settled and it is even greater to see it settled by my friend and colleague Karim Adiprasito.</p>
<p>To the three ingredients of the standard conjectures (See also the <a href="https://gilkalai.wordpress.com/2018/12/24/icm-2018-rio-4-huh-balog-morris-wormald/">previous post</a>), Poincare duality <strong>(PD</strong>), Hard Lefschetz (<strong>HL</strong>) and Hodge-Riemann (<strong>HR</strong>), Karim adds the <strong>Hall-Laman relations</strong>. Very roughly, the Hall-Laman relations  substitute<strong> (HR)</strong> and apply genericity (rather than definiteness) toward <strong>(HL)</strong>.</p>
<p>(We still need a good acronym for Hall-Laman, maybe <strong>(AHL)</strong>.)</p>
<p>One very nice feature of Karim’s proof is that <strong>vertex decomposable</strong> spheres play a special role in the path toward the proof. Those were introduced by Provan and Billera in the context of the Hirsch conjecture.</p>
<p>We have devoted <a href="https://gilkalai.wordpress.com/tag/g-conjecture/">plenty of posts</a> to the <em>g</em>-conjecture for spheres, and mentioned it in <a href="https://gilkalai.wordpress.com/page/2/?s=g-conjecture">even more posts</a>.  For an introduction to the conjecture see <a href="https://gilkalai.wordpress.com/2009/04/02/eran-nevo-the-g-conjecture-i/">Eran Nevo introductory post</a>, and the post <a href="https://gilkalai.wordpress.com/2009/04/04/how-the-g-conjecture-came-about/" rel="bookmark">How the g-Conjecture Came About</a>. There is also plenty left to be done <a href="https://gilkalai.wordpress.com/2018/06/20/beyond-the-g-conjecture-algebraic-combinatorics-of-cellular-spaces-i/">beyond the g-conjecture</a>.</p>
<p><span style="color: #0000ff;">Merry X-mas and Happy new year 2019 to all our readers.</span></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2018/12/25/amazing-karim-adiprasito-proved-the-g-conjecture-for-spheres/"><span class="datestr">at December 25, 2018 02:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=16429">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2018/12/24/icm-2018-rio-4-huh-balog-morris-wormald/">ICM 2018 Rio (4): Huh; Balog &amp; Morris; Wormald</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<p>This is my fourth report from ICM2018. (I plan one more.)  As I already mentioned, Combinatorics  was very nicely represented at ICM2018.  The combinatorics session itself was great, and there were quite a few other sessions and other lectures related to combinatorics. I also met quite a few combinatorialists. As I mentioned in my <a href="https://gilkalai.wordpress.com/2012/11/17/a-few-mathematical-snapshots-from-india-icm2010/">ICM 2010 post</a>, one thing that I enjoyed was to unexpectedly meet some old friends and this also happened in Rio (maybe a little less compared to Hyderabad as I learned to expect the unexpected). I also had an irrational expectation to unexpectedly meet the <em>same</em> people that I met unexpectedly in India. It was a pleasure meeting  Tadeusz Januszkiewicz again   but I was irrationally disappointed not to bump again into <a href="http://www-ma4.upc.edu/~oserra">Oriol Serra</a> and Anna Llado whom I had met  by surprise in Hyderabad.</p>
<p>This post will be about the Monday afternoon Session in combinatorics. Let me mention that the <a href="https://www.youtube.com/channel/UCnMLdlOoLICBNcEzjMLOc7w">ICM 2018 You Tube channel</a> now contains high quality videos for plenary and invited talks (as well as discussion panels, public lectures, and various other activities). This is a valuable resource! Here is the <a href="https://www.youtube.com/playlist?list=PLOrjUJw2wOmVE7DUBxr4CFu4TNhiJM8Hj">combinatorics session playlist</a>, and the <a href="https://www.youtube.com/playlist?list=PLOrjUJw2wOmWQ9pIGF1ObG4Ag472sg2hm">CS session</a>, and the <a href="https://www.youtube.com/playlist?list=PLOrjUJw2wOmXn3FrOaMN7ZVNqsY_fWDHw">probability and statistics</a> session, and the <a href="https://www.youtube.com/playlist?list=PLOrjUJw2wOmW5F1S9OGR6esa9XpTOTq6e">plenary lectures</a>, and the <a href="https://www.youtube.com/playlist?list=PLOrjUJw2wOmWTsHKdFtIP7H2zsvwI0Uq4">public lectures</a>. Also, here is the most recent version of my ICM paper <a href="https://gilkalai.files.wordpress.com/2018/12/icm-draft-Dec-2018.pdf">THREE PUZZLES ON MATHEMATICS, COMPUTATION, AND GAMES</a>. Last minute corrections and comments are most welcome.</p>
<h1>Monday’s afternoon combinatorics</h1>
<p>The Monday afternoon combinatorics session featured three lectures that knocked my socks off. The talks were great and I was in a perfect position to enjoy them as I knew something about the problems and some related results  and yet each lecture surprised me.  The three talks were <span id="eow-title" class="watch-title" dir="ltr" title="Combinatorial applications of the Hodge–Riemann relations – June Huh – ICM2018"><a href="https://youtu.be/ceGEZdjnxRw">Combinatorial applications of the Hodge–Riemann relations</a> </span>by June Huh, <span class="watch-title" dir="ltr" title="The method of hypergraph containers – József Balogh &amp; Robert Morris – ICM2018"><a href="https://www.youtube.com/watch?v=y1zH5Hq24OA">The method of hypergraph containers</a> by József Balogh &amp; Robert Morris,  </span><span id="eow-title" class="watch-title" dir="ltr" title="Asymptotic enumeration of graphs with given degree sequence – Nicholas Wormald – ICM2018"><a href="https://www.youtube.com/watch?v=fNisXEdZhlQ">Asymptotic enumeration of graphs with given degree sequence</a> by Nicholas Wormald. Bella Bollobas chaired the session and gave a very nice and thoughtful introduction to each of the four speakers.</span></p>
<p><span id="eow-title" class="watch-title" dir="ltr" title="Asymptotic enumeration of graphs with given degree sequence – Nicholas Wormald – ICM2018"> </span></p>
<h2>June Huh, and the Lefschetz package in combinatorics</h2>
<p></p>
<blockquote><p><strong><span style="color: #ff0000;">June Huh: The standard conjectures are both ubiquitous and fundamental</span></strong></p></blockquote>
<p class="watch-title-container"><a href="https://youtu.be/ceGEZdjnxRw"><span id="eow-title" class="watch-title" dir="ltr" title="Combinatorial applications of the Hodge–Riemann relations – June Huh – ICM2018">Combinatorial applications of the Hodge–Riemann relations</span></a></p>
<p>June Huh talked about a mysterious package of conjectures (PD), (HL) and (HR), referred to as the standard conjectures,  for certain algebras associated with geometric and combinatorial objects. PD stands for the Poincare Duality, and it asserts that certain vector spaces <img src="https://s0.wp.com/latex.php?latex=A_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_i" class="latex" title="A_i" /> and <img src="https://s0.wp.com/latex.php?latex=A_%7Bd-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{d-i}" class="latex" title="A_{d-i}" /> are dual. HD stands for Hard Lefschetz and it asserts that certain linear maps <img src="https://s0.wp.com/latex.php?latex=%5Cphi_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi_k" class="latex" title="\phi_k" /> from <img src="https://s0.wp.com/latex.php?latex=A_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_k" class="latex" title="A_k" /> to <img src="https://s0.wp.com/latex.php?latex=A_k%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_k+1" class="latex" title="A_k+1" />  have the property that their composition from <img src="https://s0.wp.com/latex.php?latex=A_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_i" class="latex" title="A_i" /> all the way to <img src="https://s0.wp.com/latex.php?latex=A_%7Bd-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{d-i}" class="latex" title="A_{d-i}" /> is an injection. (HR) stands for Hodge Riemann relations. (PD) and (HD) imply that a certain bilinear form  is nondegenerate and (HR) is a stronger statement that this form is definite!</p>
<p>June started with some startling applications of the Hard-Lefschetz theorem in combinatorics pioneered by Stanley. He then mentioned a startling new application with Wang: Consider <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> points spanning a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-dimensional space.  Let <img src="https://s0.wp.com/latex.php?latex=w_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_i" class="latex" title="w_i" /> be the number of flats of dimension <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> spanned by the point.  Motzkin  conjectured in 1936 and proved over the reals that  <img src="https://s0.wp.com/latex.php?latex=w_1+%5Cle+w_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_1 \le w_d" class="latex" title="w_1 \le w_d" />. The planar case follows from the classic Erdos deBruijn theorem. Hu and Wang used {HL} to prove <img src="https://s0.wp.com/latex.php?latex=w_i+%5Cle+w_d-i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_i \le w_d-i" class="latex" title="w_i \le w_d-i" />, <img src="https://s0.wp.com/latex.php?latex=i+%5Cle+%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \le [d/2]" class="latex" title="i \le [d/2]" /> which was conjectured by Dowling and Wilson.</p>
<p>Next came applications of (HR), starting with Huh’s proof of the log concavity of coefficients of chromatic polynomials for graphs (Read conjecture ) and the far-reaching extension by Adiprasito-Huh-Kats to general matroids (Rota’s conjecture). We mentioned the Adiprasito-Huh-Katz solution of the Rota-Heron conjecture in <a href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/">the previous post</a> and in <a href="https://gilkalai.wordpress.com/2015/08/14/updates-and-plans-iii/">this one</a>.</p>
<p>Here is the link to the ICM paper by June Huh: <a href="https://arxiv.org/abs/1711.11176">Combinatorial applications of the Hodge-Riemann relations</a>.</p>
<p> </p>
<h2>József Balogh and Rob Morris and the container method</h2>
<p></p>
<p><span class="watch-title" dir="ltr" title="The method of hypergraph containers – József Balogh &amp; Robert Morris – ICM2018"><a href="https://www.youtube.com/watch?v=y1zH5Hq24OA">The method of hypergraph containers</a> </span></p>
<p>The container theorem for hypergraphs is one of the most important tools in extremal combinatorics with many applications also to random graphs and hypergraphs, additive combinatorics, discrete geometry, and more.</p>
<p>Rob Morris explained the container theorem for triangle-free graphs. It asserts that there is a collection <img src="https://s0.wp.com/latex.php?latex=%5Ccal+C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\cal C" class="latex" title="\cal C" /> of graphs on <img src="https://s0.wp.com/latex.php?latex=n+vertices&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n vertices" class="latex" title="n vertices" /> with the following three properties:</p>
<p>(1) Every graph in the collection contains <img src="https://s0.wp.com/latex.php?latex=o%28n%5E3%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="o(n^3)" class="latex" title="o(n^3)" /> triangles,</p>
<p>(2) The number of graphs in the collection is <img src="https://s0.wp.com/latex.php?latex=n%5E%7BC+%5Ccdot+3%2F2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n^{C \cdot 3/2}" class="latex" title="n^{C \cdot 3/2}" />,</p>
<p>(3) Each triangle free graph is contained in a graph in the collection.</p>
<p>Rob explained the origins of this theorem, how it follows from a container theorem for 3-uniform hypergraphs,   and how the later extends to the very general and important container theorem for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-uniform hypergraphs that was achieved in 2012 independently by Saxton and Thomason (Here is the link to <a href="https://arxiv.org/abs/1204.6595">their paper</a>), and by Balogh, Morris, and Samotij (Here is a link to <a href="https://arxiv.org/abs/1204.6530">their paper</a>).</p>
<p>Jozsef Balogh described two consequences of the container theorem to additive combinatorics and to discrete geometry. Let me describe the result in discrete geometry by Balogh and Solymosi. The (4,3) problem ask for the size $\alpha (n)$ of the largest subset of points in general position (no three on a line) that can always be found in a planar configuration of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> points with the property that no four points lie on a line. The container method is used to show (surprisingly!) that <img src="https://s0.wp.com/latex.php?latex=%5Calpha%28n%29%3Dn%5E%7B5%2F6%2Bo%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha(n)=n^{5/6+o(1)}" class="latex" title="\alpha(n)=n^{5/6+o(1)}" /> .</p>
<p>For a recent beautiful application to <img src="https://s0.wp.com/latex.php?latex=%28p%2Cq%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(p,q)" class="latex" title="(p,q)" />-Helly type theorems see <a href="https://arxiv.org/abs/1809.06451">A new lower bound on Hadwiger-Debrunner numbers in the plane</a> by Chaya Keller and Shakhar Smorodinsky.</p>
<p>Here is a link to the ICM survey paper: <a href="https://arxiv.org/abs/1801.04584">The method of hypergraph containers</a>, by József Balogh, Robert Morris, and Wojciech Samotij</p>
<p>(In a previous post  <a href="https://gilkalai.wordpress.com/2015/01/20/midrasha-mathematicae-18-in-and-around-combinatorics/" rel="bookmark">Midrasha Mathematicae #18: In And Around Combinatorics, </a>we gave links to a series of lectures Wojiech Samotij: Toward the hypergraph “container” theorem (4 lectures) <a href="https://www.youtube.com/watch?v=SpAyBN4rccU">Video 1, </a><a href="http://youtu.be/N6rP1yUcE0M">video 2</a> <a href="https://www.youtube.com/watch?v=cSFfKhcyN14">video 3</a> <a href="https://www.youtube.com/watch?v=efVlsmiws-I">video 4</a>.)</p>
<h2>Nick Wormald and counting regular graphs.</h2>
<p></p>
<p><span id="eow-title" class="watch-title" dir="ltr" title="Asymptotic enumeration of graphs with given degree sequence – Nicholas Wormald – ICM2018"><a href="https://www.youtube.com/watch?v=fNisXEdZhlQ">Asymptotic enumeration of graphs with given degree sequence</a></span></p>
<p>How many <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-regular graphs are there? This is a very central problem in combinatorics and Nick Wormald was quite interested in its solution ever since his Ph. D.  The talk describes the early history of the problem, the early works by Wormald and McKay from the 90s,  the recent breakthrough by Antia Liebenau and Nick Wormald,  the techniques involved in the old and new proofs and some related results.</p>
<p>A good place to start is with Read’s 1958 formula for the number <img src="https://s0.wp.com/latex.php?latex=g_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_3(n)" class="latex" title="g_3(n)" /> of 3-regular graphs with n labelled vertices</p>
<p><img src="https://s0.wp.com/latex.php?latex=g_3%28n%29+%5Csim+%283n%29%21+e%5E%7B-2%7D%2F%283n%2F2%29%21288%5E%7Bn%2F2%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_3(n) \sim (3n)! e^{-2}/(3n/2)!288^{n/2}." class="latex" title="g_3(n) \sim (3n)! e^{-2}/(3n/2)!288^{n/2}." /></p>
<p>Following an important model of Bollobas for creating regular graphs, general formulas were developed for low degrees, By McKay, McKay and Wormald, and others that depend on the probability of a random graph in Bollobas’ model to be simple. (See pictures below). Some results were proven also for the high degree regime and McKay and Wormald gave in 1990 and 1997 unified conjectural formulas for the number of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-regular graphs for a wide range of parameters. Moreover these conjectures extend to a large range of vectors of degree sequences.</p>
<p>In 2017 Anita Liebenau and Nick Wormald proved all these conjectures!!! (<a href="https://arxiv.org/abs/1702.08373">Here is a link to the paper</a>.)</p>
<p>The formula for the behavior of the number of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-regular graphs with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> vertices is remarkably elegant</p>
<p><img src="https://s0.wp.com/latex.php?latex=e%5E%7B1%2F4%7D%5Csqrt%7B2%7Dd%5Ed%28n-1-d%29%5E%7Bn-1-d%7D%28n-1%29%5E%7B-%28n-1%29%7D%7B%7Bn-1%7D+%5Cchoose+%7Bd%7D%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e^{1/4}\sqrt{2}d^d(n-1-d)^{n-1-d}(n-1)^{-(n-1)}{{n-1} \choose {d}}^n" class="latex" title="e^{1/4}\sqrt{2}d^d(n-1-d)^{n-1-d}(n-1)^{-(n-1)}{{n-1} \choose {d}}^n" />.</p>
<p>The full result is very general, and the method extends further in various directions.</p>
<p>Here is the link to paper: <a href="https://arxiv.org/abs/1702.08373">Asymptotic enumeration of graphs by degree sequence, and the degree sequence of a random graph</a>, by Anita Liebenau and Nick Wormald.</p>
<h3>A bit psychedelic pictures</h3>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/IMG_2149.jpg"><img src="https://gilkalai.files.wordpress.com/2018/12/IMG_2149.jpg?w=300&amp;h=225" alt="" height="225" class="alignnone size-medium wp-image-16681" width="300" /></a>    <a href="https://gilkalai.files.wordpress.com/2018/12/IMG_2150.jpg"><img src="https://gilkalai.files.wordpress.com/2018/12/IMG_2150.jpg?w=300&amp;h=225" alt="" height="225" class="alignnone size-medium wp-image-16682" width="300" /></a></p>
<p>With Nick Wormald and Yoshi Kohayakawa just before my lecture.</p>
<h2>Some important pictures from the Session</h2>
<h3>Bela Bollobas</h3>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/bela.png"><img src="https://gilkalai.files.wordpress.com/2018/12/bela.png?w=640" alt="" class="alignnone size-full wp-image-16650" /></a></p>
<p><span style="color: #ff0000;">Bela Bollobas served as the session chair</span></p>
<h3>Nick Wormald on enumeration of regular graphs</h3>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W2.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W2.png?w=640&amp;h=406" alt="" height="406" class="alignnone size-full wp-image-16660" width="640" /></a></p>
<p><span style="color: #ff0000;">Read’s formula and Bollobas model.</span></p>
<p><img src="https://gilkalai.files.wordpress.com/2018/12/W3.png?w=640&amp;h=368" alt="" height="368" class="alignnone size-full wp-image-16661" width="640" /></p>
<p><span style="color: #ff0000;">Formulas by McKay and McKay-Wormald (above and below)</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W4.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W4.png?w=640&amp;h=352" alt="" height="352" class="alignnone size-full wp-image-16662" width="640" /></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W5.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W5.png?w=640&amp;h=370" alt="" height="370" class="alignnone size-full wp-image-16663" width="640" /></a></p>
<p><span style="color: #ff0000;">General conjectures (above and below)</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W6.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W6.png?w=640&amp;h=365" alt="" height="365" class="alignnone size-full wp-image-16664" width="640" /></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W7.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W7.png?w=640&amp;h=353" alt="" height="353" class="alignnone size-full wp-image-16665" width="640" /></a></p>
<p><span style="color: #ff0000;">The Theorem by Liebenau and Wormald (above and below)</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/W8.png"><img src="https://gilkalai.files.wordpress.com/2018/12/W8.png?w=640&amp;h=356" alt="" height="356" class="alignnone size-full wp-image-16666" width="640" /></a></p>
<p> </p>
<h3>Balogh and Morris on containers</h3>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/containers1.png"><img src="https://gilkalai.files.wordpress.com/2018/12/containers1.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16614" width="640" /></a></p>
<p><span style="color: #ff0000;">The Container theorem for triangle-free graphs</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/containers2.png"><img src="https://gilkalai.files.wordpress.com/2018/12/containers2.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16615" width="640" /></a></p>
<p><span style="color: #ff0000;">The hypergraph container theorem for 3-uniform hypergraphs</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/container3.png"><img src="https://gilkalai.files.wordpress.com/2018/12/container3.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16616" width="640" /></a></p>
<p><span style="color: #ff0000;">The hypergraph container theorem in full generality.</span></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/container4.png"><img src="https://gilkalai.files.wordpress.com/2018/12/container4.png?w=640&amp;h=371" alt="" height="371" class="alignnone size-full wp-image-16653" width="640" /></a></p>
<p><span style="color: #ff0000;">An application for the number of subsets of integers without k-term arithmetic progressions.</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/containers5.png"><img src="https://gilkalai.files.wordpress.com/2018/12/containers5.png?w=640&amp;h=348" alt="" height="348" class="alignnone size-full wp-image-16654" width="640" /></a></p>
<p><span style="color: #ff0000;">What was known and expected on the (4,3) problem (above) and the new breakthrough (below)</span></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/containers6.png"><img src="https://gilkalai.files.wordpress.com/2018/12/containers6.png?w=640&amp;h=368" alt="" height="368" class="alignnone size-full wp-image-16655" width="640" /></a></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/containers7.png"><img src="https://gilkalai.files.wordpress.com/2018/12/containers7.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16656" width="640" /></a></p>
<p><span style="color: #ff0000;">Applications of the container method</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/container8.png"><img src="https://gilkalai.files.wordpress.com/2018/12/container8.png?w=640&amp;h=332" alt="" height="332" class="alignnone size-full wp-image-16690" width="640" /></a></p>
<h3>June Huh on the standard conjectures</h3>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh1.png"><img src="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh1.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16607" width="640" /></a></p>
<p><span style="color: #ff0000;">Five seemingly unrelated mathematical objects</span></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh3.png"><img src="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh3.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16609" width="640" /></a></p>
<p><span style="color: #ff0000;">Poincare duality (PD), Hard Lefschetz (HL), and Hodge Riemann (HR).</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/icm-huh5.png"><img src="https://gilkalai.files.wordpress.com/2018/12/icm-huh5.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16610" width="640" /></a></p>
<p><span style="color: #ff0000;">A 1964 letter from Serre to Grothendieck on young Bombieri</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/icm-huh6.png"><img src="https://gilkalai.files.wordpress.com/2018/12/icm-huh6.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16611" width="640" /></a></p>
<p><span style="color: #ff0000;">The algebraic setting for the standard conjectures. </span></p>
<p><a href="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh9.png"><img src="https://gilkalai.files.wordpress.com/2018/12/icm2018-huh9.png?w=640&amp;h=360" alt="" height="360" class="alignnone size-full wp-image-16612" width="640" /></a></p>
<p><span style="color: #ff0000;">Five cases were the standard conjectures are known and the original open case.</span></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2018/12/24/icm-2018-rio-4-huh-balog-morris-wormald/"><span class="datestr">at December 24, 2018 08:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/23/introduction-to-quantum-walks/">Introduction to Quantum Walks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<hr class="wp-block-separator" />



<p>author: Beatrice Nash</p>



<p>Abstract</p>



<p>In this blog post, we give a broad overview of quantum walks and some quantum walks-based algorithms, including traversal of the glued trees graph, search, and element distinctness [3; 7; 1]. Quantum walks can be viewed as a model for quantum computation, providing an advantage over classical and other non-quantum walks based algorithms for certain applications.</p>



<h1>Continuous time quantum walks</h1>



<p>We begin our discussion of quantum walks by introducing the quantum analog of the continuous random walk. First, we review the behavior of the classical continuous random walk in order to develop the definition of the continuous quantum walk.</p>



<p>Take a graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> with vertices <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V" class="latex" title="V" /> and edges <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E" class="latex" title="E" />. The adjacency matrix <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> of <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is defined as follows:</p>



<p><img src="https://s0.wp.com/latex.php?latex=A_%7Bi%2Cj%7D+%3D+%5Cbegin%7Bcases%7D+1+%5Cquad+%26%5Ctext%7Bif+++%7D+%28i%2Cj%29+%5Cin+E+%5C%5C+0+%5Cquad+%26%5Ctext%7Botherwise%7D.+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_{i,j} = \begin{cases} 1 \quad &amp;\text{if   } (i,j) \in E \\ 0 \quad &amp;\text{otherwise}. \end{cases}" class="latex" title="A_{i,j} = \begin{cases} 1 \quad &amp;\text{if   } (i,j) \in E \\ 0 \quad &amp;\text{otherwise}. \end{cases}" /></p>



<p>And the Laplacian <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" /> is given by:</p>



<p><img src="https://s0.wp.com/latex.php?latex=L_%7Bi%2Cj%7D+%3D+%5Cbegin%7Bcases%7D+-%5Ctext%7Bdegree%7D%28i%29+%5Cquad+%26%5Ctext%7Bif+++%7D++i+%3D+j+%5C%5C+1+%5Cquad+%26%5Ctext%7Bif+++%7D+%28i%2Cj%29+%5Cin+E+%5C%5C+0+%5Cquad+%26%5Ctext%7Botherwise%7D.+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{i,j} = \begin{cases} -\text{degree}(i) \quad &amp;\text{if   }  i = j \\ 1 \quad &amp;\text{if   } (i,j) \in E \\ 0 \quad &amp;\text{otherwise}. \end{cases}" class="latex" title="L_{i,j} = \begin{cases} -\text{degree}(i) \quad &amp;\text{if   }  i = j \\ 1 \quad &amp;\text{if   } (i,j) \in E \\ 0 \quad &amp;\text{otherwise}. \end{cases}" /></p>



<p>The Laplacian determines the behavior of the classical continuous random walk, which is described by a length <img src="https://s0.wp.com/latex.php?latex=%7CV%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|V|" class="latex" title="|V|" /> vector of probabilities, <strong>p</strong>(t). The <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th entry of <strong>p</strong>(t) represents the probability of being at vertex <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />. <strong>p</strong>(t) is given by the following differential equation:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Ctext%7Bd%7D%7D%7B%5Ctext%7Bdt%7D%7D+%5Ctext%7Bp%7D_%7Bi%7D%28%5Ctext%7Bt%7D%29+%3D+%5Cunderset%7B%28i%2Cj%29+%5Cin+E%7D%7B%5Csum%7D+L_%7Bi%2Cj%7D+%5Ctext%7Bp%7D_%7Bj%7D%28%5Ctext%7Bt%7D%29%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \frac{\text{d}}{\text{dt}} \text{p}_{i}(\text{t}) = \underset{(i,j) \in E}{\sum} L_{i,j} \text{p}_{j}(\text{t}),\end{aligned}" class="latex" title="\begin{aligned} \frac{\text{d}}{\text{dt}} \text{p}_{i}(\text{t}) = \underset{(i,j) \in E}{\sum} L_{i,j} \text{p}_{j}(\text{t}),\end{aligned}" /></p>



<p>which gives the solution <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7Bp%7D%28t%29+%3D+e%5E%7BLt%7D%5Ctextbf%7Bp%7D%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\textbf{p}(t) = e^{Lt}\textbf{p}(0)" class="latex" title="\textbf{p}(t) = e^{Lt}\textbf{p}(0)" />.</p>



<p>Recalling the Schrödinger equation <img src="https://s0.wp.com/latex.php?latex=i+%5Cfrac%7B%5Ctext%7Bd%7D%7D%7B%5Ctext%7Bdt%7D%7D+%5Cleft%7C%5Cpsi%5Cright%3E%3D+H+%5Cleft%7C%5Cpsi%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \frac{\text{d}}{\text{dt}} \left|\psi\right&gt;= H \left|\psi\right&gt;" class="latex" title="i \frac{\text{d}}{\text{dt}} \left|\psi\right&gt;= H \left|\psi\right&gt;" />, one can see that by inserting a factor of <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> on the left hand side of the equation for <strong>p</strong>(t) above, the Laplacian can be treated as a Hamiltonian. One can see that the Laplacian preserves the normalization of the state of the system. Then, the solution to the differential equation:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+i+%5Cfrac%7B%5Ctext%7Bd%7D%7D%7B%5Ctext%7Bdt%7D%7D+%5Cpsi_%7Bi%7D%28%5Ctext%7Bt%7D%29+%3D+%5Cunderset%7B%28i%2Cj%29+%5Cin+E%7D%7B%5Csum%7D+L_%7Bi%2Cj%7D+%5Cpsi_%7Bj%7D%28%5Ctext%7Bt%7D%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} i \frac{\text{d}}{\text{dt}} \psi_{i}(\text{t}) = \underset{(i,j) \in E}{\sum} L_{i,j} \psi_{j}(\text{t})\end{aligned}" class="latex" title="\begin{aligned} i \frac{\text{d}}{\text{dt}} \psi_{i}(\text{t}) = \underset{(i,j) \in E}{\sum} L_{i,j} \psi_{j}(\text{t})\end{aligned}" />,</p>



<p>which is <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%28t%29%5Cright%3E+%3D+e%5E%7B-iLt%7D+%5Cleft%7C%5Cpsi%280%29%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi(t)\right&gt; = e^{-iLt} \left|\psi(0)\right&gt;" class="latex" title="\left|\psi(t)\right&gt; = e^{-iLt} \left|\psi(0)\right&gt;" />, determines the behavior of the quantum analog of the continuous random walk defined previously. A general quantum walk does not necessarily have to be defined by the Laplacian; it can be defined by any operator which “respects the structure of the graph,” that is, only allows transitions to between neighboring vertices in the graph or remain stationary [7]. To get a sense of how the behavior of the quantum walk differs from the classical one, we first discuss the example of the continuous time quantum walk on the line, before moving on to the discrete case.</p>



<h2>Continuous time quantum walk on the line</h2>



<p>An important example of the continuous time quantum walk is that defined on the infinite line. The eigenstates of the Laplacian operator for the graph representing the infinite line are the momentum states with eigenvalues <img src="https://s0.wp.com/latex.php?latex=2%28%5Ctext%7Bcos%7D%28p%29+-+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2(\text{cos}(p) - 1)" class="latex" title="2(\text{cos}(p) - 1)" />, for <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> in range <img src="https://s0.wp.com/latex.php?latex=%5B-%5Cpi%2C%5Cpi%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[-\pi,\pi]" class="latex" title="[-\pi,\pi]" />. This can be seen by representing the momentum states in terms of the position states and applying the Laplacian operator:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cleft%7Cp%5Cright%3E+%26%3D+%5Cunderset%7Bx%7D%7B%5Csum%7D+e%5E%7Bipx%7D+%5Cleft%7Cx%5Cright%3E+%5C%5C+L%5Cleft%7Cp%5Cright%3E+%26%3D+%5Cunderset%7Bx%7D%7B%5Csum%7D+e%5E%7Bipx%7D+%5Cleft%7Cx%2B1%5Cright%3E%2B+e%5E%7Bipx%7D+%5Cleft%7Cx-1%5Cright%3E+-+2e%5E%7Bipx%7D+%5Cleft%7Cx%5Cright%3E+%5C%5C+%26%3D+%5Cunderset%7Bx%7D%7B%5Csum%7D+%28e%5E%7Bip%7D+%2B+e%5E%7B-ip%7D+-+2%29+e%5E%7Bipx%7D+%5Cleft%7Cx%5Cright%3E+%5C%5C+%26%3D+2%28%5Ctext%7Bcos%7D%28p%29+-+1%29+%5Cleft%7Cp%5Cright%3E.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \left|p\right&gt; &amp;= \underset{x}{\sum} e^{ipx} \left|x\right&gt; \\ L\left|p\right&gt; &amp;= \underset{x}{\sum} e^{ipx} \left|x+1\right&gt;+ e^{ipx} \left|x-1\right&gt; - 2e^{ipx} \left|x\right&gt; \\ &amp;= \underset{x}{\sum} (e^{ip} + e^{-ip} - 2) e^{ipx} \left|x\right&gt; \\ &amp;= 2(\text{cos}(p) - 1) \left|p\right&gt;.\end{aligned}" class="latex" title="\begin{aligned} \left|p\right&gt; &amp;= \underset{x}{\sum} e^{ipx} \left|x\right&gt; \\ L\left|p\right&gt; &amp;= \underset{x}{\sum} e^{ipx} \left|x+1\right&gt;+ e^{ipx} \left|x-1\right&gt; - 2e^{ipx} \left|x\right&gt; \\ &amp;= \underset{x}{\sum} (e^{ip} + e^{-ip} - 2) e^{ipx} \left|x\right&gt; \\ &amp;= 2(\text{cos}(p) - 1) \left|p\right&gt;.\end{aligned}" /></p>



<p>Hence the probability distribution at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, <img src="https://s0.wp.com/latex.php?latex=p%28x%2Ct%29+%3D+%7C%5Cleft%3C+x%5Cright%7C+e%5E%7B-iLt%7D+%5Cleft%7C%5Cpsi%280%29%5Cright%3E+%7C+%5E%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x,t) = |\left&lt; x\right| e^{-iLt} \left|\psi(0)\right&gt; | ^{2}" class="latex" title="p(x,t) = |\left&lt; x\right| e^{-iLt} \left|\psi(0)\right&gt; | ^{2}" />, with initial position <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%280%29%5Cright%3E+%3D+%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi(0)\right&gt; = \left|0\right&gt;" class="latex" title="\left|\psi(0)\right&gt; = \left|0\right&gt;" /> is given by:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7C%5Cleft%3C+x%5Cright%7C+e%5E%7B-iLt%7D+%5Cleft%7C0%5Cright%3E+%7C+%5E%7B2%7D+%26%3D++%5Cbigg%7C+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint_%7B-%5Cpi%7D%5E%7B%5Cpi%7D+e%5E%7B-2it%28%5Ctext%7Bcos%7Dp+-+1%29%7D+%5Cleft%3C+x%7Cp%5Cright%3E+%5Ctext%7Bd%7Dp+%5Cbigg%7C%5E%7B2%7D+%5C%5C+%26%3D+%5Cbigg%7C+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint_%7B-%5Cpi%7D%5E%7B%5Cpi%7D+e%5E%7B-2it%28%5Ctext%7Bcos%7Dp+-+1%29%7D+e%5E%7Bipx%7D+%5Ctext%7Bd%7Dp+%5Cbigg%7C%5E%7B2%7D+%5C%5C+%26%3D+%7C+J_%7Bx%7D%282t%29+%7C%5E%7B2%7D.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} |\left&lt; x\right| e^{-iLt} \left|0\right&gt; | ^{2} &amp;=  \bigg| \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{-2it(\text{cos}p - 1)} \left&lt; x|p\right&gt; \text{d}p \bigg|^{2} \\ &amp;= \bigg| \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{-2it(\text{cos}p - 1)} e^{ipx} \text{d}p \bigg|^{2} \\ &amp;= | J_{x}(2t) |^{2}.\end{aligned}" class="latex" title="\begin{aligned} |\left&lt; x\right| e^{-iLt} \left|0\right&gt; | ^{2} &amp;=  \bigg| \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{-2it(\text{cos}p - 1)} \left&lt; x|p\right&gt; \text{d}p \bigg|^{2} \\ &amp;= \bigg| \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{-2it(\text{cos}p - 1)} e^{ipx} \text{d}p \bigg|^{2} \\ &amp;= | J_{x}(2t) |^{2}.\end{aligned}" /></p>



<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/quantum-1.png?w=451" alt="" class="wp-image-7071" width="451" />Figure 1.a) Probability distribution for continuous time quantum walk on the infinite line at time <img src="https://s0.wp.com/latex.php?latex=t+%3D+80&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t = 80" class="latex" title="t = 80" />.</figure>



<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/classical-1.png?w=451&amp;h=300" alt="" height="300" class="wp-image-7073" width="451" /><br />Figure 1.b) Approximate probability<br /> distribution of the continuous time random walk on the infinite line at<br /> time <img src="https://s0.wp.com/latex.php?latex=t%3D30&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=30" class="latex" title="t=30" />.<br /></figure>



<p>While the probability distribution for the classical continuous time<br /> random walk on the same graph approaches, for large <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7B4%5Cpi+t%7D%7D+e%5E%7B%5Cfrac%7B-x%5E%7B2%7D%7D%7B4t%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{4\pi t}} e^{\frac{-x^{2}}{4t}}" class="latex" title="\frac{1}{\sqrt{4\pi t}} e^{\frac{-x^{2}}{4t}}" />, or a Gaussian of width <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2\sqrt{t}" class="latex" title="2\sqrt{t}" />. One can see that the quantum walk has its largest peaks at the extrema, with oscillations in between that decrease in amplitude as one approaches the starting position at <img src="https://s0.wp.com/latex.php?latex=x%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x=0" class="latex" title="x=0" />. This is due to the destructive interference between states of different phases that does not occur in the classical case. The probability distribution of the classical walk, on the other hand, has no oscillations and instead a single peak centered at <img src="https://s0.wp.com/latex.php?latex=x%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x=0" class="latex" title="x=0" />, which widens and flattens as <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> increases.</p>



<h2>Walk on the glued trees graph</h2>



<p>A <em>glued tree</em> is a graph obtained by taking two binary trees of equal height and connecting each of the leaves of one of the trees to exactly two leaves of the other tree so that each node that was a leaf in one of the original trees now has degree exactly <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="3" class="latex" title="3" />. An example of such a graph is shown in Figure 2.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/glued-1.png?w=1024" alt="" class="wp-image-7077" />Figure 2: An example of a glued tree graph, from [2].</figure>



<p>The time for the quantum walk on this graph to reach the right root from the left one is exponentially faster than in the classical case. Consider the classical random walk on this graph. While in the left tree, the probability of transitioning to a node in the level one to the right is twice that of transitioning to a node in the level one to the left. However, while in the right tree, the opposite is true. Therefore, one can see that in the middle of the graph, the walk will get lost, as, locally, there is no way to determine which node is part of which tree. It will instead get stuck in the cycles of identical nodes and will have exponentially small probability of reaching the right node.</p>



<p>To construct a continuous time quantum walk on this graph, we consider the graph in terms of <em>columns</em>. One can visualize the columns of Figure 2 as consisting of all the nodes equidistant from the entrance and exit nodes. If each tree is height <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, then we label the columns <img src="https://s0.wp.com/latex.php?latex=0%2C1%2C%5Ctext%7B...%7D%2C2n%2C2n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0,1,\text{...},2n,2n+1" class="latex" title="0,1,\text{...},2n,2n+1" />, where column <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> contains the nodes with shortest path of length <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> from the leftmost root node. We describe the state of each column as a superposition of the states of each node in that column. The number of nodes in column <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />, <img src="https://s0.wp.com/latex.php?latex=N_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_{i}" class="latex" title="N_{i}" />, will be <img src="https://s0.wp.com/latex.php?latex=2%5E%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{i}" class="latex" title="2^{i}" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5B0%2Cn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [0,n]" class="latex" title="i \in [0,n]" /> and <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2n%2B1-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2n+1-i}" class="latex" title="2^{2n+1-i}" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%2B1%2C2n%2B1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n+1,2n+1]" class="latex" title="i \in [n+1,2n+1]" />. Then, we can define the state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\text{col} \; i\right&gt;" class="latex" title="\left|\text{col} \; i\right&gt;" /> as:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i%5Cright%3E+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7BN_%7Bi%7D%7D%7D+%5Cunderset%7Bj+%5Cin+%5Ctext%7Bcol%7D+%5C%3B+i%7D%7B%5Csum%7D+%5Cleft%7Cj%5Cright%3E.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \left|\text{col} \; i\right&gt; = \frac{1}{\sqrt{N_{i}}} \underset{j \in \text{col} \; i}{\sum} \left|j\right&gt;.\end{aligned}" class="latex" title="\begin{aligned} \left|\text{col} \; i\right&gt; = \frac{1}{\sqrt{N_{i}}} \underset{j \in \text{col} \; i}{\sum} \left|j\right&gt;.\end{aligned}" /></p>



<p>The factor of <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN_%7Bi%7D%7D%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{N_{i}}} " class="latex" title="\frac{1}{\sqrt{N_{i}}} " />latex  ensures that the state is normalized. Since the adjacency matrix <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> of the glued tree is Hermitian, then we can treat <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> as the Hamiltonian of the system determining the behavior of the quantum walk. By acting on this state with the adjacency matrix operator <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" />, we get the result (for <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5B1%2Cn-1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [1,n-1]" class="latex" title="i \in [1,n-1]" />):</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+A%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i%5Cright%3E++%26%3D+2%5Cfrac%7B%5Csqrt%7BN_%7Bi-1%7D%7D%7D%7B%5Csqrt%7BN_%7Bi%7D%7D%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i-1%5Cright%3E+%2B+%5Cfrac%7B%5Csqrt%7BN_%7Bi%2B1%7D%7D%7D%7B%5Csqrt%7BN_%7Bi%7D%7D%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i%2B1%5Cright%3E+%5C%5C+%26%3D+%5Csqrt%7B2%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i-1%5Cright%3E+%2B+%5Csqrt%7B2%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+i%2B1%5Cright%3E.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} A\left|\text{col} \; i\right&gt;  &amp;= 2\frac{\sqrt{N_{i-1}}}{\sqrt{N_{i}}} \left|\text{col} \; i-1\right&gt; + \frac{\sqrt{N_{i+1}}}{\sqrt{N_{i}}} \left|\text{col} \; i+1\right&gt; \\ &amp;= \sqrt{2} \left|\text{col} \; i-1\right&gt; + \sqrt{2} \left|\text{col} \; i+1\right&gt;.\end{aligned}" class="latex" title="\begin{aligned} A\left|\text{col} \; i\right&gt;  &amp;= 2\frac{\sqrt{N_{i-1}}}{\sqrt{N_{i}}} \left|\text{col} \; i-1\right&gt; + \frac{\sqrt{N_{i+1}}}{\sqrt{N_{i}}} \left|\text{col} \; i+1\right&gt; \\ &amp;= \sqrt{2} \left|\text{col} \; i-1\right&gt; + \sqrt{2} \left|\text{col} \; i+1\right&gt;.\end{aligned}" /><br /></p>



<p>Then for <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%2B2%2C2n%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n+2,2n]" class="latex" title="i \in [n+2,2n]" />, we get the same result, because of symmetry.<br /></p>



<p>For <img src="https://s0.wp.com/latex.php?latex=i+%3D+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i = n" class="latex" title="i = n" />:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+A%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+n%5Cright%3E+%3D+%5Csqrt%7B2%7D+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3Bn-1%5Cright%3E+%2B+2+%5Cleft%7C%5Ctext%7Bcol%7D+%5C%3B+n%5Cright%3E.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} A\left|\text{col} \; n\right&gt; = \sqrt{2} \left|\text{col} \;n-1\right&gt; + 2 \left|\text{col} \; n\right&gt;.\end{aligned}" class="latex" title="\begin{aligned} A\left|\text{col} \; n\right&gt; = \sqrt{2} \left|\text{col} \;n-1\right&gt; + 2 \left|\text{col} \; n\right&gt;.\end{aligned}" /></p>



<p>The case of <img src="https://s0.wp.com/latex.php?latex=i+%3D+n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i = n+1" class="latex" title="i = n+1" /> is symmetric. One can see that the walk on this graph is equivalent to the quantum walk on the finite line with nodes corresponding to the columns. All of the edges, excluding that between columns <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> and <img src="https://s0.wp.com/latex.php?latex=n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n+1" class="latex" title="n+1" />, have weight <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sqrt{2}" class="latex" title="\sqrt{2}" />. The edge between column <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> and <img src="https://s0.wp.com/latex.php?latex=n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n+1" class="latex" title="n+1" /> has weight <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2" class="latex" title="2" />.</p>



<p>The probability distribution of the quantum walk on this line can be roughly approximated using the infinite line. In the case of the infinite line, the probability distribution can be seen as a wave propagating with speed linear in the time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />. Thus, in time linear in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, the probability that the state is measured at distance <img src="https://s0.wp.com/latex.php?latex=2n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2n+1" class="latex" title="2n+1" /> from the starting state is <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Ctext%7Bpoly%7D+%5C%3B+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\text{poly} \; n}" class="latex" title="\frac{1}{\text{poly} \; n}" />. In [3] it is shown that the fact that the line is finite and has a single differently weighted edge from the others (that between <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> and <img src="https://s0.wp.com/latex.php?latex=n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n+1" class="latex" title="n+1" />) does not change the fact that in polynomial time, the quantum walk will travel from the left root node to the right one, although in this case there is no limiting distribution as the peaks oscillate. This was the first result that gives an exponential speed up over the classical case using quantum walks.</p>



<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/glued-tree-1.png?w=451&amp;h=291" alt="" height="291" class="wp-image-7082" width="451" />Figure 3: Although the quantum walk on the glued trees graph does not have a limiting distribution, this is an example of the resulting probability distribution at time <img src="https://s0.wp.com/latex.php?latex=t%3D30&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=30" class="latex" title="t=30" /> for a <img src="https://s0.wp.com/latex.php?latex=n%3D4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n=4" class="latex" title="n=4" /> column glued tree graph.  The x-axis corresponds to the columns.  One can see that the probability of being at the columns at either extremes is significantly larger than that of being in the middle of the graph. In contrast, the classical random walk takes exponential time to ever reach the exit root node.</figure>



<h1>Discrete time quantum walks</h1>



<p>In this section, we will first give an introduction to the discrete quantum walk, including the discrete quantum walk on the line and the Markov chain quantum walk, as defined in [7]. Next, we discuss how Grover search can be viewed as a quantum walk algorithm, which leads us into Ambainis’s quantum-walks based algorithm from [1] for the element distinctness problem, which gives a speed up over classical and other quantum non-walks based algorithms.</p>



<p>The discrete time quantum walk is defined by two operators: the <em>coin flip</em> operator, and the <em>shift</em> operator. The coin flip operator <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> determines the direction of the walk, while the shift operator <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> makes the transition to the new state conditioned on the result of the coin flip. The Hilbert space governing the walk is <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+%3D+%5Cmathcal%7BH%7D%7BC%7D+%5Cotimes+%5Cmathcal%7BH%7D%7BS%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} = \mathcal{H}{C} \otimes \mathcal{H}{S}" class="latex" title="\mathcal{H} = \mathcal{H}{C} \otimes \mathcal{H}{S}" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H}{C}" class="latex" title="\mathcal{H}{C}" /> corresponds to the space associated with the result of the coin flip operator, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%7BS%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H}{S}" class="latex" title="\mathcal{H}{S}" /> corresponds to the locations in the graph on which the walk is defined.</p>



<p>For example, consider the discrete time walk on the infinite line. Since there are two possible directions (left or right), then the Hilbert space associated with the coin flip operator is two dimensional. In the unbiased case, the coin flip is the Hadamard operator,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1++%5Cend%7Bbmatrix%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} H = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1  \end{bmatrix},\end{aligned}" class="latex" title="\begin{aligned} H = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1  \end{bmatrix},\end{aligned}" /></p>



<p>and shift operator that produces the transition from state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cj%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|j\right&gt;" class="latex" title="\left|j\right&gt;" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cj%2B1%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|j+1\right&gt;" class="latex" title="\left|j+1\right&gt;" /> or <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cj-1%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|j-1\right&gt;" class="latex" title="\left|j-1\right&gt;" />,<br /> conditioned on the result of the coin flip, is <img src="https://s0.wp.com/latex.php?latex=S+%3D+%5Cleft%7C0%5Cright%3E%5Cleft%3C+0%5Cright%7C+%5Cotimes+%5Cunderset%7Bj%7D%7B%5Csum%7D+%5Cleft%7Cj%2B1%5Cright%3E+%5Cleft%3C+j%5Cright%7C+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%3C+1%5Cright%7C+%5Cotimes+%5Cunderset%7Bj%7D%7B%5Csum%7D+%5Cleft%7Cj+-+1%5Cright%3E+%5Cleft%3C+j%5Cright%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S = \left|0\right&gt;\left&lt; 0\right| \otimes \underset{j}{\sum} \left|j+1\right&gt; \left&lt; j\right| + \left|1\right&gt;\left&lt; 1\right| \otimes \underset{j}{\sum} \left|j - 1\right&gt; \left&lt; j\right|" class="latex" title="S = \left|0\right&gt;\left&lt; 0\right| \otimes \underset{j}{\sum} \left|j+1\right&gt; \left&lt; j\right| + \left|1\right&gt;\left&lt; 1\right| \otimes \underset{j}{\sum} \left|j - 1\right&gt; \left&lt; j\right|" />.</p>



<p>Each step of the walk is determined by an application of the unitary<br /> operator <img src="https://s0.wp.com/latex.php?latex=U+%3D+S+%5Ccdot+%28H+%5Cotimes+I%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = S \cdot (H \otimes I)" class="latex" title="U = S \cdot (H \otimes I)" />. If the walk starts at position<br /> <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cx%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|x\right&gt;" class="latex" title="\left|x\right&gt;" />, then measuring the state after one application of <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" /> gives <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cx%2B1%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|x+1\right&gt;" class="latex" title="\left|x+1\right&gt;" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}" class="latex" title="\frac{1}{2}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cx-1%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|x-1\right&gt;" class="latex" title="\left|x-1\right&gt;" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2}" class="latex" title="\frac{1}{2}" />. This is exactly the same as the case of the classical random walk on the infinite line; the difference between the two walks becomes apparent after a few steps.</p>



<p>For example, the result of the walk starting at state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%280%29%5Cright%3E+%3D+%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi(0)\right&gt; = \left|0\right&gt;\left|0\right&gt;" class="latex" title="\left|\psi(0)\right&gt; = \left|0\right&gt;\left|0\right&gt;" /> after 4 steps gives:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cleft%7C%5Cpsi%281%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28+%5Cleft%7C0%5Cright%3E%5Cleft%7C1%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C-1%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%282%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+%5Cleft%7C0%5Cright%3E%5Cleft%7C2%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E+%2B+%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E+-+%5Cleft%7C1%5Cright%3E%5Cleft%7C-2%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%283%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B2%5Csqrt%7B2%7D%7D%5Cleft%28%5Cleft%7C0%5Cright%3E%5Cleft%7C3%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C1%5Cright%3E+%2B+2%5Cleft%7C0%5Cright%3E%5Cleft%7C1%5Cright%3E+-%5Cleft%7C0%5Cright%3E%5Cleft%7C-1%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C-3%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%284%29%5Cright%3E+%26%3D++%5Cfrac%7B1%7D%7B4%7D+%28%5Cleft%7C0%5Cright%3E%5Cleft%7C4%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C2%5Cright%3E+%2B+3%5Cleft%7C0%5Cright%3E%5Cleft%7C2%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E+-%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E+-%5Cleft%7C1%5Cright%3E%5Cleft%7C-2%5Cright%3E+%2B%5Cleft%7C0%5Cright%3E%5Cleft%7C-2%5Cright%3E-%5Cleft%7C1%5Cright%3E%5Cleft%7C-4%5Cright%3E%29.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \left|\psi(1)\right&gt; &amp;= \frac{1}{\sqrt{2}}\left( \left|0\right&gt;\left|1\right&gt; + \left|1\right&gt;\left|-1\right&gt; \right) \\ \left|\psi(2)\right&gt; &amp;= \frac{1}{2}\left( \left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; - \left|1\right&gt;\left|-2\right&gt; \right) \\ \left|\psi(3)\right&gt; &amp;= \frac{1}{2\sqrt{2}}\left(\left|0\right&gt;\left|3\right&gt; + \left|1\right&gt;\left|1\right&gt; + 2\left|0\right&gt;\left|1\right&gt; -\left|0\right&gt;\left|-1\right&gt; + \left|1\right&gt;\left|-3\right&gt; \right) \\ \left|\psi(4)\right&gt; &amp;=  \frac{1}{4} (\left|0\right&gt;\left|4\right&gt; + \left|1\right&gt;\left|2\right&gt; + 3\left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; -\left|0\right&gt;\left|0\right&gt; -\left|1\right&gt;\left|-2\right&gt; +\left|0\right&gt;\left|-2\right&gt;-\left|1\right&gt;\left|-4\right&gt;).\end{aligned}" class="latex" title="\begin{aligned} \left|\psi(1)\right&gt; &amp;= \frac{1}{\sqrt{2}}\left( \left|0\right&gt;\left|1\right&gt; + \left|1\right&gt;\left|-1\right&gt; \right) \\ \left|\psi(2)\right&gt; &amp;= \frac{1}{2}\left( \left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; - \left|1\right&gt;\left|-2\right&gt; \right) \\ \left|\psi(3)\right&gt; &amp;= \frac{1}{2\sqrt{2}}\left(\left|0\right&gt;\left|3\right&gt; + \left|1\right&gt;\left|1\right&gt; + 2\left|0\right&gt;\left|1\right&gt; -\left|0\right&gt;\left|-1\right&gt; + \left|1\right&gt;\left|-3\right&gt; \right) \\ \left|\psi(4)\right&gt; &amp;=  \frac{1}{4} (\left|0\right&gt;\left|4\right&gt; + \left|1\right&gt;\left|2\right&gt; + 3\left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; -\left|0\right&gt;\left|0\right&gt; -\left|1\right&gt;\left|-2\right&gt; +\left|0\right&gt;\left|-2\right&gt;-\left|1\right&gt;\left|-4\right&gt;).\end{aligned}" /></p>



<p>One can see that the distribution is becoming increasingly skewed<br /> towards the right, while in the classical case the distribution will be<br /> symmetric around the starting position. This is due to the destructive<br /> interference discussed earlier. The distribution after <img src="https://s0.wp.com/latex.php?latex=t+%3D+20&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t = 20" class="latex" title="t = 20" /> time<br /> steps is shown in Figure 4.</p>



<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/discrete.png?w=451&amp;h=291" alt="" height="291" class="wp-image-7090" width="451" />Figure 4: Distribution at time <img src="https://s0.wp.com/latex.php?latex=t+%3D+20&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t = 20" class="latex" title="t = 20" />, with <img src="https://s0.wp.com/latex.php?latex=20&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="20" class="latex" title="20" /> on the x-axis corresponding to position <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />.</figure>



<p>Now, consider the walk starting at state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%280%29%5Cright%3E+%3D+-%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi(0)\right&gt; = -\left|1\right&gt;\left|0\right&gt;" class="latex" title="\left|\psi(0)\right&gt; = -\left|1\right&gt;\left|0\right&gt;" />:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cleft%7C%5Cpsi%281%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28+-%5Cleft%7C0%5Cright%3E%5Cleft%7C1%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C-1%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%282%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+-%5Cleft%7C0%5Cright%3E%5Cleft%7C2%5Cright%3E+-+%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E+%2B+%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E+-+%5Cleft%7C1%5Cright%3E%5Cleft%7C-2%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%283%29%5Cright%3E+%26%3D+%5Cfrac%7B1%7D%7B2%5Csqrt%7B2%7D%7D%5Cleft%28-%5Cleft%7C0%5Cright%3E%5Cleft%7C3%5Cright%3E+-+%5Cleft%7C1%5Cright%3E%5Cleft%7C1%5Cright%3E+%2B+2%5Cleft%7C1%5Cright%3E%5Cleft%7C-1%5Cright%3E+-+%5Cleft%7C0%5Cright%3E%5Cleft%7C-1%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C-3%5Cright%3E+%5Cright%29+%5C%5C+%5Cleft%7C%5Cpsi%284%29%5Cright%3E+%26%3D++%5Cfrac%7B1%7D%7B4%7D+%28-%5Cleft%7C0%5Cright%3E%5Cleft%7C4%5Cright%3E+-%5Cleft%7C1%5Cright%3E%5Cleft%7C2%5Cright%3E+-+%5Cleft%7C0%5Cright%3E%5Cleft%7C2%5Cright%3E+%2B+%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E+%2B+%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E+-3%5Cleft%7C1%5Cright%3E%5Cleft%7C-2%5Cright%3E+%2B+%5Cleft%7C0%5Cright%3E%5Cleft%7C-2%5Cright%3E+-+%5Cleft%7C1%5Cright%3E%5Cleft%7C-4%5Cright%3E%29.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}\left|\psi(1)\right&gt; &amp;= \frac{1}{\sqrt{2}}\left( -\left|0\right&gt;\left|1\right&gt; + \left|1\right&gt;\left|-1\right&gt; \right) \\ \left|\psi(2)\right&gt; &amp;= \frac{1}{2}\left( -\left|0\right&gt;\left|2\right&gt; - \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; - \left|1\right&gt;\left|-2\right&gt; \right) \\ \left|\psi(3)\right&gt; &amp;= \frac{1}{2\sqrt{2}}\left(-\left|0\right&gt;\left|3\right&gt; - \left|1\right&gt;\left|1\right&gt; + 2\left|1\right&gt;\left|-1\right&gt; - \left|0\right&gt;\left|-1\right&gt; + \left|1\right&gt;\left|-3\right&gt; \right) \\ \left|\psi(4)\right&gt; &amp;=  \frac{1}{4} (-\left|0\right&gt;\left|4\right&gt; -\left|1\right&gt;\left|2\right&gt; - \left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; -3\left|1\right&gt;\left|-2\right&gt; + \left|0\right&gt;\left|-2\right&gt; - \left|1\right&gt;\left|-4\right&gt;).\end{aligned}" class="latex" title="\begin{aligned}\left|\psi(1)\right&gt; &amp;= \frac{1}{\sqrt{2}}\left( -\left|0\right&gt;\left|1\right&gt; + \left|1\right&gt;\left|-1\right&gt; \right) \\ \left|\psi(2)\right&gt; &amp;= \frac{1}{2}\left( -\left|0\right&gt;\left|2\right&gt; - \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; - \left|1\right&gt;\left|-2\right&gt; \right) \\ \left|\psi(3)\right&gt; &amp;= \frac{1}{2\sqrt{2}}\left(-\left|0\right&gt;\left|3\right&gt; - \left|1\right&gt;\left|1\right&gt; + 2\left|1\right&gt;\left|-1\right&gt; - \left|0\right&gt;\left|-1\right&gt; + \left|1\right&gt;\left|-3\right&gt; \right) \\ \left|\psi(4)\right&gt; &amp;=  \frac{1}{4} (-\left|0\right&gt;\left|4\right&gt; -\left|1\right&gt;\left|2\right&gt; - \left|0\right&gt;\left|2\right&gt; + \left|1\right&gt;\left|0\right&gt; + \left|0\right&gt;\left|0\right&gt; -3\left|1\right&gt;\left|-2\right&gt; + \left|0\right&gt;\left|-2\right&gt; - \left|1\right&gt;\left|-4\right&gt;).\end{aligned}" /></p>



<p><br /> This distribution given by this walk is the mirror image of the first.<br /> To generate a symmetric distribution, consider the start state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%280%29%5Cright%3E+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%5Cleft%7C0%5Cright%3E+-i%5Cleft%7C1%5Cright%3E%29%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi(0)\right&gt; = \frac{1}{\sqrt{2}}(\left|0\right&gt; -i\left|1\right&gt;)\left|0\right&gt;" class="latex" title="\left|\psi(0)\right&gt; = \frac{1}{\sqrt{2}}(\left|0\right&gt; -i\left|1\right&gt;)\left|0\right&gt;" />. The resulting distribution after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> steps will be <img src="https://s0.wp.com/latex.php?latex=p%28x%2Ct%29+%3D+%5Cfrac%7B1%7D%7B2%7D+p_%7B0%7D%28x%2Ct%29+%2B+%5Cfrac%7B1%7D%7B2%7D+p_%7B1%7D%28x%2Ct%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x,t) = \frac{1}{2} p_{0}(x,t) + \frac{1}{2} p_{1}(x,t)" class="latex" title="p(x,t) = \frac{1}{2} p_{0}(x,t) + \frac{1}{2} p_{1}(x,t)" />, where <img src="https://s0.wp.com/latex.php?latex=p_%7B0%7D%28x%2Ct%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{0}(x,t)" class="latex" title="p_{0}(x,t)" /> is the probability distribution after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> steps resulting from the start state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%280%29+%3D+%5Cleft%7C0%5Cright%3E%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi(0) = \left|0\right&gt;\left|0\right&gt;" class="latex" title="\psi(0) = \left|0\right&gt;\left|0\right&gt;" /> and <img src="https://s0.wp.com/latex.php?latex=p_%7B1%7D%28x%2Ct%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{1}(x,t)" class="latex" title="p_{1}(x,t)" /> is the probability distribution after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> steps resulting from the start state <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%280%29+%3D+-%5Cleft%7C1%5Cright%3E%5Cleft%7C0%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi(0) = -\left|1\right&gt;\left|0\right&gt;" class="latex" title="\psi(0) = -\left|1\right&gt;\left|0\right&gt;" />. The result will be symmetric, with peaks near the extrema, as we saw in the continuous case.</p>



<h2>Markov chain quantum walk</h2>



<p>A reversible, ergodic Markov chain with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> states can be represented by a <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \times n" class="latex" title="n \times n" /> transition matrix <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> with <img src="https://s0.wp.com/latex.php?latex=P_%7Bj%2Ci%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{j,i}" class="latex" title="P_{j,i}" /> equal to the probability of transitioning from state <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> to state <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> and <img src="https://s0.wp.com/latex.php?latex=P+%3D+P%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P = P^{*}" class="latex" title="P = P^{*}" />. Then, <img src="https://s0.wp.com/latex.php?latex=p_%7B0%7DP&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{0}P" class="latex" title="p_{0}P" />, where <img src="https://s0.wp.com/latex.php?latex=p_%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{0}" class="latex" title="p_{0}" /> is an initial probability distribution over the states, gives the distribution after one step.<br />Since <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bj%7D+P_%7Bi%2Cj%7D+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum_{j} P_{i,j} = 1" class="latex" title="\sum_{j} P_{i,j} = 1" /> for all <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />, <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> is stochastic and thus preserves normalization.</p>



<p>There are multiple ways to define a discrete quantum walk, depending on the properties of the transition matrix and the graph on which it is defined (overview provided in [4]). Here we look at the quantum walk on a Markov chain as given in [2]. For the quantum walk on this graph, we define state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Ci%5Cright%3E%5Cleft%7Cj%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|i\right&gt;\left|j\right&gt;" class="latex" title="\left|i\right&gt;\left|j\right&gt;" /> as the state that represents currently being at position <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> and facing in the direction of <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />. Then, we define the state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi_%7Bj%7D%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi_{j}\right&gt;" class="latex" title="\left|\psi_{j}\right&gt;" /> as a superposition of the states associated with position <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cleft%7C%5Cpsi_%7Bi%7D%5Cright%3E+%3D+%5Cunderset%7Bj%7D%7B%5Csum%7D+%5Csqrt%7BP_%7Bj%2Ci%7D%7D+%5Cleft%7Ci%5Cright%3E%5Cleft%7Cj%5Cright%3E.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \left|\psi_{i}\right&gt; = \underset{j}{\sum} \sqrt{P_{j,i}} \left|i\right&gt;\left|j\right&gt;.\end{aligned}" class="latex" title="\begin{aligned} \left|\psi_{i}\right&gt; = \underset{j}{\sum} \sqrt{P_{j,i}} \left|i\right&gt;\left|j\right&gt;.\end{aligned}" /></p>



<p>The unitary operator,</p>



<p><img src="https://s0.wp.com/latex.php?latex=D+%3D+2+%5Cunderset%7Bi%7D%7B%5Csum%7D+%5Cleft%7C%5Cpsi_%7Bi%7D%5Cright%3E%5Cleft%3C+%5Cpsi_%7Bi%7D%5Cright%7C+-+I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D = 2 \underset{i}{\sum} \left|\psi_{i}\right&gt;\left&lt; \psi_{i}\right| - I" class="latex" title="D = 2 \underset{i}{\sum} \left|\psi_{i}\right&gt;\left&lt; \psi_{i}\right| - I" />,</p>



<p>acts as a coin flip for the walk on this graph. Since <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> is reversible, we can let the shift operator be the unitary <img src="https://s0.wp.com/latex.php?latex=SWAP&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="SWAP" class="latex" title="SWAP" /> operator:</p>



<p><img src="https://s0.wp.com/latex.php?latex=SWAP+%3D+%5Cunderset%7Bi%2Cj%7D%7B%5Csum%7D+%5Cleft%7Ci%2Cj%5Cright%3E%5Cleft%3C+j%2Ci%5Cright%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="SWAP = \underset{i,j}{\sum} \left|i,j\right&gt;\left&lt; j,i\right|" class="latex" title="SWAP = \underset{i,j}{\sum} \left|i,j\right&gt;\left&lt; j,i\right|" />.</p>



<p>A quantum walk can also be defined for a non-reversible Markov chain using a pair of reflection operators (the coin flip operator is an example of a reflection operator). This corresponds to the construction given in [7].</p>



<h2>Search as a quantum walk algorithm</h2>



<p>Given a black box function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> and a set of inputs <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> with <img src="https://s0.wp.com/latex.php?latex=%7CS%7C+%3D+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|S| = N" class="latex" title="|S| = N" />, say we want to find whether an input <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in S" class="latex" title="x \in S" /> exists for which <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x)" class="latex" title="f(x)" /> equals some output value. We refer to the set of inputs <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> for which this is true as marked. Classically, this requires <img src="https://s0.wp.com/latex.php?latex=O%28N%2F%7CM%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N/|M|)" class="latex" title="O(N/|M|)" /> queries, for nonempty <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" />. Using the Grover search algorithm, this problem requires <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%2F%7CM%7C%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\sqrt{N/|M|})" class="latex" title="O(\sqrt{N/|M|})" /> quantum queries. In this section, we give a quantum walks based algorithm that also solves this problem in <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%2F%7CM%7C%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\sqrt{N/|M|})" class="latex" title="O(\sqrt{N/|M|})" /> time. If we define a doubly stochastic matrix <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> with uniform transitions, then we can construct a new transition matrix <img src="https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P'" class="latex" title="P'" /> from <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> as:</p>



<p><img src="https://s0.wp.com/latex.php?latex=P_%7Bi%2Cj%7D%27+%3D+%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7BN-1%7D+%5Cquad+%26%5Ctext%7Bif+%7D+i+%5Cneq+j+%5Ctext%7B+and+%7D+i+%5Cnotin+M+%5C%5C0+%5Cquad+%26%5Ctext%7Bif+%7D+i+%3D+j+%5Ctext%7B+and+%7D+i+%5Cnotin+M+%5C%5C+1+%5Cquad+%26%5Ctext%7Bif+%7D+i+%3D+j+%5Ctext%7B+and+%7D+i+%5Cin+M+%5C%5C+0+%5Cquad+%26%5Ctext%7Bif+%7D+i+%5Cneq+j+%5Ctext%7B+and+%7D+i+%5Cin+M.+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{i,j}' = \begin{cases} \frac{1}{N-1} \quad &amp;\text{if } i \neq j \text{ and } i \notin M \\0 \quad &amp;\text{if } i = j \text{ and } i \notin M \\ 1 \quad &amp;\text{if } i = j \text{ and } i \in M \\ 0 \quad &amp;\text{if } i \neq j \text{ and } i \in M. \end{cases}" class="latex" title="P_{i,j}' = \begin{cases} \frac{1}{N-1} \quad &amp;\text{if } i \neq j \text{ and } i \notin M \\0 \quad &amp;\text{if } i = j \text{ and } i \notin M \\ 1 \quad &amp;\text{if } i = j \text{ and } i \in M \\ 0 \quad &amp;\text{if } i \neq j \text{ and } i \in M. \end{cases}" /></p>



<p>Then, when the state of the first register is unmarked, the operator <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> defined in the previous section acts as a diffusion over its neighbors. When the state in the first register is marked, then <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> will act as the operator <img src="https://s0.wp.com/latex.php?latex=-I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-I" class="latex" title="-I" />, and the walk stops, as a marked state has been reached. This requires two queries to the black box function: one to check whether the input is marked, and then another to uncompute. By rearranging the order of the columns in <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> so that the columns corresponding to the non-marked elements come before the columns corresponding to the marked elements, we get:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+P%27+%3D+%5Cbegin%7Bpmatrix%7D+P_%7B0%7D+%26+0+%5C%5C+P_%7B1%7D+%26+I+%5Cend%7Bpmatrix%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} P' = \begin{pmatrix} P_{0} &amp; 0 \\ P_{1} &amp; I \end{pmatrix},\end{aligned}" class="latex" title="\begin{aligned} P' = \begin{pmatrix} P_{0} &amp; 0 \\ P_{1} &amp; I \end{pmatrix},\end{aligned}" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=P_%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{0}" class="latex" title="P_{0}" /> gives the transitions between non-marked elements and <img src="https://s0.wp.com/latex.php?latex=P_%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{1}" class="latex" title="P_{1}" /> gives the transitions from non-marked to marked elements.</p>



<p>We now look at the hitting time of the classical random walk. Assume<br /> that there is zero probability of starting at a marked vertex. Then, we<br /> can write the starting distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, where the last <img src="https://s0.wp.com/latex.php?latex=%7CM%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|M|" class="latex" title="|M|" /> elements of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, corresponding to the marked elements, are zero, as<br /> <img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Cunderset%7B%5Clambda%7D%7B%5Csum%7D+%5Calpha_%7B%5Clambda%7D+%5Cleft%7C%5Clambda%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p = \underset{\lambda}{\sum} \alpha_{\lambda} \left|\lambda\right&gt;" class="latex" title="p = \underset{\lambda}{\sum} \alpha_{\lambda} \left|\lambda\right&gt;" />, where <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> are the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=P_%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{0}" class="latex" title="P_{0}" />, and <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Clambda%5Cright%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\lambda\right&gt;" class="latex" title="\left|\lambda\right&gt;" /> are the corresponding eigenvectors, with the last <img src="https://s0.wp.com/latex.php?latex=%7CM%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|M|" class="latex" title="|M|" /> entries zero. Let <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%7B%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda^{}" class="latex" title="\lambda^{}" /> be the principal (largest) eigenvalue. Then, the probability that, after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> steps, a marked element has not yet been reached will be <img src="https://s0.wp.com/latex.php?latex=%5Csum+%28P_%7B0%7D%5E%7Bt%7Dp%29_%7Bi%7D+%5Cleq+%5Clambda%5E%7B%2At%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum (P_{0}^{t}p)_{i} \leq \lambda^{*t}" class="latex" title="\sum (P_{0}^{t}p)_{i} \leq \lambda^{*t}" />. Then, the<br /> probability that a marked element has been reached in that time will be<br /> <img src="https://s0.wp.com/latex.php?latex=%5Cgeq+1+-+%5Clambda%5E%7Bt%7D+%5Cgeq+1+-+t+%5Clambda%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\geq 1 - \lambda^{t} \geq 1 - t \lambda^{*}" class="latex" title="\geq 1 - \lambda^{t} \geq 1 - t \lambda^{*}" />. Setting<br /> <img src="https://s0.wp.com/latex.php?latex=t+%3D+%5Cfrac%7B1%7D%7B1-%5Clambda%5E%7B%2A%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t = \frac{1}{1-\lambda^{*}}" class="latex" title="t = \frac{1}{1-\lambda^{*}}" /> gives probability <img src="https://s0.wp.com/latex.php?latex=%5COmega%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega(1)" class="latex" title="\Omega(1)" /> that a marked element will be reached in that time.</p>



<p>The eigenvalues of <img src="https://s0.wp.com/latex.php?latex=P_%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{0}" class="latex" title="P_{0}" /> will be <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BN-%7CM%7C-1%7D%7BN-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{N-|M|-1}{N-1}" class="latex" title="\frac{N-|M|-1}{N-1}" /> and<br /> <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B-1%7D%7BN-%7CM%7C-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{-1}{N-|M|-1}" class="latex" title="\frac{-1}{N-|M|-1}" />. Then, the classical hitting time will be:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+t+%26%3D+%5Cfrac%7B1%7D%7B1-%5Clambda%5E%7B%2A%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B1-%5Cfrac%7BN-%7CM%7C-1%7D%7BN-1%7D%7D+%5C%5C+%26%3D+O%5Cleft%28%5Cfrac%7BN%7D%7B%7CM%7C%7D%5Cright%29.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} t &amp;= \frac{1}{1-\lambda^{*}} \\ &amp;= \frac{1}{1-\frac{N-|M|-1}{N-1}} \\ &amp;= O\left(\frac{N}{|M|}\right).\end{aligned}" class="latex" title="\begin{aligned} t &amp;= \frac{1}{1-\lambda^{*}} \\ &amp;= \frac{1}{1-\frac{N-|M|-1}{N-1}} \\ &amp;= O\left(\frac{N}{|M|}\right).\end{aligned}" /></p>



<p>It can be showed that for a walk defined by a Markov chain, the<br /> classical hitting time will be <img src="https://s0.wp.com/latex.php?latex=O%28%5Cfrac%7B1%7D%7B%5Cdelta+%5Cepsilon%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\frac{1}{\delta \epsilon})" class="latex" title="O(\frac{1}{\delta \epsilon})" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+1+-+%5Clambda%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta = 1 - \lambda^{*}" class="latex" title="\delta = 1 - \lambda^{*}" />, the <em>spectral gap</em>, and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Cleq+%5Cfrac%7B%7CM%7C%7D%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon \leq \frac{|M|}{N}" class="latex" title="\epsilon \leq \frac{|M|}{N}" /> [2].</p>



<p>Magniez <em>et al</em> proved in [6] that for a reversible, ergodic<br /> Markov chain, the quantum hitting time for a walk on this chain is<br /> within a factor of the square root of the classical hitting time. Since<br /> the walk on this input acts as a walk on a reversible Markov chain until<br /> a marked element is reached, then this is also true for a walk defined<br /> by our transition matrix <img src="https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P'" class="latex" title="P'" />. This arises from the fact that the<br /> spectral gap of the matrix describing the quantum walk corresponding to<br /> stochastic matrix <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" /> is quadratically larger than the spectral gap of<br /> the matrix describing the classical random walk corresponding to <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P" class="latex" title="P" />, the proof of which is given in [2]. Thus, the quantum hitting time<br /> is <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%2F%7CM%7C%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\sqrt{N/|M|})" class="latex" title="O(\sqrt{N/|M|})" />, which exactly matches the quantum query complexity of Grover search.</p>



<h2>Element distinctness problem</h2>



<p>Now, we describe Ambainis’s algorithm given in [1] for solving<br /> the <em>element distinctness problem</em> in <img src="https://s0.wp.com/latex.php?latex=O%28N%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N^{\frac{2}{3}})" class="latex" title="O(N^{\frac{2}{3}})" /> time, which<br /> produces a speed up over the classical algorithm, which requires <img src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N)" class="latex" title="O(N)" /> queries, and also over other known quantum algorithms that do not make use of quantum walks, which require <img src="https://s0.wp.com/latex.php?latex=O%28N%5E%7B%5Cfrac%7B3%7D%7B4%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N^{\frac{3}{4}})" class="latex" title="O(N^{\frac{3}{4}})" /> queries. The element distinctness problem is defined as follows: given a function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> on a size <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" /> set of inputs</p>



<p><img src="https://s0.wp.com/latex.php?latex=S%3D%5C%7Bx_%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S=\{x_{1}" class="latex" title="S=\{x_{1}" />,…,<img src="https://s0.wp.com/latex.php?latex=x_%7BN%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{N}\}" class="latex" title="x_{N}\}" />,</p>



<p>determine whether there exists a pair <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C%5C%3B+x_%7B2%7D+%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{1},\; x_{2} \in S" class="latex" title="x_{1},\; x_{2} \in S" /> for which <img src="https://s0.wp.com/latex.php?latex=f%28x_%7B1%7D%29+%3D+f%28x_%7B2%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x_{1}) = f(x_{2})" class="latex" title="f(x_{1}) = f(x_{2})" />.  As in the search problem defined in the previous section, this is a decision problem; we are not concerned with finding the values of these pairs, only whether at least one exists.</p>



<p>The algorithm is similar to the search algorithm described in the previous section, except we define the walk on a <em>Hamming graph</em>. A Hamming graph <img src="https://s0.wp.com/latex.php?latex=H%28N%2Cm%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H(N,m)" class="latex" title="H(N,m)" /> is defined as follows: each vertex <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> corresponds to an <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />-tuple, (<img src="https://s0.wp.com/latex.php?latex=i_%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{1}" class="latex" title="i_{1}" />,…,<img src="https://s0.wp.com/latex.php?latex=i_%7Bm%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{m}" class="latex" title="i_{m}" />), where <img src="https://s0.wp.com/latex.php?latex=i_%7Bk%7D+%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{k} \in S" class="latex" title="i_{k} \in S" /> for all <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> and repetition is allowed (that is, <img src="https://s0.wp.com/latex.php?latex=i_%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{k}" class="latex" title="i_{k}" /> may equal <img src="https://s0.wp.com/latex.php?latex=i_%7Bj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{j}" class="latex" title="i_{j}" /> for <img src="https://s0.wp.com/latex.php?latex=k+%5Cneq+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \neq j" class="latex" title="k \neq j" />), and <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> is a parameter we will choose. Edges will exist between vertices that differ in exactly one coordinate (order matters in this graph). We describe the state of each vertex as:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Ci+%5Cright%3E%3D%7C+i_%7B1%7D%2Ci_%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|i \right&gt;=| i_{1},i_{2}" class="latex" title="\left|i \right&gt;=| i_{1},i_{2}" />,…,<img src="https://s0.wp.com/latex.php?latex=i_%7Bm%7D%2Cf%28i_%7B1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{m},f(i_{1})" class="latex" title="i_{m},f(i_{1})" />,…,<img src="https://s0.wp.com/latex.php?latex=f%28i_%7Bm%7D%29%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(i_{m})&gt;" class="latex" title="f(i_{m})&gt;" /></p>



<p>Then, moving along each edge that replaces the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />th coordinate with <img src="https://s0.wp.com/latex.php?latex=x_%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{k}" class="latex" title="x_{k}" /> such that <img src="https://s0.wp.com/latex.php?latex=i_%7Bj%7D+%5Cneq+x_%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{j} \neq x_{k}" class="latex" title="i_{j} \neq x_{k}" />  requires two queries to the black box function to erase <img src="https://s0.wp.com/latex.php?latex=f%28i_%7Bj%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(i_{j})" class="latex" title="f(i_{j})" /> and compute <img src="https://s0.wp.com/latex.php?latex=f%28x_%7Bk%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x_{k})" class="latex" title="f(x_{k})" />. In the case, the marked vertices will be those that contain some <img src="https://s0.wp.com/latex.php?latex=f%28i_%7Bk%7D%29+%3D+f%28i_%7Bj%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(i_{k}) = f(i_{j})" class="latex" title="f(i_{k}) = f(i_{j})" /> for <img src="https://s0.wp.com/latex.php?latex=i_%7Bj%7D+%5Cneq+i_%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i_{j} \neq i_{k}" class="latex" title="i_{j} \neq i_{k}" />. Since the function values are stored in the description of the state, then no additional queries to the black box are required to check if in a marked state.</p>



<p>The transition matrix is given by <img src="https://s0.wp.com/latex.php?latex=P+%3D+%5Cfrac%7B1%7D%7Bm%28n-1%29%7D+%5Cunderset%7Bi+%5Cin+%5B1%2Cm%5D%7D%7B%5Csum%7D+%28J+-+I%29%5E%7B%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P = \frac{1}{m(n-1)} \underset{i \in [1,m]}{\sum} (J - I)^{(i)}" class="latex" title="P = \frac{1}{m(n-1)} \underset{i \in [1,m]}{\sum} (J - I)^{(i)}" />. <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J" class="latex" title="J" /> is the <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \times n" class="latex" title="n \times n" /> all one matrix, and the superscript <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> denotes the operator acting on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th coordinate. The factor of <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bm%28n-1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{m(n-1)}" class="latex" title="\frac{1}{m(n-1)}" /> normalizes the degree, since the graph is regular. We can compute the spectral gap of this graph to be <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bn%7D%7Bm%28n-1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{n}{m(n-1)}" class="latex" title="\frac{n}{m(n-1)}" /> (for details of this computation, see [2]). Then, noting that that the fraction of marked vertices, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />, is<br /> <img src="https://s0.wp.com/latex.php?latex=%5Cgeq+%5Cfrac%7Bm%28m-1%29%28n-2%29%5E%7Bm-2%7D%7D%7Bn%5E%7Bm%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\geq \frac{m(m-1)(n-2)^{m-2}}{n^{m}}" class="latex" title="\geq \frac{m(m-1)(n-2)^{m-2}}{n^{m}}" />, classically, the query complexity is <img src="https://s0.wp.com/latex.php?latex=m+%2B+O%28%5Cfrac%7B1%7D%7B%5Cdelta+%5Cepsilon%7D%29+%3D+m+%2B+O%28%5Cfrac%7Bn%5E%7B2%7D%7D%7Bm%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m + O(\frac{1}{\delta \epsilon}) = m + O(\frac{n^{2}}{m})" class="latex" title="m + O(\frac{1}{\delta \epsilon}) = m + O(\frac{n^{2}}{m})" />, where <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> is the queries required to construct the initial state. Setting the parameters equal to minimize with respect to <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> gives classical query complexity <img src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N)" class="latex" title="O(N)" />, as expected.</p>



<p>Then in the quantum case, <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> queries are still required to set up the state. <img src="https://s0.wp.com/latex.php?latex=O%28%5Cfrac%7Bn%7D%7B%5Csqrt%7Bm%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\frac{n}{\sqrt{m}})" class="latex" title="O(\frac{n}{\sqrt{m}})" /> queries are required to perform the walk until a marked state is reached, by [6]. Setting parameters equal gives <img src="https://s0.wp.com/latex.php?latex=O%28N%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N^{\frac{2}{3}})" class="latex" title="O(N^{\frac{2}{3}})" /> queries, as desired.</p>



<p>[1] Ambainis, A. Quantum walk algorithm for element distinctness, SIAM Journal on Computing 37(1):210-239 (2007). arXiv:quant-ph/0311001</p>



<p>[2] Childs, A. Lecture Notes on Quantum Algorithms (2017). <a href="https://www.cs.umd.edu/ amchilds/qa/qa.pdf" rel="nofollow">https://www.cs.umd.edu/ amchilds/qa/qa.pdf</a></p>



<p>[3] Childs, A., Farhi, E. Gutmann, S. An example of the difference between<br /> quantum and classical random walks. Journal of Quantum Information<br /> Processing, 1:35, 2002. Also quant-ph/0103020.</p>



<p>[4] Godsil, C., Hanmeng, Z. Discrete-Time Quantum Walks and Graph Structures<br /> (2018). arXiv:1701.04474</p>



<p>[5] Kempe, J. Quantum random walks: an introductory overview, Contemporary<br /> Physics, Vol. 44 (4) (2003) 307:327. arXiv:quant-ph/0303081</p>



<p>[6] Magniez, F., Nayak, A., Richter, P.C. et al. On the hitting times of<br /> quantum versus random walks, Algorithmica (2012) 63:91.<br /> <a href="https://doi.org/10.1007/s00453-011-9521-6" rel="nofollow">https://doi.org/10.1007/s00453-011-9521-6</a></p>



<p>[7] Szegedy, M. Quantum Speed-up of Markov Chain Based Algorithms, 45th<br /> Annual IEEE Symposium on Foundations of Computer Science (2004).<br /> <a href="https://ieeexplore.ieee.org/abstract/document/1366222" rel="nofollow">https://ieeexplore.ieee.org/abstract/document/1366222</a></p></div>







<p class="date">
by beanash <a href="https://windowsontheory.org/2018/12/23/introduction-to-quantum-walks/"><span class="datestr">at December 23, 2018 05:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/23/postdoc-in-cs-focused-on-sat-solving-at-kth-royal-institute-of-technology-apply-by-february-4-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/23/postdoc-in-cs-focused-on-sat-solving-at-kth-royal-institute-of-technology-apply-by-february-4-2019/">Postdoc in CS focused on SAT solving at KTH Royal Institute of Technology (apply by February 4, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The TCS Group at KTH invites applications for postdoc positions focusing on algorithms for solving the Boolean satisfiability problem (SAT) very efficiently for large classes of instances, and on analyzing and understanding such algorithms. The application deadline is February 4, 2019. Informal enquiries are welcome and may be sent to jakobn@kth.se .</p>
<p>Website: <a href="http://www.csc.kth.se/~jakobn/openings/J-2018-3178-Eng.php">http://www.csc.kth.se/~jakobn/openings/J-2018-3178-Eng.php</a><br />
Email: jakobn@kth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/23/postdoc-in-cs-focused-on-sat-solving-at-kth-royal-institute-of-technology-apply-by-february-4-2019/"><span class="datestr">at December 23, 2018 10:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/23/postdoc-positions-in-tcs-at-kth-royal-institute-of-technology-apply-by-february-4-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/23/postdoc-positions-in-tcs-at-kth-royal-institute-of-technology-apply-by-february-4-2019/">Postdoc positions in TCS at KTH Royal Institute of Technology (apply by February 4, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The TCS Group at KTH Royal Institute of Technology invites applications for postdoc positions in TCS. The application deadline is February 4, 2019. See <a href="https://apc.eecs.kth.se/J-2018-3169-Eng.php">https://apc.eecs.kth.se/J-2018-3169-Eng.php</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to apc@eecs.kth.se .</p>
<p>Website: <a href="https://apc.eecs.kth.se/J-2018-3169-Eng.php">https://apc.eecs.kth.se/J-2018-3169-Eng.php</a><br />
Email: apc@eecs.kth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/23/postdoc-positions-in-tcs-at-kth-royal-institute-of-technology-apply-by-february-4-2019/"><span class="datestr">at December 23, 2018 10:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6939">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/22/towards-quantum-pcp-a-proof-of-the-nlets-theorem/">Towards Quantum PCP: A Proof of the NLETS Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>By Abhijit Mudigonda, Richard Wang, and Lisa Yang</p>



<p><i>This is part of a series of blog posts for <a href="https://www.boazbarak.org/fall18seminar/">CS 229r: Physics and Computation</a>. In this post, we will talk about progress made towards resolving the quantum PCP conjecture. We’ll briefly talk about the progression from the quantum PCP conjecture to the NLTS conjecture to the NLETS theorem, and then settle on providing a proof of the NLETS theorem. This new proof, due to Nirkhe, Vazirani, and Yuen, makes it clear that the Hamiltonian family used to resolve the NLETS theorem cannot help us in resolving the NLTS conjecture.</i></p>



<h2>Introduction</h2>
<p>We are all too familiar with <b>NP</b> problems. Consider now an upgrade to <b>NP</b> problems, where an omniscient prover (we’ll call this prover Merlin) can send a polynomial-sized proof to a <b>BPP</b> (<a href="https://complexityzoo.uwaterloo.ca/Petting_Zoo#BPP">bounded-error probabilistic polynomial-time</a>) verifier (and we’ll call this verifier Arthur). Now, we have more decision problems in another complexity class, <b>MA</b> (<a href="https://complexityzoo.uwaterloo.ca/Petting_Zoo#MA">Merlin-Arthur</a>). Consider again, the analogue in the quantum realm where now the prover sends over qubits instead and the verifier is in <b>BQP</b> (<a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo:B#bqp">bounded-error quantum polynomial-time</a>). And now we have <b>QMA</b> (<a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo:Q#qma">quantum Merlin-Arthur</a>).</p>

<p>We can show that there is a hierarchy to these classes, where <b>NP</b> <img src="https://s0.wp.com/latex.php?latex=%5Csubseteq+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\subseteq " class="latex" title="\subseteq " /> <b>MA</b> <img src="https://s0.wp.com/latex.php?latex=%5Csubseteq+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\subseteq " class="latex" title="\subseteq " /> <b>QMA</b>.</p>

<p>Our goal is to talk about progress towards a <b>quantum PCP theorem</b> (and since nobody has proved it in the positive or negative, we’ll refer to it as a quantum PCP <i>conjecture</i> for now), so it might be a good idea to first talk about the PCP theorem. Suppose we take a Boolean formula, and we want to verify that it is satisfiable. Then someone comes along and presents us with a certificate — in this case, a satisfying assignment — and we can check in polynomial time that either this is indeed a satisfying assignment to the formula (a correct certificate) or it is not (an incorrect certificate).</p>

<p>But this requires that we check the entire certificate that is presented to us. Now, in comes the <b>PCP Theorem</b> (for <i>probabilistically checkable proofs</i>), which tells us that a certificate can be presented to us such that we can read a constant number of bits from the certificate, and have two things guaranteed: one, if this certificate is correct, then we will never think that it is incorrect even if we are not reading the entire certificate, and two, if we are presented with an incorrect certificate, we will reject it with high probability [<a href="https://windowsontheory.org/feed/#arora2009computational">1</a>].</p>

<p>In short, one formulation of the PCP theorem tells us that, puzzingly, we might not need to read the entirety of a proof in order to be convinced with high probability that it is a good proof or a bad proof. But a natural question arises, which is to ask: is there a quantum analogue of the PCP theorem?</p>

<h2>Progress</h2>

<p>The answer is, we’re still not sure. But to make progress towards resolving this question, we will present the work of <a href="https://arxiv.org/pdf/1802.07419.pdf">Nirkhe, Vazirani, and Yuen</a> in providing an alternate proof of an earlier result of <a href="https://arxiv.org/pdf/1510.02082.pdf">Eldar and Harrow</a> on the NLETS theorem.

</p><p>Before we state the quantum PCP conjecture, it would be helpful to review information about local Hamiltonians and the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian problem. <a href="https://windowsontheory.org/2018/12/20/quantum-hamiltonian-complexity/">A previous blog post by Ben Edelman</a> covers these topics. Now, let’s state the quantum PCP conjecture:</p>

<p><b>(<i>Quantum PCP Conjecture</i>)</b>: It is QMA-hard to decide whether a given local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H+%3D+H_%7B1%7D+%2B+...+%2B+H_%7Bm%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H = H_{1} + ... + H_{m} " class="latex" title="H = H_{1} + ... + H_{m} " /> (where each <img src="https://s0.wp.com/latex.php?latex=%7C%7CH_%7Bi%7D%7C%7C+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="||H_{i}|| \leq 1" class="latex" title="||H_{i}|| \leq 1" />) has ground state energy at most <img src="https://s0.wp.com/latex.php?latex=a+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a " class="latex" title="a " /> or at least <img src="https://s0.wp.com/latex.php?latex=b+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b " class="latex" title="b " /> when <img src="https://s0.wp.com/latex.php?latex=b-a+%5Cgeq+c%7C%7CH%7C%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b-a \geq c||H|| " class="latex" title="b-a \geq c||H|| " /> for some universal constant <img src="https://s0.wp.com/latex.php?latex=c+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c &gt; 0" class="latex" title="c &gt; 0" />.</p>

<p>Recall that MAX-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT being NP-hard corresponds to the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian problem being QMA-hard when <img src="https://s0.wp.com/latex.php?latex=b-a+%5Cgeq+1%2Fpoly%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b-a \geq 1/poly(n)" class="latex" title="b-a \geq 1/poly(n)" />. (We can refer to <a href="https://www.cs.cmu.edu/~odonnell/quantum15/lecture24.pdf">Theorem 4.1 in these scribed notes of Ryan O’Donnell’s lecture</a>, and more specifically to  <a href="https://arxiv.org/pdf/quant-ph/0406180.pdf">Kempe-Kitaev-Regev’s original paper</a> for proof of this fact.) The quantum PCP conjecture asks if this is still the case when the gap is <img src="https://s0.wp.com/latex.php?latex=c%7C%7CH%7C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c||H||" class="latex" title="c||H||" />.</p>

<p>Going back to the PCP theorem, an implication of the PCP theorem is that it is NP-hard to approximate certain problems to within some factor. Just like its classical analogue, the qPCP conjecture can be seen as stating that it is QMA-hard to approximate the ground state energy to a factor better than <img src="https://s0.wp.com/latex.php?latex=c%7C%7CH%7C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c||H||" class="latex" title="c||H||" />.</p>

<h3>Reformulation: NLTS conjecture</h3>
<p>Let’s make the observation that, taking <img src="https://s0.wp.com/latex.php?latex=a+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a " class="latex" title="a " /> to be the ground state energy, the qPCP conjecture sort of says that there exists a family of Hamiltonians for which there is no trivial state (a state generated by a low depth circuit) such that the energy is at most <img src="https://s0.wp.com/latex.php?latex=c%7C%7CH%7C%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c||H|| " class="latex" title="c||H|| " /> above the ground state energy.</p>

<p>Freedman and Hastings came up with an easier goal called the <b>No Low-Energy Trivial States conjecture</b>, or <b>NLTS conjecture</b>. We expect that ground states of local Hamiltonians are sufficiently hard to describe (if NP <img src="https://s0.wp.com/latex.php?latex=%5Cneq+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\neq " class="latex" title="\neq " /> QMA). So low-energy states might not be generated by a quantum circuit of constant depth. More formally:</p>

<p><b>(<i>NLTS Conjecture</i>)</b>: <i>There exists a universal constant <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon &gt; 0 " class="latex" title="\epsilon &gt; 0 " /> and a family of local Hamiltonians <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%28n%29%7D%5C%7D_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{(n)}\}_{n=1}^{\infty} " class="latex" title="\{H^{(n)}\}_{n=1}^{\infty} " /> where <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> acts on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> particles and consists of <img src="https://s0.wp.com/latex.php?latex=m_%7Bn%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{n} " class="latex" title="m_{n} " /> local terms, s.t. any family of states <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi_%7Bn%7D%5Crangle%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{|\psi_{n}\rangle\} " class="latex" title="\{|\psi_{n}\rangle\} " /> satisfying <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi_%7Bn%7D+%7C+H%5E%7B%28n%29%7D+%7C+%5Cpsi_%7Bn%7D%5Crangle+%5Cleq+%5Cepsilon%7C%7CH%5E%7B%28n%29%7D%7C%7C+%2B+%5Clambda_%7Bmin%7D%28H%5E%7B%28n%29%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \psi_{n} | H^{(n)} | \psi_{n}\rangle \leq \epsilon||H^{(n)}|| + \lambda_{min}(H^{(n)}) " class="latex" title="\langle \psi_{n} | H^{(n)} | \psi_{n}\rangle \leq \epsilon||H^{(n)}|| + \lambda_{min}(H^{(n)}) " /> requires circuit depth that grows faster than any constant.</i></p>

<p>To reiterate, if we did have such a family of NLTS Hamiltonians, then it we wouldn’t be able to give “easy proofs” for the minimal energy of a Hamiltonian, because we couldn’t just give a small circuit which produced a low energy state.</p>

<h2>Progress: NLETS theorem</h2>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon " class="latex" title="\epsilon " />-error states are states that differ from the ground state in at most <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon n " class="latex" title="\epsilon n " /> qubits. Now, consider <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error states (which “agree” with the ground state on most qubits). Then for bounded-degree local Hamiltonians (analogously in the classical case, those where each variable participates in a bounded number of clauses), these states are also low energy. So any theorem which applies to low energy states (such as the NLTS conjecture), should also apply to states with <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error (as in the NLETS theorem).</p>

<p>To define low-error states more formally:</p>

<p><b>Definition 2.1</b> (<img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon " class="latex" title="\epsilon " />-error states): <i>Let <img src="https://s0.wp.com/latex.php?latex=%5Crho%2C+%5Csigma+%5Cin+D%28%28%5Cmathbb%7BC%7D%5E%7Bd%7D%29%5E%7B%5Cotimes+n%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho, \sigma \in D((\mathbb{C}^{d})^{\otimes n}) " class="latex" title="\rho, \sigma \in D((\mathbb{C}^{d})^{\otimes n}) " /> (the space of positive semidefinite operators of trace norm equal to 1 on <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BC%7D%5E%7Bd%7D%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\mathbb{C}^{d})^{\otimes n}" class="latex" title="(\mathbb{C}^{d})^{\otimes n}" />). Let <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> be a local Hamiltonian acting on <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BC%7D%5E%7Bd%7D%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\mathbb{C}^{d})^{\otimes n}" class="latex" title="(\mathbb{C}^{d})^{\otimes n}" />. Then:</i></p>

<p></p><ul>
    <li><img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> is an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error state of <img src="https://s0.wp.com/latex.php?latex=%5Crho+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho " class="latex" title="\rho " /> if <img src="https://s0.wp.com/latex.php?latex=%5Cexists+S+%5Csubseteq+%5Bn%5D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exists S \subseteq [n] " class="latex" title="\exists S \subseteq [n] " /> of size at most <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon n " class="latex" title="\epsilon n " /> s.t. <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D_%7BS%7D%28%5Crho%29+%3D+%5Ctext%7BTr%7D_%7BS%7D%28%5Csigma%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}_{S}(\rho) = \text{Tr}_{S}(\sigma)" class="latex" title="\text{Tr}_{S}(\rho) = \text{Tr}_{S}(\sigma)" />.</li>
    <li><img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> is an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error state for <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> if <img src="https://s0.wp.com/latex.php?latex=%5Cexists+%5Crho+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exists \rho " class="latex" title="\exists \rho " /> s.t. <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28H%5Crho%29+%3D+%5Clambda_%7Bmin%7D%28H%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(H\rho) = \lambda_{min}(H) " class="latex" title="\text{Tr}(H\rho) = \lambda_{min}(H) " /> and <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> is an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error state for <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />.</li>
</ul><p></p>

<p>Here, see that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D_%7BS%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}_{S} " class="latex" title="\text{Tr}_{S} " /> is just the partial trace on some subset of integers <img src="https://s0.wp.com/latex.php?latex=S+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S " class="latex" title="S " />, like we’re tracing out or “disregarding” some subset of <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qubits.</p>

<p>In 2017, Eldar and Harrow showed the following result which is the NLETS theorem.</p>

<p><b>Theorem 1</b> (NLETS Theorem): <i>There exists a family of 16-local Hamiltonians <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%28n%29%7D%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{(n)}\} " class="latex" title="\{H^{(n)}\} " /> s.t. any family of <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error states <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5CPhi_%7Bn%7D%5Crangle%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{|\Phi_{n}\rangle\} " class="latex" title="\{|\Phi_{n}\rangle\} " /> for <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%28n%29%7D%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{(n)}\} " class="latex" title="\{H^{(n)}\} " /> requires circuit depth <img src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog+n%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega(\log n) " class="latex" title="\Omega(\log n) " /> where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+10%5E%7B-9%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon = 10^{-9}" class="latex" title="\epsilon = 10^{-9}" />.</i></p>

<p>In the next two sections, we will provide background for an alternate proof of the NLETS theorem due to Nirkhe, Vazirani, and Yuen. After this, we will explain why the proof of NLETS cannot be used to prove NLTS, since the local Hamiltonian family we construct for NLETS can be linearized. Nirkhe, Vazirani, and Yuen’s proof of NLETS makes use of the Feynman-Kitaev clock Hamiltonian corresponding to the circuit generating the cat state (Eldar and Harrow make use of the Tillich-Zemor hypergraph product construction; refer to section 8 of <a href="https://arxiv.org/pdf/1510.02082.pdf">their paper</a>). What is this circuit? It is this one:</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/cat_state.png?w=326&amp;h=210" alt="" height="210" class="wp-image-6977" width="326" />Image from [2]</figure></div>



<p>First, we apply the Hadamard gate (drawn as <img src="https://s0.wp.com/latex.php?latex=%5Cboxed%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\boxed{H}" class="latex" title="\boxed{H}" />) which maps the first qubit <img src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle+%5Crightarrow+%5Cfrac%7B%7C0%5Crangle+%2B+%7C1%5Crangle%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|0\rangle \rightarrow \frac{|0\rangle + |1\rangle}{\sqrt{2}}" class="latex" title="|0\rangle \rightarrow \frac{|0\rangle + |1\rangle}{\sqrt{2}}" />. Then we can think of the CNOT gates (drawn as <img src="https://s0.wp.com/latex.php?latex=%5Cbullet-%5Coplus&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bullet-\oplus" class="latex" title="\bullet-\oplus" />) as propagating whatever happens to the first qubit to the rest of the qubits. If we had the first qubit mapping to 0, then the rest of the qubits map to 0, and likewise for 1. This generates the cat state <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7BCAT%7D_%7Bn%7D%5Crangle+%3D+%5Cfrac%7B%7C0%5Crangle%5E%7B%5Cotimes+n%7D+%2B+%7C1%5Crangle%5E%7B%5Cotimes+n%7D%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{CAT}_{n}\rangle = \frac{|0\rangle^{\otimes n} + |1\rangle^{\otimes n}}{\sqrt{2}}" class="latex" title="|\textsf{CAT}_{n}\rangle = \frac{|0\rangle^{\otimes n} + |1\rangle^{\otimes n}}{\sqrt{2}}" />, which is highly entangled.</p>

<p>Why do we want a highly entangled state? Roughly our intuition for using the cat state is this: if the ground state of a Hamiltonian is highly entangled, then any quantum circuit which generates it has non-trivial depth. So if our goal is to show the existence of local Hamiltonians which have low energy or low error states that need deep circuits to generate, it makes sense to use a highly entangled state like the cat state.</p>

<h2>Quantum circuits</h2>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/operators.png?w=446&amp;h=221" alt="" height="221" class="wp-image-6978" width="446" />Image from [2]</figure></div>



<p>(We’ll write that the state of a qudit – a generalization of a qubit to more than two dimensions, and in this case <img src="https://s0.wp.com/latex.php?latex=q+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q " class="latex" title="q " /> dimensions – is a vector in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5E%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{C}^{q}" class="latex" title="\mathbb{C}^{q}" />. In our diagram above, we’ll see 4 qudits, labelled appropriately.)</p>

<p>Let’s briefly cover the definitions for the quantum circuits we’ll be using.</p>

<p>Let <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> be a unitary operator acting on a system of <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qudits (in other words, acting on <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BC%7D%5E%7Bq%7D%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\mathbb{C}^{q})^{\otimes n}" class="latex" title="(\mathbb{C}^{q})^{\otimes n}" />), where <img src="https://s0.wp.com/latex.php?latex=U+%3D+U_%7Bm%7D+%5Chdots+U_%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = U_{m} \hdots U_{1}" class="latex" title="U = U_{m} \hdots U_{1}" />. Here, each <img src="https://s0.wp.com/latex.php?latex=U_%7Bi%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U_{i} " class="latex" title="U_{i} " /> is a unitary operator (a gate) acting on at most two qudits, and <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> is a product of <img src="https://s0.wp.com/latex.php?latex=m+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m " class="latex" title="m " /> such operators.</p>

<p>If there exists a partition <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> into products of non-overlapping two-qudit unitaries (we call these layers and denote them as <img src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D+%3D+%5Cbigotimes_%7Bj%7DU_%7Bij%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{i} = \bigotimes_{j}U_{ij}" class="latex" title="L_{i} = \bigotimes_{j}U_{ij}" />, where each <img src="https://s0.wp.com/latex.php?latex=U_%7Bj%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U_{j} " class="latex" title="U_{j} " /> here is in layer <img src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{i}" class="latex" title="L_{i}" />) such that <img src="https://s0.wp.com/latex.php?latex=U+%3D+L_%7Bd%7D+%5Chdots+L_%7B1%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = L_{d} \hdots L_{1} " class="latex" title="U = L_{d} \hdots L_{1} " /> then we say <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> has <img src="https://s0.wp.com/latex.php?latex=d+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d " class="latex" title="d " /> layers.</p>

<p>In other words, <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> has size <img src="https://s0.wp.com/latex.php?latex=m+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m " class="latex" title="m " /> and circuit depth <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />.</p>

<h3>Lightcones, effect zones, shadow zones</h3>
<p>Consider <img src="https://s0.wp.com/latex.php?latex=U+%3D+L_%7Bd%7D+%5Chdots+L_%7B1%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = L_{d} \hdots L_{1} " class="latex" title="U = L_{d} \hdots L_{1} " /> and <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> an operator.</p>

<p>For <img src="https://s0.wp.com/latex.php?latex=j+%3C+d+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j &lt; d " class="latex" title="j &lt; d " /> define <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(j)} " class="latex" title="K^{(j)} " /> as the gates in layer <img src="https://s0.wp.com/latex.php?latex=j+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j " class="latex" title="j " /> whose supports overlap that of any gate in <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28j%2B1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(j+1)}" class="latex" title="K^{(j+1)}" />, …, <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28d%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(d)} " class="latex" title="K^{(d)} " /> or with <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" />.</p>

<p><b>Definition 3.1</b> (lightcone): <i>The <i>lightcone</i> of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> with respect to <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> is the union of <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28j%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(j)}" class="latex" title="K^{(j)}" />: <img src="https://s0.wp.com/latex.php?latex=K_%7BU%7D+%5Ctriangleq+%5Cbigcup_%7Bj%7D+K%5E%7B%28j%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K_{U} \triangleq \bigcup_{j} K^{(j)}" class="latex" title="K_{U} \triangleq \bigcup_{j} K^{(j)}" />.</i></p>

<p>So we can think of the lightcone as the set of gates spreading out of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> all the way to the first layer of the circuit. In our diagram, the lightcone of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> is the dash-dotted region. We have <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%283%29%7D+%3D+%5Cvarnothing&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(3)} = \varnothing" class="latex" title="K^{(3)} = \varnothing" />, <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%282%29%7D+%3D+%5C%7BU_%7B21%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(2)} = \{U_{21}\}" class="latex" title="K^{(2)} = \{U_{21}\}" />, and <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%281%29%7D+%3D+%5C%7BU_%7B11%7D%2C+U_%7B12%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K^{(1)} = \{U_{11}, U_{12}\}" class="latex" title="K^{(1)} = \{U_{11}, U_{12}\}" />.</p>

<p>We also want a definition for what comes back from the lightcone: the set of gates from the first layer (the widest part of the cone) back to the last layer.</p>

<p>Define <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%281%29%7D+%3D+K%5E%7B%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(1)} = K^{(1)}" class="latex" title="E^{(1)} = K^{(1)}" />. For <img src="https://s0.wp.com/latex.php?latex=j+%5Cgeq+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j \geq 2" class="latex" title="j \geq 2" />, let <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(j)} " class="latex" title="E^{(j)} " /> be the set of gates whose supports overlap with any gate in <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%28j-1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(j-1)}" class="latex" title="E^{(j-1)}" />.</p>

<p><b>Definition 3.2</b> (effect zone): <i>The <i>effect zone</i> of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> with respect to <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> is the union <img src="https://s0.wp.com/latex.php?latex=E_%7BU%7D%28A%29+%5Ctriangleq+%5Cbigcup_%7Bj%7D+E%5E%7B%28j%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E_{U}(A) \triangleq \bigcup_{j} E^{(j)}" class="latex" title="E_{U}(A) \triangleq \bigcup_{j} E^{(j)}" />.</i></p>

<p>In our diagram, see that <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%281%29%7D+%3D+%5C%7BU_%7B11%7D%2C+U_%7B12%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(1)} = \{U_{11}, U_{12}\}" class="latex" title="E^{(1)} = \{U_{11}, U_{12}\}" />, <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%282%29%7D+%3D+%5C%7BU_%7B21%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(2)} = \{U_{21}\}" class="latex" title="E^{(2)} = \{U_{21}\}" />, and <img src="https://s0.wp.com/latex.php?latex=E%5E%7B%283%29%7D+%3D+%5C%7BU_%7B31%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E^{(3)} = \{U_{31}\}" class="latex" title="E^{(3)} = \{U_{31}\}" />. The effect zone of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> is the dotted region.</p>

<p><b>Definition 3.3</b> (shadow of the effect zone): <i>The <i>shadow of the effect zone</i> <img src="https://s0.wp.com/latex.php?latex=W_%7BU%7D%28A%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{U}(A) " class="latex" title="W_{U}(A) " /> of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> with respect to <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> is the set of qudits acted on by the gates in the effect zone.</i></p>

<p>In our diagram, the first three qudits are effected by gates in the effect zone. So <img src="https://s0.wp.com/latex.php?latex=W_%7BU%7D%28A%29+%3D+%5C%7B1%2C+2%2C+3%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{U}(A) = \{1, 2, 3\}" class="latex" title="W_{U}(A) = \{1, 2, 3\}" />.</p>

<p>Given all of these definitions, we make the following claim which will be important later, in a proof of a generalization of NLETS.</p>

<p><b><a id="claim3"></a>Claim 3.1</b> (Disjoint lightcones): <i>Let <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> be a circuit and <img src="https://s0.wp.com/latex.php?latex=A%2C+B+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A, B " class="latex" title="A, B " /> operators. If the qudits <img src="https://s0.wp.com/latex.php?latex=B+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B " class="latex" title="B " /> acts on are disjoint from <img src="https://s0.wp.com/latex.php?latex=W_%7BU%7D%28A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W_{U}(A)" class="latex" title="W_{U}(A)" />, then the lightcones of <img src="https://s0.wp.com/latex.php?latex=A+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A " class="latex" title="A " /> and <img src="https://s0.wp.com/latex.php?latex=B+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B " class="latex" title="B " /> in <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> are disjoint.</i></p>

<h2>Toward the Feynman-Kitaev clock</h2>
<p>Now we’ll give some definitions that will become necessary when we make use of the Feynman-Kitaev Hamiltonian in our later proofs.</p>

<p>Let’s define a unary clock. It will basically help us determine whatever happened at any time little <img src="https://s0.wp.com/latex.php?latex=t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t " class="latex" title="t " /> along the total time big <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. Let <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bunary%7D%28t%2C+T%29%5Crangle+%3D+%7C0%5Crangle%5E%7B%5Cotimes%28T-t%29%7D+%5Cotimes+%7C1%5Crangle%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{unary}(t, T)\rangle = |0\rangle^{\otimes(T-t)} \otimes |1\rangle^{\otimes t}" class="latex" title="|\textsf{unary}(t, T)\rangle = |0\rangle^{\otimes(T-t)} \otimes |1\rangle^{\otimes t}" />. For our purposes today, we won’t worry about higher dimensional clocks. So we’ll write <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bclock%7D_%7Bk%7D%28t%2C+T%29%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{clock}_{k}(t, T)\rangle" class="latex" title="|\textsf{clock}_{k}(t, T)\rangle" />, but we’ll really only consider the case where <img src="https://s0.wp.com/latex.php?latex=k+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 1" class="latex" title="k = 1" />, which corresponds to <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bunary%7D%28t%2C+T%29%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{unary}(t, T)\rangle" class="latex" title="|\textsf{unary}(t, T)\rangle" />. For simplicity’s sake, we will henceforth just write <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bunary%7D%28t%29%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{unary}(t)\rangle" class="latex" title="|\textsf{unary}(t)\rangle" />.</p>

<p>Our goal is to construct something a little similar to the tableaux in the Cook-Levin theorem, so we also want to define a history state:</p>

<p><b>Definition 4.1</b> (History state): <i>Let <img src="https://s0.wp.com/latex.php?latex=C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C " class="latex" title="C " /> be a quantum circuit that acts on a witness register and an ancilla register. Let <img src="https://s0.wp.com/latex.php?latex=C_%7B1%7D%2C+...%2C+C_%7BT%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_{1}, ..., C_{T} " class="latex" title="C_{1}, ..., C_{T} " /> denote the sequence of two-local gates in <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />. Then for all <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5Cmathbb%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \in \mathbb{N}" class="latex" title="k \in \mathbb{N}" />, a state <img src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\Psi\rangle " class="latex" title="|\Psi\rangle " /> is a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-dimensional history state of <img src="https://s0.wp.com/latex.php?latex=C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C " class="latex" title="C " /> if:</i></p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5CPsi%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7BT%2B1%7D%7D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7C%5Ctextsf%7Bclock%7D_%7Bk%7D%28t%2C+T%29%5Crangle+%5Cotimes+%7C%5Cpsi_%7Bt%7D%5Crangle%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{T+1}}\sum_{t=0}^{T}|\textsf{clock}_{k}(t, T)\rangle \otimes |\psi_{t}\rangle\end{aligned} " class="latex" title="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{T+1}}\sum_{t=0}^{T}|\textsf{clock}_{k}(t, T)\rangle \otimes |\psi_{t}\rangle\end{aligned} " /></p></div>

<p>where we have the clock state to keep track of time and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_{t} " class="latex" title="\psi_{t} " /> is some state such that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_%7Bt%7D%5Crangle+%3D+C_%7Bt%7D%7C%5Cpsi_%7Bt-1%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi_{t}\rangle = C_{t}|\psi_{t-1}\rangle " class="latex" title="|\psi_{t}\rangle = C_{t}|\psi_{t-1}\rangle " /> and <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_%7B0%7D%5Crangle+%3D+%7C%5Cxi%5Crangle_%7Bwitness%7D+%5Cotimes+%7C0%5Crangle_%7Bancilla%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi_{0}\rangle = |\xi\rangle_{witness} \otimes |0\rangle_{ancilla}" class="latex" title="|\psi_{0}\rangle = |\xi\rangle_{witness} \otimes |0\rangle_{ancilla}" />. With this construction, we should be able to make a measurement to get back the state at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />.</p>

<h2>Proof of NLETS</h2>
<p>We provide a proof of (a simplified case of) the NLETS theorem proved by Nirkhe, Vazirani, and Yuen in [<a href="https://windowsontheory.org/feed/#nirkhe2018approximate">2</a>].</p>

<p><b>Theorem 2</b> (NLETS): <i>There exists a family of <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="3" class="latex" title="3" />-local Hamiltonians <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%28n%29%7D%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{(n)}\} " class="latex" title="\{H^{(n)}\} " /> on a line (Each Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> can be defined on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> particles arranged on a line such that each local Hamiltonian acts on a particle and its two neighbors) such that for all <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \in \mathbb{N}" class="latex" title="n \in \mathbb{N}" />, the circuit depth of any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error ground state for <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> is at least logarithmic in <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " />.</i></p>

<p>First, we’ll show the circuit lower bound.  Then we’ll explain why these Hamiltonians can act on particles on a line and what this implies about the potential of these techniques for proving NLTS.</p>

<p><i>Proof</i>: We will use the <b>Feynman-Kitaev clock construction</b> to construct a <img src="https://s0.wp.com/latex.php?latex=5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="5" class="latex" title="5" />-local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H}^{(n)} " class="latex" title="\mathcal{H}^{(n)} " /> for the circuit <img src="https://s0.wp.com/latex.php?latex=C_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n" class="latex" title="C_n" />: <img src="https://s0.wp.com/latex.php?latex=%7C0%5En%5Crangle+%5Cto+%7C%5Ctextsf%7BCAT%7D_n%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|0^n\rangle \to |\textsf{CAT}_n\rangle " class="latex" title="|0^n\rangle \to |\textsf{CAT}_n\rangle " />.</p>

<p>Fix <img src="https://s0.wp.com/latex.php?latex=n+%5Cin+%5Cmathbb%7BN%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n \in \mathbb{N} " class="latex" title="n \in \mathbb{N} " /> and let <img src="https://s0.wp.com/latex.php?latex=C_n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n " class="latex" title="C_n " /> have size <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />.  The Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> acts on <img src="https://s0.wp.com/latex.php?latex=T%2Bn+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T+n " class="latex" title="T+n " /> qubits and consists of several local terms depending on <img src="https://s0.wp.com/latex.php?latex=C_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n" class="latex" title="C_n" />:</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathcal%7BH%7D+%3D+H_%7Bin%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET+H_t+%2B+H_%7Bout%7D+%2B+H_%7Bstab%7D%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}\mathcal{H} = H_{in} + \sum_{t=1}^T H_t + H_{out} + H_{stab}\end{aligned} " class="latex" title="\begin{aligned}\mathcal{H} = H_{in} + \sum_{t=1}^T H_t + H_{out} + H_{stab}\end{aligned} " /></p></div>

<p>We can think of a <img src="https://s0.wp.com/latex.php?latex=T%2Bn+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T+n " class="latex" title="T+n " /> qubit state as representing a <img src="https://s0.wp.com/latex.php?latex=T+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T " class="latex" title="T " /> step computation on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qubits (i.e. for each time <img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5B0%2CT%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \in [0,T]" class="latex" title="t \in [0,T]" />, we have a <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> bit computation state <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7Bstate%7D_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\textsf{state}_t " class="latex" title="\textsf{state}_t " /> of <img src="https://s0.wp.com/latex.php?latex=C_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n" class="latex" title="C_n" />).  Intuitively, a <img src="https://s0.wp.com/latex.php?latex=T%2Bn+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T+n " class="latex" title="T+n " /> qubit state has energy <img src="https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 " class="latex" title="0 " /> with respect to <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> iff it is the history state of <img src="https://s0.wp.com/latex.php?latex=C_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n" class="latex" title="C_n" />.  This is because <img src="https://s0.wp.com/latex.php?latex=H_%7Bin%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{in} " class="latex" title="H_{in} " /> checks that at time <img src="https://s0.wp.com/latex.php?latex=t%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=0" class="latex" title="t=0" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7Bstate%7D_0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\textsf{state}_0 " class="latex" title="\textsf{state}_0 " /> consists of the input <img src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5En+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|0\rangle^n " class="latex" title="|0\rangle^n " /> to <img src="https://s0.wp.com/latex.php?latex=C_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n" class="latex" title="C_n" />.  Each <img src="https://s0.wp.com/latex.php?latex=H_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_t " class="latex" title="H_t " /> checks that <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7Bstate%7D_%7Bt%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\textsf{state}_{t} " class="latex" title="\textsf{state}_{t} " /> proceed correctly from <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7Bstate%7D_%7Bt-1%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\textsf{state}_{t-1} " class="latex" title="\textsf{state}_{t-1} " /> (i.e. that the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />th gate of <img src="https://s0.wp.com/latex.php?latex=C_n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n " class="latex" title="C_n " /> is applied correctly).  Then <img src="https://s0.wp.com/latex.php?latex=H_%7Bout%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{out} " class="latex" title="H_{out} " /> checks that at time <img src="https://s0.wp.com/latex.php?latex=t%3DT&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=T" class="latex" title="t=T" />, the output is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />.  Finally, <img src="https://s0.wp.com/latex.php?latex=H_%7Bstab%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{stab} " class="latex" title="H_{stab} " /> checks that the <img src="https://s0.wp.com/latex.php?latex=T%2Bn+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T+n " class="latex" title="T+n " /> qubit state is a superposition only over states where the first <img src="https://s0.wp.com/latex.php?latex=T+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T " class="latex" title="T " /> qubits represent “correct times” (i.e. a unary clock state where time <img src="https://s0.wp.com/latex.php?latex=t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t " class="latex" title="t " /> is represented by <img src="https://s0.wp.com/latex.php?latex=T-t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T-t " class="latex" title="T-t " /> zeros followed by <img src="https://s0.wp.com/latex.php?latex=t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t " class="latex" title="t " /> ones).</p>

<p>Therefore, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> has a unique ground state, the history state of <img src="https://s0.wp.com/latex.php?latex=C_n%7C0%5En%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n|0^n\rangle" class="latex" title="C_n|0^n\rangle" />, with energy <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" />:</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5CPsi%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%2B1%7D%7D%5Csum_%7Bt%3D0%7D%5En+%7C%5Ctextsf%7Bunary%7D%28t%29%5Crangle%5Cotimes+%7C%5Cpsi_t%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%2B1%7D%7D%5Csum_%7Bt%3D0%7D%5En+%7C%5Ctextsf%7Bunary%7D%28t%29%5Crangle%5Cotimes%7C%5Ctextsf%7BCAT%7D_%7Bt%7D%5Crangle%5Cotimes+%7C0%5Crangle%5E%7B%5Cotimes+%28n-t%29%7D%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\textsf{unary}(t)\rangle\otimes |\psi_t\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\textsf{unary}(t)\rangle\otimes|\textsf{CAT}_{t}\rangle\otimes |0\rangle^{\otimes (n-t)}\end{aligned} " class="latex" title="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\textsf{unary}(t)\rangle\otimes |\psi_t\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\textsf{unary}(t)\rangle\otimes|\textsf{CAT}_{t}\rangle\otimes |0\rangle^{\otimes (n-t)}\end{aligned} " /></p></div>

<p>Later we will show how to transform <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> into a Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qutrits on a line.  Intuitively, the structure of <img src="https://s0.wp.com/latex.php?latex=C_n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_n " class="latex" title="C_n " /> allows us to fuse the <img src="https://s0.wp.com/latex.php?latex=T%3Dn+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T=n " class="latex" title="T=n " /> time qubits and <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> state qubits and represent unused state qubits by <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2" class="latex" title="2" />.  For the Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />, the ground state becomes</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5CPsi%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%2B1%7D%7D%5Csum_%7Bt%3D0%7D%5En+%7C%5Cpsi_t%5Crangle+%3D+%5Csum_%7Bt%3D0%7D%5En+%7C%5Ctextsf%7BCAT%7D_%7Bt%7D%5Crangle%5Cotimes%7C2%5Crangle%5E%7B%5Cotimes%28n-t%29%7D%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\psi_t\rangle = \sum_{t=0}^n |\textsf{CAT}_{t}\rangle\otimes|2\rangle^{\otimes(n-t)}\end{aligned} " class="latex" title="\begin{aligned}|\Psi\rangle = \frac{1}{\sqrt{n+1}}\sum_{t=0}^n |\psi_t\rangle = \sum_{t=0}^n |\textsf{CAT}_{t}\rangle\otimes|2\rangle^{\otimes(n-t)}\end{aligned} " /></p></div>

<p>For the rest of this proof, we work with respect to <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />.</p>

<p>Let <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> be an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error state and let <img src="https://s0.wp.com/latex.php?latex=S+%5Csubseteq+%5Bn%5D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S \subseteq [n] " class="latex" title="S \subseteq [n] " /> be the subset of qutrits such that <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D_S%28%5Csigma%29+%3D+%5Ctext%7BTr%7D_S%28%7C%5CPsi%5Crangle%5Clangle%5CPsi%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}_S(\sigma) = \text{Tr}_S(|\Psi\rangle\langle\Psi|)" class="latex" title="\text{Tr}_S(\sigma) = \text{Tr}_S(|\Psi\rangle\langle\Psi|)" />.  We define two projection operators which, when applied to <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> alone, produce nontrivial measurements, but when applied to <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> together, produce trivial measurements.</p>

<p><b>Definition 5.1</b>: <i>For any <img src="https://s0.wp.com/latex.php?latex=i%5Cin%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i\in[n]" class="latex" title="i\in[n]" />, the projection operator</i></p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7DA_i+%3D+%7C0%5Crangle%5Clangle+0%7C_i+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}A_i = |0\rangle\langle 0|_i \end{aligned} " class="latex" title="\begin{aligned}A_i = |0\rangle\langle 0|_i \end{aligned} " /></p></div>

<p><i>projects onto the subspace spanned by <img src="https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 " class="latex" title="0 " /> on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th qutrit.</i></p>

<p><i>For any <img src="https://s0.wp.com/latex.php?latex=j%5Cin%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j\in[n]" class="latex" title="j\in[n]" />, the projection operator</i></p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_j+%3D+%7C1%5Crangle%5Clangle+1%7C_i%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} B_j = |1\rangle\langle 1|_i\end{aligned} " class="latex" title="\begin{aligned} B_j = |1\rangle\langle 1|_i\end{aligned} " /></p></div>

<p><i>projects onto the subspace spanned by <img src="https://s0.wp.com/latex.php?latex=1+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 " class="latex" title="1 " /> on the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />th qutrit.</i></p>

<p><b>Claim 5.1</b>:
<i>For <img src="https://s0.wp.com/latex.php?latex=i%5Cnot%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i\not\in S" class="latex" title="i\not\in S" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28A_i%5Csigma%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B-i%7D%7B2%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(A_i\sigma) = \frac{1}{2} + \frac{-i}{2(n+1)}" class="latex" title="\text{Tr}(A_i\sigma) = \frac{1}{2} + \frac{-i}{2(n+1)}" />.  For <img src="https://s0.wp.com/latex.php?latex=j%5Cnot%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j\not\in S" class="latex" title="j\not\in S" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28B_j%5Csigma%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B-j%7D%7B2%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(B_j\sigma) = \frac{1}{2} + \frac{-j}{2(n+1)}" class="latex" title="\text{Tr}(B_j\sigma) = \frac{1}{2} + \frac{-j}{2(n+1)}" />.  Note that these values are positive for any <img src="https://s0.wp.com/latex.php?latex=i%2Cj%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i,j\in [n]" class="latex" title="i,j\in [n]" />.</i></p>

<p><i>Proof</i>: If <img src="https://s0.wp.com/latex.php?latex=i+%5Cnot%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \not\in S" class="latex" title="i \not\in S" />, then measurements on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th qutrit are the same for <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma " class="latex" title="\sigma " /> and <img src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle%5Clangle%5CPsi%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\Psi\rangle\langle\Psi|" class="latex" title="|\Psi\rangle\langle\Psi|" />.</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+++++%5Ctext%7BTr%7D%28A_i%5Csigma%29+%26%3D+%5Ctext%7BTr%7D%28A_i%7C%5CPsi%5Crangle%5Clangle%5CPsi%7C%29%5C%5C+++++%26%3D+%5Ctext%7BTr%7D%5Cleft%28A_i+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Csum_%7Bt%2Ct%27%7D%7C%5Cpsi_t%5Crangle%5Clangle%5Cpsi_%7Bt%27%7D%7C%5Cright%29%5C%5C+++++%26%3D+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Csum_%7Bt%2Ct%27%7D+%5Ctext%7BTr%7D%28A_i%7C%5Cpsi_t%5Crangle%5Clangle%5Cpsi_%7Bt%27%7D%7C%29+++%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}     \text{Tr}(A_i\sigma) &amp;= \text{Tr}(A_i|\Psi\rangle\langle\Psi|)\\     &amp;= \text{Tr}\left(A_i \frac{1}{n+1}\sum_{t,t'}|\psi_t\rangle\langle\psi_{t'}|\right)\\     &amp;= \frac{1}{n+1}\sum_{t,t'} \text{Tr}(A_i|\psi_t\rangle\langle\psi_{t'}|)   \end{aligned} " class="latex" title="\begin{aligned}     \text{Tr}(A_i\sigma) &amp;= \text{Tr}(A_i|\Psi\rangle\langle\Psi|)\\     &amp;= \text{Tr}\left(A_i \frac{1}{n+1}\sum_{t,t'}|\psi_t\rangle\langle\psi_{t'}|\right)\\     &amp;= \frac{1}{n+1}\sum_{t,t'} \text{Tr}(A_i|\psi_t\rangle\langle\psi_{t'}|)   \end{aligned} " /></p></div>

<p>If <img src="https://s0.wp.com/latex.php?latex=t%3Dt%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t=t'" class="latex" title="t=t'" />, then any <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qutrit pure state cannot have nonzero weight in both <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_t " class="latex" title="\psi_t " /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt%27%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_{t'} " class="latex" title="\psi_{t'} " /> (every pure state ends in some number of <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2" class="latex" title="2" />s which tells which <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_t " class="latex" title="\psi_t " /> (if any) it can be a part of). Therefore,</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+++++%5Ctext%7BTr%7D%28A_i%5Csigma%29+%3D+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Csum_%7Bt%7D+%5Ctext%7BTr%7D%28A_i%7C%5Cpsi_t%5Crangle%5Clangle%5Cpsi_%7Bt%7D%7C%29+%3D+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Csum_t+%5Clangle+%5Cpsi_t%7CA_i%7C%5Cpsi_t%5Crangle+%5Censpace.+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}     \text{Tr}(A_i\sigma) = \frac{1}{n+1}\sum_{t} \text{Tr}(A_i|\psi_t\rangle\langle\psi_{t}|) = \frac{1}{n+1}\sum_t \langle \psi_t|A_i|\psi_t\rangle \enspace. \end{aligned} " class="latex" title="\begin{aligned}     \text{Tr}(A_i\sigma) = \frac{1}{n+1}\sum_{t} \text{Tr}(A_i|\psi_t\rangle\langle\psi_{t}|) = \frac{1}{n+1}\sum_t \langle \psi_t|A_i|\psi_t\rangle \enspace. \end{aligned} " /></p></div>

<p>If <img src="https://s0.wp.com/latex.php?latex=i+%5Cle+t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \le t" class="latex" title="i \le t" />, then projecting onto the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th qutrit gives <img src="https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 " class="latex" title="0 " /> with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" />. Therefore, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28A_i%5Csigma%29+%3D+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Cleft%28%5Cfrac%7Bn-i%2B1%7D%7B2%7D%5Cright%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B-i%7D%7B2%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(A_i\sigma) = \frac{1}{n+1}\left(\frac{n-i+1}{2}\right) = \frac{1}{2} + \frac{-i}{2(n+1)}" class="latex" title="\text{Tr}(A_i\sigma) = \frac{1}{n+1}\left(\frac{n-i+1}{2}\right) = \frac{1}{2} + \frac{-i}{2(n+1)}" />.</p>

<p>Similarly, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28B_j%5Csigma%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B-j%7D%7B2%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(B_j\sigma) = \frac{1}{2} + \frac{-j}{2(n+1)}" class="latex" title="\text{Tr}(B_j\sigma) = \frac{1}{2} + \frac{-j}{2(n+1)}" />. <img src="https://s0.wp.com/latex.php?latex=%5Csquare+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\square " class="latex" title="\square " /></p>

<p><b>Claim 5.2</b>: <i>For <img src="https://s0.wp.com/latex.php?latex=i%2Cj+%5Cnot%5Cin+S+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i,j \not\in S " class="latex" title="i,j \not\in S " /> such that <img src="https://s0.wp.com/latex.php?latex=i+%3C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i &lt; j" class="latex" title="i &lt; j" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28A_i+%5Cotimes+B_j+%5Csigma%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\text{Tr}(A_i \otimes B_j \sigma) = 0" class="latex" title="\text{Tr}(A_i \otimes B_j \sigma) = 0" />.</i></p>

<p><i>Proof</i>:
As before, we can calculate</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7BTr%7D%28A_i+%5Cotimes+B_j+%5Csigma%29+%26%3D+%5Ctext%7BTr%7D%28A_i+%5Cotimes+B_j+%7C%5CPsi%5Crangle+%5Clangle%5CPsi%7C%29+%3D+%5Cfrac%7B1%7D%7Bn%2B1%7D%5Csum_t+%5Clangle%5Cpsi_t%7CA_i%5Cotimes+B_j%7C%5Cpsi_t%5Crangle+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \text{Tr}(A_i \otimes B_j \sigma) &amp;= \text{Tr}(A_i \otimes B_j |\Psi\rangle \langle\Psi|) = \frac{1}{n+1}\sum_t \langle\psi_t|A_i\otimes B_j|\psi_t\rangle \end{aligned} " class="latex" title="\begin{aligned} \text{Tr}(A_i \otimes B_j \sigma) &amp;= \text{Tr}(A_i \otimes B_j |\Psi\rangle \langle\Psi|) = \frac{1}{n+1}\sum_t \langle\psi_t|A_i\otimes B_j|\psi_t\rangle \end{aligned} " /></p></div>

<p>If <img src="https://s0.wp.com/latex.php?latex=j+%3E+t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j &gt; t" class="latex" title="j &gt; t" />, then the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />th qutrit of <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi_t " class="latex" title="\psi_t " /> is <img src="https://s0.wp.com/latex.php?latex=2+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2 " class="latex" title="2 " /> so <img src="https://s0.wp.com/latex.php?latex=B_j%7C%5Cpsi_t%5Crangle+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B_j|\psi_t\rangle = 0" class="latex" title="B_j|\psi_t\rangle = 0" />. If <img src="https://s0.wp.com/latex.php?latex=j+%5Cle+t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j \le t" class="latex" title="j \le t" />, then <img src="https://s0.wp.com/latex.php?latex=A_i+%5Cotimes+B_j%7C%5Cpsi_t%5Crangle+%3D+0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_i \otimes B_j|\psi_t\rangle = 0 " class="latex" title="A_i \otimes B_j|\psi_t\rangle = 0 " /> because the first <img src="https://s0.wp.com/latex.php?latex=t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t " class="latex" title="t " /> qutrits of <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_t%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi_t\rangle " class="latex" title="|\psi_t\rangle " /> contain the <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7BCAT%7D_%7Bt%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{CAT}_{t}\rangle " class="latex" title="|\textsf{CAT}_{t}\rangle " /> state so under any measurement, the <img src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i " class="latex" title="i " /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" />th qutrits must be the same. <img src="https://s0.wp.com/latex.php?latex=%5Csquare+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\square " class="latex" title="\square " /></p>

<p>Now we use these claims to prove a circuit lower bound.  Let <img src="https://s0.wp.com/latex.php?latex=U+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U " class="latex" title="U " /> be a circuit generating (a state with density matrix) <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma" class="latex" title="\sigma" />.  Let <img src="https://s0.wp.com/latex.php?latex=d+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d " class="latex" title="d " /> be the depth of <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" />.</p>

<p>Consider some <img src="https://s0.wp.com/latex.php?latex=i%5Cnot%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i\not\in S" class="latex" title="i\not\in S" />.  For any operator acting on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th qutrit, its lightcone consists of at most <img src="https://s0.wp.com/latex.php?latex=2%5Ed+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^d " class="latex" title="2^d " /> gates so its effect zone consists of at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2d%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2d} " class="latex" title="2^{2d} " /> gates which act on at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2d%2B1%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2d+1} " class="latex" title="2^{2d+1} " /> qudits (called the shadow of the effect zone).</p>

<p>Assume towards contradiction that <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2d%2B1%7D+%3C+n-%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2d+1} &lt; n-\epsilon n" class="latex" title="2^{2d+1} &lt; n-\epsilon n" />. Then the shadow of any operator acting only on the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />th qutrit has size at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2d%2B1%7D+%5Cle+n+-+%7CS%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2d+1} \le n - |S| " class="latex" title="2^{2d+1} \le n - |S| " /> since <img src="https://s0.wp.com/latex.php?latex=%7CS%7C+%5Cle+%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|S| \le \epsilon n" class="latex" title="|S| \le \epsilon n" />.  So there is some <img src="https://s0.wp.com/latex.php?latex=j+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j " class="latex" title="j " /> outside of the shadow which is in the complement of <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" />.  By <a href="https://windowsontheory.org/feed/#claim3">Claim 3.1</a>, we have found two indices <img src="https://s0.wp.com/latex.php?latex=i%2Cj+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i,j " class="latex" title="i,j " /> such that any pair of operators acting on <img src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i " class="latex" title="i " /> and <img src="https://s0.wp.com/latex.php?latex=j+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j " class="latex" title="j " /> have disjoint lightcones in <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" />. WLOG let <img src="https://s0.wp.com/latex.php?latex=i+%3C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i &lt; j" class="latex" title="i &lt; j" />.  The lightcones of <img src="https://s0.wp.com/latex.php?latex=A_i%2CB_j+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A_i,B_j " class="latex" title="A_i,B_j " /> are disjoint which implies
<img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Ctext%7BTr%7D%28A_i+%5Cotimes+B_j+%5Csigma%29+%3D+%5Ctext%7BTr%7D%28A_i+%5Csigma%29%5Ccdot%5Ctext%7BTr%7D%28B_j+%5Csigma%29.%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}\text{Tr}(A_i \otimes B_j \sigma) = \text{Tr}(A_i \sigma)\cdot\text{Tr}(B_j \sigma).\end{aligned} " class="latex" title="\begin{aligned}\text{Tr}(A_i \otimes B_j \sigma) = \text{Tr}(A_i \sigma)\cdot\text{Tr}(B_j \sigma).\end{aligned} " /></p>

<p>By the two claims above, we get a contradiction.</p>

<p>Therefore, <img src="https://s0.wp.com/latex.php?latex=2%5E%7B2d%2B1%7D+%5Cge+n-%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{2d+1} \ge n-\epsilon n" class="latex" title="2^{2d+1} \ge n-\epsilon n" />.  We can take any constant epsilon: letting <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon = 1/2" class="latex" title="\epsilon = 1/2" />, we get</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7Dd+%5Cge+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Clog+%5Cfrac%7Bn%7D%7B2%7D+-+1%5Cright%29%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}d \ge \frac{1}{2}\left(\log \frac{n}{2} - 1\right)\end{aligned} " class="latex" title="\begin{aligned}d \ge \frac{1}{2}\left(\log \frac{n}{2} - 1\right)\end{aligned} " /></p></div>

<p><img src="https://s0.wp.com/latex.php?latex=%5Csquare+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\square " class="latex" title="\square " /></p>

<p>This analysis relies crucially on the fact that any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error state matches the groundstate on most qudits.  However, NLTS is concerned with states which may differ from the groundstate on many qudits, as long as they have low energy.</p>

<p><b>Remark 2.1</b>: <i>The paper of Nirkhe, Vazirani, and Yuen [<a href="https://windowsontheory.org/feed/#nirkhe2018approximate">2</a>] actually proves more:
</i></p><ul><i>
    </i><li><i>A more general lower bound: logarithmic lower bound on the circuit depth of any <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta" class="latex" title="\delta" />-approximate (<img src="https://s0.wp.com/latex.php?latex=%5Cdelta+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta " class="latex" title="\delta " /> far in L1 norm) <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-noisy state (probability distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-error states).</i></li><i>
    </i><li><i>Assuming QCMA <img src="https://s0.wp.com/latex.php?latex=%5Cneq+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\neq " class="latex" title="\neq " /> QMA (QCMA takes a <img src="https://s0.wp.com/latex.php?latex=m+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m " class="latex" title="m " /> bit witness string instead of a <img src="https://s0.wp.com/latex.php?latex=m+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m " class="latex" title="m " /> qubit state as witness), they show a superpolynomial lower bound (on the circuit depth of any <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta" class="latex" title="\delta" />-approximate <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-noisy state).</i></li><i>
    </i><li><i>“Approximate qLWC codes”, using techniques from their superpolynomial lower bound.</i></li><i>
</i></ul><p></p>

<h2>Back to NLTS – Tempering our Optimism</h2>

<p>So far, we’ve shown a local Hamiltonian family for which all low-error (in “Hamming distance”) states require logarithmic quantum circuit depth to compute, thus resolving the NLETS conjecture. Now, let’s try to tie this back into the NLTS conjecture. Since it’s been a while, let’s recall the statement of the conjecture:</p>

<p><b>Conjecture</b> (NLTS): <i>There exists a universal constant <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon &gt; 0 " class="latex" title="\epsilon &gt; 0 " /> and a family of local Hamiltonians <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%28n%29%7D%5C%7D_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{(n)}\}_{n=1}^{\infty} " class="latex" title="\{H^{(n)}\}_{n=1}^{\infty} " /> where <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> acts on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> particles and consists of <img src="https://s0.wp.com/latex.php?latex=m_%7Bn%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_{n} " class="latex" title="m_{n} " /> local terms, s.t. any family of states <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi_%7Bn%7D%5Crangle%5C%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{|\psi_{n}\rangle\} " class="latex" title="\{|\psi_{n}\rangle\} " /> satisfying <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi_%7Bn%7D+%7C+H%5E%7B%28n%29%7D+%7C+%5Cpsi_%7Bn%7D%5Crangle+%5Cleq+%5Cepsilon%7C%7CH%5E%7B%28n%29%7D%7C%7C+%2B+%5Clambda_%7Bmin%7D%28H%5E%7B%28n%29%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \psi_{n} | H^{(n)} | \psi_{n}\rangle \leq \epsilon||H^{(n)}|| + \lambda_{min}(H^{(n)}) " class="latex" title="\langle \psi_{n} | H^{(n)} | \psi_{n}\rangle \leq \epsilon||H^{(n)}|| + \lambda_{min}(H^{(n)}) " /> requires circuit depth that grows faster than any constant.</i></p>

<p>In order to resolve the NLTS conjecture, it thus suffices to exhibit a local Hamiltonian family for which all low-energy states require logarithmic quantum circuit depth to compute. We might wonder if the local Hamiltonian family we used to resolve NLETS, which has “hard ground states”, might also have hard low-energy states. Unfortunately, as we shall show, this cannot be the case. We will start by showing that Hamiltonian families that lie on constant-dimensional lattices (in a sense that we will make precise momentarily) cannot possibly be used to resolve NLTS,  and then show that the Hamiltonian family we used to prove NLTS can be linearized (made to lie on a one-dimensional lattice!).</p>

<h3>The Woes of Constant-Dimensional Lattices</h3>
<p><b>Definition 6.1</b>: <i>A local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> acting on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qubits is said to <b>lie on a graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /></b> if there is an injection of qubits into vertices of the graph such that the set of qubits in any interaction term correspond to a connected component in the graph</i>.</p>

<p><b>Theorem 2</b>: <i>If <img src="https://s0.wp.com/latex.php?latex=%28H%5E%7B%28n%29%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H^{(n)}) " class="latex" title="(H^{(n)}) " /> is a local Hamiltonian family that lies on an <img src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1)" class="latex" title="O(1)" />-dimensional lattice, then <img src="https://s0.wp.com/latex.php?latex=%28H%5E%7B%28n%29%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H^{(n)}) " class="latex" title="(H^{(n)}) " /> has a family of low-energy states with low circuit complexity. In particular, if <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> is a local Hamiltonian on a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-dimensional lattice acting on <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> qubits for large enough <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, then for any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />, there exists a state <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle " class="latex" title="|\psi\rangle " /> that can be generated by a circuit of constant depth and such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cpsi+%7C+H+%7C+%5Cpsi+%5Crangle+%5Cleq+H_0+%2B+%5Cepsilon+%7C%7CH%7C%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \psi | H | \psi \rangle \leq H_0 + \epsilon ||H|| " class="latex" title="\langle \psi | H | \psi \rangle \leq H_0 + \epsilon ||H|| " /> where <img src="https://s0.wp.com/latex.php?latex=H_0+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_0 " class="latex" title="H_0 " /> is the ground-state energy.</i></p>

<p><i>Proof</i>: In what follows, we’ll omit some of the more annoying computational details in the interest of communicating the high-level idea.</p>

<p>Start by partitioning the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-dimensional lattice (the one that <img src="https://s0.wp.com/latex.php?latex=H%5E%28n%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^(n) " class="latex" title="H^(n) " /> lives on) into hypercubes of side length <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" />. We can “restrict” <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> to a given hypercube (let’s call it <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />) by throwing away all local terms containing a qubit not in <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />. This gives us a well-defined Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Crho%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{\rho} " class="latex" title="H_{\rho} " /> on the qubits in <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />. Define <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi_%7B%5Crho%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi_{\rho}\rangle " class="latex" title="|\phi_{\rho}\rangle " /> to be the <img src="https://s0.wp.com/latex.php?latex=L%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L^d" class="latex" title="L^d" />-qubit ground state of <img src="https://s0.wp.com/latex.php?latex=H_%7B%5Crho%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{\rho}" class="latex" title="H_{\rho}" />, and define</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5Cphi%5Crangle+%3A%3D+%5Cbigotimes_%7B%5Ctext%7Bhypercubes+%7D+%5Crho%7D+%7C%5Cphi_%7B%5Crho%7D%5Crangle%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\phi\rangle := \bigotimes_{\text{hypercubes } \rho} |\phi_{\rho}\rangle\end{aligned} " class="latex" title="\begin{aligned}|\phi\rangle := \bigotimes_{\text{hypercubes } \rho} |\phi_{\rho}\rangle\end{aligned} " /></p></div>

<p>where <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi\rangle " class="latex" title="|\phi\rangle " /> is an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />-qubit state. Each <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi_%7B%5Crho%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi_{\rho}\rangle " class="latex" title="|\phi_{\rho}\rangle " /> can be generated by a circuit with at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7BL%5Ed%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{L^d} " class="latex" title="2^{L^d} " /> gates, hence at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7BL%5Ed%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{L^d} " class="latex" title="2^{L^d} " /> depth. Then, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi\rangle " class="latex" title="|\phi\rangle " /> can be generated by putting all of these individual circuits in parallel – this doesn’t violate any sort of no-cloning condition because the individual circuits act on disjoint sets of qubits. Therefore, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi\rangle " class="latex" title="|\phi\rangle " /> can be generated by a circuit of depth at most <img src="https://s0.wp.com/latex.php?latex=2%5E%7BL%5Ed%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{L^d}" class="latex" title="2^{L^d}" />. <img src="https://s0.wp.com/latex.php?latex=L+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L " class="latex" title="L " /> and <img src="https://s0.wp.com/latex.php?latex=d+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d " class="latex" title="d " /> are both constants, so <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi\rangle " class="latex" title="|\phi\rangle " /> can be generated by a constant-depth circuit.</p>

<p>We claim that, for the right choice of <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cphi%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\phi\rangle " class="latex" title="|\phi\rangle " /> is also a low-energy state. Intuitively, this is true because <img src="https://s0.wp.com/latex.php?latex=%5Cphi+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi " class="latex" title="\phi " /> can only be “worse” than a true ground state of <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)} " class="latex" title="H^{(n)} " /> on local Hamiltonian terms that do not lie entirely within a single hypercube (i.e. the boundary terms), and by choosing <img src="https://s0.wp.com/latex.php?latex=L+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L " class="latex" title="L " /> appropriately we can make this a vanishingly small fraction of the local terms of <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(n)}" class="latex" title="H^{(n)}" />. Let’s work this out explicitly.</p>

<p>Each hypercube has surface area <img src="https://s0.wp.com/latex.php?latex=2dL%5E%7Bd-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2dL^{d-1}" class="latex" title="2dL^{d-1}" />, and there are <img src="https://s0.wp.com/latex.php?latex=n%2FL%5Ed+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n/L^d " class="latex" title="n/L^d " /> hypercubes in the lattice. Thus, the total number of qubits on boundaries is at most <img src="https://s0.wp.com/latex.php?latex=2d%5Cfrac%7Bn%7D%7BL%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2d\frac{n}{L}" class="latex" title="2d\frac{n}{L}" />. The number of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-connected components containing a given point in a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-dimensional lattice is a function of <img src="https://s0.wp.com/latex.php?latex=k+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k " class="latex" title="k " /> and <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />. Both of these are constants. Therefore, the number of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-connected components containing a given vertex, and hence the number of local Hamiltonian terms containing a given qubit, is constant. Thus, the total number of violated local Hamiltonian terms is at most <img src="https://s0.wp.com/latex.php?latex=O%28%5Cfrac%7Bn%7D%7BL%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\frac{n}{L})" class="latex" title="O(\frac{n}{L})" />. Taking <img src="https://s0.wp.com/latex.php?latex=L+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L " class="latex" title="L " /> to be <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\epsilon}" class="latex" title="\frac{1}{\epsilon}" />, we get the desired bound. Note that to be fully rigorous, we need to justify that the boundary terms don’t blow up the energy, but this is left as an exercise for the reader. <img src="https://s0.wp.com/latex.php?latex=%5Csquare+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\square " class="latex" title="\square " /></p>

<h3>Linearizing the Hamiltonian</h3>
<p>Now that we have shown that Hamiltonians that live on constant-dimensional lattices cannot be used to prove NLTS, we will put the final nail in the coffin by showing that our NLETS Hamiltonian (the Feynman-Kitaev clock Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> on the circuit <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />) can be made to lie on a line (a <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />-dimensional lattice). To do so, we will need to understand the details of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> a bit better.</p>

<p><b><a id="prop6">Proposition 6.1</a></b>: <i><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> for the circuit <img src="https://s0.wp.com/latex.php?latex=C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C " class="latex" title="C " /> is <img src="https://s0.wp.com/latex.php?latex=5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="5" class="latex" title="5" />-local.</i></p>

<p><i>Proof</i>: Recall that we defined</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathcal%7BH%7D+%3A%3D+H_%7Bin%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET+H_t%2B++H_%7Bstab%7D%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}\mathcal{H} := H_{in} + \sum_{t=1}^T H_t+  H_{stab}\end{aligned} " class="latex" title="\begin{aligned}\mathcal{H} := H_{in} + \sum_{t=1}^T H_t+  H_{stab}\end{aligned} " /></p></div>

<p>Let’s go through the right-hand-side term-by-term. We will use <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathsf%7Btime%7D%28t%29%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\mathsf{time}(t)\rangle " class="latex" title="|\mathsf{time}(t)\rangle " /> to denote the <img src="https://s0.wp.com/latex.php?latex=t%5E%7B%5Ctext%7Bth%7D%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t^{\text{th}} " class="latex" title="t^{\text{th}} " /> qubit of the time register and <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathsf%7Bstate%7D%28s%29%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\mathsf{state}(s)\rangle " class="latex" title="|\mathsf{state}(s)\rangle " /> to denote the <img src="https://s0.wp.com/latex.php?latex=s%5E%7B%5Ctext%7Bth%7D%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s^{\text{th}} " class="latex" title="s^{\text{th}} " /> qubit of the state register.</p>

<p>
  </p><ul>
    <li><img src="https://s0.wp.com/latex.php?latex=H_%7Bin%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{in} " class="latex" title="H_{in} " /> needs to serially access the qubit pairs

    <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5Cmathsf%7Btime%7D%280%29%5Crangle%5Cotimes%5Ctextsf%7Bstate%7D%28s%29+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\mathsf{time}(0)\rangle\otimes\textsf{state}(s) \end{aligned} " class="latex" title="\begin{aligned}|\mathsf{time}(0)\rangle\otimes\textsf{state}(s) \end{aligned} " />

    for all <img src="https://s0.wp.com/latex.php?latex=s+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s " class="latex" title="s " /> and ensure that they are all set to <img src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|0\rangle" class="latex" title="|0\rangle" />. Thus, <img src="https://s0.wp.com/latex.php?latex=H_%7Bin%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{in} " class="latex" title="H_{in} " /> is <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2" class="latex" title="2" />-local.</li>
    <li>Each <img src="https://s0.wp.com/latex.php?latex=H_t+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_t " class="latex" title="H_t " /> term needs to access the states <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Btime%7D%28t-1%29%5Crangle%2C+%7C%5Ctextsf%7Btime%7D%28t%29%5Crangle%2C+%7C%5Ctextsf%7Btime%7D%28t%2B1%29%5Crangle%2C+%7C%5Ctextsf%7Bstate%7D%28s%29%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{time}(t-1)\rangle, |\textsf{time}(t)\rangle, |\textsf{time}(t+1)\rangle, |\textsf{state}(s)\rangle" class="latex" title="|\textsf{time}(t-1)\rangle, |\textsf{time}(t)\rangle, |\textsf{time}(t+1)\rangle, |\textsf{state}(s)\rangle" />, and  <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bstate%7D%28t%29%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{state}(t)\rangle " class="latex" title="|\textsf{state}(t)\rangle " /> and ensure that the state transitions are correct. Thus, <img src="https://s0.wp.com/latex.php?latex=H_%7Bt%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{t} " class="latex" title="H_{t} " /> is <img src="https://s0.wp.com/latex.php?latex=5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="5" class="latex" title="5" />-local.</li>
    <li><img src="https://s0.wp.com/latex.php?latex=H_%7Bstab%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{stab} " class="latex" title="H_{stab} " /> needs to access the states

    <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7C%5Ctextsf%7Btime%7D%28t%29%5Crangle+%5Cotimes+%7C%5Ctextsf%7Btime%7D%28t%2B1%29%5Crangle+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}|\textsf{time}(t)\rangle \otimes |\textsf{time}(t+1)\rangle \end{aligned} " class="latex" title="\begin{aligned}|\textsf{time}(t)\rangle \otimes |\textsf{time}(t+1)\rangle \end{aligned} " />

    and ensure that the progression of the time register is correct. Thus, <img src="https://s0.wp.com/latex.php?latex=H_%7Bstab%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{stab} " class="latex" title="H_{stab} " /> is <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2" class="latex" title="2" />-local. <img src="https://s0.wp.com/latex.php?latex=%5Csquare+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\square " class="latex" title="\square " /></li>
  </ul>
<p></p>


<p>Now, we follow an approach of [<a href="https://windowsontheory.org/feed/#aharonov2017">3</a>] to embed <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> into a line.</p>

<p><b>Theorem 3</b>: <i>The Feynman-Kitaev clock Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H " class="latex" title="H " /> can be manipulated into a <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="3" class="latex" title="3" />-local Hamiltonian acting on qutrits on a line.</i></p>

<p><i>Proof</i>: Rather than having <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> act on <img src="https://s0.wp.com/latex.php?latex=2n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2n " class="latex" title="2n " /> total qubits (<img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> time qubits and <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " /> state qubits), let’s fuse each <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Btime%7D%28i%29%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{time}(i)\rangle " class="latex" title="|\textsf{time}(i)\rangle " /> and <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Bstate%7D%28i%29%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{state}(i)\rangle " class="latex" title="|\textsf{state}(i)\rangle " /> pair into a single qudit of dimension <img src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="4" class="latex" title="4" />. If we view <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> as acting on the space of particles <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Btime%7D%28i%29%5Crangle+%5Cotimes+%7C%5Ctextsf%7Bstate%7D%28i%29%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{time}(i)\rangle \otimes |\textsf{state}(i)\rangle" class="latex" title="|\textsf{time}(i)\rangle \otimes |\textsf{state}(i)\rangle" />, we observe that, following <a href="https://windowsontheory.org/feed/#prop6">Proposition 6.1</a>, each local term needs to check at most the particles corresponding to times <img src="https://s0.wp.com/latex.php?latex=t-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t-1" class="latex" title="t-1" />, <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, and <img src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t+1" class="latex" title="t+1" />. Therefore, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> is <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="3" class="latex" title="3" />-local and on a line, as desired.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/qutrits.png?w=600" alt="" class="wp-image-6979" />Image from [2]</figure>



<p>To see that we can have <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H} " class="latex" title="\mathcal{H} " /> act on particles of dimension <img src="https://s0.wp.com/latex.php?latex=3+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="3 " class="latex" title="3 " /> (qutrits) rather than particles of dimension <img src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="4" class="latex" title="4" />, note that the degree of freedom corresponding to <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctextsf%7Btime%7D%28t%29%5Crangle+%5Cotimes+%7C%5Ctextsf%7Bstate%7D%28t%29%5Crangle+%3D+%7C0%5Crangle+%5Cotimes+%7C1%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\textsf{time}(t)\rangle \otimes |\textsf{state}(t)\rangle = |0\rangle \otimes |1\rangle " class="latex" title="|\textsf{time}(t)\rangle \otimes |\textsf{state}(t)\rangle = |0\rangle \otimes |1\rangle " /> is unused, as the <img src="https://s0.wp.com/latex.php?latex=t%5E%7B%5Ctext%7Bth%7D%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t^{\text{th}} " class="latex" title="t^{\text{th}} " /> qubit of the state is never nonzero until timestamp <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />. Thus, we can take the vectors</p>

<div style="text-align: center;"><p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7C0%5Crangle+%3A%3D+%7C1%5Crangle%5Cotimes%7C0%5Crangle%2C+%7C1%5Crangle+%3A%3D+%7C1%5Crangle%5Cotimes%7C1%5Crangle%2C+%7C2%5Crangle+%3A%3D+%7C0%5Crangle%5Cotimes%7C0%5Crangle+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} |0\rangle := |1\rangle\otimes|0\rangle, |1\rangle := |1\rangle\otimes|1\rangle, |2\rangle := |0\rangle\otimes|0\rangle \end{aligned} " class="latex" title="\begin{aligned} |0\rangle := |1\rangle\otimes|0\rangle, |1\rangle := |1\rangle\otimes|1\rangle, |2\rangle := |0\rangle\otimes|0\rangle \end{aligned} " /></p></div>

<p>as a basis for each qutrit.</p>

<p>Even though we’ve shown that the clock Hamiltonian for our original circuit cannot be used to prove NLTS (which is still weaker than the original Quantum PCP conjecture) this does not necessarily rule out the use of this approach for other “hard” circuits which might then allow us to prove NLTS. Furthermore, NLETS is independently interesting, as the notion of being low “Hamming distance” away from vectors is exactly what is used in error-correcting codes.</p>

<h1>References</h1>
<ul>
<li><a id="arora2009computational">[1]</a> Sanjeev Arora and Boaz Barak. <i>Computational complexity: a modern approach.</i> Cambridge University Press, 2009.</li>
<li><a id="nirkhe2018approximate">[2]</a> Chinmay Nirkhe, Umesh Vazirani,  and Henry Yuen. Approximate low-weight check codes and circuit lower bounds for noisy ground states. <i>arXiv preprint arXiv:1802.07419</i>, 2018.</li>
<li><a id="aharonov2017">[3]</a> Dorit Aharonov, Wim van Dam, Julia Kempe, Zeph Landau, Seth Lloyd, and Oded Regev. Adiabatic quantum computation is equivalent to standard quantum computation. <i>SIAM J. Comput.</i>, 2007.</li></ul></div>







<p class="date">
by richardmwang <a href="https://windowsontheory.org/2018/12/22/towards-quantum-pcp-a-proof-of-the-nlets-theorem/"><span class="datestr">at December 23, 2018 01:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6948">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/22/quantum-approximate-optimization-algorithm-and-applications/">Quantum Approximate Optimization Algorithm and Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h2>Motivation</h2>
<p> </p>
<p>Quantum computers have demonstrated great potential for solving certain problems more efficiently than their classical counterpart. Algorithms based on the quantum Fourier transform (QFT) such as Shor’s algorithm offer an exponential speed-up, while amplitude-amplification algorithms such as Grover’s search algorithm provide us with a polynomial speedup. The concept of “quantum supremacy” (quantum computers outperforming classical computers) has been explored for three general groups of problems:</p>
<ol>
<li>Structured problems, such as factoring and discrete logarithm. Out quantum computer takes advantage of the structure of these classes of problems to offer an exponential speedup compared to the best known classical alternative. While these speedups are the most promising, they require a large number of resources and are cannot be feasibly implemented in the near future.</li>
<li>Quantum Simulations, originally proposed by Richard Feynman in the late 80s was thought to be the first motivation behind exploring quantum computation. Due to the fact that the space of all possible states of the system scales exponentially with the addition of a new element (eg. an atom), complex systems are very difficult to simulate classically. It has been shown that we can use a quantum computer to tackle interesting problems in quantum chemistry and chemical engineering. Furthermore, there are results on sampling the output of random quantum circuits which have been used for “quantum supremacy experiments”.</li>
<li>General constraint satisfaction and optimization problems. Since these problems are NP-hard it is widely believed that we cannot gain an exponential speedup using a quantum computer, however, we can obtain quadratic speedup but utilizing a variation of Grover’s algorithm.</li>
</ol>
<p>While these quantum algorithms are very exciting, they are beyond the capabilities of our near-term quantum computers; for example, any useful application of Shor’s factoring algorithm requires anywhere between tens of thousands to millions of qubits with error correction compared to quantum devices with hundreds of qubits that we might have available in the next few years.</p>
<p>Recently there has been increasing interest in hybrid classical-quantum algorithms among the community. The general idea behind this approach is to supplement the noisy intermediate-scale quantum (NISQ) devices with classical computers. In this blog post, we discuss the Quantum Approximate Optimization Algorithm (QAOA), which is a hybrid algorithm, alongside some of its applications.</p>
<h2>Introduction</h2>
<p>QAOA is used for optimizing combinatorial problems. Let’s assume a problem with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> bits and <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" /> clauses. Each clause is a constraint on a subset of the bits which satisfies a certain assignment. We can define a cost function as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=C%28z%29%3D%5Csum_%7B%5Calpha%3D1%7D%5Em+C_%5Calpha+%28z%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(z)=\sum_{\alpha=1}^m C_\alpha (z) " class="latex" title="C(z)=\sum_{\alpha=1}^m C_\alpha (z) " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=z%3Dz_1z_2...z_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z=z_1z_2...z_n" class="latex" title="z=z_1z_2...z_n" /> is the bit string. In this article we consider a minimization problem, therefore we want <img src="https://s0.wp.com/latex.php?latex=C_%5Calpha%28z%29%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_\alpha(z)=0" class="latex" title="C_\alpha(z)=0" /> if <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" /> satisfies clause <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> and 1 otherwise. Note that in the case of a maximization problem we only need to switch the value assigned to a satisfactory clause to 1. Our objective is to find a (qu)bit string that minimizes (or maximizes) our cost function.</p>
<p>At a higher level, we start with a quantum state in a uniform superposition of all possible inputs <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" />. This can be accomplished with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits which span a space of size <img src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n" class="latex" title="2^n" />. Our goal is to come up with a series of operations that would evolve our initial quantum state into a superposition of states in which the valid solutions would have a significantly higher probability than other states. In manner, upon sampling the quantum state we are likely to get the correct solution with high probability. QAOA uses the cost function to construct a set of operations that would be able to efficiently map the unifrom superposition state into the desired quantum state. These operators involve single qubits rotations around the x-axis, and multiqubit rotations around the z-axis of our qubits.</p>
<p>Now let’s discuss the details of QAOA. For this algorithm we assume that our quantum computer works in the computation basis of <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C0+%5Cright+%3E+%2C+%5Cleft+%7C+1+%5Cright+%3E+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left |0 \right &gt; , \left | 1 \right &gt; " class="latex" title="\left |0 \right &gt; , \left | 1 \right &gt; " />. We start by setting our initial state to a uniform superposition over computational basis states:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7Cs+%5Cright+%3E+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5En%7D%7D%5Csum_%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D+%5Cleft+%7Cz+%5Cright+%3E+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left |s \right &gt; = \frac{1}{\sqrt{2^n}}\sum_{z \in \{0,1\}^n} \left |z \right &gt; " class="latex" title="\left |s \right &gt; = \frac{1}{\sqrt{2^n}}\sum_{z \in \{0,1\}^n} \left |z \right &gt; " /></p>
<p>Next, we define a unitary operator using the cost function as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28%5Chat%7BC%7D%2C%5Cgamma%29+%3D+e%5E%7Bi%5Cgamma+%5Chat%7BC%7D%7D%3D+%5Cprod_%7B%5Calpha+%3D+1%7D%5Em+e%5E%7B-i%5Cgamma+%5Chat%7BC%7D_%5Calpha%7D%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(\hat{C},\gamma) = e^{i\gamma \hat{C}}= \prod_{\alpha = 1}^m e^{-i\gamma \hat{C}_\alpha} " class="latex" title="U(\hat{C},\gamma) = e^{i\gamma \hat{C}}= \prod_{\alpha = 1}^m e^{-i\gamma \hat{C}_\alpha} " /></p>
<p>Here we convert every clause <img src="https://s0.wp.com/latex.php?latex=C_%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_\alpha" class="latex" title="C_\alpha" /> to a Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BC_%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{C_\alpha}" class="latex" title="\hat{C_\alpha}" /> consisting of Pauli Z ($\sigma^z$) operators. Just as a review, the two Pauli operators (X and Z) used in this blog post are representated as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Csigma%5Ex+%3D+%5Cbegin%7Bpmatrix%7D%C2%A0%C2%A0%C2%A0+0+%26+1+%5C%5C%C2%A0%C2%A0%C2%A0+1+%26+0+%5C%5C%5Cend%7Bpmatrix%7D+%5C%3A+%5C%3A+%5C%3A+%5C%3A++%5Csigma%5Ez+%3D+%5Cbegin%7Bpmatrix%7D%C2%A0%C2%A0%C2%A0+1+%26+0+%5C%5C%C2%A0%C2%A0%C2%A0+0+%26+-1+%5C%5C%5Cend%7Bpmatrix%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma^x = \begin{pmatrix}    0 &amp; 1 \\    1 &amp; 0 \\\end{pmatrix} \: \: \: \:  \sigma^z = \begin{pmatrix}    1 &amp; 0 \\    0 &amp; -1 \\\end{pmatrix} " class="latex" title="\sigma^x = \begin{pmatrix}    0 &amp; 1 \\    1 &amp; 0 \\\end{pmatrix} \: \: \: \:  \sigma^z = \begin{pmatrix}    1 &amp; 0 \\    0 &amp; -1 \\\end{pmatrix} " /></p>
<p>For example if <img src="https://s0.wp.com/latex.php?latex=C_%5Calpha%3Dx+%5Coplus+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_\alpha=x \oplus y" class="latex" title="C_\alpha=x \oplus y" /> we can map the clause to <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BC_%5Calpha%7D%3D%5Cfrac%7B1%7D%7B2%7D%281%2B%5Csigma%5Ez_x+%5Csigma%5Ez_y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{C_\alpha}=\frac{1}{2}(1+\sigma^z_x \sigma^z_y)" class="latex" title="\hat{C_\alpha}=\frac{1}{2}(1+\sigma^z_x \sigma^z_y)" /> for a minimization problem. If <img src="https://s0.wp.com/latex.php?latex=x%3D%5Cleft+%7C0+%5Cright+%3E+%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x=\left |0 \right &gt;  " class="latex" title="x=\left |0 \right &gt;  " /> , then <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5Ez_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma^z_x" class="latex" title="\sigma^z_x" /> will return a value of 1, and if <img src="https://s0.wp.com/latex.php?latex=x%3D%5Cleft+%7C1+%5Cright+%3E+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x=\left |1 \right &gt; " class="latex" title="x=\left |1 \right &gt; " /> the operator will return -1. The same applies to qubit <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> as well. Therefore it is not hard to see that if <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> have the same value, then the operator <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BC_%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{C_\alpha}" class="latex" title="\hat{C_\alpha}" /> as defined above will result in a 1, and it’ll result in 0 otherwise. Furthermore, since <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{C}" class="latex" title="\hat{C}" /> has integer eigenvalues we can restrict the angle <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\gamma" class="latex" title="\gamma" /> to lie in <img src="https://s0.wp.com/latex.php?latex=%5B0%2C2%5Cpi%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[0,2\pi]" class="latex" title="[0,2\pi]" />.</p>
<p>Next, we define the admixing Hamiltonian:</p>
<p><img src="https://s0.wp.com/latex.php?latex=B%3D%5Csum_%7Bj%3D1%7D%5En+%5Csigma%5Ex_j+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B=\sum_{j=1}^n \sigma^x_j " class="latex" title="B=\sum_{j=1}^n \sigma^x_j " /></p>
<p>and use it to define a unitary operator which consists of a product of commuting one qubit operations:</p>
<p><img src="https://s0.wp.com/latex.php?latex=U%28B%2C%5Cbeta%29+%3D+e%5E%7B-i%5Cbeta+B%7D%3D+%5Cprod_%7Bj%3D1%7D%5En+e%5E%7B-i+%5Cbeta+%5Csigma_j%5Ex%7D%C2%A0%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(B,\beta) = e^{-i\beta B}= \prod_{j=1}^n e^{-i \beta \sigma_j^x}  " class="latex" title="U(B,\beta) = e^{-i\beta B}= \prod_{j=1}^n e^{-i \beta \sigma_j^x}  " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cin+%5B0%2C%5Cpi%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \in [0,\pi]" class="latex" title="\beta \in [0,\pi]" />. It’s easy to see that <img src="https://s0.wp.com/latex.php?latex=U%28%5CHat%7BC%7D%2C%5Cgamma%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(\Hat{C},\gamma)" class="latex" title="U(\Hat{C},\gamma)" /> couples 2 or more qubits, while <img src="https://s0.wp.com/latex.php?latex=U%28B%2C%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(B,\beta)" class="latex" title="U(B,\beta)" /> performs a single qubit rotation on the qubits in our system. Using these unitaries and our initial state we define a QAOA angle-dependent “ansatz” state as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%C2%A0%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D+%5Cright+%3E%3D+U%28B%2C%5Cbeta_p%29U%28%5CHat%7BC%7D%2C%5Cgamma_p%29...U%28B%2C%5Cbeta_1%29U%28%5CHat%7BC%7D%2C%5Cgamma_1%29+%5Cleft+%7Cs+%5Cright+%3E+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left |  \boldsymbol{\gamma},\boldsymbol{\beta} \right &gt;= U(B,\beta_p)U(\Hat{C},\gamma_p)...U(B,\beta_1)U(\Hat{C},\gamma_1) \left |s \right &gt; " class="latex" title="\left |  \boldsymbol{\gamma},\boldsymbol{\beta} \right &gt;= U(B,\beta_p)U(\Hat{C},\gamma_p)...U(B,\beta_1)U(\Hat{C},\gamma_1) \left |s \right &gt; " /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=p%5Cgeq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p\geq 1" class="latex" title="p\geq 1" /> is the “depth” of our QAOA circuit, and <img src="https://s0.wp.com/latex.php?latex=%5Cboldsymbol%7B%5Cgamma%7D%3D%28%5Cgamma_p%2C...%2C%5Cgamma_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\boldsymbol{\gamma}=(\gamma_p,...,\gamma_1)" class="latex" title="\boldsymbol{\gamma}=(\gamma_p,...,\gamma_1)" />, <img src="https://s0.wp.com/latex.php?latex=%5Cboldsymbol%7B%5Cbeta%7D%3D%28%5Cbeta_p%2C...%2C%5Cbeta_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\boldsymbol{\beta}=(\beta_p,...,\beta_1)" class="latex" title="\boldsymbol{\beta}=(\beta_p,...,\beta_1)" /> are each a vector of length <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> controlling the angles for each layer. In the worst case scenario this state can be produce by a quantum circuit of depth <img src="https://s0.wp.com/latex.php?latex=mp%2Bp&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="mp+p" class="latex" title="mp+p" />, however by taking advantage of the structure of the instance we can further reduce the number of layers required. Let <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> be the expectation of <img src="https://s0.wp.com/latex.php?latex=%5Chat%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{C}" class="latex" title="\hat{C}" /> in our ansatz:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_p%28%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D%29%3D%5Cleft+%3C+%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D+%5Cright+%7C+%5Chat%7BC%7D+%5Cleft+%7C+%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D+%5Cright+%3E++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p(\boldsymbol{\gamma},\boldsymbol{\beta})=\left &lt; \boldsymbol{\gamma},\boldsymbol{\beta} \right | \hat{C} \left | \boldsymbol{\gamma},\boldsymbol{\beta} \right &gt;  " class="latex" title="F_p(\boldsymbol{\gamma},\boldsymbol{\beta})=\left &lt; \boldsymbol{\gamma},\boldsymbol{\beta} \right | \hat{C} \left | \boldsymbol{\gamma},\boldsymbol{\beta} \right &gt;  " /></p>
<p>and let <img src="https://s0.wp.com/latex.php?latex=M_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M_p" class="latex" title="M_p" /> be the minimum of <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> over angles,</p>
<p><img src="https://s0.wp.com/latex.php?latex=M_p%3D%5Cmin_%7B%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D%7D+F_p%28%5Cboldsymbol%7B%5Cgamma%7D%2C%5Cboldsymbol%7B%5Cbeta%7D%29.++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M_p=\min_{\boldsymbol{\gamma},\boldsymbol{\beta}} F_p(\boldsymbol{\gamma},\boldsymbol{\beta}).  " class="latex" title="M_p=\min_{\boldsymbol{\gamma},\boldsymbol{\beta}} F_p(\boldsymbol{\gamma},\boldsymbol{\beta}).  " /></p>
<p>Note that minimization at <img src="https://s0.wp.com/latex.php?latex=p-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p-1" class="latex" title="p-1" /> layers can be viewed as a constrained minimization at <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> layers, therefore</p>
<p><img src="https://s0.wp.com/latex.php?latex=M_p+%5Cleq+M_%7Bp-1%7D++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M_p \leq M_{p-1}  " class="latex" title="M_p \leq M_{p-1}  " /></p>
<p>Using an adiabatic approach [1] We can show that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bp+%5Crightarrow+%5Cinfty%7D+M_p+%3D+%5Cmin_z+C%28z%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lim_{p \rightarrow \infty} M_p = \min_z C(z) " class="latex" title="\lim_{p \rightarrow \infty} M_p = \min_z C(z) " /></p>
<p>Based on these results our QAOA algorithm will look like the following:</p>
<ul>
<li> c: pick a <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /></li>
<li>c: choose a set of angles <img src="https://s0.wp.com/latex.php?latex=%28%5CVec%7B%5Cgamma%7D_0%2C%5CVec%7B%5Cbeta%7D_0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\Vec{\gamma}_0,\Vec{\beta}_0)" class="latex" title="(\Vec{\gamma}_0,\Vec{\beta}_0)" /></li>
<li>q: prepare <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%3E+%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " class="latex" title="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " /></li>
<li>q: compute <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /></li>
<li>c: perform gradient descend/ascend on <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> and get a new set of angles <img src="https://s0.wp.com/latex.php?latex=%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\Vec{\gamma},\Vec{\beta})" class="latex" title="(\Vec{\gamma},\Vec{\beta})" /></li>
<li>repeat from step 3 till convergence</li>
<li>report the measurement result of <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%3E+%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " class="latex" title="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " /> in computational basis</li>
</ul>
<p>If <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> does not asymptotically grow with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> <img src="https://s0.wp.com/latex.php?latex=F_p%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p(\Vec{\gamma},\Vec{\beta})" class="latex" title="F_p(\Vec{\gamma},\Vec{\beta})" /> can be efficiently computed in <img src="https://s0.wp.com/latex.php?latex=O%28m%5E2%2Bmn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(m^2+mn)" class="latex" title="O(m^2+mn)" /></p>
<h2>Application: MaxCut</h2>
<p>In this section we apply the QAOA algorithm to the MaxCut problem with bounded degree. MaxCut is an NP-hard problem that asks for a subset <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> of the vertex set such that the number of edges between <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> and the complementary subset is as large as possible. While QAOA does not offer a theoretical guarantee to solve MaxCut in polynomial time, it offers a path to utilizing NISQ devices for tackling such optimization problems and discuss patterns in such problems that can be used for reducing the number of steps required.</p>
<p>For this section, let’s assume <img src="https://s0.wp.com/latex.php?latex=p%3DO%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p=O(1)" class="latex" title="p=O(1)" />, and we have a graph with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> vertices and an edge set <img src="https://s0.wp.com/latex.php?latex=%5C%7B%3Cjk%3E%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{&lt;jk&gt;\}" class="latex" title="\{&lt;jk&gt;\}" /> of size <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />. We can construct a cost function to be maximized as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=C+%3D+%5Csum_%7B%3Cjk%3E%7D+C_%7B%3Cjk%3E%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C = \sum_{&lt;jk&gt;} C_{&lt;jk&gt;} " class="latex" title="C = \sum_{&lt;jk&gt;} C_{&lt;jk&gt;} " /></p>
<p><img src="https://s0.wp.com/latex.php?latex=C_%7B%3Cjk%3E%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+%281-%5Csigma%5Ez_j+%5Csigma%5Ez_k%29++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_{&lt;jk&gt;} = \frac{1}{2} (1-\sigma^z_j \sigma^z_k)  " class="latex" title="C_{&lt;jk&gt;} = \frac{1}{2} (1-\sigma^z_j \sigma^z_k)  " /></p>
<p>We can the compute the angle dependent cost of our ansatz as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_p%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29%3D%5Csum_%7B%3Cjk%3E%7D%5Cleft+%3C%7Bs%7D+%5Cright+%7C+U%5E%5Cdagger%28C%2C%5Cgamma_1%29...U%5E%5Cdagger%28B%2C%5Cbeta_p%29+C_%7B%3Cjk%3E%7DU%28B%2C%5Cbeta_p%29+...+U%28C%2C%5Cgamma_1%29+%5Cleft+%7Cs+%5Cright+%3E++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p(\Vec{\gamma},\Vec{\beta})=\sum_{&lt;jk&gt;}\left &lt;{s} \right | U^\dagger(C,\gamma_1)...U^\dagger(B,\beta_p) C_{&lt;jk&gt;}U(B,\beta_p) ... U(C,\gamma_1) \left |s \right &gt;  " class="latex" title="F_p(\Vec{\gamma},\Vec{\beta})=\sum_{&lt;jk&gt;}\left &lt;{s} \right | U^\dagger(C,\gamma_1)...U^\dagger(B,\beta_p) C_{&lt;jk&gt;}U(B,\beta_p) ... U(C,\gamma_1) \left |s \right &gt;  " /></p>
<p>Let’s consider the operation associated with some edge <img src="https://s0.wp.com/latex.php?latex=%3Cjk%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="&lt;jk&gt;" class="latex" title="&lt;jk&gt;" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=U+%5E%5Cdagger%28C%2C%5Cgamma_1%29...U%5E%5Cdagger%28B%2C%5Cbeta_p%29+C_%7B%3Cjk%3E%7DU%28B%2C%5Cbeta_p%29+...+U%28C%2C%5Cgamma_1%29++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U ^\dagger(C,\gamma_1)...U^\dagger(B,\beta_p) C_{&lt;jk&gt;}U(B,\beta_p) ... U(C,\gamma_1)  " class="latex" title="U ^\dagger(C,\gamma_1)...U^\dagger(B,\beta_p) C_{&lt;jk&gt;}U(B,\beta_p) ... U(C,\gamma_1)  " /></p>
<p>Since QAOA consists of local operations, we may take advantage by thinking about the problem in terms of subproblems (or subgraphs) involving certain nodes. This property will allow us to simplify our clauses even further depending on the desired depth <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> of our quantum circuit, therefore decreasing the amount of resources necessary to implement the algorithm.</p>
<p>The operator <img src="https://s0.wp.com/latex.php?latex=C_%7B%3Cjk%3E%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_{&lt;jk&gt;}" class="latex" title="C_{&lt;jk&gt;}" /> includes qubits (nodes) <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />, therefore the sequence of operators above will only involve qubits that are at most distance <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> away from qubits <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />. Let’s consider the example of <img src="https://s0.wp.com/latex.php?latex=p%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p=1" class="latex" title="p=1" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Crightarrow+U%5E%5Cdagger%28C%2C%5Cgamma_1%29e%5E%7Bi%5Cbeta_1%28%5Csigma%5Ex_j+%2B+%5Csigma%5Ex_k%29%7D+C_%7B%3Cjk%3E%7D+e%5E%7B-i%5Cbeta_1%28%5Csigma%5Ex_j+%2B+%5Csigma%5Ex_k%29%7D+U%28C%2C%5Cgamma_1%29.++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rightarrow U^\dagger(C,\gamma_1)e^{i\beta_1(\sigma^x_j + \sigma^x_k)} C_{&lt;jk&gt;} e^{-i\beta_1(\sigma^x_j + \sigma^x_k)} U(C,\gamma_1).  " class="latex" title="\rightarrow U^\dagger(C,\gamma_1)e^{i\beta_1(\sigma^x_j + \sigma^x_k)} C_{&lt;jk&gt;} e^{-i\beta_1(\sigma^x_j + \sigma^x_k)} U(C,\gamma_1).  " /></p>
<p>It’s easy to see that any factor of <img src="https://s0.wp.com/latex.php?latex=U%28C%2C%5Cgamma_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U(C,\gamma_1)" class="latex" title="U(C,\gamma_1)" /> that does not depend on <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> or <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> will commute through and cancel out. Since the degree is bounded, each subgraph contains a number of qubits that is independent of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, which allows for the evaluation of <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> in terms of subsystems of size independent of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />.</p>
<p>For an subgraph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> define:</p>
<p><img src="https://s0.wp.com/latex.php?latex=C_G%3D%5Csum_%7B%3Cl+l%5E%5Cprime%3E%7D+C_%7B%3Cl+l%5E%5Cprime%3E%7D%C2%A0+%5C%3A+%5C%3A+%5C%3A+%5C%3A+U%28C_G%2C%5Cgamma%29%3De%5E%7B-i+%5Cgamma+C_G%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_G=\sum_{&lt;l l^\prime&gt;} C_{&lt;l l^\prime&gt;}  \: \: \: \: U(C_G,\gamma)=e^{-i \gamma C_G} " class="latex" title="C_G=\sum_{&lt;l l^\prime&gt;} C_{&lt;l l^\prime&gt;}  \: \: \: \: U(C_G,\gamma)=e^{-i \gamma C_G} " /></p>
<p><img src="https://s0.wp.com/latex.php?latex=B_G+%3D+%5Csum_%7Bj+%5Cin+G%7D+%5Csigma%5Ex_j%C2%A0+%5C%3A+%5C%3A+%5C%3A+%5C%3A+U%28B_G%2C%5Cbeta%29+%3D+e%5E%7B-i+%5Cbeta+B_G%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B_G = \sum_{j \in G} \sigma^x_j  \: \: \: \: U(B_G,\beta) = e^{-i \beta B_G} " class="latex" title="B_G = \sum_{j \in G} \sigma^x_j  \: \: \: \: U(B_G,\beta) = e^{-i \beta B_G} " /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+s%2CG+%5Cright+%3E+%C2%A0%C2%A0%3D+%5Cprod_%7Bl+%5Cin+G%7D+%5Cleft+%7C%2B+%5Cright+%3E+_l++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left | s,G \right &gt;   = \prod_{l \in G} \left |+ \right &gt; _l  " class="latex" title="\left | s,G \right &gt;   = \prod_{l \in G} \left |+ \right &gt; _l  " /></p>
<p>We can define our total cost as a sum over the cost of each subgraph:</p>
<p><img src="https://s0.wp.com/latex.php?latex=f_g%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29%3D%5Cleft+%3C+s%2Cg%28j%2Ck%29+%5Cright+%7C%C2%A0+U+%5E%5Cdagger%28C_%7Bg%28j%2Ck%29%7D%2C%5Cgamma_1%29...U%5E%5Cdagger%28B_%7Bg%28j%2Ck%29%7D%2C%5Cbeta_p%29+C_%7B%3Cjk%3E%7DU%28B_%7Bg%28j%2Ck%29%7D%2C%5Cbeta_p%29+...+U%28C_%7Bg%28j%2Ck%29%7D%2C%5Cgamma_1%29+%5Cleft+%7Cs%2Cg%28j%2Ck%29+%5Cright+%3E++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_g(\Vec{\gamma},\Vec{\beta})=\left &lt; s,g(j,k) \right |  U ^\dagger(C_{g(j,k)},\gamma_1)...U^\dagger(B_{g(j,k)},\beta_p) C_{&lt;jk&gt;}U(B_{g(j,k)},\beta_p) ... U(C_{g(j,k)},\gamma_1) \left |s,g(j,k) \right &gt;  " class="latex" title="f_g(\Vec{\gamma},\Vec{\beta})=\left &lt; s,g(j,k) \right |  U ^\dagger(C_{g(j,k)},\gamma_1)...U^\dagger(B_{g(j,k)},\beta_p) C_{&lt;jk&gt;}U(B_{g(j,k)},\beta_p) ... U(C_{g(j,k)},\gamma_1) \left |s,g(j,k) \right &gt;  " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=g%28j%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g(j,k)" class="latex" title="g(j,k)" /> is a subgraph of type <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g" class="latex" title="g" /> and “…” is used to omit the sequence of angle depending unitaries constructed using the elements of <img src="https://s0.wp.com/latex.php?latex=%5CVec%7B%5Cgamma%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Vec{\gamma}" class="latex" title="\Vec{\gamma}" /> and <img src="https://s0.wp.com/latex.php?latex=%5CVec%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Vec{\beta}" class="latex" title="\Vec{\beta}" />. <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> is then</p>
<p><img src="https://s0.wp.com/latex.php?latex=F_p%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29%3D%5Csum_g+w_g+f_g%28%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%29++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p(\Vec{\gamma},\Vec{\beta})=\sum_g w_g f_g(\Vec{\gamma},\Vec{\beta})  " class="latex" title="F_p(\Vec{\gamma},\Vec{\beta})=\sum_g w_g f_g(\Vec{\gamma},\Vec{\beta})  " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=w_g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_g" class="latex" title="w_g" /> is the number of occurrence of the subgraph <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g" class="latex" title="g" /> in the original edge sum. The function <img src="https://s0.wp.com/latex.php?latex=f_g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f_g" class="latex" title="f_g" /> does not depend on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> and <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />, and the only dependence on these variables comes through the weights <img src="https://s0.wp.com/latex.php?latex=w_g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_g" class="latex" title="w_g" /> from the original graph. The maximum number of qubits that can appear in our sequence of operators comes when the subgraph is a tree. For a graph with maximum degree <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" />, the number of qubits in this tree is</p>
<p><img src="https://s0.wp.com/latex.php?latex=q_%7Btree%7D%3D2%5B%5Cfrac%7B%28v-1%29%5E%7Bp%2B1%7D-1%7D%7B%28v-1%29-1%7D%5D++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q_{tree}=2[\frac{(v-1)^{p+1}-1}{(v-1)-1}]  " class="latex" title="q_{tree}=2[\frac{(v-1)^{p+1}-1}{(v-1)-1}]  " /></p>
<p>(or <img src="https://s0.wp.com/latex.php?latex=2p%2B2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2p+2" class="latex" title="2p+2" /> if <img src="https://s0.wp.com/latex.php?latex=v%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v=2" class="latex" title="v=2" />), which is independent of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> and <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m" class="latex" title="m" />. Therefore we can see that for constant <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> <img src="https://s0.wp.com/latex.php?latex=F_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p" class="latex" title="F_p" /> can be efficiently computed.</p>
<p>Next, let’s consider the spread of C measured in the state <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%3E+%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " class="latex" title="\left | \Vec{\gamma},\Vec{\beta} \right &gt;  " />.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft+%3C%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%7C+C%5E2%5Cleft+%7C%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D%5Cright+%3E+%C2%A0-%5Cleft+%3C+%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%7C+C+%5Cleft+%7C+%5CVec%7B%5Cgamma%7D%2C%5CVec%7B%5Cbeta%7D+%5Cright+%3E+%5E2+%5Cleq+2%5B%5Cfrac%7B%28v-1%29%5E%7B2p%2B2%7D-1%7D%7B%28v-1%29-1%7D%5D.m++&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left &lt;\Vec{\gamma},\Vec{\beta} \right | C^2\left |\Vec{\gamma},\Vec{\beta}\right &gt;  -\left &lt; \Vec{\gamma},\Vec{\beta} \right | C \left | \Vec{\gamma},\Vec{\beta} \right &gt; ^2 \leq 2[\frac{(v-1)^{2p+2}-1}{(v-1)-1}].m  " class="latex" title="\left &lt;\Vec{\gamma},\Vec{\beta} \right | C^2\left |\Vec{\gamma},\Vec{\beta}\right &gt;  -\left &lt; \Vec{\gamma},\Vec{\beta} \right | C \left | \Vec{\gamma},\Vec{\beta} \right &gt; ^2 \leq 2[\frac{(v-1)^{2p+2}-1}{(v-1)-1}].m  " /></p>
<p>For fixed <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> we see that the standard deviation of <img src="https://s0.wp.com/latex.php?latex=C%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C(z)" class="latex" title="C(z)" /> is upper-bounded by <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bm%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\sqrt{m})" class="latex" title="O(\sqrt{m})" />. Using this fact and the appropriate probability bounds we can see that the result of measuring the cost function of the state <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5Cvec%7B%5Cgamma_%7Bopt%7D%7D%2Cvec%7B%5Cbeta_%7Bopt%7D%7D+%5Cright+%3E+%C2%A0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left | \vec{\gamma_{opt}},vec{\beta_{opt}} \right &gt;  " class="latex" title="\left | \vec{\gamma_{opt}},vec{\beta_{opt}} \right &gt;  " /> will be very close to the intended value of <img src="https://s0.wp.com/latex.php?latex=F_p%28%5Cvec%7B%5Cgamma_%7Bopt%7D%7D%2C%5Cvec%7B%5Cbeta_%7Bopt%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_p(\vec{\gamma_{opt}},\vec{\beta_{opt}})" class="latex" title="F_p(\vec{\gamma_{opt}},\vec{\beta_{opt}})" /> which bounds the uncertainty present in quantum measurement.</p>
<h2>Bibliography</h2>
<p>[1] E. Farhi, J. Goldstone, and S. Gutmann, “A Quantum Approximate Optimization Algorithm,” 2014.</p>
<p>[2] J. S. Otterbach, et. al, “Unsupervised Machine Learning on a Hybrid Quantum Computer,” 2017.</p></div>







<p class="date">
by karamlou <a href="https://windowsontheory.org/2018/12/22/quantum-approximate-optimization-algorithm-and-applications/"><span class="datestr">at December 22, 2018 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15529">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2018/12/21/explanations-and-explorations/">Explanations and Explorations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Comparing proofs for the Jaccard metric</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/KalidAzad.jpg"><img src="https://rjlipton.files.wordpress.com/2018/12/KalidAzad.jpg?w=142&amp;h=158" alt="" height="158" class="alignright wp-image-15530" width="142" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BetterExplained <a href="https://betterexplained.com/about/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Kalid Azad is the founder of the <a href="https://betterexplained.com/">website</a> <em>Better Explained</em>. It is devoted to explaining mathematical concepts. He also has written <a href="https://betterexplained.com/ebook/math/">two</a> <a href="https://www.amazon.com/gp/product/B017ZXWY3U/">books</a>. </p>
<p>
Today we discuss how some proofs provide a concise <em>explanation</em> whereas others promote <em>exploration</em> of related concepts.<br />
<span id="more-15529"></span></p>
<p>
Azad’s site has a rich <a href="https://betterexplained.com/articles/proofs-vs-explanations/">page</a> titled, “Math Proofs vs. Explanations (aka Nutrition vs. Taste).” It argues that the best <em>explanations</em> start with an analogy to a relation that readers already understand. Even if the connection is not sharp, it can be refined once the reader’s attention is solid. This is opposed to a formal proof in which every step is sharp and correct but intuition is wanting.</p>
<p>
To this we add the role proofs can play in <em>exploration</em>. If you have one proof of a theorem that you understand, there is value in seeking other proofs that use other ideas. Usually we think of ideas as coming first—as thoughts we refine into a proof. The advantage of starting with a proof is already having certitude and sharpness—you know a recipe that works and now can try varying and augmenting it. </p>
<p>
</p><p></p><h2> Jaccard Distance as Example </h2><p></p>
<p></p><p>
Azad’s page gives examples of proofs for the Pythagorean Theorem and for <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Ctheta%7D+%3D+%5Ccos%28%5Ctheta%29+%2B+i%5Csin%28%5Ctheta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{i\theta} = \cos(\theta) + i\sin(\theta)}" class="latex" title="{e^{i\theta} = \cos(\theta) + i\sin(\theta)}" />. It then quotes from William Thurston’s <a href="http://arxiv.org/abs/math/9404236">essay</a> “On Proofs and Progress in Mathematics,” which we once <a href="https://rjlipton.wordpress.com/2014/09/18/lets-mention-foundations/">mentioned</a>. We will use the example of Jaccard distance <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta}" class="latex" title="{J_\delta}" /> from our previous <a href="https://rjlipton.wordpress.com/2018/12/14/explaining-the-jaccard-metric/">post</a>. We start with this definition: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++J_%5Cdelta%28A%2CB%29+%3D+%5Cfrac%7B%7CA+%5C%3B%5CDelta%5C%3B+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  J_\delta(A,B) = \frac{|A \;\Delta\; B|}{|A \cup B|}, " class="latex" title="\displaystyle  J_\delta(A,B) = \frac{|A \;\Delta\; B|}{|A \cup B|}, " /></p>
<p>now using <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta}" class="latex" title="{\Delta}" /> for the symmetric difference <img src="https://s0.wp.com/latex.php?latex=%7B%28A+%5Ccup+B%29+%5Csetminus+%28A+%5Ccap+B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(A \cup B) \setminus (A \cap B)}" class="latex" title="{(A \cup B) \setminus (A \cap B)}" />. So the triangle inequality becomes, for any finite sets <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" />: <a name="triangle"></a></p><a name="triangle">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CA+%5C%3B%5CDelta%5C%3B+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cleq+%5Cfrac%7B%7CA+%5C%3B%5CDelta%5C%3B+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5C%3B%5CDelta%5C%3B+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{|A \;\Delta\; C|}{|A \cup C|} \leq \frac{|A \;\Delta\; B|}{|A \cup B|} + \frac{|B \;\Delta\; C|}{|B \cup C|}. \ \ \ \ \ (1)" class="latex" title="\displaystyle  \frac{|A \;\Delta\; C|}{|A \cup C|} \leq \frac{|A \;\Delta\; B|}{|A \cup B|} + \frac{|B \;\Delta\; C|}{|B \cup C|}. \ \ \ \ \ (1)" /></p>
</a><p><a name="triangle"></a> We think the proof we gave in the last post is simple and direct and intuitive but maybe not explorative. It first connects the solid understanding that without the denominators this would be the well-known triangle inequality for Hamming distance. To reprise, it considers <img src="https://s0.wp.com/latex.php?latex=%7BA%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,C}" class="latex" title="{A,C}" /> fixed and varies <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> to arrive at that simpler fact in three steps:</p>
<ol>
<li>
If <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> contains <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> elements not in <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" /> then removing them subtracts <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> from both right-hand numerators and both right-hand denominators. Since those fractions are each <img src="https://s0.wp.com/latex.php?latex=%7B%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{&lt; 1}" class="latex" title="{&lt; 1}" /> (else <a href="https://rjlipton.wordpress.com/feed/#triangle">1</a> would be immediately true), the right-hand side goes down. <p></p>
</li><li>
Then we have <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> and can replace the denominators by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" /> without increasing the right-hand side. <p></p>
</li><li>
Now we have a common denominator and a statement equivalent to the known truth about Hamming distance. Since undoing the first two steps to restore the original <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> can only increase the right-hand side, (<a href="https://rjlipton.wordpress.com/feed/#triangle">1</a>) is proved in all cases.
</li></ol>
<p>
This reasoning readily extends to nonnegative measures <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> on <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" /> besides simple counting, provided the removal of elements from <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> makes the same additive-or-proportional change to <img src="https://s0.wp.com/latex.php?latex=%7Bf%28A+%5C%3B%5CDelta%5C%3B+B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(A \;\Delta\; B)}" class="latex" title="{f(A \;\Delta\; B)}" /> as it does to <img src="https://s0.wp.com/latex.php?latex=%7Bf%28A+%5Ccup+B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(A \cup B)}" class="latex" title="{f(A \cup B)}" />, and likewise for the other fraction. </p>
<p>
</p><p></p><h2> Three Snapshot Proofs </h2><p></p>
<p></p><p>
The first short proof should join the pantheon of half-page journal papers. Under fair use, here it is in one screenshot:</p>
<p><a href="https://rjlipton.files.wordpress.com/2018/12/GilbertProof1972b.png"><img src="https://rjlipton.files.wordpress.com/2018/12/GilbertProof1972b.png?w=295&amp;h=410" alt="" height="410" class="aligncenter wp-image-15547" width="295" /></a></p>
<p>
Perhaps this is <i>too</i> short. We think this proof would have been more satisfying if a few more lines of calculation had been added. Let us divide the region <img src="https://s0.wp.com/latex.php?latex=%7BT_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_1}" class="latex" title="{T_1}" /> into its inner part <img src="https://s0.wp.com/latex.php?latex=%7BT_%7B1i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{1i}}" class="latex" title="{T_{1i}}" /> and outer part <img src="https://s0.wp.com/latex.php?latex=%7BT_%7B1o%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_{1o}}" class="latex" title="{T_{1o}}" /> and do likewise for <img src="https://s0.wp.com/latex.php?latex=%7BT_2%2CT_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_2,T_3}" class="latex" title="{T_2,T_3}" />. Then it seems the intent was: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++d%28S_1%2CS_3%29+%26%3D%26+1+-+%5Cfrac%7B%7CS_1+%5Ccap+S_3%7C%7D%7B%7CS_1+%5Ccup+S_3%7C%7D++%3D+1+-+%5Cfrac%7B%7CT_%7B2i%7D%7C+%2B+%7CV%7C%7D%7B%7CU%7C+-+%7CT_%7B2o%7D%7C%7D%5C%5C+%26%5Cleq%26+1+-+%5Cfrac%7B%7CV%7C%7D%7B%7CU%7C%7D+%3D+%5Cfrac%7B%7CU%7C+-+%7CV%7C%7D%7B%7CU%7C%7D+%3D+%5Cfrac%7B%7CT_1%7C+%2B+%7CT_2%7C+%2B+%7CT_3%7C%7D%7B%7CU%7C%7D%5C%5C+%26%5Cleq%26+%5Cfrac%7B%7CT_1%7C+%2B+%7CT_2%7C%7D%7B%7CU%7C%7D+%2B+%5Cfrac%7B%7CT_2%7C+%2B+%7CT_3%7C%7D%7B%7CU%7C%7D%5C%5C+%26%5Cleq%26+%5Cfrac%7B%7CT_1%7C+%2B+%7CT_2%7C%7D%7B%7CU%7C+-+%7CT_%7B3o%7D%7C%7D+%2B+%5Cfrac%7B%7CT_2%7C+%2B+%7CT_3%7C%7D%7B%7CU%7C+-+%7CT_%7B1o%7D%7C%7D+%3D+d%28S_1%2CS_2%29+%2B+d%28S_2%2CS_3%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  d(S_1,S_3) &amp;=&amp; 1 - \frac{|S_1 \cap S_3|}{|S_1 \cup S_3|}  = 1 - \frac{|T_{2i}| + |V|}{|U| - |T_{2o}|}\\ &amp;\leq&amp; 1 - \frac{|V|}{|U|} = \frac{|U| - |V|}{|U|} = \frac{|T_1| + |T_2| + |T_3|}{|U|}\\ &amp;\leq&amp; \frac{|T_1| + |T_2|}{|U|} + \frac{|T_2| + |T_3|}{|U|}\\ &amp;\leq&amp; \frac{|T_1| + |T_2|}{|U| - |T_{3o}|} + \frac{|T_2| + |T_3|}{|U| - |T_{1o}|} = d(S_1,S_2) + d(S_2,S_3). \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  d(S_1,S_3) &amp;=&amp; 1 - \frac{|S_1 \cap S_3|}{|S_1 \cup S_3|}  = 1 - \frac{|T_{2i}| + |V|}{|U| - |T_{2o}|}\\ &amp;\leq&amp; 1 - \frac{|V|}{|U|} = \frac{|U| - |V|}{|U|} = \frac{|T_1| + |T_2| + |T_3|}{|U|}\\ &amp;\leq&amp; \frac{|T_1| + |T_2|}{|U|} + \frac{|T_2| + |T_3|}{|U|}\\ &amp;\leq&amp; \frac{|T_1| + |T_2|}{|U| - |T_{3o}|} + \frac{|T_2| + |T_3|}{|U| - |T_{1o}|} = d(S_1,S_2) + d(S_2,S_3). \end{array} " /></p>
<p>The end uses the symmetric-difference definition of <img src="https://s0.wp.com/latex.php?latex=%7BJ_%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_{\delta}}" class="latex" title="{J_{\delta}}" />, so perhaps fully expanding this paper’s intent would have been longer. One can also begin with that definition to get a shorter calculation, but it skips over the <img src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B%7CV%7C%7D%7B%7CU%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 - \frac{|V|}{|U|}}" class="latex" title="{1 - \frac{|V|}{|U|}}" /> step. Indeed, it does not mention <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> at all, so it was not intended. The proof by Artur Grygorian and Ionut Iacob in a short <a href="https://www.tandfonline.com/doi/abs/10.1080/07468342.2018.1526020">paper</a> in last October’s <em>College J. Math.</em> strikes us as a similar-style proof. </p>
<p>
The second proof comes from a MathOverflow <a href="https://mathoverflow.net/q/315845">thread</a>. It assigns a variable to each region of the Venn diagram, forms the fractions, and cross-multiplies to obtain “the following monstrosity”:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2018/12/JaccardEquation.jpg"><img src="https://rjlipton.files.wordpress.com/2018/12/JaccardEquation.jpg?w=400&amp;h=232" alt="" height="232" class="aligncenter wp-image-15532" width="400" /></a></p>
<p>
The fact that no coefficient is negative completes the proof. This is clear from a computer algebra system, but what about <em>why</em> no negative term appears? </p>
<p>
We have realized since the last post that the second of two proofs given in the 2016 <a href="https://arxiv.org/pdf/1612.02696.pdf">paper</a> by Sven Kosub, which we linked in that post, is really equivalent to ours. This is easier to see if one just presumes <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Cemptyset%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\emptyset) = 0}" class="latex" title="{f(\emptyset) = 0}" /> in the following:</p>
<p><a href="https://rjlipton.files.wordpress.com/2018/12/KosubProof.png"><img src="https://rjlipton.files.wordpress.com/2018/12/KosubProof.png?w=500&amp;h=360" alt="" height="360" class="aligncenter wp-image-15533" width="500" /></a></p>
<p>
Here <em>sub-modularity</em> is a standard property for which Kosub cites the equivalent condition that whenever <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq D}" class="latex" title="{B \subseteq D}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cnotin+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \notin D}" class="latex" title="{x \notin D}" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28B+%5Ccup%5C%7Bx%5C%7D%29+-+f%28B%29+%5Cgeq+f%28D+%5Ccup+%5C%7Bx%5C%7D%29+-+f%28D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(B \cup\{x\}) - f(B) \geq f(D \cup \{x\}) - f(D). " class="latex" title="\displaystyle  f(B \cup\{x\}) - f(B) \geq f(D \cup \{x\}) - f(D). " /></p>
<p>This suffices for step 1 of our earlier proof, first taking <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+A+%5Ccup+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = A \cup B}" class="latex" title="{D = A \cup B}" /> then <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+B+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = B \cup C}" class="latex" title="{D = B \cup C}" />; the rest of that proof needs only that <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is monotone (and implicitly <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Cemptyset%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\emptyset) = 0}" class="latex" title="{f(\emptyset) = 0}" />). </p>
<p>
</p><p></p><h2> A Magical Proof </h2><p></p>
<p></p><p>
Now we look at proofs that add ideas. The first one still strikes us as clean and magical. We are computer scientists so it is natural to think of finite sets as binary-valued vectors of length <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. They have a <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> in position <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> precisely when <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> is in the set. Of course <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is the size of the “universe.” </p>
<p>
Now let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> be such a non-zero vector. The key is to use a probabilistic proof. We will show that we can relate the Jaccard distance to the outcome of a simple random experiment. The experiment once selected leads to a simple proof—it only requires the union bound. Recall this is the fact that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%5BE_%7B1%7D+%5Cvee+E_%7B2%7D%5D+%5Cle+P%5BE_%7B1%7D%5D+%2BP%5BE_%7B2%7D%5D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  P[E_{1} \vee E_{2}] \le P[E_{1}] +P[E_{2}], " class="latex" title="\displaystyle  P[E_{1} \vee E_{2}] \le P[E_{1}] +P[E_{2}], " /></p>
<p>for any two events <img src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_{1}}" class="latex" title="{E_{1}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BE_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_{2}}" class="latex" title="{E_{2}}" />. </p>
<p>
The cool idea is to look at the permutations of the vector <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. For a permutation <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi}" class="latex" title="{\pi}" /> let us define <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi(X)}" class="latex" title="{\pi(X)}" /> to be 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7B%5Cpi%281%29%7D%2C%5Ccdots%2Cx_%7B%5Cpi%28n%29%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{\pi(1)},\cdots,x_{\pi(n)}. " class="latex" title="\displaystyle  x_{\pi(1)},\cdots,x_{\pi(n)}. " /></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bfirst%7D%28X%29%3Di%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{first}(X)=i}" class="latex" title="{\mathsf{first}(X)=i}" /> provided <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{i}}" class="latex" title="{x_{i}}" /> is the first value that is equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. Of course since <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is non-empty it follows that this is well defined. </p>
<p>
Note <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bfirst%7D%28%5Cpi%28X%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{first}(\pi(X))}" class="latex" title="{\mathsf{first}(\pi(X))}" /> is a random variable that depends on the choice of the permutation <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi}" class="latex" title="{\pi}" />. The key is to see that the probability that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bfirst%7D%28%5Cpi%28X%29%29%3D%5Cmathsf%7Bfirst%7D%28%5Cpi%28Y%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{first}(\pi(X))=\mathsf{first}(\pi(Y))}" class="latex" title="{\mathsf{first}(\pi(X))=\mathsf{first}(\pi(Y))}" /> when we average over all permutations uniformly is equal to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CX+%5Ccap+Y%7C%7D%7B%7CX+%5Ccup+Y%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{|X \cap Y|}{|X \cup Y|}. " class="latex" title="\displaystyle  \frac{|X \cap Y|}{|X \cup Y|}. " /></p>
<p>This follows by noting that there are <img src="https://s0.wp.com/latex.php?latex=%7B%7CXY%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|XY|}" class="latex" title="{|XY|}" /> ways to select the same <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> and there are <img src="https://s0.wp.com/latex.php?latex=%7B%7CX+%5Ccup+Y%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|X \cup Y|}" class="latex" title="{|X \cup Y|}" /> total ways to select an <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />. Complementing gives us that the probability of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bfirst%7D%28%5Cpi%28X%29%29+%5Cneq+%5Cmathsf%7Bfirst%7D%28%5Cpi%28Y%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{first}(\pi(X)) \neq \mathsf{first}(\pi(Y))}" class="latex" title="{\mathsf{first}(\pi(X)) \neq \mathsf{first}(\pi(Y))}" /> equals <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta(X,Y)}" class="latex" title="{J_\delta(X,Y)}" />.</p>
<p>
Now hark back to our sets <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" />. The event </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7Bfirst%7D%28%5Cpi%28A%29%29+%5Cneq+%5Cmathsf%7Bfirst%7D%28%5Cpi%28C%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{first}(\pi(A)) \neq \mathsf{first}(\pi(C)) " class="latex" title="\displaystyle  \mathsf{first}(\pi(A)) \neq \mathsf{first}(\pi(C)) " /></p>
<p>is subsumed by the disjunction of events </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7Bfirst%7D%28%5Cpi%28A%29%29+%5Cneq+%5Cmathsf%7Bfirst%7D%28%5Cpi%28B%29%29+%5Cvee+%5Cmathsf%7Bfirst%7D%28%5Cpi%28B%29%29+%5Cneq+%5Cmathsf%7Bfirst%7D%28%5Cpi%28C%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{first}(\pi(A)) \neq \mathsf{first}(\pi(B)) \vee \mathsf{first}(\pi(B)) \neq \mathsf{first}(\pi(C)) " class="latex" title="\displaystyle  \mathsf{first}(\pi(A)) \neq \mathsf{first}(\pi(B)) \vee \mathsf{first}(\pi(B)) \neq \mathsf{first}(\pi(C)) " /></p>
<p>regardless of what <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is. By the simple union bound, the probability of the first event is at most the sum of the probabilities of the latter two events. We have thus proved </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++J_%5Cdelta%28A%2CC%29+%5Cleq+J_%5Cdelta%28A%2CB%29+%2B+J_%5Cdelta%28B%2CC%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  J_\delta(A,C) \leq J_\delta(A,B) + J_\delta(B,C). " class="latex" title="\displaystyle  J_\delta(A,C) \leq J_\delta(A,B) + J_\delta(B,C). " /></p>
<p>
The last step is the same as in the proof that Hamming distance is a metric. What does the randomized view gain us? It gains a nice interpretation of <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta(A,B)}" class="latex" title="{J_\delta(A,B)}" /> as the probability that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> hash to different values under the <em>min-hash</em> function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bfirst%7D%5Ccirc%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{first}\circ\pi}" class="latex" title="{\mathsf{first}\circ\pi}" /> for random <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi}" class="latex" title="{\pi}" />. Min-hashing is used all the time—see this <a href="http://infolab.stanford.edu/~ullman/mmds/ch3.pdf">book chapter</a> by Jure Leskovec, Anand Rajaraman, and Jeffrey Ullman, with this proof in section 3.3.3. 		 </p>
<p></p><h2> A Gradient Idea </h2><p></p>
<p></p><p>
Atri Rudra suggested to us the “game” of adjusting <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> one element at a time to walk it toward an extreme value. The sets <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> can be adjusted too. We start by assuming the triangle inequality (<a href="https://rjlipton.wordpress.com/feed/#triangle">1</a>) is false and make moves that can only keep it that way, until we reach a case where it is obviously true. </p>
<p>
Step 1 of our proof already plays this game by removing from <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> any elements not in <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />. So we really start the game with <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> and we want to walk it to <img src="https://s0.wp.com/latex.php?latex=%7BB+%3D+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B = A \cup C}" class="latex" title="{B = A \cup C}" />. Simply replacing the denominators <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup B|}" class="latex" title="{|A \cup B|}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CB+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|B \cup C|}" class="latex" title="{|B \cup C|}" /> in (<a href="https://rjlipton.wordpress.com/feed/#triangle">1</a>) by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" /> was good in step 2 of the proof but is not a legal move in this game. </p>
<p>
What we can do legally is add elements from <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccap+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cap C}" class="latex" title="{A \cap C}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />: those leave the denominators unchanged but lower the numerators <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5C%3B%5CDelta%5C%3B+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \;\Delta\; B|}" class="latex" title="{|A \;\Delta\; B|}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CB+%5C%3B%5CDelta%5C%3B+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|B \;\Delta\; C|}" class="latex" title="{|B \;\Delta\; C|}" />. The interesting case is when we want to add to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> an element from <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Csetminus+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \setminus C}" class="latex" title="{A \setminus C}" /> or from <img src="https://s0.wp.com/latex.php?latex=%7BC+%5Csetminus+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C \setminus A}" class="latex" title="{C \setminus A}" />. The former add decreases the numerator <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5C%3B%5CDelta%5C%3B+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \;\Delta\; B|}" class="latex" title="{|A \;\Delta\; B|}" /> and increases the denominator <img src="https://s0.wp.com/latex.php?latex=%7B%7CB+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|B \cup C|}" class="latex" title="{|B \cup C|}" /> while leaving <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup B}" class="latex" title="{A \cup B}" /> unchanged, but it <em>increases</em> the numerator <img src="https://s0.wp.com/latex.php?latex=%7B%7CB+%5C%3B%5CDelta%5C%3B+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|B \;\Delta\; C|}" class="latex" title="{|B \;\Delta\; C|}" />. Let us abstract the right-hand side of (<a href="https://rjlipton.wordpress.com/feed/#triangle">1</a>) to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7Bq%7D+%2B+%5Cfrac%7Br%7D%7Bs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{p}{q} + \frac{r}{s}}" class="latex" title="{\frac{p}{q} + \frac{r}{s}}" />. Then the former add converts it to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7Bp-1%7D%7Bq%7D+%2B+%5Cfrac%7Br%2B1%7D%7Bs%2B1%7D+%5Cqquad%5Ctext%7Band+the+latter+add+to%7D%5Cqquad+%5Cfrac%7Bp%2B1%7D%7Bq%2B1%7D+%2B+%5Cfrac%7Br-1%7D%7Bs%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{p-1}{q} + \frac{r+1}{s+1} \qquad\text{and the latter add to}\qquad \frac{p+1}{q+1} + \frac{r-1}{s}. " class="latex" title="\displaystyle  \frac{p-1}{q} + \frac{r+1}{s+1} \qquad\text{and the latter add to}\qquad \frac{p+1}{q+1} + \frac{r-1}{s}. " /></p>
<p>If <em>both</em> moves increase the right-hand side, then we must have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7Bp%7D%7Bq%7D+%2B+%5Cfrac%7Br%7D%7Bs%7D+%3C+%5Cfrac%7Bp-1%7D%7Bq%7D+%2B+%5Cfrac%7Br%2B1%7D%7Bs%2B1%7D%2C+%5Cquad%5Ctext%7Bso%7D%5Cquad+%5Cfrac%7B1%7D%7Bq%7D+%3C+%5Cfrac%7Br%2B1%7D%7Bs%2B1%7D+-+%5Cfrac%7Br%7D%7Bs%7D+%3D+%5Cfrac%7Bs-r%7D%7Bs%28s%2B1%29%7D+%3C+%5Cfrac%7B1%7D%7Bs%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{p}{q} + \frac{r}{s} &lt; \frac{p-1}{q} + \frac{r+1}{s+1}, \quad\text{so}\quad \frac{1}{q} &lt; \frac{r+1}{s+1} - \frac{r}{s} = \frac{s-r}{s(s+1)} &lt; \frac{1}{s+1}. " class="latex" title="\displaystyle  \frac{p}{q} + \frac{r}{s} &lt; \frac{p-1}{q} + \frac{r+1}{s+1}, \quad\text{so}\quad \frac{1}{q} &lt; \frac{r+1}{s+1} - \frac{r}{s} = \frac{s-r}{s(s+1)} &lt; \frac{1}{s+1}. " /></p>
<p>And from </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7Bp%7D%7Bq%7D+%2B+%5Cfrac%7Br%7D%7Bs%7D+%3C+%5Cfrac%7Bp%2B1%7D%7Bq%2B1%7D+%2B+%5Cfrac%7Br-1%7D%7Bs%7D%2C+%5Cquad%5Ctext%7Bwe+get%7D%5Cquad+%5Cfrac%7B1%7D%7Bs%7D+%3C+%5Cfrac%7B1%7D%7Bq%2B1%7D%5C%3B.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{p}{q} + \frac{r}{s} &lt; \frac{p+1}{q+1} + \frac{r-1}{s}, \quad\text{we get}\quad \frac{1}{s} &lt; \frac{1}{q+1}\;. " class="latex" title="\displaystyle  \frac{p}{q} + \frac{r}{s} &lt; \frac{p+1}{q+1} + \frac{r-1}{s}, \quad\text{we get}\quad \frac{1}{s} &lt; \frac{1}{q+1}\;. " /></p>
<p>But cross-multiplying gives the contradiction <img src="https://s0.wp.com/latex.php?latex=%7Bq%2B+1+%3C+s+%3C+s%2B1+%3C+q%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q+ 1 &lt; s &lt; s+1 &lt; q}" class="latex" title="{q+ 1 &lt; s &lt; s+1 &lt; q}" />. So one or both moves must always be possible. This grows <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> to include either all of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> or all of <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. The rest of the argument to gobble up all of <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" /> we leave to you, dear readers.</p>
<p>
Compared to the above proofs, this is tedious. But it captures some tensions among the sizes of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" />, <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> that may inform intuitions about Jaccard similarity under changes in the sets. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Which proof do you like best for explanation and which for creative impulse? </p>
<p>
This is our <img src="https://s0.wp.com/latex.php?latex=%7B801%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{801^{st}}" class="latex" title="{801^{st}}" /> post. We intended this discussion as number 800 but were surprised to find the simple proof by reduction to triangle for Hamming distance (steps numbered 1-2-3 above). Are we really the first to write it down, with acknowledgment also to Kosub?</p>
<p>
[some typo fixes]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2018/12/21/explanations-and-explorations/"><span class="datestr">at December 22, 2018 01:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6524">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/20/tensor-networks-matrix-product-states-dmrg/">Tensor Networks, Matrix Product States and Density Matrix Renormalization Group</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>by Fred Zhang</strong></p>
<p><em>This is the second installment of a three-part series of posts on quantum Hamiltonian complexity based on lectures given by the authors in <a href="https://www.boazbarak.org/fall18seminar/">Boaz and Tselil’s seminar</a>. For the basic definitions of local Hamiltonians, see <a href="https://windowsontheory.org/2018/12/20/quantum-hamiltonian-complexity/">Ben’s first post</a>. Also check out <a href="https://windowsontheory.org/2018/12/18/a-1d-area-law-for-gapped-local-hamiltonians/">Boriana and Prayaag’s followup note</a> on area laws.</em></p>
<p>This post introduces tensor networks and matrix product states (MPS). These are useful linear-algebraic objects for describing quantum states of low entanglement.</p>
<p>We then discuss how to efficiently compute the ground states of the Hamiltonians of <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{1}" class="latex" title="{1}" />D quantum systems (using classical computers). The density matrix renormalization group (DMRG), due to White (1992, 1993), is arguably the most successful heuristic for this problem. We describe it in the language of tensor networks and MPS.</p>
<p><b>1. Introduction </b></p>
<p class="p1">We are interested in computing the ground state—the minimum eigenvector—of a quantum Hamiltonian, a <img src="https://s0.wp.com/latex.php?latex=2%5En+%5Ctimes+2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n \times 2^n" class="latex" title="2^n \times 2^n" /> complex matrix that governs the evolution of a quantum system of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits. We restrict our attention to the local Hamiltonian, where the matrix is a sum of Hamiltonians each acting only on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> qubits.  In the previous article, we discussed some hardness results. Namely, a local Hamiltonian can be used to encode SAT instances, and we further gave a proof that computing the ground state is QMA-Complete.</p>
<p>Despite the hardness results, physicists have come up with a variety of heuristics for solving this problem. If quantum interactions occur locally, we would hope that its ground state has low entanglement and thus admits a succinct classical representation. Further, we hope to find such a representation efficiently, using classical computers.</p>
<p>In this note, we will see <i>tensor networks</i> and <i>matrix product states</i> that formalize the idea of succinctly representing quantum states of low entanglement. As a side remark for the theoretical computer scientists here, one motivation to study tensor network is that it provides a powerful visual tool for thinking about linear algebra. It turns indices into edges in a graph and summations over indices into contractions of edges. In particular, we will soon see that the most useful inequality in TCS and mathematics can be drawn as a cute tensor network.</p>
<p></p><div style="width: 276px;" id="attachment_6535" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note0x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note0x.png?w=600" alt="" class="wp-image-6535 size-full" /></a><p class="wp-caption-text">Guess what this is?</p></div><p></p>
<p>In the end, we will discuss the density matrix renormalization group (DMRG), which has established itself as “the most powerful tool for treating 1D quantum systems” over the last decade [<a href="https://windowsontheory.org/feed/#Xfehske2007computational">FSW07</a>]. For many 1D systems that arise from practice, the heuristic efficiently finds an (approximate) ground state in its matrix product state, specified only by a small number of parameters.</p>
<p><b>2. Tensor Networks </b></p>
<p>Now let us discuss our first subject, <i>tensor networks</i>. If you have not seen <i>tensors</i> before, it is a generalization of matrices. In computer scientists’ language, a matrix is a two-dimensional array, and a tensor is a multi-dimensional array. In other words, if we think of a matrix as a square, then a 3 dimensional tensor looks like a cube. Formally, a (complex) n dimensional tensor <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{T}" class="latex" title="{T}" /> maps <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> indices to complex values, namely, to its entries:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T+%3A+%5Bd_1%5D+%5Ctimes+%5Bd_2%5D+%5Ctimes+%5Ccdots+%5Ctimes+%5Bd_n%5D+%5Crightarrow+%5Cmathbb%7BC%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle T : [d_1] \times [d_2] \times \cdots \times [d_n] \rightarrow \mathbb{C}." class="latex" title="\displaystyle T : [d_1] \times [d_2] \times \cdots \times [d_n] \rightarrow \mathbb{C}." /></p>
<p>The simplest tensor network is a graphical notation for a tensor. For an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{n}" class="latex" title="{n}" />-dimensional tensor <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, we draw a star graph and label the center as <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> and the edges as the indices. To evaluate this tensor network, we put values on the edges, <i>i.e.</i>, indices, and then the tensor network would spit out its entry specified by the indices.</p>
<p></p><div style="width: 170px;" id="attachment_6550" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note2x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note2x.png?w=600" alt="" class="wp-image-6550 size-full" /></a><p class="wp-caption-text">A simple tensor network of 4 dimensions <a name="figsimp-1"></a></p></div><p></p>
<p></p><div style="width: 354px;" id="attachment_6551" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note3x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note3x.png?w=600" alt="" class="wp-image-6551 size-full" /></a><p class="wp-caption-text">Evaluating a simple tensor network, <img src="https://s0.wp.com/latex.php?latex=%7BT%281%2C5%2C3%2C1%29%3D1%2F%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T(1,5,3,1)=1/\sqrt{2}}" class="latex" title="{T(1,5,3,1)=1/\sqrt{2}}" />. The numbers are chosen arbitrarily.<a name="figsimp-2"></a></p></div><p></p>
<p>Notice that the degree of the center is the number of indices. Hence, a tensor network of degree <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> is a vector, and that of degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> is a matrix, and so forth.<a name="figsimple-tn"></a></p>
<p></p><div style="width: 34px;" id="attachment_6574" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note6x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note6x.png?w=600" alt="" class="wp-image-6574 size-full" /></a><p class="wp-caption-text">A vector</p></div><p></p>
<p></p><div style="width: 106px;" id="attachment_6575" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note7x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note7x.png?w=600" alt="" class="wp-image-6575 size-full" /></a><p class="wp-caption-text">A matrix</p></div><p></p>
<p></p><div style="width: 158px;" id="attachment_6576" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note8x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note8x.png?w=600" alt="" class="wp-image-6576 size-full" /></a><p class="wp-caption-text">A 3d tensor</p></div><p></p>
<p>How is this related to quantum information? For the sake of genearlity we will deal with qudits in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BC%7D%5Ed%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathbb{C}^d}" class="latex" title="{\mathbb{C}^d}" />, instead of qubits in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BC%7D%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathbb{C}^2}" class="latex" title="{\mathbb{C}^2}" />. Now recall that a quantum state <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi_n%5Crangle%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{|\psi_n\rangle}" class="latex" title="{|\psi_n\rangle}" /> of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{n}" class="latex" title="{n}" /> qudits can be encoded as an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> dimensional tensor. It can be written as</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_n%5Crangle+%3D+%5Cdisplaystyle%5Csum_%7Bi_1%2C%5Ccdots%2Ci_n+%3D+0%7D%5E%7Bd-1%7D+T%28i_1%2C%5Ccdots%2C+i_n%29+%7Ci_1%2C%5Ccdots%2C+i_n+%5Crangle.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi_n\rangle = \displaystyle\sum_{i_1,\cdots,i_n = 0}^{d-1} T(i_1,\cdots, i_n) |i_1,\cdots, i_n \rangle." class="latex" title="|\psi_n\rangle = \displaystyle\sum_{i_1,\cdots,i_n = 0}^{d-1} T(i_1,\cdots, i_n) |i_1,\cdots, i_n \rangle." /></p>
<p>It is easy to see that all the information, namely, the amplitudes, is just the tensor <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. In the later sections, we will see more powerful examples of using tensor networks to represent a quantum state.</p>
<p>So far our discussion is focused merely on these little pictures. The power of tensor networks come from its composition rules, which allow us to join two simple tensor networks together and impose rich internal structures.</p>
<p><b> 2.1. Composition Rules </b></p>
<p>We introduce two ways of joining two simple tensor networks. Roughly speaking, they correspond to multiplication and summation, and I will give the definitions by showing examples, instead of stating them in the full formalism</p>
<p><strong>Rule #1: Tensor Product.</strong> The product rule allows us to put two tensor networks together and view them as a whole. The resulting tensor is the tensor product of the two if we think of them as vectors. More concretely, consider the following picture.</p>
<p></p><div style="width: 364px;" id="attachment_6564" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note10x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note10x.png?w=600" alt="" class="wp-image-6564 size-full" /></a><p class="wp-caption-text">This is viewed as a single tensor network <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> of 7 edges<span> </span>.<a name="figtp"></a></p></div><p></p>
<p>The definition of this joint tensor <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{T}" class="latex" title="{T}" /> is</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=T%28i_1%2Ci_2%2C%5Ccdots%2C+i_7%29+%3D+T_1%28i_1%2Ci_2%2Ci_3%2Ci_4%29+T_2%28i_5%2Ci_6%2Ci_7%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T(i_1,i_2,\cdots, i_7) = T_1(i_1,i_2,i_3,i_4) T_2(i_5,i_6,i_7)." class="latex" title="T(i_1,i_2,\cdots, i_7) = T_1(i_1,i_2,i_3,i_4) T_2(i_5,i_6,i_7)." /></p>
<p><strong>Rule #2: Edge Contractions</strong>. At this moment, we can only make up disconnected tensor networks. Edge contractions allow us to link two tensor networks. Suppose we have two <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> dimensional tensor networks. Contracting two edges, one from each, gives us a tensor network of <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> <i>free edges</i>. This now corresponds a tensor of <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> dimensions.</p>
<p></p><div style="width: 310px;" id="attachment_6579" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note12x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note12x.png?w=300&amp;h=116" alt="" height="116" class="size-medium wp-image-6579" width="300" /></a><p class="wp-caption-text">Two 3d tensors</p></div><p></p>
<p></p><div style="width: 310px;" id="attachment_6563" class="wp-caption aligncenter"><a href="https://windowsontheory.files.wordpress.com/2018/12/note13x.png"><img src="https://windowsontheory.files.wordpress.com/2018/12/note13x.png?w=300&amp;h=116" alt="" height="116" class="size-medium wp-image-6563" width="300" /></a><p class="wp-caption-text">Join two tensor networks by contracting an edge</p></div><p></p>
<p>We name the contracted edge as <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />. The definition of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T%28i_1%2Ci_2%2Cj_1%2Cj_2%29+%3D%5Csum_k+T_1%28i_1%2Ci_2%2C+k%29+T_2%28j_1%2Cj_2%2C+k%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle T(i_1,i_2,j_1,j_2) =\sum_k T_1(i_1,i_2, k) T_2(j_1,j_2, k)." class="latex" title="\displaystyle T(i_1,i_2,j_1,j_2) =\sum_k T_1(i_1,i_2, k) T_2(j_1,j_2, k)." /></p>
<p><b> 2.2. Useful Examples </b></p>
<p>Before we move on, let’s take some examples. Keep in mind that the degree of the vertex determines the number of indices (dimensions of this tensor).</p>
<p></p><div style="width: 127px;" id="attachment_6584" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note15x.png?w=600" alt="note15x" class="alignnone size-full wp-image-6584" /><p class="wp-caption-text">vector inner product <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+u%2Cv+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle u,v \rangle}" class="latex" title="{\langle u,v \rangle}" /></p></div><p></p>
<p></p><div style="width: 178px;" id="attachment_6585" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note16x.png?w=600" alt="note16x" class="alignnone size-full wp-image-6585" /><p class="wp-caption-text">Matrix inner product</p></div><p></p>
<p>Here, one needs to remember that an edge between two tensor nodes is a summation over the index corresponding to the edge. For example, in the vector inner product picture, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+u%2Cv%5Crangle+%3D+%5Csum_i+u_i+%5Ccdot+v_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle u,v\rangle = \sum_i u_i \cdot v_i}" class="latex" title="{\langle u,v\rangle = \sum_i u_i \cdot v_i}" />, where edge is labeled as <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />. Now you would realize that this picture</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/note0x.png?w=600" alt="" class="wp-image-6535 size-full aligncenter" /></p>
<p>is the famous</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%5Clangle+u%2Cv+%5Crangle%5E2+%5Cleq+%5C%7Cu%5C%7C%5E2+%5C%7Cv%5C%7C%5E2.+%5Cquad%5Cquad+%28%5Ctext%7BCauchy-Schwarz+inequality%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle u,v \rangle^2 \leq \|u\|^2 \|v\|^2. \quad\quad (\text{Cauchy-Schwarz inequality}) " class="latex" title="\langle u,v \rangle^2 \leq \|u\|^2 \|v\|^2. \quad\quad (\text{Cauchy-Schwarz inequality}) " /></p>
<p>For us, the most important building block is matrix multiplication. Let <img src="https://s0.wp.com/latex.php?latex=%7BH%3DMN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H=MN}" class="latex" title="{H=MN}" />. By definition</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=H%28i%2Cj%29+%3D+%5Csum_k+M%28i%2Ck%29+N%28k%2C+j%29.+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H(i,j) = \sum_k M(i,k) N(k, j). " class="latex" title="H(i,j) = \sum_k M(i,k) N(k, j). " /></p>
<p>This is precisely encoded in the picture below.</p>
<p></p><div style="width: 276px;" id="attachment_6587" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note20x.png?w=600" alt="note20x.png" class="alignnone size-full wp-image-6587" /><p class="wp-caption-text">Matrix multiplication, <img src="https://s0.wp.com/latex.php?latex=%7BMN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{MN}" class="latex" title="{MN}" />.<span style="background-color: #ffffff; color: #3d596d; font-size: 16px;"> </span><a style="color: #3d596d; font-size: 16px;" name="figmat-mul"></a><span style="background-color: #ffffff; color: #3d596d; font-size: 16px;"> </span></p></div><p></p>
<p>We are ready to talk about matrix product states. In the language of tensor network, a matrix product state is the following picture.</p>
<p></p><div style="width: 590px;" id="attachment_6588" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note21x.png?w=600" alt="note21x" class="alignnone size-full wp-image-6588" /><p class="wp-caption-text">A matrix product state.</p></div><p></p>
<p>As the degrees indicate, the two boundary vertices <img src="https://s0.wp.com/latex.php?latex=%7BA_1%2CA_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_1,A_n}" class="latex" title="{A_1,A_n}" /> represent matrices and the internal vertices represent <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />-dimensional tensors. We can view each matrix as a set of (column) vectors and each <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />-dimensional tensor as a stack of matrices. Then each one of the free edges picks out a vector or a matrix, and the contracted edges multiply them together which gives out a scalar. If this confused you, move on to the next section. I will introduce the formal definition of matrix product states, and you will see that it is just the picture above.</p>
<p><b>3. Matrix Product States </b></p>
<p>Before giving the definition, let’s talk about how matrix product state (MPS) naturally arises from the study of quantum states with low entanglement. Matrix product state can be viewed as a generalization of <i>product state</i>—(pure) quantum state with no entanglement. Let’s consider a simple product state <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi\rangle}" class="latex" title="{|\psi\rangle}" /> of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> qubits. It can be factorized: <a name="eqnmps0"></a></p>
<p align="center"><a name="eqnmps0"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C%5Cpsi%5Crangle+%3D+%5Cleft%28%5Csum_%7Bi%3D0%7D%5E1+%5Calpha_1%5Ei%5C+%7Ci%5Crangle+%5Cright%29%5Cleft%28%5Csum_%7Bj%3D0%7D%5E1+%5Calpha_2%5Ej+%5C+%7Cj%5Crangle%5Cright%29%5Cnonumber+%3D+%5Csum_%7Bi%2Cj%3D0%7D%5E1+%5Calpha_1%5Ei+%5Calpha_2%5Ej%5C+%7Cij%5Crangle+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle |\psi\rangle = \left(\sum_{i=0}^1 \alpha_1^i\ |i\rangle \right)\left(\sum_{j=0}^1 \alpha_2^j \ |j\rangle\right)\nonumber = \sum_{i,j=0}^1 \alpha_1^i \alpha_2^j\ |ij\rangle \ \ \ \ \ (1)" class="latex" title="\displaystyle |\psi\rangle = \left(\sum_{i=0}^1 \alpha_1^i\ |i\rangle \right)\left(\sum_{j=0}^1 \alpha_2^j \ |j\rangle\right)\nonumber = \sum_{i,j=0}^1 \alpha_1^i \alpha_2^j\ |ij\rangle \ \ \ \ \ (1)" /></p>
<p><a name="eqnmps0"></a><br />
<a name="eqnmps0"></a> This state is described by <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> complex scalars <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B%5Calpha_1%5E0%2C%5Calpha_1%5E1%2C%5Calpha_2%5E0%2C%5Calpha_2%5E1%5Cright%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left\{\alpha_1^0,\alpha_1^1,\alpha_2^0,\alpha_2^1\right\}}" class="latex" title="{\left\{\alpha_1^0,\alpha_1^1,\alpha_2^0,\alpha_2^1\right\}}" />, and there is nothing quantum about it. However, if the state has entanglement among its qubits, then we know that it is impossible to be factorized and thereby written as (<a href="https://windowsontheory.org/feed/#eqnmps0">1</a>). MPS generalizes the form of (<a href="https://windowsontheory.org/feed/#eqnmps0">1</a>) by replacing the scalars with matrices and vectors.</p>
<p>More formally, a matrix product state starts with the following setup. For an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qudit system, we associate</p>
<ul>
<li>a qudit in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,n\}}" class="latex" title="{\{1,n\}}" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> vectors <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7BA_1%5E%7Bj_1%7D%5Cright%5C%7D%2C+%5Cleft%5C%7BA_n%5E%7Bj_n%7D%5Cright%5C%7D+%5Cin+%5Cmathbb%7BR%7D%5ED%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left\{A_1^{j_1}\right\}, \left\{A_n^{j_n}\right\} \in \mathbb{R}^D}" class="latex" title="{\left\{A_1^{j_1}\right\}, \left\{A_n^{j_n}\right\} \in \mathbb{R}^D}" />; and</li>
<li>a qudit <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B2%2C3%2C%5Ccdots%2C+n-1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{2,3,\cdots, n-1\}}" class="latex" title="{\{2,3,\cdots, n-1\}}" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> matrices <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7BA_i%5E%7Bj_i%7D%5Cright%5C%7D%5Cin+%5Cmathbb%7BR%7D%5E%7BD%5Ctimes+D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left\{A_i^{j_i}\right\}\in \mathbb{R}^{D\times D}}" class="latex" title="{\left\{A_i^{j_i}\right\}\in \mathbb{R}^{D\times D}}" />.</li>
</ul>
<p>Here, <img src="https://s0.wp.com/latex.php?latex=%7Bj_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j_i}" class="latex" title="{j_i}" /> range from <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bd-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d-1}" class="latex" title="{d-1}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is called <i>bond dimension</i>. One can think of the set of vectors as a <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> by <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> matrix and the set of matrices as a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> by <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> by <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> three-dimensional tensor. Then let them correspond to the vertices in MPS picture. With this setup, a quantum state is in matrix product state if it can be written as</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle+%3D+%5Csum_%7Bj_1%2C%5Ccdots%2Cj_n%3D1%7D%5En+A_1%5E%7Bj_1%7D+A_2%5E%7Bj_2%7D+%5Ccdots+A_n%5E%7Bj_n%7D+%7Cj_1+j_2%5Ccdots+j_n%5Crangle.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle = \sum_{j_1,\cdots,j_n=1}^n A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n} |j_1 j_2\cdots j_n\rangle." class="latex" title="|\psi\rangle = \sum_{j_1,\cdots,j_n=1}^n A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n} |j_1 j_2\cdots j_n\rangle." /></p>
<p>It is important to keep in mind that <img src="https://s0.wp.com/latex.php?latex=%7BA_1%5E%7Bj_1%7D%2CA_n%5E%7Bj_n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_1^{j_1},A_n^{j_n}}" class="latex" title="{A_1^{j_1},A_n^{j_n}}" /> are two vectors, and the other inner terms are matrices, and we get a scalar from the product. Thus, this represents the tensor <img src="https://s0.wp.com/latex.php?latex=%7BT%28j_1%2Cj_2%2C%5Ccdots%2C+j_n%29+%3D+A_1%5E%7Bj_1%7D+A_2%5E%7Bj_2%7D+%5Ccdots+A_n%5E%7Bj_n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T(j_1,j_2,\cdots, j_n) = A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n}}" class="latex" title="{T(j_1,j_2,\cdots, j_n) = A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n}}" />.</p>
<p>Now back to the picture,</p>
<p></p><div style="width: 590px;" id="attachment_6588" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note21x.png?w=600" alt="note21x" class="alignnone size-full wp-image-6588" /><p class="wp-caption-text">MPS</p></div><p></p>
<p>notice that each amplitude <img src="https://s0.wp.com/latex.php?latex=%7B+A_1%5E%7Bj_1%7D+A_2%5E%7Bj_2%7D+%5Ccdots+A_n%5E%7Bj_n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n}}" class="latex" title="{ A_1^{j_1} A_2^{j_2} \cdots A_n^{j_n}}" /> from the equation above is an output of the tensor in the picture, where the free edges take values <img src="https://s0.wp.com/latex.php?latex=%7Bj_1%2C+j_2+%2C%5Ccdots%2C+j_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j_1, j_2 ,\cdots, j_n}" class="latex" title="{j_1, j_2 ,\cdots, j_n}" />. Also, as discussed earlier, the contracted edges in MPS tensor network correspond to matrix and vector multiplications, so the tensor <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is precisely represented by the picture.</p>
<p>The complexity of the MPS is closely related to the bond dimension <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />. In particular, the number of parameters in this model is <img src="https://s0.wp.com/latex.php?latex=%7BO%28ndD%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(ndD^2)}" class="latex" title="{O(ndD^2)}" />. We would expect that with higher <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />, we may describe quantum states of more entanglement. In other words, the representation power of an MPS increases with <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />. In principle, one can represent any quantum state as an MPS; however, <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> can be exponentially large. See, <i>e.g.</i>, Section 4.1.3 of~\cite{schollwock2011density} for a proof. On the other extreme, the product state example shows that if <img src="https://s0.wp.com/latex.php?latex=%7BD%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D=1}" class="latex" title="{D=1}" />, one can represent and <i>only</i> represent unentangled states. To summarize, here is the picture you should keep in mind.</p>
<p></p><div style="width: 776px;" id="attachment_6594" class="wp-caption alignnone"><img src="https://windowsontheory.files.wordpress.com/2018/12/note33x.png?w=600" alt="note33x" class="alignnone size-full wp-image-6594" /><p class="wp-caption-text">Representation power of MPS increases with bond dimension D.</p></div><p></p>
<p><a name="figpower"></a></p>
<p> </p>
<p><b>4. Density Matrix Renormalization Group </b></p>
<p>We are now ready to describe Density Matrix Renormalization Group, proposed originally in [<a href="https://windowsontheory.org/feed/#XPhysRevLett.69.2863">Whi92</a>, <a href="https://windowsontheory.org/feed/#XPhysRevB.48.10345">Whi93</a>]. As mentioned earlier, it does not come with provable guarantees. In fact, one can construct artificial hard instances such that the algorithm get stuck at certain local minima [<a href="https://windowsontheory.org/feed/#Xschuch2008computational">SCV08</a>]. However, it has remained one of the most successful heuristics for <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />D systems. We refer the readers to [<a href="https://windowsontheory.org/feed/#Xschollwock2011density">Sch11</a>] for a complete survey.</p>
<p>DMRG is a simple alternating minimization scheme for computing the ground state of a <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />D Hamiltonian. We start with an arbitrary MPS. Then each step we optimize over the set of matrices <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7BA_i%5E%7Bj_i%7D%5Cright%5C%7D_%7Bj_i%3D0%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left\{A_i^{j_i}\right\}_{j_i=0}^d}" class="latex" title="{\left\{A_i^{j_i}\right\}_{j_i=0}^d}" /> associated with site <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />, while fixing everything else, and iterate until convergence. (You may wonder if one can simultaneously optimize over multiple sites. It turns out that it is an NP-hard problem<span class="LinLibertineT-tlf-ot-1x-x-90"> </span><span class="cite"><span class="LinLibertineT-tlf-ot-1x-x-90">[</span><a href="https://windowsontheory.org/feed/#XPhysRevLett.97.260501">Eis06</a><span class="LinLibertineT-tlf-ot-1x-x-90">]</span></span>.)</p>
<p>Formally, the Hamiltonian problem can be phrased as a eigenvalue problem given a Hermitian matrix <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />, and thus we want to optimize over all <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi\rangle}" class="latex" title="{|\psi\rangle}" /> in MPS of a fixed bond dimension <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> <a name="eqnham"></a></p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7B%7C%5Cpsi%5Crangle%7D%5Cfrac%7B%5Clangle+%5Cpsi%7C+H+%7C+%5Cpsi+%5Crangle%7D%7B%5Clangle+%5Cpsi+%7C%7C%5Cpsi+%5Crangle%7D.+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\min_{|\psi\rangle}\frac{\langle \psi| H | \psi \rangle}{\langle \psi ||\psi \rangle}. " class="latex" title="\min_{|\psi\rangle}\frac{\langle \psi| H | \psi \rangle}{\langle \psi ||\psi \rangle}. " /></p>
<p>Here, we assume that the input Hamiltonian is in the product form. In particular, it means that it can be written as a tensor network as</p>
<p></p><div style="width: 430px;" id="attachment_6596" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note36x.png?w=600" alt="note36x" class="alignnone size-full wp-image-6596" /><p class="wp-caption-text">Input <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />D Hamiltonian is of the particular product form.</p></div><p></p>
<p>so the numerator of the optimization objective looks like</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/note37x.png?w=600" alt="note37x" class=" size-full wp-image-6597 aligncenter" /><a name="figdmrg1"></a></p>
<p>The DMRG works with the Langrangian of the objective. For some <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda&gt;0}" class="latex" title="{\lambda&gt;0}" />, we will consider <a name="eqndmrg2"></a></p>
<p align="center"><a name="eqndmrg2"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7B%7C%5Cpsi%5Crangle%7D%5C%2C%5C%2C+%7B%5Clangle+%5Cpsi%7C+H+%7C+%5Cpsi+%5Crangle%7D+-+%5Clambda+%7B%5Clangle+%5Cpsi+%7C%7C%5Cpsi+%5Crangle%7D.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{|\psi\rangle}\,\, {\langle \psi| H | \psi \rangle} - \lambda {\langle \psi ||\psi \rangle}. \ \ \ \ \ (2)" class="latex" title="\displaystyle \min_{|\psi\rangle}\,\, {\langle \psi| H | \psi \rangle} - \lambda {\langle \psi ||\psi \rangle}. \ \ \ \ \ (2)" /></p>
<p><a name="eqndmrg2"></a><br />
<a name="eqndmrg2"></a>DMRG optimizes over the set of matrices associated with one qudit. Both terms in (<a href="https://windowsontheory.org/feed/#eqndmrg2">2</a>) are quadratic in this set of matrices.</p>
<p></p><div style="width: 919px;" id="attachment_6598" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/note39x.png?w=600" alt="note39x" class="alignnone size-full wp-image-6598" /><p class="wp-caption-text">The Langrangian <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Clangle+%5Cpsi%7C+H+%7C+%5Cpsi+%5Crangle%7D+-+%5Clambda+%7B%5Clangle+%5Cpsi+%7C%7C%5Cpsi+%5Crangle%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\langle \psi| H | \psi \rangle} - \lambda {\langle \psi ||\psi \rangle}}" class="latex" title="{{\langle \psi| H | \psi \rangle} - \lambda {\langle \psi ||\psi \rangle}}" /> as a tensor network.</p></div><p></p>
<p>Now to optimize over the set of parameters associated with one site, calculus tells you to set the (partial) derivative to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />, and the derivative of a quadratic thing is linear. Without going through any algebra, we can guess that the derivative of  with respect to a particular site, say the second one, is the same picture except removing the second site on one side.</p>
<p></p><div style="width: 919px;" id="attachment_6599" class="wp-caption alignnone"><img src="https://windowsontheory.files.wordpress.com/2018/12/note40x.png?w=600" alt="note40x" class="alignnone size-full wp-image-6599" /><p class="wp-caption-text">The derivative that we set to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> and solve.</p></div><p></p>
<p>Notice that the unknown is still there, on the bottom side of each term. The trick of DMRG is to view the rest of the network as a linear map applied to the unknown.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/note41x.png?w=600" alt="note41x" class="alignnone size-full wp-image-6600" /></p>
<p>Given <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, we now have a clean numerical linear algebra problem of solving</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+H%27x+%3D+%5Clambda+Bx.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle H'x = \lambda Bx. \ \ \ \ \ (3)" class="latex" title="\displaystyle H'x = \lambda Bx. \ \ \ \ \ (3)" /></p>
<p>This is called a generalized eigenvalue problem, and it is well studied. Importantly, for <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />D systems, <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> is typically very sparse, which enables very fast solvers in practice. Finally, DMRG sweeps over the sites one after another and stops until convergence is achieved.</p>
<p><b>5. Concluding Remarks </b></p>
<p class="noindent">Our presentation of tensor networks and MPS roughly follows <span class="cite">[<a href="https://windowsontheory.org/feed/#Xgharibian2015quantum">GHLS15</a>]</span>, a nice introductory survey on quantum Hamiltonian complexity.</p>
<p>The notion of tensor networks extends well beyond 1D systems, and a generalization of MPS is called tensor product state. It leads to algorithms for higher dimensional quantum systems. One may read <span class="cite">[<a href="https://windowsontheory.org/feed/#Xcirac2009renormalization">CV09</a>]</span> for a comprehensive survey.</p>
<p>Tensor network has been interacting with other concepts. Within physics, it has been used in quantum error correction <span class="cite">[<a href="https://windowsontheory.org/feed/#Xferris2014tensor">FP14</a>, <a href="https://windowsontheory.org/feed/#Xpastawski2015holographic">PYHP15</a>]</span>, conformal field theory <span class="cite">[<a href="https://windowsontheory.org/feed/#Xorus2014advances">Orú14</a>]</span>, and statistical mechanics <span class="cite">[<a href="https://windowsontheory.org/feed/#XPhysRevLett.115.180405">EV15</a>]</span>. In TCS , we have found its connections with Holographic algorithms <span class="cite">[<a href="https://windowsontheory.org/feed/#Xvaliant2008holographic">Val08</a>, <a href="https://windowsontheory.org/feed/#Xcai2016complete">CGW16</a>]</span>, arithmetic complexity <span class="cite">[<a href="https://windowsontheory.org/feed/#Xbeaudry2007complexity">BH07</a>, <a href="https://windowsontheory.org/feed/#Xcapelli2016arithmetic">CDM16</a>, <a href="https://windowsontheory.org/feed/#Xaustrin19">AKK19</a>]</span>, and spectral algorithms <span class="cite">[<a href="https://windowsontheory.org/feed/#Xmoitra2018spectral">MW18</a>]</span>. In machine learning, it has been applied to probabilistic graphical models <span class="cite">[<a href="https://windowsontheory.org/feed/10.1093/imaiai/iay009">RS18</a>]</span>, tensor decomposition <span class="cite">[<a href="https://windowsontheory.org/feed/#Xcichocki2016low">CLO16</a>]</span>, and quantum machine learning <span class="cite">[<a href="https://windowsontheory.org/feed/#X10.1088/2058-9565/aaea94">HPM18</a>]</span>.</p>
<p>For DMRG, we have only given a rough outline, with many details omitted, such as how to set <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> and how to obtain the Hamiltonian in the matrix product form, and how to compute the linear maps <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> for each iteration. An interested reader may read <span class="cite">[<a href="https://windowsontheory.org/feed/#Xschollwock2005density">Sch05</a>, <a href="https://windowsontheory.org/feed/#Xschollwock2011density">Sch11</a>]</span>.</p>
<p><strong>References</strong></p>
<p class="bibitem"><span class="biblabel">[AKK19] <span class="bibsp">   </span></span><a id="Xaustrin19"></a>Per Austrin, Peeri Kaski, and Kaie Kubjas. Tensor network complexity of multilinear maps. In <span class="LinLibertineTI-tlf-ot-1x-x-109">Proceedings of the 2019 Conference on Innovations in Theoretical Computer Science</span>. ACM, 2019.</p>
<p class="bibitem"><span class="biblabel">[BH07] <span class="bibsp">   </span></span><a id="Xbeaudry2007complexity"></a>Martin Beaudry and Markus Holzer. The complexity of tensor circuit evaluation. <span class="LinLibertineTI-tlf-ot-1x-x-109">Computational</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">Complexity</span>, 16(1):60, 2007.</p>
<p class="bibitem"><span class="biblabel">[CDM16] <span class="bibsp">   </span></span><a id="Xcapelli2016arithmetic"></a>Florent Capelli, Arnaud Durand, and Stefan Mengel. e arithmetic complexity of tensor contraction. <span class="LinLibertineTI-tlf-ot-1x-x-109">eory of Computing Systems</span>, 58(4):506{527, 2016.</p>
<p class="bibitem"><span class="biblabel">[CGW16] <span class="bibsp">   </span></span><a id="Xcai2016complete"></a>Jin-Yi Cai, Heng Guo, and Tyson Williams. A complete dichotomy rises from the capture of vanishing signatures. <span class="LinLibertineTI-tlf-ot-1x-x-109">SIAM Journal on Computing</span>, 45(5):1671{1728, 2016.</p>
<p class="bibitem"><span class="biblabel">[CLO16] <span class="bibsp">   </span></span><a id="Xcichocki2016low"></a>Andrzej Cichocki, Namgil Lee, Ivan V Oseledets, A-H Phan, Qibin Zhao, and D Mandic. Low-rank tensor networks for dimensionality reduction and large-scale optimization problems: Perspectives and challenges part 1. <span class="LinLibertineTI-tlf-ot-1x-x-109">arXiv preprint arXiv:1609.00893</span>, 2016.</p>
<p class="bibitem"><span class="biblabel">[CV09] <span class="bibsp">   </span></span><a id="Xcirac2009renormalization"></a>J Ignacio Cirac and Frank Verstraete. Renormalization and tensor product states in spin chains and laices. <span class="LinLibertineTI-tlf-ot-1x-x-109">Journal of Physics A: Mathematical and Theoretical</span>, 42(50):504004, 2009.</p>
<p class="bibitem"><span class="biblabel">[Eis06] <span class="bibsp">   </span></span><a id="XPhysRevLett.97.260501"></a>Jens Eisert. Computational difficulty of global variations in the density matrix renormalization group. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev. Le.</span>, 97:260501, Dec 2006.</p>
<p class="bibitem"><span class="biblabel">[EV15] <span class="bibsp">   </span></span><a id="XPhysRevLett.115.180405"></a>G. Evenbly and G. Vidal. Tensor network renormalization. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev. Le.</span>, 115:180405, Oct 2015.</p>
<p class="bibitem"><span class="biblabel">[FP14] <span class="bibsp">   </span></span><a id="Xferris2014tensor"></a>Andrew J Ferris and David Poulin. Tensor networks and quantum error correction. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev.</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">Le.</span>, 113(3):030501, 2014.</p>
<p class="bibitem"><span class="biblabel">[FSW07] <span class="bibsp">   </span></span><a id="Xfehske2007computational"></a>Holger Fehske, Ralf Schneider, and Alexander Weie. <span class="LinLibertineTI-tlf-ot-1x-x-109">Computational Many-Particle Physics</span>. Springer, 2007.</p>
<p class="bibitem"><span class="biblabel">[GHLS15] <span class="bibsp">   </span></span><a id="Xgharibian2015quantum"></a>Sevag Gharibian, Yichen Huang, Zeph Landau, and Seung Woo Shin. Quantum Hamiltonian complexity. <span class="LinLibertineTI-tlf-ot-1x-x-109">Foundations and Trends in Theoretical Computer Science</span>, 10(3):159, 2015.</p>
<p class="bibitem"><span class="biblabel">[HPM18]<span class="bibsp">   </span></span><a id="X10.1088/2058-9565/aaea94"></a>William James Huggins, Piyush Patil, Bradley Mitchell, K Birgia Whaley, and Miles Stoudenmire. Towards quantum machine learning with tensor networks. Qu<span class="LinLibertineTI-tlf-ot-1x-x-109">antum Science and</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">Technology</span>, 2018.</p>
<p class="bibitem"><span class="biblabel">[MW18] <span class="bibsp">   </span></span><a id="Xmoitra2018spectral"></a>Ankur Moitra and Alexander S Wein. Spectral methods from tensor networks. <span class="LinLibertineTI-tlf-ot-1x-x-109">arXiv preprint</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">arXiv:1811.00944</span>, 2018.</p>
<p class="bibitem"><span class="biblabel">[Orú14] <span class="bibsp">   </span></span><a id="Xorus2014advances"></a>Román Orús. Advances on tensor network theory: symmetries, fermions, entanglement, and holography. <span class="LinLibertineTI-tlf-ot-1x-x-109">e European Physical Journal B</span>, 87(11):280, 2014.</p>
<p class="bibitem"><span class="biblabel">[PYHP15] <span class="bibsp">   </span></span><a id="Xpastawski2015holographic"></a>Fernando Pastawski, Beni Yoshida, Daniel Harlow, and John Preskill. Holographic quantum error-correcting codes: Toy models for the bulk/boundary correspondence. <span class="LinLibertineTI-tlf-ot-1x-x-109">Journal of High Energy</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">Physics</span>, 2015(6):149, 2015.</p>
<p class="bibitem"><span class="biblabel">[RS18] <span class="bibsp">   </span></span><a id="Xdoi:10.1093/imaiai/iay009"></a>Elina Robeva and Anna Seigal. Duality of graphical models and tensor networks. <span class="LinLibertineTI-tlf-ot-1x-x-109">Information</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">and Inference: A Journal of the IMA</span>, 2018.</p>
<p class="bibitem"><span class="biblabel">[Sch05] <span class="bibsp">   </span></span><a id="Xschollwock2005density"></a>Ulrich Schollwöck. The density-matrix renormalization group. <span class="LinLibertineTI-tlf-ot-1x-x-109">Rev. Mod. Phys.</span>, 77(1):259, 2005.</p>
<p class="bibitem"><span class="biblabel">[Sch11] <span class="bibsp">   </span></span><a id="Xschollwock2011density"></a>Ulrich Schollwöck. The density-matrix renormalization group in the age of matrix product states. <span class="LinLibertineTI-tlf-ot-1x-x-109">Annals of Physics</span>, 326(1):96, 2011.</p>
<p class="bibitem"><span class="biblabel">[SCV08] <span class="bibsp">   </span></span><a id="Xschuch2008computational"></a>Norbert Schuch, Ignacio Cirac, and Frank Verstraete. Computational difficulty of finding matrix product ground states. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev. Le.</span>, 100(25):250501, 2008.</p>
<p class="bibitem"><span class="biblabel">[Val08] <span class="bibsp">   </span></span><a id="Xvaliant2008holographic"></a>Leslie G Valiant. Holographic algorithms. <span class="LinLibertineTI-tlf-ot-1x-x-109">SIAM Journal on Computing</span>, 37(5):1565, 2008.</p>
<p class="bibitem"><span class="biblabel">[Whi92] <span class="bibsp">   </span></span><a id="XPhysRevLett.69.2863"></a>Steven R. White. Density matrix formulation for quantum renormalization groups. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev.</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">Le.</span>, 69:2863, Nov 1992.</p>
<p class="bibitem"><span class="biblabel">[Whi93] <span class="bibsp">   </span></span><a id="XPhysRevB.48.10345"></a>Steven R. White. Density-matrix algorithms for quantum renormalization groups. <span class="LinLibertineTI-tlf-ot-1x-x-109">Phys. Rev.</span> <span class="LinLibertineTI-tlf-ot-1x-x-109">B</span>, 48:10345, Oct 1993.</p></div>







<p class="date">
by Fred Zhang <a href="https://windowsontheory.org/2018/12/20/tensor-networks-matrix-product-states-dmrg/"><span class="datestr">at December 20, 2018 09:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/20/efficient-preparation-of-thermal-states-of-quantum-systems-natural-or-artificial/">Efficient preparation of thermal states of quantum systems: natural or artificial</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<article class="post-content"><p>Cross-posted from <a href="https://wsmoses.com/blog/2018/12/18/boaz/">https://wsmoses.com/blog/2018/12/18/boaz/</a></p><p>Lecturer: Aram Harrow</p><p>Scribes: Sinho Chewi, <a href="http://wsmoses.com">William S. Moses,</a> Tasha Schoenstein, Ary Swaminathan</p><p>November 9, 2018</p></article><p><br /></p><article class="post-content"><h3 id="outline">Outline</h3><p>Sampling from thermal states was one of the first and (initially) most important uses of computers. In this blog post, we will discuss both classical and quantum Gibbs distributions, also known as thermal equilibrium states. We will then discuss Markov chains that have Gibbs distributions as stationary distributions. This leads into a discussion of the equivalence of mixing in time (i.e. the Markov chain quickly equilibrates over time) and mixing in space (i.e. sites that are far apart have small correlation). For the classical case, this equivalence is known. After discussing what is known classically, we will discuss difficulties that arise in the quantum case, including (approximate) Quantum Markov states and the equivalence of mixing in the quantum case.</p><h1 id="gibbs-distributions">Gibbs distributions</h1><p>We have already learned about phase transitions in a <a href="https://windowsontheory.org/feed/https_//windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/">previous blog post</a>, but they are important, so we will review them again. The <strong>Gibbs</strong> or <strong>thermal distribution</strong> is defined as follows: Suppose that we have an <strong>energy function</strong> <img src="https://s0.wp.com/latex.php?latex=E+%3A+%7B%5C%7B0%2C1%5C%7D%7D%5En+%5Cto+%7B%5Cmathbb+R%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E : {\{0,1\}}^n \to {\mathbb R}" class="latex" title="E : {\{0,1\}}^n \to {\mathbb R}" /> , which takes <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> -bit strings to real numbers. Usually, <img src="https://s0.wp.com/latex.php?latex=E+%3D+%5Csum_%7Bi%3D1%7D%5Em+E_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E = \sum_{i=1}^m E_i" class="latex" title="E = \sum_{i=1}^m E_i" /> , where each <img src="https://s0.wp.com/latex.php?latex=E_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E_i" class="latex" title="E_i" /> term depends only on a few bits. For example, the energy might be the number of unsatisfied clauses in a 3-SAT formula, or it may arise from the Ising model. The Gibbs distribution is</p><p><span style="display: block;"> <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cfrac%7B%5Cexp%5C%7B-E%28x%29%2FT%5C%7D%7D%7BZ%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x) = \frac{\exp\{-E(x)/T\}}{Z}" class="latex" title="p(x) = \frac{\exp\{-E(x)/T\}}{Z}" /> </span></p><p>where the normalization factor in the denominator, also called the <strong>partition function</strong>, is <img src="https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bx+%5Cin+%7B%5C%7B0%2C1%5C%7D%7D%5En%7D+%5Cexp%5C%7B-E%28x%29%2FT%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z = \sum_{x \in {\{0,1\}}^n} \exp\{-E(x)/T\}" class="latex" title="Z = \sum_{x \in {\{0,1\}}^n} \exp\{-E(x)/T\}" /> . Another, perhaps more operational, way to define the Gibbs distribution is:</p><p><span style="display: block;"> <img src="https://s0.wp.com/latex.php?latex=p+%3D+%5C%3B%5Cmathrm%7Barg%5C%2Cmax%7D_%7Bq+%5Cin+%7B%5Cmathcal%7BP%7D%7D%28%7B%5C%7B0%2C1%5C%7D%7D%5En%29%7D+H%28q%29%7E%5Ctext%7Bsubject+to+the+constraint%7D%7E+%5Clangle%7Bq%2CE%7D%5Crangle+%3D+%5Cbar%7BE%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p = \;\mathrm{arg\,max}_{q \in {\mathcal{P}}({\{0,1\}}^n)} H(q)~\text{subject to the constraint}~ \langle{q,E}\rangle = \bar{E}." class="latex" title="p = \;\mathrm{arg\,max}_{q \in {\mathcal{P}}({\{0,1\}}^n)} H(q)~\text{subject to the constraint}~ \langle{q,E}\rangle = \bar{E}." /> </span></p><p>In this expression, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D%28%7B%5C%7B0%2C1%5C%7D%7D%5En%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathcal{P}}({\{0,1\}}^n)" class="latex" title="{\mathcal{P}}({\{0,1\}}^n)" /> is the set of probability distributions on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\{0,1\}}^n" class="latex" title="{\{0,1\}}^n" /> , <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is the Shannon entropy, and <img src="https://s0.wp.com/latex.php?latex=%5Cbar+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bar E" class="latex" title="\bar E" /> is a constant representing the average energy. We are thinking of probability distributions and <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E" class="latex" title="E" /> as vectors of size <img src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n" class="latex" title="2^n" /> . It turns out that if we solve this optimization problem, then the Gibbs distribution is the unique solution.</p><h2 id="uses-of-gibbs-distributions">Uses of Gibbs distributions</h2><p>Why is it useful to work with Gibbs distributions?</p><ul><li><p>Gibbs distributions arise naturally in statistical physics systems, such as constraint satisfaction problems (CSPs), the Ising model, and spin glasses. One approach to deal with Gibbs distributions is through <a href="https://windowsontheory.org/feed/https_//windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">belief propagation</a> (BP), which yields exact inference on tree graphical models and sometimes phase transition predictions on loopy graphs. Instead, we will focus on a different approach, namely, <em>sampling</em> from the Gibbs distribution.</p></li><li><p>If we want to minimize <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E" class="latex" title="E" /> (say, to find a 3-SAT solution), we can use <strong>simulated annealing</strong>. The idea of annealing is that we want to produce a crystal; a crystal is the lowest energy configuration of molecules. If we heat up the substance to a liquid and then cool it quickly, we will not get a nice crystal, because little bits of the material will point in different directions. In order to form a crystal, we need to cool the system slowly.</p><p>In computer science terms, we take a sample from a high temperature because sampling is generally easier at a higher temperature than at a lower temperature. We then use that sample as the starting point for an equilibration process at a slightly lower temperature, and repeat this procedure. If we reach zero temperature, then we are sampling from the minimizers of <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E" class="latex" title="E" /> . In practice, the system will usually stop mixing before we get to zero temperature, but this is a good heuristic. You can think of this process as gradient descent, with some additional randomness.</p></li><li><p>Gibbs distributions are used to simulate physical systems.</p></li><li><p>Gibbs distributions are used in Bayesian inference due to the Hammersley-Clifford theorem, which will be discussed next.</p></li><li><p>Gibbs distributions are also connected to multiplicative weights for linear programming (not discussed in this blog post).</p></li></ul><h2 id="bayesian-inference--the-hammersley-clifford-theorem">Bayesian inference &amp; the Hammersley-Clifford theorem</h2><p>In order to present the Hammersley-Clifford theorem, we must first discuss Markov networks. For this part, we will generalize our setup to a finite alphabet <img src="https://s0.wp.com/latex.php?latex=%5CSigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma" class="latex" title="\Sigma" /> , so the energy function is now a function <img src="https://s0.wp.com/latex.php?latex=%5CSigma%5En+%5Cto+%5Cmathbb+R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma^n \to \mathbb R" class="latex" title="\Sigma^n \to \mathbb R" /> .</p><h3 id="markov-chains">Markov chains</h3><p>First, let us recall the idea of a <strong>Markov chain</strong> with variables <img src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1" class="latex" title="X_1" /> , <img src="https://s0.wp.com/latex.php?latex=X_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_2" class="latex" title="X_2" /> , <img src="https://s0.wp.com/latex.php?latex=X_3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_3" class="latex" title="X_3" /> .</p></article>


<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/p1.png?w=600" alt="" class="wp-image-6784" /></figure>



<p>The random variables <img src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1" class="latex" title="X_1" /> , <img src="https://s0.wp.com/latex.php?latex=X_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_2" class="latex" title="X_2" /> , <img src="https://s0.wp.com/latex.php?latex=X_3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_3" class="latex" title="X_3" /> form a Markov chain if their joint distribution can be written in a factored way: <img src="https://s0.wp.com/latex.php?latex=p%28x_1%2Cx_2%2Cx_3%29+%3D+p_%7B1%2C2%7D%28x_1%2Cx_2%29p_%7B3+%5Cmid+2%7D%28x_3+%5Cmid+x_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x_1,x_2,x_3) = p_{1,2}(x_1,x_2)p_{3 \mid 2}(x_3 \mid x_2)" class="latex" title="p(x_1,x_2,x_3) = p_{1,2}(x_1,x_2)p_{3 \mid 2}(x_3 \mid x_2)" /> . For example, imagine that <img src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1" class="latex" title="X_1" /> , <img src="https://s0.wp.com/latex.php?latex=X_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_2" class="latex" title="X_2" /> , <img src="https://s0.wp.com/latex.php?latex=X_3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_3" class="latex" title="X_3" /> represent the weather on Monday, Tuesday, and Wednesday respectively. These random variables form a Markov chain if, conditioned on the weather on Tuesday, we have all of the information we need to forecast the weather on Wednesday. Another way to say this is that conditioned on the weather on Tuesday, then the weather on Monday and the weather on Wednesday are <strong>conditionally independent</strong>. Note that the weather on Monday and the weather on Wednesday are <em>not</em> independent; there can be correlations, but these correlations are mediated through the weather on Tuesday. It is important to note that the definition of a Markov chain is symmetric with respect to going forwards or backwards in time, so we can also write the conditional independence condition as <img src="https://s0.wp.com/latex.php?latex=p%28x_1%2Cx_2%2Cx_3%29+%3D+p_%7B2%2C3%7D%28x_2%2Cx_3%29+p_%7B1+%5Cmid+2%7D%28x_1+%5Cmid+x_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x_1,x_2,x_3) = p_{2,3}(x_2,x_3) p_{1 \mid 2}(x_1 \mid x_2)" class="latex" title="p(x_1,x_2,x_3) = p_{2,3}(x_2,x_3) p_{1 \mid 2}(x_1 \mid x_2)" /> .</p>



<p>The conditional independence condition can also be written as <img src="https://s0.wp.com/latex.php?latex=p_%7B1%2C3+%5Cmid+2%7D%28x_1%2C+x_3+%5Cmid+x_2%29+%3D+p_%7B1+%5Cmid+2%7D%28x_1+%5Cmid+x_2%29+p_%7B3+%5Cmid+2%7D%28x_3+%5Cmid+x_2%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{1,3 \mid 2}(x_1, x_3 \mid x_2) = p_{1 \mid 2}(x_1 \mid x_2) p_{3 \mid 2}(x_3 \mid x_2)." class="latex" title="p_{1,3 \mid 2}(x_1, x_3 \mid x_2) = p_{1 \mid 2}(x_1 \mid x_2) p_{3 \mid 2}(x_3 \mid x_2)." /> Recall that for two random variables <img src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_1" class="latex" title="X_1" /> and <img src="https://s0.wp.com/latex.php?latex=X_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_2" class="latex" title="X_2" /> with joint distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> , they are independent, i.e., <img src="https://s0.wp.com/latex.php?latex=p%28x_1%2Cx_2%29+%3D+p_1%28x_1%29+p_2%28x_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x_1,x_2) = p_1(x_1) p_2(x_2)" class="latex" title="p(x_1,x_2) = p_1(x_1) p_2(x_2)" /> , if and only if <img src="https://s0.wp.com/latex.php?latex=I%28X_1%3B+X_2%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(X_1; X_2) = 0" class="latex" title="I(X_1; X_2) = 0" /> , where <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I" class="latex" title="I" /> here denotes the mutual information. Similarly, conditional independence is equivalent to the <strong>conditional mutual information</strong> <img src="https://s0.wp.com/latex.php?latex=I%28X_1%3B+X_3+%5Cmid+X_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(X_1; X_3 \mid X_2)" class="latex" title="I(X_1; X_3 \mid X_2)" /> equaling zero. This quantity is defined as <img src="https://s0.wp.com/latex.php?latex=I%28X_1%3BX_3+%5Cmid+X_2%29+%3D+H%28X_1+%5Cmid+X_2%29+%2B+H%28X_3+%5Cmid+X_2%29+-+H%28X_1%2C+X_3+%5Cmid+X_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(X_1;X_3 \mid X_2) = H(X_1 \mid X_2) + H(X_3 \mid X_2) - H(X_1, X_3 \mid X_2)" class="latex" title="I(X_1;X_3 \mid X_2) = H(X_1 \mid X_2) + H(X_3 \mid X_2) - H(X_1, X_3 \mid X_2)" /> .</p>



<p>Keep in mind that conditional independence is characterized in two equivalent ways: via an algebraic condition on the distributions, and via mutual information.</p>



<h3 id="markov-networks">Markov networks</h3>



<p>A <strong>Markov network</strong> is like a Markov chain, but with more random variables and a more interesting structure. Imagine that we have a graph, where each node is associated with a random variable and the edges encode possible correlations. A Markov network has the property that if we take any disjoint collection of nodes <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> , <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> such that <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> are fully separated by <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> (that is, any path from <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> to <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> must go through <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , or alternatively, removing <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> leaves <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> disconnected), then <img src="https://s0.wp.com/latex.php?latex=I%28X_A%3B+X_C+%5Cmid+X_B%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(X_A; X_C \mid X_B) = 0" class="latex" title="I(X_A; X_C \mid X_B) = 0" /> . The notation <img src="https://s0.wp.com/latex.php?latex=X_A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X_A" class="latex" title="X_A" /> here means the collection of random variables associated with the nodes in <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> .</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/p2.png?w=600" alt="" class="wp-image-6785" /></figure>



<p>For example:</p>



<p>Here, if <img src="https://s0.wp.com/latex.php?latex=A%3D%5C%7B1%2C5%2C6%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A=\{1,5,6\}" class="latex" title="A=\{1,5,6\}" /> , <img src="https://s0.wp.com/latex.php?latex=B%3D%5C%7B2%2C7%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B=\{2,7\}" class="latex" title="B=\{2,7\}" /> , and <img src="https://s0.wp.com/latex.php?latex=C%3D%5C%7B3%2C4%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C=\{3,4\}" class="latex" title="C=\{3,4\}" /> , then <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> separates <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> .</p>



<p>A Markov network is also called a <strong>graphical model</strong> or a <strong>Markov random field</strong>; and yet another name for them is <em>Gibbs distribution</em>, which is the content of the following theorem:</p>



<p><strong>Theorem 1</strong> (Hammersley-Clifford Theorem): <em>Let <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> be a strictly positive distribution on <img src="https://s0.wp.com/latex.php?latex=%5CSigma%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma^n" class="latex" title="\Sigma^n" /> . Then, <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> can be represented as a Markov network with respect to a graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> if and only if <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> can be expressed as a Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+%5Cexp%5C%7B-%5Csum_%7BC+%5Cin+%7B%5Cmathcal%7BC%7D%7D%28G%29%7D+E_C%28x_C%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x) \propto \exp\{-\sum_{C \in {\mathcal{C}}(G)} E_C(x_C)\}" class="latex" title="p(x) \propto \exp\{-\sum_{C \in {\mathcal{C}}(G)} E_C(x_C)\}" /> , where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathcal{C}}(G)" class="latex" title="{\mathcal{C}}(G)" /> is the set of cliques (fully connected subsets) of <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> . </em></p>



<p>This theorem says that Markov networks are the same as Gibbs states, <em>with the same notion of locality</em>.</p>



<p>The Hammersley-Clifford theorem implies an area law for mutual information; we will explain what this is and sketch why this is true. Divide a system into two disjoint pieces <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . We want to know about the mutual information between <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , <img src="https://s0.wp.com/latex.php?latex=I%28A%3BB%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A;B)" class="latex" title="I(A;B)" /> . The Hammersley-Clifford theorem gives us a bound which depends only on the size of the boundary <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> between these sets. For simplicity, assume <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+%5Csubseteq+B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial \subseteq B" class="latex" title="\partial \subseteq B" /> . Also, assume that the interactions have bounded range; then, the Hammersley-Clifford theorem tells us that <img src="https://s0.wp.com/latex.php?latex=I%28A%3B+B+%5Cmid+%5Cpartial%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A; B \mid \partial) = 0" class="latex" title="I(A; B \mid \partial) = 0" /> .</p>



<p>Now, we will use the fact <img src="https://s0.wp.com/latex.php?latex=I%28A%3B+B+%5Cmid+%5Cpartial%29+%3D+I%28A%3B+B%2C%5Cpartial%29+-+I%28A%3B+%5Cpartial%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A; B \mid \partial) = I(A; B,\partial) - I(A; \partial)" class="latex" title="I(A; B \mid \partial) = I(A; B,\partial) - I(A; \partial)" /> . We can see this by writing out the expressions, but the intuition is that the term on the left asks about how much <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> knows about <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , having already known about <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> . This equals how much <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> knows about <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> combined, minus how much <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> knows about <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> alone. In this case, since we said <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+%5Csubseteq+B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial \subseteq B" class="latex" title="\partial \subseteq B" /> , then <img src="https://s0.wp.com/latex.php?latex=I%28A%3B+B%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A; B)" class="latex" title="I(A; B)" /> is the same as <img src="https://s0.wp.com/latex.php?latex=I%28A%3B+B%2C+%5Cpartial%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A; B, \partial)" class="latex" title="I(A; B, \partial)" /> . In general, however, we have an upper bound:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=I%28A%3BB%29+%5Cle+I%28A%3B+B%2C+%5Cpartial%29+%3D+I%28A%3B+%5Cpartial%29+%2B+I%28A%3BB+%5Cmid+%5Cpartial%29+%5Cle+H%28%5Cpartial%29+%5Cle+%7C%5Cpartial%7C+%5Clog+%7C%5CSigma%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A;B) \le I(A; B, \partial) = I(A; \partial) + I(A;B \mid \partial) \le H(\partial) \le |\partial| \log |\Sigma|" class="latex" title="I(A;B) \le I(A; B, \partial) = I(A; \partial) + I(A;B \mid \partial) \le H(\partial) \le |\partial| \log |\Sigma|" /> </p>



<p>In this calculation, we have used <img src="https://s0.wp.com/latex.php?latex=I%28A%3B+%5Cpartial%29+%3D+H%28%5Cpartial%29+-+H%28%5Cpartial+%5Cmid+A%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I(A; \partial) = H(\partial) - H(\partial \mid A)" class="latex" title="I(A; \partial) = H(\partial) - H(\partial \mid A)" /> (the information between <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> is the amount by which the entropy of <img src="https://s0.wp.com/latex.php?latex=%5Cpartial&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial" class="latex" title="\partial" /> gets reduced once we know <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> ) and <img src="https://s0.wp.com/latex.php?latex=H%28%5Cpartial+%5Cmid+A%29+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H(\partial \mid A) \ge 0" class="latex" title="H(\partial \mid A) \ge 0" /> (which is true classically).</p>



<p>Since the mutual information only scales with the <em>surface area</em> of the boundary and not with the area of the two regions <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , this is known as an <em>area law</em> <a href="https://windowsontheory.org/feed/#gharibian">[1]</a>.</p>



<h3 id="relationship-to-bayesian-inference">Relationship to Bayesian inference</h3>



<p>In Bayesian inference, we have a model for a system which can be very complicated. The model represents our assumptions on how parts of the system are causally related to the rest of the system. We have some observations, and we want to sample from a distribution conditionally on the fixed observations. Sampling from a conditional distribution is not the same as sampling from the original distribution, but we can still formally represent the conditional distribution as a Markov network. Therefore, sampling from Markov networks is a broadly useful task.</p>



<p>As an example of a complicated Bayesian model, consider a <em>hierarchical Bayesian model</em> <a href="https://windowsontheory.org/feed/#keener">[2]</a>. Bayesian statistics requires choosing a prior distribution, and when there is a natural parameterized family of priors that a statistician can use, it may make sense to introduce a distribution over the priors; this is known as <em>introducing a hyperparameter</em>, and inference in the resulting hierarchical model (including computation of the posterior distribution) is frequently intractable. However, it is still desirable to work with these models because they are often more accurate than models in which the prior is handpicked by a statistician.</p>



<h1 id="sampling-from-gibbs-distributions">Sampling from Gibbs distributions</h1>



<p>The task of sampling from an arbitrary Gibbs distribution is MA-complete <a href="https://windowsontheory.org/feed/#crosson_making_2010">[3]</a>, and it is not hard to see that at low enough temperatures this problem is at least NP-hard. So, how do we sample from these distributions?</p>



<p>This section will discuss Monte Carlo Markov chain (MCMC) methods, namely the Metropolis-Hastings algorithm and Glauber dynamics. Readers familiar with these methods may wish to skip to the discussion of <a href="https://windowsontheory.org/feed/#scn_mixing_in_time">mixing in time</a>. For readers who wish to build more intuition about Markov chains before proceeding, see the <a href="https://windowsontheory.org/feed/#scn_appendix">Appendix</a>, where the simple example of the random walk on a cycle is treated in detail.</p>



<h2 id="monte-carlo-markov-chain-mcmc-methods">Monte Carlo Markov chain (MCMC) methods</h2>



<p>The general approach is to use a Markov chain. Let <img src="https://s0.wp.com/latex.php?latex=%5COmega%3D%5CSigma%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega=\Sigma^n" class="latex" title="\Omega=\Sigma^n" /> be the possible states of the system. Effectively, a Markov chain is a way of doing a random walk over <img src="https://s0.wp.com/latex.php?latex=%5COmega&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega" class="latex" title="\Omega" /> .</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/p3.png?w=600" alt="" class="wp-image-6786" /></figure>



<p>The transition probabilities of the Markov chain are<sup><a href="https://windowsontheory.org/feed/#fn_1">1</a></sup> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+P%7D%5C%7BX%28t%2B1%29+%3D+y+%5Cmid+X%28t%29+%3D+x%5C%7D+%3D+T_%7By%2Cx%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathbb P}\{X(t+1) = y \mid X(t) = x\} = T_{y,x}." class="latex" title="{\mathbb P}\{X(t+1) = y \mid X(t) = x\} = T_{y,x}." /> Here, <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is the <strong>transition probability matrix</strong>. The column at index <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> of <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is the probability distribution of the next state of the Markov chain, if the current state is <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> . The row at index <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> is a row of probability values which give the probabilities of jumping into state <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> from every other state. It has the properties that its entries are non-negative and for every <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> , <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7By+%5Cin+%5COmega%7D+T_%7By%2Cx%7D+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum_{y \in \Omega} T_{y,x} = 1" class="latex" title="\sum_{y \in \Omega} T_{y,x} = 1" /> . These properties say that <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is a (column) <strong>stochastic matrix</strong>.</p>



<p>Suppose we start at a state <img src="https://s0.wp.com/latex.php?latex=x%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x(0)" class="latex" title="x(0)" /> ; or, more generally, we will start with a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> over <img src="https://s0.wp.com/latex.php?latex=%5COmega&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega" class="latex" title="\Omega" /> . If we move according to the chain once, the distribution will be <img src="https://s0.wp.com/latex.php?latex=Tp&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Tp" class="latex" title="Tp" /> . If we move agian, the distribution will be <img src="https://s0.wp.com/latex.php?latex=T%5E2+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T^2 p" class="latex" title="T^2 p" /> . In general, after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> movements, the distribution is <img src="https://s0.wp.com/latex.php?latex=T%5Et+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T^t p" class="latex" title="T^t p" /> . So, we can express the dynamics of the chain as matrix-vector multiplication.</p>



<p>It is worth mentioning that if we are simulating the chain on a computer and we are manipulating <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> -bit numbers, then these probability vectors are of size <img src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n" class="latex" title="2^n" /> so it becomes impractical to store the entire probability distributions.</p>



<p>The justification for our algorithms is the following theorem.</p>



<p><strong>Theorem 2</strong> (Perron-Frobenius Theorem): <em>If <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> is a stochastic aperiodic matrix, then one of the eigenvalues is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> , and all other eigenvalues have magnitude strictly less than <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> . There is a unique probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi" class="latex" title="\pi" /> such that <img src="https://s0.wp.com/latex.php?latex=T%5Cpi+%3D+%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T\pi = \pi" class="latex" title="T\pi = \pi" /> . </em></p>



<p>The theorem implies that <img src="https://s0.wp.com/latex.php?latex=T%5Et+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T^t p" class="latex" title="T^t p" /> will converge to the stationary distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi" class="latex" title="\pi" /> as <img src="https://s0.wp.com/latex.php?latex=t%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\to\infty" class="latex" title="t\to\infty" /> . So, if we want to sample from a distribution, this provides a method of doing so: cook up a Markov chain that equilibrates to the desired distribution, and then run the Markov chain until convergence. <em>A priori</em>, it is not obvious how we can design the Markov chain. At first, our problem was to sample from a probability distribution (a vector), and now we have changed the problem to designing an entire matrix, which does not appear to make our task easier.</p>



<p>Now, the question becomes: how does one come up with Markov chains that give you the desired stationary distribution?</p>



<h2 id="metropolis-hastings-algorithm">Metropolis-Hastings algorithm</h2>



<p>The first algorithm we will introduce is the <strong>Metropolis-Hastings algorithm</strong>. One more desirable feature of a Markov chain is that it satisfies <strong>detailed balance</strong>, which says <img src="https://s0.wp.com/latex.php?latex=%5Cpi_x+T_%7By%2Cx%7D+%3D+%5Cpi_y+T_%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_x T_{y,x} = \pi_y T_{x,y}" class="latex" title="\pi_x T_{y,x} = \pi_y T_{x,y}" /> for all <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> . This condition says that if we pick a point with probability according to the stationary distribution and transition, the probability of picking <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and then moving to <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> should be the same as picking <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> and then moving to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> .</p>



<p>For a Markov chain in equilibrium, the total amount of probability flowing out of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> must equal the total amount of probability flowing into <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> . For example, the United States might export products to Europe and import from China. Detailed balance says that the flow along each edge must balance, which is a more demanding condition. In the example with country trade deficits, we are requiring that all bilateral trade deficits must be zero.</p>



<p>Mathematically, detailed balance implies that <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> can be transformed, via similarity transformations, into a symmetric matrix. The Metropolis-Hastings algorithm says that we should choose <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> with the property <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BT_%7Bx%2Cy%7D%7D%7BT_%7By%2Cx%7D%7D+%3D+%5Cfrac%7B%5Cpi_x%7D%7B%5Cpi_y%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{T_{x,y}}{T_{y,x}} = \frac{\pi_x}{\pi_y}." class="latex" title="\frac{T_{x,y}}{T_{y,x}} = \frac{\pi_x}{\pi_y}." /> Suppose that we have an underlying graph on our state space, and suppose that we are at a state <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> . The algorithm chooses a random neighbor, say <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> , and then accepts or rejects this move with some probability. If the move is accepted, then we move to <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> and continue the algorithm from there. Otherwise, if the move is rejected, then we stay at <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> . We are free to choose any underlying graph (as long as it is connected and has a self-loop), and then we will tune the acceptance probability so that detailed balance holds.</p>



<p>Look at the trial move <img src="https://s0.wp.com/latex.php?latex=x%5Cto+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x\to y" class="latex" title="x\to y" /> . One way we can accomplish detailed balance is by looking at the ratio <img src="https://s0.wp.com/latex.php?latex=%5Cpi_y%2F%5Cpi_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_y/\pi_x" class="latex" title="\pi_y/\pi_x" /> . If <img src="https://s0.wp.com/latex.php?latex=%5Cpi_y%2F%5Cpi_x+%5Cge+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_y/\pi_x \ge 1" class="latex" title="\pi_y/\pi_x \ge 1" /> , then always accept the move. If <img src="https://s0.wp.com/latex.php?latex=%5Cpi_y%2F%5Cpi_x+%3C+1+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_y/\pi_x &lt; 1 " class="latex" title="\pi_y/\pi_x &lt; 1 " /> , then accept the move with probability <img src="https://s0.wp.com/latex.php?latex=%5Cpi_x%2F%5Cpi_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_x/\pi_y" class="latex" title="\pi_x/\pi_y" /> .</p>



<p>To get an idea for how the algorithm works, suppose that our underlying graph is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> -regular. Then, for neighbors <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> ,</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7DT_%7By%2Cx%7D+%26%3D+%5Cmin%5CBigl%5C%7B1%2C+%5Cfrac%7B%5Cpi_y%7D%7B%5Cpi_x%7D%5CBigr%5C%7D+%5Cfrac%7B1%7D%7Bd%7D%2C+%5C%5C+T_%7Bx%2Cy%7D+%26%3D+%5Cmin%5CBigl%5C%7B1%2C+%5Cfrac%7B%5Cpi_x%7D%7B%5Cpi_y%7D%5CBigr%5C%7D+%5Cfrac%7B1%7D%7Bd%7D%5C%3B%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned}T_{y,x} &amp;= \min\Bigl\{1, \frac{\pi_y}{\pi_x}\Bigr\} \frac{1}{d}, \\ T_{x,y} &amp;= \min\Bigl\{1, \frac{\pi_x}{\pi_y}\Bigr\} \frac{1}{d}\;\end{aligned} " class="latex" title="\begin{aligned}T_{y,x} &amp;= \min\Bigl\{1, \frac{\pi_y}{\pi_x}\Bigr\} \frac{1}{d}, \\ T_{x,y} &amp;= \min\Bigl\{1, \frac{\pi_x}{\pi_y}\Bigr\} \frac{1}{d}\;\end{aligned} " /> </p>



<p><strong>Claim</strong>: <img src="https://s0.wp.com/latex.php?latex=T_%7By%2Cx%7D+%5Cpi_x+%3D+%5Cfrac%7B1%7D%7Bd%7D+%5Cmin%5C%7B%5Cpi_x%2C%5Cpi_y%5C%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_{y,x} \pi_x = \frac{1}{d} \min\{\pi_x,\pi_y\}," class="latex" title="T_{y,x} \pi_x = \frac{1}{d} \min\{\pi_x,\pi_y\}," /> which is manifestly symmetric in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> ; thus, we have reversibility. This is the basic idea of the Metropolis-Hastings algorithm.</p>



<p>How does it work for a Gibbs distribution <img src="https://s0.wp.com/latex.php?latex=%5Cpi_x+%3D+%5Cexp%5C%7B-E%28x%29%2FT%5C%7D%2FZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_x = \exp\{-E(x)/T\}/Z" class="latex" title="\pi_x = \exp\{-E(x)/T\}/Z" /> , where the energy function might, for example, count the number of violated clauses in a 3-SAT formula? In this case, we might be a little worried. The numerator of <img src="https://s0.wp.com/latex.php?latex=%5Cpi_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_x" class="latex" title="\pi_x" /> is pretty easy to compute (we can count how many violated constraints there are), but the denominator is hard to compute. In general, it is #P-hard to compute the denominator, because as <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> drops to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> , the partition function in this case approaches the number of 3-SAT solutions. So, how do we calculate the ratios <img src="https://s0.wp.com/latex.php?latex=%5Cpi_y%2F%5Cpi_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_y/\pi_x" class="latex" title="\pi_y/\pi_x" /> that the algorithm requires? We’re able to do this because the ratio does not depend on <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> :</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpi_y%7D%7B%5Cpi_x%7D+%3D+%5Cexp+%5Cfrac%7BE%28x%29-E%28y%29%7D%7BT%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\pi_y}{\pi_x} = \exp \frac{E(x)-E(y)}{T}." class="latex" title="\frac{\pi_y}{\pi_x} = \exp \frac{E(x)-E(y)}{T}." /> </p>



<p>Suppose that the energy is a sum of local terms, and the underlying graph corresponds to modifying one site at at a time. What this means is that the graph is <img src="https://s0.wp.com/latex.php?latex=%5COmega+%3D+%7B%5C%7B0%2C1%5C%7D%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega = {\{0,1\}}^n" class="latex" title="\Omega = {\{0,1\}}^n" /> and the edges in the graph correspond to flipping exactly one bit. In this case, it becomes very easy to evaluate the computations needed for the algorithm; in fact, we can even do them in parallel.</p>



<p>How do we choose the underlying graph? The key idea is that we do not want the majority of our moves to be rejected. A good example to keep in mind is the <strong>Ising model</strong>, where the configurations are <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5C%7B0%2C1%5C%7D%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in {\{0,1\}}^n" class="latex" title="x \in {\{0,1\}}^n" /> and the energy is <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+-%5Csum_%7Bi%2Cj%3D1%7D%5En+J_%7Bi%2Cj%7D+x_i+x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = -\sum_{i,j=1}^n J_{i,j} x_i x_j" class="latex" title="E(x) = -\sum_{i,j=1}^n J_{i,j} x_i x_j" /> . If <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_{i,j} \ge 0" class="latex" title="J_{i,j} \ge 0" /> for all <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> , <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> , then we say that the model is <strong>ferromagnetic</strong> (we obtain lower energy by making the sites agree with each other). Of course, an <strong>antiferromagnetic</strong> model is just the opposite of this.</p>



<p>Assume that the bits are laid out in a square and <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D+%3D+J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_{i,j} = J" class="latex" title="J_{i,j} = J" /> if <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> are neighbors on the square, and <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_{i,j} = 0" class="latex" title="J_{i,j} = 0" /> if they are not. As we vary the quantity <img src="https://s0.wp.com/latex.php?latex=J%2FT&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J/T" class="latex" title="J/T" /> , we observe a <em>phase transition</em>. If <img src="https://s0.wp.com/latex.php?latex=J%2FT&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J/T" class="latex" title="J/T" /> is small, then the coupling between the random variables is weak and the different parts of the system are almost independent; we call this the <strong>disordered phase</strong>. If <img src="https://s0.wp.com/latex.php?latex=J%2FT&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J/T" class="latex" title="J/T" /> is large, then the spins want to align in the same direction and the Gibbs distribution will look almost like the following: with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" /> , all spins are <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> , and with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" /> , all spins are <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> ; we call this the <strong>ordered phase</strong>.</p>



<p>In the disordered phase, when the spins do not need to align so closely, the Metropolis-Hastings algorithm will work well. In the ordered phase, the algorithm is doomed. Indeed, suppose that most of the spins are <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> . As time proceeds, any <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> s will switch to <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> . There may be islands of <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> spins initially, but it will be energetically favorable for these islands to shrink over time. Therefore, there will be an exponentially small chance for the system to switch to a configuration with mostly <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> ’s, and thus the chain takes exponentially long to mix. Here, people are interested in understanding the <em>autocorrelation time</em>, because the goal is to run the chain for some time, get one sample, run the chain for some more time, get another sample, etc.</p>



<h2 id="glauber-dynamics">Glauber dynamics</h2>



<p>This next method (<strong>Glauber dynamics</strong>) is essentially the same as Metropolis-Hastings, but this is not immediately obvious. We are at a state <img src="https://s0.wp.com/latex.php?latex=x+%3D+%28x_1%2C%5Cdotsc%2Cx_n%29+%5Cin+%5CSigma%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x = (x_1,\dotsc,x_n) \in \Sigma^n" class="latex" title="x = (x_1,\dotsc,x_n) \in \Sigma^n" /> . (For the Metropolis-Hastings algorithm, we could be walking on a state space without a product structure. However, Glauber dynamics requires a product structure.) Then, we update <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> to <img src="https://s0.wp.com/latex.php?latex=%28x_1%2C%5Cdotsc%2Cx_%7Bi-1%7D%2Cx_i%27%2Cx_%7Bi%2B1%7D%2C%5Cdotsc%2Cx_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x_1,\dotsc,x_{i-1},x_i',x_{i+1},\dotsc,x_n)" class="latex" title="(x_1,\dotsc,x_{i-1},x_i',x_{i+1},\dotsc,x_n)" /> with chance <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Bi%5Cmid+-i%7D%28x_i%27+%5Cmid+x_1%2C%5Cdotsc%2Cx_%7Bi-1%7D%2Cx_%7Bi%2B1%7D%2C%5Cdotsc%2Cx_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_{i\mid -i}(x_i' \mid x_1,\dotsc,x_{i-1},x_{i+1},\dotsc,x_n)" class="latex" title="\pi_{i\mid -i}(x_i' \mid x_1,\dotsc,x_{i-1},x_{i+1},\dotsc,x_n)" /> . In other words, we hold all other bits fixed, and conditioned on those other bits, we resample the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> th bit. Like Metropolis-Hastings, <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi" class="latex" title="\pi" /> is stationary for this chain.</p>



<p>It is not obvious that these conditional distributions can be computed efficiently, but it is possible since normalizing the conditional distribution only requires summing over the possible configurations for a single random variable. On a Markov network, the conditional probability is <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Bi+%5Cmid+N%28i%29%7D%28x_i%27+%5Cmid+x_%7BN%28i%29%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi_{i \mid N(i)}(x_i' \mid x_{N(i)})" class="latex" title="\pi_{i \mid N(i)}(x_i' \mid x_{N(i)})" /> , where <img src="https://s0.wp.com/latex.php?latex=N%28i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N(i)" class="latex" title="N(i)" /> denotes the set of neighbors of <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> . This makes the computation a constant-sized calculation (i.e., does not depend on the size of the system).</p>



<p>For example, in the Ising model, suppose we are at state <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5C%7B%5Cpm+1%5C%7D%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in {\{\pm 1\}}^n" class="latex" title="x \in {\{\pm 1\}}^n" /> . In Glauber dynamics, we pick a vertex <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n]" class="latex" title="i \in [n]" /> u.a.r. and update it to <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> with probability <img src="https://s0.wp.com/latex.php?latex=p_%7Bi+%5Cmid+N%28i%29%7D%28%2B+%5Cmid+x_%7BN%28i%29%7D%29+%3D+%5Cfrac%7B%5Cexp%28T%5E%7B-1%7D%5Csum_%7Bj%5Cin+N%28i%29%7D+x_j%29%7D%7B%5Cexp%28-T%5E%7B-1%7D+%5Csum_%7Bj%5Cin+N%28i%29%7D+x_j%29+%2B+%5Cexp%28T%5E%7B-1%7D+%5Csum_%7Bj%5Cin+N%28i%29%7D+x_j%29%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{i \mid N(i)}(+ \mid x_{N(i)}) = \frac{\exp(T^{-1}\sum_{j\in N(i)} x_j)}{\exp(-T^{-1} \sum_{j\in N(i)} x_j) + \exp(T^{-1} \sum_{j\in N(i)} x_j)}." class="latex" title="p_{i \mid N(i)}(+ \mid x_{N(i)}) = \frac{\exp(T^{-1}\sum_{j\in N(i)} x_j)}{\exp(-T^{-1} \sum_{j\in N(i)} x_j) + \exp(T^{-1} \sum_{j\in N(i)} x_j)}." /></p>



<h1 id="scn:mixing_in_time">Mixing in time</h1>



<p>Mixing in time means that the dynamics will equilibrate rapidly. It turns out that this is equivalent to mixing in space, which means that <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi" class="latex" title="\pi" /> itself has decaying correlations. For example, the Ising model at low temperature has a lot of long-range correlations, but at high temperature it does not. For the high temperature regime, we can prove that mixing in time occurs. We will prove this for the ferromagnetic Ising model. The result is known more generally, but the proofs are much easier for the Ising model.</p>



<p>People have known about the Metropolis-Hastings algorithm since the 1950s, but only recently have researchers been able to prove convergence guarantees for the 2D Ising model. There is a large gap between theory and practice, but in some situations we can prove that the algorithm works.</p>



<p>Sampling from the distribution is roughly equivalent to estimating the partition function (sampling-counting equivalence). There have been many papers addressing tasks such as estimating the non-negative permanent, the number of colorings of a graph, etc.<sup><a href="https://windowsontheory.org/feed/#fn_2">2</a></sup> A dominant way of accomplishing these tasks is proving that the Metropolis-Hastings algorithm converges for these problems. It is easy to find algorithms for these problems that converge to Gibbs distributions, but the convergence may take exponential time.</p>



<p>We will look at the situation when the energy function looks like the Ising model, in the sense that the interactions are local and reflect the structure of some underlying space. Also, assume that the interactions are of size <img src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1)" class="latex" title="O(1)" /> and that the scaling comes from the size of the system. When can we expect that our algorithms work? There are two main cases when we can argue that there should be rapid mixing.</p>



<ul><li>High temperature regime: The system is very disordered, and in the limit as the temperature approaches infinity, we get the uniform distribution.</li><li>One-dimension: In 1D, we can exactly compute the partition function using dynamic programming. Before, we mentioned that if there are a sea of <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> s and an island of <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> s, then it is energetically favorable for the island to shrink; note that this is no longer true in 1D. In a way, 1D systems are more “boring” because they cannot exhibit arbitrarily long-range correlations.</li></ul>



<p>In this part of the blog post, we will try to be more proof-oriented. We will start by explaining why it is plausible that high temperature means that the chain will mix rapidly in time.</p>



<h2 id="coupling-method">Coupling method</h2>



<p>One method of proving rates of convergence for Markov chains is by analzying the spectral gap. Another method is the <strong>coupling method</strong>.</p>



<p>The idea behind the coupling method is to start with two configurations <img src="https://s0.wp.com/latex.php?latex=X%280%29%2CY%280%29+%5Cin+%5COmega&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0),Y(0) \in \Omega" class="latex" title="X(0),Y(0) \in \Omega" /> . We want each one to evolve under the Markov chain.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/p4.png?w=600" alt="" class="wp-image-6787" /></figure>



<p>The key part is that there is still some freedom with respect to what the dynamics looks like. In particular, we are allowed to correlate the <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> and <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> processes. Thus, we are defining a joint transition probability <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+P%7D%5C%7BX%281%29%3Dx%281%29%2CY%281%29%3Dy%281%29+%5Cmid+X%280%29%2CY%280%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathbb P}\{X(1)=x(1),Y(1)=y(1) \mid X(0),Y(0)\}" class="latex" title="{\mathbb P}\{X(1)=x(1),Y(1)=y(1) \mid X(0),Y(0)\}" /> . We want to design the process such that <img src="https://s0.wp.com/latex.php?latex=X%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(1)" class="latex" title="X(1)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(1)" class="latex" title="Y(1)" /> are closer together than <img src="https://s0.wp.com/latex.php?latex=X%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0)" class="latex" title="X(0)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(0)" class="latex" title="Y(0)" /> . Imagine that we have two particles bouncing around. Each particle follows the dynamics of <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> , but they are correlated so that they drift together, and once they meet, they stick together. It turns out that the mixing time can be upper bounded by the time it takes for the particles to meet each other.</p>



<p>Assume we have some sort of distance function <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bdist%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{dist}" class="latex" title="\;\mathrm{dist}" /> on the underlying space and we can prove that <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7B%5Cmathbb+E%7D%5C%3B%5Cmathrm%7Bdist%7D%28X%281%29%2CY%281%29%29+%5Cle+%5Cexp%28-%5Calpha%29+%5C%3B%5Cmathrm%7Bdist%7D%28X%280%29%2CY%280%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{\mathbb E}\;\mathrm{dist}(X(1),Y(1)) \le \exp(-\alpha) \;\mathrm{dist}(X(0),Y(0))" class="latex" title="\;\mathrm{\mathbb E}\;\mathrm{dist}(X(1),Y(1)) \le \exp(-\alpha) \;\mathrm{dist}(X(0),Y(0))" /> . Then, it turns out that the mixing time <img src="https://s0.wp.com/latex.php?latex=t_%7B%5Crm+mix%7D%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_{\rm mix}(\epsilon)" class="latex" title="t_{\rm mix}(\epsilon)" /> , i.e. the time required to get within <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> of the stationary distribution, is upper bounded as</p>



<p> <img src="https://s0.wp.com/latex.php?latex=t_%7B%5Crm+mix%7D%28%5Cepsilon%29+%5Cle+%5Cfrac%7B%5Clog%5C%7B%28%5C%3B%5Cmathrm%7Bdiam%7D%5COmega%29%2F%5Cepsilon%5C%7D%7D%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_{\rm mix}(\epsilon) \le \frac{\log\{(\;\mathrm{diam}\Omega)/\epsilon\}}{\alpha}" class="latex" title="t_{\rm mix}(\epsilon) \le \frac{\log\{(\;\mathrm{diam}\Omega)/\epsilon\}}{\alpha}" /> </p>



<p>Initially, the two particles can be <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bdiam%7D%5COmega&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{diam}\Omega" class="latex" title="\;\mathrm{diam}\Omega" /> apart, but the expected distance is exponentially shrinking as we run the coupling, so the mixing time is logarithmic in the diameter.</p>



<p>The distance between probability distributions is defined as follows. Let <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> be two probability distributions on <img src="https://s0.wp.com/latex.php?latex=%5COmega&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega" class="latex" title="\Omega" /> . Then, the metric is:<sup><a href="https://windowsontheory.org/feed/#fn_3">3</a></sup></p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B2%7D+%5C%7Cp-q%5C%7C_1+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bx+%5Cin+%5COmega%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmin_%7B%5Csubstack%7B%28X%2CY%29+%5Csim+r+%5Cin+%7B%5Cmathcal%7BP%7D%7D%28%5COmega+%5Ctimes+%5COmega%29+%5C%5C+r_1+%3D+p+%5C%5C+r_2+%3D+q%7D%7D+%7B%5Cmathbb+P%7D_r%5C%7BX+%5Cne+Y%5C%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{2} \|p-q\|_1 = \frac{1}{2}\sum_{x \in \Omega}|p(x)-q(x)| = \min_{\substack{(X,Y) \sim r \in {\mathcal{P}}(\Omega \times \Omega) \\ r_1 = p \\ r_2 = q}} {\mathbb P}_r\{X \ne Y\}." class="latex" title="\frac{1}{2} \|p-q\|_1 = \frac{1}{2}\sum_{x \in \Omega}|p(x)-q(x)| = \min_{\substack{(X,Y) \sim r \in {\mathcal{P}}(\Omega \times \Omega) \\ r_1 = p \\ r_2 = q}} {\mathbb P}_r\{X \ne Y\}." /> </p>



<p>In this expression, <img src="https://s0.wp.com/latex.php?latex=r_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_1" class="latex" title="r_1" /> and <img src="https://s0.wp.com/latex.php?latex=r_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r_2" class="latex" title="r_2" /> denote the first and second marginals of <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> respectively. The minimum is taken over all <em>couplings</em> of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> . This is the correct way to measure the distance between distributions. To give some intuition for this quantity, the quantity on the right represents the best <em>test</em> to distinguish the two distributions. If <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> are the same, we can take a coupling in which <img src="https://s0.wp.com/latex.php?latex=X+%5Csim+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X \sim p" class="latex" title="X \sim p" /> and <img src="https://s0.wp.com/latex.php?latex=Y+%5Csim+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \sim q" class="latex" title="Y \sim q" /> are always identical. If <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> have disjoint supports, then no matter what coupling we use, <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> and <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> will never be equal.</p>



<p>It suffices to consider when <img src="https://s0.wp.com/latex.php?latex=X%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0)" class="latex" title="X(0)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(0)" class="latex" title="Y(0)" /> are neighbors, i.e. at distance <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> apart. This is because if we have <img src="https://s0.wp.com/latex.php?latex=X%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0)" class="latex" title="X(0)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(0)" class="latex" title="Y(0)" /> far apart, then we could look at the path between them and reduce to the case when they are neighbors. Formally, this is known as <em>path coupling</em>. The formal statement is in Theorem 12.3 of <a href="https://windowsontheory.org/feed/#nature">[4]</a>:</p>



<p><strong>Theorem 3</strong>: <em>Let <img src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Gamma" class="latex" title="\Gamma" /> be a connected weighted graph on the state space, where no edge has weight less than <img src="https://s0.wp.com/latex.php?latex=d_%7B%5Cmin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d_{\min}" class="latex" title="d_{\min}" /> . Let <img src="https://s0.wp.com/latex.php?latex=d%28C%2CC%27%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d(C,C')" class="latex" title="d(C,C')" /> be the length of the shortest path from <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> to <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" /> in <img src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Gamma" class="latex" title="\Gamma" /> and let <img src="https://s0.wp.com/latex.php?latex=d_%7B%5Cmax%7D+%3D+%5Cmax_%7BC%2CC%27+%5Cin+%5COmega%7D+d%28C%2CC%27%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d_{\max} = \max_{C,C' \in \Omega} d(C,C')" class="latex" title="d_{\max} = \max_{C,C' \in \Omega} d(C,C')" /> be the diameter of <img src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Gamma" class="latex" title="\Gamma" /> . Suppose there is a coupling such that for some <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\delta &gt; 0" class="latex" title="\delta &gt; 0" /> </em>,</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7B%5Cmathbb+E%7D%5Cbigl%5Bd%5Cbigl%28X%281%29%2CY%281%29%5Cbigr%29+%5Cbigm%5Cvert+%5Cbigl%28X%280%29%2CY%280%29%5Cbigr%29+%3D+%28C%2CC%27%29%5Cbigr%5D+%5Cle+%281-%5Cdelta%29d%28C%2CC%27%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{\mathbb E}\bigl[d\bigl(X(1),Y(1)\bigr) \bigm\vert \bigl(X(0),Y(0)\bigr) = (C,C')\bigr] \le (1-\delta)d(C,C')" class="latex" title="\;\mathrm{\mathbb E}\bigl[d\bigl(X(1),Y(1)\bigr) \bigm\vert \bigl(X(0),Y(0)\bigr) = (C,C')\bigr] \le (1-\delta)d(C,C')" /> </p>



<p><em>for all neighboring pairs <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> , <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" /> , i.e., those pairs connected by an edge in <img src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Gamma" class="latex" title="\Gamma" /> . Then, the mixing time is bounded by </em></p>



<p> <img src="https://s0.wp.com/latex.php?latex=t_%7B%5Crm+mix%7D%28%5Cepsilon%29+%5Cle+%5Cfrac%7B%5Clog%28%5Cepsilon%5E%7B-1%7Dd_%7B%5Cmax%7D%2Fd_%7B%5Cmin%7D%29%7D%7B%5Cdelta%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_{\rm mix}(\epsilon) \le \frac{\log(\epsilon^{-1}d_{\max}/d_{\min})}{\delta}." class="latex" title="t_{\rm mix}(\epsilon) \le \frac{\log(\epsilon^{-1}d_{\max}/d_{\min})}{\delta}." /> </p>



<h2 id="glauber-dynamics-at-high-temperature">Glauber dynamics at high temperature</h2>



<p>Recall that in Glauber dynamics, we pick a site <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> randomly and then update the site conditioned on its neighbors. The first way we will couple together <img src="https://s0.wp.com/latex.php?latex=X%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(1)" class="latex" title="X(1)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(1)" class="latex" title="Y(1)" /> is by picking the <em>same</em> site for both of them.</p>



<ol><li>Pick a random <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n]" class="latex" title="i \in [n]" /> .</li><li>If <img src="https://s0.wp.com/latex.php?latex=%7BX%280%29%7D_%7BN%28i%29%7D+%3D+%7BY%280%29%7D_%7BN%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{X(0)}_{N(i)} = {Y(0)}_{N(i)}" class="latex" title="{X(0)}_{N(i)} = {Y(0)}_{N(i)}" /> , then set <img src="https://s0.wp.com/latex.php?latex=%7BX%281%29%7D_i+%3D+%7BY%281%29%7D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{X(1)}_i = {Y(1)}_i" class="latex" title="{X(1)}_i = {Y(1)}_i" /> (if the neighborhoods of the two points agree, then update them the same way). Otherwise, update them using the best possible coupling, i.e., pick a coupling for <img src="https://s0.wp.com/latex.php?latex=%28%7BX%281%29%7D_i%2C+%7BY%281%29%7D_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="({X(1)}_i, {Y(1)}_i)" class="latex" title="({X(1)}_i, {Y(1)}_i)" /> which minimizes <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+P%7D%5C%7B+%7BX%281%29%7D_i+%5Cne+%7BY%281%29%7D_i+%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathbb P}\{ {X(1)}_i \ne {Y(1)}_i \}" class="latex" title="{\mathbb P}\{ {X(1)}_i \ne {Y(1)}_i \}" /> .</li></ol>



<p>So if <img src="https://s0.wp.com/latex.php?latex=X%280%29+%3D+Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0) = Y(0)" class="latex" title="X(0) = Y(0)" /> , then the points will never drift apart. The reason why analyzing this coupling is non-trivial is because there is a chance that the distance between the two points can <em>increase</em>.</p>



<p>Assume that the degree of the graph is <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> . Suppose that <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bdist%7D%28X%280%29%2CY%280%29%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{dist}(X(0),Y(0)) = 1" class="latex" title="\;\mathrm{dist}(X(0),Y(0)) = 1" /> , that is, there is a single <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BX%280%29%7D_a+%5Cne+%7BY%280%29%7D_a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{X(0)}_a \ne {Y(0)}_a" class="latex" title="{X(0)}_a \ne {Y(0)}_a" /> . What will happen to <img src="https://s0.wp.com/latex.php?latex=X%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(1)" class="latex" title="X(1)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(1)" class="latex" title="Y(1)" /> ? We start by picking a random <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in [n]" class="latex" title="i \in [n]" /> . There are three cases:</p>



<ol><li><img src="https://s0.wp.com/latex.php?latex=i+%5Cnotin+%28%5C%7Ba%5C%7D+%5Ccup+N%28a%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \notin (\{a\} \cup N(a))" class="latex" title="i \notin (\{a\} \cup N(a))" /> (with probability <img src="https://s0.wp.com/latex.php?latex=1+-+%28%5CDelta+%2B+1%29%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - (\Delta + 1)/n" class="latex" title="1 - (\Delta + 1)/n" /> ): Nothing changes; <img src="https://s0.wp.com/latex.php?latex=X%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0)" class="latex" title="X(0)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(0)" class="latex" title="Y(0)" /> agree at <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> , and <img src="https://s0.wp.com/latex.php?latex=X%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(1)" class="latex" title="X(1)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(1)" class="latex" title="Y(1)" /> will also agree at <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> . The distance remains at <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> .</li><li><img src="https://s0.wp.com/latex.php?latex=i+%3D+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i = a" class="latex" title="i = a" /> (with probability <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/n" class="latex" title="1/n" /> ): We picked the one spot in which the two configurations differ. The neighborhoods of <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> are the same for <img src="https://s0.wp.com/latex.php?latex=X%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0)" class="latex" title="X(0)" /> and <img src="https://s0.wp.com/latex.php?latex=Y%280%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y(0)" class="latex" title="Y(0)" /> , so we update in the same way for both processes, and the distance drops to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> .</li><li><img src="https://s0.wp.com/latex.php?latex=i+%5Cin+N%28a%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \in N(a)" class="latex" title="i \in N(a)" /> (with probability <img src="https://s0.wp.com/latex.php?latex=%5CDelta%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta/n" class="latex" title="\Delta/n" /> ): We could have different updates. Here, we have to use the high temperature assumption, which says that if we change one bit, the probability of a configuration cannot change too much.In the Ising model, <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%5Csum_%7Bi%2Cj%3D1%7D%5En+J_%7Bi%2Cj%7D+x_i+x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = \sum_{i,j=1}^n J_{i,j} x_i x_j" class="latex" title="E(x) = \sum_{i,j=1}^n J_{i,j} x_i x_j" /> . Changing <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> can bias the energy by at most <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5Cmax_i+J_%7Bi%2Ca%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta\max_i J_{i,a}" class="latex" title="\Delta\max_i J_{i,a}" /> , so the expected distance afterwards is <img src="https://s0.wp.com/latex.php?latex=1+%2B+O%28%5Cmax_%7Bi%2Cj%3D1%7D%5En+J_%7Bi%2Cj%7D%2FT%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 + O(\max_{i,j=1}^n J_{i,j}/T)" class="latex" title="1 + O(\max_{i,j=1}^n J_{i,j}/T)" /> .</li></ol>



<p>Adding these cases up to get the overall expected distance gives</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7B%5Cmathbb+E%7D%5C%3B%5Cmathrm%7Bdist%7D%5Cbigl%28X%281%29%2C+Y%281%29%5Cbigr%29+%3D+1-%5Cfrac%7B1%7D%7Bn%7D+%2B+%5Cunderbrace%7BO%5CBigl%28%5Cfrac%7B%5CDelta+J_%7B%5Cmax%7D%7D%7BT%7D%5CBigr%29%7D_%7B%5Cle+1%7D%5Cfrac%7B1%7D%7Bn%7D+%3D+1+-+%5Cfrac%7Bc%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{\mathbb E}\;\mathrm{dist}\bigl(X(1), Y(1)\bigr) = 1-\frac{1}{n} + \underbrace{O\Bigl(\frac{\Delta J_{\max}}{T}\Bigr)}_{\le 1}\frac{1}{n} = 1 - \frac{c}{n}" class="latex" title="\;\mathrm{\mathbb E}\;\mathrm{dist}\bigl(X(1), Y(1)\bigr) = 1-\frac{1}{n} + \underbrace{O\Bigl(\frac{\Delta J_{\max}}{T}\Bigr)}_{\le 1}\frac{1}{n} = 1 - \frac{c}{n}" /> </p>



<p>for <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> large enough, so the expected distance will shrink. This argument also tells us how large the temperature must be, which is important for applications. This gives us <img src="https://s0.wp.com/latex.php?latex=t_%7B%5Crm+mix%7D%28%5Cepsilon%29+%3D+O%5CBigl%28n%5Clog%5Cfrac%7Bn%7D%7B%5Cepsilon%7D%5CBigr%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_{\rm mix}(\epsilon) = O\Bigl(n\log\frac{n}{\epsilon}\Bigr)." class="latex" title="t_{\rm mix}(\epsilon) = O\Bigl(n\log\frac{n}{\epsilon}\Bigr)." /> Notice that this is the same dependence as the coupon collector problem. Therefore, in the high temperature regime, the system behaves qualitatively as if there are no correlations.</p>



<h2 id="temporal-and-spatial-mixing-equivalence">Temporal and spatial mixing equivalence</h2>



<p>The analysis of Glauber dynamics at high temperature is already a version of the equivalence between mixing in time and mixing in space. It says that if the correlations even with the immediate neighbors of a node are weak, then Glauber dynamics rapidly mixes.</p>



<p>Now, we want to consider the situation in which there can be strong correlations between immediate neighbors, but weak correlation with far away sites. We want to show that spatial mixing implies temporal mixing.</p>



<p>We will give a few definitions of correlation decay. (Note: The definitions of correlation decay below are not exactly the ones from Aram’s lecture. These definitions are from <a href="https://windowsontheory.org/feed/#martinelli1">[5]</a> and <a href="https://windowsontheory.org/feed/#martinelli2">[6]</a>.)</p>



<p>For non-empty <img src="https://s0.wp.com/latex.php?latex=W+%5Csubseteq+V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W \subseteq V" class="latex" title="W \subseteq V" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%5Cin+%5CSigma%5E%7BV%5Csetminus+W%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tau \in \Sigma^{V\setminus W}" class="latex" title="\tau \in \Sigma^{V\setminus W}" /> , let <img src="https://s0.wp.com/latex.php?latex=%5Cmu_W%5E%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_W^\tau" class="latex" title="\mu_W^\tau" /> be the distribution of the spins in <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> conditional on the spins in <img src="https://s0.wp.com/latex.php?latex=V+%5Csetminus+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V \setminus W" class="latex" title="V \setminus W" /> being fixed to <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tau" class="latex" title="\tau" /> . For <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%5Csubseteq+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta \subseteq W" class="latex" title="\Delta \subseteq W" /> , let <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7BW%2C%5CDelta%7D%5E%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{W,\Delta}^\tau" class="latex" title="\mu_{W,\Delta}^\tau" /> be the marginal of <img src="https://s0.wp.com/latex.php?latex=%5Cmu_W%5E%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_W^\tau" class="latex" title="\mu_W^\tau" /> on the spins in <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> . We will assume that the interactions between the spins have finite range <img src="https://s0.wp.com/latex.php?latex=r+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r &gt; 0" class="latex" title="r &gt; 0" /> , and <img src="https://s0.wp.com/latex.php?latex=%5Cpartial_r+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial_r W" class="latex" title="\partial_r W" /> denotes the <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> -boundary of <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> , i.e., <img src="https://s0.wp.com/latex.php?latex=%5C%7Bv+%5Cin+V+%5Csetminus+W+%3A+%5C%3B%5Cmathrm%7Bdist%7D%28v%2CW%29+%5Cle+r%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{v \in V \setminus W : \;\mathrm{dist}(v,W) \le r\}" class="latex" title="\{v \in V \setminus W : \;\mathrm{dist}(v,W) \le r\}" /> .</p>



<ul><li>(<strong>Weak decay of correlations</strong>) Weak spatial mixing holds for <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> if there exist constants <img src="https://s0.wp.com/latex.php?latex=C%2C+%5Cxi+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C, \xi &gt; 0" class="latex" title="C, \xi &gt; 0" /> such that for any subset <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%5Csubseteq+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta \subseteq W" class="latex" title="\Delta \subseteq W" /> , <img src="https://s0.wp.com/latex.php?latex=%5Csup_%7B%5Ctau%2C%5Ctau%27+%5Cin+%5CSigma%5E%7BV%5Csetminus+W%7D%7D%5C%7C%5Cmu_%7BW%2C%5CDelta%7D%5E%5Ctau+-+%5Cmu_%7BW%2C%5CDelta%7D%5E%7B%5Ctau%27%7D%5C%7C_1+%5Cle+C%5Csum_%7Bx%5Cin%5CDelta%2C+%5C%3B+y+%5Cin+%5Cpartial_r+W%7D+%5Cexp%5CBigl%28-+%5Cfrac%7B%5C%3B%5Cmathrm%7Bdist%7D%28x%2Cy%29%7D%7B%5Cxi%7D%5CBigr%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sup_{\tau,\tau' \in \Sigma^{V\setminus W}}\|\mu_{W,\Delta}^\tau - \mu_{W,\Delta}^{\tau'}\|_1 \le C\sum_{x\in\Delta, \; y \in \partial_r W} \exp\Bigl(- \frac{\;\mathrm{dist}(x,y)}{\xi}\Bigr)." class="latex" title="\sup_{\tau,\tau' \in \Sigma^{V\setminus W}}\|\mu_{W,\Delta}^\tau - \mu_{W,\Delta}^{\tau'}\|_1 \le C\sum_{x\in\Delta, \; y \in \partial_r W} \exp\Bigl(- \frac{\;\mathrm{dist}(x,y)}{\xi}\Bigr)." /></li><li>(<strong>Strong decay of correlations</strong>) Strong spatial mixing holds for <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="W" class="latex" title="W" /> if there exist constants <img src="https://s0.wp.com/latex.php?latex=C%2C%5Cxi+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C,\xi &gt; 0" class="latex" title="C,\xi &gt; 0" /> such that for every <img src="https://s0.wp.com/latex.php?latex=%5CDelta+%5Csubseteq+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta \subseteq W" class="latex" title="\Delta \subseteq W" /> and every <img src="https://s0.wp.com/latex.php?latex=%5Ctau%2C%5Ctau%27+%5Cin+%5CSigma%5E%7BV%5Csetminus+W%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tau,\tau' \in \Sigma^{V\setminus W}" class="latex" title="\tau,\tau' \in \Sigma^{V\setminus W}" /> differing only at site <img src="https://s0.wp.com/latex.php?latex=y+%5Cin+V%5Csetminus+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y \in V\setminus W" class="latex" title="y \in V\setminus W" /> , <img src="https://s0.wp.com/latex.php?latex=%5C%7C%5Cmu_%7BW%2C%5CDelta%7D%5E%5Ctau+-+%5Cmu_%7BW%2C%5CDelta%7D%5E%7B%5Ctau%27%7D%5C%7C_1+%5Cle+C%5Cexp%5CBigl%28-%5Cfrac%7B%5C%3B%5Cmathrm%7Bdist%7D%28y%2C%5CDelta%29%7D%7B%5Cxi%7D%5CBigr%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\|\mu_{W,\Delta}^\tau - \mu_{W,\Delta}^{\tau'}\|_1 \le C\exp\Bigl(-\frac{\;\mathrm{dist}(y,\Delta)}{\xi}\Bigr)." class="latex" title="\|\mu_{W,\Delta}^\tau - \mu_{W,\Delta}^{\tau'}\|_1 \le C\exp\Bigl(-\frac{\;\mathrm{dist}(y,\Delta)}{\xi}\Bigr)." /></li><li>(<strong>Strong decay of correlations</strong>) Strong spatial mixing in the <em>truncated</em> sense holds for <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V" class="latex" title="V" /> if there exist <img src="https://s0.wp.com/latex.php?latex=n%2C+%5Cxi+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n, \xi &gt; 0" class="latex" title="n, \xi &gt; 0" /> such that for all functions <img src="https://s0.wp.com/latex.php?latex=f%2C+g+%3A+%5COmega+%5Cto+%7B%5Cmathbb+R%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f, g : \Omega \to {\mathbb R}" class="latex" title="f, g : \Omega \to {\mathbb R}" /> which depend only on the sites at <img src="https://s0.wp.com/latex.php?latex=%5CLambda_f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Lambda_f" class="latex" title="\Lambda_f" /> and <img src="https://s0.wp.com/latex.php?latex=%5CLambda_g&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Lambda_g" class="latex" title="\Lambda_g" /> respectively and such that <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bdist%7D%28%5CLambda_f%2C%5CLambda_g%29+%5Cge+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{dist}(\Lambda_f,\Lambda_g) \ge n" class="latex" title="\;\mathrm{dist}(\Lambda_f,\Lambda_g) \ge n" /> , <img src="https://s0.wp.com/latex.php?latex=%5Csup_%7B%5Ctau+%5Cin+%5CSigma%5E%7BV%5Csetminus+W%7D%7D+%5C%3B%5Cmathrm%7Bcov%7D_%7B%5Cmu_W%5E%5Ctau%7D%28f%2C+g%29+%5Cle+%7C%5CLambda_f%7C%7C%5CLambda_g%7C%5C%7Cf%5C%7C_%5Cinfty+%5C%7Cg%5C%7C_%5Cinfty+%5Cexp%5CBigl%28-%5Cfrac%7Bd%28%5CLambda_f%2C%5CLambda_g%29%7D%7B%5Cxi%7D%5CBigr%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sup_{\tau \in \Sigma^{V\setminus W}} \;\mathrm{cov}_{\mu_W^\tau}(f, g) \le |\Lambda_f||\Lambda_g|\|f\|_\infty \|g\|_\infty \exp\Bigl(-\frac{d(\Lambda_f,\Lambda_g)}{\xi}\Bigr)." class="latex" title="\sup_{\tau \in \Sigma^{V\setminus W}} \;\mathrm{cov}_{\mu_W^\tau}(f, g) \le |\Lambda_f||\Lambda_g|\|f\|_\infty \|g\|_\infty \exp\Bigl(-\frac{d(\Lambda_f,\Lambda_g)}{\xi}\Bigr)." /></li></ul>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\xi" class="latex" title="\xi" /> is the <strong>correlation length</strong> (in physics, it is the characteristic length scale of a system). In the disordered phase, the correlation length is a constant independent of system size. For our purposes, the main consequence of these definitions is that the effective interaction range of each spin is <img src="https://s0.wp.com/latex.php?latex=O%28%5Cxi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\xi)" class="latex" title="O(\xi)" /> . For the Ising model, there is a key simplification due to <em>monotonicity</em>. Namely, the ferromagnetic Ising model has the nice property (which is not true for other models) that if we flip a sign from <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> to <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> , this only makes <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> more likely everywhere. This is because the spins want to agree. There are a lot of boundary conditions to consider, but here, due to monotonicity, we only need to consider two: all of the spins are <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> , and all of the spins are <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> . All <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> spins will give the highest probability of a <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> spin, and all <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> spin will give the lowest probability of a <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> spin. This monotonicity property is generally not required for time-space mixing equivalence to hold, but it greatly simplifies proofs.</p>



<p>It is a very non-obvious fact that all of these notions of spatial mixing are equivalent. We will sketch a proof that strong correlation decay implies that <img src="https://s0.wp.com/latex.php?latex=t_%7B%5Crm+mix%7D+%3D+O%28n%5Clog+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_{\rm mix} = O(n\log n)" class="latex" title="t_{\rm mix} = O(n\log n)" /> .</p>



<p>The idea is to use another coupling argument. Let <img src="https://s0.wp.com/latex.php?latex=X%280%29%2C+Y%280%29+%5Cin+%7B%5C%7B%5Cpm+1%5C%7D%7D%5EV&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X(0), Y(0) \in {\{\pm 1\}}^V" class="latex" title="X(0), Y(0) \in {\{\pm 1\}}^V" /> differ in one coordinate, i.e., <img src="https://s0.wp.com/latex.php?latex=%7BX%280%29%7D_a+%5Cne+%7BY%280%29%7D_a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{X(0)}_a \ne {Y(0)}_a" class="latex" title="{X(0)}_a \ne {Y(0)}_a" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%280%29%7D_i+%3D+%7BY%280%29%7D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{X(0)}_i = {Y(0)}_i" class="latex" title="{X(0)}_i = {Y(0)}_i" /> for <img src="https://s0.wp.com/latex.php?latex=i+%5Cne+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i \ne a" class="latex" title="i \ne a" /> . We want to argue that the expected distance between the processes will decrease. The proof uses a generalization of Glauber dynamics called <strong>block Glauber dynamics</strong>. In Glauber dynamics, we take a single spin and resample it conditioned on its neighbors. In block Glauber dynamics, we take an <img src="https://s0.wp.com/latex.php?latex=L%5Ctimes+L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L\times L" class="latex" title="L\times L" /> box and resample it conditioned on its neighbors. There is an argument, called <em>canonical paths</em>, which can be used to show that if block Glauber dynamics mixes, then regular Glauber dynamics also mixes (slightly more slowly; we lose a <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bpoly%7D%28L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{poly}(L)" class="latex" title="\;\mathrm{poly}(L)" /> factor, but anyway <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" /> will be a large constant) so analyzing block Glauber dynamics is fine.</p>



<p>If <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> lies in the box, then the expected change in distance is <img src="https://s0.wp.com/latex.php?latex=-L%5E2%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-L^2/n" class="latex" title="-L^2/n" /> . If <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> is far away from the box, then there is no change. If <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> is in the boundary of the box, then it is possible for the distance to increase. However, strong spatial mixing allows us to control the influence of a single site, so the expected change in distance is bounded by <img src="https://s0.wp.com/latex.php?latex=O%28L%5Cxi%5E2%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(L\xi^2/n)" class="latex" title="O(L\xi^2/n)" /> . Now, since <img src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\xi" class="latex" title="\xi" /> is a constant, if we choose <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" /> sufficiently large, then we will have the same situation as in the high temperature case: the expected distance will exponentially shrink over time.</p>



<h1 id="quantum-systems">Quantum systems</h1>



<p>The quantum version of Markov chains has many more difficulties. The first difficulty is that the Hammersley-Clifford theorem (which we have been relying on throughout this blog post) fails.</p>



<h2 id="notation">Notation</h2>



<p>To properly discuss what we mean, let’s set up some notation. Readers already familiar with density matrices, quantum entropy, and quantum mutual information may wish to skip to the next subsection. Most of the time we discuss quantum objects here, we’ll be using density matricies, often denoted <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> . A density matrix can be thought of as an extension to regular quantum states <img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Cpsi%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{\psi}\rangle " class="latex" title="|{\psi}\rangle " /> , where there is some classical source of uncertainty.</p>



<p>A density matrix is a positive semidefinite matrix with trace <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> . This extends the notion of a classical probability distribution; in the quantum setting, a classical probability distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> (thought of as a vector whose entries sum to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> ) is represented as the density matrix <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Bdiag%7D%28p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{diag}(p)" class="latex" title="\;\mathrm{diag}(p)" /> .</p>



<p>For example, we can consider a situation in which there is a <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" /> probability that we started with the quantum state <img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Cpsi%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{\psi}\rangle " class="latex" title="|{\psi}\rangle " /> and a <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" /> probability that we started with the quantum state <img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Cphi%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{\phi}\rangle " class="latex" title="|{\phi}\rangle " /> . This would be denoted as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%5Cfrac%7B1%7D%7B2%7D+%7C%7B%5Cpsi%7D%5Crangle+%5Clangle%7B%5Cpsi%7D%7C+%2B+%5Cfrac%7B1%7D%7B2%7D+%7C%7B%5Cphi%7D%5Crangle+%5Clangle%7B%5Cphi%7D%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho = \frac{1}{2} |{\psi}\rangle \langle{\psi}| + \frac{1}{2} |{\phi}\rangle \langle{\phi}| " class="latex" title="\rho = \frac{1}{2} |{\psi}\rangle \langle{\psi}| + \frac{1}{2} |{\phi}\rangle \langle{\phi}| " /> </p>



<p>Density matricies are generally useful for a lot of tasks, but for our purposes a density matrix will be used to discuss both the classical and quantum “uncertainty” we have about what state we have.</p>



<p>Now let’s also talk about a second important piece of notation: the tensor product. Often when discussing quantum states, it is important to discuss multiple quantum states simultaneously. For example, Alice has one system <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and Bob has another system <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . However, these systems might be entangled, meaning that the results of the two systems are correlated.</p>



<p>For instance, let us consider the following state:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Cpsi%7D%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28+%7C%7B%2B%7D%5Crangle+_A+%7C%7B%2B%7D%5Crangle+_B+%2B+%7C%7B-%7D%5Crangle+_A+%7C%7B-%7D%5Crangle+_B+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{\psi}\rangle = \frac{1}{\sqrt{2}}\left( |{+}\rangle _A |{+}\rangle _B + |{-}\rangle _A |{-}\rangle _B \right)" class="latex" title="|{\psi}\rangle = \frac{1}{\sqrt{2}}\left( |{+}\rangle _A |{+}\rangle _B + |{-}\rangle _A |{-}\rangle _B \right)" /> </p>



<p>This particular state has the property that Alice and Bob will always both measure <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="+" class="latex" title="+" /> or they will both measure <img src="https://s0.wp.com/latex.php?latex=-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-" class="latex" title="-" /> . The notation for tensors is often ambiguous in the literature as there are many ways of specifying tensors. For instance, above we used subscripts to explicitly denote which particle was in system <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and which was in system <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . One may also choose to simply use the index of the system as below. The symbol <img src="https://s0.wp.com/latex.php?latex=%5Cotimes&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\otimes" class="latex" title="\otimes" /> is used to denote a tensor between states (where it is assumed that the first state is system <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and the second, system <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> ). Gradually folks may shorten the notation as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7C%7B%5Cpsi%7D%5Crangle+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28+%7C%7B%2B%7D%5Crangle+%7C%7B%2B%7D%5Crangle+%2B+%7C%7B-%7D%5Crangle+%7C%7B-%7D%5Crangle+%5Cright%29%5C%5C+%7C%7B%5Cpsi%7D%5Crangle+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%2B+%7C%7B--%7D%5Crangle+%5Cright%29%5C%5C+%7C%7B%5Cpsi%7D%5Crangle+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D+%5Cbegin%7Bpmatrix%7D+1%5C%5C0+%5Cend%7Bpmatrix%7D+%5Cotimes+%5Cbegin%7Bpmatrix%7D+0%5C%5C1+%5Cend%7Bpmatrix%7D+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}}\left( |{+}\rangle |{+}\rangle + |{-}\rangle |{-}\rangle \right)\\ |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}}\left( |{++}\rangle + |{--}\rangle \right)\\ |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\0 \end{pmatrix} \otimes \begin{pmatrix} 0\\1 \end{pmatrix} \end{aligned} " class="latex" title="\begin{aligned} |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}}\left( |{+}\rangle |{+}\rangle + |{-}\rangle |{-}\rangle \right)\\ |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}}\left( |{++}\rangle + |{--}\rangle \right)\\ |{\psi}\rangle &amp;= \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\0 \end{pmatrix} \otimes \begin{pmatrix} 0\\1 \end{pmatrix} \end{aligned} " /> </p>



<p>These are all notations for the same state. Let’s now talk about this state in the context of a density matrix. The density matrix of this state is as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Crho_%7BA%2CB%7D+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%2B+%7C%7B--%7D%5Crangle+%5Cright%29+%5Cleft%28+%5Clangle%7B%2B%2B%7D%7C+%2B+%5Clangle%7B--%7D%7C+%5Cright%29%5C%5C+%5Crho_%7BA%2CB%7D+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B--%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B--%7D%7C+%5Cright%29+%5C%5C+%5Crho_%7BA%2CB%7D+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cbegin%7Bpmatrix%7D+1%260%260%261%5C%5C0%260%260%260%5C%5C0%260%260%260%5C%5C1%260%260%261+%5Cend%7Bpmatrix%7D%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \rho_{A,B} &amp;= \frac{1}{2} \left( |{++}\rangle + |{--}\rangle \right) \left( \langle{++}| + \langle{--}| \right)\\ \rho_{A,B} &amp;= \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) \\ \rho_{A,B} &amp;= \frac{1}{2} \begin{pmatrix} 1&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0\\1&amp;0&amp;0&amp;1 \end{pmatrix}\end{aligned} " class="latex" title="\begin{aligned} \rho_{A,B} &amp;= \frac{1}{2} \left( |{++}\rangle + |{--}\rangle \right) \left( \langle{++}| + \langle{--}| \right)\\ \rho_{A,B} &amp;= \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) \\ \rho_{A,B} &amp;= \frac{1}{2} \begin{pmatrix} 1&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0\\1&amp;0&amp;0&amp;1 \end{pmatrix}\end{aligned} " /> </p>



<p>Writing the density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> as <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BA%2CB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{A,B}" class="latex" title="\rho_{A,B}" /> makes explicit that this is the density matrix over systems <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> .</p>



<p>A crucial operation that one will often perform using density matricies is the partial trace. The partial trace is a way of allowing us to consider only a smaller part of the larger part of the system, while taking into account the influence of the larger system around it.</p>



<p>Here’s an example: Suppose Bob wants to know what his state is. However, Bob really doesn’t care about Alice’s system and just wants to know what the density matrix for his system is. Bob’s density matrix is simply the following density matrix (a 50% chance of being in <img src="https://s0.wp.com/latex.php?latex=%7C%7B%2B%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{+}\rangle " class="latex" title="|{+}\rangle " /> and a 50% chance of being in <img src="https://s0.wp.com/latex.php?latex=%7C%7B-%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{-}\rangle " class="latex" title="|{-}\rangle " /> ).</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Crho_%7BB%7D+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%7D%5Crangle+%5Clangle%7B%2B%7D%7C+%2B+%7C%7B-%7D%5Crangle+%5Clangle%7B-%7D%7C+%5Cright%29+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \rho_{B} &amp;= \frac{1}{2} \left( |{+}\rangle \langle{+}| + |{-}\rangle \langle{-}| \right) \end{aligned} " class="latex" title="\begin{aligned} \rho_{B} &amp;= \frac{1}{2} \left( |{+}\rangle \langle{+}| + |{-}\rangle \langle{-}| \right) \end{aligned} " /> </p>



<p>More explicitly, we could write the following:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Crho_%7BB%7D+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%7D%5Crangle+_B+%5Clangle%7B%2B%7D%7C+_B+%2B+%7C%7B-%7D%5Crangle+_B+%5Clangle%7B-%7D%7C+_B+%5Cright%29+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \rho_{B} &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B + |{-}\rangle _B \langle{-}| _B \right) \end{aligned} " class="latex" title="\begin{aligned} \rho_{B} &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B + |{-}\rangle _B \langle{-}| _B \right) \end{aligned} " /> </p>



<p>The partial trace is an operation that will let us take our original density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BA%2CB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{A,B}" class="latex" title="\rho_{A,B}" /> and generates a new density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_B" class="latex" title="\rho_B" /> that ignores system <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> . This is specifically called the partial trace over <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> , or <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Btr%7D_A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{tr}_A" class="latex" title="\;\mathrm{tr}_A" /> .</p>



<p>So how do we do this? We simply sum over the state <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> (effectively taking a trace, but only along one axis):</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5C%3B%5Cmathrm%7Btr%7D_A+%5Crho_%7BA%2CB%7D+%26%3D+%5Csum_i+%5Clangle%7Bi%7D%7C+_A+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B--%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B--%7D%7C+%5Cright%29+%7C%7Bi%7D%5Crangle+_A%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \;\mathrm{tr}_A \rho_{A,B} &amp;= \sum_i \langle{i}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{i}\rangle _A\end{aligned} " class="latex" title="\begin{aligned} \;\mathrm{tr}_A \rho_{A,B} &amp;= \sum_i \langle{i}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{i}\rangle _A\end{aligned} " /> </p>



<p>This is easier to evaluate using certain choices of notation:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5C%3B%5Cmathrm%7Btr%7D_A+%5Crho_%7BA%2CB%7D+%26%3D+%5Clangle%7B%2B%7D%7C+_A+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B--%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B--%7D%7C+%5Cright%29+%7C%7B%2B%7D%5Crangle+_A+%5C%5C%26%5Cqquad+%7B%7D%2B+%5Clangle%7B-%7D%7C+_A+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B%2B%2B%7D%5Crangle+%5Clangle%7B--%7D%7C+%2B+%7C%7B--%7D%5Crangle+%5Clangle%7B--%7D%7C+%5Cright%29+%7C%7B-%7D%5Crangle+_A%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%7D%5Crangle+_B+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B%2B%7D%5Crangle+_B+%5Clangle%7B--%7D%7C+%5Cright%29+%7C%7B%2B%7D%5Crangle+_A+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B-%7D%5Crangle+_B+%5Clangle%7B%2B%2B%7D%7C+%2B+%7C%7B-%7D%5Crangle+_B+%5Clangle%7B--%7D%7C+%5Cright%29+%7C%7B-%7D%5Crangle+_A%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%7D%5Crangle+_B+%5Clangle%7B%2B%7D%7C+_B+%5Cright%29+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B-%7D%5Crangle+_B+%5Clangle%7B-%7D%7C+_B+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%28+%7C%7B%2B%7D%5Crangle+_B+%5Clangle%7B%2B%7D%7C+_B+%2B+%7C%7B-%7D%5Crangle+_B+%5Clangle%7B-%7D%7C+_B+%5Cright%29+%3D+%5Crho_B%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \;\mathrm{tr}_A \rho_{A,B} &amp;= \langle{+}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{+}\rangle _A \\&amp;\qquad {}+ \langle{-}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{-}\rangle _A\\ &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{++}| + |{+}\rangle _B \langle{--}| \right) |{+}\rangle _A + \frac{1}{2} \left( |{-}\rangle _B \langle{++}| + |{-}\rangle _B \langle{--}| \right) |{-}\rangle _A\\ &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B \right) + \frac{1}{2} \left( |{-}\rangle _B \langle{-}| _B \right) = \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B + |{-}\rangle _B \langle{-}| _B \right) = \rho_B\end{aligned} " class="latex" title="\begin{aligned} \;\mathrm{tr}_A \rho_{A,B} &amp;= \langle{+}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{+}\rangle _A \\&amp;\qquad {}+ \langle{-}| _A \frac{1}{2} \left( |{++}\rangle \langle{++}| + |{--}\rangle \langle{++}| + |{++}\rangle \langle{--}| + |{--}\rangle \langle{--}| \right) |{-}\rangle _A\\ &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{++}| + |{+}\rangle _B \langle{--}| \right) |{+}\rangle _A + \frac{1}{2} \left( |{-}\rangle _B \langle{++}| + |{-}\rangle _B \langle{--}| \right) |{-}\rangle _A\\ &amp;= \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B \right) + \frac{1}{2} \left( |{-}\rangle _B \langle{-}| _B \right) = \frac{1}{2} \left( |{+}\rangle _B \langle{+}| _B + |{-}\rangle _B \langle{-}| _B \right) = \rho_B\end{aligned} " /> </p>



<p>This gives us the answer that we had expected.</p>



<p>We now have all of the tools we need to talk about quantum entropy. Intuitively, entropy can be thought of as the amount of uncertainty we have for our system, or equivalently the amount of information it takes to define our system. The entropy for a quantum system <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> is defined as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H%28%5Crho%29+%26%3D+-%5C%3B%5Cmathrm%7Btr%7D%28%5Crho+%5Clog_2+%5Crho%29%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} H(\rho) &amp;= -\;\mathrm{tr}(\rho \log_2 \rho)\end{aligned} " class="latex" title="\begin{aligned} H(\rho) &amp;= -\;\mathrm{tr}(\rho \log_2 \rho)\end{aligned} " /> </p>



<p>Note that here we use the shorthand <img src="https://s0.wp.com/latex.php?latex=%5Crho_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_B" class="latex" title="\rho_B" /> to denote <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Btr%7D_A+%5Crho_%7BA%2CB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{tr}_A \rho_{A,B}" class="latex" title="\;\mathrm{tr}_A \rho_{A,B}" /> . Here, writing <img src="https://s0.wp.com/latex.php?latex=%5C%3B%5Cmathrm%7Btr%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\;\mathrm{tr}" class="latex" title="\;\mathrm{tr}" /> without the subscript indicates that this is the full or normal trace that one might expect (or equivalently performing the partial trace over all systems). We can now define the conditional entropy of a system as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7BH%28A+%5Cmid+B%29%7D_%5Crho+%26%3D+H%28%5Crho_%7BA%2CB%7D%29+-+H%28%5Crho_B%29%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} {H(A \mid B)}_\rho &amp;= H(\rho_{A,B}) - H(\rho_B)\end{aligned} " class="latex" title="\begin{aligned} {H(A \mid B)}_\rho &amp;= H(\rho_{A,B}) - H(\rho_B)\end{aligned} " /> </p>



<p>This definition intuitively makes sense since we can think of conditional entropy as the amount of information it takes to describe our joint system <img src="https://s0.wp.com/latex.php?latex=%28A%2CB%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(A,B)" class="latex" title="(A,B)" /> , given that we already know what <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> is.</p>



<p>We can now discuss quantum mutual information, the amount of information that measuring system <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> will provide you about system <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . Like the classical case, this is defined as follows:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7BI%28A%3BB%29%7D_%5Crho+%26%3D+%7BH%28A%2CB%29%7D_%5Crho+-+%7BH%28A%5Cmid+B%29%7D_%5Crho+-+%7BH%28B%5Cmid+A%29%7D_%5Crho%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} {I(A;B)}_\rho &amp;= {H(A,B)}_\rho - {H(A\mid B)}_\rho - {H(B\mid A)}_\rho\end{aligned} " class="latex" title="\begin{aligned} {I(A;B)}_\rho &amp;= {H(A,B)}_\rho - {H(A\mid B)}_\rho - {H(B\mid A)}_\rho\end{aligned} " /> </p>



<p>We can now finally discuss <strong>quantum mutual information (QCMI)</strong>, defined as follows: <img src="https://s0.wp.com/latex.php?latex=%7BI%28A%3BB+%5Cmid+C%29%7D_%5Crho+%3D+%7BI%28A%3BB%2CC%29%7D_%5Crho+-+%7BI%28A%3BC%29%7D_%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{I(A;B \mid C)}_\rho = {I(A;B,C)}_\rho - {I(A;C)}_\rho" class="latex" title="{I(A;B \mid C)}_\rho = {I(A;B,C)}_\rho - {I(A;C)}_\rho" /> . With some algebraic simplifications, one can arrive at the expression:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%7BI%28A%3BB+%5Cmid+C%29%7D_%5Crho+%26%3D+%7BH%28A%2CC%29%7D_%5Crho+%2B+%7BH%28B%2CC%29%7D_%5Crho+-+%7BH%28A%2CB%2CC%29%7D_%5Crho+-+%7BH%28C%29%7D_%5Crho.%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} {I(A;B \mid C)}_\rho &amp;= {H(A,C)}_\rho + {H(B,C)}_\rho - {H(A,B,C)}_\rho - {H(C)}_\rho.\end{aligned} " class="latex" title="\begin{aligned} {I(A;B \mid C)}_\rho &amp;= {H(A,C)}_\rho + {H(B,C)}_\rho - {H(A,B,C)}_\rho - {H(C)}_\rho.\end{aligned} " /> </p>



<p>The QCMI equals <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0" class="latex" title="0" /> if and only if <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> is a <strong>quantum Markov state</strong>. Classically, the entropic characterization of conditional independence corresponds to an algebraic characterization.</p>



<h2 id="recovery-maps">Recovery Maps</h2>



<p>Here, the algebraic characterization is more grueling. We have</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BABC%7D+%3D+%5Cexp%28%5Clog+%5Crho_%7BAB%7D+%2B+%5Clog+%5Crho_%7BBC%7D+-+%5Clog+%5Crho_B%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{ABC} = \exp(\log \rho_{AB} + \log \rho_{BC} - \log \rho_B)" class="latex" title="\rho_{ABC} = \exp(\log \rho_{AB} + \log \rho_{BC} - \log \rho_B)" /> </p>



<p>Equivalently,</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BABC%7D+%3D+%5Crho_%7BAB%7D%5E%7B1%2F2%7D+%5Crho_B%5E%7B-1%2F2%7D+%5Crho_%7BBC%7D%5Crho_B%5E%7B-1%2F2%7D+%5Crho_%7BAB%7D%5E%7B1%2F2%7D+%3D+R_%7BB%5Cto+AB%7D%28%5Crho_%7BBC%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{ABC} = \rho_{AB}^{1/2} \rho_B^{-1/2} \rho_{BC}\rho_B^{-1/2} \rho_{AB}^{1/2} = R_{B\to AB}(\rho_{BC})" class="latex" title="\rho_{ABC} = \rho_{AB}^{1/2} \rho_B^{-1/2} \rho_{BC}\rho_B^{-1/2} \rho_{AB}^{1/2} = R_{B\to AB}(\rho_{BC})" /> </p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=R_%7BB%5Cto+AB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_{B\to AB}" class="latex" title="R_{B\to AB}" /> is called the <strong>Petz recovery map</strong>,<sup><a href="https://windowsontheory.org/feed/#fn_4">4</a></sup> <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BB%5Cto+AB%7D%28X%29+%3D+%5Crho_%7BAB%7D%5E%7B1%2F2%7D+%5Crho_B%5E%7B-1%2F2%7D+X%5Crho_B%5E%7B-1%2F2%7D+%5Crho_%7BAB%7D%5E%7B1%2F2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{B\to AB}(X) = \rho_{AB}^{1/2} \rho_B^{-1/2} X\rho_B^{-1/2} \rho_{AB}^{1/2}" class="latex" title="\rho_{B\to AB}(X) = \rho_{AB}^{1/2} \rho_B^{-1/2} X\rho_B^{-1/2} \rho_{AB}^{1/2}" /> . One can think of a recovery may as a way that we can reconstruct the entire system <img src="https://s0.wp.com/latex.php?latex=A%2C+B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A, B" class="latex" title="A, B" /> using just system <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . It is not obvious that this is a quantum channel, but it is.</p>



<p>Suppose <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" /> is a probability distribution, so <img src="https://s0.wp.com/latex.php?latex=%5Crho+%3D%5C%3B%5Cmathrm%7Bdiag%7D%28p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho =\;\mathrm{diag}(p)" class="latex" title="\rho =\;\mathrm{diag}(p)" /> for some vector <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> . Then, all of the density matrices are diagonal and commuting. Then, the recovery map means that we divide by <img src="https://s0.wp.com/latex.php?latex=p_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_B" class="latex" title="p_B" /> and multiply by <img src="https://s0.wp.com/latex.php?latex=p_%7BAB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{AB}" class="latex" title="p_{AB}" /> , i.e., multiply by <img src="https://s0.wp.com/latex.php?latex=p_%7BA+%5Cmid+B%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{A \mid B}" class="latex" title="p_{A \mid B}" /> . This is the natural thing to do if we lost our information about <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and were trying to figure out what <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> was based on our knowledge of <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> . This is why <img src="https://s0.wp.com/latex.php?latex=R_%7BB%5Cto+A%2CB%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_{B\to A,B}" class="latex" title="R_{B\to A,B}" /> is known as a <em>recovery</em> map, and it is used to discuss conditional distributions in the quantum setting. In the classical case, if we start with <img src="https://s0.wp.com/latex.php?latex=B%2C+C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B, C" class="latex" title="B, C" /> , look only at <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , and use this to reconstruct <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> , then we would have the whole state in a Markov chain. That is why this is a plausible quantum version of being a Markov chain.</p>



<p>However, quantum Gibbs states are not, in general, quantum Markov chains. The failure of this statement to hold is related to <em>topological order</em>, which is similar to the degrees of freedom that show up in error correcting codes.</p>



<h2 id="quantum-markov-networks">Quantum Markov Networks</h2>



<p>Here, we will formally define a quantum Markov network. The reference for this is <a href="https://windowsontheory.org/feed/#leifer">[7]</a>.</p>



<p>Let <img src="https://s0.wp.com/latex.php?latex=G+%3D+%28V%2C+E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G = (V, E)" class="latex" title="G = (V, E)" /> be a finite graph. We associate with each vertex <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v \in V" class="latex" title="v \in V" /> a Hilbert space <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BH%7D%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\mathcal{H}}_v" class="latex" title="{\mathcal{H}}_v" /> and we consider a density matrix <img src="https://s0.wp.com/latex.php?latex=%5Crho_V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_V" class="latex" title="\rho_V" /> acting on <img src="https://s0.wp.com/latex.php?latex=%5Cbigotimes_%7Bv%5Cin+V%7D+%7B%5Cmathcal%7BH%7D%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bigotimes_{v\in V} {\mathcal{H}}_v" class="latex" title="\bigotimes_{v\in V} {\mathcal{H}}_v" /> . Then, <img src="https://s0.wp.com/latex.php?latex=%28G%2C+%5Crho_V%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(G, \rho_V)" class="latex" title="(G, \rho_V)" /> is a <strong>quantum Markov network</strong> if for all <img src="https://s0.wp.com/latex.php?latex=U%5Csubseteq+V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U\subseteq V" class="latex" title="U\subseteq V" /> , <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" /> is conditionally independent of <img src="https://s0.wp.com/latex.php?latex=V+%5Csetminus+%28U+%5Ccup+%5Cpartial+U%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V \setminus (U \cup \partial U)" class="latex" title="V \setminus (U \cup \partial U)" /> given <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial U" class="latex" title="\partial U" /> , where the conditional independence statement is w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Crho_V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_V" class="latex" title="\rho_V" /> and means that the corresponding QCMI satisfies <img src="https://s0.wp.com/latex.php?latex=%7BI%28U%3B+V%5Csetminus+%28U+%5Ccup+%5Cpartial+U%29+%5Cmid+%5Cpartial+U%29%7D_%7B%5Crho_V%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{I(U; V\setminus (U \cup \partial U) \mid \partial U)}_{\rho_V} = 0" class="latex" title="{I(U; V\setminus (U \cup \partial U) \mid \partial U)}_{\rho_V} = 0" /> .</p>



<p>A quantum Markov network is called <strong>positive</strong> if <img src="https://s0.wp.com/latex.php?latex=%5Crho_V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_V" class="latex" title="\rho_V" /> has full rank. (Recall that in the statement of the Hammersley-Clifford Theorem, , it is assumed that the distribution is strictly positive.)</p>



<p>Now, consider the following example. First, we introduce the Pauli matrices</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Csigma%5Ex+%3A%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%2C+%5Cqquad+%5Csigma%5Ez+%3A%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bbmatrix%7D%2C+%5Cqquad+%5Csigma%5Ey+%3A%3D+%5Cbegin%7Bbmatrix%7D+0+%26+-i+%5C%5C+i+%26+0+%5Cend%7Bbmatrix%7D.%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} \sigma^x := \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}, \qquad \sigma^z := \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}, \qquad \sigma^y := \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}.\end{aligned} " class="latex" title="\begin{aligned} \sigma^x := \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}, \qquad \sigma^z := \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}, \qquad \sigma^y := \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}.\end{aligned} " /> </p>



<p>We define a Hamiltonian on three qubits <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> , <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> , <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> by</p>



<p> <img src="https://s0.wp.com/latex.php?latex=H+%3A%3D+%28%5Csigma_A%5Ex+%5Csigma_B%5Ex+%2B+%5Csigma_A%5Ey+%5Csigma_B%5Ey+%2B+%5Csigma_A%5Ez+%5Csigma_B%5Ez%29+I_C+%2B+I_A+%28%5Csigma_B%5Ex+%5Csigma_C%5Ex+%2B+%5Csigma_B%5Ey+%5Csigma_C%5Ey+%2B+%5Csigma_B%5Ez+%5Csigma_C%5Ez%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H := (\sigma_A^x \sigma_B^x + \sigma_A^y \sigma_B^y + \sigma_A^z \sigma_B^z) I_C + I_A (\sigma_B^x \sigma_C^x + \sigma_B^y \sigma_C^y + \sigma_B^z \sigma_C^z)" class="latex" title="H := (\sigma_A^x \sigma_B^x + \sigma_A^y \sigma_B^y + \sigma_A^z \sigma_B^z) I_C + I_A (\sigma_B^x \sigma_C^x + \sigma_B^y \sigma_C^y + \sigma_B^z \sigma_C^z)" /> </p>



<p>(Juxtaposition in the above expression signifies the tensor product as discussed before.) Finally, for <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta &gt; 0" class="latex" title="\beta &gt; 0" /> , we define the Gibbs state</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BA%2CB%2CC%7D%28%5Cbeta%29+%3A%3D+%5Cfrac%7B1%7D%7BZ%28%5Cbeta%29%7D+%5Cexp%28-%5Cbeta+H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{A,B,C}(\beta) := \frac{1}{Z(\beta)} \exp(-\beta H)" class="latex" title="\rho_{A,B,C}(\beta) := \frac{1}{Z(\beta)} \exp(-\beta H)" /> </p>



<p>The Hamiltonian here has local terms which correspond to interactions <img src="https://s0.wp.com/latex.php?latex=%28A%2CB%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(A,B)" class="latex" title="(A,B)" /> , <img src="https://s0.wp.com/latex.php?latex=%28B%2C+C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(B, C)" class="latex" title="(B, C)" /> . However, it can be shown that the QCMI between <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> conditioned on <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B" class="latex" title="B" /> w.r.t. <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{A,B,C}" class="latex" title="\rho_{A,B,C}" /> is non-zero, which means that this is not a quantum Markov network w.r.t. the line graph <img src="https://s0.wp.com/latex.php?latex=A+%5Cleftrightarrow+B+%5Cleftrightarrow+C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A \leftrightarrow B \leftrightarrow C" class="latex" title="A \leftrightarrow B \leftrightarrow C" /> . This demonstrates the failure of the Hammersley-Clifford Theorem in the quantum setting.</p>



<h2 id="important-results">Important Results</h2>



<p>We will briefly discuss the results of two papers.</p>



<ol><li><a href="https://windowsontheory.org/feed/#brandao1">[8]</a> This paper shows that mixing in space implies mixing in time in the quantum case. However, the result of the paper only applies to commuting Hamiltonians. For commuting Hamiltonians, it turns out that quantum Gibbs states are quantum Markov networks. They use a version of Glauber dynamics, which can be simulated on a quantum computer but are also plausible dynamics for a physical system in nature. This is a difficult paper to read, but it is worth digesting if you want to work in the field.</li><li><a href="https://windowsontheory.org/feed/#brandao2">[9]</a> This second paper is much easier and more general, covering non-commuting Hamiltonians, but it requires more conditions. They give a method of preparing the Gibbs state which can run on a quantum computer, but the dynamics are not plausible as a physical system because they are too complicated. The more complicated dynamics allows them to make the proof work. The paper also uses QCMI.They have two assumptions. The first assumption looks like mixing in space (weak correlation decay). The second assumption is that the state looks approximately like a quantum Markov network (this is definitely not met in general). A very important paper in this space is a recent breakthrough (<a href="https://windowsontheory.org/feed/#fawzi">[10]</a>) which characterizes quantum Markov chains. They show that if the QCMI is bounded by <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> , then the recovery map <img src="https://s0.wp.com/latex.php?latex=R_%7BB%5Cto+A%2CB%7D%28%5Crho_%7BBC%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_{B\to A,B}(\rho_{BC})" class="latex" title="R_{B\to A,B}(\rho_{BC})" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon'" class="latex" title="\epsilon'" /> -close to <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7BABC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{ABC}" class="latex" title="\rho_{ABC}" /> , i.e., low QCMI implies that the recovery map works well. This is trivial to prove classically, but very difficult in the quantum world.The algorithm in <a href="https://windowsontheory.org/feed/#brandao2">[9]</a> is very elegant. Essentially, we take the entire system and punch out constant-sized boxes. If we can reconstruct the region outside of the boxes, then we can use the recovery maps to reconstruct the regions inside of the boxes, and the boxes are far apart enough so they are almost independent. For this argument, we must assume that the QCMI decays exponentially. Whenever we have exponential decay, we get a correlation decay that sets the size of the boxes. It is very difficult to condition on quantum states, but recovery maps provide a sense in which it is meaningful to do so. The paper gives an efficient method of preparing Gibbs states and simulating quantum systems on quantum computers.</li></ol>



<h1 id="additional-reading">Additional reading</h1>



<p>The standard treatment of information theory is <a href="https://windowsontheory.org/feed/#info">[11]</a>. This book contains definitions and properties of entropy, conditional entropy, mutual information, and conditional mutual information.</p>



<p>To see a treatment of the subject of Markov chains from the perspective of probability theory, see <a href="https://windowsontheory.org/feed/#durrett1">[12]</a> or the mathematically more sophisticated counterpart <a href="https://windowsontheory.org/feed/#durrett2">[13]</a>. An introduction to coupling can be found in <a href="https://windowsontheory.org/feed/#mitzenmacher">[14]</a>, as well as <a href="https://windowsontheory.org/feed/#nature">[4]</a> (the latter also contains an exposition to spatial mixing). The connection between Markov chain mixing and the so-called <em>logarithmic Sobolev inequality</em> is described in <a href="https://windowsontheory.org/feed/#cesi">[15]</a>.</p>



<h1 id="scn:appendix">Appendix: Intuition for Markov chains</h1>



<h2 id="random-walk-on-the-cycle">Random walk on the cycle</h2>



<p>We have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> points on the cycle, <img src="https://s0.wp.com/latex.php?latex=0%2C1%2C%5Cdotsc%2Cn-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0,1,\dotsc,n-1" class="latex" title="0,1,\dotsc,n-1" /> . At each step, we move left or right with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/2" class="latex" title="1/2" /> . We can write the transition matrix as</p>



<p> <img src="https://s0.wp.com/latex.php?latex=T+%3D+%5Cfrac%7BS+%2B+S%5E%7B-1%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T = \frac{S + S^{-1}}{2}" class="latex" title="T = \frac{S + S^{-1}}{2}" /> </p>



<p>where <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> is the shift operator <img src="https://s0.wp.com/latex.php?latex=S+%7C%7Bx%7D%5Crangle+%3D+%7C%7Bx%2B1+%5Cbmod+n%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S |{x}\rangle = |{x+1 \bmod n}\rangle " class="latex" title="S |{x}\rangle = |{x+1 \bmod n}\rangle " /> . The matrix <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S" class="latex" title="S" /> is diagonalized by the Fourier transform. Define, for <img src="https://s0.wp.com/latex.php?latex=k%3D0%2C1%2C%5Cdotsc%2Cn-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k=0,1,\dotsc,n-1" class="latex" title="k=0,1,\dotsc,n-1" /> ,</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%7C%7B%5Ctilde+k%7D%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+%5Csum_%7Bx%3D0%7D%5E%7Bn-1%7D+%5Cexp%5CBigl%28+%5Cfrac%7B2%5Cpi+i+k+x%7D%7Bn%7D+%5CBigr%29+%7C%7Bx%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|{\tilde k}\rangle = \frac{1}{\sqrt n} \sum_{x=0}^{n-1} \exp\Bigl( \frac{2\pi i k x}{n} \Bigr) |{x}\rangle " class="latex" title="|{\tilde k}\rangle = \frac{1}{\sqrt n} \sum_{x=0}^{n-1} \exp\Bigl( \frac{2\pi i k x}{n} \Bigr) |{x}\rangle " /> </p>



<p>We have the same amount of amplitude at every point, but there is a varying phase which depends on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> . If <img src="https://s0.wp.com/latex.php?latex=k+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 0" class="latex" title="k = 0" /> , we get the all-ones vector. If <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> is small, then the phase is slowly varying. If <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> is large, then the phase is rapidly varying. Look at what happens after we apply the shift operator:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+S+%7C%7B%5Ctilde+k%7D%5Crangle+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+%5Csum_%7Bx%3D0%7D%5E%7Bn-1%7D+%5Cexp%5CBigl%28+%5Cfrac%7B2%5Cpi+i+k+x%7D%7Bn%7D+%5CBigr%29+%7C%7Bx%2B1+%5Cbmod+n%7D%5Crangle+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+%5Csum_%7Bx%3D1%7D%5En+%5Cexp%5CBigl%28+%5Cfrac%7B2%5Cpi+i+k+%28x-1%29%7D%7Bn%7D+%5CBigr%29+%7C%7Bx+%5Cbmod+n%7D%5Crangle+%3D+%5Cexp%5CBigl%28-+%5Cfrac%7B2%5Cpi+i+k%7D%7Bn%7D+%5CBigr%29+%7C%7B%5Ctilde+k%7D%5Crangle+.%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} S |{\tilde k}\rangle &amp;= \frac{1}{\sqrt n} \sum_{x=0}^{n-1} \exp\Bigl( \frac{2\pi i k x}{n} \Bigr) |{x+1 \bmod n}\rangle \\ &amp;= \frac{1}{\sqrt n} \sum_{x=1}^n \exp\Bigl( \frac{2\pi i k (x-1)}{n} \Bigr) |{x \bmod n}\rangle = \exp\Bigl(- \frac{2\pi i k}{n} \Bigr) |{\tilde k}\rangle .\end{aligned} " class="latex" title="\begin{aligned} S |{\tilde k}\rangle &amp;= \frac{1}{\sqrt n} \sum_{x=0}^{n-1} \exp\Bigl( \frac{2\pi i k x}{n} \Bigr) |{x+1 \bmod n}\rangle \\ &amp;= \frac{1}{\sqrt n} \sum_{x=1}^n \exp\Bigl( \frac{2\pi i k (x-1)}{n} \Bigr) |{x \bmod n}\rangle = \exp\Bigl(- \frac{2\pi i k}{n} \Bigr) |{\tilde k}\rangle .\end{aligned} " /> </p>



<p>After the shift, we pick up an additional phase based on how rapidly the phase is varying. From this, we get:</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+T+%7C%7B%5Ctilde%7Bk%7D%7D%5Crangle+%26%3D+%5Cfrac%7B%5Cexp%282%5Cpi+i+k+%2F+n%29+%2B+%5Cexp%28-2%5Cpi+i+k+%2F+n%29%7D%7B2%7D+%7C%7B%5Ctilde%7Bk%7D%7D%5Crangle+%3D+%5Ccos%5CBigl%28%5Cfrac%7B2%5Cpi+k%7D%7Bn%7D%5CBigr%29+%7C%7B%5Ctilde%7Bk%7D%7D%5Crangle+.%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} T |{\tilde{k}}\rangle &amp;= \frac{\exp(2\pi i k / n) + \exp(-2\pi i k / n)}{2} |{\tilde{k}}\rangle = \cos\Bigl(\frac{2\pi k}{n}\Bigr) |{\tilde{k}}\rangle .\end{aligned} " class="latex" title="\begin{aligned} T |{\tilde{k}}\rangle &amp;= \frac{\exp(2\pi i k / n) + \exp(-2\pi i k / n)}{2} |{\tilde{k}}\rangle = \cos\Bigl(\frac{2\pi k}{n}\Bigr) |{\tilde{k}}\rangle .\end{aligned} " /> </p>



<p>The eigenvalues are</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Clambda_k+%3D+%5Ccos+%5Cfrac%7B2%5Cpi+k%7D%7Bn%7D%2C+%5Cqquad+k%3D0%2C1%2C%5Cdotsc%2Cn-1.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_k = \cos \frac{2\pi k}{n}, \qquad k=0,1,\dotsc,n-1." class="latex" title="\lambda_k = \cos \frac{2\pi k}{n}, \qquad k=0,1,\dotsc,n-1." /> </p>



<p>Only <img src="https://s0.wp.com/latex.php?latex=k+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k = 0" class="latex" title="k = 0" /> will give me an eigenvalue of <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> .</p>



<p>How do we analyze <img src="https://s0.wp.com/latex.php?latex=T%5Et+%7C%7Bp%7D%5Crangle+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T^t |{p}\rangle " class="latex" title="T^t |{p}\rangle " /> ? We should Fourier transform the distribution.</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+T%5Et+%7C%7Bp%7D%5Crangle+%3D+T%5Et+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D+p_k+%7C%7B%5Ctilde%7Bk%7D%7D%5Crangle+%3D+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D+p_k+%5Clambda_k%5Et+%7C%7B%5Ctilde+k%7D%5Crangle+.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} T^t |{p}\rangle = T^t \sum_{k=0}^{n-1} p_k |{\tilde{k}}\rangle = \sum_{k=0}^{n-1} p_k \lambda_k^t |{\tilde k}\rangle .\end{aligned}" class="latex" title="\begin{aligned} T^t |{p}\rangle = T^t \sum_{k=0}^{n-1} p_k |{\tilde{k}}\rangle = \sum_{k=0}^{n-1} p_k \lambda_k^t |{\tilde k}\rangle .\end{aligned}" /> </p>



<p>If <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> is odd, then as <img src="https://s0.wp.com/latex.php?latex=t%5Crightarrow%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\rightarrow\infty" class="latex" title="t\rightarrow\infty" /> , <img src="https://s0.wp.com/latex.php?latex=%5Clambda_k%5Et+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_k^t \to 0" class="latex" title="\lambda_k^t \to 0" /> for all <img src="https://s0.wp.com/latex.php?latex=k%3D1%2C%5Cdotsc%2Cn-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k=1,\dotsc,n-1" class="latex" title="k=1,\dotsc,n-1" /> , so <img src="https://s0.wp.com/latex.php?latex=T%5Et+%5Cto+%7C%7B%5Cpi%7D%5Crangle+%5Clangle%7B1_n%7D%7C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T^t \to |{\pi}\rangle \langle{1_n}| " class="latex" title="T^t \to |{\pi}\rangle \langle{1_n}| " /> . Whatever you put into this operator, you get <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\pi" class="latex" title="\pi" /> out.</p>



<h2 id="spectral-gap">Spectral gap</h2>



<p>The example of the random walk on the cycle shows that there is generally a unique stationary distribution and suggests that the speed of convergence is determined by how close the other eigenvalues are to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> . Specifically, suppose for simplicity that the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" /> are <img src="https://s0.wp.com/latex.php?latex=1+%3D+%5Clambda_0+%5Cge+%5Clambda_1%5Cge%5Ccdots+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 = \lambda_0 \ge \lambda_1\ge\cdots \ge 0" class="latex" title="1 = \lambda_0 \ge \lambda_1\ge\cdots \ge 0" /> (real and positive). Then, the convergence time is on the order of <img src="https://s0.wp.com/latex.php?latex=%5Csim+1%2F%281-%5Clambda_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sim 1/(1-\lambda_1)" class="latex" title="\sim 1/(1-\lambda_1)" /> .</p>



<p>Typically, the distance of the eigenvalues from <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> reflects the size of the physical system. Even from the simple example, we can get some physical intuition from this. If <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> is small, then the spectral gap is <img src="https://s0.wp.com/latex.php?latex=%5Ccos%282%5Cpi+k%2Fn%29+%3D+1-O%28k%5E2%2Fn%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\cos(2\pi k/n) = 1-O(k^2/n^2)" class="latex" title="\cos(2\pi k/n) = 1-O(k^2/n^2)" /> . Thus, the convergence time is <img src="https://s0.wp.com/latex.php?latex=%5Csim+1%2F%281-%5Clambda_1%29+%5Csim+n%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sim 1/(1-\lambda_1) \sim n^2" class="latex" title="\sim 1/(1-\lambda_1) \sim n^2" /> , which is indeed the convergence time for a random walk on a cycle.</p>



<h2 id="references">References</h2>



<hr class="wp-block-separator" />



<ol><li>S. Gharibian, Y. Huang, Z. Landau, and S. W. Shin, “Quantum Hamiltonian complexity,” <em>Found. Trends Theor. Comput. Sci.</em>, vol. 10, no. 3, pp. front matter, 159–282, 2014. </li><li>R. W. Keener, <em>Theoretical statistics</em>. Springer, New York, 2010, p. xviii+538. </li><li>E. Crosson, D. Bacon, and K. R. Brown, “Making Classical Ground State Spin Computing Fault-Tolerant,” <em>Physical Review E</em>, vol. 82, no. 3, Sep. 2010. </li><li>C. Moore and S. Mertens, <em>The nature of computation</em>. Oxford University Press, Oxford, 2011, p. xviii+985. </li><li>F. Martinelli, “Lectures on Glauber dynamics for discrete spin models,” in <em>Lectures on probability theory and statistics (Saint-Flour, 1997)</em>, vol. 1717, Springer, Berlin, 1999, pp. 93–191. </li><li>F. Martinelli and E. Olivieri, “Finite volume mixing conditions for lattice spin systems and exponential approach to equilibrium of Glauber dynamics,” in <em>Cellular automata and cooperative systems (Les Houches, 1992)</em>, vol. 396, Kluwer Acad. Publ., Dordrecht, 1993, pp. 473–490. </li><li>M. S. Leifer and D. Poulin, “Quantum graphical models and belief propagation,” <em>Ann. Physics</em>, vol. 323, no. 8, pp. 1899–1946, 2008. </li><li>M. J. Kastoryano and F. G. S. L. Brandão, “Quantum Gibbs samplers: the commuting case,” <em>Comm. Math. Phys.</em>, vol. 344, no. 3, pp. 915–957, 2016. </li><li>F. G. S. L. Brandão and M. J. Kastoryano, “Finite correlation length implies efficient preparation of quantum thermal states,” <em>ArXiv e-prints</em>, Sep. 2016. </li><li>O. Fawzi and R. Renner, “Quantum conditional mutual information and approximate Markov chains,” <em>Comm. Math. Phys.</em>, vol. 340, no. 2, pp. 575–611, 2015. </li><li>T. M. Cover and J. A. Thomas, <em>Elements of information theory</em>, Second. Wiley-Interscience [John Wiley &amp; Sons], Hoboken, NJ, 2006, p. xxiv+748. </li><li>R. Durrett, <em>Essentials of stochastic processes</em>. Springer, Cham, 2016, p. ix+275. </li><li>R. Durrett, <em>Probability: theory and examples</em>, Fourth., vol. 31. Cambridge University Press, Cambridge, 2010, p. x+428. </li><li>M. Mitzenmacher and E. Upfal, <em>Probability and computing</em>, Second. Cambridge University Press, Cambridge, 2017, p. xx+467. </li><li>F. Cesi, “Quasi-factorization of the entropy and logarithmic Sobolev inequalities for Gibbs random fields,” <em>Probab. Theory Related Fields</em>, vol. 120, no. 4, pp. 569–584, 2001. </li></ol>



<hr class="wp-block-separator" />



<ol><li>This is the opposite of the probabilists’ convention, i.e., the transition probability matrix that we define here is the <em>transpose</em> of the one usually found in most probability theory textbooks. <a href="https://windowsontheory.org/feed/#fnref_1"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li><li>As a side note, it may be a good research question to investigate to what extent quantum algorithms can be used to compute summations whose terms are possibly negative. In quantum Monte Carlo, the quantum Hamiltonian is converted to a classical energy function; this conversion always works, but sometimes you end up with complex energies, which is terrible for estimating the partition function because terms can cancel each other out. <a href="https://windowsontheory.org/feed/#fnref_2"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li><li>You may recognize this as the total variation norm. <a href="https://windowsontheory.org/feed/#fnref_3"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li><li>Petz wrote about quantum relative entropy in 1991, way before it was cool. <a href="https://windowsontheory.org/feed/#fnref_4"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li></ol></div>







<p class="date">
by wsmoses <a href="https://windowsontheory.org/2018/12/20/efficient-preparation-of-thermal-states-of-quantum-systems-natural-or-artificial/"><span class="datestr">at December 20, 2018 09:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/20/theory-blog-aggregator-up/">Theory Blog Aggregator Up!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <strong>Theory of Computing Blog Aggregator</strong> is now back online at a new website: <a href="http://cstheory-feed.org/" rel="nofollow">http://cstheory-feed.org/</a> . There is also a twitter feed at <a href="https://twitter.com/cstheory" rel="nofollow">https://twitter.com/cstheory</a> .</p>
<p>See <a href="http://blog.geomblog.org/2018/12/the-theorycs-blog-aggregator-reborn.html">this blog post</a> by Suresh Venkatasubramanian (who, together with Arnab Bhattacharyya, is responsible for the aggregator’s revival – thank you!!) for more details. This is a good opportunity to thank Arvind Narayanan who created the software to run it and maintained it all these years.</p>
<p>If you don’t want to rely on the aggregator to follow windows on theory, you can use the <strong>“Follow Blog by email”</strong> button on our side bar, and join the 590 other happy customers who don’t need to wait to the feed to get the <a href="https://windowsontheory.org/category/physics/">latest lecture notes</a> from our physics and computation seminar.</p></div>







<p class="date">
by windowsontheory <a href="https://windowsontheory.org/2018/12/20/theory-blog-aggregator-up/"><span class="datestr">at December 20, 2018 09:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/20/quantum-hamiltonian-complexity/">What is Quantum Hamiltonian Complexity?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>by Ben Edelman</strong></p>
<p><em>This is the first installment of a three-part series of posts on quantum Hamiltonian complexity based on lectures given the authors in <a href="https://www.boazbarak.org/fall18seminar/">Boaz and Tselil’s seminar</a>. The second installment is <a href="https://windowsontheory.org/2018/12/20/tensor-networks-matrix-product-states-dmrg/">here</a>, and the third installment is <a href="https://windowsontheory.org/2018/12/18/a-1d-area-law-for-gapped-local-hamiltonians/">here</a>.</em></p>
<p>Quantum Hamiltonian complexity is a growing area of study that has important ramifications for both physics and computation. Our hope is that these three posts will provide an accessible (and incomplete) preview of the subject for readers who know the basics of theoretical computer science and quantum information. Much of the material is adapted from an <a href="https://arxiv.org/abs/1401.3916">excellent survey by Gharibian et al.</a>.</p>
<p>In a nutshell, quantum Hamiltonian complexity is the study of the <em>local Hamiltonian problem</em>. Why is this problem important enough to justify the existence of an entire subfield? To illustrate why, here are two informal characterizations of it:</p>
<ol>
<li>To a <strong>physicist</strong>, the local Hamiltonian problem is a formalization of the difficulty of simulating and understanding many-particle quantum systems. There are deep connections between the complexity of this problem and the amount of quantum entanglement in a system. In practical terms, physicists would love to be able to solve this problem on a regular basis, and they’ve developed a rich theory of heuristics to that end.</li>
<li>To a <strong>computer scientist</strong>, local Hamiltonian problem is the quantum version of constraint satisfaction problems. Any CSP can be written as a local Hamiltonian problem; and just as constraint satisfaction is the prototypical NP-complete problem by the Cook-Levin theorem, the local Hamiltonian problem plays the equivalent role for QMA (a quantum analogue of NP) by the “quantum Cook-Levin theorem.” The connections to classical complexity go on… there is even a <a href="https://arxiv.org/pdf/1309.7495.pdf">quantum PCP conjecture</a>!</li>
</ol>
<p>But let’s take a step back and start at the beginning. To make sure we understand what a quantum Hamiltonian is and why it is important, it will be instructive to briefly rehash some of the <a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/">fundamentals of classical statistical mechanics</a>.</p>
<h2>Classical energy and ground states</h2>
<p>In the classical world, a physical system can be in any one of various states <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \mathcal{X}" class="latex" title="x \in \mathcal{X}" />, each of which is a vector, with different coordinates representing different particles. Every state of the system has an <em>energy</em>, given by an energy function <img src="https://s0.wp.com/latex.php?latex=E%3A+%5Cmathcal%7BX%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E: \mathcal{X} \to \mathbb{R}" class="latex" title="E: \mathcal{X} \to \mathbb{R}" />. For example, in the classic Ising model of ferromagnetism, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D+%3D+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{X} = \{\pm 1\}^n" class="latex" title="\mathcal{X} = \{\pm 1\}^n" />. Each coordinate <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" /> represents the spin of atom <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />, and atoms <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="j" class="latex" title="j" /> interact with each other whenever <img src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i,j)" class="latex" title="(i,j)" /> is an edge in a graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />, which is usually a low-dimensional lattice. Energy for this system is defined as <img src="https://s0.wp.com/latex.php?latex=E%28x%29+%3D+%5Csum_%7B%28i%2Cj%29+%5Cin+G%7D-x_i+x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(x) = \sum_{(i,j) \in G}-x_i x_j" class="latex" title="E(x) = \sum_{(i,j) \in G}-x_i x_j" />.</p>
<p>Suppose we ignore our system for a long time, letting it interact with its external environment until, in the limit, it reaches thermal equilibrium at temperature <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T" class="latex" title="T" />. Then the probability the system is in state <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> is given by Boltzmann’s distribution: <img src="https://s0.wp.com/latex.php?latex=%5CPr%5Bx%5D+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+E%28x%29%7D%7D%7BZ%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Pr[x] = \frac{e^{-\beta E(x)}}{Z}" class="latex" title="\Pr[x] = \frac{e^{-\beta E(x)}}{Z}" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cpropto+1%2FT&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\beta \propto 1/T" class="latex" title="\beta \propto 1/T" /> and <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> is the partition function required to normalize the probabilities. As the temperature tends to infinity, this distribution will approach the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{X}" class="latex" title="\mathcal{X}" />, and as the temperature tends to absolute zero, the distribution will approach the uniform distribution over the states with minimum energy. We call these minimum energy states <em>ground states</em>, and we call their energy the <em>ground state energy</em>. If we want to calculate something about a system, then it is often crucial to know the ground states and ground state energy of the system. Going back to our example, the Ising model has two ground states whenever <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is connected. These are the states <img src="https://s0.wp.com/latex.php?latex=%28%2B1%2C%2B1%2C%5Cldots%2C%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(+1,+1,\ldots,+1)" class="latex" title="(+1,+1,\ldots,+1)" /> and <img src="https://s0.wp.com/latex.php?latex=%28-1%2C-1%2C%5Cldots%2C-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(-1,-1,\ldots,-1)" class="latex" title="(-1,-1,\ldots,-1)" /> in which all atoms have the same spin. The ground state energy is <img src="https://s0.wp.com/latex.php?latex=-%7C%5C%7Bi%2Cj%3A%28i%2Cj%29+%5Cin+G%5C%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-|\{i,j:(i,j) \in G\}|" class="latex" title="-|\{i,j:(i,j) \in G\}|" />.</p>
<h2>Quantum Hamiltonians</h2>
<p>A quantum Hamiltonian is essentially the quantum analogue of the classical energy function. Unlike with classical systems, when a quantum system is in a given <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />-qubit state <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cpsi%5Cright%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left|\psi\right\rangle" class="latex" title="\left|\psi\right\rangle" />, it doesn’t have a determinate energy. Instead, when we measure the energy, the value we obtain may be probabilistic and will correspond to one of the eigenvalues of the observable matrix for energy. This Hermitian matrix, denoted <img src="https://s0.wp.com/latex.php?latex=H+%5Cin+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D+%5Ctimes+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" class="latex" title="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" />, is the quantum Hamiltonian, and just as the energy function characterizes a classical system, the Hamiltonian characterizes a quantum system. For a given eigenvector <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda_i%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\lambda_i\rangle" class="latex" title="|\lambda_i\rangle" /> of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> with eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_i" class="latex" title="\lambda_i" />, when we measure the energy of <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" /> we obtain the result <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_i" class="latex" title="\lambda_i" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cpsi%7C%5Clambda_i%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle\psi|\lambda_i\rangle" class="latex" title="\langle\psi|\lambda_i\rangle" />, and the system collapses to the state <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda_i%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\lambda_i\rangle" class="latex" title="|\lambda_i\rangle" /> (assuming the eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_i" class="latex" title="\lambda_i" /> has multiplicity 1). Thus, the ground state and ground state energy of a quantum system with eigenvalue <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> are the minimum eigenvalue <img src="https://s0.wp.com/latex.php?latex=%5Clambda_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0" class="latex" title="\lambda_0" /> of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> and the corresponding eigenvector <img src="https://s0.wp.com/latex.php?latex=%7C%5Clambda_0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\lambda_0\rangle" class="latex" title="|\lambda_0\rangle" />.</p>
<p>The Boltzmann distribution also has a quantum analogue. A quantum system at thermal equilibrium will be in the following mixed state: <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7B%5Ctext%7Beq%7D%7D+%3D+%5Cfrac%7Be%5E%7B-%5Cbeta+H%7D%7D%7BZ%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{\text{eq}} = \frac{e^{-\beta H}}{Z}" class="latex" title="\rho_{\text{eq}} = \frac{e^{-\beta H}}{Z}" />. As the temperature approaches absolute zero, <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7B%5Ctext%7Beq%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_{\text{eq}}" class="latex" title="\rho_{\text{eq}}" /> will approach a superposition over the ground states.</p>
<p>Not only does the Hamiltonian tell us the energy of a system, it also describes the time evolution of the system (as long as it is closed). Schrödinger’s equation states that <img src="https://s0.wp.com/latex.php?latex=-i+%5Chbar+%5Cfrac%7Bd%7C%5Cpsi%5Crangle%7D%7Bdt%7D+%3D+H%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-i \hbar \frac{d|\psi\rangle}{dt} = H|\psi\rangle" class="latex" title="-i \hbar \frac{d|\psi\rangle}{dt} = H|\psi\rangle" />, where <img src="https://s0.wp.com/latex.php?latex=%5Chbar&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hbar" class="latex" title="\hbar" /> is Planck’s constant and <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> is time. Thus, if a closed system is in the state <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle_0" class="latex" title="|\psi\rangle_0" /> at time 0, its state at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> will be <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle_t+%3D+e%5E%7B-itH%2F%5Chbar%7D%7C%5Cpsi%5Crangle_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle_t = e^{-itH/\hbar}|\psi\rangle_0" class="latex" title="|\psi\rangle_t = e^{-itH/\hbar}|\psi\rangle_0" />. Since <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is Hermitian, <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-itH%2F%5Chbar%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e^{-itH/\hbar}" class="latex" title="e^{-itH/\hbar}" /> is unitary, which is another way of saying that quantum mechanical states are subject to unitary evolution.</p>
<h1>The Local Hamiltonian problem</h1>
<p>As we have seen, understanding the Hamiltonian of a quantum system is crucial for understanding both the system’s equilibrium behavior and its time evolution. There are a huge variety of questions physicists are interested in asking about systems, all of which boil down to questions about equilibrium behavior, time evolution, or both. There is a single problem that captures the complexity of many of these questions, in the sense that most of the questions can’t be answered without solving it. This is the problem of estimating the ground state energy of the Hamiltonian. Especially in condensed matter physics, this problem is ubiquitous.</p>
<p>Formally, we will study the following promise problem: (note: this will not be our final formulation)</p>
<hr />
<p><strong>The “Hamiltonian Problem”</strong></p>
<p>Given a Hermitian matrix <img src="https://s0.wp.com/latex.php?latex=H+%5Cin+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D+%5Ctimes+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" class="latex" title="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" /> and non-negative reals <img src="https://s0.wp.com/latex.php?latex=a%2C+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a, b" class="latex" title="a, b" /> with <img src="https://s0.wp.com/latex.php?latex=b+%5Cgeq+a%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b \geq a+1" class="latex" title="b \geq a+1" />,</p>
<ul>
<li>If <img src="https://s0.wp.com/latex.php?latex=%5Clambda_0%28H%29+%5Cleq+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0(H) \leq a" class="latex" title="\lambda_0(H) \leq a" />, output YES<p></p>
</li>
<li>
<p>If <img src="https://s0.wp.com/latex.php?latex=%5Clambda_0%28H%29+%5Cgeq+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0(H) \geq b" class="latex" title="\lambda_0(H) \geq b" />, output NO</p>
</li>
</ul>
<hr />
<p>One issue with this definition is that the input includes an enormous <img src="https://s0.wp.com/latex.php?latex=2%5En+%5Ctimes+2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n \times 2^n" class="latex" title="2^n \times 2^n" /> matrix. For a reasonable-sized system, there’d be no use in even trying to solve this problem through classical computation, and how to deal with it in the quantum computing setting is far from obvious. Luckily, physicists have found that in real-life systems, interactions tend to be <em>local</em>, and if we consider the special case of <em>local Hamiltonians</em>, the input for the problem is of reasonable size.</p>
<p></p><div style="width: 295px;" id="attachment_6361" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/circle_diagram0.png?w=285&amp;h=300" alt="circle_diagram0" height="300" class="aligncenter size-medium wp-image-6361" width="285" /><p class="wp-caption-text">Hamiltonians are too big to work with. What if we restrict our focus to local Hamiltonians?</p></div><p></p>
<p>A <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian is a Hamiltonian that is decomposed into a sum of terms, each of which represents a Hamiltonian acting on a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-unit subset of the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> qubits in the system. In other words, <img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Csum_i+%28H_i%29_%7BS_i%7D+%5Cotimes+I_%7B%5Bn%5D%5Cbackslash+S_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H = \sum_i (H_i)_{S_i} \otimes I_{[n]\backslash S_i}" class="latex" title="H = \sum_i (H_i)_{S_i} \otimes I_{[n]\backslash S_i}" />, where each <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_i" class="latex" title="S_i" /> is a subset of <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[n]" class="latex" title="[n]" /> of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />. For brevity’s sake, we abuse notation and write <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> as <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sum_i H_i" class="latex" title="\sum_i H_i" />. We can think of the <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" />’s as local constraints, and the ground state as the state that simultaneously satisfies the constraints to the maximal possible extent. Here, then, is the new-and-improved problem definition:</p>
<hr />
<p><strong><img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-Local Hamiltonian Problem</strong></p>
<p>Given a Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H+%5Cin+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D+%5Ctimes+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" class="latex" title="H \in (\mathbb{C}^2)^{\otimes n} \times (\mathbb{C}^2)^{\otimes n}" /> specified as a collection of <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> local interactions <img src="https://s0.wp.com/latex.php?latex=%5C%7BH_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H_i\}" class="latex" title="\{H_i\}" />, and non-negative reals <img src="https://s0.wp.com/latex.php?latex=a%2C+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a, b" class="latex" title="a, b" /> with <img src="https://s0.wp.com/latex.php?latex=b+%5Cgeq+a%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b \geq a+1" class="latex" title="b \geq a+1" />,</p>
<ul>
<li>If <img src="https://s0.wp.com/latex.php?latex=%5Clambda_0%28H%29+%5Cleq+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0(H) \leq a" class="latex" title="\lambda_0(H) \leq a" />, output YES</li>
<li>If <img src="https://s0.wp.com/latex.php?latex=%5Clambda_0%28H%29+%5Cgeq+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0(H) \geq b" class="latex" title="\lambda_0(H) \geq b" />, output NO</li>
</ul>
<hr />
<p>Presuming the matrices <img src="https://s0.wp.com/latex.php?latex=%5C%7BH_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H_i\}" class="latex" title="\{H_i\}" /> and the reals <img src="https://s0.wp.com/latex.php?latex=a%2C+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a, b" class="latex" title="a, b" /> are specified to polynomial precision, then the input size is polynomial in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, since <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> is a constant and each of the matrices <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" /> has <img src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Ccdot+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k \cdot 2^k" class="latex" title="2^k \cdot 2^k" /> entries. Thus, not only is our new problem physically realistic, it is also a problem we might hope to attack with classical computation. However, we will later see that in fact this problem is likely hard even for quantum computers. The remaining installments in this series of notes will deal with further restrictions of the class of Hamiltonians for which the local Hamiltonian problem may be tractable.</p>
<h2>Computer science motivation</h2>
<p>As we mentioned in the intro, the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian problem (henceforth denoted <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH) doesn’t just have myriad applications in physics—it is also important from a computer science perspective because it is a quantum generalization of constraint satisfiability (you may have noticed that quantum analogues of classical concepts are a running theme). Specifically, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-CSP is a special case of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH.</p>
<p>Suppose we have a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-CSP instance <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi" class="latex" title="\varphi" />, and we want to turn it into a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH instance. A clause <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> with constituent variables <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots%2C+x_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1, \ldots, x_k" class="latex" title="x_1, \ldots, x_k" /> becomes a <img src="https://s0.wp.com/latex.php?latex=2%5Ek+%5Ctimes+2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^k \times 2^k" class="latex" title="2^k \times 2^k" /> diagonal <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}" class="latex" title="\{0,1\}" /> matrix <img src="https://s0.wp.com/latex.php?latex=H_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C" class="latex" title="H_C" /> acting on the qubits <img src="https://s0.wp.com/latex.php?latex=%7Cx_1%5Crangle%2C%5Cldots%2C%7Cx_k%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|x_1\rangle,\ldots,|x_k\rangle" class="latex" title="|x_1\rangle,\ldots,|x_k\rangle" />. Note that the rows and columns of this matrix are indexed by the assignment vectors <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \{0,1\}^k" class="latex" title="x \in \{0,1\}^k" />. Formally, <img src="https://s0.wp.com/latex.php?latex=H_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C" class="latex" title="H_C" /> encodes the truth table of <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> in the following manner: <img src="https://s0.wp.com/latex.php?latex=%28H_C%29_%7Bx%2Cx%7D+%3D+1+-+C%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H_C)_{x,x} = 1 - C(x)" class="latex" title="(H_C)_{x,x} = 1 - C(x)" />. Another way of stating this is <img src="https://s0.wp.com/latex.php?latex=H_C+%3D+%5Csum_%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5Ek%5Ctext%7B+s.t.+%7DC%28x%29%3D0%7D%7Cx%5Crangle%5Clangle%7Bx%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C = \sum_{x \in \{0,1\}^k\text{ s.t. }C(x)=0}|x\rangle\langle{x}|" class="latex" title="H_C = \sum_{x \in \{0,1\}^k\text{ s.t. }C(x)=0}|x\rangle\langle{x}|" />.</p>
<p>Informally, <img src="https://s0.wp.com/latex.php?latex=H_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C" class="latex" title="H_C" /> takes the clauses of <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi" class="latex" title="\varphi" /> and turns them into local quantum interactions. We’ve constructed <img src="https://s0.wp.com/latex.php?latex=H_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C" class="latex" title="H_C" /> so that it has two eigenvalues: 0 and 1. The eigenspace corresponding to 0 is spanned by the set of computational basis vectors <img src="https://s0.wp.com/latex.php?latex=%7Cx%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|x\rangle" class="latex" title="|x\rangle" /> that satisfy <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />, and the eigenspace corresponding to 1 is spanned by the computational basis vectors that don’t satisfy <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />. In effect, when we consider <img src="https://s0.wp.com/latex.php?latex=H_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_C" class="latex" title="H_C" /> as a term of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />, we are giving an energy penalty to any variable assignment that doesn’t satisfy <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" />. <img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Csum_%7BC%7DH_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H = \sum_{C}H_C" class="latex" title="H = \sum_{C}H_C" /> will have the eigenvalue 0 (in other words, a ground state energy of 0) if and only if there is some assignment of the variables <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> that satisfies all of the clauses (in other words, iff <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi" class="latex" title="\varphi" /> is satisfiable). Otherwise, the ground state energy of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> will be at least 1, so determining whether <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi" class="latex" title="\varphi" /> is satisfiable is equivalent to solving <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH with inputs <img src="https://s0.wp.com/latex.php?latex=a+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a = 0" class="latex" title="a = 0" />, and <img src="https://s0.wp.com/latex.php?latex=b+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b = 1" class="latex" title="b = 1" />. (In fact, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH generalizes MAX-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-CSP, since the ground state energy of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is exactly the number of clauses minus the maximum number of satisfiable clauses.)</p>
<p><span id="more-6358"></span></p>
<p></p>
<p></p>
<p>Let’s work through an example. Consider the following 2-SAT formula:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x_1%2Cx_2%2Cx_3%29+%3D+%28x_1+%5Cvee+x_2%29+%5Cwedge+%28%5Coverline%7Bx_1%7D+%5Cvee+x_3%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi(x_1,x_2,x_3) = (x_1 \vee x_2) \wedge (\overline{x_1} \vee x_3)" class="latex" title="\varphi(x_1,x_2,x_3) = (x_1 \vee x_2) \wedge (\overline{x_1} \vee x_3)" /></p>
<p>The truth table for the first clause <img src="https://s0.wp.com/latex.php?latex=C_1+%3D+%28x_1+%5Cvee+x_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C_1 = (x_1 \vee x_2)" class="latex" title="C_1 = (x_1 \vee x_2)" /> is:</p>
<table>
<tbody>
<tr>
<th style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1" class="latex" title="x_1" /></th>
<th style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=x_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_2" class="latex" title="x_2" /></th>
<th style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=x_1+%5Cvee+x_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1 \vee x_2" class="latex" title="x_1 \vee x_2" /></th>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>So <img src="https://s0.wp.com/latex.php?latex=H_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_1" class="latex" title="H_1" /> is the following matrix:</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_1+%3D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_1 = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}" class="latex" title="H_1 = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}" /></p>
<p>We also have</p>
<p><img src="https://s0.wp.com/latex.php?latex=H_2+%3D+%5Cbegin%7Bpmatrix%7D+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_2 = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}" class="latex" title="H_2 = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}" /></p>
<p>Then,<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H+%26%3D+%28H_1%29_%7B1%2C2%7D+%5Cotimes+I_%7B3%7D+%2B+%28H_2%29_%7B1%2C3%7D+%5Cotimes+I_%7B2%7D+%5C%5C+%26%3D+%5Cbegin%7Bpmatrix%7D+1%26%26%26%26%26%26%26+%5C%5C+%261%26%26%26%26%26%26+%5C%5C+%26%260%26%26%26%26%26+%5C%5C+%26%26%260%26%26%26%26+%5C%5C+%26%26%26%260%26%26%26+%5C%5C+%26%26%26%26%260%26%26+%5C%5C+%26%26%26%26%26%260%26+%5C%5C+%26%26%26%26%26%26%260+%5Cend%7Bpmatrix%7D+%2B+%5Cbegin%7Bpmatrix%7D+0%26%26%26%26%26%26%26+%5C%5C+%260%26%26%26%26%26%26+%5C%5C+%26%260%26%26%26%26%26+%5C%5C+%26%26%260%26%26%26%26+%5C%5C+%26%26%26%261%26%26%26+%5C%5C+%26%26%26%26%260%26%26+%5C%5C+%26%26%26%26%26%261%26+%5C%5C+%26%26%26%26%26%26%260+%5Cend%7Bpmatrix%7D+%5C%5C+%26%3D+%5Cbegin%7Bpmatrix%7D+1%26%26%26%26%26%26%26+%5C%5C+%261%26%26%26%26%26%26+%5C%5C+%26%260%26%26%26%26%26+%5C%5C+%26%26%260%26%26%26%26+%5C%5C+%26%26%26%261%26%26%26+%5C%5C+%26%26%26%26%260%26%26+%5C%5C+%26%26%26%26%26%261%26+%5C%5C+%26%26%26%26%26%26%260+%5Cend%7Bpmatrix%7D%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{aligned} H &amp;= (H_1)_{1,2} \otimes I_{3} + (H_2)_{1,3} \otimes I_{2} \\ &amp;= \begin{pmatrix} 1&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;1&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;0&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;0&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix} + \begin{pmatrix} 0&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;0&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;1&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;1&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix} \\ &amp;= \begin{pmatrix} 1&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;1&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;1&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;1&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix}\end{aligned}" class="latex" title="\begin{aligned} H &amp;= (H_1)_{1,2} \otimes I_{3} + (H_2)_{1,3} \otimes I_{2} \\ &amp;= \begin{pmatrix} 1&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;1&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;0&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;0&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix} + \begin{pmatrix} 0&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;0&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;1&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;1&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix} \\ &amp;= \begin{pmatrix} 1&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;1&amp;&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;0&amp;&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;0&amp;&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;1&amp;&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;0&amp;&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;1&amp; \\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;0 \end{pmatrix}\end{aligned}" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> has diagonal entries that are zero, so it has 0 as an eigenvalue. We can therefore conclude that <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\varphi" class="latex" title="\varphi" /> is satisfiable. (In this example it was easy to write out <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> and see that it has zeros on the diagonal, but when <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> is large, <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> becomes exponentially big, so we can’t just compute it explicitly and look through its diagonal entries.)</p>
<h1>Quantum Cook-Levin Theorem</h1>
<p>We’ve seen that any <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-CSP problem can be thought of as a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH problem (with a diagonal Hamiltonian matrix). And the analogy can be drawn even further. One reason <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-CSP is so useful is that it (and in particular 3-SAT) is NP-complete, according to the Cook-Levin Theorem. 3-SAT captures the difficulty of classical efficiently verifiable computation. It may not come as a surprise, then, that <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH captures the difficulty of <em>quantum</em> efficiently verifiable computation. This result is the “quantum Cook-Levin theorem”, but before we see it we need to define the complexity class QMA, the quantum analogue of NP.</p>
<p>Because quantum computation is probabilistic, QMA is more precisely the quantum analogue of MA (Merlin Arthur), which allows the verifier to have a chance of error:</p>
<hr />
<p><strong>MA</strong></p>
<p><img src="https://s0.wp.com/latex.php?latex=L+%5Cin+%5Ctext%7BMA%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L \in \text{MA}" class="latex" title="L \in \text{MA}" /> iff there exists a probabilistic poly-time verifier <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V" class="latex" title="V" /> and a polynomial <img src="https://s0.wp.com/latex.php?latex=p%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(n)" class="latex" title="p(n)" /> such that</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=%5Cforall+x+%5Cin+L%2C+%5Cexists+y+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bp%28n%29%7D%2C%5Cquad+%5CPr%5BV%28x%2Cy%29+%3D+1%5D+%5Cgeq+%5Cfrac%7B2%7D%7B3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall x \in L, \exists y \in \{0,1\}^{p(n)},\quad \Pr[V(x,y) = 1] \geq \frac{2}{3}" class="latex" title="\forall x \in L, \exists y \in \{0,1\}^{p(n)},\quad \Pr[V(x,y) = 1] \geq \frac{2}{3}" /><p></p>
</li>
<li>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+x+%5Cnotin+L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall x \notin L" class="latex" title="\forall x \notin L" />, <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%7Cy%5Crangle+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bp%28n%29%7D%2C%5Cquad+%5CPr%5BV%28x%2Cy%29+%3D+1%5D+%5Cleq+%5Cfrac%7B1%7D%7B3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall |y\rangle \in \{0,1\}^{p(n)},\quad \Pr[V(x,y) = 1] \leq \frac{1}{3}" class="latex" title="\forall |y\rangle \in \{0,1\}^{p(n)},\quad \Pr[V(x,y) = 1] \leq \frac{1}{3}" /></p>
</li>
</ul>
<hr />
<p>For QMA, the verifier is a quantum computer and the witness is a quantum state. Moreover, we’re interested in the complexity of promise problems:</p>
<hr />
<p><strong>QMA</strong></p>
<p>A promise problem <img src="https://s0.wp.com/latex.php?latex=L+%3D+L_%7Byes%7D+%5Ccup+L_%7Bno%7D+%5Cin+%5Ctext%7BQMA%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L = L_{yes} \cup L_{no} \in \text{QMA}" class="latex" title="L = L_{yes} \cup L_{no} \in \text{QMA}" /> iff there exists a quantum poly-time verifier <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V" class="latex" title="V" /> and a polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> such that</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=%5Cforall+x+%5Cin+L_%7Byes%7D%2C+%5Cexists+%7Cy%5Crangle+%5Cin+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+p%28%7Cx%7C%29%7D%2C%5Cquad+%5CPr%5BV%28%7Cx%5Crangle%7Cy%5Crangle%29+%3D+1%5D+%5Cgeq+%5Cfrac%7B2%7D%7B3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall x \in L_{yes}, \exists |y\rangle \in (\mathbb{C}^2)^{\otimes p(|x|)},\quad \Pr[V(|x\rangle|y\rangle) = 1] \geq \frac{2}{3}" class="latex" title="\forall x \in L_{yes}, \exists |y\rangle \in (\mathbb{C}^2)^{\otimes p(|x|)},\quad \Pr[V(|x\rangle|y\rangle) = 1] \geq \frac{2}{3}" /><p></p>
</li>
<li>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cforall+x+%5Cnotin+L_%7Bno%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall x \notin L_{no}" class="latex" title="\forall x \notin L_{no}" />, <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%7Cy%5Crangle+%5Cin+%28%5Cmathbb%7BC%7D%5E2%29%5E%7B%5Cotimes+p%28%7Cx%7C%29%7D%2C%5Cquad+%5CPr%5BV%28%7Cx%5Crangle%7Cy%5Crangle%29+%3D+1%5D+%5Cleq+%5Cfrac%7B1%7D%7B3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\forall |y\rangle \in (\mathbb{C}^2)^{\otimes p(|x|)},\quad \Pr[V(|x\rangle|y\rangle) = 1] \leq \frac{1}{3}" class="latex" title="\forall |y\rangle \in (\mathbb{C}^2)^{\otimes p(|x|)},\quad \Pr[V(|x\rangle|y\rangle) = 1] \leq \frac{1}{3}" /></p>
</li>
</ul>
<hr />
<p>A problem is QMA-complete if it is in QMA and if any problem in QMA can be reduced to it in polynomial time. In 2002, Kitaev proved that <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH is QMA-complete for all <img src="https://s0.wp.com/latex.php?latex=k+%5Cgeq+5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \geq 5" class="latex" title="k \geq 5" />. This was the first time a natural problem was shown to be QMA-complete. In 2003 Kempe and Regev proved that 3-LH is QMA-complete, and finally in 2006 Kempe, Kitaev and Regev proved that 2-LH is QMA complete, achieving the best possible result unless P = QMA. (3-SAT is NP-complete but 2-SAT is in P, so it may seem curious that 2-LH is QMA-complete. But in fact, this isn’t too surprising, because as we mentioned earlier, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-LH corresponds to MAX-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT, and MAX-2-SAT is NP-complete.)</p>
<hr />
<p><strong>“Quantum Cook-Levin Theorem”</strong></p>
<p>The 2-local Hamiltonian problem is QMA-complete.</p>
<hr />
<p><em>A very sketchy proof sketch.    </em>This theorem is called the quantum Cook-Levin theorem not just because of the result, but also because the proof is along the same lines as the proof of the Cook-Levin theorem.</p>
<p>Recall that in the proof of the Cook-Levin theorem, we start with a verifier Turing machine that takes as input <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(x,y)" class="latex" title="(x,y)" /> and, in time <img src="https://s0.wp.com/latex.php?latex=p%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(n)" class="latex" title="p(n)" /> accepts iff <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> is a valid witness for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> being in the language. We then devise (for each <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />) a 3-SAT formula such that any satisfying solution to the instance must be an encoding of a valid history of the Turing machine from start to finish on the input <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />. The constraints must guarantee that (a) the input indeed starts with <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />, (b) at every time step <img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5B1%2Cp%28n%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t \in [1,p(n)]" class="latex" title="t \in [1,p(n)]" /> the state of the machine correctly follows from its state at time <img src="https://s0.wp.com/latex.php?latex=t-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t-1" class="latex" title="t-1" />, and that (c) the final state of the machine indicates acceptance. The constraints for (a) and (c) are trivial, and the reason we can do (b) is because Turing machines compute <em>locally</em>.</p>
<p>For our quantum Cook-Levin proof, we follow the same template. Given a quantum circuit, we construct a local Hamiltonian that has ground energy below some constant only if there is a quantum encoding of the circuit that includes the proper (a) initial state, (b) intermediate computation, and (c) final state. As before, (a) and (c) are easy, because the parts of the initial and final states we need to ‘inspect’ (with the local terms of the Hamiltonian) are essentially classical. But when we try to compute local constraints for (b), we run into a big problem: entanglement.</p>
<p>Consider some step of the computation. This will consist of applying a quantum gate <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" /> to a state <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" /> to obtain <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle+%3D+U%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle = U|\psi\rangle" class="latex" title="|\psi'\rangle = U|\psi\rangle" />. Even assuming we’ve already written down constraints to verify that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" /> is correct, it is non-trivial to write down constraints to verify that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle+%3D+U%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle = U|\psi\rangle" class="latex" title="|\psi'\rangle = U|\psi\rangle" /> because <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle" class="latex" title="|\psi'\rangle" /> may differ from <img src="https://s0.wp.com/latex.php?latex=U%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U|\psi\rangle" class="latex" title="U|\psi\rangle" /> in a highly <em>non-local</em> way if there is entanglement between far-flung qubits. For example, suppose for the sake of illustration that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle+%2B+%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" class="latex" title="|\psi\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" /> and <img src="https://s0.wp.com/latex.php?latex=U+%3D+I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = I" class="latex" title="U = I" />. And suppose we want to check that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle+%2B+%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" class="latex" title="|\psi'\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" /> with 1-local constraints. Unfortunately, there is no way to distinguish <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle+%2B+%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" class="latex" title="\frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle+-+%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{2}}(|00\rangle - |11\rangle)" class="latex" title="\frac{1}{\sqrt{2}}(|00\rangle - |11\rangle)" /> by looking at one qubit at a time: the reduced density matrix of either state for either qubit is the same: <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I" class="latex" title="I" />/2. There are examples like this that apply for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local constraints for any <img src="https://s0.wp.com/latex.php?latex=k+%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k &gt;1" class="latex" title="k &gt;1" />, so we can’t even verify the ‘trivial’ gate <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="I" class="latex" title="I" />, let alone gates that actually change the state. It would seem that we are stuck.</p>
<p>Luckily, although quantum superposition makes this problem more difficult, we can actually use superposition in a clever manner in order to surmount the difficulty. Instead of encoding the states <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" /> and <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle" class="latex" title="|\psi'\rangle" /> separately, we can put them in superposition in a way that will allow us to verify that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle+%3D+U%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle = U|\psi\rangle" class="latex" title="|\psi'\rangle = U|\psi\rangle" />. Suppose again that <img src="https://s0.wp.com/latex.php?latex=U+%3D+I&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U = I" class="latex" title="U = I" />, so we want to check that <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle+%3D+%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle = |\psi\rangle" class="latex" title="|\psi'\rangle = |\psi\rangle" />. Let <img src="https://s0.wp.com/latex.php?latex=%7C%5Ceta%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C%5Cpsi%5Crangle%7C0%5Crangle+%2B+%7C%5Cpsi%27%5Crangle%7C1%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\eta\rangle = \frac{1}{\sqrt{2}}(|\psi\rangle|0\rangle + |\psi'\rangle|1\rangle)" class="latex" title="|\eta\rangle = \frac{1}{\sqrt{2}}(|\psi\rangle|0\rangle + |\psi'\rangle|1\rangle)" />. Then, just by looking at the last qubit of <img src="https://s0.wp.com/latex.php?latex=%7C%5Ceta%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\eta\rangle" class="latex" title="|\eta\rangle" />, we can tell how close <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle" class="latex" title="|\psi'\rangle" /> is to <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" />: the reduced density matrix of the last qubit contains information about the angle between <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi\rangle" class="latex" title="|\psi\rangle" /> and <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%27%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi'\rangle" class="latex" title="|\psi'\rangle" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bpmatrix%7D+1+%26+%5Clangle%5Cpsi%7C%5Cpsi%27%5Crangle+%5C%5C+%5Clangle%5Cpsi%7C%5Cpsi%27%5Crangle+%26+1+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\begin{pmatrix} 1 &amp; \langle\psi|\psi'\rangle \\ \langle\psi|\psi'\rangle &amp; 1 \end{pmatrix}" class="latex" title="\begin{pmatrix} 1 &amp; \langle\psi|\psi'\rangle \\ \langle\psi|\psi'\rangle &amp; 1 \end{pmatrix}" /></p>
<p>The challenge is to describe a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> that has a state with energy below some parameter <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> whenever <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> is a valid witness for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />, and otherwise has no state with energy below <img src="https://s0.wp.com/latex.php?latex=b%3Ea&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b&gt;a" class="latex" title="b&gt;a" />. We won’t cover the details here, but the crucial idea is that when <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> is a valid witness for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />, the following ‘witness state’ (which is a superposition of the states of the quantum computer over all the time steps) will have low energy for a carefully-devised <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%7C%5Ceta%5Crangle+%3D+%5Cfrac%7B1%7D%7Bp%28n%29%7D%5Csum_%7Bt%3D0%7D%5E%7Bp%28n%29%7D%28U_t%5Ccdots+U_1%7C%5Cpsi_0%5Crangle%29%5Cotimes%7Ct%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\eta\rangle = \frac{1}{p(n)}\sum_{t=0}^{p(n)}(U_t\cdots U_1|\psi_0\rangle)\otimes|t\rangle" class="latex" title="|\eta\rangle = \frac{1}{p(n)}\sum_{t=0}^{p(n)}(U_t\cdots U_1|\psi_0\rangle)\otimes|t\rangle" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_0%5Crangle+%3D+%7Cx%5Crangle%7Cy%5Crangle%7C0%5Crangle%5E%7B%5Cotimes+m%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\psi_0\rangle = |x\rangle|y\rangle|0\rangle^{\otimes m}" class="latex" title="|\psi_0\rangle = |x\rangle|y\rangle|0\rangle^{\otimes m}" /> is the initial state of the computation, and <img src="https://s0.wp.com/latex.php?latex=%7Ct%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|t\rangle" class="latex" title="|t\rangle" /> is called the “clock register”. Note that because the size of the clock register is logarithmic in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, we actually need to use <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log(n)" class="latex" title="\log(n)" />-local constraints. For 5-local constraints to suffice, the witness state will need to be a little more complicated (the proof for 2-local constraints is even more difficult). Even for the case of <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log(n)" class="latex" title="\log(n)" />-LH, the complete proof must demonstrate that no state besides the witness state has low energy.</p>
<p style="text-align: right;">□</p>
<h1>Roadmap</h1>
<p>The upshot is that 2-LH is the canonical QMA-complete problem. This is a beautiful result from a quantum complexity theory perspective, but from a physics perspective it is very bad news. The QMA-completeness of the local Hamiltonian problem means that (presuming BQP <img src="https://s0.wp.com/latex.php?latex=%5Cneq&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\neq" class="latex" title="\neq" /> QMA) we can’t solve 2-LH, and we couldn’t even solve it with a quantum computer. Because of the central importance of finding the ground energy, this in turn means that <em>almost anything a physicist would like to compute about a system is intractable</em>.</p>
<p>So is all hope lost? No! Just as we started out wanting to understand Hamiltonians in general and restricted our focus to <em>local</em> Hamiltonians, the approach the physics community has taken is to focus on even more restricted classes of Hamiltonians that still capture interesting physical systems. One route is to restrict the topology of the system encoded by the Hamiltonian: for example, in many physics models, the particles form a low-dimensional lattice and the only interactions between them are 2-local interactions along edges. Even this isn’t enough, though: the problem remains QMA-hard on many simple topologies like lattices (for example, 2-LH on the 2-D lattice is QMA-complete, and 2-LH on even the 1-D lattice is QMA-complete when instead of qubits we are dealing with 8-dimensional qudits). So we add a further restriction, which is to focus on <em>gapped</em> Hamiltonians: these are Hamiltonians for which there is a constant gap between the ground energy and the second-lowest energy.</p>
<p></p><div style="width: 392px;" id="attachment_6405" class="wp-caption aligncenter"><img src="https://windowsontheory.files.wordpress.com/2018/12/circle_diagram1.png?w=382&amp;h=336" alt="circle_diagram1.png" height="336" class="  wp-image-6405 aligncenter" width="382" /><p class="wp-caption-text">Even local Hamiltonians are intractable in general. Gapped Hamiltonians on low-dimensional lattices, though, may be tractable.</p></div><p></p>
<p>Thus, in the notes to follow, we will focus our energies on trying to solve the gapped local Hamiltonian problem for 1-D and 2-D lattices. The reason there is hope in these settings is that entanglement is (or is conjectured to be) limited by ‘area laws’. In the next post, Fred Zhang will describe a diagrammatic language (‘tensor networks’) for thinking about low-entanglement quantum states, and he’ll show how physicists solve the local Hamiltonian problem for gapped 1-D systems.</p></div>







<p class="date">
by benedelman <a href="https://windowsontheory.org/2018/12/20/quantum-hamiltonian-complexity/"><span class="datestr">at December 20, 2018 09:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5365">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2018/12/20/incidences-in-a-recent-work-of-walsh/">Incidences in a Recent Work of Walsh</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Recently, Miguel Walsh posted a very interesting paper on arXiv. The main purpose of the paper is to study various properties of polynomials and varieties. These properties are related to incidence problems – some originally arose from studying incidences. Walsh also presents new incidence bounds as applications of his results. In this post I’ll briefly […]<p></p></div></div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2018/12/20/incidences-in-a-recent-work-of-walsh/"><span class="datestr">at December 20, 2018 05:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=614">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2018/12/20/50m-to-northeastern-computer-science/">$50M to Northeastern Computer Science</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;"><a href="https://www.bostonglobe.com/business/2018/12/17/northeastern-receives-million-gift-further-studies/ygZaKf1F56SzNSCB818zJO/story.html">Northeastern Computer Science is receiving a $50M gift</a>.  If you are looking for a faculty position, check out our many openings, including the <a href="https://cstheory-jobs.org/2018/10/21/faculty-at-northeastern-university-apply-by-december-22-2018/">joint math-cs position</a>. Also if you are applying for a PhD take a look at our college.  In particular as I mentioned already <a href="https://emanueleviola.wordpress.com/2018/12/03/i-am-looking-for-students/">I am looking for students</a>.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2018/12/20/50m-to-northeastern-computer-science/"><span class="datestr">at December 20, 2018 03:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/19/goldstine-postdoctoral-fellow-at-ibm-t-j-watson-research-center-apply-by-january-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/19/goldstine-postdoctoral-fellow-at-ibm-t-j-watson-research-center-apply-by-january-31-2019/">Goldstine Postdoctoral Fellow at IBM T.J. Watson Research Center (apply by January 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Mathematical Sciences department of the IBM T.J. Watson Research Center invites applications for the Herman Goldstine Memorial Postdoctoral Fellowship for research in mathematical and computer sciences. The department provides an atmosphere in which basic research is combined with work on practical applications. More details are available at <a href="https://www.research.ibm.com/goldstine/">https://www.research.ibm.com/goldstine/</a></p>
<p>Website: <a href="https://www.research.ibm.com/goldstine/">https://www.research.ibm.com/goldstine/</a><br />
Email: gldpost2@us.ibm.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/19/goldstine-postdoctoral-fellow-at-ibm-t-j-watson-research-center-apply-by-january-31-2019/"><span class="datestr">at December 19, 2018 10:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1470">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2018/12/18/2019-godel-prize/">2019 Gödel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>If I write a post and the blog aggregator is down, does it still make a sound?</p>
<hr />
<p> </p>
<p>The call for nomination for the 2019 Gödel Prize is out and the deadline is February 15th. For all awards, we sometimes have the tendency to think that worthy candidates have surely been nominated by others. Often it is not the case (and thus worthy candidates are often left behind). So if there is a paper or papers deserving nomination, please nominate! The call for nomination is below.</p>
<hr />
<p> </p>
<h1>The Gödel Prize 2019 – Call for Nominations</h1>
<p>Deadline: February 15, 2019</p>
<p>The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery, Special Interest Group on Algorithms and Computation Theory (ACM SIGACT). The award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The 27th Gödel Prize will be awarded at 51st Annual ACM Symposium on the Theory of Computing to be held during June 23-26, 2019 in Phoenix, AZ. The Prize is named in honor of Kurt Gödel in recognition of his major contributions to mathematical logic and of his interest, discovered in a letter he wrote to John von Neumann shortly before von Neumann’s death, in what has become the famous “P versus NP” question. The Prize includes an award of USD 5,000.</p>
<p><strong>Award Committee: </strong>The 2019 Award Committee consists of Anuj Dawar (Cambridge University), Robert Krauthgamer (Weizmann Institute), Joan Feigenbaum (Yale University), Giuseppe Persiano (Università di Salerno), Omer Reingold (Chair, Stanford University) and Daniel Spielman (Yale University).</p>
<p><strong>Eligibility:</strong> The 2019 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if: – The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1st, 2006. – The paper was published in a recognized refereed journal no later than December 31, 2018. The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked. The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>
<p><strong>Nominations:</strong></p>
<p>Nominations for the award should be submitted by email to the Award Committee Chair: <a href="mailto:reingold@stanford.edu">reingold@stanford.edu</a>. Please make sure that the Subject line of all nominations and related messages begin with “Goedel Prize 2019.” To be considered, nominations for the 2019 Prize must be received by February 15, 2019.</p>
<p>A nomination package should include:</p>
<p>1. A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>
<p>2. A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that no such publication has occurred.</p>
<p>3. A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>
<p>4. A support letter or letters signed by at least two members of the scientific community.</p>
<p>Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a nominated publication is not in English, the nomination package must include an extended summary written in English.</p>
<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p>
<p><strong>Selection Process:</strong></p>
<p>The Award Committee is free to use any other sources of information in addition to the ones mentioned above. It may split the award among multiple papers, or declare no winner at all. All matters relating to the selection process left unspecified in this document are left to the discretion of the Award Committee.</p>
<p><strong>Recent Winners</strong></p>
<p>(all winners since 1993 are listed at <a href="http://www.sigact.org/Prizes/Godel/">http://www.sigact.org/Prizes/Godel/</a> and <a href="http://eatcs.org/index.php/goedel-prize">http://eatcs.org/index.php/goedel-prize</a>):</p>
<p><strong>2018:</strong> Oded Regev, On lattices, learning with errors, random linear codes, and cryptography, Journal of the ACM (JACM), Volume 56 Issue 6, 2009 (preliminary version in Symposium on Theory of Computing, STOC 2005).</p>
<p><strong>2017:</strong> Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith, Calibrating Noise to Sensitivity in Private Data Analysis, Journal of Privacy and Confidentiality, Volume 7, Issue 3, 2016 (preliminary version in Theory of Cryptography, TCC 2006).</p>
<p><strong>2016:</strong> Stephen Brookes, A Semantics for Concurrent Separation Logic. Theoretical Computer Science 375(1-3): 227-270 (2007). Peter W. O’Hearn, Resources, Concurrency, and Local Reasoning. Theoretical Computer Science 375(1-3): 271-307 (2007).</p>
<p><strong>2015:</strong> Dan Spielman and Shang-Hua Teng, Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems, Proc. 36th ACM Symposium on Theory of Computing, pp. 81-90, 2004; Spectral sparsification of graphs, SIAM J. Computing 40:981-1025, 2011; A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning, SIAM J. Computing 42:1-26, 2013; Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM J. Matrix Anal. Appl. 35:835-885, 2014.</p>
<p><strong>2014: </strong>Ronald Fagin, Amnon Lotem, and Moni Naor, Optimal Aggregation Algorithms for Middleware, Journal of Computer and System Sciences 66(4): 614–656, 2003.</p>
<p><strong>2013: </strong>Antoine Joux, A one round protocol for tripartite Diffie-Hellman, J. Cryptology 17(4): 263-276, 2004. Dan Boneh and Matthew K. Franklin, Identity-Based Encryption from the Weil pairing, SIAM J. Comput. 32(3): 586-615, 2003.</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2018/12/18/2019-godel-prize/"><span class="datestr">at December 18, 2018 08:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6607">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/18/a-1d-area-law-for-gapped-local-hamiltonians/">A 1D Area Law for Gapped Local Hamiltonians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>(<em>This post is based on part of a lecture delivered by Boriana Gjura and Prayaag Venkat. See also posts by <a href="https://windowsontheory.org/2018/12/20/quantum-hamiltonian-complexity/">Ben Edelman</a> and <a href="https://windowsontheory.org/2018/12/20/tensor-networks-matrix-product-states-dmrg/">Fred Zhang</a> for more context on Quantum Hamiltonian Complexity.</em>)</p>
<h1>Introduction</h1>
<p>In this post we present the Area Law conjecture and prove it rigorously,<br />
emphasizing the emergence of approximate ground state projectors as a<br />
useful and promising tool.</p>
<p>The difficulty of understanding many-body physics lies in this dichotomy<br />
between the power of quantum on the one hand, and the fact that even<br />
describing a quantum state requires exponential resources on the other.<br />
A natural way to proceed is to ask if the special class of physically<br />
relevant states admits succinct descriptions. As seen in a previous<br />
post, the QMA-completenes of the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-local Hamiltonian problem suggests<br />
that even ground states of local Hamiltonians do not admit such<br />
descriptions. Looking at these ground states, the following questions<br />
arise naturally.</p>
<p><em>When do the ground states of local Hamiltonians have a special<br />
structure?</em> <em>When does that structure allow for a meaningful short<br />
description?</em> <em>When does that structure allow us to compute properties<br />
of them?</em></p>
<p>As stated informally, the answer to these questions is yes for (gapped)<br />
1D systems, and it is an open problem to answer these in higher<br />
dimensions.</p>
<h1>The Area Law</h1>
<p>The Area Law is inspired by the Holographic principle in Cosmology,<br />
which informally states that the total information in a black hole<br />
resides on the boundary. That is, the complexity of the system depends<br />
on the size of its boundary, not its volume.</p>
<h2>Preliminaries</h2>
<p>For the sake of clarity, we assume that <img src="https://s0.wp.com/latex.php?latex=H+%3D+%5Csum_%7Bi%3D1%7D%5Em+H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H = \sum_{i=1}^m H_i" class="latex" title="H = \sum_{i=1}^m H_i" /> is a<br />
2-local Hamiltonian on qubits, each <img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+%5C%7C+H_i+%5C%7C+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 \leq \| H_i \| \leq 1" class="latex" title="0 \leq \| H_i \| \leq 1" /> is a<br />
projection <img src="https://s0.wp.com/latex.php?latex=%28H_i%5E2+%3D+H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H_i^2 = H_i)" class="latex" title="(H_i^2 = H_i)" />, <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is frustration free, (meaning that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clambda_0%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_0(H)" class="latex" title="\lambda_0(H)" />, the lowest eigenvalue, is zero) and there is a unique<br />
ground state <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle" class="latex" title="| \psi_0 \rangle" />.</p>
<p>These conditions simplify our proof. It will be easy to generalize to<br />
<img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+%5C%7C+H_i+%5C%7C+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 \leq \| H_i \| \leq 1" class="latex" title="0 \leq \| H_i \| \leq 1" /> and relaxing to <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" />-local on qudits (dimension<br />
<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> particles). The most significant assumption is that <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is<br />
frustration-free: more work is necessary to make the proof work<br />
otherwise.</p>
<p>To formalize our conjecture, we define a notion of von Neumman entropy<br />
below.</p>
<blockquote><p><strong>Definition:</strong> Consider any state <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle" class="latex" title="| \psi \rangle" /> lying in a bipartite Hilbert space<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D_A+%5Cotimes+%5Cmathcal%7BH%7D_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{H}_A \otimes \mathcal{H}_B" class="latex" title="\mathcal{H}_A \otimes \mathcal{H}_B" />, and perform a Schmidt<br />
decomposition to obtain<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle+%3D+%5Csum_%7Bi+%3D1%7D%5E%7Bd%7D+%5Clambda_i+%7C+u_i+%5Crangle_A%7C+v_i+%5Crangle_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle = \sum_{i =1}^{d} \lambda_i | u_i \rangle_A| v_i \rangle_B" class="latex" title="| \psi \rangle = \sum_{i =1}^{d} \lambda_i | u_i \rangle_A| v_i \rangle_B" />.<br />
The Von Neumann entanglement entropy of <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle" class="latex" title="| \psi \rangle" /> across region A<br />
is defined as<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S%28A%29_%7B%7C+%5Cpsi+%5Crangle%7D+%3D+%5Csum_i+%5Clambda_i%5E2+%5Cln+%5Cfrac%7B1%7D%7B%5Clambda_i%5E2%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S(A)_{| \psi \rangle} = \sum_i \lambda_i^2 \ln \frac{1}{\lambda_i^2}." class="latex" title="\displaystyle S(A)_{| \psi \rangle} = \sum_i \lambda_i^2 \ln \frac{1}{\lambda_i^2}." /></p></blockquote>
<p>Why is this a good notion of entropy? If<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle+%3D+%7C+L+%5Crangle+%5Cotimes+%7C+R+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle = | L \rangle \otimes | R \rangle" class="latex" title="| \psi \rangle = | L \rangle \otimes | R \rangle" />, then<br />
<img src="https://s0.wp.com/latex.php?latex=S_%7B%7C+%5Cpsi+%5Crangle%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_{| \psi \rangle} = 0" class="latex" title="S_{| \psi \rangle} = 0" />, i.e. the product state has no entanglement. On<br />
the other hand, the maximally entangled state<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle+%3D+%5Csum_%7Bi+%3D+1%7D%5ED+%5Cfrac%7B1%7D%7B%5Csqrt%7BD%7D%7D+%7C+i+%5Crangle_A+%5Cotimes+%7C+i+%5Crangle_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle = \sum_{i = 1}^D \frac{1}{\sqrt{D}} | i \rangle_A \otimes | i \rangle_B" class="latex" title="| \psi \rangle = \sum_{i = 1}^D \frac{1}{\sqrt{D}} | i \rangle_A \otimes | i \rangle_B" /><br />
has entropy <img src="https://s0.wp.com/latex.php?latex=S_%7B%7C+%5Cpsi+%5Crangle%7D+%3D+%5Cln%28D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_{| \psi \rangle} = \ln(D)" class="latex" title="S_{| \psi \rangle} = \ln(D)" />. In general, we have that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_%7B%7C+%5Cpsi+%5Crangle%7D+%5Cleq+%5Cln%28%5Cdim+%5Ctext%7BHilbert+space%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S_{| \psi \rangle} \leq \ln(\dim \text{Hilbert space})." class="latex" title="\displaystyle S_{| \psi \rangle} \leq \ln(\dim \text{Hilbert space})." /><br />
This is a<br />
direct quantum generalization of the fact that a classical probability<br />
distribution on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> elements has entropy at most <img src="https://s0.wp.com/latex.php?latex=%5Cln+D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ln D" class="latex" title="\ln D" />, and this is<br />
achieved by the uniform distribution.\<br />
We can now state the Area Law.</p>
<blockquote><p><strong>Conjecture:</strong> Let <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle" class="latex" title="| \psi_0 \rangle" /> be the ground state of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />, as defined above.<br />
Then for any region <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" /> (i.e. a subset of qubits),</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_%7B%7C+%5Cpsi_0+%5Crangle%7D+%5Cleq+%5Cln+%28%5Cdim+%5Cpartial+A%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S_{| \psi_0 \rangle} \leq \ln (\dim \partial A)," class="latex" title="\displaystyle S_{| \psi_0 \rangle} \leq \ln (\dim \partial A)," /><br />
where <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial A" class="latex" title="\partial A" /><br />
denotes the boundary of the region <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" />, the set of qubits that interact<br />
through <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> with at least a qubit outside of region <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A" class="latex" title="A" />.</p></blockquote>
<p>This conjecture is illustrated by the following figure.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/area2.png?w=600" alt="area2" class="alignnone size-full wp-image-6610" /><br />
We prove the 1-D version of this conjecture. Note that for a 1D system<br />
(i.e. qubits on a line with nearest neighbor interactions) it suffices<br />
to consider bipartitions of the Hilbert space defined by a “cut” (see<br />
the figure below). By a cut, we mean a partition of the qubits into two<br />
sets consisting of all the qubits to the left and right, respectively,<br />
of a fixed qubit <img src="https://s0.wp.com/latex.php?latex=i%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i^*" class="latex" title="i^*" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/area1.png?w=600" alt="area1" class="alignnone size-full wp-image-6609" /></p>
<blockquote><p><strong>Theorem:</strong> Let <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle" class="latex" title="| \psi_0 \rangle" /> be as above. Then for any cut <img src="https://s0.wp.com/latex.php?latex=C+%3D+%28i%5E%2A%2C+i%5E%2A%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C = (i^*, i^*+1)" class="latex" title="C = (i^*, i^*+1)" />,<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S%28C%29_%7B%7C+%5Cpsi_0+%5Crangle%7D+%5Cleq+O%281%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S(C)_{| \psi_0 \rangle} \leq O(1)." class="latex" title="\displaystyle S(C)_{| \psi_0 \rangle} \leq O(1)." /></p></blockquote>
<h2>Approximate Ground State Projectors</h2>
<p>In the case where all <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" /> commute, consider the operator<br />
<img src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cprod_i+%28I+-+H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="A = \prod_i (I - H_i)" class="latex" title="A = \prod_i (I - H_i)" />. This operator will project any product state<br />
onto the ground state. Therefore, we can get a representation of the<br />
ground state as a matrix product state (as we saw in a previous post on<br />
tensor networks). Can we generalize this idea when the <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" /> do not<br />
necessarily commute?</p>
<p>A natural idea is to define an operator that approximately projects a<br />
state onto the vector we want (the ground state), while shrinking it on<br />
the other directions. We also want this operator to not increase<br />
entanglement too much. The proof of the area law will involve<br />
constructing a low-rank tensor approximate factorization of the ground<br />
state by starting with a factorized state, which has a good overlap with<br />
the ground state, and then repeatedly applying such an operator.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/agsp.png?w=600" alt="agsp" class="alignnone size-full wp-image-6608" /></p>
<p>We first give a formal definition of an approximate ground state<br />
projector. In the following, under the assumption of the existence of<br />
such an object, we will prove the area law. At the end, we will show how<br />
to construct it.</p>
<blockquote><p><strong>Definition:</strong> <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> is a <img src="https://s0.wp.com/latex.php?latex=%28D%2C%5CDelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(D,\Delta)" class="latex" title="(D,\Delta)" />–<em>approximate ground state projection</em> (AGSP) if:<br />
1. <img src="https://s0.wp.com/latex.php?latex=K+%7C+%5Cpsi_0+%5Crangle+%3D+%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K | \psi_0 \rangle = | \psi_0 \rangle" class="latex" title="K | \psi_0 \rangle = | \psi_0 \rangle" /><br />
2. For every <img src="https://s0.wp.com/latex.php?latex=%7C+%5CGamma+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \Gamma \rangle" class="latex" title="| \Gamma \rangle" /> such that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5CGamma+%7C+%5Cpsi_0+%5Crangle+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \Gamma | \psi_0 \rangle = 0" class="latex" title="\langle \Gamma | \psi_0 \rangle = 0" />, then<br />
<img src="https://s0.wp.com/latex.php?latex=%5C%7C+K+%7C+%5CGamma+%5Crangle+%5C%7C%5E2+%5Cleq+%5CDelta+%5C%7C+%7C+%5CGamma+%5Crangle+%5C%7C%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| K | \Gamma \rangle \|^2 \leq \Delta \| | \Gamma \rangle \|^2" class="latex" title="\| K | \Gamma \rangle \|^2 \leq \Delta \| | \Gamma \rangle \|^2" />.<br />
3. <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> has entanglement rank at most <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />. That is, <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> admits a Schmidt<br />
decomposition of the form <img src="https://s0.wp.com/latex.php?latex=K+%3D+%5Csum_%7Bi%3D1%7D%5E%7BD%7D+L_i+%5Cotimes+R_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K = \sum_{i=1}^{D} L_i \otimes R_i" class="latex" title="K = \sum_{i=1}^{D} L_i \otimes R_i" />,<br />
where <img src="https://s0.wp.com/latex.php?latex=L_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_i" class="latex" title="L_i" /> and <img src="https://s0.wp.com/latex.php?latex=R_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_i" class="latex" title="R_i" /> act only on the particles to the left and<br />
right of the cut, respectively.</p></blockquote>
<p>The first step of the proof of the Area Law is to find a product state<br />
that has a good overlap with the ground state. That is, it should have a<br />
constant overlap that is not dependent on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />, the number of particles<br />
in the system.</p>
<blockquote><p><strong>Lemma 1:</strong> Suppose there exists a <img src="https://s0.wp.com/latex.php?latex=%28D%2C+%5CDelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(D, \Delta)" class="latex" title="(D, \Delta)" />-AGSP such that<br />
<img src="https://s0.wp.com/latex.php?latex=D+%5CDelta+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta \leq \frac{1}{2}" class="latex" title="D \Delta \leq \frac{1}{2}" />. Fix a partition <img src="https://s0.wp.com/latex.php?latex=%28A%2C+%5Cbar%7BA%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(A, \bar{A})" class="latex" title="(A, \bar{A})" /> of the space<br />
on which the Hamiltonian acts. Then there exists a product state<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi+%5Crangle+%3D+%7C+L+%5Crangle_A+%5Cotimes+%7C+R+%5Crangle_%7B%5Cbar%7BA%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi \rangle = | L \rangle_A \otimes | R \rangle_{\bar{A}}" class="latex" title="| \phi \rangle = | L \rangle_A \otimes | R \rangle_{\bar{A}}" /> such that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clangle+%5Cphi+%7C+%5Cpsi_0+%5Crangle+%3D+%5Cmu+%5Cgeq+%5Cfrac%7B1%7D%7B%5Csqrt%7B2D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \langle \phi | \psi_0 \rangle = \mu \geq \frac{1}{\sqrt{2D}}" class="latex" title="\displaystyle \langle \phi | \psi_0 \rangle = \mu \geq \frac{1}{\sqrt{2D}}" /></p></blockquote>
<p><strong>Proof:</strong><br />
Let <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi \rangle" class="latex" title="| \phi \rangle" /> be a product state with the largest overlap in<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle" class="latex" title="| \psi_0 \rangle" />, meaning that it maximizes.<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi+%7C+%5Cpsi_0+%5Crangle+%3D+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \phi | \psi_0 \rangle = \mu" class="latex" title="\langle \phi | \psi_0 \rangle = \mu" /> and can be written as<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C+%5Cphi+%5Crangle+%3D+%5Cmu+%7C+%5Cpsi_0+%5Crangle+%2B+%5Csqrt%7B1+-+%5Cmu%5E2%7D+%7C+%5Cpsi%5E%7B%5Cperp%7D+%5Crangle%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle | \phi \rangle = \mu | \psi_0 \rangle + \sqrt{1 - \mu^2} | \psi^{\perp} \rangle," class="latex" title="\displaystyle | \phi \rangle = \mu | \psi_0 \rangle + \sqrt{1 - \mu^2} | \psi^{\perp} \rangle," /><br />
where the latter is some state orthogonal to the ground state. We apply<br />
<img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> to get<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+K+%7C+%5Cphi+%5Crangle+%3D+%5Cmu+%7C+%5Cpsi_0+%5Crangle+%2B+%5Cdelta+%7C+%5Cpsi%27+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle K | \phi \rangle = \mu | \psi_0 \rangle + \delta | \psi' \rangle" class="latex" title="\displaystyle K | \phi \rangle = \mu | \psi_0 \rangle + \delta | \psi' \rangle" /><br />
where <img src="https://s0.wp.com/latex.php?latex=%7C%5Cdelta%7C%5E2+%5Cleq+%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\delta|^2 \leq \Delta" class="latex" title="|\delta|^2 \leq \Delta" /> and <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi%27+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi' \rangle" class="latex" title="| \psi' \rangle" /> is normalized. By<br />
the properties of the operator <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />, the decomposition of<br />
<img src="https://s0.wp.com/latex.php?latex=K+%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K | \psi_0 \rangle" class="latex" title="K | \psi_0 \rangle" /> has at most <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> terms. Using the Cauchy-Schwarz<br />
inequality:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Baligned%7D+%5Cmu+%3D+%5Cbig%7C+%5Csum_i+%5Clambda_i+%5Clangle+%5Cpsi_0+%7C+%7C+L+%5Crangle_i+%7C+R+%5Crangle_i%5Cbig%7C+%26%5Cleq+%5Csqrt%7B%5Csum_i+%5Clambda_i+%5E2%7D+%5Csqrt%7B%5Clangle+%5Cpsi_0+%7C+%7C+L+%5Crangle_i+%7C+R+%5Crangle_i%5E2%7D+%5C%5C+%26%5Cleq+%5Csqrt%7B%5Cmu%5E2+%2B+%5CDelta%7D+%5Csqrt%7BD%7D+%5Csqrt%7B%5Cmax_i+%5Clangle+%5Cpsi_0+%7C+%7C+L+%5Crangle_i+%7C+R+%5Crangle_i%7D%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \begin{aligned} \mu = \big| \sum_i \lambda_i \langle \psi_0 | | L \rangle_i | R \rangle_i\big| &amp;\leq \sqrt{\sum_i \lambda_i ^2} \sqrt{\langle \psi_0 | | L \rangle_i | R \rangle_i^2} \\ &amp;\leq \sqrt{\mu^2 + \Delta} \sqrt{D} \sqrt{\max_i \langle \psi_0 | | L \rangle_i | R \rangle_i}\end{aligned}" class="latex" title="\displaystyle \begin{aligned} \mu = \big| \sum_i \lambda_i \langle \psi_0 | | L \rangle_i | R \rangle_i\big| &amp;\leq \sqrt{\sum_i \lambda_i ^2} \sqrt{\langle \psi_0 | | L \rangle_i | R \rangle_i^2} \\ &amp;\leq \sqrt{\mu^2 + \Delta} \sqrt{D} \sqrt{\max_i \langle \psi_0 | | L \rangle_i | R \rangle_i}\end{aligned}" /></p>
<p>Therefore there exists a product state such that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cgeq+%7C+%5Clangle+%5Cpsi_0+%7C+%7C+L+%5Crangle_i+%7C+R+%5Crangle_i%7C+%5Cgeq+%5Cfrac%7B%5Cmu%7D%7B%5Csqrt%7BD%28%5Cmu%5E2+%2BD%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu \geq | \langle \psi_0 | | L \rangle_i | R \rangle_i| \geq \frac{\mu}{\sqrt{D(\mu^2 +D)}}" class="latex" title="\mu \geq | \langle \psi_0 | | L \rangle_i | R \rangle_i| \geq \frac{\mu}{\sqrt{D(\mu^2 +D)}}" />,<br />
which implies that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E2+%5Cgeq+%5Cfrac%7B1%7D%7B2D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu^2 \geq \frac{1}{2D}" class="latex" title="\mu^2 \geq \frac{1}{2D}" />, as desired. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>Starting with a product state with a big enough overlap with the ground<br />
state, the proof of the next lemma shows how we can repeatedly apply<br />
AGSPs to bound the entropy across the cut.</p>
<blockquote><p><strong>Lemma 2:</strong> Suppose there exists a <img src="https://s0.wp.com/latex.php?latex=%28D%2C+%5CDelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(D, \Delta)" class="latex" title="(D, \Delta)" />-AGSP such that<br />
<img src="https://s0.wp.com/latex.php?latex=D+%5CDelta+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta \leq \frac{1}{2}" class="latex" title="D \Delta \leq \frac{1}{2}" />, and a product state<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi+%5Crangle+%3D+%7C+L+%5Crangle_A+%5Cotimes+%7C+R+%5Crangle_%7B%5Cbar%7BA%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi \rangle = | L \rangle_A \otimes | R \rangle_{\bar{A}}" class="latex" title="| \phi \rangle = | L \rangle_A \otimes | R \rangle_{\bar{A}}" /> such that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cphi+%7C+%5Cpsi_o+%5Crangle+%3D+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle \phi | \psi_o \rangle = \mu" class="latex" title="\langle \phi | \psi_o \rangle = \mu" />. Then<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_%7B%7C+%5Cpsi_0+%5Crangle%7D+%5Cleq+O%281%29+%5Cfrac%7B%5Clog+%5Cmu%7D%7B%5Clog+%5CDelta%7D+%5Clog%7BD%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S_{| \psi_0 \rangle} \leq O(1) \frac{\log \mu}{\log \Delta} \log{D}" class="latex" title="\displaystyle S_{| \psi_0 \rangle} \leq O(1) \frac{\log \mu}{\log \Delta} \log{D}" /></p></blockquote>
<p>But before we prove this, let’s state an intuitive result of Eckart and<br />
Young, which provides an upper bound for the overlap of two states given<br />
the Schmidt rank of one.</p>
<blockquote><p><strong>Lemma (Eckart-Young):</strong> Let <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle+%5Cin+L+%5Cotimes+R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle \in L \otimes R" class="latex" title="| \psi \rangle \in L \otimes R" /> be a normalized<br />
vector with Schmidt decomposition<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi+%5Crangle+%3D+%5Csum_i+%5Clambda_i+%7C+u_i+%5Crangle%7C+v_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi \rangle = \sum_i \lambda_i | u_i \rangle| v_i \rangle" class="latex" title="| \psi \rangle = \sum_i \lambda_i | u_i \rangle| v_i \rangle" />, where<br />
<img src="https://s0.wp.com/latex.php?latex=%5Clambda_1+%5Cgeq+%5Clambda_2+%5Cgeq+%5Cdots&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_1 \geq \lambda_2 \geq \dots" class="latex" title="\lambda_1 \geq \lambda_2 \geq \dots" />. Then for any normalized<br />
<img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi+%5Crangle+%5Cin+L+%5Cotimes+R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi \rangle \in L \otimes R" class="latex" title="| \phi \rangle \in L \otimes R" /> it holds that<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C+%5Clangle+%5Cpsi+%7C+%5Cphi+%5Crangle%7C+%5Cleq+%5Csqrt%7B%5Csum_%7Bi%7D+%5Clambda_i%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle | \langle \psi | \phi \rangle| \leq \sqrt{\sum_{i} \lambda_i^2}" class="latex" title="\displaystyle | \langle \psi | \phi \rangle| \leq \sqrt{\sum_{i} \lambda_i^2}" /></p></blockquote>
<p>Using this result, we continue the proof.</p>
<p><strong>Proof of Lemma 2:</strong><br />
Write the <img src="https://s0.wp.com/latex.php?latex=%28D%2C+%5CDelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(D, \Delta)" class="latex" title="(D, \Delta)" />-AGSP as <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />. Given a state <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi \rangle" class="latex" title="| \phi \rangle" />, we<br />
denote by <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi_l+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi_l \rangle" class="latex" title="| \phi_l \rangle" /> the normalized stated after applying <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /><br />
projections on the state, namely<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C+%5Cphi_l+%5Crangle+%3D+%5Cfrac%7BK%5El+%7C+%5Cphi+%5Crangle%7D%7B%5C%7CK%5El+%7C+%5Cphi+%5Crangle%5C%7C%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle | \phi_l \rangle = \frac{K^l | \phi \rangle}{\|K^l | \phi \rangle\|}." class="latex" title="\displaystyle | \phi_l \rangle = \frac{K^l | \phi \rangle}{\|K^l | \phi \rangle\|}." /><br />
By the way we’ve defined <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />, the Schmidt rank of <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cphi_l+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \phi_l \rangle" class="latex" title="| \phi_l \rangle" /><br />
increases by a power of <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" />, and this state has an improved overlap with<br />
the ground state, that is:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+SR%28%7C+%5Cphi_l+%5Crangle%29+%5Cleq+D%5El%2C+%5Cquad+%5Clangle+%5Cpsi_0+%7C+%5Cphi_l+%5Crangle+%5Cgeq+%5Cfrac%7B%5Cmu%7D%7B%5Csqrt%7B%5Cmu%5E2+%2B+%5CDelta%5El%281+-+%5Cmu%5E2%29%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle SR(| \phi_l \rangle) \leq D^l, \quad \langle \psi_0 | \phi_l \rangle \geq \frac{\mu}{\sqrt{\mu^2 + \Delta^l(1 - \mu^2)}}." class="latex" title="\displaystyle SR(| \phi_l \rangle) \leq D^l, \quad \langle \psi_0 | \phi_l \rangle \geq \frac{\mu}{\sqrt{\mu^2 + \Delta^l(1 - \mu^2)}}." /></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle+%3D+%5Csum_i+%5Clambda_i+%7C+L_i+%5Crangle%7C+R_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle = \sum_i \lambda_i | L_i \rangle| R_i \rangle" class="latex" title="| \psi_0 \rangle = \sum_i \lambda_i | L_i \rangle| R_i \rangle" /> be<br />
the Schmidt decomposition of the ground state relative to the cut<br />
<img src="https://s0.wp.com/latex.php?latex=%28i%5E%2A%2C+i%5E%2A%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(i^*, i^*+1)" class="latex" title="(i^*, i^*+1)" />, where <img src="https://s0.wp.com/latex.php?latex=%5Clambda_1+%5Cgeq+%5Clambda_2+%5Cgeq+%5Cdots&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_1 \geq \lambda_2 \geq \dots" class="latex" title="\lambda_1 \geq \lambda_2 \geq \dots" />. The<br />
Eckart-Young theorem gives:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5E%7BD%5El%7D+%5Clambda_i%5E2+%5Cgeq+%7C+%5Clangle+%5Cpsi_0+%7C+%5Cphi_l+%5Crangle%7C%5E2+%5Cgeq+%5Cfrac%7B%5Cmu%5E2%7D%7B%5Cmu%5E2+%2B+%5CDelta%5El+%281+-+%5Cmu%5E2%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \sum_{i=1}^{D^l} \lambda_i^2 \geq | \langle \psi_0 | \phi_l \rangle|^2 \geq \frac{\mu^2}{\mu^2 + \Delta^l (1 - \mu^2)}" class="latex" title="\displaystyle \sum_{i=1}^{D^l} \lambda_i^2 \geq | \langle \psi_0 | \phi_l \rangle|^2 \geq \frac{\mu^2}{\mu^2 + \Delta^l (1 - \mu^2)}" /><br />
from which<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3E+D%5El%7D+%5Clambda_i%5E2+%5Cleq+1+-+%5Cfrac%7B%5Cmu%5E2%7D%7B%5Cmu%5E2+%2B+%5CDelta%5El+%281+-+%5Cmu%5E2%29%7D+%5Cleq+%5Cfrac%7B%5CDelta%5El%7D%7B%5Cmu%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \sum_{i&gt; D^l} \lambda_i^2 \leq 1 - \frac{\mu^2}{\mu^2 + \Delta^l (1 - \mu^2)} \leq \frac{\Delta^l}{\mu^2}" class="latex" title="\displaystyle \sum_{i&gt; D^l} \lambda_i^2 \leq 1 - \frac{\mu^2}{\mu^2 + \Delta^l (1 - \mu^2)} \leq \frac{\Delta^l}{\mu^2}" /></p>
<p>We choose<br />
<img src="https://s0.wp.com/latex.php?latex=l_0+%3D+2+%5Cfrac%7B%5Clog+%5Cmu%7D%7B%5Clog+%5CDelta%7D+-+%5Cfrac%7B%5Clog+2%7D%7B%5Clog+%5CDelta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l_0 = 2 \frac{\log \mu}{\log \Delta} - \frac{\log 2}{\log \Delta}" class="latex" title="l_0 = 2 \frac{\log \mu}{\log \Delta} - \frac{\log 2}{\log \Delta}" /> such<br />
that <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta%5E%7Bl_0%7D%7D%7B%5Cmu%5E2%7D+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\Delta^{l_0}}{\mu^2} \leq \frac{1}{2}" class="latex" title="\frac{\Delta^{l_0}}{\mu^2} \leq \frac{1}{2}" />. Let’s now bound the<br />
worst case entropy accross the AGSP cut. The first <img src="https://s0.wp.com/latex.php?latex=D%5E%7Bl_0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D^{l_0}" class="latex" title="D^{l_0}" /> Schmidt<br />
coefficients account for an entropy of at most <img src="https://s0.wp.com/latex.php?latex=l_0+%5Clog+D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l_0 \log D" class="latex" title="l_0 \log D" />. For the<br />
remaining coefficients, we group them in chunks of size <img src="https://s0.wp.com/latex.php?latex=D%5E%7Bl_0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D^{l_0}" class="latex" title="D^{l_0}" /> in<br />
intervals <img src="https://s0.wp.com/latex.php?latex=%5BD%5E%7Bkl_0%7D+%2B+1%2C+D%5E%7B%28k%2B1%29l_0%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[D^{kl_0} + 1, D^{(k+1)l_0}]" class="latex" title="[D^{kl_0} + 1, D^{(k+1)l_0}]" /> indexed by <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />. For each of<br />
these intervals, the corresponding entropy can be upper bounded by<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7B%5Cdelta%5E%7Bkl_0%7D%7D%7B%5Cmu%5E2%7D+%5Clog+D%5E%7B%28k%2B1%29l_0%7D+%3D+l_0+%5Cfrac%7B1%7D%7B2%5Ek%7D+%28k%2B1%29+%5Clog+D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \frac{\delta^{kl_0}}{\mu^2} \log D^{(k+1)l_0} = l_0 \frac{1}{2^k} (k+1) \log D" class="latex" title="\displaystyle \frac{\delta^{kl_0}}{\mu^2} \log D^{(k+1)l_0} = l_0 \frac{1}{2^k} (k+1) \log D" /><br />
where here <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta%5E%7Bkl_0%7D%7D%7B%5Cmu%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\Delta^{kl_0}}{\mu^2}" class="latex" title="\frac{\Delta^{kl_0}}{\mu^2}" /> is an upper bound on the total<br />
probability mass in the interval, and <img src="https://s0.wp.com/latex.php?latex=D%5E%7B-%28k%2B1%29l_0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D^{-(k+1)l_0}" class="latex" title="D^{-(k+1)l_0}" /> a lower bound on<br />
the size of any Schmidt coefficient (squared) in the interval. This<br />
follows from the fact that they are organized in descending order and<br />
must sum to 1.</p>
<p>Therefore, the total entropy is<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Baligned%7D+S%28%28i%5E%2A+%2Ci%5E%2A+%2B1%29%29+%26%5Cleq+l_0+%5Clog+D+%2B+%5Csum_%7Bk+%5Cgeq+1%7D+l_0%5Cfrac%7B1%7D%7B2%5Ek%7D%28k%2B1%29+%5Clog+D+%5C%5C+%26%5Cleq+l_0+%5Clog+D+%2B+l_0+%5Clog+D+%5Csum_%7Bk+%5Cgeq+1%7D+%5Cfrac%7B1%7D%7B2%5Ek%7D%28k%2B1%29+%5C%5C+%26%5Cleq+O%281%29+l_0+%5Clog+D+%5C%5C+%26%5Cleq+O%281%29+%5Cfrac%7B%5Clog+%5Cmu%7D%7B%5Clog+%5CDelta%7D%5Clog+D%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle \begin{aligned} S((i^* ,i^* +1)) &amp;\leq l_0 \log D + \sum_{k \geq 1} l_0\frac{1}{2^k}(k+1) \log D \\ &amp;\leq l_0 \log D + l_0 \log D \sum_{k \geq 1} \frac{1}{2^k}(k+1) \\ &amp;\leq O(1) l_0 \log D \\ &amp;\leq O(1) \frac{\log \mu}{\log \Delta}\log D\end{aligned}" class="latex" title="\displaystyle \begin{aligned} S((i^* ,i^* +1)) &amp;\leq l_0 \log D + \sum_{k \geq 1} l_0\frac{1}{2^k}(k+1) \log D \\ &amp;\leq l_0 \log D + l_0 \log D \sum_{k \geq 1} \frac{1}{2^k}(k+1) \\ &amp;\leq O(1) l_0 \log D \\ &amp;\leq O(1) \frac{\log \mu}{\log \Delta}\log D\end{aligned}" /></p>
<p>This bound depends on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> and <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" />. This is sufficient to imply the<br />
Area Law because our AGSP constructions will have constant <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<h1>AGSP Construction</h1>
<p>In this section, our goal is to construct an AGSP with the correct<br />
tradeoff in parameters. We briefly recall the intuition behind the<br />
properties that an AGSP must satisfy. First, the AGSP should leave the<br />
ground state untouched. Second, it shrinks states which are orthogonal<br />
to the fround state. Third, it should not increase the entanglement<br />
across the cut by too much (see the figure below).</p>
<p>A depiction of the third property in the AGSP definition for a 1D<br />
system. The property that <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> is a sum of tensor products of operators<br />
which only act on one “side” of the<br />
system.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/schmidt-rank.png?w=600" alt="schmidt-rank" class="alignnone size-full wp-image-6614" /><br />
We are interested in understanding a construction of an AGSP for three<br />
reasons:</p>
<ol>
<li>As we saw before, we can prove a 1D area law assuming the existence<br />
of an AGSP with good parameters.</li>
<li>AGSPs have been used a building block in an provably polynomial-time<br />
algorithm for finding ground states of gapped 1D local Hamiltonians.</li>
<li>They appear to be a general purpose tool with potential applications<br />
in other contexts.</li>
</ol>
<h2>Details of the construction</h2>
<p>One way to think about the the first two properties in the AGSP<br />
definition is in terms of the relationship of the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> and<br />
the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> (see the figure below). The first property says<br />
that the ground state of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> should be an eigenvector of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> with<br />
eigenvalue 1. The second property says that all other eigenvectors of<br />
<img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> should be mapped to eigenvectors of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> with eigenvalue at most<br />
<img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sqrt{\Delta}" class="latex" title="\sqrt{\Delta}" />.</p>
<p>In this plot, the horizontal axis represents the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /><br />
and the vertical axis represents the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />. The ground<br />
state of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />, which corresponds to the eigenvalue 0, should remain fixed<br />
upon applying <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> to<br />
it.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/spectrum-plot.png?w=600" alt="spectrum-plot" class="alignnone size-full wp-image-6615" /><br />
In the following, we will aim to construct and AGSP whose tradeoff in<br />
parameters allows us to achieve the following result.</p>
<blockquote><p><strong>Theorem:</strong> <img src="https://s0.wp.com/latex.php?latex=S_%7B%7C+%5Cpsi_0+%5Crangle%7D%28A%29+%3D+O%28%5Cfrac%7B%5Clog%5E2+d%7D%7B%5Cepsilon%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_{| \psi_0 \rangle}(A) = O(\frac{\log^2 d}{\epsilon})" class="latex" title="S_{| \psi_0 \rangle}(A) = O(\frac{\log^2 d}{\epsilon})" />.</p></blockquote>
<h3>Attempt 1</h3>
<p>As a first attempt, we will evaluate the quality of the following<br />
operator:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+K+%3D+%281-%5Cfrac%7BH%7D%7B%5C%7C+H+%5C%7C%7D%29%5El%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle K = (1-\frac{H}{\| H \|})^l," class="latex" title="\displaystyle K = (1-\frac{H}{\| H \|})^l," /><br />
where <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> is some natural<br />
number that we will decide upon later. We now check the three properties<br />
for this particular choice of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />.</p>
<p>First, it is clear that <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> does not change the ground state, since the<br />
ground state is an eigenvector of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> with eigenvalue 0<br />
(frustration-free case). Second, because any other eigenvalue of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is<br />
at least <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> (the energy gap), we have that<br />
<img src="https://s0.wp.com/latex.php?latex=%5CDelta+%5Cleq+%281-%5Cfrac%7B%5Cepsilon%7D%7B%5C%7C+H+%5C%7C%7D%29%5El+%5Capprox+e%5E%7B-%5Cfrac%7B%5Cepsilon%7D%7B%5C%7C+H+%5C%7C%7D+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta \leq (1-\frac{\epsilon}{\| H \|})^l \approx e^{-\frac{\epsilon}{\| H \|} l}" class="latex" title="\Delta \leq (1-\frac{\epsilon}{\| H \|})^l \approx e^{-\frac{\epsilon}{\| H \|} l}" />.<br />
Third, for the time being, let us think of <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> as roughly corresponding<br />
to <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> (we will formalize this later).</p>
<p>Unfortunately, these values <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> and <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> will not allow us to<br />
achieve the desired tradeoff of <img src="https://s0.wp.com/latex.php?latex=D+%5CDelta+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta \leq \frac{1}{2}" class="latex" title="D \Delta \leq \frac{1}{2}" />. To see why<br />
this is, observe that because we have <img src="https://s0.wp.com/latex.php?latex=n-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n-1" class="latex" title="n-1" /> local Hamiltonians, the term<br />
<img src="https://s0.wp.com/latex.php?latex=%5C%7C+H+%5C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H \|" class="latex" title="\| H \|" /> could be on the order of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> (recall that<br />
<img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+%5C%7C+H_i+%5C%7C+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0 \leq \| H_i \| \leq 1" class="latex" title="0 \leq \| H_i \| \leq 1" />), so we would need to choose <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> proportional<br />
to <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> to reduce <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> to a constant (see figure below). Unfortunately, this will result in a<br />
corresponding increase in <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" />, violating <img src="https://s0.wp.com/latex.php?latex=D+%5CDelta+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta \leq \frac{1}{2}" class="latex" title="D \Delta \leq \frac{1}{2}" />.</p>
<p>The first attempt an AGSP corresponds to the polynomial is depicted in<br />
blue. This polynomial does not decay at a fast enough rate, so the<br />
second attempt will replace it with another polynomials, depicted in<br />
red, which decays much<br />
faster.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/polynomials.png?w=600" alt="polynomials" class="alignnone size-full wp-image-6613" /></p>
<h3>Attempt 2</h3>
<p>In summary, the two weaknesses of the previous approach we need to<br />
address are:</p>
<ol>
<li>The large <img src="https://s0.wp.com/latex.php?latex=%5C%7C+H+%5C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H \|" class="latex" title="\| H \|" /> term in the exponent substantially reduces the<br />
shinking effect of applying the AGSP. We will address this using the<br />
technique of <em>Hamiltonian truncation.</em></li>
<li>For a fixed <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> (roughly corresponding to the entanglement rank of<br />
<img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" />), the function <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+%281+-+%5Cfrac%7Bx%7D%7B%5C%7C+H+%5C%7C%7D%29%5El&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x) = (1 - \frac{x}{\| H \|})^l" class="latex" title="f(x) = (1 - \frac{x}{\| H \|})^l" /> does not decay<br />
fast enough after the point <img src="https://s0.wp.com/latex.php?latex=x+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x = 0" class="latex" title="x = 0" /> (which is the ground state<br />
energy). We will address this using <em>Chebyshev polynomials</em>.</li>
</ol>
<p>As discussed before, the <img src="https://s0.wp.com/latex.php?latex=%5C%7C+H+%5C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H \|" class="latex" title="\| H \|" /> term is large because it involves<br />
contribution from all <img src="https://s0.wp.com/latex.php?latex=n-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n-1" class="latex" title="n-1" /> local Hamiltonians. Intuitively, we might<br />
believe that only the local Hamiltonians near the site of the cut itself<br />
should play a large role in the entanglement of the ground state across<br />
the cut. Specifically, if we write<br />
<img src="https://s0.wp.com/latex.php?latex=H+%3D+H_1+%2B+%5Cldots+H_%7Bn-1%7D+%3D+H_L+%2B+H_%7Bi_1%7D+%2B+%5Cldots+H_%7Bi_s%7D+%2B+H_R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H = H_1 + \ldots H_{n-1} = H_L + H_{i_1} + \ldots H_{i_s} + H_R" class="latex" title="H = H_1 + \ldots H_{n-1} = H_L + H_{i_1} + \ldots H_{i_s} + H_R" />, where<br />
<img src="https://s0.wp.com/latex.php?latex=H_%7Bi_j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{i_j}" class="latex" title="H_{i_j}" /> are the <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s" class="latex" title="s" /> local Hamiltonians neighboring the cut and <img src="https://s0.wp.com/latex.php?latex=H_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_L" class="latex" title="H_L" /><br />
and <img src="https://s0.wp.com/latex.php?latex=H_R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_R" class="latex" title="H_R" /> are the “aggregrated” Hamiltonians of all the remaining local<br />
Hamiltonians to the left and right, respectively, of <img src="https://s0.wp.com/latex.php?latex=H_%7Bi_1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{i_1}" class="latex" title="H_{i_1}" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=H_%7Bi_s%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{i_s}" class="latex" title="H_{i_s}" />, then we are asserting that <img src="https://s0.wp.com/latex.php?latex=H_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_L" class="latex" title="H_L" /> and <img src="https://s0.wp.com/latex.php?latex=H_R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_R" class="latex" title="H_R" /> contribute<br />
significantly to <img src="https://s0.wp.com/latex.php?latex=%5C%7C+H+%5C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H \|" class="latex" title="\| H \|" /> but are not important when trying to<br />
characterize the ground state (see figure below).</p>
<p>We keep track of the local Hamiltonians corresponding to the <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s" class="latex" title="s" /><br />
qudits directly surrounding the cut. The remaining local Hamiltonians<br />
are aggregated into the Hamiltonians <img src="https://s0.wp.com/latex.php?latex=H_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_L" class="latex" title="H_L" /> and <img src="https://s0.wp.com/latex.php?latex=H_R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_R" class="latex" title="H_R" />, whose eigenvalues<br />
will be truncated to define<br />
<img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/truncation.png?w=600" alt="truncation" class="alignnone size-full wp-image-6616" /><br />
Mathematically, if we let<br />
<img src="https://s0.wp.com/latex.php?latex=H%27+%3D+H%5E%7B%5Cleq+t%7D_L+%2B+H_%7Bi_1%7D+%2B+%5Cldots+H_%7Bi_s%7D+%2B+H%5E%7B%5Cleq+t%7D_R&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H' = H^{\leq t}_L + H_{i_1} + \ldots H_{i_s} + H^{\leq t}_R" class="latex" title="H' = H^{\leq t}_L + H_{i_1} + \ldots H_{i_s} + H^{\leq t}_R" />, where<br />
<img src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cleq+t%7D_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{\leq t}_i" class="latex" title="H^{\leq t}_i" /> denotes operator obtained by truncating all eigenvalues<br />
of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> to be at most <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" />, we claim the following:</p>
<blockquote><p><strong>Claim:</strong><br />
1. <img src="https://s0.wp.com/latex.php?latex=%5C%7C+H%27+%5C%7C+%5Cleq+s+%2B+2t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H' \| \leq s + 2t" class="latex" title="\| H' \| \leq s + 2t" /><br />
2. <img src="https://s0.wp.com/latex.php?latex=%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="| \psi_0 \rangle" class="latex" title="| \psi_0 \rangle" /> is a ground state of <img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" />.<br />
3. <img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" /> has gap <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%27+%3D+%5COmega+%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon' = \Omega (\epsilon)" class="latex" title="\epsilon' = \Omega (\epsilon)" /></p></blockquote>
<p>It is straightforward to verify the two first two claims, We omit the<br />
proof of the third claim. Intuitively, we have chosen <img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" /> to be an<br />
approximation of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> which has much smaller norm.</p>
<p>Next, we come to the task of designing a degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> which<br />
decays faster than our first attempt <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+%281+-+%5Cfrac%7Bx%7D%7B%5C%7C+H+%5C%7C%7D%29%5El&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(x) = (1 - \frac{x}{\| H \|})^l" class="latex" title="f(x) = (1 - \frac{x}{\| H \|})^l" /><br />
after the point <img src="https://s0.wp.com/latex.php?latex=x%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x=0" class="latex" title="x=0" />. In other words, we would like <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> to satisfy<br />
<img src="https://s0.wp.com/latex.php?latex=p%280%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(0) = 1" class="latex" title="p(0) = 1" /> (fixing the ground energy) and map the interval<br />
<img src="https://s0.wp.com/latex.php?latex=%5B%5Cepsilon%2C+%5C%7C+H%27+%5C%7C%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[\epsilon, \| H' \|]" class="latex" title="[\epsilon, \| H' \|]" /> to the interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2C%5CDelta%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[0,\Delta]" class="latex" title="[0,\Delta]" /> for as small a<br />
<img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> as possible. It turns out that the degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> Chebyshev<br />
polynomial (of the first kind) is precisely the polynomial which has<br />
this behavior (see the appendix for a review of Chebyshev polynomials).<br />
Letting <img src="https://s0.wp.com/latex.php?latex=T_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_l" class="latex" title="T_l" /> denote the degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> Chebyshev polynomial, we take our<br />
polynomial to be a scaled and shifted version of it:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p%28x%29+%3D+T_l%5Cleft%28%5Cfrac%7B%5C%7C+H%27+%5C%7C+%2B+%5Cepsilon%27+-+2x%7D%7B%5C%7C+H%27+%5C%7C+-+%5Cepsilon%27%7D%5Cright%29%2FT_l%5Cleft%28%5Cfrac%7B%5C%7C+H%27+%5C%7C+%2B+%5Cepsilon%27%7D%7B%5C%7C+H%27+%5C%7C+-+%5Cepsilon%27%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle p(x) = T_l\left(\frac{\| H' \| + \epsilon' - 2x}{\| H' \| - \epsilon'}\right)/T_l\left(\frac{\| H' \| + \epsilon'}{\| H' \| - \epsilon'}\right)" class="latex" title="\displaystyle p(x) = T_l\left(\frac{\| H' \| + \epsilon' - 2x}{\| H' \| - \epsilon'}\right)/T_l\left(\frac{\| H' \| + \epsilon'}{\| H' \| - \epsilon'}\right)" /><br />
Given this definition of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, we take our new candidate AGSP to be<br />
<img src="https://s0.wp.com/latex.php?latex=K+%3D+p%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K = p(H)" class="latex" title="K = p(H)" />, which has the following guarantees:</p>
<blockquote><p><strong>Claim:</strong><br />
1. <img src="https://s0.wp.com/latex.php?latex=K+%7C+%5Cpsi_0+%5Crangle+%3D+%7C+%5Cpsi_0+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K | \psi_0 \rangle = | \psi_0 \rangle" class="latex" title="K | \psi_0 \rangle = | \psi_0 \rangle" /><br />
2. <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> achieves<br />
<img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3D+O%28%5Cexp%7B-%5Csqrt%7B%5Cfrac%7B%5Cepsilon%27%7D%7B%5C%7C+H%27+%5C%7C%7D%7D+l%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta = O(\exp{-\sqrt{\frac{\epsilon'}{\| H' \|}} l})" class="latex" title="\Delta = O(\exp{-\sqrt{\frac{\epsilon'}{\| H' \|}} l})" /><br />
3. <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> achieves <img src="https://s0.wp.com/latex.php?latex=D+%3D+O%28d%5E%7B%5Cfrac%7Bl%7D%7Bs%7D+%2B+s%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D = O(d^{\frac{l}{s} + s})" class="latex" title="D = O(d^{\frac{l}{s} + s})" /></p></blockquote>
<p>The first claim is straightforward to verify. The proof of the second<br />
claim is ommitted; it involves calculations making use of standard<br />
properties of Chebyshev polynomials. We remark that the dependence of<br />
the exponent <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" /> on <img src="https://s0.wp.com/latex.php?latex=%5C%7C+H%27+%5C%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\| H' \|" class="latex" title="\| H' \|" /> has been reduced by a quadratic<br />
amount, compared with the first attempt at an AGSP. We now sketch the<br />
main ideas behind the proof of the third claim. After that, we will<br />
choose values of <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s" class="latex" title="s" /> and <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> so that the desired tradeoff <img src="https://s0.wp.com/latex.php?latex=D+%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta" class="latex" title="D \Delta" /> is<br />
achieved.</p>
<p>We have defined <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> as a degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> polynomial in <img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" />, so we may write<br />
it as: <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+K+%3D+%5Csum_%7Bj%3D0%7D%5E%7Bl%7D+c_i+%28H%27%29%5Ej%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle K = \sum_{j=0}^{l} c_i (H')^j," class="latex" title="\displaystyle K = \sum_{j=0}^{l} c_i (H')^j," /> where <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_i" class="latex" title="c_i" /> are coefficients<br />
whose particular values are not relevant to the analysis of the Schmidt<br />
rank of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> across the given cut. It is straightforward to check that<br />
the Schmidt rank of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> is bounded by the sum of the ranks of the<br />
monomials <img src="https://s0.wp.com/latex.php?latex=%28H%27%29%5Ej&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H')^j" class="latex" title="(H')^j" /> (subadditivity). So, it suffices to analyze the<br />
Schmidt rank of the monomial of the largest degree, <img src="https://s0.wp.com/latex.php?latex=%28H%27%29%5El&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(H')^l" class="latex" title="(H')^l" />.</p>
<p>From our expression for <img src="https://s0.wp.com/latex.php?latex=H%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H'" class="latex" title="H'" />, we can expand this power to get:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28H%27%29%5El+%3D+%28H%5E%7B%5Cleq+t%7D_L+%2B+H_%7Bj_1%7D+%2B+%5Cldots+H_%7Bj_s%7D+%2B+H%5E%7B%5Cleq+t%7D_R%29%5El+%3D+%5Csum_%7Bj_1%2C%5Cldots%2Cj_l%7D+H_%7Bj_1%7D+%5Ccdots+H_%7Bj_l%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle (H')^l = (H^{\leq t}_L + H_{j_1} + \ldots H_{j_s} + H^{\leq t}_R)^l = \sum_{j_1,\ldots,j_l} H_{j_1} \cdots H_{j_l}," class="latex" title="\displaystyle (H')^l = (H^{\leq t}_L + H_{j_1} + \ldots H_{j_s} + H^{\leq t}_R)^l = \sum_{j_1,\ldots,j_l} H_{j_1} \cdots H_{j_l}," /><br />
where the summation is over all products of <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> Hamiltonians from the<br />
set <img src="https://s0.wp.com/latex.php?latex=%5C%7BH%5E%7B%5Cleq+t%7D_L%2C+H_%7Bi_1%7D%2C+%5Cldots+H_%7Bi_s%7D%2C+H%5E%7B%5Cleq+t%7D_R%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{H^{\leq t}_L, H_{i_1}, \ldots H_{i_s}, H^{\leq t}_R\}" class="latex" title="\{H^{\leq t}_L, H_{i_1}, \ldots H_{i_s}, H^{\leq t}_R\}" />. Now, one<br />
natural approach to proceed is to analyze the Schmidt rank of each term<br />
in the summation, and then sum over all the terms. Unfortunately, this<br />
approach will not work because there are exponentially many terms in the<br />
summation, so our estimate of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D" class="latex" title="D" /> will be too large. To address this<br />
issue, it is possible to collect terms cleverly so that the total number<br />
of terms is small. However, we will not provide the details here; we<br />
will just sketch how analyze the Schmidt rank of an individual term.</p>
<p>The term <img src="https://s0.wp.com/latex.php?latex=H_%7Bj_1%7D+%5Ccdots+H_%7Bj_l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{j_1} \cdots H_{j_l}" class="latex" title="H_{j_1} \cdots H_{j_l}" /> consists of a product of <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /><br />
Hamiltonians. Since there are <img src="https://s0.wp.com/latex.php?latex=s%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s+1" class="latex" title="s+1" /> possible locations of a cut that is<br />
crossed by one of the <img src="https://s0.wp.com/latex.php?latex=H_%7Bj_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{j_k}" class="latex" title="H_{j_k}" />, an average cut will have<br />
<img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bl%7D%7Bs%2B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{l}{s+1}" class="latex" title="\frac{l}{s+1}" /> of the <img src="https://s0.wp.com/latex.php?latex=H_%7Bj_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{j_k}" class="latex" title="H_{j_k}" /> crossing it. For each Hamiltonian which<br />
crosses this average cut, the Schmidt rank will be multiplied by a<br />
factor of <img src="https://s0.wp.com/latex.php?latex=d%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d^2" class="latex" title="d^2" />. Hence, the Schmidt rank of the term<br />
<img src="https://s0.wp.com/latex.php?latex=H_%7Bj_1%7D+%5Ccdots+H_%7Bj_l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_{j_1} \cdots H_{j_l}" class="latex" title="H_{j_1} \cdots H_{j_l}" /> is <img src="https://s0.wp.com/latex.php?latex=O%28d%5E%5Cfrac%7B2l%7D%7Bs%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(d^\frac{2l}{s})" class="latex" title="O(d^\frac{2l}{s})" />. This bound on the<br />
Schmidt rank is for an average cut <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" />, not necessarily the cut <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> we<br />
fixed in the beginning. The two cuts are separated by at most <img src="https://s0.wp.com/latex.php?latex=O%28s%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(s)" class="latex" title="O(s)" /><br />
qudits because <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" /> is one of the <img src="https://s0.wp.com/latex.php?latex=s%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s+1" class="latex" title="s+1" /> cuts surrouding <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> (see figure<br />
below), so one can relate the Schmidt rank across these two cuts. For<br />
each Hamiltonian between the two cuts, the high-level idea is that the<br />
entanglement rank is multiplied by at most a factor of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />. Because the<br />
entanglement rank across <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" /> is <img src="https://s0.wp.com/latex.php?latex=O%28d%5E%5Cfrac%7B2l%7D%7Bs%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(d^\frac{2l}{s})" class="latex" title="O(d^\frac{2l}{s})" />, the entanglement<br />
rank across <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> is <img src="https://s0.wp.com/latex.php?latex=O%28d%5E%7B%5Cfrac%7B2l%7D%7Bs%7D+%2B+s%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(d^{\frac{2l}{s} + s})" class="latex" title="O(d^{\frac{2l}{s} + s})" />.</p>
<p>The entanglement across cut <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C" class="latex" title="C" /> is analyzed bounding the entanglement<br />
across a nearby cut <img src="https://s0.wp.com/latex.php?latex=C%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C'" class="latex" title="C'" /> and then relating the<br />
two.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/cut-to-avg-cut-D.png?w=600" alt="cut-to-avg-cut-D" class="alignnone size-full wp-image-6612" /><br />
Putting all the pieces together, we have that:</p>
<ol>
<li>By Hamiltonian truncation and the Chebyshev polynomial construction,<br />
<img src="https://s0.wp.com/latex.php?latex=%5CDelta+%3D+O%28e%5E%7B-l%2F+%5Csqrt%7Bs%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta = O(e^{-l/ \sqrt{s}})" class="latex" title="\Delta = O(e^{-l/ \sqrt{s}})" />.</li>
<li>By the entanglement rank analysis, <img src="https://s0.wp.com/latex.php?latex=D+%3D+O%28d%5E%7B%5Cfrac%7Bl%7D%7Bs%7D+%2B+s%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D = O(d^{\frac{l}{s} + s})" class="latex" title="D = O(d^{\frac{l}{s} + s})" />.</li>
</ol>
<p>One can now check that if we set <img src="https://s0.wp.com/latex.php?latex=l+%3D+O%28s%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l = O(s^2)" class="latex" title="l = O(s^2)" /> and<br />
<img src="https://s0.wp.com/latex.php?latex=s+%3D+O%28%5Cfrac%7B%5Clog%5E2+d%7D%7B%5Cepsilon%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s = O(\frac{\log^2 d}{\epsilon})" class="latex" title="s = O(\frac{\log^2 d}{\epsilon})" /> (where large enough constants are<br />
hidden inside the <img src="https://s0.wp.com/latex.php?latex=O%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(\cdot)" class="latex" title="O(\cdot)" />), then we achieve the desired tradeoff of<br />
<img src="https://s0.wp.com/latex.php?latex=D+%5CDelta+%5Cleq+%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D \Delta \leq \frac{1}{2}" class="latex" title="D \Delta \leq \frac{1}{2}" />. Note that <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> and <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="s" class="latex" title="s" /> are constants because<br />
<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> are constants.</p>
<h2>Towards an area law in high dimensions</h2>
<p>One of the major open problems in this line of work is to prove an area<br />
law for 2D systems. We now discuss one potential line of attack on this<br />
problem, based on a reduction to the 1D area law we have proven using<br />
AGSPs.</p>
<p>Consider a 2D system, as depicted in the figure below. We can form a<br />
sequence of concentric shells of qudits of increasing radii and<br />
“contract” each shell into a single qudit of higher dimension. That is,<br />
if each of the original qudits have dimension <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> and we consider the<br />
shell at radius <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" />, the dimension of the contracted qudit is roughly<br />
<img src="https://s0.wp.com/latex.php?latex=d%5Er&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d^r" class="latex" title="d^r" /> because there are roughly <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> qudits in the shell. The system of<br />
contracted qudits is now a 1D system in the sense that the shell at<br />
radius <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r" class="latex" title="r" /> only interacts with the shells at radii <img src="https://s0.wp.com/latex.php?latex=r-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r-1" class="latex" title="r-1" /> and <img src="https://s0.wp.com/latex.php?latex=r%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r+1" class="latex" title="r+1" />.</p>
<p>A 2D system can be reduced to a 1D system by decomposing it into a<br />
sequence of concentric<br />
“shells”.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/two-dim-area-law.png?w=600" alt="two-dim-area-law" class="alignnone size-full wp-image-6617" /><br />
Suppose that we were able to show an area law as discussed previously,<br />
but with the following dependence on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_%7B%7C+%5Cpsi_0+%5Crangle%7D%28A%29+%3D+O%28%5Cfrac%7B%5Clog+d%7D%7B%5Cepsilon%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S_{| \psi_0 \rangle}(A) = O(\frac{\log d}{\epsilon})." class="latex" title="\displaystyle S_{| \psi_0 \rangle}(A) = O(\frac{\log d}{\epsilon})." /><br />
If we replace<br />
<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> with <img src="https://s0.wp.com/latex.php?latex=d%5Er&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d^r" class="latex" title="d^r" />, the dimensionality in our new 1D system, we get that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S_%7B%7C+%5Cpsi_0+%5Crangle%7D%28A%29+%3D+O%28%5Cfrac%7Br%7D%7B%5Cepsilon%7D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle S_{| \psi_0 \rangle}(A) = O(\frac{r}{\epsilon})," class="latex" title="\displaystyle S_{| \psi_0 \rangle}(A) = O(\frac{r}{\epsilon})," /><br />
which is exactly an<br />
area law in 2D (recalling that surface area of a circle is proportional<br />
to its radius). In fact, even the weaker result of proving that<br />
<img src="https://s0.wp.com/latex.php?latex=S_%7B%7C+%5Cpsi_0+%5Crangle%7D%28A%29+%3D+O%28%5Cfrac%7B%5Clog%5E%7B2-%5Calpha%7D+d%7D%7B%5Cepsilon%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="S_{| \psi_0 \rangle}(A) = O(\frac{\log^{2-\alpha} d}{\epsilon})" class="latex" title="S_{| \psi_0 \rangle}(A) = O(\frac{\log^{2-\alpha} d}{\epsilon})" /> for<br />
some <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha &gt; 0" class="latex" title="\alpha &gt; 0" /> would imply a “sub-volume law”, already a major<br />
breakthrough. This suggests that constructing AGSPs with better<br />
parameters may be a plausible approach to proving an area law.</p>
<h1>Future Directions</h1>
<p>In addition to proving an area law in 2D, there are several interesting<br />
questions raised by what we discussed here. See the survey of Gharibian<br />
et al. for more details.</p>
<ol>
<li>Do area laws hold for gapless (i.e. non-constant gap, or “critical”)<br />
systems? No, there are known counterexamples.</li>
<li>Do area laws hold for degenerate systems (where the ground state is<br />
not unique)? Yes, the AGSP approach still works for systems with<br />
groundspaces of constant dimension. It is unknown whether area laws<br />
hold when the groundspace is of dimension polynomial in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />.</li>
<li>Can one design practical algorithms for finding ground states of<br />
gapped 1D local Hamiltonians? Yes, there is one approach based on<br />
AGSPs.</li>
<li>Do area laws hold in general for systems where the interactions are<br />
not necessarily described by a lattice? No, there are known<br />
counterexamples to such a “generalized area law”?</li>
</ol>
<h1>Acknowledgements</h1>
<p>We would like to thank Boaz Barak and Tselil Schramm for their guidance<br />
in preparing the talk on which this post is based.</p>
<p>Most of this post (including some figures) draws heavily upon the<br />
following resources: lecture notes from courses taught by Thomas Vidick and Umesh Vazirani,<br />
repectively, and a survey by Gharibian et al.<br />
In these notes, we have opted to illustrate the main ideas, while<br />
glossing over technical details. Our goal with this approach is to<br />
prepare the reader to more easily understand the original sources and<br />
highlight the some of the key concepts.</p>
<h1>Appendix: Background on Chebyshev Polynomials</h1>
<p>The Chebsyhev polynomials <img src="https://s0.wp.com/latex.php?latex=%5C%7BT_l%5C%7D_%7Bl+%5Cin+%5Cmathbb%7BN%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{T_l\}_{l \in \mathbb{N}}" class="latex" title="\{T_l\}_{l \in \mathbb{N}}" /> are a family of<br />
polynomials with many special properties. They are defined as<br />
<img src="https://s0.wp.com/latex.php?latex=T_0%28x%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_0(x) = 1" class="latex" title="T_0(x) = 1" />, <img src="https://s0.wp.com/latex.php?latex=T_1%28x%29+%3D+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_1(x) = x" class="latex" title="T_1(x) = x" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+T_l%28x%29+%3D+2xT_%7Bl-1%7D%28x%29+-+T_%7Bl-2%7D%28x%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\displaystyle T_l(x) = 2xT_{l-1}(x) - T_{l-2}(x)." class="latex" title="\displaystyle T_l(x) = 2xT_{l-1}(x) - T_{l-2}(x)." /><br />
The first few polynomials are plotted below (Source:<br />
&lt;<a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials&amp;gt" rel="nofollow">https://en.wikipedia.org/wiki/Chebyshev_polynomials&amp;gt</a>;).</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/chebyshevs.png?w=600" alt="chebyshevs" class="alignnone size-full wp-image-6611" /><br />
Below, we state a property (proof ommitted) which provides some<br />
intuition as to why Chebyshev polynomials are the “correct” choice for<br />
our construction. Intuitively, it says that among all degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /><br />
polynomials which are trapped inside <img src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[-1,1]" class="latex" title="[-1,1]" /> on the interval <img src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[-1,1]" class="latex" title="[-1,1]" />,<br />
<img src="https://s0.wp.com/latex.php?latex=T_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T_l" class="latex" title="T_l" /> dominates <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> outside of <img src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="[-1,1]" class="latex" title="[-1,1]" />.</p>
<blockquote><p><strong>Claim:</strong> Let <img src="https://s0.wp.com/latex.php?latex=p%3A+%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p: \mathbb{R} \rightarrow \mathbb{R}" class="latex" title="p: \mathbb{R} \rightarrow \mathbb{R}" /> be a degree-<img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> polynomial<br />
such that <img src="https://s0.wp.com/latex.php?latex=%7Cp%28x%29%7C+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|p(x)| \leq 1" class="latex" title="|p(x)| \leq 1" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in [-1,1]" class="latex" title="x \in [-1,1]" />. Then for all<br />
<img src="https://s0.wp.com/latex.php?latex=%7Cx%7C+%5Cgeq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|x| \geq 1" class="latex" title="|x| \geq 1" />, <img src="https://s0.wp.com/latex.php?latex=%7Cp%28x%29%7C+%5Cleq+%7CT_l%28x%29%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|p(x)| \leq |T_l(x)|" class="latex" title="|p(x)| \leq |T_l(x)|" />.</p></blockquote>
<h1>References</h1>
<ol>
<li>Sevag Gharibian, Yichen Huang, Zeph Landau, Seung Woo Shin, et al. “Quantum Hamiltonian Complexity”. Foundations and Trends in Theoretical Computer Science, 10(3):159–282, 2015.</li>
<li>Thomas Vidick. Lecture notes for CS286 seminar in computer science: “Around the quantum PCP conjecture”. <a href="http://users.cms.caltech.edu/~vidick/teaching/286_qPCP/index.html" rel="nofollow">http://users.cms.caltech.edu/~vidick/teaching/286_qPCP/index.html</a>, Fall 2014.</li>
<li>Umesh Vazirani. Lecture notes for CS294-4 “Quantum computation”. <a href="https://people.eecs.berkeley.edu/~vazirani/f16quantum.html" rel="nofollow">https://people.eecs.berkeley.edu/~vazirani/f16quantum.html</a>, Fall 2016.</li>
</ol></div>







<p class="date">
by prayaagvenkat <a href="https://windowsontheory.org/2018/12/18/a-1d-area-law-for-gapped-local-hamiltonians/"><span class="datestr">at December 18, 2018 06:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/18/assistant-professor-mathematics-mscs-department-at-university-of-illinois-at-chicago-uic-apply-by-january-14-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/18/assistant-professor-mathematics-mscs-department-at-university-of-illinois-at-chicago-uic-apply-by-january-14-2019/">Assistant Professor, Mathematics (MSCS) Department at University of Illinois at Chicago (UIC) (apply by January 14, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The department of Mathematics (MSCS) is looking for an Assistant Professor to join the Mathematical Computer Science group. Theoretical computer science is a target area, and all TCS subfields will be considered. (Note that at UIC, cs theory research is split between CS and Math.)</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/jobs/13215">https://www.mathjobs.org/jobs/jobs/13215</a><br />
Email: lreyzin@uic.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/18/assistant-professor-mathematics-mscs-department-at-university-of-illinois-at-chicago-uic-apply-by-january-14-2019/"><span class="datestr">at December 18, 2018 03:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6798965022412098128">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2018/12/two-recent-achievements-by-csgssi.html">Two recent achievements by CS@GSSI researchers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><div><div>This week has seen two more achievements by some of my colleagues in the CS group at the GSSI.  </div><div> </div><div>Omar Inverso, postdoctoral researcher in the Computer Science group at the GSSI, has won a Silver Medal at the <a style="color: #1155cc;" href="https://sv-comp.sosy-lab.org/2019/" target="_blank">2019 8th International Competition on Software Verification (SV-COMP 2019)</a>. Quoting from the paper at <a style="color: #1155cc;" href="https://www.sosy-lab.org/research/pub/2017-TACAS.Software_Verification_with_Validation_of_Results.pdf" target="_blank">https://www.sosy-lab.org/research/pub/2017-TACAS.Software_Verification_with_Validation_of_Results.pdf</a>, </div><div><br /></div>"Software verification is an increasingly important research area, and the annual Competition on Software Verification (SV-COMP) is the showcase of the state of the art in the area, in particular, of the effectiveness and efficiency that is currently achieved by tool implementations of the most recent ideas, concepts, and algorithms for fully-automatic verification."<span style="color: #500050;" class="m_5740790743348516540gmail-im"><div><br /></div><div>As can be seen from the results of the competition at </div><div> </div><div><a style="color: #1155cc;" href="https://sv-comp.sosy-lab.org/2019/results/results-verified/" target="_blank">https://sv-comp.sosy-lab.org/2019/results/results-verified/</a>, </div><div><br /></div></span><div>Omar Inverso's <a style="color: #1155cc;" href="https://github.com/omainv/cseq" target="_blank">Lazy-CSeq</a>, a software tool for the automated analysis of complex concurrent programs, came second in the category ConcurrencySafety and was beaten only by a tool developed by a Chinese team from Tsinghua University, which is widely considered the best technical university in China. This is a remarkable achievement and continues Omar Inverso's success in that competition, where he won Gold and Silver medals in 2016, and Silver and Bronze medals in 2017, in the Concurrency category. It is also worth noting that Omar Inverso is developing <a style="color: #1155cc;" href="https://github.com/omainv/cseq" target="_blank">Lazy-CSeq</a> and related software-analysis tools alone, whereas competing tools are largely the result of a team effort. </div></div><div></div><div><br /></div><div><div dir="ltr">Third-year Computer Science students <span style="color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"> </span><a style="color: #3778cd; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;" href="https://sites.google.com/view/emiliocruciani/" target="_blank">Emilio Cruciani</a> and  <a style="color: #4d469c; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;" href="https://robertoverdecchia.github.io/" target="_blank">Roberto Verdecchia</a> have done it again! They have followed up on their ICSE 2018 paper with  <span style="color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"></span><a style="color: #4d469c; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;" href="http://cin.ufpe.br/~bafm" target="_blank">Breno Miranda</a><span style="color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;">(UFPE, Brazil) </span><span style="color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;">and </span><a style="color: #4d469c; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;" href="http://bertolino.isti.cnr.it/" target="_blank">Antonia Bertolino</a><span style="color: #444444; font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px;"> (ISTI - CNR, Italy)  </span>(see <a style="color: #1155cc;" href="http://processalgebra.blogspot.com/2017/12/first-year-computer-science-students-at.html" target="_blank">http://processalgebra.blogspot.com/2017/12/first-year-computer-science-students-at.html</a> for a few words on that achievement) with another paper, entitled "Scalable Approaches for Test Suite Reduction ", that has been accepted to the ICSE 2019 Technical Track (<a style="color: #1155cc;" href="https://2019.icse-conferences.org/track/icse-2019-Technical-Papers#event-overview" target="_blank">https://2019.icse-conferences.org/track/icse-2019-Technical-Papers#event-overview</a>). To put this achievement into perspective, ICSE is the premiere conference in software engineering. ICSE 2019 had 529 submissions, out of which  109 papers were accepted (with an acceptance rate of 21%).</div><div dir="ltr"><br /></div><div>The ICSE 2019 paper present an approach to test-suite reduction, which aims at decreasing software-regression-testing costs by selecting a representative subset from large-size test suites. It presents a family of novel, very efficient approaches for similarity-based test suite reduction that apply algorithms borrowed from the big-data domain together with smart heuristics for finding an evenly spread subset of test cases. The results of the experimental evaluation show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency.</div><div> </div><div>Congratulations to Emilio, Omar and Roberto! </div></div></div><div class="commentbar"><p></p><span href="http://processalgebra.blogspot.com/feeds/6798965022412098128/comments/default" class="commentbutton"></span><a href="http://processalgebra.blogspot.com/feeds/6798965022412098128/comments/default"><img src="/images/feed-icon.png" class="commenticon" /> Subscribe to comments</a>  | <a href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6798965022412098128"><img src="/images/post-icon.png" class="commenticon" /> Post a comment</a></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2018/12/two-recent-achievements-by-csgssi.html"><span class="datestr">at December 18, 2018 12:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/17/faculty-associate-or-full-professor-at-university-of-california-san-diego-apply-by-january-17-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/17/faculty-associate-or-full-professor-at-university-of-california-san-diego-apply-by-january-17-2019/">faculty (associate or full professor) at University of California, San Diego (apply by January 17, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/apply/JPF01994">https://apol-recruit.ucsd.edu/apply/JPF01994</a><br />
Email: slovett@ucsd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/17/faculty-associate-or-full-professor-at-university-of-california-san-diego-apply-by-january-17-2019/"><span class="datestr">at December 17, 2018 07:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/17/faculty-assistant-professor-at-university-of-california-san-diego-apply-by-january-17-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/17/faculty-assistant-professor-at-university-of-california-san-diego-apply-by-january-17-2019/">faculty (assistant professor) at University of California, San Diego (apply by January 17, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/apply/JPF01984">https://apol-recruit.ucsd.edu/apply/JPF01984</a><br />
Email: slovett@ucsd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/17/faculty-assistant-professor-at-university-of-california-san-diego-apply-by-january-17-2019/"><span class="datestr">at December 17, 2018 07:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4021">Why are amplitudes complex?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>[By prior agreement, this post will be cross-posted on <a href="https://blogs.msdn.microsoft.com/visualstudio/2018/11/15/q-advent-calendar-2018/">Microsoft’s Q# blog</a>, even though it has nothing to do with the Q# programming language.  It does, however, contain many examples that might be fun to implement in Q#!]</p>



<p>Why should Nature have been quantum-mechanical?  It’s totally unclear what would count as an answer to such a question, and also totally clear that people will never stop asking it.</p>



<p>Short of an ultimate answer, we can at least try to explain why, if you want this or that <em>piece</em> of quantum mechanics, then the rest of the structure is inevitable: why quantum mechanics is an <a href="https://www.scottaaronson.com/papers/island.pdf">“island in theoryspace,”</a> as I put it in 2003.</p>



<p>In this post, I’d like to focus on a question that any “explanation” for QM at some point needs to address, in a non-question-begging way: <strong>why should amplitudes have been complex numbers?  </strong>When I was a grad student, it was his relentless focus on that question, and on others in its vicinity, that made me a lifelong fan of Chris Fuchs (see for example his <a href="https://arxiv.org/abs/quant-ph/0105039">samizdat</a>), despite my philosophical differences with him.</p>



<p>It’s not that complex numbers are a <em>bad</em> choice for the foundation of the deepest known description of the physical universe—far from it!  (They’re a field, they’re algebraically closed, they’ve got a norm, how much more could you want?)  It’s just that they seem like a <em>specific</em> choice, and not the only possible one.  There are also the real numbers, for starters, and in the other direction, the <a href="https://en.wikipedia.org/wiki/Quaternion">quaternions</a>.</p>



<p>Quantum mechanics over the reals or the quaternions still has constructive and destructive interference among amplitudes, and unitary transformations, and probabilities that are absolute squares of amplitudes.  Moreover, these variants turn out to lead to precisely the same power for quantum computers—namely, the class <a href="https://en.wikipedia.org/wiki/BQP">BQP</a>—as “standard” quantum mechanics, the one over the complex numbers.  So none of those are relevant differences.</p>



<p>Indeed, having just finished teaching an undergrad <a href="https://www.scottaaronson.com/blog/?p=3943">Intro to Quantum Information</a> course, I can attest that the complex nature of amplitudes is needed only rarely—shockingly rarely, one might say—in quantum computing and information.  Real amplitudes typically suffice.  <a href="https://en.wikipedia.org/wiki/Quantum_teleportation">Teleportation</a>, <a href="https://en.wikipedia.org/wiki/Superdense_coding">superdense coding</a>, the <a href="https://en.wikipedia.org/wiki/Bell%27s_theorem">Bell inequality</a>, <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>, <a href="https://en.wikipedia.org/wiki/Quantum_key_distribution">quantum key distribution</a>, the <a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm">Deutsch-Jozsa</a> and <a href="https://www.scottaaronson.com/qclec/18.pdf">Bernstein-Vazirani</a> and <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon</a> and <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">Grover</a> algorithms, <a href="https://en.wikipedia.org/wiki/Quantum_error_correction">quantum error-correction</a>: all of those and more can be fully explained without using a single <em>i</em> that’s not a summation index.  (<a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s factoring algorithm</a> is an exception; it’s much more natural with complex amplitudes.  But as the previous paragraph implied, their use is removable even there.)</p>



<p>It’s true that, if you look at even the simplest “real” examples of quantum systems—or as a software engineer might put it, at the application layers built on top of the quantum OS—then complex numbers are everywhere, in a way that seems impossible to remove.  The <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation">Schrödinger equation</a>, <a href="https://en.wikipedia.org/wiki/Stationary_state">energy eigenstates</a>, the <a href="https://en.wikipedia.org/wiki/Canonical_commutation_relation">position/momentum commutation relation</a>, the state space of a <a href="https://en.wikipedia.org/wiki/Spin-%C2%BD">spin-1/2 particle</a> in 3-dimensional space: none of these make much sense without complex numbers (though it can be fun to try).</p>



<p>But from a sufficiently Olympian remove, it feels circular to use any of this as a “reason” for why quantum mechanics should’ve involved complex amplitudes in the first place.  It’s like, once your OS provides a certain core functionality (in this case, complex numbers), it’d be surprising if the application layer <em>didn’t</em> exploit that functionality to the hilt—especially if we’re talking about fundamental physics, where we’d like to imagine that nothing is wasted or superfluous (hence Rabi’s famous question about the muon: “who ordered that?”).</p>



<p>But why should the quantum OS have provided complex-number functionality at all?  Is it possible to answer that question purely in terms of the OS’s internal logic (i.e., abstract quantum information), making minimal reference to how the OS will eventually get used?  Maybe not—but if so, then that itself would seem worthwhile to know.</p>



<p>If we stick to abstract quantum information language, then the most “obvious, elementary” argument for why amplitudes should be complex numbers is one that I spelled out in <em><a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a></em>, as well as my <a href="https://www.scottaaronson.com/papers/island.pdf">Is quantum mechanics an island in theoryspace?</a> paper.  Namely, it seems desirable to be able to implement a “fraction” of any unitary operation U: for example, some V such that V<sup>2</sup>=U, or V<sup>3</sup>=U.  With complex numbers, this is trivial: we can simply diagonalize U, or use the Hamiltonian picture (i.e., take e<sup>-iH/2</sup> where U=e<sup>-iH</sup>), both of which ultimately depend on the complex numbers being algebraically closed.  Over the reals, by contrast, a 2×2 orthogonal matrix like $$ U = \left(\begin{array}[c]{cc}1 &amp; 0\\0 &amp; -1\end{array}\right)$$</p>



<p>has no 2×2 orthogonal square root, as follows immediately from its determinant being -1.  If we want a square root of U (or rather, of something that acts like U on a subspace) while sticking to real numbers only, then we need to add another dimension, like so: $$ \left(\begin{array}[c]{ccc}1 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 0\\0 &amp; 0&amp;-1\end{array}\right)=\left(\begin{array}[c]{ccc}1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1\\0 &amp; -1 &amp; 0\end{array}\right)  ^{2} $$</p>



<p>This is directly related to the fact that there’s no way for a Flatlander to “reflect herself” (i.e., switch her left and right sides while leaving everything else unchanged) by any continuous motion, unless she can lift off the plane and rotate herself through the third dimension.  Similarly, for <em>us</em> to reflect ourselves would require rotating through a fourth dimension.</p>



<p>One could reasonably ask: is that it?  Aren’t there any “deeper” reasons in quantum information for why amplitudes should be complex numbers?</p>



<p>Indeed, there are certain phenomena in quantum information that, slightly mysteriously, work out more elegantly if amplitudes are complex than if they’re real.  (By “mysteriously,” I mean not that these phenomena can’t be 100% verified by explicit calculations, but simply that I don’t know of any deep principle by which the results of those calculations could’ve been predicted in advance.)</p>



<p>One famous example of such a phenomenon is due to Bill Wootters: if you take a uniformly random pure state in d dimensions, and then you measure it in an orthonormal basis, what will the probability distribution (p<sub>1</sub>,…,p<sub>d</sub>) over the d possible measurement outcomes look like?  The answer, amazingly, is that you’ll get a <em>uniformly random probability distribution</em>: that is, a uniformly random point on the simplex defined by p<sub>i</sub>≥0 and p<sub>1</sub>+…+p<sub>d</sub>=1.  This fact, which I’ve used in several papers, is closely related to <a href="http://mathworld.wolfram.com/ArchimedesHat-BoxTheorem.html">Archimedes’ Hat-Box Theorem</a>, beloved by friend-of-the-blog Greg Kuperberg.  But here’s the kicker: it only works if amplitudes are complex numbers.  If amplitudes are real, then the resulting distribution over distributions will be too bunched up near the corners of the probability simplex; if they’re quaternions, it will be too bunched up near the middle.</p>



<p>There’s an even more famous example of such a Goldilocks coincidence—one that’s been elevated, over the past two decades, to exalted titles like “the Axiom of Local Tomography.”  Namely: suppose we have an unknown finite-dimensional mixed state ρ, shared by two players Alice and Bob.  For example, ρ might be an <a href="https://en.wikipedia.org/wiki/Bell_state">EPR pair</a>, or a correlated classical bit, or simply two qubits both in the state |0⟩.  We imagine that Alice and Bob share many identical copies of ρ, so that they can learn more and more about it by measuring this copy in this basis, that copy in that basis, and so on.</p>



<p>We then ask: can ρ be fully determined from the joint statistics of <em>product measurements</em>—that is, measurements that Alice and Bob can apply separately and locally to their respective subsystems, with no communication between them needed?  A good example here would be the set of measurements that arise in a <a href="https://en.wikipedia.org/wiki/Bell_test_experiments">Bell experiment</a>—measurements that, despite being local, certify that Alice and Bob must share an entangled state.</p>



<p>If we asked the analogous question for classical probability distributions, the answer is clearly “yes.”  That is, once you’ve specified the individual marginals, and you’ve <em>also</em> specified all the possible correlations among the players, you’ve fixed your distribution; there’s nothing further to specify.</p>



<p>For quantum mixed states, the answer again turns out to be yes, <em>but only because amplitudes are complex numbers!</em>  In quantum mechanics over the reals, you could have a 2-qubit state like $$ \rho=\frac{1}{4}\left(\begin{array}[c]{cccc}1 &amp; 0 &amp; 0 &amp; -1\\0 &amp; 1 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1 &amp; 0\\-1&amp; 0 &amp; 0 &amp; 1\end{array}\right) ,$$</p>



<p>which clearly isn’t the maximally mixed state, yet which is indistinguishable from the maximally mixed state by any local measurement that can be specified using real numbers only.  (Proof: exercise!)</p>



<p>In quantum mechanics over the quaternions, something even “worse” happens: namely, the tensor product of two <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a> matrices need not be Hermitian.  Alice’s measurement results might be described by the 2×2 quaternionic density matrix $$ \rho_{A}=\frac{1}{2}\left(\begin{array}[c]{cc}1 &amp; -i\\i &amp; 1\end{array}\right), $$</p>



<p>and Bob’s results might be described by the 2×2 quaternionic density matrix $$ \rho_{B}=\frac{1}{2}\left(\begin{array}[c]{cc}1 &amp; -j\\j &amp; 1\end{array}\right), $$</p>



<p>and yet there might not be (and in this case, isn’t) any 4×4 quaternionic density matrix corresponding to ρ<sub>A</sub>⊗ρ<sub>B</sub>, which would explain both results separately.</p>



<p>What’s going on here?  Why do the local measurement statistics <em>underdetermine</em> the global quantum state with real amplitudes, and <em>overdetermine</em> it with quaternionic amplitudes, being in one-to-one correspondence with it only when amplitudes are complex?</p>



<p>We can get some insight by looking at the number of independent real parameters needed to specify a d-dimensional Hermitian matrix.  Over the complex numbers, the number is exactly d<sup>2</sup>: we need 1 parameter for each of the d diagonal entries, and 2 (a real part and an imaginary part) for each of the d(d-1)/2 upper off-diagonal entries (the lower off-diagonal entries being determined by the upper ones).  Over the real numbers, by contrast, “Hermitian matrices” are just real symmetric matrices, so the number of independent real parameters is only d(d+1)/2.  And over the quaternions, the number is d+4[d(d-1)/2] = 2d(d-1).</p>



<p>Now, it turns out that the Goldilocks phenomenon that we saw above—with local measurement statistics determining a unique global quantum state when and only when amplitudes are complex numbers—ultimately boils down to the simple fact that $$ (d_A d_B)^2 = d_A^2 d_B^2, $$</p>



<p>but $$\frac{d_A d_B (d_A d_B + 1)}{2} &gt; \frac{d_A (d_A + 1)}{2} \cdot \frac{d_B (d_B + 1)}{2},$$</p>



<p>and conversely $$ 2 d_A d_B (d_A d_B – 1) &lt; 2 d_A (d_A – 1) \cdot 2 d_B (d_B – 1).$$</p>



<p>In other words, only with complex numbers does the number of real parameters needed to specify a “global” Hermitian operator, exactly match the product of the number of parameters needed to specify an operator on Alice’s subsystem, and the number of parameters needed to specify an operator on Bob’s.  With real numbers it overcounts, and with quaternions it undercounts.</p>



<p>A major research goal in quantum foundations, since at least the early 2000s, has been to “derive” the formalism of QM purely from “intuitive-sounding, information-theoretic” postulates—analogous to how, in 1905, some guy whose name I forget derived the otherwise strange-looking Lorentz transformations purely from the assumption that the laws of physics (including a fixed, finite value for the speed of light) take the same form in every inertial frame.  There have been some nontrivial successes of this program: most notably, the “axiomatic derivations” of QM due to <a href="https://arxiv.org/abs/quant-ph/0101012">Lucien Hardy</a> and (more recently) <a href="https://arxiv.org/abs/1011.6451">Chiribella et al.</a>  Starting from axioms that sound suitably general and nontechnical (if sometimes unmotivated and weird), these derivations perform the impressive magic trick of <em>deriving</em> the full mathematical structure of QM: complex amplitudes, unitary transformations, tensor products, the Born rule, everything.</p>



<p>However, in every such derivation that I know of, some axiom needs to get introduced to capture “local tomography”: i.e., the “principle” that composite systems must be uniquely determined by the statistics of local measurements.  And while this principle might sound vague and unobjectionable, to those in the business, it’s obvious what it’s going to be used for the second it’s introduced.  Namely, it’s going to be used to rule out quantum mechanics over the real numbers, which would otherwise be a model for the axioms, and thus to “explain” why amplitudes have to be complex.</p>



<p>I confess that I was always dissatisfied with this.  For I kept asking myself: would I have ever formulated the “Principle of Local Tomography” in the first place—or if someone else had proposed it, would I have ever accepted it as intuitive or natural—if I didn’t <em>already know</em> that QM over the complex numbers just happens to satisfy it?  And I could never honestly answer “yes.”  It always felt to me like a textbook example of drawing the target around where the arrow landed—i.e., of handpicking your axioms so that they yield a predetermined conclusion, which is then no more “explained” than it was at the beginning.</p>



<p>Two months ago, something changed for me: namely, I smacked into the “Principle of Local Tomography,” and its reliance on complex numbers, in my own research, when I hadn’t in any sense set out to look for it.  This still doesn’t convince me that the principle is any sort of <em>a-priori</em> necessity.  But it at least convinces me that it’s, you know, the sort of thing you can smack into when you’re not looking for it.</p>



<p>The aforementioned smacking occurred while I was writing up a small part of a huge paper with Guy Rothblum, about a new connection between so-called “gentle measurements” of quantum states (that is, measurements that don’t damage the states much), and the subfield of classical CS called <a href="https://en.wikipedia.org/wiki/Differential_Privacy">differential privacy</a>.  That connection is a story in itself; let me know if you’d like me to blog about it separately.  Our paper should be on the arXiv any day now; in the meantime, <a href="https://www.scottaaronson.com/talks/dpgentle-mit.ppt">here are some PowerPoint slides</a>.</p>



<p>Anyway, for the paper with Guy, it was of interest to know the following: suppose we have a two-outcome measurement E (let’s say, on n qubits), and suppose it accepts every product state with the same probability p.  Must E then accept every entangled state with probability p as well?  Or, a closely-related question: suppose we know E’s acceptance probabilities on every product state.  Is that enough to determine its acceptance probabilities on <em>all</em> n-qubit states?</p>



<p>I’m embarrassed to admit that I dithered around with these questions, finding complicated proofs for special cases, before I finally stumbled on the one-paragraph, obvious-in-retrospect “Proof from the Book” that slays them in complete generality.</p>



<p>Here it is: if E accepts every product state with probability p, then clearly it accepts every separable mixed state (i.e., every convex combination of product states) with the same probability p.  Now, a well-known result of <a href="https://arxiv.org/abs/quant-ph/9811018">Braunstein et al.</a>, from 1998, states that (surprisingly enough) the separable mixed states have <em>nonzero density</em> within the set of all mixed states, in any given finite dimension.  Also, the probability that E accepts ρ can be written as f(ρ)=Tr(Eρ), which is linear in the entries of ρ.  OK, but a linear function that’s determined on a subset of nonzero density is determined everywhere.  And in particular, if f is constant on that subset then it’s constant everywhere, QED.</p>



<p>But what does any of this have to do with why amplitudes are complex numbers?  Well, it turns out that the 1998 Braunstein et al. result, which was the linchpin of the above argument, only works in complex QM, not in real QM.  We can see its failure in real QM by simply counting parameters, similarly to what we did before.  An n-qubit density matrix requires 4<sup>n</sup> real parameters to specify (OK, 4<sup>n</sup>-1, if we demand that the trace is 1).  Even if we restrict to n-qubit density matrices with real entries only, we still need 2<sup>n</sup>(2<sup>n</sup>+1)/2 parameters.  By contrast, it’s not hard to show that an n-qubit real <em>separable</em> density matrix can be specified using only 3<sup>n</sup> real parameters—and indeed, that any such density matrix lies in a 3<sup>n</sup>-dimensional subspace of the full 2<sup>n</sup>(2<sup>n</sup>+1)/2-dimensional space of 2<sup>n</sup>×2<sup>n</sup> symmetric matrices.  (This is simply the subspace spanned by all possible tensor products of n <a href="https://en.wikipedia.org/wiki/Pauli_matrices">Pauli</a> I, X, and Z matrices—excluding the Y matrix, which is the one that involves imaginary numbers.)</p>



<p>But it’s not only the Braunstein et al. result that fails in real QM: the fact that I wanted for my paper with Guy fails as well.  As a counterexample, consider the 2-qubit measurement that accepts the state ρ with probability Tr(Eρ), where $$ E=\frac{1}{2}\left(\begin{array}[c]{cccc}1 &amp; 0 &amp; 0 &amp; -1\\0 &amp; 1 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1 &amp; 0\\-1 &amp; 0 &amp; 0 &amp; 1\end{array}\right).$$</p>



<p>I invite you to check that this measurement, which we specified using a real matrix, accepts every product state (a|0⟩+b|1⟩)(c|0⟩+d|1⟩), where a,b,c,d are real, with the same probability, namely 1/2—just like the “measurement” that simply returns a coin flip without even looking at the state at all.  And yet the measurement can clearly be nontrivial on entangled states: for example, it always rejects $$\frac{\left|00\right\rangle+\left|11\right\rangle}{\sqrt{2}},$$ and it always accepts $$ \frac{\left|00\right\rangle-\left|11\right\rangle}{\sqrt{2}}.$$</p>



<p>Is it a coincidence that we used exactly the same 4×4 matrix (up to scaling) to produce a counterexample to the real-QM version of Local Tomography, and <em>also</em> to the real-QM version of the property I wanted for the paper with Guy?  Is anything <em>ever</em> a coincidence in this sort of discussion?</p>



<p>I claim that, looked at the right way, Local Tomography and the property I wanted are the same property, their truth in complex QM is the same truth, and their falsehood in real QM is the same falsehood.  Why?  Simply because Tr(Eρ), the probability that the measurement E accepts the mixed state ρ, is a function of two Hermitian matrices E and ρ (both of which can be either “product” or “entangled”), and—crucially—is symmetric under the interchange of E and ρ.</p>



<p>Now it’s time for another confession.  We’ve identified an elegant property of quantum mechanics that’s true but only because amplitudes are complex numbers: namely, if you know the probability that your quantum circuit accepts every product state, then you also know the probability that it accepts an arbitrary state.  Yet, despite its elegance, this property turns out to be nearly useless for “real-world applications” in quantum information and computing.  The reason for the uselessness is that, for the property to kick in, you really do need to know the probabilities on product states almost <em>exactly</em>—meaning (say) to 1/exp(n) accuracy for an n-qubit state.</p>



<p>Once again a simple example illustrates the point.  Suppose n is even, and suppose our measurement simply projects the n-qubit state onto a tensor product of n/2 Bell pairs.  Clearly, this measurement accepts every n-qubit product state with exponentially small probability, even as it accepts the entangled state <br />$$\left(\frac{\left|00\right\rangle+\left|11\right\rangle}{\sqrt{2}}\right)^{\otimes n/2}$$</p>



<p>with probability 1.  But this implies that noticing the nontriviality on entangled states, would require knowing the acceptance probabilities on product states to exponential accuracy.</p>



<p>In a sense, then, I come back full circle to my original puzzlement: why <em>should</em> Local Tomography, or (alternatively) the-determination-of-a-circuit’s-behavior-on-arbitrary-states-from-its-behavior-on-product-states, have been important principles for Nature’s laws to satisfy?  Especially given that, in practice, the exponential accuracy required makes it difficult or impossible to exploit these principles anyway?  How could we have known a-priori that these principles would be important—if indeed they <em>are</em> important, and are not just mathematical <a href="https://en.wikipedia.org/wiki/Spandrel_(biology)">spandrels</a>?</p>



<p>But, while I remain less than 100% satisfied about “why the complex numbers? why not just the reals?,” there’s <em>one</em> conclusion that my recent circling-back to these questions has made me fully confident about.  Namely: quantum mechanics over the quaternions is a <strong>flaming garbage fire</strong>, which would’ve been rejected at an extremely early stage of God and the angels’ deliberations about how to construct our universe.</p>



<p>In the literature, when the question of “why not quaternionic amplitudes?” is discussed at all, you’ll typically read things about how the parameter-counting doesn’t quite work out (just like it doesn’t for real QM), or how the tensor product of quaternionic Hermitian matrices need not be Hermitian.  In <a href="https://arxiv.org/abs/0911.1761">this paper by McKague</a>, you’ll read that the CHSH game is winnable with probability 1 in quaternionic QM, while in <a href="https://arxiv.org/abs/quant-ph/0307017">this paper by Fernandez and Schneeberger</a>, you’ll read that the non-commutativity of the quaternions introduces an order-dependence even for spacelike-separated operations.</p>



<p>But none of that does justice to the enormity of the problem.  To put it bluntly: unless something clever is done to fix it, quaternionic QM allows superluminal signaling.  This is easy to demonstrate: suppose Alice holds a qubit in the state |1⟩, while Bob holds a qubit in the state |+⟩ (yes, this will work even for unentangled states!)  Also, let $$U=\left(\begin{array}[c]{cc}1 &amp; 0\\0 &amp; j\end{array}\right)  ,~~~V=\left(\begin{array}[c]{cc}1 &amp; 0\\0&amp; i\end{array}\right).$$</p>



<p>We can calculate that, if Alice applies U to her qubit and then Bob applies V to his qubit, Bob will be left with the state $$ \frac{j \left|0\right\rangle + <br />k \left|1\right\rangle}{\sqrt{2}}.$$</p>



<p>By contrast, if Alice decided to apply U only <em>after</em> Bob applied V, Bob would be left with the state <br />$$ \frac{j \left|0\right\rangle – k \left|1\right\rangle}{\sqrt{2}}.$$</p>



<p>But Bob can distinguish these two states with certainty, for example by applying the unitary $$ \frac{1}{\sqrt{2}}\left(\begin{array}[c]{cc}j &amp; k\\k &amp; j\end{array}\right). $$</p>



<p>Therefore Alice communicated a bit to Bob.</p>



<p>I’m aware that there’s a whole literature on quaternionic QM, including for example a <a href="https://www.amazon.com/Quaternionic-Quantum-Mechanics-International-Monographs/dp/019506643X">book by Adler</a>.  Would anyone who knows that literature be kind enough to enlighten us on how it proposes to escape the signaling problem?  Regardless of the answer, though, it seems worth knowing that the “naïve” version of quaternionic QM—i.e., the version that gets invoked in quantum information discussions like the ones I mentioned above—is just immediately blasted to smithereens by the signaling problem, without the need for any subtle considerations like the ones that differentiate real from complex QM.</p>



<p><font color="red"><b>Update (Dec. 20):</b></font> In response to this post, Stephen Adler was kind enough to email me with further details about his quaternionic QM proposal, and to allow me to <a href="https://www.scottaaronson.com/blog/?p=4021#comment-1796606">share them here</a>.  Briefly, Adler completely agrees that quaternionic QM inevitably leads to superluminal signaling—but in his proposal, the surprising and nontrivial part is that quaternionic QM would reduce to standard, complex QM at large distances.  In particular, the strength of a superluminal signal would fall off exponentially with distance, quickly becoming negligible beyond the Planck or grand unification scales.  Despite this, Adler says that he eventually abandoned his proposal for quaternionic QM, since he was unable to make specific particle physics ideas work out (but the quaternionic QM proposal then influenced his later work).</p>



<p><font color="red"><strong>Unrelated Update (Dec. 18):</strong></font> Probably many of you have already seen it, and/or already know what it covers, but the <a href="https://www.nytimes.com/2018/12/17/science/donald-knuth-computers-algorithms-programming.html">NYT profile of Donald Knuth</a> (entitled “The Yoda of Silicon Valley”) is enjoyable and nicely written.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4021"><span class="datestr">at December 17, 2018 04:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6363">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/16/algorithmic-and-information-theoretic-decoding-thresholds-for-low-density-parity-check-code/">Algorithmic and Information Theoretic Decoding Thresholds for Low density Parity-Check Code</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h3>by Jeremy Dohmann, Vanessa Wong, Venkat Arun</h3>

<h2>Abstract</h2>

<p>We will discuss error-correcting codes: specifically, low-density parity-check (LDPC) codes. We first describe their construction and information-theoretical decoding thresholds, <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />. </p>

<p>Belief propagation (BP) (<a href="https://d1b10bmlvqabco.cloudfront.net/attach/jjsqup73w3r2s3/ihaz9phexeb6fb/jmch5yx8edqc/229R_Talk_Notes_2.pdf">see Tom’s notes</a>) can be used to decode these. We analyze BP to find the maximum error-rate upto which BP succeeds.</p>

<p>After this point, BP will reach a suboptimal fixed point with high probability. This is lower than the information-theoretic bound, illustrating a gap between algorithmic and information-theoretic thresholds for decoding.</p>

<p>Then we outline a proof of a theorem suggesting that any efficient algorithm, not just BP, will fail after the algorithmic threshold. This is because there is a phase transition at the algorithmic threshold, after which there exist an exponentially large number of suboptimal `metastable’ states near the optimal solution. Local search algorithms will tend to get stuck at these suboptimal points.</p>

<h3>Introduction to Error Correcting Codes</h3>

<h4>Motivation</h4>

<p>Alice wants to send a message to Bob, but their channel of communication is such that Bob receives a corrupted version of what Alice sent. Most practical communication devices are imperfect and introduce errors in the messages they are transmitting. For instance, if Alice sends 3V, Bob will really receive three volts plus some noise (we have chosen to ignore some inconvenient practical details here). In many cases, this noise is quite small, e.g. it could be less than 0.5V in 99.99% of cases. So, in principle, Alice could have had <em>very </em>reliable delivery by just choosing to always send 0V for a logical 0 and 5V for logical 1, using checksums to detect the occasional error. But this is wasteful. Alice could have squeezed more levels between 0 and 5V to get a higher bitrate. This causes errors, but Alice can introduce redundancy in the bits she is transmitting which can enable Bob to decode the correct message with high probability. Since it is much easier to control redundancy in encoding than in physical quantities, practical communication devices often choose choose to pack enough bits into their physical signal that errors are relatively quite likely, relying instead on redundancy in their encoding to recover from the errors. Redundancy is also used in storage, where we don’t have the option of detecting an error and retransmitting the message.</p>

<p>Some errors in communication are caused by thermal noise. These are unpredictable and unavoidable, but the errors they cause can be easily modeled; they cause bit-flips in random positions in the message. There are other sources of error. The clocks on the two devices may not be correctly synchronized, causing systematic bit-flips in a somewhat predictable pattern. A sudden surge of current (e.g. because someone turned on the lights, and electricity sparked between the contacts) can corrupt a large contiguous segment of bits. Or the cable could simply be cut in which case no information gets through. These other kinds of errors are often harder to model (and easier to detect/mitigate), so we have to remain content with merely detecting them. Thus for the remainder of this blog post, we shall restrict ourselves to an error model where each bit is corrupted with some fixed (but potentially unknown) probability, independent of the other bits. For simplicity, we shall primarily consider the Binary Erasure Channel (BEC), where a bit either goes through successfully, or the receiver <em>knows </em>that there has been an error (though we will introduce some related channels along the way).</p>

<p>Claude Shannon found that given any channel, there is a bitrate below which it is possible to communicate reliably with vanishing error rate. Reliable communication cannot be achieved above this bitrate. Hence this threshold bitrate is called the channel capacity. He showed that random linear codes are an optimal encoding scheme that achieves channel capacity. We will only briefly discuss random linear codes, but essentially they work by choosing random vectors in the input space and mapping them randomly to vectors in the encoded space. Unfortunately we do not have efficient algorithms for decoding these codes (mostly due to the randomness in their construction), and it is conjectured that one doesn’t exist. Recently Low-Density Parity Check (LDPC) codes have gained in popularity. They are simple to construct, and can be efficiently decoded at error levels quite close to the theoretical limits.</p>

<p>With LDPC codes, there are three limits of interest for any given channel and design bitrate (M/N): 1) the error level upto which an algorithm can efficiently decode them, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" />, 2) the error level level upto which they can be decoded by a computationally unbounded decoder, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_c" class="latex" title="\epsilon_c" /> and, 3) the error level beyond which <em>no </em>encoding scheme can achieve reliable communication, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_s" class="latex" title="\epsilon_s" />. Obviously, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cle+%5Cepsilon_c+%5Cle+%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d \le \epsilon_c \le \epsilon_s" class="latex" title="\epsilon_d \le \epsilon_c \le \epsilon_s" />, and in general these inequalities can be strict. Our goal here is to study the gap between <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_c" class="latex" title="\epsilon_c" />. We will sometimes refer to these three quantities as <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />, <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />, and <img src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{shannon}" class="latex" title="p_{shannon}" /> when discussing channels besides the BEC. This is following the notation of Mezard and Montanari (2009).</p>

<p>More formally, information theory concerns reliable communication via an unreliable channel. To mitigate the errors in message transmission, error correcting codes introduce some type of systematic redundancy in the transmitted message. Encoding maps are applied to the information sequence to get the encoded message that is transmitted through the channel. The decoding map, on the other hand, is applied to the noisy channel bit (see Figure below). Each message encoded is comprised of <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> bits and <img src="https://s0.wp.com/latex.php?latex=N%3EM&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N&gt;M" class="latex" title="N&gt;M" /> redundant sequences of bits in an error correcting code. <img src="https://s0.wp.com/latex.php?latex=2%5EM&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^M" class="latex" title="2^M" /> possible codewords form a “codebook” <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BC%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\mathbb{C}|" class="latex" title="|\mathbb{C}|" /> in binary space <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}" class="latex" title="\{0,1\}" />.</p>

<p>Claude Shannon’s code ensembles proved that it is easier to construct stochastic (characterized by good properties and high probability) models vs. deterministic code designs. Stochastic models were able to achieve optimal error correcting code performance in comparison to a more rigidly constructed model, proving that it was possible to communicate with a vanishing error probability as long as the rate of transmission <img src="https://s0.wp.com/latex.php?latex=R%3DM%2FN&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R=M/N" class="latex" title="R=M/N" /> is smaller than the channel capacity, a measure of the maximum mutual information between channel input and output.</p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/encodedecode.png?w=456&amp;h=261" alt="" height="261" class="wp-image-6366" width="456" /><strong>Fig 1</strong> Schematic of the communication model of information communication.</figure>

<p>Thus, in order to construct an optimal error correcting code, one must first define the subset of the space of encoding maps, endow the set with probability distributions, and subsequently define the associated decoding map for each of the encoding maps in the codes. We have included a section in the A that gives a thorough discussion of random code ensembles which are known to achieve optimal decoding, whether via scoring decoding success by bit error rate or decoded word error rate. We will also show a perspective which uses principles from statistical physics to unify the two (often called finite-temperature decoding). From hereon out we will discuss LDPC and explore how the values of <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />, <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> and <img src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{shannon}" class="latex" title="p_{shannon}" /> for various channels reveal deep things about the structure of the decoding solution space.</p>

<h3>Low-density Parity Check Code</h3>

<p>LDPC codes are linear and theoretically excellent error correcting codes that communicate at a rate close to the Shannon capacity. The LDPC codebook is a linear subspace of <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5EN+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}^N " class="latex" title="\{0,1\}^N " />. For an MxN sparse matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{H}" class="latex" title="\mathbb{H}" />, the codebook is defined as the kernel:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D+%3D+%7B+%5Cunderline%7Bx%7D+%5Cin+%5C%7B0%2C1%5C%7D%5EN%3A%5Cmathbb%7BH%7D%5Cunderline%7Bx%7D%3D%5Cunderline%7B0%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{C} = { \underline{x} \in \{0,1\}^N:\mathbb{H}\underline{x}=\underline{0}}" class="latex" title="\mathbb{C} = { \underline{x} \in \{0,1\}^N:\mathbb{H}\underline{x}=\underline{0}}" /></p>

<p><br />where all the multiplications and sums involved in <br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D+%5Cunderline%7Bx%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{H} \underline{x} " class="latex" title="\mathbb{H} \underline{x} " /> are computed modulo 2.</p>

<p>Matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{H}" class="latex" title="\mathbb{H}" />is called the <strong>parity check matrix</strong> and the size of the codebook is <img src="https://s0.wp.com/latex.php?latex=2%5E%7BN-rank%28%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{N-rank(\mathbb{H})}" class="latex" title="2^{N-rank(\mathbb{H})}" />. Given this code, encoding is a linear operation when mapping an <img src="https://s0.wp.com/latex.php?latex=N+x+L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N x L" class="latex" title="N x L" /> binary generating matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{G}" class="latex" title="\mathbb{G}" /> (the codebook is the image of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BG%7D%3A%5Cmathbb%7BC%7D%3D%7Bx%3D%5Cmathbb%7BG%7D%5Cunderline%7Bz%7D%2C+%5Ctext%7Bwhere+%7D+%5Cunderline%7Bz%7D+%5Cin+%5C%7B0%2C1%5C%7D%5EL%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{G}:\mathbb{C}={x=\mathbb{G}\underline{z}, \text{where } \underline{z} \in \{0,1\}^L} " class="latex" title="\mathbb{G}:\mathbb{C}={x=\mathbb{G}\underline{z}, \text{where } \underline{z} \in \{0,1\}^L} " />) such that <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bz%7D%5Crightarrow+%5Cunderline%7Bx%7D+%3D%5Cmathbb%7BG%7Dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{z}\rightarrow \underline{x} =\mathbb{G}z" class="latex" title="\underline{z}\rightarrow \underline{x} =\mathbb{G}z" />).</p>

<p>Every coding scheme has three essential properties that determine its utility: the geometry of its codebook and the way it sparsely distributes proper codewords within the encoding space <img src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5E%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{0,1\}^{N}" class="latex" title="\{0,1\}^{N}" /> (c.f. our mention of sphere packing as a geometric analogy for RLCs), the ease with which one can <em>construct a code</em> which sparsely distributes codes within the encoding space, and the existence of fast algorithms to perform effective decoding.</p>

<p>A coding scheme over a given channel (whether it be <a href="https://en.wikipedia.org/wiki/Binary_symmetric_channel">BSC</a>, <a href="https://en.wikipedia.org/wiki/Binary_erasure_channel">BEC</a>, <a href="https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise">AWGN</a>, etc.) also has three parameters of interest, <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d} " class="latex" title="p_{d} " /> which is the error rate above which some chosen algorithm cannot perform error-free decoding, <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> above which even exhaustively enumerating over all <img src="https://s0.wp.com/latex.php?latex=2%5E%7BM%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{M}" class="latex" title="2^{M}" /> codewords in the codebook and calculating the MAP probability does not successfully decode, <img src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{shannon}" class="latex" title="p_{shannon}" /> which is the capacity of the channel, an error rate above which no decoding scheme could perform error-free decoding.</p>

<h4>LDPC codebook geometry and <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /></h4>

<p>On the subject of codebook geometry, it is well known that LDPC ensembles, in expectation, produce sparsely distributed codewords. This means that valid codewords (i.e. those that pass all parity checks) are far apart from one another in Hamming space and thus require a relatively large number of bits to be lost in order for one codeword to degenerate into another one. There is an important property called the distance enumerator which determines the expected number of codewords in a tight neighborhood of any given codeword. If for a given distance the expected number is exponentially small, then the coding scheme is robust up to error rates causing that degree of distortion. We discuss a proof of LDPC distance properties in Appendix B and simply state here that LDPCs are good at sparsely distributing valid codewords within the encoding space. The property <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />, introduced above is intimately related to the geometry of the codebook and the properties of the noisy channel being decoded over.</p>

<p>The information theoretic threshold, <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> is the noise level above which MAP decoding no longer successfully performs decoding. <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> is important because it is the error value above which we could theoretically always perform (slow) decoding below <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> by enumerating all <img src="https://s0.wp.com/latex.php?latex=2%5E%7BM%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^{M}" class="latex" title="2^{M}" /> codewords in the codebook and calculating the one with the highest MAP probability.</p>

<p>Every LDPC ensemble has some different <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> for a given channel. WFor now we will simply remark that LDPC ensembles are effective because they can be chosen such that <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> closely approaches the <img src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{shannon}" class="latex" title="p_{shannon}" /> limit for many channels. Even more importantly, we will show that it is <em>likely </em>that there is no fast algorithm for which <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> = <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> for general LDPCs over a general noisy channel.</p>

<p>We will not derive the <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> for any LDPC ensembles over any channels (I recommend <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">chapter 15 of this book for details</a>), but we will, in section 3, present results derived by other researchers.</p>

<h4>Ease of construction</h4>

<p>On the subject of ease of construction and ease of decoding, there is a much simpler graphical representation of LDPC codes which can be used to demonstrate LDPC tractability.</p>

<p>LDPCs can be thought of as bipartite regular graphs, where there are N variable nodes which are connected to M parity check nodes according to randomly chosen edges based on the degree distribution of the LDPC. Though the appendix discusses general degree distributions we will discuss here only (d,k) regular bipartite graphs, in which all variables have d edges and all parity check nodes have k edges, and how to generate them under the configuration model.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Configuration_model">configuration model</a> can be used to generate a bipartite graph with (d,k) degree distribution by initially assigning all variable nodes d half-edges, all parity check nodes k half-edges, and then randomly linking up half-edges between the two sets, deleting all nodes which end up being paired an even number of times, and collapsing all odd numbered multi-edges into a single edge. This system doesn’t work perfectly but for large N, the configuration model will generate a graph for which most nodes have the proper degree. Thus it is relatively easy to generate random graphs which represent LDPC codes of any desired uniform degree distribution. An example of this graphical representation is in figure 2</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/graph.png?w=600" alt="" class="wp-image-6382" /><strong>Fig 2 </strong>Factor graph of a (2,3) regular LDPC code with factor nodes as black squares and variable nodes as white circles, and notation for BP messages.</figure>

<h2>How the graphical model relates to fast decoding</h2>

<p>The graphical model of LDPCs is useful because it is both easy to construct and presents a natural way to perform fast decoding. In fact, the fast graph-based decoding algorithm, <a href="http://{https://en.wikipedia.org/wiki/Belief_propagation">Belief Propagation</a>, we use has a <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> which likely represents the upper limit on fast decoding for LDPCs.</p>

<p>We have seen <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">recently </a>that bipartite graphical models <br />which represent a factorized probability distribution can be used to calculate marginal probabilities of individual variable nodes (what a mouthful!).</p>

<p>Basically, if the structure of the graph reflects some underlying probability distribution (e.g. the probability that noisy bit <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i" class="latex" title="y_i" /> was originally sent as bit <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" />) then we can use an iterative algorithm called Belief Propagation (see blog post above) to actually converge to the exact probability distribution for each <img src="https://s0.wp.com/latex.php?latex=P%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(y_i~|~x_{i})" class="latex" title="P(y_i~|~x_{i})" />.</p>

<p>This is important because when we perform decoding, we would like to estimate the marginal probability of each individual variable node (bit in our received vector), and simply set the variable to be the most likely value (this is known as bit-MAP decoding, discussed earlier). As mentioned above, under certain conditions the Belief Propagation algorithm correctly calculates those marginal probabilities for noise rates up to an algorithmic threshold <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />.</p>

<p>The Belief Propagation algorithm is an iterative message passing algorithm in which messages are passed between variable nodes and parity check/factor nodes such that, if the messages converge to a fixed point, the messages encode the marginal probabilities of each variable node. Thus BP, if it succeeds can perform bit-MAP decoding and thus successfully decode.</p>

<p>We will show in the next section how the configuration model graphs map to a factorized probability distribution and mention the <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> for BP. In the following section we will show an example of decoding over the binary erasure channel, then finally we will show motivation to suggest that the <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> for BP over LDPCs represents a hard upper limit above which no fast decoding algorithms exist.</p>

<h3>Decoding Errors via Belief Propagation</h3>

<p>As mentioned above (<a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">again, please see Tom’s excellent blog post for details</a>), the belief propagation algorithm is a useful inference algorithm for stochastic models and sparse graphs derived from computational problems exhibiting thresholding behavior. As discussed, symbol/bit MAP decoding of error correcting codes can be regarded as a statistical inference problem. In this section, we will explore BP decoding to determine the threshold for reliable communication and according optimization for LDPC code ensembles in communication over a binary input output symmetric memoryless channel (BSC or BMS).</p>

<h4>Algorithm Overview</h4>

<p>Recall that the conditional distribution of the channel input <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" /> given the output <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" /> is given by and that we wish to find the <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" /> that maximizes the below probability given <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" /></p>

<p><img src="https://s0.wp.com/latex.php?latex=p%28%5Cunderline%7Bx%7D%7C%5Cunderline%7By%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28y%29%7D%5Cprod_%7Bi%3D1%7D%5E%7BN%7DQ%28y_i%7Cx_i%29+%5Cprod_%7Ba%3D1%7D%5E%7BM%7D+%5Cmathbb%7BI%7D%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^{N}Q(y_i|x_i) \prod_{a=1}^{M} \mathbb{I}(x_{i_{1^{a}}} \otimes ... ~x_{k(a)^a} = 0)" class="latex" title="p(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^{N}Q(y_i|x_i) \prod_{a=1}^{M} \mathbb{I}(x_{i_{1^{a}}} \otimes ... ~x_{k(a)^a} = 0)" /> (1)</p>

<p style="text-align: left;"><br />Where <img src="https://s0.wp.com/latex.php?latex=Q%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(y_i~|~x_{i})" class="latex" title="Q(y_i~|~x_{i})" /> is the conditional probability of <img src="https://s0.wp.com/latex.php?latex=y_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_{i}" class="latex" title="y_{i}" /> of observing noisy bit <img src="https://s0.wp.com/latex.php?latex=y_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_{i}" class="latex" title="y_{i}" /> given that <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{i}" class="latex" title="x_{i}" /> was sent. For the BSC we have <img src="https://s0.wp.com/latex.php?latex=Q%28y_%7Bi%7D+%3D+1+%7C+x_%7Bi%7D+%3D+1%29+%3D+Q%28y_%7Bi%7D+%3D+0+%7C+x_%7Bi%7D+%3D+0%29+%3D+1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(y_{i} = 1 | x_{i} = 1) = Q(y_{i} = 0 | x_{i} = 0) = 1-p" class="latex" title="Q(y_{i} = 1 | x_{i} = 1) = Q(y_{i} = 0 | x_{i} = 0) = 1-p" /> and <img src="https://s0.wp.com/latex.php?latex=Q%28y_%7Bi%7D+%3D+1+%7C+x_%7Bi%7D+%3D+0%29+%3D+Q%28y_%7Bi%7D+%3D+0+%7C+x_%7Bi%7D+%3D+1%29+%3D+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(y_{i} = 1 | x_{i} = 0) = Q(y_{i} = 0 | x_{i} = 1) = p" class="latex" title="Q(y_{i} = 1 | x_{i} = 0) = Q(y_{i} = 0 | x_{i} = 1) = p" />.</p>

<p>Furthermore</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D+%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes%C2%A0+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{I} (x_{i_{1^{a}}} \otimes  ... ~x_{k(a)^a} = 0)" class="latex" title="\mathbb{I} (x_{i_{1^{a}}} \otimes  ... ~x_{k(a)^a} = 0)" /></p>

<p>is an indicator variable which takes value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" /> if <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" /> satisfies parity check a and 0 otherwise. In particular, the product of these indicators takes into account the fact that 0 probability should be assigned to hypotheses <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" /> which aren’t in the code book (indicated by at least one parity check failing).</p>

<p>We would like to design a message passing scheme such that the incoming messages for a given variable node encode their marginal probabilities <img src="https://s0.wp.com/latex.php?latex=p%28x_i%7E%7C%7Ey_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p(x_i~|~y_i)" class="latex" title="p(x_i~|~y_i)" />.</p>

<p>Note, first and foremost that this probability can be <em>factorized </em>a la BP factor graphs such that there is a factor node for each parity check node <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> which contributes probability</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D+%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{I} (x_{i_{1^{a}}} \otimes ... ~x_{k(a)^a} = 0)" class="latex" title="\mathbb{I} (x_{i_{1^{a}}} \otimes ... ~x_{k(a)^a} = 0)" /></p>

<p>and a factor node for each channel probability term <img src="https://s0.wp.com/latex.php?latex=Q%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q(y_i~|~x_{i})" class="latex" title="Q(y_i~|~x_{i})" />. Since each channel probability term is only connected to a single variable, its message never gets updated during BP and so we omit it from the factor graphs (e.g. note that figure 2 only has parity check nodes and variable nodes)</p>

<p>The message passing scheme ends up taking the form</p>

<p><img src="https://s0.wp.com/latex.php?latex=v_%7Bi%5Crightarrow+a%7D%5E%7B%28t%2B1%29%7D%28x_i%29+%5Cpropto+Q%28y_%7Bi%7D+%7C+x_%7Bi%7D%29+%5Cprod_%7Bb+%5Cin+%5Cpartial+i+%5Csetminus+a%7D+%5Chat%7Bv%7D%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v_{i\rightarrow a}^{(t+1)}(x_i) \propto Q(y_{i} | x_{i}) \prod_{b \in \partial i \setminus a} \hat{v}{b \rightarrow i}^{(t)}" class="latex" title="v_{i\rightarrow a}^{(t+1)}(x_i) \propto Q(y_{i} | x_{i}) \prod_{b \in \partial i \setminus a} \hat{v}{b \rightarrow i}^{(t)}" /> (2)</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bv%7D_%7Ba+%5Crightarrow+i%7D%28x_%7Bi%7D%29+%5Cpropto+%5Csum_%7B%7Bx_%7Bj%7D%7D%7D+%5Cmathbb%7BI%7D+%28x_%7Bi%7D+%5Cotimes+x_%7Bj_%7B1%7D%7D+...+x_%7Bj_%7Bk-1%7D%7D+%3D+0%29+%5Cprod_%7Bj+%5Cin+%5Cpartial+a+%5Csetminus+i%7D+v_%7Bj%5Crightarrow+a%7D%5E%7B%28t%29%7D%28x_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{v}_{a \rightarrow i}(x_{i}) \propto \sum_{{x_{j}}} \mathbb{I} (x_{i} \otimes x_{j_{1}} ... x_{j_{k-1}} = 0) \prod_{j \in \partial a \setminus i} v_{j\rightarrow a}^{(t)}(x_j)" class="latex" title="\hat{v}_{a \rightarrow i}(x_{i}) \propto \sum_{{x_{j}}} \mathbb{I} (x_{i} \otimes x_{j_{1}} ... x_{j_{k-1}} = 0) \prod_{j \in \partial a \setminus i} v_{j\rightarrow a}^{(t)}(x_j)" /> (3)</p>

<p>Where <img src="https://s0.wp.com/latex.php?latex=%5Cpartial+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\partial a" class="latex" title="\partial a" /> denotes the neighborhood of factor node <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> and the sum in (3) is over all possible configurations of the neighbors of <img src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="a" class="latex" title="a" /> not including <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />.</p>

<p>Messages are passed along the edges as distributions over binary valued variables described by the log-likelihoods</p>

<p><img src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Clog+%5Cfrac%7Bv_%7Bi%5Crightarrow+a%280%29%7D%7D%7Bv_%7Bi%5Crightarrow+a+%281%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i\rightarrow a} = \frac{1}{2}\log \frac{v_{i\rightarrow a(0)}}{v_{i\rightarrow a (1)}}" class="latex" title="h_{i\rightarrow a} = \frac{1}{2}\log \frac{v_{i\rightarrow a(0)}}{v_{i\rightarrow a (1)}}" /> (4)</p>

<p><img src="https://s0.wp.com/latex.php?latex=u_%7Bi%5Crightarrow+a%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Clog+%5Cfrac%7B%5Chat%7Bv%7D%7Bi%5Crightarrow+a%280%29%7D%7D%7B%5Chat%7Bv%7D%7Bi%5Crightarrow+a+%281%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u_{i\rightarrow a} = \frac{1}{2}\log \frac{\hat{v}{i\rightarrow a(0)}}{\hat{v}{i\rightarrow a (1)}}" class="latex" title="u_{i\rightarrow a} = \frac{1}{2}\log \frac{\hat{v}{i\rightarrow a(0)}}{\hat{v}{i\rightarrow a (1)}}" /> (5)</p>

<p><br />We also introduce the a priori log likelihood for bit <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> given the received message <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i" class="latex" title="y_i" />:</p>

<p><img src="https://s0.wp.com/latex.php?latex=B_%7Bi%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+log+%5Cfrac%7BQ%28y_%7Bi%7D+%7C+0%29%7D%7BQ%28y_%7Bi%7D+%7C+1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B_{i} = \frac{1}{2} log \frac{Q(y_{i} | 0)}{Q(y_{i} | 1)}" class="latex" title="B_{i} = \frac{1}{2} log \frac{Q(y_{i} | 0)}{Q(y_{i} | 1)}" /></p>

<p>Once we parametrize the messages as log-likelihoods, it turns out we can rewrite our update rules in terms of the parametrized values h and u, making updates much simpler:</p>

<p><img src="https://s0.wp.com/latex.php?latex=h_%7Bi+%5Crightarrow+a%7D%5E%7B%28t%2B1%29%7D+%3D+B_i+%2B+%5Csum_%7Bb+%5Cin+%5Cpartial+i+%5Csetminus+a%7D+u_%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i \rightarrow a}^{(t+1)} = B_i + \sum_{b \in \partial i \setminus a} u_{b \rightarrow i}^{(t)}" class="latex" title="h_{i \rightarrow a}^{(t+1)} = B_i + \sum_{b \in \partial i \setminus a} u_{b \rightarrow i}^{(t)}" /> (6)</p>

<p><img src="https://s0.wp.com/latex.php?latex=u_%7Bb%5Crightarrow+i%7D%5E%7B%28t%29%7D+%3D+atanh%7B+%5Cprod_%7Bj+%5Cin+%5Cpartial+a+%5Csetminus+i%7D+tanh%28h_%7Bj+%5Crightarrow+a%7D%5E%7B%28t%29%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u_{b\rightarrow i}^{(t)} = atanh{ \prod_{j \in \partial a \setminus i} tanh(h_{j \rightarrow a}^{(t)})}" class="latex" title="u_{b\rightarrow i}^{(t)} = atanh{ \prod_{j \in \partial a \setminus i} tanh(h_{j \rightarrow a}^{(t)})}" /> (7)</p>

<p>Given a set of messages, we would perform decoding via the overall log likelihood <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5E%7B%28t%2B1%29%7D+%3D+B_i+%2B+%5Csum_%7Bb+%5Cin+%5Cpartial+i%7D+u_%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i}^{(t+1)} = B_i + \sum_{b \in \partial i} u_{b \rightarrow i}^{(t)}" class="latex" title="h_{i}^{(t+1)} = B_i + \sum_{b \in \partial i} u_{b \rightarrow i}^{(t)}" />. Where <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{i}" class="latex" title="x_{i}" /> gets decoded to 0 for <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i} &gt; 0" class="latex" title="h_{i} &gt; 0" /> and 1 for <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D+%3C+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i} &lt; 0" class="latex" title="h_{i} &lt; 0" />.</p>

<p>Typically BP is run until it converges to a set of messages that decode to a word in the codebook, or until a max number of iterations have occurred. Other stopping criteria exist such as the messages between time step t and t+1 being all within some small <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> of one another.</p>

<p>It is important to note some properties of BP:</p>

<ol><li><strong>BP always terminates in <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> steps if the factor graph is a tree of depth <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /></strong></li><li><strong>It is not known under what circumstances so called “loopy” BP will converge for non-tree graphs</strong></li></ol>

<p>Because factor graphs of LDPC codes are relatively sparse, they appear “locally tree-like”, a property which is believed to play a crucial role in BP convergence over the factorized probability distribution used in LDPC MAP decoding (eqn 1). As mentioned above BP manages to converge on many sorts of non tree-like graphs given that they have “nice” probability distributions. For example <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=2ahUKEwic8ozUqoTfAhXLxlkKHWKGDfYQFjABegQICBAC&amp;url=https%3A%2F%2Fstatweb.stanford.edu%2F~souravc%2Ftalk_spin.pdf&amp;usg=AOvVaw0LixaqHUkY6G795LkiyEgX">the SK model</a> is known to converge even though the underlying factor graph is a complete graph!</p>

<p>It turns out that BP converges under some noise levels for LDPC decoding, and that the threshold at which it fails to converge, <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />, represents a phase transition to a generically different regime in the solution space of the codebook. It’s been noted elsewhere that the BP threshold is often the threshold of fast solving for many cool problems; e.g. <a href="https://www.amazon.com/Nature-Computation-Cristopher-Moore/dp/0199233217">k-SAT</a>. This is because it is often thought to generically represent the “best” possible local (ergo “fast”) algorithm for those problems</p>

<p>In appendix C we will show some important properties of BP. The following tables summarize important results for several ensembles and channels. Note how close the information theoretic threshold for LDPCs is to the actual shannon limit <img src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{shannon}" class="latex" title="p_{shannon}" /> for the channels below.</p>

<p style="text-align: center;"><strong>Table 1</strong>: Thresholds for BSC<br />Various thresholds for BP over LDPC codes in a Binary Symmetric Channel</p>

<table class="wp-block-table has-fixed-layout is-style-stripes"><tbody><tr><td>d</td><td>k</td><td><img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /></td><td><img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /></td><td>Shannon limit</td></tr><tr><td>3</td><td>4</td><td>.1669</td><td>.2101</td><td>.2145</td></tr><tr><td>3</td><td>5</td><td>.1138</td><td>.1384</td><td>.1461</td></tr><tr><td>3</td><td>6</td><td>.084</td><td>.101</td><td>.11</td></tr><tr><td>4</td><td>6</td><td>.1169</td><td>.1726</td><td>.174</td></tr></tbody></table>

<p style="text-align: center;">See Mezard and Montanari, 2009 Chapt 15. for this table</p>

<p style="text-align: center;"><strong>Table 2</strong>: Thresholds for BEC<br />Various thresholds for BP over LDPC codes in a Binary Erasure Channel</p>

<table class="wp-block-table has-fixed-layout is-style-stripes"><tbody><tr><td>d</td><td>k</td><td><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_{d}" class="latex" title="\epsilon_{d}" /></td><td><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_{c}" class="latex" title="\epsilon_{c}" /></td><td>Shannon limit</td></tr><tr><td>3</td><td>4</td><td>.65</td><td>.746</td><td>.75</td></tr><tr><td>3</td><td>5</td><td>.52</td><td>.59</td><td>.6</td></tr><tr><td>3</td><td>6</td><td>.429</td><td>.4882</td><td>.5</td></tr><tr><td>4</td><td>6</td><td>.506</td><td>.66566</td><td>.6667</td></tr></tbody></table>

<p style="text-align: center;">See Mezard and Montanari, 2009 Chapt 15. for this table</p>

<p>We will now show exact behavior of the (3,6) LDPC ensemble over the binary erasure channel.</p>

<h3>Algorithmic Thresholds for Belief Propagation (BP)</h3>

<h4>Definitions and notation</h4>

<p><strong>Definition 1.</strong> In a <strong>Binary Erasure Channel (BEC)</strong>, when the transmitter sends a bit <img src="https://s0.wp.com/latex.php?latex=%5Cin+%7B0%2C+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\in {0, 1}" class="latex" title="\in {0, 1}" />, the receiver receives the correct bit with probability <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - \epsilon" class="latex" title="1 - \epsilon" /> or an error symbol <img src="https://s0.wp.com/latex.php?latex=%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="*" class="latex" title="*" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />.</p>

<p>For BECs, the Shannon capacity—the maximum number of data bits that can be transmitted per encoded bit—is given by <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - \epsilon" class="latex" title="1 - \epsilon" />.</p>

<p><strong>Definition 2.</strong> An <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" />-bit <strong>Error Correcting Code (ECC)</strong> is defined by a codebook <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D+%5Csubset+%7B0%2C+1%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{C} \subset {0, 1}^N" class="latex" title="\mathcal{C} \subset {0, 1}^N" />. The transmitter encodes information as an element of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" />. The receiver receives a corrupted version <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> of the transmitted codeword. To decode, it picks an <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x \in \mathcal{C}" class="latex" title="x \in \mathcal{C}" /> that is most likely given <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> and the channel characteristics.</p>

<p>For ease of discourse, we have refrained from defining ECC in full generality.</p>

<p><strong>Definition 3.</strong> An <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" />-bit <img src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\lambda, \rho)" class="latex" title="(\lambda, \rho)" /> Low Density Parity Check Code (LDPC) is an ECC with <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D+%3D+%7Bx+%7C+Hx+%3D+0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{C} = {x | Hx = 0}" class="latex" title="\mathcal{C} = {x | Hx = 0}" />. Here <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is an <img src="https://s0.wp.com/latex.php?latex=M+%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M \times N" class="latex" title="M \times N" /> matrix and arithmetic is over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{Z}_2" class="latex" title="\mathbb{Z}_2" />. <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is a sparse parity-check matrix. <img src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+%5Csum_i%5Clambda_ix%5E%7Bi-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda(x) = \sum_i\lambda_ix^{i-1}" class="latex" title="\lambda(x) = \sum_i\lambda_ix^{i-1}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29+%3D+%5Csum_i%5Crho_ix%5E%7Bi-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho(x) = \sum_i\rho_ix^{i-1}" class="latex" title="\rho(x) = \sum_i\rho_ix^{i-1}" /> are finite polynomials that characterize <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />; <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda_i" class="latex" title="\lambda_i" /> is the fraction of columns with <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />s and <img src="https://s0.wp.com/latex.php?latex=%5Crho_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho_i" class="latex" title="\rho_i" /> is the fraction of rows with <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1" class="latex" title="1" />s. Since these are fractions/probabilities, they must be normalized. Hence <img src="https://s0.wp.com/latex.php?latex=%5Clambda%281%29+%3D+%5Crho%281%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda(1) = \rho(1) = 1" class="latex" title="\lambda(1) = \rho(1) = 1" />. <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is a random matrix, and therefore has full rank with high probability.</p>

<p>In an LDPC code, <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BC%7D%7C+%3D+2%5E%7BN+-+M%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="|\mathcal{C}| = 2^{N - M}" class="latex" title="|\mathcal{C}| = 2^{N - M}" />. Hence every <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" /> bits contain <img src="https://s0.wp.com/latex.php?latex=N-M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N-M" class="latex" title="N-M" /> bits of information, making the rate <img src="https://s0.wp.com/latex.php?latex=1+-+M+%2F+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - M / N" class="latex" title="1 - M / N" />. Over binary erasure channels (BECs), on receiving <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" />, the decoder must choose an <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> such that <img src="https://s0.wp.com/latex.php?latex=x_i+%3D+y_i+%5Cforall+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i = y_i \forall i" class="latex" title="x_i = y_i \forall i" /> such that <img src="https://s0.wp.com/latex.php?latex=y_i+%5Cneq+%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i \neq *" class="latex" title="y_i \neq *" />, and <img src="https://s0.wp.com/latex.php?latex=Hx+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Hx = 0" class="latex" title="Hx = 0" /> (<img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" />, <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y_i" class="latex" title="y_i" /> denote the <img src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i^{th}" class="latex" title="i^{th}" /> bit of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> respectively). That is, the bits that were successfully transmitted should be preserved; other bits should be chosen to satisfy the parity check equations. If multiple correct choices of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" /> are possible, then <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> cannot be unambiguously decoded.</p>

<h4>BP/Peeling algorithm</h4>

<p>In general, decoding can be computationally hard. But there exists an error rate <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" />, a function of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda" class="latex" title="\lambda" /> and <img src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho" class="latex" title="\rho" />, below which belief propagation succeeds in decoding. Let <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_c" class="latex" title="\epsilon_c" /> be the maximum error rate upto which successful decoding is possible (i.e. we can unambiguously determine the transmitted codeword) and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_s" class="latex" title="\epsilon_s" /> be the Shannon limit, then <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cleq+%5Cepsilon_c+%5Cleq+%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d \leq \epsilon_c \leq \epsilon_s" class="latex" title="\epsilon_d \leq \epsilon_c \leq \epsilon_s" />. In general, these inequalities can be strict, illustrating the gap between what is information theoretically possible, and what is computationally feasible.</p>

<p>Belief propagation (BP) for decoding LDPC-codes is equivalent to a simple peeling algorithm. Let us first describe the factor-graph representation for decoding. This is denoted in figure 3. Variables on the left are the received symbol <img src="https://s0.wp.com/latex.php?latex=%5Cin+%7B0%2C+1%2C+%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\in {0, 1, *}" class="latex" title="\in {0, 1, *}" />. Factor nodes on the right denote the parity-check constraint (rows of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />). The XOR of variables connected to each factor node must be 0.</p>

<p>BP/the peeling algorithm works as follows. For simplicity of exposition, consider that the all zeros code-word has been transmitted. Since this is a linear code, there is no loss of generality. At first, only the <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - \epsilon" class="latex" title="1 - \epsilon" /> variables that were successfully transmitted are fully determined. In the next round, the factor nodes that have exactly one undetermined variable can determine that variable using their parity-check constraint.</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/bp-fails.png?w=600" alt="" class="wp-image-6433" /><strong>Fig 3</strong> An example of a received code-word and corresponding parity-check constraints that is information-theoretically determined (only the all zeros codeword satisfies all constraints), but cannot be decoded by the belief propagation algorithm because no factor node has exactly one unknown variable. Here, only the V0 is correctly received. Constraints F1, F2, F3 and F4 imply that V1=V2=V3=V4=V5. If V0=0, then all the rest must be 0s to satisfy F0 (note, the only other valid codeword is all ones).</figure>

<h4>BP isn’t perfect</h4>

<p>This algorithm is not perfect. Figure 3 is an example of a received codeword which <em>can </em>be unambiguously decoded — only the all zeros codeword satisfies all the constraints— but the BP algorithm fails, because at any point, all factor nodes have more than one unknown variable. It seems that the only way to solve problems like that is to exhaustively understand the implications of the parity-check equations. If this examples seems contrived, that is because it is. Decoding becomes harder as the degree and number of constraints increases; we had to add a lot of constraints to make this example work. Fortunately, if the graph is sparse, BP succeeds. We prove this in the following theorem:</p>

<h4>Phase transitions for BP</h4>

<p><strong>Theorem 1.</strong> A <img src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\lambda, \rho)" class="latex" title="(\lambda, \rho)" /> LDPC code can be decoded by BP as <img src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N \rightarrow \infty" class="latex" title="N \rightarrow \infty" /> when the error rate is less than <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" />:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+%5Cmathrm%7Binf%7D_%7Bz+%5Cin+%280%2C+1%29%7D%5Cleft%5B%5Cfrac%7Bz%7D%7B%5Clambda%281+-+%5Crho%281+-+z%29%29%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" class="latex" title="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" /></p>

<p><em>Proof.</em> To prove this, let us analyze the density evolution. For BECs, this is particularly simple as we only need to keep track of the fraction of undetermined variables and factor nodes at timestep <img src="https://s0.wp.com/latex.php?latex=t%3A%7Elatex+z_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t:~latex z_t" class="latex" title="t:~latex z_t" /> and <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bz%7D_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\hat{z}_t" class="latex" title="\hat{z}_t" /> respectively. As <img src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N \rightarrow \infty" class="latex" title="N \rightarrow \infty" />, these fractions are probabilities. A factor node is determined when all of its variables are determined (note: if all but one is determined, the last one can be immediately determined). The following recursion relations hold:</p>

<p><img src="https://s0.wp.com/latex.php?latex=z_%7Bt%2B1%7D+%3D+%5Cepsilon%5Clambda%28%5Chat%7Bz%7D_t%29+%5C%3A%5C%3A%5C%3A%5Cmathrm%7Band%7D%5C%3A%5C%3A%5C%3A+%5Chat%7Bz%7D_t+%3D+1+-+%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z_{t+1} = \epsilon\lambda(\hat{z}_t) \:\:\:\mathrm{and}\:\:\: \hat{z}_t = 1 - \rho(1 - z_t)" class="latex" title="z_{t+1} = \epsilon\lambda(\hat{z}_t) \:\:\:\mathrm{and}\:\:\: \hat{z}_t = 1 - \rho(1 - z_t)" /> (8)</p>

<p>The first holds because a variable node is undetermined at timestep <img src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t+1" class="latex" title="t+1" /> if it was originally undetermined (which happens with probability <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />) <em>and </em>if it isn’t determined in the last step, which happens with probability say <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />. Now,</p>

<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Cmathbf%7BP%7D%28degree%3D2%29%5Chat%7Bz%7D_t+%2B+%5Cmathbf%7BP%7D%28degree%3D3%29%5Chat%7Bz%7D_t%5E2+%2B+%5Ccdots+%3D+%5Clambda%28%5Chat%7Bz%7D_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p = \mathbf{P}(degree=2)\hat{z}_t + \mathbf{P}(degree=3)\hat{z}_t^2 + \cdots = \lambda(\hat{z}_t)" class="latex" title="p = \mathbf{P}(degree=2)\hat{z}_t + \mathbf{P}(degree=3)\hat{z}_t^2 + \cdots = \lambda(\hat{z}_t)" /></p>

<p>A similar reasoning holds for the second relation. <img src="https://s0.wp.com/latex.php?latex=1+-+z_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - z_t" class="latex" title="1 - z_t" /> is the probability that a given neighboring variable node is determined. <img src="https://s0.wp.com/latex.php?latex=%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho(1 - z_t)" class="latex" title="\rho(1 - z_t)" /> is the probability that at-most one is undetermined, and hence this function node is determined. <img src="https://s0.wp.com/latex.php?latex=1+-+%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - \rho(1 - z_t)" class="latex" title="1 - \rho(1 - z_t)" /> is the probability that this function node is undetermined.</p>

<p>Composing the two relations in equation 8, we get the recursion:</p>

<p><img src="https://s0.wp.com/latex.php?latex=z_%7Bt%2B1%7D+%3D+F_%7B%5Cepsilon%7D%28z%29+%3D+%5Cepsilon+%5Clambda%281+-+%5Crho%281+-+z_t%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z_{t+1} = F_{\epsilon}(z) = \epsilon \lambda(1 - \rho(1 - z_t))" class="latex" title="z_{t+1} = F_{\epsilon}(z) = \epsilon \lambda(1 - \rho(1 - z_t))" /></p>

<p>An example of <img src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z)" class="latex" title="F(z)" /> is shown in Figure 4 for <img src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+x%5E2%2C+%5Crho%28x%29+%3D+x%5E5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda(x) = x^2, \rho(x) = x^5" class="latex" title="\lambda(x) = x^2, \rho(x) = x^5" />. That is a (3,6) regular graph where variable nodes and function nodes have 3 and 6 neighbors respectively. On the left, <img src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z)" class="latex" title="F(z)" /> is always below <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" />. Hence the recursion with <img src="https://s0.wp.com/latex.php?latex=z_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z_0" class="latex" title="z_0" /> starting from the far right will converge to the fixed point <img src="https://s0.wp.com/latex.php?latex=F%28z%29+%3D+z+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z) = z = 0" class="latex" title="F(z) = z = 0" />. But on the right, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> is large enough that <img src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z)" class="latex" title="F(z)" /> intersects <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" /> at a non-zero point. Hence the recursion will converge to the higher fixed point instead, without ever reaching the `correct’ fixed point. BP therefore gets stuck at a suboptimal solution, though information-theoretically a correct solution exists. This can be interpreted as a glassy state, where many deep local minima are present, and BP will converge to the wrong minimum.</p>

<p>The condition for BP to converge is <img src="https://s0.wp.com/latex.php?latex=F_%5Cepsilon%28z%29+%5Cle+z+%5C%3A%5C%3A+%5Cforall+z+%5Cin+%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_\epsilon(z) \le z \:\: \forall z \in (0, 1)" class="latex" title="F_\epsilon(z) \le z \:\: \forall z \in (0, 1)" />. Hence the threshold error rate, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" />, below which this condition holds is:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+%5Cmathrm%7Binf%7D_%7Bz+%5Cin+%280%2C+1%29%7D%5Cleft%5B%5Cfrac%7Bz%7D%7B%5Clambda%281+-+%5Crho%281+-+z%29%29%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" class="latex" title="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" /></p>

<p>For (3, 6) regular graphs, <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Capprox+0.429&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d \approx 0.429" class="latex" title="\epsilon_d \approx 0.429" /> ∎</p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-3-6-above.png?w=460&amp;h=293" alt="" height="293" class="wp-image-6442" width="460" /></figure>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-3-6-below.png?w=463&amp;h=295" alt="" height="295" class="wp-image-6443" width="463" /><br /><strong>Fig 4</strong> The recursion relation for a (3,6) regular graph, where <img src="https://s0.wp.com/latex.php?latex=F_%5Cepsilon%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F_\epsilon(z)" class="latex" title="F_\epsilon(z)" /> is plotted against <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" />. The identity function <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z" class="latex" title="z" /> is also shown (in blue). Here <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Capprox0.429&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d \approx0.429" class="latex" title="\epsilon_d \approx0.429" />. The graph above and below show the case the error rate is below and above <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> respectively.</figure>

<p>Another interesting phase transition can be observed. As <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> increases, for some values of <img src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\lambda, \rho)" class="latex" title="(\lambda, \rho)" />, the first intersection of <img src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z)" class="latex" title="F(z)" /> and <img src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F(z)" class="latex" title="F(z)" /> happens at a non-zero point. For others, it starts of at <img src="https://s0.wp.com/latex.php?latex=z%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="z=0" class="latex" title="z=0" /> and goes up continuously. In the former case, the decoding error rate jumps discontinuously as <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> increases from 0 to a non-zero values. For the latter, it increases continuously.</p>

<p>To see the gap between <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> and what can be information theoretically, we look at what happens when the degrees of the LDPC code is increased while keeping the rate constant. Specifically consider the <img src="https://s0.wp.com/latex.php?latex=%28l%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(l, k)" class="latex" title="(l, k)" /> regular graph (i.e. <img src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+x%5E%7Bl-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\lambda(x) = x^{l-1}" class="latex" title="\lambda(x) = x^{l-1}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29+%3D+x%5E%7Bk-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rho(x) = x^{k-1}" class="latex" title="\rho(x) = x^{k-1}" />) as <img src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \rightarrow \infty" class="latex" title="k \rightarrow \infty" /> while <img src="https://s0.wp.com/latex.php?latex=l+%2F+k+%3D+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l / k = 0.5" class="latex" title="l / k = 0.5" /> is fixed. Note that the rate of the code is <img src="https://s0.wp.com/latex.php?latex=1+-+l+%2F+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 - l / k" class="latex" title="1 - l / k" />. This is shown in Figure 5. <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> decreases toward 0. But as <img src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \rightarrow \infty" class="latex" title="k \rightarrow \infty" />, it should become information-theoretically easier to decode. In fact, as <img src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k \rightarrow \infty" class="latex" title="k \rightarrow \infty" />, the code approaches a random linear code, which is known to achieve Shannon capacity. Hence we can believe that the information-theoretically achievable decoding rate is non-decreasing. Thus there is a gap between what is information theoretically possible to decode, and what is computationally feasible using Belief Propagation.</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-ed-lim-inf.png?w=600" alt="" class="wp-image-6447" /><strong>Fig 5</strong> <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> decreases as <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> increases, while the rate <img src="https://s0.wp.com/latex.php?latex=%3D+1+-+l%2Fk+%3D+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="= 1 - l/k = 0.5" class="latex" title="= 1 - l/k = 0.5" /> is fixed. In fact <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blim%7D_%7Bk+%5Crightarrow+%5Cinfty%7D%5Cepsilon_d+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathrm{lim}_{k \rightarrow \infty}\epsilon_d = 0" class="latex" title="\mathrm{lim}_{k \rightarrow \infty}\epsilon_d = 0" />.</figure>

<p>Finally we would like to mention that it is possible to choose a sequence of polynomials <img src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(\lambda, \rho)" class="latex" title="(\lambda, \rho)" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" /> approaches the Shannon limit. While it is non-trivial to sample exactly from this distribution, good approximations exist and LDPC codes can achieve close to channel capacity over binary erasure channels.</p>

<h3>The solution space in <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D%5Cleq+p+%5Cleq+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}\leq p \leq p_{c}" class="latex" title="p_{d}\leq p \leq p_{c}" /></h3>

<h4>The energy landscape of LDPC decoding</h4>

<p>We have already shown the exact location of the <img src="https://s0.wp.com/latex.php?latex=p_%7BMAP%7D+%3D+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{MAP} = p_{c}" class="latex" title="p_{MAP} = p_{c}" /> threshold above which decoding is not possible for the LDPC ensemble and have also investigated the point at which the BP algorithm fails, <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />.</p>

<p>It should not be surprising to us that any given algorithm we attempt to throw at the problem fails at a certain point below <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />. In fact there are many simple, random algorithms from the class of Markov-chain Monte Carlos which give fast run times but which fail at values far below even <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />. The failing point of a particular algorithm, per se, is not necessarily very significant. We shouldn’t expect that any given algorithm, besides explicitly calculating the symbol MAP by traversing the entire codebook, would be able to achieve <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />.</p>

<p>What is of interest to us here, is that <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> marks a provable threshold in the solution space of LDPC decoding during which it is likely no locally-based methods, and therefore no <em>fast </em>algorithms can decode with nonzero probability. We will show later precisely the number and energy levels of these metastable states for the BEC. Proof of this transition for other channel types is outside the scope of this lecture.</p>

<p>In this section we will rephrase decoding as an energy minimization problem and use three techniques to explore the existence of metastable states and their effect on local search algorithms.</p>

<p>In particular we will first use a generic local search algorithm that attempts to approximately solve energy minimization expression of decoding.</p>

<p>We will next use a more sophisticated Markov chain Monte Carlo method called simulated annealing. Simulated annealing is useful because it offers a perspective that more closely models real physical processes and that has the property that its convergence behavior closely mimics the structure of the metastable configurations.</p>

<h4>Energy minimization problem</h4>

<p>To begin, we will reframe our problem in terms of constraint satisfaction.</p>

<p>The codewords of an LDPC code are solutions of a CSP. The variables are the bits of the word and the constraints are the parity check equations. Though this means our constraints are a system of linear equations, our problem here is made more complicated by the fact that we are searching for not just ANY solution to the system but for a particular solution, namely the transmitted codeword.</p>

<p>The received message <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{y}}" class="latex" title="\underbar{\textit{y}}" /> tells us where we should look for the solution.</p>

<p>Assume we are using the binary-input, memoryless, output-symmetric channel with transition probability <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D%28y+%7C+x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbf{Q}(y | x)" class="latex" title="\mathbf{Q}(y | x)" />.</p>

<p>The probability that <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{x}}" class="latex" title="\underbar{\textit{x}}" /> was the transmitted codeword, given <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{y}}" class="latex" title="\underbar{\textit{y}}" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%7C+%5Cunderbar%7B%5Ctextit%7By%7D%7D%29+%3D+%5Cmu_%7By%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{P}(\underbar{\textit{x}} | \underbar{\textit{y}}) = \mu_{y}(\underbar{\textit{x}})" class="latex" title="\mathbb{P}(\underbar{\textit{x}} | \underbar{\textit{y}}) = \mu_{y}(\underbar{\textit{x}})" /></p>

<p>Where</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%7D%28%5Cunderline%7Bx%7D%7C%5Cunderline%7By%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28y%29%7D%5Cprod_%7Bi%3D1%7D%5ENQ%28y_i%7Cx_i%29%5Cprod_%7Ba%3D1%7D%5EM%5Cmathbb%7BI%7D%28x_%7Bi_1%5Ea%7D%5Cotimes+...+%5Cotimes+x_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{y}(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^NQ(y_i|x_i)\prod_{a=1}^M\mathbb{I}(x_{i_1^a}\otimes ... \otimes x_{k(a)^a} = 0)" class="latex" title="\mu_{y}(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^NQ(y_i|x_i)\prod_{a=1}^M\mathbb{I}(x_{i_1^a}\otimes ... \otimes x_{k(a)^a} = 0)" /> (10)</p>

<p>We can associate an optimization problem with this code. In particular, define <img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underbar{\textit{x}})" class="latex" title="E(\underbar{\textit{x}})" /> to be twice the number of parity check equations violated by <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{y}}" class="latex" title="\underbar{\textit{y}}" />.</p>

<p>We have already discussed how symbol MAP computes the marginals of the distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{y}(\underbar{\textit{x}})" class="latex" title="\mu_{y}(\underbar{\textit{x}})" /> and how word MAP finds its argmax.</p>

<p>We shall here discuss two related problems</p>

<ul><li>optimizing the energy function within a subset of the configuration space defined by the received word</li><li>sampling from a ’tilted’ Boltzmann distribution associated with the energy</li></ul>

<p>Define the log-likelihood of x being the input given the received y to be</p>

<p><img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderline%7By%7D%7D%28%5Cunderline%7Bx%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5EN+Q%28y_i%7Cx_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underline{y}}(\underline{x}) = \sum_{i=1}^N Q(y_i|x_i)" class="latex" title="L_{\underline{y}}(\underline{x}) = \sum_{i=1}^N Q(y_i|x_i)" /> (11)</p>

<p>If we assume WLOG that the all zero codeword was transmitted, by the law of large numbers, for large N the log-likelihood <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" class="latex" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" /> of this codeword is close to <img src="https://s0.wp.com/latex.php?latex=-Nh&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="-Nh" class="latex" title="-Nh" /> where <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h" class="latex" title="h" /> is the channel entropy. The probability of an order-N deviation away from this value is exponentially small.</p>

<p>This suggests that we should look for the transmitted codeword amongst those <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%5Cin+%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{x}} \in \mathbb{C}" class="latex" title="\underbar{\textit{x}} \in \mathbb{C}" /> such that <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" class="latex" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" /> is close to h.</p>

<p>The constraint version of our decoding strategy – known as typical-pairs decoding – is thus, find <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{x}}" class="latex" title="\underbar{\textit{x}}" /> such that <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" />. This constraint will be referred to as the `distance constraint’ and we should consider the situation where if exactly one codeword satisfies the distance constraint, return it.</p>

<p>Since codewords are global energy minima (<img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underbar{\textit{x}}) = 0" class="latex" title="E(\underbar{\textit{x}}) = 0" /> for all <img src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%5Cin+%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underbar{\textit{x}} \in \mathbb{C}" class="latex" title="\underbar{\textit{x}} \in \mathbb{C}" />), we can phrase typical-pairs decoding as an optimization problem</p>

<p>Minimize <img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underbar{\textit{x}})" class="latex" title="E(\underbar{\textit{x}})" /> subject to <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" />.</p>

<p>This decoding succeeds iff the minimum is non-degenerate. This happens with high probability for <img src="https://s0.wp.com/latex.php?latex=p+%3C+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p &lt; p_{c}" class="latex" title="p &lt; p_{c}" /> and with zero probability for <img src="https://s0.wp.com/latex.php?latex=p+%3E+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p &gt; p_{c}" class="latex" title="p &gt; p_{c}" />. In particular, there are exponentially many degenerate energy minima above <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" />.</p>

<p>Similar to what we have seen elsewhere in the course, there exists a generically intermediate regime <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D%5Cleq+p+%5Cleq+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}\leq p \leq p_{c}" class="latex" title="p_{d}\leq p \leq p_{c}" /> in which the global energy minimum is still the correct codeword bu there is an exponentially large number of local energy minima obscuring it (see figure 6).</p>

<p>What is so special about BP is that the threshold at which these exponentially many metastable states proliferate is exactly the algorithmic threshold <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> for BP which we proved earlier.</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/cartoon.png?w=600" alt="" class="wp-image-6456" /><strong>Fig 6</strong> A cartoon landscape for the Energy function defined above (number of violated checks). <em>Left</em>: The energy has a unique global minimum with <img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderline%7Bx%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underline{x}) = 0" class="latex" title="E(\underline{x}) = 0" /> (the transmitted codeword) and no local minima. <em>Center</em>: many deep local minima appear, although the global minimum is non degenerate. <em>Right</em>: more than one codeword is compatible with the likelihood constraint, the global minimum <img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderline%7Bx%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underline{x}) = 0" class="latex" title="E(\underline{x}) = 0" /> is degenerate <em>adapted from Mezard and Montanari, 2009 Chapt 21</em></figure>

<p>While finding solutions <img src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E(\underbar{\textit{x}}) = 0" class="latex" title="E(\underbar{\textit{x}}) = 0" /> amounts to Gaussian elimination, the constraint <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" /> is not a linear constraint. Thus one needs to use some sort of more advanced search procedure to find satisfying vectors <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" />.</p>

<p>We will show that if one resorts to local-search-based algorithms, the metastable states above <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> block the algorithm. Furthermore, we suggest that the behavior of the local algorithms discussed below are typical of all local search algorithms (including BP) and that it is very likely the case that no fast algorithm exists capable of finding global energy minima without getting caught in the metastable states which proliferate above <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" />.</p>

<p>Below is the simplest of local search algorithms, <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta" class="latex" title="\Delta" />-local search.</p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/deltasearch.png?w=548&amp;h=294" alt="" height="294" class="wp-image-6457" width="548" /><strong>Fig 7</strong><em> Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>

<p>Delta search typefies local search algorithms. It walks semi-randomly through the landscape searching for low energy configurations. Its parameter is defined such that, when stuck in a metastable state it can climb out of it in polynomial time if the steepness of its energy barrier is <img src="https://s0.wp.com/latex.php?latex=%5Cleq+%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\leq \Delta" class="latex" title="\leq \Delta" />. Thus its failure in the <img src="https://s0.wp.com/latex.php?latex=p+%5Cgeq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p \geq p_{d}" class="latex" title="p \geq p_{d}" /> region suggests that there are no barriers of constant size and that barriers of order N are the norm.</p>

<h4>MCMC and the relaxation time of a random walk</h4>

<p>We can understand the geometry of the metastable states in greater detail by reframing our MAP problem as follows:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28%5Cbeta%29%7Dexp%7B-%5Cbeta+%5Ccdot+E%28%5Cunderline%7Bx%7D%29%7D%5Cprod_%7Bi%3D1%7D%5EN+Q%28y_i%7Cx_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu_{y,\beta}(\underline{x}) = \frac{1}{Z(\beta)}exp{-\beta \cdot E(\underline{x})}\prod_{i=1}^N Q(y_i|x_i)" class="latex" title="\mu_{y,\beta}(\underline{x}) = \frac{1}{Z(\beta)}exp{-\beta \cdot E(\underline{x})}\prod_{i=1}^N Q(y_i|x_i)" /> (12)</p>

<p>This form is referred to as the `tilted’ Boltzmann because it is a Boltzmann distribution biased by the likelihood function.</p>

<p>In the low temperature limit this reduces to eqn 10 because it finds support only over words in the codebook.</p>

<p>This distribution more closely mimics physical systems. For nonzero temperature it allows support over vectors which are not actually in our codebook but still have low distance to our received message and have low energy – this allows us to probe the metastable states which trap our local algorithms. This is referred to as a code with `soft’ parity check constraints as our distribution permits decodings which fail some checks.</p>

<p>We will use the following algorithm excerpted from Mezard and Montanari Chapt 21:</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/anneal.png?w=600" alt="" class="wp-image-6458" /><strong>Fig 8</strong> <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>

<p>Where a Glauber update consists of scanning through the bits of the current proposed configuration and flipping the value of bit <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> with probability</p>

<p><img src="https://s0.wp.com/latex.php?latex=w_%7Bi%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+%5Cfrac%7B%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%5E%7B%28i%29%7D%29%7D%7B%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%5E%7B%28i%29%7D%29+%2B+%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_{i}(\underbar{\textit{x}}) = \frac{\mu_{y,\beta}(\underline{x}^{(i)})}{\mu_{y,\beta}(\underline{x}^{(i)}) + \mu_{y,\beta}(\underline{x})}" class="latex" title="w_{i}(\underbar{\textit{x}}) = \frac{\mu_{y,\beta}(\underline{x}^{(i)})}{\mu_{y,\beta}(\underline{x}^{(i)}) + \mu_{y,\beta}(\underline{x})}" /> (13)</p>

<p>Where <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" /> is the current configuration and <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D%5E%7B%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}^{(i)}" class="latex" title="\underline{x}^{(i)}" /> is the configuration obtained by flipping <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}" class="latex" title="\underline{x}" />‘s <img src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i^{th}" class="latex" title="i^{th}" /> bit</p>

<p>This method is a spin-off of traditional Markov chain Monte-Carlo algorithms with the variation that we lower the temperature according to an annealing schedule that initially assigns probability to all states proportional to the likelihood component of equation 12, allowing the chain to randomly sample the configuration space in the neighborhood of the received noisy word, until in the low temperature limit it becomes concentrated near to configurations which are proper codewords.</p>

<p>This method is useful to us because MCMCs are good models of how randomized methods of local searching for optimal configurations occurs in physical systems. Furthermore, the convergence of MCMCs and the time it takes them to converge tells us both the properties of the energy wells they terminate in and the barriers between minima in the energy landscape.</p>

<p>Let’s now show a property relating convergence times of MCMCs and energy barriers known as the Arrhenius law.</p>

<p>If we take the example of using a simple MCMC random walk with the update rule below over the following landscape</p>

<p><img src="https://s0.wp.com/latex.php?latex=w%28x%5Crightarrow+x%27%29+%3D+min+%5C%7Be%5E%7B-%5Cbeta+%5BE%28x%27%29-E%28x%29%5D%7D%2C%7E1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w(x\rightarrow x') = min \{e^{-\beta [E(x')-E(x)]},~1\}" class="latex" title="w(x\rightarrow x') = min \{e^{-\beta [E(x')-E(x)]},~1\}" /></p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/fig1311.png?w=540&amp;h=291" alt="" height="291" class="wp-image-6462" width="540" /><strong>Fig 9</strong> This represents a random walk along a line in which there are two ground states separated by an energy barrier of height <img src="https://s0.wp.com/latex.php?latex=%5CDelta+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta E" class="latex" title="\Delta E" />.  <em>Excerpted from Mezard and Montanari, 2009 Chapt 13</em></figure>

<p>We find that the expected number of time steps to cross from one well to another is governed by the Arrhenius law <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%5Capprox+exp%7B%5Cbeta+%5CDelta+E%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tau \approx exp{\beta \Delta E}" class="latex" title="\tau \approx exp{\beta \Delta E}" />.</p>

<p>In general, if there exists a largest energy barrier between any two components of the configuration space (also known as the bottleneck) the time it takes to sample both components, also known as the relaxation time of the MCMC is <img src="https://s0.wp.com/latex.php?latex=%5Ctau_%7Bexp%7D+%5Cgeq+O%28e%5E%7B%5Cbeta+%5CDelta+E%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tau_{exp} \geq O(e^{\beta \Delta E})" class="latex" title="\tau_{exp} \geq O(e^{\beta \Delta E})" /></p>

<p>With this in mind, we can apply our simulated annealing MCMC to LDPC decoding and examine the properties of the bottlenecks, or metastable states, in our configuration landscape.</p>

<h4>Exact values of the metastable energy states for the BEC</h4>

<p>It is a well-known consequence of the 1RSB cavity method that the number of metastable states of energy <img src="https://s0.wp.com/latex.php?latex=Ne&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Ne" class="latex" title="Ne" /> grows like <img src="https://s0.wp.com/latex.php?latex=exp%28N%5CSigma%5E%7Be%7D%28e%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="exp(N\Sigma^{e}(e))" class="latex" title="exp(N\Sigma^{e}(e))" /> where <img src="https://s0.wp.com/latex.php?latex=%5CSigma%5E%7Be%7D%28e%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma^{e}(e)" class="latex" title="\Sigma^{e}(e)" /> is known as the energetic complexity function, a function whose form is implied by the 1RSB cavity equations. This computation can be carried out using a method called Survey Propagation which constructs a factor graph of the messages passed in the original BP factor graph and estimates the values of the marginals of the messages via another round of BP (hence the name 1-step RSB).</p>

<p>Neglecting the actual form of the calculations I will show the following approximate results for the BEC.</p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/fig213.png?w=602&amp;h=254" alt="" height="254" class="wp-image-6463" width="602" /><strong>Fig 10</strong> Metastable states for random elements of the (3,6) regular ensemble used over the BEC (<img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+.4294&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d = .4294" class="latex" title="\epsilon_d = .4294" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c+%3D+.4882&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_c = .4882" class="latex" title="\epsilon_c = .4882" />. <em>Left</em>: the complexity function as a function of energy density above <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d" class="latex" title="\epsilon_d" />. <em>Right</em>: the maximum and minimum energy densities of metastable states as a function of the erasure probability. Note that at <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cleq+.45+%5Cleq+%5Cepsilon_c+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_d \leq .45 \leq \epsilon_c " class="latex" title="\epsilon_d \leq .45 \leq \epsilon_c " />latex the curve is positive only for non zero metastable energy densities. This indicates exponentially many metastable states. At erasure rates above <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon_c" class="latex" title="\epsilon_c" /> there are exponentially many degenerate ground states. <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>

<p>In the regime <img src="https://s0.wp.com/latex.php?latex=p+%5Cgeq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p \geq p_{d}" class="latex" title="p \geq p_{d}" /> there exists a zero-energy word corresponding to the correct solution. On top of this, there exist non-trivial solutions to the 1RSB method yielding a complexity curve positive in the regime (<img src="https://s0.wp.com/latex.php?latex=e_%7Bc%7D%2C+e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e_{c}, e_{d}" class="latex" title="e_{c}, e_{d}" />). The positive complexity means that there are exponentially many such states and their finite energy means they violate a finite fraction of the parity checks, making their energy wells relatively deep.</p>

<p>As the error rate of the channel increases above <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> the minimum energy of the metastable state reaches zero continuously. This means at noise levels above <img src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{c}" class="latex" title="p_{c}" /> there are an exponential number of zero-energy states corresponding to configurations which aren’t code words. These codewords are separated by energy barriers <img src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N)" class="latex" title="O(N)" /> thus making the relaxation-time of local algorithms, by the Arrhenius law <img src="https://s0.wp.com/latex.php?latex=exp%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="exp(N)" class="latex" title="exp(N)" /> in this regime.</p>

<figure class="wp-block-image is-resized"><img src="https://windowsontheory.files.wordpress.com/2018/12/fig214.png?w=622&amp;h=229" alt="" height="229" class="wp-image-6464" width="622" /><strong>Fig 11</strong> Decoding random codes from the (3,6) ensemble over the BEC. In both cases <img src="https://s0.wp.com/latex.php?latex=N+%3D+10%5E4&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N = 10^4" class="latex" title="N = 10^4" />, and the annealing schedule consists of <img src="https://s0.wp.com/latex.php?latex=t_max+%3D+1000&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t_max = 1000" class="latex" title="t_max = 1000" /> equidistant temperatures in <img src="https://s0.wp.com/latex.php?latex=T+%3D+1+%2F+%5Cbeta+%5Cin+%5B0%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="T = 1 / \beta \in [0,1]" class="latex" title="T = 1 / \beta \in [0,1]" />. <em>Left</em>: indicates convergence to the correct ground state at <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+.4+%3C+%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon = .4 &lt; \epsilon_d" class="latex" title="\epsilon = .4 &lt; \epsilon_d" />. <em>Right</em>: indicates convergence to approximately the energy density of the highest metastable states <img src="https://s0.wp.com/latex.php?latex=e_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e_d" class="latex" title="e_d" /> (as calculated by the complexity function via 1RSB) for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+.6+%3E+%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon = .6 &gt; \epsilon_{c}" class="latex" title="\epsilon = .6 &gt; \epsilon_{c}" />. <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em><br /><br /></figure>

<p>Here you can see a rough sketch of convergence of the simulated annealing algorithm. As the temperature decreases in the <img src="https://s0.wp.com/latex.php?latex=p+%5Cleq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p \leq p_{d}" class="latex" title="p \leq p_{d}" /> regime the algorithm converges to a 0 energy ground state. In the figure on the right we can see that simulated annealing converges to the horizontal line here which corresponds to the energy of the highest metastable state <img src="https://s0.wp.com/latex.php?latex=e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e_{d}" class="latex" title="e_{d}" /> for the BEC at <img src="https://s0.wp.com/latex.php?latex=p+%3D+.6&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p = .6" class="latex" title="p = .6" />.</p>

<p>Thus we see our local search algorithms end up being attracted to the highest energy of the metastable state.</p>

<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2018/12/fig215.png?w=600" alt="" class="wp-image-6465" /><strong>Fig 12</strong> Decoding random codes as in figure 11. Here we plot the minimum energy density achieved through simulated annealing plotted as a function of the erasure probability of the BEC. The continuous line indicates the highest lying metastable states as calculated from the complexity function via 1RSB.<br /><em> Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>

<p>Though there is not necessarily an exact correspondence between the residual energy at T=0 for simulated annealing and the highest metastable state <img src="https://s0.wp.com/latex.php?latex=e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e_{d}" class="latex" title="e_{d}" /> we see that across all values of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> that at T=0, <img src="https://s0.wp.com/latex.php?latex=e_%7Bann%7D+%5Capprox+e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="e_{ann} \approx e_{d}" class="latex" title="e_{ann} \approx e_{d}" /> suggesting local search tends to get caught in the deepest metastable energy wells.</p>

<p>This discussion shows that the algorithmic threshold of BP, <img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d}" class="latex" title="p_{d}" /> indicates the onset of a truly different regime within the energy landscape of the codebook. Metastable states of <img src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(N)" class="latex" title="O(N)" /> hight proliferate and become exponentially difficult to escape from via local search methods. Thus the failure of BP likely indicates a regime in which no fast algorithms can perform decoding, even though decoding is still theoretically possible when below <img src="https://s0.wp.com/latex.php?latex=p_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_c" class="latex" title="p_c" />, e.g. via exhaustive search of the codebook.</p>

<h3>Appendix A: Random Code Ensembles</h3>

<p>In an RCE, encoding maps applied to the information sequence are chosen with uniform probability over a solution space. Two decoding schemes are often used and applied to the noise – word MAP and symbol MAP decoding. MAP, otherwise known as “maximum <em>a priori</em> probability” works by maximizing the probability distribution to output the most probable transmission. Word MAP decoding schemes output the codeword with the highest probability by minimizing the block error probability, which is otherwise known as the probability with respect to the channel distribution that the decoded word is different than the true transmitted word. Symbol MAP decoding, on the other hand, minimizes the fraction of incorrect bits averaged over the transmitted code word (bit error rate).</p>

<p>RCE code is defined by the codebook in Hamming space, or the set of all binary strings of length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" />. In a Hamming space characterized by uniform probability, the number of codewords at a given Hamming distance are a function of the distance enumerator. Distance enumerators take as parameters different weights, given that probabilities of codewords are independent of each other. The distance enumerator shows that, for small enough fractional distance from the true message <img src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_0" class="latex" title="x_0" />, the growth rate is negative and the average number of codewords at small distance from <img src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_0" class="latex" title="x_0" /> vanishes exponentially with N. The Gilbert-Varshamov distance, a lower bound threshold, shows that the the average number of codewords is exponentially large at points where the weight numerator is concentrated.</p>

<p>We look at the performance of RCE code in communication over the Binary Symmetric Channel (BSC), where it is assumed that there is a probability p that transmitted bits will be “flipped” (i.e. with probability p, 1 becomes 0 and 0 becomes 1). With BSCs, channel input and output are the same length N sequences of bits. At larger noise levels, there are an exponential number of codewords closer to <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="y" class="latex" title="y" /> and decoding is unsuccessful. However, decoding via the symbol MAP decoding scheme shows that the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th bit is decoded by maximizing the marginal probability and amasses contributions from all codewords in the set. Above a threshold, the bit error rate is the same as if the message was transmitted without encoding and decoding, but below this, the RCE seems to work quite well in transmission.</p>

<p>Finite temperature decoding has also been looked at as an interpolation between the two MAP decoding schemes. At low noise, a completely ordered phase can be observed as compared to a glassy phase at higher noise channels. Similar to the a statistical physics model, we can also note an entropy dominated paramagnetic phase at higher temperatures.</p>

<p>Each decoding scheme can be analogized to “sphere packing”, where each probability in the Hamming space distribution represents a sphere of radius r. Decoding schemes have partitions in the Hamming space, so these spheres must be disjoint. If not, intersecting spheres must be eliminated. The lower bound of the remaining spheres is then given by Gilbert-Varshamov bound, whereas the upper bound is dictated by the Hamming distance.</p>

<p>Another random code beside the RCE is the RLC, or random linear code. Encoding in an RLC forms a scheme similar to a linear map, of which all points are equiprobable. The code is specified by an <img src="https://s0.wp.com/latex.php?latex=N+x+M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N x M" class="latex" title="N x M" /> binary matrix, otherwise known as the generating matrix, and it is projected to be error-free below the Shannon capacity.</p>

<p>There are several sources of randomness in codes. Codes are chosen randomly from an ensemble and the codeword to be transmitted is chosen with uniform probability from the code, according to the theorem of source-channel separation. The channel output is then distributed according to a probabilistic process accounting for channel noise and decoding is done by constructing another probability distribution over possible channel inputs and by estimating its signal bit marginal. The decision on the <em>i</em>-th bit is dependent on the distribution. Thus, complications may arise in distinguishing between the two levels of randomness: code, channel input, and noise (“quenched” disorder) versus Boltzmann probability distributions.</p>

<h3>Appendix B: Weight enumerators and code performance</h3>

<p>The geometric properties of the LDPC codebooks is given by studying the distance enumerator <img src="https://s0.wp.com/latex.php?latex=N_%7B%5Cunderline%7Bx%7D0%7D%28d%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N_{\underline{x}0}(d)" class="latex" title="N_{\underline{x}0}(d)" /> to give the number of codewords at Hamming distance <em>d</em> from <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}_0" class="latex" title="\underline{x}_0" />. This takes all-zeros codewords as the reference and uses the weight enumerator, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%28w%29%3D%5Cmathbb%7BN%7D%7B%5Cunderline%7Bx_0%7D%7D%28d%3Dw%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{N}(w)=\mathbb{N}{\underline{x_0}}(d=w)" class="latex" title="\mathbb{N}(w)=\mathbb{N}{\underline{x_0}}(d=w)" /> as the denomination (number of codewords having weight equal to <em>w</em>). To estimate the expected weight enumerator <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%29%3D%5Cmathbb%7BE%7D%5Cmathbb%7BN%7D%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(w)=\mathbb{E}\mathbb{N}(w)" class="latex" title="\mathcal{N}(w)=\mathbb{E}\mathbb{N}(w)" /> for a random code in the <img src="https://s0.wp.com/latex.php?latex=LDPC_N%28%5CLambda%2CP%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="LDPC_N(\Lambda,P)" class="latex" title="LDPC_N(\Lambda,P)" /> ensemble, we know that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{N}(w)" class="latex" title="\mathbb{N}(w)" /> grows exponentially in block-length <em>N</em>, and that each codeword has a weight <img src="https://s0.wp.com/latex.php?latex=w%3DNw&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w=Nw" class="latex" title="w=Nw" /> that grows linearly with N. The exponential growth rate <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w)" class="latex" title="\phi(w)" /> is defined by</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%3DNw%29+%3D+e%5E%7BN+%5Cphi+%28w%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(w=Nw) = e^{N \phi (w)}" class="latex" title="\mathcal{N}(w=Nw) = e^{N \phi (w)}" /> (14)</p>

<p>denoting an ‘annealed average’, or a disordered system that could be dominated by rare instances in the ensemble. This gives an upper bound on the number of ‘colored factor graphs’ that have an even number of weighted incident edges divided by the total number of factor graphs in the ensemble.</p>

<p>On the other hand, for graphs of fixed degrees with N variable nodes of degree<em> l</em> and M function nodes of degree <em>k</em>, the total number of edges F is given by <img src="https://s0.wp.com/latex.php?latex=F%3DMk%3DNl&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F=Mk=Nl" class="latex" title="F=Mk=Nl" />. A valid colored graph would have <img src="https://s0.wp.com/latex.php?latex=E%3Dwl&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="E=wl" class="latex" title="E=wl" /> edges, with the number of variable nodes given in <img src="https://s0.wp.com/latex.php?latex=%7BN%5Cchoose+w%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{N\choose w}" class="latex" title="{N\choose w}" /> ways, l assignments of weighted sockets to nodes, and l assignments of unweighted sockets to nodes outside the set. If we take <img src="https://s0.wp.com/latex.php?latex=m_r&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="m_r" class="latex" title="m_r" /> to denote the number of function nodes with weighted sockets under the constraints of <img src="https://s0.wp.com/latex.php?latex=%5CSigma_%7Br%3D0%7D%5Ekm_r%3DM&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma_{r=0}^km_r=M" class="latex" title="\Sigma_{r=0}^km_r=M" /> and <img src="https://s0.wp.com/latex.php?latex=%5CSigma_%7Br%3D0%7D%5Ekrm_r%3Dlw&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Sigma_{r=0}^krm_r=lw" class="latex" title="\Sigma_{r=0}^krm_r=lw" />, we find the number of ways to color the function node sockets by</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%28k%2CM%2Cw%29+%3D+%5Csum_%7Bm_%7B0%7D%2C...m_%7Bk%7D%7D%5E%7Beven%7D%7BM%5Cchoose+m_%7B0%7D%2C...%2Cm_%7Bk%7D%7D%5Cprod_%7Br%7D%7Bk%5Cchoose+r%7D%5E%7Bm_%7Br%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{C}(k,M,w) = \sum_{m_{0},...m_{k}}^{even}{M\choose m_{0},...,m_{k}}\prod_{r}{k\choose r}^{m_{r}}" class="latex" title="\mathbb{C}(k,M,w) = \sum_{m_{0},...m_{k}}^{even}{M\choose m_{0},...,m_{k}}\prod_{r}{k\choose r}^{m_{r}}" /> (15)</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D%5CBig%28%5Csum_%7Br%3D0%7D%5Ekm_r%3DM%5CBig%29%5Cmathbb%7BI%7D%5CBig%28%5Csum_%7Br%3D0%7D%5Ekrm_r%3Dlw%5CBig%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{I}\Big(\sum_{r=0}^km_r=M\Big)\mathbb{I}\Big(\sum_{r=0}^krm_r=lw\Big)" class="latex" title="\mathbb{I}\Big(\sum_{r=0}^km_r=M\Big)\mathbb{I}\Big(\sum_{r=0}^krm_r=lw\Big)" /> (16)</p>

<p>If we aim to join variable and check nodes so that colorings are matched, knowing that there are <img src="https://s0.wp.com/latex.php?latex=%28lw%29%21%28F-lw%29%21&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(lw)!(F-lw)!" class="latex" title="(lw)!(F-lw)!" /> possible matchings in each ensemble element, this yields the following formula:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%29%3D%5Cfrac%7B%28lw%29%21%28F-lw%29%21%7D%7BF%21%7D%7BN%5Cchoose+w%7D%5Cmathbb%7BC%7D%28k%2CM%2Cw%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(w)=\frac{(lw)!(F-lw)!}{F!}{N\choose w}\mathbb{C}(k,M,w)" class="latex" title="\mathcal{N}(w)=\frac{(lw)!(F-lw)!}{F!}{N\choose w}\mathbb{C}(k,M,w)" /> (17)</p>

<p>At low noise limits, code performance depends on the existence of codewords at distances close to the transmitted codeword. Starting with degree 1 and knowing that the parametric representation for weights is given by </p>

<p><img src="https://s0.wp.com/latex.php?latex=w+%3D+%5Csum_%7Bl%3D1%7D%5E%7Bl_%7Bmax%7D%7D%5CLambda_l%5Cfrac%7Bxy%5El%7D%7B1%2Bxy%5El%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w = \sum_{l=1}^{l_{max}}\Lambda_l\frac{xy^l}{1+xy^l}" class="latex" title="w = \sum_{l=1}^{l_{max}}\Lambda_l\frac{xy^l}{1+xy^l}" /> (18)</p>

<p>derive that </p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29+%3D+-%5Cfrac%7B1%7D%7B2%7Dw%5Clog%28w%2F%5CLambda_1%5E2%29%2BO%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w) = -\frac{1}{2}w\log(w/\Lambda_1^2)+O(w)" class="latex" title="\phi(w) = -\frac{1}{2}w\log(w/\Lambda_1^2)+O(w)" /> (19)</p>

<p>when <img src="https://s0.wp.com/latex.php?latex=x%2Cy%2Cz&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x,y,z" class="latex" title="x,y,z" /> scale to <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sqrt{w}" class="latex" title="\sqrt{w}" />. This shows that the exponential growth rate <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w)" class="latex" title="\phi(w)" /> is strictly positive when <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w" class="latex" title="w" /> is sufficiently small, and that the expected number of codewords within a small Hamming distance from a given codeword is exponential in N. If we take the logarithm of the expected weight enumerator and plot this versus the reduced weight <img src="https://s0.wp.com/latex.php?latex=w%3Dw%2FN&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w=w/N" class="latex" title="w=w/N" /> for an irregular code with <img src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l_{min}=1" class="latex" title="l_{min}=1" />, we see that <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w)" class="latex" title="\phi(w)" /> is positive near the origin, but that its dervative diverges as <img src="https://s0.wp.com/latex.php?latex=w%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w\rightarrow 0" class="latex" title="w\rightarrow 0" />. Since this means that each codeword is surrounded by a large number of very close other codewords, this makes the code a very bad ECC and thus, makes it hard to discriminate between codewords at Hamming distances <img src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="O(1)" class="latex" title="O(1)" /> with noisy observations. Applying this same logic to <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l" class="latex" title="l" /> of min 2, we still observe that <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w)" class="latex" title="\phi(w)" /> tends to 0 more quickly as <img src="https://s0.wp.com/latex.php?latex=w%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w\rightarrow 0" class="latex" title="w\rightarrow 0" /> in the present case. If we assume that this holds beyond the asymptotic regime, we get</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cbar%7B%5Cmathcal%7BN%7D%7D%28w%29+%3D+e%5E%7BAw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bar{\mathcal{N}}(w) = e^{Aw}" class="latex" title="\bar{\mathcal{N}}(w) = e^{Aw}" /> (20)</p>

<p>or that the number of codewords around a particular codeword is <img src="https://s0.wp.com/latex.php?latex=o%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="o(N)" class="latex" title="o(N)" /> until a Hamming distance <img src="https://s0.wp.com/latex.php?latex=d_%2A+%5Csimeq+%5Clog+N%2FA&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d_* \simeq \log N/A" class="latex" title="d_* \simeq \log N/A" />, otherwise known as the “effective minimum distance”. For <img src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D+%5Cgeq+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l_{min} \geq 3" class="latex" title="l_{min} \geq 3" />, we find:</p>

<p><br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29+%5Csimeq+%5CBig%28%5Cfrac%7Bl_%7Bmin%7D-2%7D%7B2%7D%5CBig%29w%5Clog%5CBig%28%5Cfrac%7Bw%7D%7B%5CLambda_%7Bl_min%7D%7D%5CBig%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(w) \simeq \Big(\frac{l_{min}-2}{2}\Big)w\log\Big(\frac{w}{\Lambda_{l_min}}\Big)" class="latex" title="\phi(w) \simeq \Big(\frac{l_{min}-2}{2}\Big)w\log\Big(\frac{w}{\Lambda_{l_min}}\Big)" /> (21)</p>

<p>suggesting that LDPC codes with this property have good short distance behavior. Thus, any error that changes a fraction of the bits smaller than <img src="https://s0.wp.com/latex.php?latex=w_%7B%2A%7D%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w_{*}/2" class="latex" title="w_{*}/2" /> can be corrected in the absence of codewords within an extensive distance <img src="https://s0.wp.com/latex.php?latex=Nw_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Nw_{*}" class="latex" title="Nw_{*}" />.</p>

<p>Let us now focus on the capacity of LDPC codes to correct typical errors in a probabilistic channel. For binary symmetric channels that flip each transmitted bit independently with probability <img src="https://s0.wp.com/latex.php?latex=p%3C%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p&lt;\frac{1}{2}" class="latex" title="p&lt;\frac{1}{2}" />. If the all-zero codeword <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D%5E%7B%280%29%7D+%3D%5Cunderline%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{x}^{(0)} =\underline{0}" class="latex" title="\underline{x}^{(0)} =\underline{0}" /> has been transmitted as <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" />, whose components are iid random variables that take value 0 with probability <img src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1-p" class="latex" title="1-p" /> and value 1 with probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, then we use the MAP decoding strategy to minimize the block error rate and output the codeword closest to the channel output <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" />. The expectation value of the code ensemble <img src="https://s0.wp.com/latex.php?latex=P_B+%3D+%5Cmathbb%7BE%7DP_B%28%5Cmathbb%7BC%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_B = \mathbb{E}P_B(\mathbb{C})" class="latex" title="P_B = \mathbb{E}P_B(\mathbb{C})" /> is an indicator of code ensemble performances. We will show that, as <img src="https://s0.wp.com/latex.php?latex=N%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N\rightarrow \infty" class="latex" title="N\rightarrow \infty" />, codes with <img src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D%5Cgeq+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="l_{min}\geq 3" class="latex" title="l_{min}\geq 3" /> will undergo a phase transition separating a low noise from a high noise phase. To derive a lower bound for the capacity of LDPC codes in a BSC channel, we take <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%3D2%5E%7BNR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{N}=2^{NR}" class="latex" title="\mathbb{N}=2^{NR}" /> as the size of the codebook <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{C}" class="latex" title="\mathbb{C}" /> and, by union bound:</p>

<p><img src="https://s0.wp.com/latex.php?latex=P_%7BB%7D%28%5Cmathbb%7BC%7D%29%3D+%5Cmathbb%7BP%7D%5CBig%5C%7B%5Cexists+%5Calpha+%5Cneq+0+%5Ctext%7Bs.t.+%7D+d%28%5Cunderline%7Bx%7D%5E%7B%28%5Calpha%29%7D%2C%5Cunderline%7By%7D%29%5Cleq+d%28%5Cunderline%7B0%7D%2C%5Cunderline%7By%7D%29%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{B}(\mathbb{C})= \mathbb{P}\Big\{\exists \alpha \neq 0 \text{s.t. } d(\underline{x}^{(\alpha)},\underline{y})\leq d(\underline{0},\underline{y})\Big\}" class="latex" title="P_{B}(\mathbb{C})= \mathbb{P}\Big\{\exists \alpha \neq 0 \text{s.t. } d(\underline{x}^{(\alpha)},\underline{y})\leq d(\underline{0},\underline{y})\Big\}" /> (22)</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Csum_%7B%5Calpha%3D1%7D%5E%7B%5Ctextit%7BN%7D-1%7D%5Cmathbb%7BP%7D%5CBig%5C%7Bd%28%5Cunderline%7Bx%7D%5E%7B%28%5Calpha%29%7D%2C%5Cunderline%7By%7D%29+%5Cleq+d%28%5Cunderline%7B0%7D%2C%5Cunderline%7By%7D%29%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\leq \sum_{\alpha=1}^{\textit{N}-1}\mathbb{P}\Big\{d(\underline{x}^{(\alpha)},\underline{y}) \leq d(\underline{0},\underline{y})\Big\}" class="latex" title="\leq \sum_{\alpha=1}^{\textit{N}-1}\mathbb{P}\Big\{d(\underline{x}^{(\alpha)},\underline{y}) \leq d(\underline{0},\underline{y})\Big\}" /> (23)</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Csum_%7Bw%3D1%7D%5EN+%5Ctextit%7BN%7D%28w%29e%5E%7B-%5Cgamma+w%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\leq \sum_{w=1}^N \textit{N}(w)e^{-\gamma w}" class="latex" title="\leq \sum_{w=1}^N \textit{N}(w)e^{-\gamma w}" /> (24)</p>

<p>This derivation proves that the block error probability depends on the weight enumerator and the <img src="https://s0.wp.com/latex.php?latex=exp%28-%5Cgamma+w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="exp(-\gamma w)" class="latex" title="exp(-\gamma w)" />. This second term shows that an increase in the weight of the codeword corresponds to their contribution being scaled down by an exponential factor. This is because it is less likely that the received message <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" /> will be closer to a codeword of large weight than to the all-zero codeword. A geometric construction of this phenomena implies that for large enough <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, Shannon’s Theorem implies that <img src="https://s0.wp.com/latex.php?latex=P_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_B" class="latex" title="P_B" /> is bounded away from 0 for any non-vanishing rate <img src="https://s0.wp.com/latex.php?latex=R+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R &gt; 0" class="latex" title="R &gt; 0" /> so that at any p less than the ML threshold for which the <img src="https://s0.wp.com/latex.php?latex=lim_%7BN%5Crightarrow+%5Cinfty%7DP_B%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="lim_{N\rightarrow \infty}P_B=0" class="latex" title="lim_{N\rightarrow \infty}P_B=0" />, one can communicate with an arbitrarily small error probability. At a probability equal to the lower bound, the upper bound on <img src="https://s0.wp.com/latex.php?latex=P_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_B" class="latex" title="P_B" /> is dominated by codewords of weight <img src="https://s0.wp.com/latex.php?latex=w+%5Capprox+N%5CTilde%7Bw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="w \approx N\Tilde{w}" class="latex" title="w \approx N\Tilde{w}" />, suggesting that each time an error occurs, a finite fraction of the its are decoded incorrectly and that this fraction doesn’t change very much per transmission. The construction also illustrates that this fraction of incorrectly decoded bits jumps discontinuously from 0 to a finite value when <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" /> crosses the critical value <img src="https://s0.wp.com/latex.php?latex=p_%7BML%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{ML}" class="latex" title="p_{ML}" />, constituting a “gap.” This gap is close to a factor of 2.</p>

<h3>Appendix C: BP performance</h3>

<p>See figure 2 for an illustration of a factor graph illustrating this relationship. Again, recall that for LDPC code ensembles in large block-length limits, the degree distributions of variable nodes and check nodes are given by <img src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%7B%5CLambda_t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Lambda = {\Lambda_t}" class="latex" title="\Lambda = {\Lambda_t}" /> and <img src="https://s0.wp.com/latex.php?latex=P+%3D+%7BP_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P = {P_k}" class="latex" title="P = {P_k}" /> respectively, where we assume that messages are initialized to <img src="https://s0.wp.com/latex.php?latex=u_%7Ba%5Crightarrow+i%7D%5E%7B%280%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u_{a\rightarrow i}^{(0)} = 0" class="latex" title="u_{a\rightarrow i}^{(0)} = 0" /> for simplicity. This implies that the bit error probability is independent of the transmitted codeword and that therefore, we have the freedom to assume transmission of the all-zero codeword. In analyzing the recursion at the basis of the BP algorithm, we can show that decoding performance improves over time on the basis of symmetry and physical degradation.</p>

<h4>Symmetry</h4>

<p>Symmetry of channel log-likelihood and the variables appearing in density evolution are attributes of a desired BMS channel, suggesting that symmetry is preserved by BP operations in evolution. If we assume that the factor graph associated with an LDPC code is “tree-like”, we can apply BP decoding with a symmetric random initial condition and note that the messages <img src="https://s0.wp.com/latex.php?latex=u_%7Ba%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="u_{a\rightarrow i}^{(t)}" class="latex" title="u_{a\rightarrow i}^{(t)}" /> and <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i\rightarrow a}^{(t)}" class="latex" title="h_{i\rightarrow a}^{(t)}" /> are symmetric variables at all <img src="https://s0.wp.com/latex.php?latex=t%5Cgeq+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\geq 0" class="latex" title="t\geq 0" />. This observance of symmetry is analogous to the Nishimori condition in spin glasses and holds for the MAP log-likelihood of a bit as well.</p>

<h4>Physical degradation</h4>

<p>Let’s first define physical degradation with BMS channels. If we take two channels BMS(1) and BMS(2) denoted by transition matrices</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5C%7BQ_%7B1%7D%28y%7Cx%29%5C%7D%2C+%5C%7BQ_%7B2%7D%28y%7Cx%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{Q_{1}(y|x)\}, \{Q_{2}(y|x)\}" class="latex" title="\{Q_{1}(y|x)\}, \{Q_{2}(y|x)\}" /> and output alphabets <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BY%7D_1%2C%5Cmathbb%7BY%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{Y}_1,\mathbb{Y}_2" class="latex" title="\mathbb{Y}_1,\mathbb{Y}_2" />, then BMS(2) is physically degraded with respect to BMS(1) if there exists a third channel C with input alphabet <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BY%7D_1%2C%5Cmathbb%7BY%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{Y}_1,\mathbb{Y}_2" class="latex" title="\mathbb{Y}_1,\mathbb{Y}_2" /> such that BMS(2) is the concatenation of BMS(1) and C. If we represent transition matrix C as <img src="https://s0.wp.com/latex.php?latex=%5C%7BR%28y_2%7Cy_1%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{R(y_2|y_1)\}" class="latex" title="\{R(y_2|y_1)\}" /> we can represent the above physical degradation as</p>

<p><img src="https://s0.wp.com/latex.php?latex=Q_2%28y_2%7Cx%29+%3D+%5Csum%7By_1+%5Cin+%5Ctextit%7BY%7D_1%7DR%28y_2%7Cy_1%29Q_1%28y_1%7Cx%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Q_2(y_2|x) = \sum{y_1 \in \textit{Y}_1}R(y_2|y_1)Q_1(y_1|x)" class="latex" title="Q_2(y_2|x) = \sum{y_1 \in \textit{Y}_1}R(y_2|y_1)Q_1(y_1|x)" /> (25)</p>

<p> This is analogous to a Markov chain <img src="https://s0.wp.com/latex.php?latex=X+%5Crightarrow+Y_1+%5Crightarrow+Y_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X \rightarrow Y_1 \rightarrow Y_2" class="latex" title="X \rightarrow Y_1 \rightarrow Y_2" /> following partial ordering. Channel reliability is then ordered by measures of conditional entropy and bit error rate. This extends to symmetric random variables, which are associated with BMS channels.</p>

<h4>Thresholds</h4>

<p>We then fix a particular LDPC code and look at BP messages as random variables due to randomness in the vector <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{y}" class="latex" title="\underline{y}" /> with regards to the proposition down below, showing that the bit error rate decreases monotonously with time:</p>

<p><strong>Proposition:</strong><em> If <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cr%7D%28F%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B_{i,r}(F)" class="latex" title="B_{i,r}(F)" /> is a tree, then <img src="https://s0.wp.com/latex.php?latex=h_i%5E%7B%280%29%7D%5Cpreceq+h_i%5E%7B%281%29%7D+%5Cpreceq+...+%5Cpreceq+h_i%5E%7B%28t-1%29%7D+%5Cpreceq+h_i%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_i^{(0)}\preceq h_i^{(1)} \preceq ... \preceq h_i^{(t-1)} \preceq h_i^{(t)}" class="latex" title="h_i^{(0)}\preceq h_i^{(1)} \preceq ... \preceq h_i^{(t-1)} \preceq h_i^{(t)}" /> for any <img src="https://s0.wp.com/latex.php?latex=t%5Cleq+r-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\leq r-1" class="latex" title="t\leq r-1" />. Analogously, if <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%5Crightarrow+a%2Cr%7D%28F%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="B_{i\rightarrow a,r}(F)" class="latex" title="B_{i\rightarrow a,r}(F)" />, then <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D%5E%7B%280%29%7D%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%281%29%7D+%5Cpreceq+...+%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%28t-1%29%7D+%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{i\rightarrow a}^{(0)}\preceq h_{i\rightarrow a}^{(1)} \preceq ... \preceq h_{i\rightarrow a}^{(t-1)} \preceq h_{i\rightarrow a}^{(t)}" class="latex" title="h_{i\rightarrow a}^{(0)}\preceq h_{i\rightarrow a}^{(1)} \preceq ... \preceq h_{i\rightarrow a}^{(t-1)} \preceq h_{i\rightarrow a}^{(t)}" /> for any <img src="https://s0.wp.com/latex.php?latex=t%5Cleq+r-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\leq r-1" class="latex" title="t\leq r-1" />.</em></p>

<p>Density evolution in this manner is a useful estimate of the number of distributions of density evolution variables <img src="https://s0.wp.com/latex.php?latex=%7Bh%5E%7B%28t%29%7D%2Cu%5E%7B%28t%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{h^{(t)},u^{(t)}}" class="latex" title="{h^{(t)},u^{(t)}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bh_%7B%2A%7D%5E%7B%28t%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{h_{*}^{(t)}}" class="latex" title="{h_{*}^{(t)}}" />. By looking again at the bit error rate <img src="https://s0.wp.com/latex.php?latex=P_%7Bb%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{b}^{(t)}" class="latex" title="P_{b}^{(t)}" /> and the conditional entropy <img src="https://s0.wp.com/latex.php?latex=H%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H^{(t)}" class="latex" title="H^{(t)}" /> as both monotonically decreasing functions of the number of iterations and conversely, monotonically increasing functions of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p" class="latex" title="p" />, we can derive a finite limit <img src="https://s0.wp.com/latex.php?latex=P_b%5E%7BBP%7D+%5Cequiv+%5Clim_%7Bt%5Crightarrow%5Cinfty%7DP_b%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_b^{BP} \equiv \lim_{t\rightarrow\infty}P_b^{(t)}" class="latex" title="P_b^{BP} \equiv \lim_{t\rightarrow\infty}P_b^{(t)}" />. The corresponding BP threshold can then be defined as</p>

<p><img src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D+%5Cequiv+%5Csup+%5CBig%5C%7B+p+%5Cin+%5B0%2C1%2F2%5D+%3A+P_b%5E%7BBP%7D%28p%29%3D0+%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_{d} \equiv \sup \Big\{ p \in [0,1/2] : P_b^{BP}(p)=0 \Big\}" class="latex" title="p_{d} \equiv \sup \Big\{ p \in [0,1/2] : P_b^{BP}(p)=0 \Big\}" /></p>

<p>For <img src="https://s0.wp.com/latex.php?latex=p+%5Cleq+p_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p \leq p_d" class="latex" title="p \leq p_d" />, however, increasing the number of iterations does not help as the bit error rate is asymptotically lower bounded by <img src="https://s0.wp.com/latex.php?latex=P_%7Bb%7D%5E%7BBP%7D%28p%29%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P_{b}^{BP}(p)&gt;0" class="latex" title="P_{b}^{BP}(p)&gt;0" /> for a fixed number of iterations. Good LDPC codes are thus designed with a large BP threshold with design rate <img src="https://s0.wp.com/latex.php?latex=R_%7Bdes%7D%3D1-P%27%281%29%2F%5CLambda+%27%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R_{des}=1-P'(1)/\Lambda '(1)" class="latex" title="R_{des}=1-P'(1)/\Lambda '(1)" /> to maximize the threshold noise level for a given degree distribution pair. This ensemble will have a finite fraction of variable nodes of degree 2 and a large number of codewords with small weight, which ultimately prevent the block error probability from vanishing as <img src="https://s0.wp.com/latex.php?latex=N%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N\rightarrow \infty" class="latex" title="N\rightarrow \infty" />.</p>

<h4>References</h4>

<p>[1] Marc Mezard and Andrea Montanari. <br /><em>Information, Physics, and Computation</em>. <br />Oxford Graduate Texts, 2009.</p></div>







<p class="date">
by Jeremy Dohmann <a href="https://windowsontheory.org/2018/12/16/algorithmic-and-information-theoretic-decoding-thresholds-for-low-density-parity-check-code/"><span class="datestr">at December 17, 2018 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2018/12/14/explaining-the-jaccard-metric/">Explaining The Jaccard Metric</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Why is it a metric?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/TwoJaccards.png"><img src="https://rjlipton.files.wordpress.com/2018/12/TwoJaccards.png?w=150&amp;h=140" alt="" height="140" class="alignright wp-image-15508" width="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://alchetron.com/Paul-Jaccard">source 1</a>, <a href="https://orepic.com/paul.jaccard">source 2</a></font></td>
</tr>
</tbody>
</table>
<p>
Paul Jaccard was a botanist who worked at ETH in Zurich during much of the first half of the 20th century. He created, or discovered, the similarity notion that became the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard metric</a>. Very neat having a metric named after you. </p>
<p>
Today we discuss proofs and explanations that the metric is indeed a metric.<br />
<span id="more-15507"></span></p>
<p>
The Jaccard <i>index</i> <img src="https://s0.wp.com/latex.php?latex=%7BJ%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J(A,B)}" class="latex" title="{J(A,B)}" /> is the ratio of <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccap+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cap B|}" class="latex" title="{|A \cap B|}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup B|}" class="latex" title="{|A \cup B|}" />.  The metric is	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctext%7B%5B%2A%5D%7D+%5Cqquad++J_%7B%5Cdelta%7D%28A%2CB%29+%3D+1+-+J%28A%2CB%29+%3D+1+-+%5Cfrac%7B%7CA+%5Ccap+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \text{[*]} \qquad  J_{\delta}(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|} " class="latex" title="\displaystyle \text{[*]} \qquad  J_{\delta}(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|} " /></p>
<p>provided <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> are not both empty sets. If <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> are both empty then <img src="https://s0.wp.com/latex.php?latex=%7BJ%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J(A,B)}" class="latex" title="{J(A,B)}" /> by definition is <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and so <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta(A,B)}" class="latex" title="{J_\delta(A,B)}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />.  Generally we can assume that all the sets are non-empty.</p>
<p>
The key question is to show that this satisfies the <b>triangle inequality</b>. That is, we must show that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%5D%7D%5Cqquad+J_%5Cdelta%28A%2CC%29+%5Cle+J_%5Cdelta%28A%2CB%29+%2B+J_%5Cdelta%28B%2CC%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \text{[**]}\qquad J_\delta(A,C) \le J_\delta(A,B) + J_\delta(B,C). " class="latex" title="\displaystyle  \text{[**]}\qquad J_\delta(A,C) \le J_\delta(A,B) + J_\delta(B,C). " /></p>
<p>
Many proofs of this are known, and it has been remarked that some are fairly complicated. Some are short, but <a href="https://www.tandfonline.com/doi/abs/10.1080/07468342.2018.1526020">continued</a> <a href="https://arxiv.org/pdf/1612.02696.pdf">recent</a> <a href="https://www.researchgate.net/publication/326912768_Distance_Between_Sets_-_A_survey">interest</a> seems to say they haven’t satisfied as <em>explanations</em>. </p>
<p>
We think we can supply a quick explanation, if you are already familiar with the triangle inequality holding for Hamming distance on sets: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cleq+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  |A \oplus C| \leq |A \oplus B| + |B \oplus C|, " class="latex" title="\displaystyle  |A \oplus C| \leq |A \oplus B| + |B \oplus C|, " /></p>
<p>
where <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Coplus+B+%3D+%28A+%5Ccup+B%29+%5Csetminus+%28A+%5Ccap+B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \oplus B = (A \cup B) \setminus (A \cap B)}" class="latex" title="{A \oplus B = (A \cup B) \setminus (A \cap B)}" /> is symmetric difference. By rewriting [*] as <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28A%2CB%29+%3D+%7CA+%5Coplus+B%7C%2F%7CA+%5Ccup+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta(A,B) = |A \oplus B|/|A \cup B|}" class="latex" title="{J_\delta(A,B) = |A \oplus B|/|A \cup B|}" /> we can see that [**] becomes similar but includes denominators. If [**] is <em>false</em> then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%2A%5D%7D%5Cqquad+%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%3E+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \text{[***]}\qquad \frac{|A \oplus C|}{|A \cup C|} &gt; \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" title="\displaystyle  \text{[***]}\qquad \frac{|A \oplus C|}{|A \cup C|} &gt; \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " /></p>
<p>Now if we have <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> then replacing both right-hand denominators by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" /> cannot make the right-hand side of [***] bigger. But then we have a common denominator, and we can see from Hamming distance that [**] must be true. </p>
<p>
So suppose <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> includes <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> elements not in <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />. The left-hand side of [***] is at most <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, so each right-hand fraction must be of the form <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{p}{q}}" class="latex" title="{\frac{p}{q}}" /> where $latex {p <q> \frac{p – b}{q – b}}&amp;fg=000000$, so removing those elements from <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> would also make the right-hand side of [***] smaller. That brings us back to the <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> case and the previous contradiction. So [**] must be true. That’s our proof and explanation in brief. </q></p>
<p>
</p><p></p><h2> A Careful Take </h2><p></p>
<p></p><p>
We will do the above proof more slowly and carefully, to ensure it is really clear. A convention: to make the formulas a bit more readable we use <img src="https://s0.wp.com/latex.php?latex=%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{AB}" class="latex" title="{AB}" /> to denote the intersection of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. So the Jaccard metric is now 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++J_%5Cdelta%28A%2CB%29+%3D+1+-+%5Cfrac%7B%7CAB%7C%7D%7B%7CA+%5Ccup+B%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  J_\delta(A,B) = 1 - \frac{|AB|}{|A \cup B|}. " class="latex" title="\displaystyle  J_\delta(A,B) = 1 - \frac{|AB|}{|A \cup B|}. " /></p>
<p>First we explain the main ideas:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> We will argue that the intermediate set <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> in the triangle inequality can be constrained. The set <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> can be a subset of <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />. The intuition is that any extra elements in <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> can only make the triangle inequality weaker.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> We will replace the definition of the Jaccard metric by equivalent one. This new definition is much closer to a known metric.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> We will reduce the triangle inequality finally to a known triangle inequality.</p>
<p>
<em>Proof:</em> </p>
<p>
Let’s assume that <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" /> are the sets, and we wish to prove the triangle inequality: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " /></p>
<p>
<b>Claim</b>: We can assume that <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" />. Suppose there was an element <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> but not in <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />. Then removing <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> from <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> would only tighten the triangle inequality. That is the LHS 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} " class="latex" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} " /></p>
<p>stays the same and the RHS terms 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%5Ctext%7B+and+%7D+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1-\frac{|A B|}{|A \cup B|} \text{ and } 1-\frac{|B C|}{|B \cup C|}, " class="latex" title="\displaystyle  1-\frac{|A B|}{|A \cup B|} \text{ and } 1-\frac{|B C|}{|B \cup C|}, " /></p>
<p>can only decrease.</p>
<p>
<b>Claim</b>: We can re-write <img src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{J_\delta(X,Y)}" class="latex" title="{J_\delta(X,Y)}" /> as 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CX+%5Coplus+Y%7C%7D%7B%7CX+%5Ccup+Y%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{|X \oplus Y|}{|X \cup Y|}. " class="latex" title="\displaystyle  \frac{|X \oplus Y|}{|X \cup Y|}. " /></p>
<p>As noted above, <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Coplus+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X \oplus Y}" class="latex" title="{X \oplus Y}" /> is the set of elements that are in <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> but not both. </p>
<p>
<b>Claim</b>: We note that after applying the last claim three times, [**] becomes: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" title="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " /></p>
<p>
<b>Claim</b>	: Since <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> we can multiply by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" /> and get that the triangle inequality is implied by 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " class="latex" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " /></p>
<p>Note this uses that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%7CB+%5Coplus+C%7C%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  |A \oplus B| + |B \oplus C| \le |A \oplus B| \frac{|A \cup C|}{|A \cup B|} + |B \oplus C|\frac{|A \cup C|}{|B \cup C|}, " class="latex" title="\displaystyle  |A \oplus B| + |B \oplus C| \le |A \oplus B| \frac{|A \cup C|}{|A \cup B|} + |B \oplus C|\frac{|A \cup C|}{|B \cup C|}, " /></p>
<p>which follows from 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%5Cle+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%5Ctext%7B+and+%7D+1+%5Cle+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1 \le \frac{|A \cup C|}{|A \cup B|} \text{ and } 1 \le \frac{|A \cup C|}{|B \cup C|}. " class="latex" title="\displaystyle  1 \le \frac{|A \cup C|}{|A \cup B|} \text{ and } 1 \le \frac{|A \cup C|}{|B \cup C|}. " /></p>
<p>
<b>Claim</b>	: But the last step is that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C| " class="latex" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C| " /></p>
<p>is just the triangle inequality for the Hamming distance. It uses that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x+%5Coplus+y%29+%5Cle+%28x+%5Coplus+z%29+%2B+%28z+%5Coplus+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x \oplus y) \le (x \oplus z) + (z \oplus y) " class="latex" title="\displaystyle  (x \oplus y) \le (x \oplus z) + (z \oplus y) " /></p>
<p>holds for any single bits <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z}" class="latex" title="{x,y,z}" />.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
</p><p></p><h2> How We Found The Proof </h2><p></p>
<p></p><p>
If you’re curious how we found the above, we were trying to check a different kind of proof.  We started with the statement of the triangle inequality: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%5D%7D%5Cqquad+1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \text{[**]}\qquad 1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" title="\displaystyle  \text{[**]}\qquad 1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " /></p>
<p>
This looks a bit scary, with its multiple ratios and addition and subtraction. But the following feature jumps out: </p>
<blockquote><p><b> </b> <em>The right side depends on <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> but the left side does not. </em>
</p></blockquote>
<p>This suggests the idea of asking what happens if we change <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> one element at a time? Can we “walk” it to an extreme point at which the truth of [**] is obvious? A promising start was that we could remove any elements from <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> that are not in <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />, as shown above. So we can assume <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" />. Can we move <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> toward <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" /> or at least <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Coplus+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \oplus C}" class="latex" title="{A \oplus C}" /> while preserving the implication of truth for [**]? </p>
<p>
Seen in this light, our above proof’s replacing the denominators by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" /> is an “illegal move”—not a change to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />—so less interesting. But we happened to notice it worked. Let’s follow the train of thought from where we got that <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" /> can be assumed. Okay, what the next move to make with <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />? Look again at the key expression: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " /></p>
<p>Can we simplify this in some way? The answer is yes. The structure of <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> minus an expression suggested that perhaps we could combine the <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and the ratios. Indeed it is not too hard to note that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cfrac%7B%7CXY%7C%7D%7B%7CX+%5Ccup+Y%7C%7D+%3D+%5Cfrac%7B%7CX+%5Coplus+Y%7C%7D%7B%7CX+%5Ccup+Y%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  1-\frac{|XY|}{|X \cup Y|} = \frac{|X \oplus Y|}{|X \cup Y|}. " class="latex" title="\displaystyle  1-\frac{|XY|}{|X \cup Y|} = \frac{|X \oplus Y|}{|X \cup Y|}. " /></p>
<p>Okay perhaps this is not obvious. It is not trivial, but it is a standard idea that <img src="https://s0.wp.com/latex.php?latex=%7B1+-+p%2Fq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 - p/q}" class="latex" title="{1 - p/q}" /> looks like the complementation of the “probabilty” ratio <img src="https://s0.wp.com/latex.php?latex=%7Bp%2Fq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p/q}" class="latex" title="{p/q}" />. Once you think of this the exact formula follows. Thus we can re-write the required triangle inequality now as 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" title="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " /></p>
<p>We are almost done. The ratios are annoying, so can we get rid of them? We have assumed that <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \subseteq A \cup C}" class="latex" title="{B \subseteq A \cup C}" />. So it seems like a good idea to assume—for the moment—that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is actually equal to <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" />. But then it is easy to see that <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+B+%3D+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup B = A \cup C}" class="latex" title="{A \cup B = A \cup C}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB+%5Ccup+C+%3D+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B \cup C = A \cup C}" class="latex" title="{B \cup C = A \cup C}" />. So the above becomes after multiplying by <img src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|A \cup C|}" class="latex" title="{|A \cup C|}" />, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " class="latex" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " /></p>
<p>This looks really nice, no more ratios. Wait what is this expression? The <img src="https://s0.wp.com/latex.php?latex=%7B%5Coplus%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\oplus}" class="latex" title="{\oplus}" /> is the exclusive-or function and it is not hard to note that this is the classic Hamming distance. So this inequality is a fact. Recall the Hamming distance records the number of differences between two bit-vectors, but sets are really just bit vectors.</p>
<p>
Are we there yet? Almost. We only need to argue that if <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is less than <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \cup C}" class="latex" title="{A \cup C}" /> the inequality we need is actually stronger. So we are done. </p>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do you like our proof? Is it clear from the intro—or from the second section? Or are you still unsure why the Jaccard metric satisfies the triangle inequality? We may follow up with more about other proofs.</p>
<p>
We don’t think the photo used by this online Alchetron <a href="https://alchetron.com/Paul-Jaccard">bio</a> of Paul Jaccard is the botanist. It looks too modern, for one. No other photo seems extant. Google Images guesses Alchetron’s image to be Adrian Herzog, but we think its highest Jaccard index is to <a href="https://orepic.com/">Orepic</a> user <a href="https://orepic.com/paul.jaccard">paul.jaccard</a>, as shown at top. Can you solve the mystery of who? </p>
<p></p><p><br />
[Switched to standard J-sub-delta notation to clarify and fix issues, fixed HTML conversion glitch with p-q fractions]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2018/12/14/explaining-the-jaccard-metric/"><span class="datestr">at December 15, 2018 01:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/14/faculty-at-duke-university-apply-by-december-15-2018/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/14/faculty-at-duke-university-apply-by-december-15-2018/">Faculty at Duke University (apply by December 15, 2018)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Dec 15 deadline is soft. Applications submitted in the next couple of weeks would also receive consideration. All ranks. All areas in theoretical computer science including algorithms, complexity, and cryptography.</p>
<p>Website: <a href="https://www.cs.duke.edu/openings_faculty">https://www.cs.duke.edu/openings_faculty</a><br />
Email: jschmidt@cs.duke.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/14/faculty-at-duke-university-apply-by-december-15-2018/"><span class="datestr">at December 14, 2018 05:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/14/head-of-school-in-computer-science-and-engineering-at-unsw-sydney-australia-apply-by-january-18-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/14/head-of-school-in-computer-science-and-engineering-at-unsw-sydney-australia-apply-by-january-18-2019/">Head of School in Computer Science and Engineering at UNSW Sydney, Australia (apply by January 18, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UNSW Sydney is recruiting a Head of School for the School of Computer Science &amp; Engineering.<br />
This is a continuing position, normally at the level of professor.</p>
<p>Website: <a href="http://external-careers.jobs.unsw.edu.au/cw/en/job/495688/head-of-school-cse">http://external-careers.jobs.unsw.edu.au/cw/en/job/495688/head-of-school-cse</a><br />
Email: sergeg@cse.unsw.edu.au</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/14/head-of-school-in-computer-science-and-engineering-at-unsw-sydney-australia-apply-by-january-18-2019/"><span class="datestr">at December 14, 2018 06:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=6351">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2018/12/13/ising-perceptron-under-gaussian-disorder-and-k-nae-sat/">Ising Perceptron under Gaussian Disorder, and k-NAE-SAT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner</strong></p>
<p>Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena.</p>
<p>This post is based on Professor Nike Sun’s guest lecture on the Ising Perceptron model and regular NAE-SAT for CS 229R: Physics and Computation. These are both examples of random constraint satisfaction problems, where we have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> variables <img src="https://s0.wp.com/latex.php?latex=x_1%2C+...%2C+x_n+%5Cin+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1, ..., x_n \in \{-1, 1\}" class="latex" title="x_1, ..., x_n \in \{-1, 1\}" /> and certain relations, or constraints, between the <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_i" class="latex" title="x_i" />, and we wish to approximate the number of solutions or visualize the geometry of the solutions. For both problems, the problem instance can be random: for example, the linear constraints in the Ising perceptron model are random, and the clauses in the NAE-SAT instance are chosen at random. As in the previous blog posts, to understand the geometry of solutions, statistical physicists think of sampling random solutions from <img src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{-1, 1\}^n" class="latex" title="\{-1, 1\}^n" />, which introduces a second type of randomness.</p>
<p>This post is meant to be expository. For interested readers, we point to useful references at the end for more rigorous treatments of these topics.</p>
<p><strong>1. Perceptron Model</strong></p>
<p>The Ising Perceptron under Gaussian disorder asks how many points in <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}^n" class="latex" title="\{\pm 1\}^n" /> survive <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> half-plane bisections (i.e. are satisfying assignments for <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> constraints), where the planes’ coefficients are drawn from standard Gaussians. Like many problems in statistical physics, there is likely a critical capacity of constraints where having more constraints yields no survivors with high probability, and having fewer constraints yields survivors with high probability. This lecture gives an overview of a proof for one side of this physics prediction, i.e. the existence of a lower bound critical capacity, where having fewer constraints yields survivors with positive probability. Briefly, we use the <em>Cavity method</em> to approximate the distribution of the number of satisfying assignments, then attempt to use the second moment method on that distribution to get our lower bound. A direct application fails, however, due to the variance of the Gaussian constraints. The solution is to carefully choose exactly what to condition on without destroying the model. Specifically, we iteratively compute values related to the Gaussian disorder, after which we are able remove enough variance for the second moment method to work and thus establish the lower bound for the Ising Perceptron’s critical capacity. The result holds subject to an analytical condition which is detailed in the paper (Condition 1.2 in [6]) and which remains to be rigorously verified.</p>
<p><strong>1.1. Problem</strong></p>
<p>We pick a random direction in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{R}^n" class="latex" title="\mathbb{R}^n" />, and delete all vertices in the hypercube <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}^n" class="latex" title="\{\pm 1\}^n" /> which are in the half-space negatively correlated with that direction. We repeat this process of picking a random half space and deleting points <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> times, and see if any points in the hypercube survive. Formally, we define the perceptron model as follows:</p>
<p><strong>Definition 1:</strong> Let <img src="https://s0.wp.com/latex.php?latex=G+%3D+%28g_%7B%5Cmu%7B%7Di%7D%29_%7B%5Cmu%5Cge+1%2C+i%5Cge+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}" class="latex" title="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}" /> array of i.i.d. standard Gaussians. Let <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> be the largest integer such that:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Csum_%7Bi%3D1%7D%5EN%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0+%5Chspace%7B0.1cm%7D+%5Cforall+%5Cmu%5Cle+M%5Cright%5C%7D%5Cneq+%5Cemptyset.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset." class="latex" title="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset." /></p>
<p>More compactly, <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> is the largest integer such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Cfrac%7BGJ%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0%5Cright%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}" class="latex" title="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}" /></p>
<p>is nonempty, where the inequality is taken pointwise, <img src="https://s0.wp.com/latex.php?latex=G+%5Cin+%5Cmathbb%7BR%7D%5E%7BM+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \in \mathbb{R}^{M \times N}" class="latex" title="G \in \mathbb{R}^{M \times N}" /> is an array of i.i.d. std. Gaussians, and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J" class="latex" title="J" /> is treated as a vector in <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5EN+%5Csubset+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{\pm 1\}^N \subset \mathbb{R}^N" class="latex" title="\{\pm 1\}^N \subset \mathbb{R}^N" />.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/CS229-Pic-1.png?w=600" alt="CS229 Pic 1" class="alignnone size-full wp-image-6353" /></p>
<p>In the late 80’s, physicists conjectured that there is a critical capacity <img src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_{*}" class="latex" title="\alpha_{*}" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D+%5Coverset%7BP%7D%7B%5Cto%7D+%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{M}{N} \overset{P}{\to} \alpha_{*}" class="latex" title="\frac{M}{N} \overset{P}{\to} \alpha_{*}" /> where <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> is a function of <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N" class="latex" title="N" />. The predicted critical capacity has been studied, for example in [7,9]. Our goal is to establish a lower bound on <img src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_*" class="latex" title="\alpha_*" /> for the Ising Perceptron under Gaussian disorder. To do this, let <img src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)" class="latex" title="Z(G)" /> denote the random variable which measures how many choices of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> variables survive <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M" class="latex" title="M" /> Gaussian constraints in <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> (this is the partition function). We want to show <img src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z &gt; 0" class="latex" title="Z &gt; 0" /> with high probability when there are <img src="https://s0.wp.com/latex.php?latex=M+%3C+%5Calpha_%7B%2A%7DN&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="M &lt; \alpha_{*}N" class="latex" title="M &lt; \alpha_{*}N" /> constraints. To this end, the second moment method seems promising:</p>
<p><strong>Second Moment Method: </strong>If <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> is a nonnegative random variable with finite variance, then</p>
<p><img src="https://s0.wp.com/latex.php?latex=P%28Z+%3E+0%29+%5Cge+%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D%5BZ%5D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7D%5BZ%5E2%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}" class="latex" title="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}" /></p>
<p>However, this method actually fails due to various sources of variance in the perceptron model. We will briefly sketch the fix as given in [6].</p>
<p>Before we can start, however, what does the distribution of <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> even look like? This is actually quite computationally intensive; we will use the <strong>cavity equations</strong>, a technique developed in [12,13], to approximate <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" />‘s distribution.</p>
<p><strong>1.2. Cavity Method</strong></p>
<p>The goal of the cavity method is to see how the solution space changes as we remove a row or a column of the matrix <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> with i.i.d. standard Gaussian entries, assuming that <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is fixed. Since our matrix <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is big and difficult to deal with, we try to see how the solution space changes as we add one row or one column at a time. Why is this valuable? We can think of the system of variables and constraints as an interaction between the rows (constraints) and columns (variables), so the number of solutions should behave proportionally to the product of the solutions attributed to each variable and each constraint. With this as motivation, define <img src="https://s0.wp.com/latex.php?latex=G_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G_{-\mu}" class="latex" title="G_{-\mu}" /> as the matrix obtained by removing row <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" /> and <img src="https://s0.wp.com/latex.php?latex=G%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G^{-i}" class="latex" title="G^{-i}" /> as the matrix obtained by removing column <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />. We can approximate</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}" class="latex" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}" /></p>
<p>since we can think of the partition function as receiving a multiplicative factor from each addition of a row and each addition of a column. Thus, the cavity method seeks to compute <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G_{-\mu})" class="latex" title="Z(G)/Z(G_{-\mu})" /> and <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G^{-i})." class="latex" title="Z(G)/Z(G^{-i})." /></p>
<p><strong>1.2.1. Removing a constraint</strong></p>
<p>Our goal in computing <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G_{-\mu})" class="latex" title="Z(G)/Z(G_{-\mu})" /> is to understand how the solution space <img src="https://s0.wp.com/latex.php?latex=SOL%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="SOL(G_{-\mu})" class="latex" title="SOL(G_{-\mu})" /> changes when we add in the constraint <img src="https://s0.wp.com/latex.php?latex=%5Cmu.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu." class="latex" title="\mu." /> Recalling that <img src="https://s0.wp.com/latex.php?latex=J+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J \in \{\pm 1\}^n" class="latex" title="J \in \{\pm 1\}^n" /> is our vector that is potentially in the solution space, if we define</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%3A%3D+%5Csum_%7Bi+%3D+1%7D%5E%7BN%7D%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}}," class="latex" title="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}}," /></p>
<p>then adding the constraint <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" /> is equivalent to forcing <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu \ge 0." class="latex" title="\Delta_\mu \ge 0." /> We will try to understand the distribution of <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu" class="latex" title="\Delta_\mu" /> under the Gibbs measure <img src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\nu_{-\mu}" class="latex" title="\nu_{-\mu}" />, which is the uniform measure on the solution space without the constraint <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" />. This way, we can determine the probability of <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu \ge 0" class="latex" title="\Delta_\mu \ge 0" /> under the Gibbs measure, which will equal <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G_{-\mu})." class="latex" title="Z(G)/Z(G_{-\mu})." /> To do so, we will use the <strong>Replica Symmetric Cavity Assumption</strong> (which we will abbreviate as RS cavity assumption). The RS cavity assumption lets us assume that the <img src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i" class="latex" title="J_i" />‘s for <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1 \le i \le N" class="latex" title="1 \le i \le N" /> are independent under <img src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\nu_{-\mu}" class="latex" title="\nu_{-\mu}" />. As the <img src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i" class="latex" title="J_i" />‘s are some discrete sample space that depend on our constraints, we do not actually have full independence, but the RS cavity assumption tells us there is very little dependence between the <img src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i" class="latex" title="J_i" />‘s, so we pretend they are independent.</p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu" class="latex" title="\Delta_\mu" /> should be approximately normally distributed, since it is the sum of many “independent” terms <img src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu+i%7D+J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_{\mu i} J_i" class="latex" title="g_{\mu i} J_i" /> by the RS Cavity assumption. Now, define <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu" class="latex" title="h_\mu" /> as the expectation of <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu" class="latex" title="\Delta_\mu" /> under the Gibbs measure without the constraint <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" />, and <img src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v_\mu" class="latex" title="v_\mu" /> as the variance of <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Delta_\mu" class="latex" title="\Delta_\mu" /> under the Gibbs measure without the constraint <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mu" class="latex" title="\mu" />. It will also turn out that the variance <img src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v_\mu" class="latex" title="v_\mu" /> will concentrate around a constant <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma^2." class="latex" title="\sigma^2." /> Thus,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Capprox+%5Cnu_%7B-%5Cmu%7D%28%5CDelta_%7B%5Cmu%7D+%5Cge+0%29+%3D+%5Coverline%7B%5CPhi%7D+%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})." class="latex" title="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})." /></p>
<p>Here, <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7B%5CPhi%7D%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\overline{\Phi}(\frac{-h_\mu}{\sigma})" class="latex" title="\overline{\Phi}(\frac{-h_\mu}{\sigma})" /> equals the probability that a random <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28h_%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(h_\mu, \sigma^2)" class="latex" title="\mathcal{N}(h_\mu, \sigma^2)" /> Gaussian distribution is positive, or equivalently, the probability that a standard Gaussian <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(0, 1)" class="latex" title="\mathcal{N}(0, 1)" /> distribution is greater than <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{-h_\mu}{\sigma}." class="latex" title="\frac{-h_\mu}{\sigma}." /></p>
<p><strong>1.2.2. Removing a spin</strong></p>
<p>To calculate the cavity equation for removing one column, we think of removing a column as deleting one spin from <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J" class="latex" title="J" />. Define <img src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J^{-i}" class="latex" title="J^{-i}" /> as the vector resulting from removing spin <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> from <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J" class="latex" title="J" />. We want to calculate <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G^{-i})." class="latex" title="Z(G)/Z(G^{-i})." /> Note that it is possible for <img src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D+%5Cnot%5Cin+SOL%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J^{-i} \not\in SOL(G^{-i})" class="latex" title="J^{-i} \not\in SOL(G^{-i})" /> but <img src="https://s0.wp.com/latex.php?latex=J+%5Cin+SOL%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J \in SOL(G)" class="latex" title="J \in SOL(G)" /> now, which complicates this calculation. It is possible to overcome this difficulty by passing to positive temperature, though this makes the calculations incredibly difficult. We do not worry about these issues here, and just briefly sketch how <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G^{-i})" class="latex" title="Z(G)/Z(G^{-i})" /> is computed.</p>
<p>To compute <img src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)/Z(G^{-i})" class="latex" title="Z(G)/Z(G^{-i})" />, we will split the numerator into a sum of two terms based on the sign of <img src="https://s0.wp.com/latex.php?latex=J_i%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i," class="latex" title="J_i," /> which will allow us to compute not only <img src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G)" class="latex" title="Z(G)" /> but also <img src="https://s0.wp.com/latex.php?latex=%5Clangle+J_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\langle J_i \rangle" class="latex" title="\langle J_i \rangle" />, which represents the magnetization at spin <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />. A series of complicated calculations (see the lecture notes for the details) will give us</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D+%3D+%5Cfrac%7B%5Cexp%28H_i%29%2B%5Cexp%28-H_i%29%7D%7Bexp%28c%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}" class="latex" title="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}" /></p>
<p>for some constant <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c" class="latex" title="c" />, where <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" /> is a quantity that compares how much more correlated spin <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" /> is to the constraints than all other spins. The <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%2BH_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(+H_i)" class="latex" title="\exp(+H_i)" /> will come from the solutions for <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> with <img src="https://s0.wp.com/latex.php?latex=J_i+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i = 1" class="latex" title="J_i = 1" /> and the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\exp(-H_i)" class="latex" title="\exp(-H_i)" /> will come from the solutions with <img src="https://s0.wp.com/latex.php?latex=J_i+%3D+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="J_i = -1" class="latex" title="J_i = -1" />.</p>
<p>The above equations allow us to deduce that</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7Bh_%5Cmu%7D%7B%5Csigma%7D%5Cright%29+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7B%5Cexp%28H_i%29+%2B+%5Cexp%28-H_i%29%7D%7B%5Cexp%28c%29%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}." class="latex" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}." /></p>
<p><strong>1.3. The Randomness of G</strong></p>
<p>In the previous section, we regarded <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> as fixed. We now use the these results but allow for <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> to be random again. Recall <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />‘s entries were i.i.d. Gaussians. We thus get that <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu" class="latex" title="h_\mu" />, as a linear combination of the <img src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_{\mu{}i}" class="latex" title="g_{\mu{}i}" />‘s, is a Gaussian. We thus can write <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu+%5Csim+%5Cmathcal%7BN%7D%280%2C+q%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu \sim \mathcal{N}(0, q)." class="latex" title="h_\mu \sim \mathcal{N}(0, q)." /> Similarly, <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" /> is a linear combination of the <img src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_{\mu{}i}" class="latex" title="g_{\mu{}i}" />‘s and must be Gaussian. We thus write <img src="https://s0.wp.com/latex.php?latex=H_i+%5Csim+%5Cmathcal%7BN%7D%280%2C+%5Cpsi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i \sim \mathcal{N}(0, \psi)" class="latex" title="H_i \sim \mathcal{N}(0, \psi)" />.</p>
<p>With some calculations detailed in the lecture notes and [12], we get <em>Gardner’s Formula</em>:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cln+Z%7D%7BN%7D+%5Crightarrow+%5Calpha+%5Cint+%5Cln+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7B%5Csqrt%7Bq%7D+z%7D%7B%5Csigma%7D%5Cright%29+%5Cvarphi%28z%29+dz+%2B+%5Cint+%5Cln+%5Cleft%282+%5Ccosh%28%5Csqrt%7B%5Cpsi+z%7D%29%5Cright%29%5Cvarphi%28z%29+dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz" class="latex" title="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> is our capacity, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{M}{N}" class="latex" title="\frac{M}{N}" />. It turns out that <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3D+1-q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\sigma^2 = 1-q" class="latex" title="\sigma^2 = 1-q" /> and <img src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cpsi%281-q%29%2F2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c = \psi(1-q)/2," class="latex" title="c = \psi(1-q)/2," /> so the above equation only depends on two parameters, <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cpsi.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\psi." class="latex" title="\psi." /> It will turn out that <img src="https://s0.wp.com/latex.php?latex=q%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q, \psi" class="latex" title="q, \psi" /> have relations dependent on each other based on our definitions of <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu" class="latex" title="h_\mu" /> and <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" />, and these will give us a fixed point equation for <img src="https://s0.wp.com/latex.php?latex=%28q%2C+%5Cpsi%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(q, \psi)," class="latex" title="(q, \psi)," /> which has two solutions: one near <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \approx 1" class="latex" title="\alpha \approx 1" /> and one near <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha \approx 0.83." class="latex" title="\alpha \approx 0.83." /> It is believed that the second point is correct, meaning that the critical capacity should equal <img src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A+%3D+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_* = 0.83." class="latex" title="\alpha_* = 0.83." /></p>
<p><strong>1.4. Second Moment Method</strong></p>
<p>Now that we have Gardner’s formula, solving for <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> gives us an approximation for its distribution as</p>
<p><img src="https://s0.wp.com/latex.php?latex=Z+%5Csim+%5Cexp%5C%7BN%5Cmathcal%7BG%7D%28%5Calpha%29+%2B+%5Cmathcal%7BN%7D%280%2C+N%5Cvarepsilon%5E2%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}" class="latex" title="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{G}(\alpha)" class="latex" title="\mathcal{G}(\alpha)" /> is Gardner’s formula, and the Gaussian noise <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2CN%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathcal{N}(0,N\varepsilon^2)" class="latex" title="\mathcal{N}(0,N\varepsilon^2)" /> arises from the Gaussian distribution of the <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu" class="latex" title="h_\mu" />‘s and <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" />‘s. Again, we are interested in the probability that <img src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z &gt; 0" class="latex" title="Z &gt; 0" />, but due to the approximate nature of the above equation, we cannot work with this distribution directly, and instead can try the second moment method. However, the exponentiated Gaussian makes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cgg+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2" class="latex" title="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2" /> and the moment method just gives <img src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z&gt;0) \ge 0" class="latex" title="P(Z&gt;0) \ge 0" />, whereas we need <img src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z&gt;0)" class="latex" title="P(Z&gt;0)" /> with high probability.</p>
<p>The fault here lies with Gaussian noise due to the <img src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_\mu" class="latex" title="h_\mu" />‘s and <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" />‘s, so it is natural to consider what happens when we condition on them, getting rid of the noise. We denote</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D+%3D+%28h_%5Cmu%29_%7B%5Cmu+%3D+1%7D%5EM+%5Ctext%7B+and+%7D+%5Cunderline%7BH%7D+%3D+%28H_i%29_%7Bi%3D1%7D%5EN.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N." class="latex" title="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N." /></p>
<p>Then, we can compute that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Clog+%5Cmathbb%7BE%7D%5BZ+%7C+%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Cto+%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)" class="latex" title="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)" /></p>
<p>in probability as <img src="https://s0.wp.com/latex.php?latex=N%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="N\to \infty" class="latex" title="N\to \infty" />. (This is not an exact equality becacuse of the approximations made in the derivation of Gardner’s formula.) Moreover, we will have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Capprox+%5Cmathbb%7BE%7D%5BZ%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2" class="latex" title="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2" />. The second moment method gives us the desired lower bound on <img src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z&gt;0)" class="latex" title="P(Z&gt;0)" />. However, note that these <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{H},\underline{h}" class="latex" title="\underline{H},\underline{h}" /> must satisfy the equations that defined them. In vector form, we have</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5Ctanh+%5Cunderline%7BH%7D+%3D+%5Cunderline%7Bh%7D+%2B+b_%2AF%28%5Cunderline%7Bh%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})" class="latex" title="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})" /><br />
<img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5ET%5Ctanh+F%28%5Cunderline%7Bh%7D%29+%3D+%5Cunderline%7BH%7D+%2B+d_%2A%5Ctanh%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}" class="latex" title="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=b_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="b_*" class="latex" title="b_*" />, <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="F" class="latex" title="F" />, and <img src="https://s0.wp.com/latex.php?latex=d_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d_*" class="latex" title="d_*" /> are constants and functions that appear in the rigorous definitions of <img src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_{\mu}" class="latex" title="h_{\mu}" /> and <img src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H_i" class="latex" title="H_i" />.<br />
Here arises a problem, however. We cannot solve these equations easily, and furthermore when we condition on <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{H}" class="latex" title="\underline{H}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h}" class="latex" title="\underline{h}" />, the <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />‘s that satisfy the above are not necessarily representative of matrices of standard Gaussians. Hence, simply conditioning on knowing the values of <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h},\underline{H}" class="latex" title="\underline{h},\underline{H}" /> destroys the model and will not prove the lower bound on <img src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z&gt;0)" class="latex" title="P(Z&gt;0)" /> for Gaussian disorder.</p>
<p>To solve this problem, we introduce iteration: first, initialize <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h}^{(0)}" class="latex" title="\underline{h}^{(0)}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{H}^{(1)}" class="latex" title="\underline{H}^{(1)}" /> (initialized to specific values detailed in [6]. Then, a simplified version of each time step’s update looks like</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D+%3D+%5Cfrac%7BG%5Ctanh%27%7B%5Cunderline%7BH%7D%5E%7B%28t%29%7D%7D%7D%7B%5Csqrt%7BN%7D%7D+-+b_%2AF%28%5Cunderline%7Bh%7D%5E%7B%28t+-+1%29%7D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)})," class="latex" title="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)})," /><br />
<img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D+%3D+%5Cfrac%7BG%5ETF%28%5Cunderline%7Bh%7D%5Et%29%7D%7B%5Csqrt%7BN%7D%7D+-+d_%2A%5Ctanh%7B%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}." class="latex" title="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}." /></p>
<p>It has been proven (see [2,4]) that <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h}^{(t)}" class="latex" title="\underline{h}^{(t)}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{H}^{(t)}" class="latex" title="\underline{H}^{(t)}" /> converge as <img src="https://s0.wp.com/latex.php?latex=t%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t\to\infty" class="latex" title="t\to\infty" />, and moreover the convergent values are distributed by what looks like a Gaussian at each time step. Since this sequence of <img src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\underline{h},\underline{H}" class="latex" title="\underline{h},\underline{H}" /> converge (at a rate that is independent of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />) and are representative of Gaussian <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />, for a special kind of truncated partition function <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\tilde Z" class="latex" title="\tilde Z" />, conditioning on these iteration values allows the second moment method to work and gives</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+Z+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D%5E2+%5Capprox+%5Cmathbb%7BE%7D%5B%5Ctilde+Z%5E2+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]" class="latex" title="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]" /></p>
<p>which is then enough to establish the lower bound <img src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(Z&gt;0)" class="latex" title="P(Z&gt;0)" /> for the non-truncated partition function <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> (see [6] for details, see also [3] for closely related computations).</p>
<p><strong>2. <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAE-SAT</strong></p>
<p>This lecture also gave a brief overview of results in <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAESAT. In the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-SAT problem, we ask whether a boolean formula in <img src="https://s0.wp.com/latex.php?latex=CNF&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="CNF" class="latex" title="CNF" /> form, with each clause containing exactly <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" /> literals, has a satisfying solution. For <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAESAT, we instead require that the satisfying solution is not uniform on any clause; equivalently, each clause must contain at least one true and at least one false value. Finally, we will restrict our set of possible boolean formulae to those for which every variable is contained in exactly <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" /> clauses; we call this model <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-regular <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAESAT. We briefly outline the critical capacities of clauses where the solution space changes. For more background on the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAESAT probem and its variants, see [10].</p>
<p>As with the perceptron model, we are concerned with the expected number of solutions <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Z" class="latex" title="Z" /> of this system where the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d" class="latex" title="d" />-regular formula is chosen uniformly at random. It turns out that for any fixed <img src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cfrac%7Bd%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha=\frac{d}{k}" class="latex" title="\alpha=\frac{d}{k}" />, the expected proportion of satisfiable solutions as <img src="https://s0.wp.com/latex.php?latex=n%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n\to\infty" class="latex" title="n\to\infty" /> converges to some <img src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\alpha)" class="latex" title="f(\alpha)" />. More on this, the model in general and the critical <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" /> constants mentioned below can be found in [15]. Via an application of Markov’s inequality and Jensen’s Inequality for the convex function <img src="https://s0.wp.com/latex.php?latex=x%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x^n" class="latex" title="x^n" />, <img src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\alpha)" class="latex" title="f(\alpha)" /> may be bounded above by <img src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29%3D%28%5Cmathbb%7BE%7D+Z%29%5E%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}" class="latex" title="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}" />, shown graphically below.</p>
<p><img src="https://windowsontheory.files.wordpress.com/2018/12/CS-229-Pic-2.png?w=600" alt="CS 229 Pic 2" class="alignnone size-full wp-image-6352" /><br />
The diagram also shows the nature of the expected assortment of the solutions of a random <img src="https://s0.wp.com/latex.php?latex=d-&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="d-" class="latex" title="d-" />regular <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="k" class="latex" title="k" />-NAESAT for ranges of values of <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha" class="latex" title="\alpha" />. Namely, physics analysis suggests the following:</p>
<ul>
<li>For <img src="https://s0.wp.com/latex.php?latex=0%3C%5Calpha%3C%5Calpha_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="0&lt;\alpha&lt;\alpha_d" class="latex" title="0&lt;\alpha&lt;\alpha_d" />, w.h.p. the solutions are concentrated in a single large cluster.</li>
<li>For <img src="https://s0.wp.com/latex.php?latex=%5Calpha_d%3C%5Calpha%3C%5Calpha_%7B%5Ctext%7Bcond%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}" class="latex" title="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}" />, w.h.p. the solutions are distributed among a large number of clusters.</li>
<li>For <img src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%5Ctext%7Bcond%7D%7D%3C%5Calpha_%7B%5Ctext%7Bsat%7D%7D%3C0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0" class="latex" title="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0" />, the function <img src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(\alpha)" class="latex" title="f(\alpha)" /> breaks away from <img src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f^{RS}(\alpha)" class="latex" title="f^{RS}(\alpha)" />, and w.h.p. the solutions are concentrated in a small number of clusters.</li>
<li>For <img src="https://s0.wp.com/latex.php?latex=%5Calpha%3E%5Calpha_%7B%5Ctext%7Bsat%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\alpha&gt;\alpha_{\text{sat}}" class="latex" title="\alpha&gt;\alpha_{\text{sat}}" />, w.h.p. there are no satisfiable solutions.</li>
</ul>
<p>One final note for this model: the solutions to satisfiability problems tend to be more clumpy than in the perceptron model, so correlation decay methods won’t immediately work. See [15] for how this is handled.</p>
<p><strong>References</strong></p>
<ol>
<li>N. Bansal. Constructive algorithms for discrepancy minimization. In <em>Proc. FOCS</em> 2010, pages 3–10.</li>
<li>M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. <em>IEEE Trans. Inform. Theory</em>, 57(2):764-785, 2011.</li>
<li>Bolthausen, E., 2018. A Morita type proof of the replica-symmetric formula for SK. arXiv preprint arXiv:1809.07972.</li>
<li>E. Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. <em>Commun. Math. Phys.</em>, 325(1):333-366, 2014.</li>
<li>T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. <em>IEEE Trans. Electron. Comput.</em> 14(3):326-334.</li>
<li>J. Ding and N. Sun. Capacity lower bound for the Ising perceptron. <a href="https://arxiv.org/pdf/1809.07742.pdf" rel="nofollow">https://arxiv.org/pdf/1809.07742.pdf</a></li>
<li>E. Gardner and B. Derrida. Optimal storage properties of neural network models. <em>J. Phys. A.</em>, 21(1): 271–284, 1988.</li>
<li>J. H. Kim and J. R. Roche. Covering cubes by random half cubes, with applications to binary neural networks. <em>J. Comput. Syst. Sci.</em>, 56(2):223–252, 1998</li>
<li>W. Krauth and M. Mézard. Storage capacity of memory networks with binary couplings. <em>J. Phy.</em> 50(20): 3057-3066, 1989.</li>
<li>F. Krzakala et al. “Gibbs states and the set of solutions of random constraint satisfaction problems.” <i>Proc. Natl. Acad. Sci.</i> 104.25 (2007): 10318-10323.</li>
<li>S. Lovett and R. Meka. Constructive discrepancy minimization by walking on the edges. <em>SIAM J. Comput.</em>, 44(5):1573-1582.</li>
<li>M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method. <em>J. Phys. A.</em>, 22(12):2181, 1989</li>
<li>M. Mézard and G. Parisi and M. Virasoro. SK Model: The Replica Solution without Replicas. <em>Europhys. Lett.</em>, 1(2): 77-82, 1986.</li>
<li>M. Shcherbina and B. Tirozzi. Rigorous solution of the Gardner problem. <em>Commun. Math. Phys.</em>, 234(3):383-422, 2003.</li>
<li>A. Sly and N. Sun and Y. Zhang. The Number of Solutions for Random Regular NAE-SAT. In <em>Proc. FOCS</em> 2016, pages 724-731.</li>
<li>J. Spencer. Six standard deviations suffice. <em>Trans. Amer. Math. Soc.</em> 289 (1985), 679-706</li>
<li>M. Talagrand. Intersecting random half cubes. <em>Random Struct. Algor.</em>, 15(3-4):436–449, 1999.</li>
</ol></div>







<p class="date">
by degeneratetriangle <a href="https://windowsontheory.org/2018/12/13/ising-perceptron-under-gaussian-disorder-and-k-nae-sat/"><span class="datestr">at December 14, 2018 04:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2018/12/13/postdoc-at-national-university-of-singapore-nus-apply-by-march-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2018/12/13/postdoc-at-national-university-of-singapore-nus-apply-by-march-15-2019/">Postdoc at National University of Singapore (NUS) (apply by March 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One post-doctoral research fellow fellow position is available at the School of Computing, National University of Singapore, Singapore for the project: Beyond NP Revolution.</p>
<p>Interested candidates should send their resume and statement about their research to Kuldeep Meel (meel@comp.nus.edu.sg).</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~meel/theory-postdoc.html">https://www.comp.nus.edu.sg/~meel/theory-postdoc.html</a><br />
Email: meel@comp.nus.edu.sg</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2018/12/13/postdoc-at-national-university-of-singapore-nus-apply-by-march-15-2019/"><span class="datestr">at December 13, 2018 07:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2018/211">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2018/211">TR18-211 |  Parametric Shortest Paths in Planar Graphs | 

	Kshitij Gajjar, 

	Jaikumar  Radhakrishnan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://example.com/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We construct a family of planar graphs $(G_n: n\geq 4)$, where $G_n$ has $n$ vertices including a source vertex $s$ and a sink vertex $t$, and edge weights that change linearly with a parameter $\lambda$ such that, as $\lambda$ increases, the cost of the shortest path from $s$ to $t$ has $n^{\Omega(\log n)}$ break points. This shows that lower bounds obtained earlier by Carstensen (1983) and Mulmuley & Shah (2000) for general graphs also hold for planar graphs. A conjecture of Nikolova (2009) states that the number of break points in $n$-vertex planar graphs is bounded by a polynomial in $n$; our result refutes this conjecture.

Gusfield (1980) and Dean (2009) showed that the number of break points for an $n$-vertex graph is $n^{\log n + O(1)}$ assuming linear edge weights; we show that if the edge weights are allowed to vary as a polynomial of degree at most $d$, then the number of break points is $n^{\log n + O(\alpha(n)^d)}$, where $\alpha(n)$ is the slowly growing inverse Ackermann function. This upper bound arises from Davenport-Schinzel sequences.<p></p></div></div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2018/211"><span class="datestr">at December 12, 2018 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-7727782887784240943">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2018/12/csgssi-highlights-for-2018.html">CS@GSSI: Highlights for 2018</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>It is the time of the year when one sees "Best of 2018 in <i>X</i> (for some <i>X</i>)"/"Year in Review" articles appear just about everywhere in print and on the internet. Since I need to deliver a ten-minute presentation to the Scientific Advisory Board of the  <a href="http://www.gssi.it/" target="_blank">Gran Sasso Science Institute (GSSI)</a> discussing the highlights in 2018 for the <a href="https://cs.gssi.it/" target="_blank">computer science group</a>, I thought that it might be a good idea to prepare for that task by following the trend and dumping some of my thoughts in this post.<br /><br />So what were the main achievements of the CS@GSSI group in 2018? With only ten minutes at my disposal, I needed to make some hard choices and I decided to focus on the kudos that were bestowed on our students. After all, "our students' success is our success" and the GSSI is also an international PhD school.<br /><br />Before I do so, however, let me give a bird's eye view of the research activity of the group and of the changes in personnel we have had in 2018. In November 2017, the group consisted of two full professors (one, <a href="http://www.di.univaq.it/flammini/" target="_blank">Michele Flammini</a>,  on loan from the University of L'Aquila), one tenure-track assistant professor, two non-tenute-track assistant professors and one postdoc. Those six people had to manage about 40 PhD students at various stages in their doctoral studies and this task could only be achieved with the help of external supervisors. Today, the faculty members in the group have not changed, but  the number of postdocs has grown to eight from November 2018. (We have been very lucky in hiring some superb young scientists!) At the same time, we have admitted eight new PhD students, seven of whom are from outside Italy and three are women, and are happy to have <a href="http://pageperso.lif.univ-mrs.fr/~shantanu.das/en/" target="_blank">Shantanu Das</a> (Marseille) as a visiting professor for a few months. <br /><br /><b>Publications</b><br /><br />Despite the work resulting from having to manage a fairly large number of PhD students, give courses and deal with the administrative red tape that accompanies just about everything at an Italian university, my colleagues at CS@GSSI and our students managed to produce a substantial number of high-quality publications including 16 journal papers, 53 conference papers, four book chapters, two edited volumes and five refereed short abstracts/posters. (Note that these are publications with GSSI affiliation, so there is only one paper authored by one of the new seven postdocs.)<br /><br />Essentially every high-quality conference in the fields of research covered by CS@GSSI has seen the presentation of at least one paper from the group. To wit, my colleagues published seven papers in top AI conferences (AAAI, AAMAS, IJCAI), seven papers in high-quality TCS conferences (including STOC and ICALP), two papers in the top SE conference (ICSE) and one in the top conference on foundations of programming languages (POPL). All the journal papers are in high-class outlets. Here let me just highlight that I have covered the result of a collaborative effort between researchers in algorithms and software engineering <a href="https://processalgebra.blogspot.com/2018/09/shortest-path-algorithms-applied-to.html" target="_blank">elsewhere</a>.<br /><br />Two of the papers are co-authored with researchers from the Urban Studies group at at the GSSI, and  witness the cross-disciplinary work carried out by my colleagues in topics such as Smart Cities.<br /><br /><b>Student awards and honours</b><br /><br />It is hard to choose what student achievements to highlight, so let me say that all our PhD students had  an excellent year and that the three contributions below are just a small sample of the work done by the PhD  students at the institute.<br /><ul><li>CS@GSSI student <a href="http://www.gssi.it/people/students/students-computer-science/item/897-abdelsalam-ahmed-mohamed-ahmed" target="_blank">Ahmed Abdelsalam</a> has contributed to  the proposal that won the first prize at the <a href="https://www.softfire.eu/open-calls/softfire-challenge/" target="_blank">Interworking stream at the SoftFIRE Challenge 2018</a>, which carries a 40,000 € prize. Ahmed's work on IPv6 Segment Routing (SRv6) and his recently developed SRv6-aware version of the network intrusion and detection system Snort played a key role in the award-winning proposal. Every Linux user is  running GSSI software! Ahmed's network intrusion and detection system might well be the work carried out at the GSSI that is most widely used worldwide and has the most impact on a day to day basis. Ahmed now works for Cisco in Rome. </li><li>CS@GSSI student <a href="https://sites.google.com/view/emiliocruciani/" target="_blank">Emilio Cruciani </a>has published papers in the top conference in software engineering (ICSE) (jointly with  fellow student <a href="https://robertoverdecchia.github.io/" target="_blank">Roberto Verdecchia</a>, <a href="http://cin.ufpe.br/~bafm/" target="_blank">Breno Miranda</a> and <a href="http://bertolino.isti.cnr.it/" target="_blank">Antonia Bertolino</a>; see <a href="http://processalgebra.blogspot.com/2017/12/first-year-computer-science-students-at.html" target="_blank">here</a> for a post I wrote on one of the two papers), and in two of the three top conferences in the theory of artificial intelligence (AAAI and AAMAS). To put these achievements in perspective, a conference like AAAI has over 7,000 submissions and an acceptance rate of around 16%. </li><li>CS@GSSI student <a href="https://robertoverdecchia.github.io/" target="_blank">Roberto Verdecchia</a>, whom I already mentioned in the previous highlight, has received the Early Career Researcher Award from the International Conference on Software Architectures. </li></ul>Let me also mention that my colleagues also receive awards for their sterling service to the community. By way of example, <a href="http://cs.gssi.infn.it/catia.trubiani/" target="_blank">Catia Trubiani</a> received an Exceptional Reviewer Award from ICSA.   <br /><ul></ul><b>Software tools</b><br /><br />Despite its very limited size, CS@GSSI devotes a substantial amount of effort to tool development. I have already mentioned the contribution given by Ahmed to the Linux kernel (IPv6 SHR). Other software tools developed by my colleagues include:<br /><ul><li><a href="https://dblp.org/pers/hd/i/Inverso:Omar" target="_blank">Omar Inverso</a>'s <a href="https://github.com/omainv/cseq" target="_blank">Lazy-CSeq</a>, an award-winning automated analysis of complex concurrent programs, and his CSeq-fpmath and CSeq-fpmath-ILP, for the automated verification of data-intensive programs such as control system software; </li><li>a prototype tool for the automated analysis of multi-agent-based models, developed by <a href="https://dblp.org/pers/hd/s/Stefano:Luca_Di" target="_blank">Luca Di Stefano</a>, Omar and Rocco De Nicola. </li></ul><b>Addendum dated 16 December 2018:</b>  <a href="https://dblp.org/pers/hd/i/Inverso:Omar" target="_blank">Omar Inverso</a>'s <a href="https://github.com/omainv/cseq" target="_blank">Lazy-CSeq</a> has just received yet another accolade! It has won a Silver Medal at the <a href="https://sv-comp.sosy-lab.org/2019/" target="_blank">2019 8th International Competition on Software Verification (SV-COMP 2019).</a> As can be seen from the results of the competition available <a href="https://sv-comp.sosy-lab.org/2019/results/results-verified/" target="_blank">here</a>,   Omar's tool <a href="https://github.com/omainv/cseq/releases" target="_blank">Lazy-CSeq</a> came second in the category ConcurrencySafety and was beaten only by a <a href="https://github.com/yinliangze/yogar-cbmc" target="_blank">tool</a> developed by a Chinese team from Tsinghua University (which is widely considered the best technical university in China). <br /><ul></ul><b>Events</b><br /><br />The CS@GSSI group organized the following events in 2018: <br /><ul><li><a href="https://cs.gssi.it/sea2018/" target="_blank">SEA 2018 - 17th International Symposium on Experimental Algorithms</a>, </li><li><a href="http://icities2018.disim.univaq.it/" target="_blank">I-CiTies 2018</a>  and </li><li><a href="http://icetcs.ru.is/" target="_blank">ICE-TCS</a>/GSSI Workshop 2018  </li></ul>All the events were well attended and fruitful. (To whet you appetite, let me say that we will be organizing <a href="https://cs.gssi.it/sirocco2019/" target="_blank">SIROCCO 2019</a>.)<br /><br /><b>Collaborations with other groups at the GSSI</b><br /><br />I think that it is fair to say that the CS group at the GSSI is playing the role of glue within the institute: like Nokia claimed it used to do, it is connecting people! In fact, I do believe that CS is and will increasingly become the hub of the institute. Here are some examples to substantiate my claim, in addition to the two papers published jointly with researchers in Urban Studies. <br /><ul><li>We have run joint seminars with Urban Studies and Mathematics (and brought <a href="https://www2.mathematik.tu-darmstadt.de/~kohlenbach/" target="_blank">an ICM 2018 invited section speaker</a> to the GSSI). </li><li>We have a joint postdoc with Urban Studies (Geotouch project on tourist flow). </li><li>We have submitted two joint project proposals with Urban Studies. </li><li>Catia Trubiani participates in a  COST action related to machine learning with researchers in the Astroparticle Physics group. </li></ul><b>Submission of grant proposals</b><br /><br />We have at least eight grant proposals under evaluation, submitted to funding agencies in Italy, European Union and Iceland. Moreover, we have received funding for one PhD position from a local start-up company and from the municipality of L'Aquila for a project on an innovative platform for tourism.<br /><br />Of course, I hope that some of those grant applications will be successful. However, money isn’t an end in itself. Lots of it does not necessarily lead to good, let alone great, science! In my career, I have seen richly funded projects produce much less than expected (and sometimes no science at all), whereas some grassroots projects with little or no funding have led to great advances.<br /><br />I do hope that, regardless of the funding situation, my colleagues will go from strength to strength, produce the best work they can and serve as role models for the PhD students at the GSSI. One cannot ask for more.<br /><br /><b>Self-evaluation for 2018</b><br /><ul><li>The CS group is already a productive and internationally respected group, despite its very limited size.</li><li>We are lucky to have excellent students, and brilliant postdocs and young faculty.</li><li>CS is already a hub at GSSI, but is grossly over-committed. We need to hire a good number of excellent faculty soon! If you are interested, send an <a href="https://processalgebra.blogspot.com/2018/11/calls-for-expressions-of-interest-in.html" target="_blank">expression of interest</a>. </li></ul><br /><b>The future</b><br /><br />I think that 2019 will be a key year for the growth of CS@GSSI. I looked at my crystal ball and made a few predictions, but I prefer to air them during the meeting with the Scientific Advisory Board of the GSSI on Saturday without making them public on the internet. After all, "verba volant, scripta manent!"<br /><br />I just hope that the GSSI will continue to give us the freedom to hire the best people we can find and allow us to do our work.  <br /><br /><br /><br /><br /><br /><ul></ul><br /><ul> </ul></div><div class="commentbar"><p></p><span href="http://processalgebra.blogspot.com/feeds/7727782887784240943/comments/default" class="commentbutton"></span><a href="http://processalgebra.blogspot.com/feeds/7727782887784240943/comments/default"><img src="/images/feed-icon.png" class="commenticon" /> Subscribe to comments</a>  | <a href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7727782887784240943"><img src="/images/post-icon.png" class="commenticon" /> Post a comment</a></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2018/12/csgssi-highlights-for-2018.html"><span class="datestr">at December 12, 2018 02:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=16618">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/">Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant Solved the Mihail-Vazirani Conjecture for Matroids!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<p><strong><span style="color: #ff0000;"><img src="https://gilkalai.files.wordpress.com/2018/12/milenaumesh.png?w=640" alt="Milena+Umesh" class="alignnone size-full wp-image-16635" /><br />
Milena Mihail and Umesh Vazirani</span></strong></p>
<p>I thank Nati Linial, Dan Spielman and Karim Adiprasito for sharing the news with me.</p>
<h2>The Mihail-Vazirani conjecture for matroids and Feder-Mihail’s theorem</h2>
<p>Consider a collection <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> vectors. A basis is a subset of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> of linearly independent vectors that span the subspace spanned by <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" />.  We can define a graph whose vertices are all bases and two bases are adjacent if their symmetric difference has two elements. Mihail and Vazirani conjectured that for every set <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> of vertices in this graph the number of edges between <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> to its complement <img src="https://s0.wp.com/latex.php?latex=%5Cbar+Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bar Y" class="latex" title="\bar Y" /> is at least <img src="https://s0.wp.com/latex.php?latex=%5Cmin+%28%7CY%7C+%7C%2C%5Cbar+Y%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\min (|Y| |,\bar Y|)" class="latex" title="\min (|Y| |,\bar Y|)" />.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> consists of the elements of the standard basis in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathbb R^d" class="latex" title="\mathbb R^d" /> and their negatives then the graph we obtain is the graph of the discrete <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> dimensional discrete cube and the assertion of the Mihail-Vazirani conjecture is well known.</p>
<p>The Mihail-Vazirani conjecture was formulated (and has now been proved) for arbitrary matroids. Think about a matroid as a collection <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K" class="latex" title="K" /> of subsets (the independent sets of the matroid) of some ground set <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="X" class="latex" title="X" /> that closed under taking subsets (hence a simplicial complex) with the following property: For every <img src="https://s0.wp.com/latex.php?latex=Y+%5Csubset+X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \subset X" class="latex" title="Y \subset X" />, the induced simplicial complex on <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> denoted by <img src="https://s0.wp.com/latex.php?latex=K%28Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K(Y)" class="latex" title="K(Y)" />  is pure. In other words, for every <img src="https://s0.wp.com/latex.php?latex=Y+%5Csubset+X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y \subset X" class="latex" title="Y \subset X" />, all maximal independent subsets of <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> have the same cardinality.</p>
<p>In a pioneering 1992 paper Tomás Feder and Milena Mihail proved the conjecture for balanced matroids.</p>
<h2>Mihail and Vazirani conjecture for 0-1 polytopes.</h2>
<p>An even more general conjecture by Mihail and Vazirani is still open. It asserts that for every 0-1 polytope, for every set <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> of vertices, the number of edges between <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Y" class="latex" title="Y" /> to its complement <img src="https://s0.wp.com/latex.php?latex=%5Cbar+Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\bar Y" class="latex" title="\bar Y" /> is at least <img src="https://s0.wp.com/latex.php?latex=%5Cmin+%28%7CY%7C%2C+%7C%5Cbar+Y%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\min (|Y|, |\bar Y|)" class="latex" title="\min (|Y|, |\bar Y|)" />.</p>
<h2>The new breakthrough</h2>
<p>Here is the link to the paper.</p>
<p class="title mathjax"><a href="https://arxiv.org/abs/1811.01816">Log-Concave Polynomials II: High-Dimensional Walks and an FPRAS for Counting Bases of a Matroid</a> by Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant</p>
<p>Let me mention another paper by a subset of the authors on related questions, another paper by Brändén and Huh, who independently proved the strong log-concavity of several of the polynomials appeared in the ALOV  paper, and are be several forthcoming papers by these two groups.</p>
<p><strong>Here is the abstract:</strong> We design an FPRAS to count the number of bases of any matroid given by an independent set oracle, and to estimate the partition function of the random cluster model of any matroid in the regime where <span id="MathJax-Element-1-Frame" class="MathJax"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="mn">0</span><span id="MathJax-Span-4" class="mo">&lt;</span><span id="MathJax-Span-5" class="mi">q</span><span id="MathJax-Span-6" class="mo">&lt;</span><span id="MathJax-Span-7" class="mn">1</span></span></span></span>. Consequently, we can sample random spanning forests in a graph and (approximately) compute the reliability polynomial of any matroid. We also prove the thirty year old conjecture of Mihail and Vazirani that the bases exchange graph of any matroid has expansion at least 1. One of our key observations is a close connection between pure simplicial complexes and multiaffine homogeneous polynomials. Specifically, if <span id="MathJax-Element-2-Frame" class="MathJax"><span id="MathJax-Span-8" class="math"><span id="MathJax-Span-9" class="mrow"><span id="MathJax-Span-10" class="mi">X</span></span></span></span> is a pure simplicial complex with positive weights on its maximal faces, we can associate with <span id="MathJax-Element-3-Frame" class="MathJax"><span id="MathJax-Span-11" class="math"><span id="MathJax-Span-12" class="mrow"><span id="MathJax-Span-13" class="mi">X</span></span></span></span> a multiaffine homogeneous polynomial <img src="https://s0.wp.com/latex.php?latex=p_X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_X" class="latex" title="p_X" /> such that the eigenvalues of the localized random walks on <span id="MathJax-Element-5-Frame" class="MathJax"><span id="MathJax-Span-21" class="math"><span id="MathJax-Span-22" class="mrow"><span id="MathJax-Span-23" class="mi">X</span></span></span></span> correspond to the eigenvalues of the Hessian of derivatives of <img src="https://s0.wp.com/latex.php?latex=p_X&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="p_X" class="latex" title="p_X" />.</p>
<h2>Spectral negative dependence and Hodge-Riemann</h2>
<p>A key paragraph regarding the method is the following:</p>
<p>“Our approach has a close connection to the original plan of Feder and Mihail (1992) who used the negative correlation property of balanced matroids to show that the bases exchange walk mixes rapidly. Unfortunately, most interesting matroids do not satisfy negative correlation. But it was observed [AKH18; HW16; AOV18] that all matroids satisfy a spectral negative dependence property.” AKH18 (a typo, it should be AHK18) is the solution for the Rota-Heron conjecture by Adiprasito, Huh and Katz that relies on the Hodge-Riemann property for matroids that we mentioned <a href="https://gilkalai.wordpress.com/2015/08/14/updates-and-plans-iii/">in this post</a>. (I still feel in debt for a more detailed blog post about Adiprasito, Huh and Katz’ breakthrough!).”</p>
<p>I think that high dimensional expanders and random walks on them also plays a role in the new development.</p>
<h3><strong>Update:</strong> Related very recent papers</h3>
<p class="title is-5 mathjax"><a href="https://arxiv.org/abs/1807.00929">Log-Concave Polynomials I: Entropy and a Deterministic Approximation Algorithm for Counting Bases of Matroids</a></p>
<p class="title is-5 mathjax"><span class="search-hit">Authors:</span> Nima Anari, Shayan Oveis Gharan, Cynthia Vinzant</p>
<p><a href="https://arxiv.org/abs/1811.01600">Log-Concave Polynomials III: Mason’s Ultra-Log-Concavity Conjecture for Independent Sets of Matroids</a></p>
<p><span class="search-hit">Authors:</span> Nima Anari, Kuikui Liu, Shayan Oveis Gharan, Cynthia Vinzant</p>
<p><a href="https://arxiv.org/abs/1811.01696">Hodge-Riemann relations for Potts model partition functions</a></p>
<p><span class="search-hit">Authors:</span> Petter Brändén, June Huh</p>
<p class="title is-5 mathjax"><a href="https://arxiv.org/abs/1806.02675">Correlation bounds for fields and matroids</a></p>
<p class="authors"><span class="search-hit">Authors:</span> June Huh, Benjamin Schröter, Botong Wang</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/"><span class="datestr">at December 11, 2018 09:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
