<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 28, 2022 02:38 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=619">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/04/28/tcs-talk-wednesday-may-4-vera-traub-eth-zurich/">TCS+ talk: Wednesday, May 4 — Vera Traub, ETH Zürich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://people.math.ethz.ch/~vtraub/"><strong>Vera Traub</strong></a> from ETH Zürich will speak about “<em>Approximation Algorithms for Connectivity Augmentation Problems</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Augmentation problems are a fundamental class of network design problems. They ask about the cheapest way to increase the (edge-)connectivity of a graph by adding edges among a given set of options. One of the most elementary and intensely studied augmentation problems is the (Weighted) Tree Augmentation Problem. Here, a spanning tree has to be augmented into a 2-edge-connected graph.</p>
<p>Classic techniques for network design yield 2-approximation algorithms for a wide class of augmentation problems. For the Unweighted Tree Augmentation Problem, better-than-2 approximations are known for more than 20 years. However, only recently the first better-than-2 approximations have been found for the more general Unweighted Connectivity Augmentation Problem and Weighted Tree Augmentation Problem. In this talk we will discuss these recent advances.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/04/28/tcs-talk-wednesday-may-4-vera-traub-eth-zurich/"><span class="datestr">at April 28, 2022 10:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6405">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6405">My first-ever attempt to create a meme!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<figure class="wp-block-image size-large"><img src="https://www.scottaaronson.com/4thinkers.jpg" alt="" /></figure></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6405"><span class="datestr">at April 27, 2022 11:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/060">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/060">TR22-060 |  How much randomness is needed to convert MA protocols to AM protocols? | 

	Nikolay Vereshchagin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Merlin-Arthur  class of languages MA is included into Arthur-Merlin  class AM, and into PP. For a  standard transformation of a given MA protocol with  Arthur's message (= random string) of length $a$ and Merlin's message of length $m$ to a PP machine, the latter needs  $O(ma)$ random bits. The same holds for simulating MA protocols by AM protocols: in the resulting AM protocol the length of Arthur's message (= random string) is $O(ma)$. And the same holds for simulating heuristic MA protocols by  heuristic  AM protocols as well. In the paper  [A. Knop, Circuit Lower Bounds for Average-Case MA, CSR 2015] it was conjectured that, in the transformation of heuristic MA protocols to heuristic AM protocols, $O(ma)$ can be replaced by a polynomial of $a$ only. A similar question can be asked for normal MA and AM protocols, and for the simulation of MA protocols by PP machines. In the present paper we show that, relative to an oracle,  both latter questions answer in the negative and Knop's conjecture is false. Moreover,  the same is true for simulation of MA protocols by AM protocols in which the  error probability is not bounded away from 1/2, the so called PP$\cdot$NP protocols. The latter protocols generalize both AM protocols and  PP machines.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/060"><span class="datestr">at April 27, 2022 04:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8309">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/">A personal FAQ on the math education controversies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I was a contact person for the <a href="https://windowsontheory.org/2021/12/03/an-alarming-trend-in-k-12-math-education/">open letter on K-12 math education</a>, and am in strong support of the <a href="https://scottaaronson.blog/?p=6389">recent letter on the role of data science in math education</a> (and would encourage readers that are faculty members in California to sign it). Since I tend to see the same questions and objections arise time and again, I thought it would be useful to write my responses to these. The following is my own opinions only, and does not reflect the views of any other contact person or signer of these letters. I also apologize in advance for the lack of links and references below. However, if you ask questions in the comments, I’d be happy to give more sources.</p>



<p><strong>Q: The status quo in US math education is terrible and doesn’t work for many children, especially students of color and low income students. Aren’t these letter writers just trying to protect a flawed system that worked well for them?</strong></p>



<p><strong>A: </strong>The US math education system is very complex, but I agree that it is largely failing a large chunk of our students. But that doesn’t mean that any change will necessarily be for the better. Just because a system is bad, doesn’t mean you can’t make it worse. We have a responsibility to “first do no harm” when we experiment on millions of children of our largest state, and the proposed changes are not based on any solid evidence, and in my opinion (and the opinion of many other experts) will make things worse. Indeed, there is no shortage of experiments in mathematics education that yielded no or even negative progress in terms of equity.</p>



<p>Also, not all change needs to be curricular change. The inequalities in the system were not created because of the math curriculum, and will not be fixed by it. Many top-performing countries use a fairly traditional math curriculum, some strongly influenced by the Soviet system.  The inequalities in the US arise from huge disparities in the resources at school, and a highly unequal society at large. I personally think that improving education is much more about support for students, resources, tutoring, teacher training, etc, than whether we teach logarithms using method X or method Y.</p>



<p><strong>Q:</strong> <strong>Shouldn’t CS and STEM faculty stay out of this debate, and leave it to the math education faculty that are the true subject matter experts?</strong></p>



<p><strong>A:</strong> The education system has many stakeholders, including students, parents, teachers, citizens, employers, and post-secondary educators, and they all should be heard. STEM careers and other quantitative fields are the fastest growing in opportunities and student interest in college. As such, one of the goals (though not the only one) of K-12 math education is to prepare students to have the option to major in STEM in college, if that is what they want to do. STEM faculty are the best equipped to <a href="https://mathematics.stanford.edu/news/submission-ca-math-framework-revision">say what is needed</a> for success in their field, and what they see is the impact of K-12 preparation. Math Ed and STEM faculty can and should be working together on the  K-12 curriculum. </p>



<p><strong>Q: But not all students will go to STEM in college. Shouldn’t the K-12 math education system also offer something for the students that are not interested in STEM?</strong></p>



<p><strong>A: </strong>I agree that not all students will go to STEM and that (for example) not all high-school students should be forced to take calculus. However, if we are offering an option that is designed for students that are not interested in STEM then we should be honest about it. At the moment high-school data science courses are marketed as a way to “have your cake and eat it too” – courses that are on one hand easier than algebra and calculus and on the other hand give just as good or maybe even better preparation for a career in tech or data science. This is misinformation, and the students most likely to fall for it are the ones with the least resources. In addition, the type of thinking developed by rigorous math courses can benefit students throughout their lives and careers, regardless of the path they take.</p>



<p><strong>Q: Isn’t equity more important than giving students opportunities to advance in math? Shouldn’t mathematical education policy be focused on the kids that are struggling the most and facing most challenges, in particular students of color?</strong></p>



<p><strong>A: </strong>I personally think equity and expanding access to mathematical education is absolutely crucial. This is why I was and am involved in initiatives including <a href="https://www.addiscoder.com/">AddisCoder</a>, <a href="https://jamcoders.org.jm/">JamCoders</a>, <a href="https://womenintheory.wordpress.com/">Women In Theory</a>, and <a href="https://boazbk.github.io/tcs-summerschool/">New Horizons in TCS</a>. But true equity is about actually educating students more, not about moving the goalposts and claiming success. This is doubly true in the context of the US education system. There will be many routes open to well-resourced students to bypass any limitations of the public system. These include private tutoring, courses such as Russian School of Math, Art of Problem Solving, or simply opting out of the public system altogether. Also, due to local control, wealthier districts are likely to opt out of any reforms that they perceive (correctly) as giving worse preparation for post-secondary success. </p>



<p>Hence changes such as the CMF will disproportionately harm low-income students and students of color, and make it less likely for them to succeed in STEM. Not coincidentally, many educators, researchers, and practitioners of color have signed both letters, while the CMF itself has no Black authors. There are efforts that actually do work to decrease educational gaps: these include Bob Moses’ <a href="https://www.nytimes.com/2001/01/07/education/algebra-project-bob-moses-empowers-students.html">Algebra Project</a>, Adrian Mims’ (contact person for one of the letters) <a href="https://thecalculusproject.org/">Calculus Project</a>,  <a href="https://www.theescalanteprogram.org/">Jaime Escalante</a>  (from “stand and deliver”) math program, and the <a href="https://www.aeaweb.org/articles?id=10.1257/app.3.3.158">Harlem Children’s Zone</a>. Notably, none of these projects involve lowering the bar. </p>



<p><strong>Q: Maybe the problem is with the STEM college curriculum as well? For example, why do students need calculus for a computer science degree when hardly any software engineer uses it? If colleges would make their curriculum more practically oriented then we wouldn’t need to teach high-school kids this hard math.</strong></p>



<p><strong>A:</strong> There is a narrow answer and a deeper answer to this question. The narrow answer is that the goal of K-12 education is to prepare students for success in the world as it exists now. If you want to fight these battles at the higher education level, then you should fight them and win them, and only later change the K-12 education to fit the new college curricula.</p>



<p>However, there is a deeper answer why university education has always been about more than just giving students the minimal vocational skills. We believe that our mission is not just to give students some tools that they’ll use in the first job out of school, but broader ways of thinking that will help them keep up with new developments throughout their careers. Computer Science is a great example of this. Fifteen years ago, most computer scientists didn’t need to know much about algebra, probability or calculus, but these days deep learning is fast expanding to every area of CS. The time between an academic paper to a real-world product is getting shorter and shorter, and one skill a computer scientist needs these days is the ability to read a complex technical text and not be afraid of learning new math. Foundational courses such as college calculus and linear algebra (which themselves build on high-school Algebra II and  pre-calculus) are crucial for this skill.</p>



<p><strong>Q: Aren’t you devaluing data science and saying it’s less important than Algebra or Calculus?</strong></p>



<p><strong>A: </strong>Absolutely not. I think literacy with data is an essential skill for any citizen in our modern society, and strongly support it being taught for every K-12 student. This does not mean that it can or should replace the basic math foundational skills. Data science can and be included in variety of courses, ranging from the computational and natural sciences to the social sciences and even humanities. (For example see the courses satisfying Harvard’s <a href="https://oue.fas.harvard.edu/quantitative-reasoning-data">quantitative reasoning with data requirement</a>.)</p>



<p>“Data science” is an evolving field and at the moment not very well defined, and as such there are data science courses of vastly different types. A high-school course or module in data proficiency can be extremely beneficial for students, but without prerequisites such as algebra, probability, and programming, there is a severe limit to the depth that it can go to. For students who will not go into STEM or data science, such a data literacy course would be highly recommended. Students who will take a deeper course later on are better served by foundational courses such as Algebra, Pre-calculus, and Calculus. </p>



<p>By the way, there is nothing about data science that makes it inherently easier than algebra or calculus. While (univariate) calculus is ultimately about functions you can draw on a paper and reason intuitively about their graphs, issues of correlations vs causation are highly subtle, and even experts could get it wrong. The skills and rigorous modes of thinking developed in courses such as Algebra II and beyond are required to develop a true understanding of data science. </p>



<p>There is another reason why a prerequisite-free data literacy course should not be considered as part of the math curriculum, which was eloquently put by <a href="https://blog.mathed.page/2019/10/14/freakonomics-radio-on-math-curriculum/">Henri Picciotto</a> (see also <a href="https://blog.mathed.page/2021/12/14/yet-more-on-the-california-framework-part-1/">this</a>): <em>“in math we should not teach black-box formulas and software packages that students cannot possibly understand thoroughly. We have been moving towards teaching math for understanding at all levels. There is no reason to use data analysis as an excuse to backtrack. Let science and social studies teachers use standard deviation, correlation coefficient, regression, confidence interval, and the like without understanding the underlying assumptions and the reasoning and calculations that lead to those. Math teachers should not. “</em></p>



<p><strong>Q: Isn’t the CMF an evidence-based proposal that is backed by a huge number of citations?</strong></p>



<p><strong>A:</strong> The short answer to this question is “No”. The medium answer is that the CMF contains many citations but often the research cited is <a href="https://notepad.michaelpershan.com/more-youcubed-research-that-is-difficult-to-explain/">sloppy</a>, or is cited incorrectly. (For example, they make plenty of <a href="https://docs.google.com/document/d/1WO4gCR0fCHtlN8uN5lRztAsdqEGuRgKE/edit#bookmark=id.d8y5jr2fqobx" target="_blank" rel="noreferrer noopener">unsupported</a> <a href="https://docs.google.com/document/d/1WO4gCR0fCHtlN8uN5lRztAsdqEGuRgKE/edit#bookmark=id.biqb9dlec448" target="_blank" rel="noreferrer noopener">claims</a> on neuroscience ,whereas essentially all neuroscientists agree that our understanding of the brain is nowhere near the level that it could be used to guide curriculum development.) The long answer is out of scope for this blog post, but here are some links to analyses done by other people. I will update this blog as more are put out (this is a 900 page document after all), but some people that wrote about this include <a href="https://notepad.michaelpershan.com/what-is-happening-in-california-with-math-right-now/">Michael Pershan</a> and <a href="https://twitter.com/BethKellySF/status/1516875836573454338?s=20&amp;t=ewWPTA0FYdJBpJNDzeTjDw">Beth Kelly</a> (see also <a href="https://twitter.com/BethKellySF/status/1518991575526699008?s=20&amp;t=ewWPTA0FYdJBpJNDzeTjDw">this</a>). Brian Conrad has been working on fuller analysis of the CMF, and I will update this post (as well as <a href="https://twitter.com/boazbaraktcs">tweet</a> about it) as parts of it become available.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/"><span class="datestr">at April 27, 2022 03:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/059">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/059">TR22-059 |  Diameter versus Certificate Complexity of Boolean Functions | 

	Siddhesh Chaubal, 

	Anna Gal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this paper, we introduce a measure of Boolean functions we call diameter, that captures the relationship between certificate complexity and several other measures of Boolean functions. Our measure can be viewed as a variation on alternating number, but while alternating number can be exponentially larger than certificate complexity, we show that diameter is always upper bounded by certificate complexity. We argue that estimating diameter may help to get improved bounds on certificate complexity in terms of sensitivity, and other measures.

Previous results due to Lin and Zhang  imply that $s(f) \ge \Omega(n^{1/3})$ for transitive functions with constant alternating number. We improve and extend this bound and prove that $s(f) \ge \sqrt{n}$ for transitive functions with constant alternating number, as well as for transitive functions with constant diameter. We also show that $bs(f) \ge \Omega(n^{3/7})$ for transitive functions under the weaker condition that the ``minimum'' diameter is constant.

Furthermore, we prove that the log-rank conjecture holds for functions of the form $f(x \oplus y)$ for functions $f$ with diameter bounded above by a polynomial of the logarithm of the Fourier sparsity of the function $f$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/059"><span class="datestr">at April 27, 2022 03:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12488">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12488">Distances Release with Differential Privacy in Tree and Grid Graph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fan:Chenglin.html">Chenglin Fan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Ping.html">Ping Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12488">PDF</a><br /><b>Abstract: </b>Data about individuals may contain private and sensitive information. The
differential privacy (DP) was proposed to address the problem of protecting the
privacy of each individual while keeping useful information about a population.
Sealfon (2016) introduced a private graph model in which the graph topology is
assumed to be public while the weight information is assumed to be private.
That model can express hidden congestion patterns in a known transportation
system. In this paper, we revisit the problem of privately releasing
approximate distances between all pairs of vertices in (Sealfon 2016). Our goal
is to minimize the additive error, namely the difference between the released
distance and actual distance under private setting. We propose improved
solutions to that problem for several cases.
</p>
<p>For the problem of privately releasing all-pairs distances, we show that for
tree with depth $h$, we can release all-pairs distances with additive error
$O(\log^{1.5} h \cdot \log^{1.5} V)$ for fixed privacy parameter where $V$ the
number of vertices in the tree, which improves the previous error bound
$O(\log^{2.5} V)$, since the size of $h$ can be as small as $O(\log V)$. Our
result implies that a $\log V$ factor is saved, and the additive error in tree
can be smaller than the error on array/path. Additionally, for the grid graph
with arbitrary edge weights, we also propose a method to release all-pairs
distances with additive error $\tilde O(V^{3/4}) $ for fixed privacy
parameters. On the application side, many cities like Manhattan are composed of
horizontal streets and vertical avenues, which can be modeled as a grid graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12488"><span class="datestr">at April 27, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12468">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12468">A Review of In-Memory Space-Efficient Data Structures for Temporal Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brito:Luiz_F=_A=.html">Luiz F. A. Brito</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traven=ccedil=olo:Bruno_A=_N=.html">Bruno A. N. Travençolo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albertini:Marcelo_K=.html">Marcelo K. Albertini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12468">PDF</a><br /><b>Abstract: </b>Temporal graphs model relationships among entities over time. Recent studies
applied temporal graphs to abstract complex systems such as continuous
communication among participants of social networks. Often, the amount of data
is larger than main memory, therefore, we need specialized structures that
balance space usage and query efficiency. In this paper, we review
space-efficient data structures that bring large temporal graphs from external
memory to primary memory and speed up specialized queries. We found a great
variety of studies using data compression techniques and self-indexed
compressed data structures. We point further research directions to improve the
current state-of-the-art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12468"><span class="datestr">at April 27, 2022 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12399">Streaming Algorithms for High-Dimensional Robust Statistics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pensia:Ankit.html">Ankit Pensia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pittas:Thanasis.html">Thanasis Pittas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12399">PDF</a><br /><b>Abstract: </b>We study high-dimensional robust statistics tasks in the streaming model. A
recent line of work obtained computationally efficient algorithms for a range
of high-dimensional robust estimation tasks. Unfortunately, all previous
algorithms require storing the entire dataset, incurring memory at least
quadratic in the dimension. In this work, we develop the first efficient
streaming algorithms for high-dimensional robust statistics with near-optimal
memory requirements (up to logarithmic factors). Our main result is for the
task of high-dimensional robust mean estimation in (a strengthening of) Huber's
contamination model. We give an efficient single-pass streaming algorithm for
this task with near-optimal error guarantees and space complexity nearly-linear
in the dimension. As a corollary, we obtain streaming algorithms with
near-optimal space complexity for several more complex tasks, including robust
covariance estimation, robust regression, and more generally robust stochastic
optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12399"><span class="datestr">at April 27, 2022 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12397">Tolerant Bipartiteness Testing in Dense Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raychaudhury:Rahul.html">Rahul Raychaudhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sen:Sayantan.html">Sayantan Sen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12397">PDF</a><br /><b>Abstract: </b>Bipartite testing has been a central problem in the area of property testing
since its inception in the seminal work of Goldreich, Goldwasser and Ron
[FOCS'96 and JACM'98]. Though the non-tolerant version of bipartite testing has
been extensively studied in the literature, the tolerant variant is not well
understood. In this paper, we consider the following version of tolerant
bipartite testing: Given a parameter $\varepsilon \in (0,1)$ and access to the
adjacency matrix of a graph $G$, we can decide whether $G$ is
$\varepsilon$-close to being bipartite or $G$ is at least
$(2+\Omega(1))\varepsilon$-far from being bipartite, by performing
$\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon ^3}\right)$ queries and in
$2^{\widetilde{\mathcal{O}}(1/\varepsilon)}$ time. This improves upon the
state-of-the-art query and time complexities of this problem of
$\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon ^6}\right)$ and
$2^{\widetilde{\mathcal{O}}(1/\varepsilon^2)}$, respectively, from the work of
Alon, Fernandez de la Vega, Kannan and Karpinski (STOC'02 and JCSS'03), where
$\widetilde{\mathcal{O}}(\cdot)$ hides a factor polynomial in $\log
\frac{1}{\varepsilon}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12397"><span class="datestr">at April 27, 2022 10:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12358">Polylogarithmic Sketches for Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12358">PDF</a><br /><b>Abstract: </b>Given $n$ points in $\ell_p^d$, we consider the problem of partitioning
points into $k$ clusters with associated centers. The cost of a clustering is
the sum of $p^{\text{th}}$ powers of distances of points to their cluster
centers. For $p \in [1,2]$, we design sketches of size
poly$(\log(nd),k,1/\epsilon)$ such that the cost of the optimal clustering can
be estimated to within factor $1+\epsilon$, despite the fact that the
compressed representation does not contain enough information to recover the
cluster centers or the partition into clusters. This leads to a streaming
algorithm for estimating the clustering cost with space
poly$(\log(nd),k,1/\epsilon)$. We also obtain a distributed memory algorithm,
where the $n$ points are arbitrarily partitioned amongst $m$ machines, each of
which sends information to a central party who then computes an approximation
of the clustering cost. Prior to this work, no such streaming or
distributed-memory algorithm was known with sublinear dependence on $d$ for $p
\in [1,2)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12358"><span class="datestr">at April 27, 2022 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12340">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12340">Lattices Without a Big Constant and With Noise</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gortler:Steven_J=.html">Steven J. Gortler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theran:Louis.html">Louis Theran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12340">PDF</a><br /><b>Abstract: </b>We show how Frieze's analysis of subset sum solving using lattices can be
done with out any large constants and without flipping. We apply the variant
without the large constant to inputs with noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12340"><span class="datestr">at April 27, 2022 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12303">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12303">On converses to the polynomial method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bri=euml=t:Jop.html">Jop Briët</a>, Francisco Escudero Gutiérrez <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12303">PDF</a><br /><b>Abstract: </b>A surprising 'converse to the polynomial method' of Aaronson et al. (CCC'16)
shows that any bounded quadratic polynomial can be computed exactly in
expectation by a 1-query algorithm up to a universal multiplicative factor
related to the famous Grothendieck constant. A natural question posed there
asks if bounded quartic polynomials can be approximated by $2$-query quantum
algorithms. Arunachalam, Palazuelos and the first author showed that there is
no direct analogue of the result of Aaronson et al. in this case. We improve on
this result in the following ways: First, we point out and fix a small error in
the construction that has to do with a translation from cubic to quartic
polynomials. Second, we give a completely explicit example based on techniques
from additive combinatorics. Third, we show that the result still holds when we
allow for a small additive error. For this, we apply an SDP characterization of
Gribling and Laurent (QIP'19) for the completely-bounded approximate degree.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12303"><span class="datestr">at April 27, 2022 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12162">Budgeted Out-tree Maximization with Submodular Prizes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Gianlorenzo D'Angelo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Delfaraz:Esmaeil.html">Esmaeil Delfaraz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gilbert:Hugo.html">Hugo Gilbert</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12162">PDF</a><br /><b>Abstract: </b>We consider a variant of the prize collecting Steiner tree problem in which
we are given a \emph{directed graph} $D=(V,A)$, a monotone submodular prize
function $p:2^V \rightarrow \mathbb{R}^+ \cup \{0\}$, a cost function $c:V
\rightarrow \mathbb{Z}^{+}$, a root vertex $r \in V$, and a budget $B$. The aim
is to find an out-subtree $T$ of $D$ rooted at $r$ that costs at most $B$ and
maximizes the prize function. We call this problem \emph{Directed Rooted
Submodular Out-tree} (\textbf{DRSO}).
</p>
<p>Very recently, Ghuge and Nagarajan [SODA\ 2020] gave a quasi-polynomial-time
$O\left(\frac{\log n'}{\log \log n'}\right)$-approximation algorithm for the
case in which the costs are associated to the edges, where $n'$ is the number
of vertices in an optimal solution.
</p>
<p>In this paper we give a polynomial-time algorithm for \textbf{DRSO} that
guarantees an approximation factor of $O(\sqrt{B}/\epsilon^3)$ at the cost of a
budget violation of a factor $1+\epsilon$, for any $\epsilon \in (0,1]$. The
same result holds for the edge-cost case, to our knowledge this is the first
polynomial-time approximation algorithm for this case.
</p>
<p>We further show that the unrooted version of \textbf{DRSO} can be
approximated to a factor of $O(\sqrt{B})$ without budget violation, which is an
improvement over the factor $O(\Delta \sqrt{B})$ given in~[Kuo et al.\
IEEE/ACM\ Trans.\ Netw.\ 2015] for the undirected and unrooted case, where
$\Delta$ is the maximum degree of the graph. Finally, we provide some
new/improved approximation bounds for several related problems, including the
additive-prize version of \textbf{DRSO}, the maximum budgeted connected set
cover problem, and the budgeted sensor cover problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12162"><span class="datestr">at April 27, 2022 11:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12129">Mirror Games Against an Open Book Player</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Roey Magen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naor:Moni.html">Moni Naor</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12129">PDF</a><br /><b>Abstract: </b>Mirror games were invented by Garg and Schnieder (ITCS 2019). Alice and Bob
take turns (with Alice playing first) in declaring numbers from the set {1,2,
...2n}. If a player picks a number that was previously played, that player
loses and the other player wins. If all numbers are declared without
repetition, the result is a draw. Bob has a simple mirror strategy that assures
he won't lose and requires no memory. On the other hand, Garg and Schenier
showed that every deterministic Alice needs memory of size linear in $n$ in
order to secure a draw.
</p>
<p>Regarding probabilistic strategies, previous work showed that a model where
Alice has access to a secret random perfect matching over {1,2, ...2n} allows
her to achieve a draw in the game w.p. a least 1-1/n and using only polylog
bits of memory.
</p>
<p>We show that the requirement for secret bits is crucial: for an `open book'
Alice with no secrets (Bob knows her memory but not future coin flips) and
memory of at most n/4c bits for any c&gt;2, there is a Bob that wins w.p. close to
1-2^{-c/2}.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12129"><span class="datestr">at April 27, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12073">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12073">One-pass additive-error subset selection for $\ell_{p}$ subspace approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deshpande:Amit.html">Amit Deshpande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pratap:Rameshwar.html">Rameshwar Pratap</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12073">PDF</a><br /><b>Abstract: </b>We consider the problem of subset selection for $\ell_{p}$ subspace
approximation, that is, to efficiently find a \emph{small} subset of data
points such that solving the problem optimally for this subset gives a good
approximation to solving the problem optimally for the original input.
Previously known subset selection algorithms based on volume sampling and
adaptive sampling \cite{DeshpandeV07}, for the general case of $p \in [1,
\infty)$, require multiple passes over the data. In this paper, we give a
one-pass subset selection with an additive approximation guarantee for
$\ell_{p}$ subspace approximation, for any $p \in [1, \infty)$. Earlier subset
selection algorithms that give a one-pass multiplicative $(1+\epsilon)$
approximation work under the special cases. Cohen \textit{et al.}
\cite{CohenMM17} gives a one-pass subset section that offers multiplicative
$(1+\epsilon)$ approximation guarantee for the special case of $\ell_{2}$
subspace approximation. Mahabadi \textit{et al.} \cite{MahabadiRWZ20} gives a
one-pass \emph{noisy} subset selection with $(1+\epsilon)$ approximation
guarantee for $\ell_{p}$ subspace approximation when $p \in \{1, 2\}$. Our
subset selection algorithm gives a weaker, additive approximation guarantee,
but it works for any $p \in [1, \infty)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12073"><span class="datestr">at April 27, 2022 11:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.12055">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.12055">Faster Fundamental Graph Algorithms via Learned Predictions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Justin_Y=.html">Justin Y. Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vakilian:Ali.html">Ali Vakilian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fred.html">Fred Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.12055">PDF</a><br /><b>Abstract: </b>We consider the question of speeding up classic graph algorithms with
machine-learned predictions. In this model, algorithms are furnished with extra
advice learned from past or similar instances. Given the additional
information, we aim to improve upon the traditional worst-case run-time
guarantees. Our contributions are the following:
</p>
<p>(i) We give a faster algorithm for minimum-weight bipartite matching via
learned duals, improving the recent result by Dinitz, Im, Lavastida, Moseley
and Vassilvitskii (NeurIPS, 2021);
</p>
<p>(ii) We extend the learned dual approach to the single-source shortest path
problem (with negative edge lengths), achieving an almost linear runtime given
sufficiently accurate predictions which improves upon the classic fastest
algorithm due to Goldberg (SIAM J. Comput., 1995);
</p>
<p>(iii) We provide a general reduction-based framework for learning-based graph
algorithms, leading to new algorithms for degree-constrained subgraph and
minimum-cost $0$-$1$ flow, based on reductions to bipartite matching and the
shortest path problem.
</p>
<p>Finally, we give a set of general learnability theorems, showing that the
predictions required by our algorithms can be efficiently learned in a PAC
fashion.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.12055"><span class="datestr">at April 27, 2022 11:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.11894">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.11894">Properly learning monotone functions via local reconstruction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vasilyan:Arsen.html">Arsen Vasilyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.11894">PDF</a><br /><b>Abstract: </b>We give a $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$-time algorithm for properly
learning monotone Boolean functions under the uniform distribution over
$\{0,1\}^n$. Our algorithm is robust to adversarial label noise and has a
running time nearly matching that of the state-of-the-art improper learning
algorithm of Bshouty and Tamon [BT96] and an information-theoretic lower bound
of [BCO+15]. Prior to this work, no proper learning algorithm with running time
smaller than $2^{\Omega(n)}$ was known to exist.
</p>
<p>The core of our proper learner is a local computation algorithm for sorting
binary labels on a poset. Our algorithm is built on a body of work on
distributed greedy graph algorithms; specifically we rely on a recent work of
Ghaffari and Uitto [GU19], which gives an efficient algorithm for computing
maximal matchings in a graph in the LCA model of [RTVX11, ARVX11]. The
applications of our local sorting algorithm extend beyond learning on the
Boolean cube: we also give a tolerant tester for Boolean functions over general
posets that distinguishes functions that are $\varepsilon/3$-close to monotone
from those that are $\varepsilon$-far. Previous tolerant testers for the
Boolean cube only distinguished between $\varepsilon/\Omega(\sqrt{n})$-close
and $\varepsilon$-far.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.11894"><span class="datestr">at April 27, 2022 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6389">An update on the campaign to defend serious math education in California</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong>Update (April 27):</strong> Boaz Barak—Harvard CS professor, longtime friend-of-the-blog, and coauthor of my previous guest post on this topic—has just written an <a href="https://windowsontheory.org/2022/04/27/a-personal-faq-on-the-math-education-controversies/">awesome FAQ</a>, providing his personal answers to the most common questions about what I called our “campaign to defend serious math education.”  It directly addresses several issues that have already come up in the comments.  Check it out!</p>



<hr class="wp-block-separator" />



<p>As you might remember, last December I hosted a <a href="https://scottaaronson.blog/?p=6146">guest post</a> about the “California Mathematics Framework” (CMF), which was set to cause radical changes to precollege math in California—e.g., eliminating 8th-grade algebra and making it nearly impossible to take AP Calculus.  I linked to an <a href="https://sites.google.com/view/k12mathmatters/home">open letter</a> setting out my and my colleagues’ concerns about the CMF.  That letter went on to receive more than 1700 signatures from STEM experts in industry and academia from around the US, including recipients of the Nobel Prize, Fields Medal, and Turing Award, as well as a lot of support from college-level instructors in California. </p>



<p>Following widespread pushback, a <a href="https://www.cde.ca.gov/ci/ma/cf/">new version of the CMF</a> appeared in mid-March.  I and others are gratified that the new version significantly softens the opposition to acceleration in high school math and to calculus as a central part of mathematics.  Nonetheless, we’re still concerned that the new version promotes a narrative about data science that’s a recipe for cutting kids off from any chance at earning a 4-year college degree in STEM fields (including, ironically, in data science itself).</p>



<p>To that end, some of my Californian colleagues have <a href="https://sites.google.com/view/mathindatamatters/home"><strong>issued a new statement today</strong></a> on behalf of academic staff at 4-year colleges in California, aimed at clearing away the fog on how mathematics is related to data science.  I strongly encourage my readers on the academic staff at 4-year colleges in California to sign this commonsense statement, which has already been signed by over 250 people (including, notably, at least 50 from Stanford, home of two CMF authors).</p>



<p>As a public service announcement, I’d also like to bring to wider awareness <a href="https://www.law.cornell.edu/regulations/california/5-CCR-Sec-18533">Section 18533 of the California Education Code</a>, for submitting written statements to the California State Board of Education (SBE) about errors, objections, and concerns in curricular frameworks such as the CMF.  </p>



<p>The SBE is scheduled to vote on the CMF in mid-July, and their remaining meeting before then is on May 18-19 according to <a href="https://www.cde.ca.gov/be/ag/st/index.asp">this site</a>, so it is really at the May meeting that concerns need to be aired.  Section 18533 requires submissions to be written (yes, snail mail) and postmarked at least 10 days before the SBE meeting.  So to make your voice heard by the SBE, please send your written concern by certified mail (for tracking, but <em>not</em> requiring signature for delivery), no later than Friday May 6, to State Board of Education, c/o Executive Secretary of the State Board of Education, 1430 N Street, Room 5111, Sacramento, CA 95814, complemented by an email submission to <a href="mailto:sbe@cde.ca.gov" target="_blank" rel="noreferrer noopener">sbe@cde.ca.gov</a> and <a href="mailto:mathframework@cde.ca.gov" target="_blank" rel="noreferrer noopener">mathframework@cde.ca.gov</a>.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6389"><span class="datestr">at April 26, 2022 05:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/058">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/058">TR22-058 |  Separations in Proof Complexity and TFNP | 

	Mika Göös, 

	Alexandros Hollender, 

	Siddhartha Jain, 

	Gilbert Maystre, 

	William Pires, 

	Robert Robere, 

	Ran Tao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
It is well-known that Resolution proofs can be efficiently simulated by Sherali-Adams (SA) proofs. We show, however, that any such simulation needs to exploit huge coefficients: Resolution cannot be efficiently simulated by SA when the coefficients are written in unary. We also show that Reversible Resolution (a variant of MaxSAT Resolution) cannot be efficiently simulated by Nullstellensatz (NS).

These results can be interpreted in the language of total NP search problems. We show that PPADS, PPAD, SOPL are captured by unary-SA, unary-NS, and Reversible Resolution, respectively. Consequently, relative to an oracle, PLS $\not\subseteq$ PPADS and SOPL $\not\subseteq$ PPA.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/058"><span class="datestr">at April 26, 2022 03:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/057">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/057">TR22-057 |  When Arthur has Neither Random Coins nor Time to Spare: Superfast Derandomization of Proof Systems | 

	Lijie Chen, 

	Roei Tell</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
What is the actual cost of derandomization? And can we get it for free? These questions were recently raised by Doron et. al (FOCS 2020) and have been attracting considerable interest. In this work we extend the study of these questions to the setting of *derandomizing interactive proofs systems*. 

First, we show conditional derandomization of $\mathcal{MA}$ and of $\mathcal{AM}$ with *optimal runtime overhead*, where optimality is under the  $\#NSETH$ assumption. Specifically, denote by $\mathcal{AM}\mathcal{TIME}^{[\rightleftharpoons c]}[T]$ a protocol with $c$ turns of interaction in which the verifier runs in polynomial time $T$. We prove that for every $\epsilon&gt;0$ there exists $\delta&gt;0$ such that:

1. $\mathcal{MATIME}[T]\subseteq \mathcal{NTIME}[T^{2+\epsilon}]$, and

2. $\mathcal{AM}\mathcal{TIME}^{[\rightleftharpoons c]}[T] \subseteq \mathcal{NTIME}[n\cdot T^{\lceil c/2 \rceil+\epsilon}]$,

where $(1)$ follows if there is a property $\Pi$ of Boolean functions that can be recognized from a $2^n$-length truth-table in $\mathcal{NTIME}[2^{(2+\epsilon/3)\cdot n}]$ such that functions with $\Pi$ are hard for $(\mathcal{N}\cap co\mathcal{N})\mathcal{TIME}[2^{(2-\delta)\cdot n}]/2^{(1-\delta)\cdot n}$; and $(2)$ follows if for every $k\ge1$ there is a $\Pi$ that can be recognized from a $2^n$-length truth-table in $\mathcal{NTIME}[2^{(k+\epsilon/3)\cdot n}]$ such that functions with $\Pi$ are hard for $\mathcal{MAMTIME}[2^{(1-\delta)\cdot k n}]/2^{(1-\delta)\cdot n}$.

To obtain faster derandomization, we introduce the notion of a *deterministic effective argument system*: This is a deterministic verifier $V$ such that correct claims $x\in L$ can be proved to $V$ (i.e., there is a proof $\pi$ such that $V(x,\pi)=1$), and for every probabilistic polynomial-time adversary $\tilde{P}$, the probability that $\tilde{P}$ finds an incorrect claim $x\notin L$ and a misleading proof $\pi$ such that $V(x,\pi)=1$ is negligible.

Under strong hardness assumptions, we prove that *any constant-round proof system can be compiled into a deterministic effective argument system, with essentially no time overhead*. As one corollary, under the foregoing hardness assumptions, for every $\epsilon&gt;0$ there is a deterministic verifier $V$ that gets as input an $n$-bit formula of size $2^{o(n)}$, runs in time $2^{\epsilon \cdot n}$, and satisfies the following: For every formula $\Phi$ there is a proof $\pi$ such that $V(\Phi,\pi)$ prints the number of satisfying assignments for $\Phi$; and for every adversary $\tilde{P}$ running in time $2^{O(n)}$, the probability that $\tilde{P}$ finds $\Phi$ and $\pi$ such that $V(\Phi,\pi)$ prints an incorrect count is $2^{-\omega(n)}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/057"><span class="datestr">at April 25, 2022 07:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-889555349371097512">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/the-roeder-problem-was-solved-before-i.html">The Roeder Problem was Solved Before I Posed it (how we missed it)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(This is a joint post with David and Tomas Harris.)</p><p><br /></p><p>In my an earlier post (see <a href="https://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">here</a>) I discussed the MATH behind a problem that I worked on, with David and Tomas Harris, that we later found out had already been solved. In this post we discuss HOW this happened. </p><p>Recall that Bill Gasarch read a column of Oliver Roeder (see <a href="https://fivethirtyeight.com/features/pick-a-number-any-number/">here</a>) on Nate Silvers' blog where he challenged his readers to the following:</p><p>Find the longest sequence using numbers from {1,...,100} such that every number is either a factor or multiple of the previous number. (A later column (see <a href="https://fivethirtyeight.com/features/is-this-bathroom-occupied/">here</a> ) revealed the answer to be 77 via a computer search, which we note is not a human-readable proof.)</p><p>Bill wrote a blog post (see <a href="https://blog.computationalcomplexity.org/2017/09/a-problem-i-thought-was-interesting-now.html">here</a>) and an open problems column (see <a href="https://www.cs.umd.edu/~gasarch/open/nate.pdf">here</a> ) asking about the general case of {1,...,n}. Before doing this Bill DID try to check the literature to see what was known, but he didn't check very hard since this was not going to be a published paper. Also, he vaguely thought that if it was a known problem then one of his readers would tell him.</p><p>QUESTION: Is it appropriate to blog on things that you have not done a search of the literature on?</p><p>ANSWER: Yes, but you should SAY SO in the blog post.</p><p>As measured by comments, the post did not generate much interest- 10 comments. 2 were me (Gasarch) responding to comments.</p><p>David (who has a PhD from UMCP under Aravind Srinivasan) asked Bill to find a HS project for his son Tomas. Bill gave Tomas the sequence problem (as he called it) to look at- perhaps write a program to find what happens for {1,...,n} for small n, perhaps find human-readable proofs of weaker bound, for small n or for n=100.</p><p>David got interested in the MATH behind the problem so the project became three projects: Tomas would look at the programing aspects and the human-readable aspects, David would look at the Math, and Bill would...  hmmm, not clear what Bill would do, but he did write up a great deal of it and cleaned up some of the proofs.</p><p>David showed</p><p>Omega( n/( (log n)^{1.68} )  LE  L(n)  LE  O( n/( (log n)^{0.79} ). </p><p>Tomas and Bill obtained a human-readable proof that L(100) LE 83. (Comments on my blog sketched a proof that L(100) LE 83, and someone else that L(100) LE 80). See my previous post (<a href="https://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">here</a>) for more on the known numbers for L. </p><p>At that point David did a brief literature search; however, he didn't know what to look for.</p><p>BILL still thought of this as a HS project so he didn't think much about a paper coming out of it, or if it was original. So he didn't do the due diligence of seeing what was already known.</p><p>David and Tomas were busy working on it, so they only did a few cursory checks of the literature.</p><div><div><br /></div><div>With the two results above,  we had a paper! David then looked much more carefully at the literature. He DID find some earlier papers -- he did a Google search for Roeder's puzzle, which mentioned another mathematician, who was quoted in a blog by another mathematician, who eventually mentioned Pomerance's old paper on the topic. Once he found a reference to an actual math paper it was easy to use Google Scholar to find forward/backward citations and find the current state of the art.</div><div><br /></div><div>His email had subject title</div><div><br /></div><div>                        SHUT IT ALL DOWN!!!</div><div><br /></div><div>Which made Bill think it involved a nuclear reactor undergoing The China Syndrome rather than just telling us that other people did had better and earlier results. </div><div><br /></div><div><p class="MsoNormal">In 1995 Gerald Tenenbaum showed, in a paper written in French,  that there exists a,b such that </p><p class="MsoNormal">                               n/(log n)^a LE L(n) LE n/(log n)^b (see <a href="http://www.numdam.org/item/ASENS_1995_4_28_2_115_0/">here</a>). </p><p class="MsoNormal">More recently, in 2021, Saias showed, in a paper written in French, that </p><p class="MsoNormal">                                      L(n) GE (0.3 - o(1)) n/log n (see <a href="https://arxiv.org/abs/2107.03855">here</a>). </p></div><div><br /></div><div>SO, why didn't Bill, David, Tomas find that it was already known until late in the process:</div><div><br /></div><div>1) They didn't know the right search term: <i>Divisor Graph</i></div><div><br /></div><div>2) The literature was in French so the right search term is <i>graphe divisoriel</i></div><div><br /></div><div>3) The transition from FUN HS PROJECT to SERIOUS MATH PAPER was somewhat abrupt and caught Bill by surprise.</div><div><br /></div><div>Was this a disappointment?</div><div><br /></div><div>1) We all learned some math from it, so that was nice.</div><div><br /></div><div>2) We were in a position to read and understand the paper since we knew all of the difficulties --- however, it was in French which I do not read. David reads some, Tomas does not read French.  I prefer to be scooped in English, but even then  I might not be able to read up on the problem since  math is... hard. When did math get so hard? see my blog on that <a href="https://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html">here</a>. When did CS theory get so hard? See my blog on that <a href="https://blog.computationalcomplexity.org/2021/11/when-did-computer-science-theory-get-so.html">here</a>.)</div><div><br /></div><div>Could this happen again?</div><div><br /></div><div>1) Yes. Language barriers are hard to overcome. Though this is rare nowadays--- not much serious mathematics seems to be done outside English. French mathematicians seem to like to keep their language alive, although they probably know English as well. There may be a few other countries (China, perhaps), where English language skills are not advanced and researchers are cut off from the English literature.</div><div><br /></div><div>2) Yes. I've heard of cases where many people discovered the same theorem but were unaware of each others results since they were in different fields.</div><div><br /></div><div>3) Is it easier or harder to reprove a theorem now then it was X years ago?</div><div><br /></div></div><div>We have better search tools, but we also have more to search. </div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/the-roeder-problem-was-solved-before-i.html"><span class="datestr">at April 25, 2022 01:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6387">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6387">On form versus meaning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>There is a fundamental difference between form and meaning. Form is the physical structure of something, while meaning is the interpretation or concept that is attached to that form. For example, the form of a chair is its physical structure – four legs, a seat, and a back. The meaning of a chair is that it is something you can sit on.</p>



<p>This distinction is important when considering whether or not an AI system can be trained to learn semantic meaning. AI systems are capable of learning and understanding the form of data, but they are not able to attach meaning to that data. In other words, AI systems can learn to identify patterns, but they cannot understand the concepts behind those patterns.</p>



<p>For example, an AI system might be able to learn that a certain type of data is typically associated with the concept of “chair.” However, the AI system would not be able to understand what a chair is or why it is used. In this way, we can see that an AI system trained on form can never learn semantic meaning.</p>



<p>–GPT3, when I gave it the prompt “Write an essay proving that an AI system trained on form can never learn semantic meaning” <img width="16" alt="😃" src="https://static.xx.fbcdn.net/images/emoji.php/v9/taa/1.5/16/1f603.png" height="16" /></p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6387"><span class="datestr">at April 24, 2022 09:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/056">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/056">TR22-056 |  Optimal Coding Theorems in Time-Bounded Kolmogorov Complexity | 

	Zhenjian Lu, 

	Igor Carboni Oliveira, 

	Marius Zimand</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The classical coding theorem in Kolmogorov complexity states that if an $n$-bit string $x$ is sampled with probability $\delta$ by an algorithm with prefix-free domain then K$(x) \leq \log(1/\delta) + O(1)$. In a recent work, Lu and Oliveira [LO21] established an unconditional time-bounded version of this result, by showing that if $x$ can be efficiently sampled with probability $\delta$ then rKt$(x) = O(\log(1/\delta)) + O(\log n)$, where rKt denotes the randomized analogue of Levin's Kt complexity. Unfortunately, this result is often insufficient when transferring applications of the classical coding theorem to the time-bounded setting, as it  achieves a  $O(\log(1/\delta))$ bound instead of the information-theoretic optimal $\log(1/\delta)$.

Motivated by this discrepancy, we investigate optimal coding theorems in the time-bounded setting. Our main contributions can be summarised as follows.

Efficient coding theorem for rKt with a factor of $2$. Addressing a question from [LO21], we show that if $x$ can be efficiently sampled with probability at least $\delta$ then rKt$(x) \le (2 + o(1)) \cdot \log(1/\delta) + O\!\left(\log n\right)$. As in previous work, our coding theorem is efficient in the sense that it provides a polynomial-time probabilistic algorithm that, when given $x$, the code of the sampler, and $\delta$,  it outputs, with  probability $\ge 0.99$, a probabilistic representation of $x$ that certifies this rKt complexity bound.

Optimality under a cryptographic assumption. Under a hypothesis about the security of cryptographic pseudorandom generators, we show that no efficient coding theorem can achieve a bound of the form rKt$(x) \leq (2 - o(1)) \cdot \log(1/\delta) +$ poly$(\log n)$. Under a weaker assumption, we exhibit a gap between efficient coding theorems and existential coding theorems with near-optimal parameters.

Optimal coding theorem for pK$^t$ and unconditional Antunes-Fortnow. We consider pK$^t$ complexity [GKLO22], a variant of rKt where the randomness is public and the time bound is fixed. We observe the existence of an optimal coding theorem for pK$^t$, and employ this result to establish an unconditional version of a theorem of Antunes and Fortnow [AF09] which characterizes the worst-case running times of languages that are in average polynomial-time over all P-samplable distributions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/056"><span class="datestr">at April 24, 2022 08:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/055">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/055">TR22-055 |  Simple Hard Instances for Low-Depth Algebraic Proofs | 

	Nashlen Govindasamy, 

	Tuomas Hakoniemi, 

	Iddo  Tzameret</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove super-polynomial lower bounds on the size of propositional proof systems operating with constant-depth algebraic circuits over fields of zero characteristic. Specifically, we show that the subset-sum variant $\sum_{i,j,k,l\in[n]} z_{ijkl}x_ix_jx_kx_l-\beta = 0$, for Boolean variables, does not have polynomial-size IPS refutations where the refutations are multilinear and written as constant-depth circuits.

Andrews and Forbes (STOC’22) established recently a constant-depth IPS lower bound, but their hard instance does not have itself small constant-depth circuits, while our instance is computable already with small depth-2 circuits. 

Our argument relies on extending the recent breakthrough lower bounds against constant-depth algebraic circuits by Limaye, Srinivasan and Tavenas (FOCS’21) to the functional lower bound framework of Forbes, Shpilka, Tzameret and Wigderson (ToC’21), and may be of independent interest. Specifically, we construct a polynomial $f$ computable with small-size constant-depth circuits, such that the multilinear polynomial computing $1/f$ over Boolean values and its appropriate set-multilinear projection are hard for constant-depth circuits.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/055"><span class="datestr">at April 24, 2022 08:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/054">TR22-054 |  A Lower Bound for $k$-DNF Resolution on Random CNF Formulas via Expansion | 

	Anastasia Sofronova, 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Random $\Delta$-CNF formulas are one of the few candidates that are expected to be hard to refute in any proof system. One of the frontiers in the direction of proving lower bounds on these formulas is the $k$-DNF Resolution proof system (aka $\mathrm{Res}(k)$). Assume we sample $m$ clauses over $n$ variables. There are two known lower bounds.
1. Segerlind, Buss, Impagliazzo showed an exponential lower bound for any constant $k$, $\Delta = \mathcal{O}(k^2)$ in case $m = \mathcal{O}(n^{7 / 6})$.
2. Alekhnovich showed lower bound for $k = \mathcal{O}(\sqrt{\frac{\log n}{\log\log n}})$, any $\Delta \ge 3$ in case $m = \mathcal{O}(n)$.


Both of these papers used the same technique: the so-called small restriction switching lemma. However, they used different properties of the dependency graph of the random formula. In this paper we present a new technique with the same flavour though based on a different complexity measure that we call closure covering. We use only the expansion of the dependency graph of the formula. This technique allows us to unify and improve both of these bounds simultaneously. In particular, as a corollary we show:
1) an exponential lower bound for any constant $k$, $\Delta = \mathcal{O}(1)$ in case $m = \mathrm{poly}(n)$;
2) an exponential lower bound for $k = \mathcal{O}(\sqrt{\log n})$, $\Delta = \mathcal{O}(1)$ in case $m = \mathcal{O}(n)$.

It is the first lower bound that works for clause density $\frac{m}{n} &gt; n^{1 / 6}$ (density can even be superpolynomial for random $\log n$-CNF).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/054"><span class="datestr">at April 24, 2022 07:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/053">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/053">TR22-053 |  On the Complexity of Algebraic Numbers, and the Bit-Complexity of Straight-Line Programs | 

	Eric Allender, 

	Nikhil Balaji, 

	Samir Datta, 

	Rameshwar Pratap</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We investigate the complexity of languages that correspond to algebraic real numbers, and we present improved upper bounds on the complexity of these languages.  Our key technical contribution is the presentation of improved uniform TC^0 circuits
for division, matrix powering, and related problems, where the improvement is in terms of "majority depth" (initially studied by Maciel and Therien). As a corollary, we obtain improved bounds on the complexity of certain problems involving arithmetic circuits, which are known to lie in the counting hierarchy, and we answer a question posed by Yap.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/053"><span class="datestr">at April 23, 2022 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6362">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6362">Back</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Thanks to everyone who asked whether I’m OK!  Yeah, I’ve been living, loving, learning, teaching, worrying, procrastinating, just not blogging.</p>



<hr class="wp-block-separator" />



<p>Last week, Takashi Yamakawa and Mark Zhandry posted a preprint to the arXiv, <a href="https://arxiv.org/abs/2204.02063">“Verifiable Quantum Advantage without Structure,”</a> that represents some of the most exciting progress in quantum complexity theory in years.  I wish I’d thought of it.  tl;dr they show that relative to a random oracle (!), there’s an NP search problem that quantum computers can solve exponentially faster than classical ones.  And yet this is 100% consistent with the Aaronson-Ambainis Conjecture!</p>



<hr class="wp-block-separator" />



<p>A student brought my attention to <a href="https://deduckproject.github.io/quantle/">Quantle</a>, a variant of Wordle where you need to guess a true equation involving 1-qubit quantum states and unitary transformations.  It’s really well-done!  Possibly the best quantum game I’ve seen.</p>



<hr class="wp-block-separator" />



<p>Last month, Microsoft <a href="https://www.microsoft.com/en-us/research/blog/microsoft-has-demonstrated-the-underlying-physics-required-to-create-a-new-kind-of-qubit/">announced on the web</a> that it had achieved an experimental breakthrough in topological quantum computing: not <em>quite</em> the creation of a topological qubit, but some of the underlying physics required for that.  This followed their needing to <a href="https://www.nature.com/articles/d41586-021-00612-z">retract</a> their previous claim of such a breakthrough, due to the criticisms of Sergey Frolov and others.  One imagines that they would’ve taken far greater care this time around.  Unfortunately, a research paper doesn’t seem to be available yet.  Anyone with further details is welcome to chime in.</p>



<hr class="wp-block-separator" />



<p>Woohoo!  Maximum flow, maximum bipartite matching, matrix scaling, and isotonic regression on posets (among many others)—all algorithmic problems that I was familiar with way back in the 1990s—are now solvable in nearly-linear time, thanks to a <a href="https://arxiv.org/abs/2203.00671">breakthrough by Chen et al.</a>!  Many undergraduate algorithms courses will need to be updated.</p>



<hr class="wp-block-separator" />



<p>For those interested, Steve Hsu <a href="https://infoproc.blogspot.com/2022/04/scott-aaronson-quantum-computing.html">recorded a podcast with me</a> where I talk about quantum complexity theory.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6362"><span class="datestr">at April 23, 2022 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19926">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/">A Paradox</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>If you can’t solve a problem, it’s because you’re playing by the rules—Paul Arden</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/db/" rel="attachment wp-att-19928"><img width="220" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/db.jpg?resize=220%2C165&amp;ssl=1" class="alignright wp-image-19928" height="165" /></a></p>
<p>
Dietrich Braess is a professor of mathematics at Ruhr University in Bochum, Germany.  He is famous for the discovery of a paradox. When he was doing research on traffic modeling in 1968, he discovered that the flow in a road network could be <i>hindered</i> by <i>adding</i> a new road.</p>
<p>
Today we note a connection between him and the new ACM Athena Award winner, Éva Tardos, whom we congratulate.</p>
<p>
The paradox has a paradox riding on it.  The original is about the game theory of elective actions by drivers individually trying to optimize their routes. Yet it shows up in purely physical dynamics.  Consider the following <a href="http://www.davros.org/science/roadparadox.html">network</a> of springs and strings:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/par/" rel="attachment wp-att-19929"><img width="100" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/par.png?resize=100%2C192&amp;ssl=1" class="aligncenter wp-image-19929" height="192" /></a></p>
<p>
Quoting Clive Feathers, who <a href="http://www.davros.org/">studies</a> Britain’s railways and the London Underground:</p>
<blockquote><p><b> </b> <em> If all the strings are taut, then each of them will be carrying one-third of the weight of the block. So if the purple string is cut, the remaining strings will carry more of the weight and the block will drop down a bit as the springs take up the load, right?</em></p><em>
<p>
Wrong!</p>
</em><p><em>
As shown, each string has a tension of 4N, so the springs will each be under a tension of 8N and will stretch accordingly. Now, when the purple string is cut the remaining strings have a tension of 6N, and so does each spring. So there is less tension in the springs, and they will contract accordingly, making the block rise. </em>
</p></blockquote>
<p>
</p><p></p><h2> The Braess Paradox </h2><p></p>
<p></p><p>
Who cares about springs and strings? Indeed. </p>
<p>
The point is that this paradox arises in problems we study in CS. It is named now as <a href="https://en.wikipedia.org/wiki/Braess's_paradox">Braess’s Paradox</a>. Springs and strings can be changed to edges. Edges in turn relate to networks of many kinds; the following figure comes from a <a href="https://iopscience.iop.org/article/10.1088/1367-2630/aad490">paper</a> on power grids:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/smile/" rel="attachment wp-att-19931"><img width="500" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/smile.jpg?resize=500%2C282&amp;ssl=1" class="aligncenter wp-image-19931" height="282" /></a></p>
<p>
The original paradox is that adding one or more roads to a road network can slow down overall traffic flow through it. The new road initially makes its use locally optimal for everyone until a saturation point <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> is reached. The limit on choices before and after using the new road, however, also makes the <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> usage increase the cost of the previously optimal strategies. This evolves a worse configuration for everyone that is not locally improvable. </p>
<p>
The flip side is that a clog-prone network can sometimes be improved by removing certain parts of it. This paradox has been used to explain instances of improved traffic flow when existing major roads are closed. Here are two examples:</p>
<ul>
<li>
<a href="https://resources.mpi-inf.mpg.de/departments/d1/teaching/ws12/ct/Braess-paradox.pdf">In Stuttgart, Germany</a> after investments into the road network in 1969, the traffic situation did not improve until a section of newly-built road was closed for traffic again.<p></p>
<p></p></li><li>
<a href="https://www.nytimes.com/1990/12/25/health/what-if-they-closed-42d-street-and-nobody-noticed.html">ON Earth Day</a> this year (1990), New York City’s Transportation Commissioner decided to close 42d Street, which as every New Yorker knows is always congested. “Many predicted it would be doomsday,” said the Commissioner, Lucius J. Riccio. “You didn’t need to be a rocket scientist or have a sophisticated computer queuing model to see that this could have been a major problem.”<p></p>
<p>
But to everyone’s surprise, Earth Day generated no historic traffic jam. Traffic flow actually improved when 42d Street was closed.</p>
</li></ul>
<p>
</p><p></p><h2> Today’s News and the Paradox </h2><p></p>
<p></p><p>
This started as a post on Éva Tardos, who is a professor at Cornell University. She just was named the 2022-2023 ACM <a href="https://awards.acm.org/athena">Athena Lecturer</a>:</p>
<blockquote><p><b> </b> <em> This award celebrates women researchers who have made fundamental contributions to Computer Science. Each year ACM honors a preeminent woman computer scientist as the Athena Lecturer. The recipient gives an invited talk at a major ACM conference of her choice. </em>
</p></blockquote>
<p></p><p>
What is the connection between Tardos as Athena Lecturer and the Braess Paradox? Indeed, the citation notes her contributions to algorithmic game theory. These are game-theory situations in her purview.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/et-3/" rel="attachment wp-att-19933"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/et.jpg?resize=300%2C200&amp;ssl=1" class="aligncenter wp-image-19933" height="200" /></a></p>
<p>
She has many papers that bound approximations to optimal solutions. Often the answers are likely to be hard to solve exactly—they are NP-complete for example. But she has some wonderful papers that study questions where paradoxes apply regardless of whether <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" /> is true.</p>
<p>
One is the <a href="http://www.timroughgarden.org/papers/mcbp.pdf">paper</a> by Henry Lin, Tim Roughgarden, Eva Tardos, and Asher Walkover, and is titled, “Stronger Bounds On Braess’s Paradox And The Maximum Latency Of Selfish Routing.” It addresses the flip-side paradox: how much can the performance of a network be improved by removing one edge? One might expect that removing an edge might be less likely to improve the smallest networks, as well as less impactful on larger networks, so that the highest improvement would come in some mid-size example. In fact, they show that Braess’s original example of four nodes and five edges has the highest improvement. </p>
<p>
They also show that the problem of finding the maximum improvement possible within a sub-network of a large network is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-hard to approximate, even sub-exponentially. We wonder how this connects to an earlier <a href="https://mathweb.ucsd.edu/~fan/wp/braess_rsa.pdf">paper</a> by Fan Chung, Stephen Young, and Wenbo Zhao showing that expander graphs tend to have many subgraphs exhibiting the Braess paradox. This is surprising because expanders by-and-large promote free network flows.</p>
<p></p><p>
We add that Tardos is just the third Athena Lecturer from the theory area since the first award in 2006: Shafi Goldwasser in 2008 and Nancy Lynch in 2012. Congrats, Éva. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I did not know about the Braess Paradox before reading about her award; Ken thinks he saw it a decade or so ago. Previously my favorite result of Tardos was in the <a href="https://www.cs.cornell.edu/~eva/Gap.Between.Monotone.NonMonotone.Circuit.Complexity.is.Exponential.pdf">paper</a>, “The Gap Between Monotone And Non-Monotone Circuit Complexity Is Exponential.” Now not so clear.</p>
<p>
What is your favorite of her results?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/04/22/a-paradox/"><span class="datestr">at April 22, 2022 04:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/missingness/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/missingness/">Missingness Bias in Model Debugging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2204.08945" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/missingness" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br /></p>

<p><i>
“Missingness”, or the absence of features from an input, is a concept that is fundamental to many model debugging tools. In our <a href="https://arxiv.org/abs/2204.08945">latest paper</a>, we examine the challenges of implementing missingness in computer vision. In particular, we demonstrate how current approximations of missingness introduce biases into the debugging process of computer vision models. We then show how a natural implementation of missingness based on VITs can mitigate these biases and lead to more reliable model debugging.
</i></p>

<p>Deep learning models can learn powerful features. However, these learned features can be unintuitive or spurious. Indeed, recent studies have pointed out that ML models often leverage unexpected—and, in fact, undesirable—associations. These include image pathology detection models that rely on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologist</a> and image classifiers that focus too much on <a href="https://arxiv.org/abs/2006.09994">backgrounds</a> or on <a href="https://arxiv.org/abs/1811.12231">texture</a>. Such prediction mechanisms can cause models to fail in downstream tasks or new environments.</p>

<p>How can we detect model failures caused by relying on such brittle or undesirable associations? Answering this question is a major goal of model debugging. Work in this context brought forth techniques that allow, for example, surfacing <a href="https://arxiv.org/abs/1704.05796">human aligned</a> <a href="https://gradientscience.org/glm_saga/">concepts</a>, <a href="https://gradientscience.org/3db/">probing specific types of bias a model uses</a>, or highlighting features that were <a href="https://arxiv.org/abs/1904.07451">important for a</a> <a href="https://arxiv.org/abs/1602.04938">specific prediction</a>.</p>

<p>A common theme in many of these debugging methods is to study model behavior on so-called counterfactual inputs, i.e., inputs with and without specific features. For example, consider the image of a dog being held by its owner below. By removing the owner from the image, we can study how much our model’s prediction depends on the presence of a human. In a similar vein, we can remove parts of the dog (head, body, paws) to identify which ones among them are most critical for correctly classifying the image. This concept of “the absence of a feature” from the input is sometimes referred to as <a href="https://distill.pub/2020/attribution-baselines/">missingness</a>.</p>

<p><img src="https://gradientscience.org/assets/missingness/woman_dog_example.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
        By removing parts of the image, we can study how much our model depends on a given input feature (such as the human, or the dog's snout).  
</div>

<p>Indeed, this primitive of missingness is used quite a lot in model debugging techniques. For example, widely-used methods such as <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1703.01365">integrated gradients</a> leverage it. It also has been applied to <a href="https://arxiv.org/abs/1802.07623">radiology images</a> to understand the regions of the scan that are important for diagnoses, or to <a href="https://arxiv.org/abs/2110.04301">flag spurious correlations</a>. Finally, in natural language processing, model designers often remove words <a href="https://arxiv.org/abs/2010.12487">to understand their impact on the output</a>.</p>

<p><img src="https://gradientscience.org/assets/missingness/main_lime.png" style="width: 100%;" alt="Example of LIME" /></p>
<div class="footnote">
LIME, a popular model debugging method, relies on the  missingness primitive to generate model explanations. The method first generates a set of ablations by randomly removing parts of the image. These ablations are then used to generate a model explanation, typically obtained by fitting a sparse linear model to them. 
</div>

<h2 id="challenges-of-implementing-missingness-in-computer-vision">Challenges of implementing missingness in computer vision</h2>

<p>Missingness is a rather intuitive notion: we simply would like the model to predict as if the corresponding part of the input didn’t exist. Also, in a domain such as NLP implementing this primitive is fairly straightforward: we simply drop the corresponding words from the sentence. However, in the context of computer vision, its proper implementation turns out to be much more challenging. This is because images are spatially contiguous objects: it is unclear how to leave a “hole” in the image.</p>

<p>Researchers thus have come up with all kinds of ways to “fill” such hole, i.e., approximating missingness by replacing the region with other pixels. This involves <a href="https://arxiv.org/pdf/1703.01365.pdf">blacking out the pixels</a>, replacing them with <a href="https://arxiv.org/pdf/1706.03825.pdf">random noise</a>, or <a href="https://arxiv.org/pdf/1704.03296.pdf">blurring the corresponding image region</a>. However, these approaches, even though very natural, turn out to have unintended effects. For example, researchers found that <a href="https://distill.pub/2020/attribution-baselines/">saliency maps generated with integrated gradients are quite sensitive to the chosen baseline color of the filling</a>, and thus can change significantly based on the (arbitrary) choice of that missingness approximation.</p>

<p><img src="https://gradientscience.org/assets/missingness/various_approx_original.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
    Practitioners often approximate missingness in computer vision by replacing the removed region with other types of pixels. Examples include blacking out, using the mean pixel color, filling with random noise, or blurring the image.
</div>



<h2 id="missingness-approximations-can-create-bias">Missingness approximations can create bias</h2>

<p>So, what impact do such missingness approximations actually have on the resulting model predictions? In our <a href="https://arxiv.org/abs/2204.08945">recent work</a>, we systematically investigate the bias that these approximations can imbue. Specifically, <i>we find that models end up using masked out regions to make predictions rather than simply ignoring them.</i></p>

<p>For example, take a look below at the image of a spider from ImageNet, where we have removed various regions from the image by blacking out the corresponding pixels.</p>

<div style="overflow: auto; text-align: center;" id="bias_examples_widget"></div>
<div class="footnote">
    Various regions of an image are removed by blacking out pixels. Regardless of what regions from the image are masked, a ResNet-50 outputs a wrong class. 
</div>

<p>It turns out that irrespective of what subregions of the image are removed, a standard CNN (e.g., ResNet-50) outputs incorrect classes, even when most of the foreground object is not obscured. In fact, taking a closer look at the randomly masked images, we find that the model seems to be relying on the masking pattern itself to make the prediction (e.g., predicting class “crossword”).</p>

<p>To analyze this bias more quantitatively, we measure how missingness approximations impact the output class distribution of a Resnet-50 classifier (over the whole dataset).  Specifically, we iteratively black out parts of ImageNet images, and keep track of how the probability of predicting any one class changes. Before blacking out any pixels, the model predicts a roughly equal number of each class on the ImageNet test set (which is what one would expect given that this test set is class balanced). However, when we apply missingness approximations, this distribution skews heavily toward classes such as maze, crossword puzzle, and carton.</p>

<div id="anno"> 
    <canvas width="30%" id="resnet_bar_chart" height="15%"></canvas>
</div>
<div class="footnote">
    We measure the shift in output class distribution after applying a (blacking out) missingness approximations. As patches are increasingly blacked out, the ResNet-50 classifier's predictions skew from a uniform distribution toward a few specific classes such as maze, crossword puzzle, and carton.
</div>

<p>Our <a href="https://arxiv.org/abs/2204.08945">paper</a> investigates this missingness bias in more depth by exploring different approximations, datasets, mask sizes, etc.</p>

<h2 id="missingness-bias-in-model-debugging-a-case-study-on-lime">Missingness bias in model debugging: A case study on LIME</h2>
<p>As we mentioned earlier, missingness primitive is often used by model debugging techniques—how does the above-mentioned bias impact them? To answer this question, we focus on a popular feature attribution method: local interpretable model-agnostic explanations, or <a href="https://arxiv.org/abs/1602.04938">LIME</a>.</p>

<div style="overflow: auto; text-align: center;" id="lime_examples_widget"></div>
<div class="footnote">
    Examples of generated LIME explanations represented as a heat map. High-intensity regions contribute more to the prediction of the model than low-intensity ones. On the right, the top 20 image regions identified by the LIME explanation are masked in black.
</div>

<p>The image above depicts an example LIME explanation generated for a ResNet-50 classifier. Specifically, for a given image, LIME generates a heatmap highlighting the regions of the image based on how much they impact prediction (according to LIME). While sometimes LIME highlights the foreground object (which is what one would expect), in most cases it also highlights rather irrelevant regions scattered all over the image. Why is this the case?</p>

<p>Although we can’t say for sure, we suspect that this behavior is caused in large part by the fact that the underlying ResNet-50 classifier relies on masked regions in the image to make predictions, and this tricks it into believing that these regions are important. In <a href="https://arxiv.org/abs/2204.08945">our paper</a>, we perform a more detailed quantitative analysis of the missingness bias in LIME explanations, and find that it indeed makes them more inconsistent and, during evaluation, indistinguishable from random explanations.</p>

<h2 id="a-more-natural-implementation-of-missingness">A more natural implementation of missingness</h2>
<p>What is the right way to represent missing pixels then? Ideally, since replacing pixels with other pixels can lead to missingness bias, we would like to be able to remove these regions altogether. But what about the need of having the images be represented by contiguous inputs?</p>

<p>Certainly, convolutional neural networks (CNNs) require such spatial contiguity because convolutions slide filters across the image. But do we even need to use CNNs? Not really!</p>

<p>How about we turn to a different architecture:  <a href="https://arxiv.org/abs/2010.11929">vision transformers</a> (ViTs)? Unlike CNNs, ViTs do not use convolutions and operate instead on sets of patches that correspond to positionally encoded regions of the image. Specifically, a ViT has two stages when processing an input image:</p>

<ul>
  <li><strong>Tokenization</strong>: split the image into square patches, and positionally encode these patches into tokens.</li>
  <li><strong>Self-attention</strong>: pass the set of tokens through several self-attention layers.</li>
</ul>

<p>After tokenization, the self-attention layers operate on the set of tokens rather than the entire image. Thus, ViTs enables a far more natural implementation of missingness: <i>we can simply drop the tokens that encode the region we want to remove!</i></p>

<p><img src="https://gradientscience.org/assets/missingness/vit_dropping.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
    ViTs split the image into a set of tokens, where each token represents a patch in the image. In order to implement missingness, we can simply drop the tokens corresponding to the regions we would like to remove.
</div>

<h2 id="mitigating-missingness-bias-through-vits">Mitigating missingness bias through ViTs</h2>

<p>As we now demonstrate,  using this more natural implementation of missingness turns out to substantially mitigate the missingness bias that we saw earlier. Indeed, recall that, with a ResNet-50, blacking out pixels biased the model toward specific classes (such as crossword). But, a similarly sized ViT-small (ViT-S) architecture is able to side-step this bias (when we implement token dropping), maintaining a correct (or at least related) prediction.</p>

<div style="overflow: auto; text-align: center;" id="bias_examples_widget2"></div>
<div class="footnote">
   When we remove image regions by dropping tokens with a ViT-S classifier, the model maintains its original prediction or, at least, predicts a related class.
</div>

<p>This difference is even more striking when we examine the output class distribution corresponding to removal of image regions: while for our ResNet-50 classifier we have seen this distribution being skewed toward certain classes, for the ViT classifier, this distribution remains close to uniform.</p>

<div id="anno"> 
    <canvas width="30%" id="vit_bar_chart" height="15%"></canvas>
</div>
<div class="footnote">
The shift in output class distribution after dropping tokens using a ViT architecture. In contrast to the ResNet-50 classifier, ViT largely maintains its original uniform class distribution.
</div>

<h2 id="improving-model-debugging-with-vits">Improving model debugging with ViTs</h2>

<p>So, we have seen that ViTs allow us to mitigate missingness bias. Can we thus use ViTs to improve model debugging too? We return to our case study of LIME. As we demonstrated earlier, LIME relies on a notion of missingness, and is thus vulnerable to missingness bias when using approximations like blacking out pixels. What happens when we use ViTs and token dropping instead?</p>

<div style="overflow: auto; text-align: center;" id="lime_examples_widget2"></div>
<div class="footnote">
Examples of generated LIME explanations for a ResNet (blacking out pixels) and a ViT (dropping tokens). 
</div>

<p>The figure above displays examples of the LIME explanations generated for a ResNet classifier and for a ViT classifier (dropping tokens). Qualitatively, one can see that the explanations for the ViT seem more aligned with human intuition, highlighting the main object instead of regions in the background. We confirm these observations with a more quantitative analysis in <a href="https://arxiv.org/abs/2204.08945">our paper</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we studied how missingness approximations can lead to biases and, in turn, impact the model debugging techniques that leverage them. We also demonstrated how transformer-based architectures can enable a seamless implementation of missingness that allows us to side-step missingness bias and lead to more reliable model debugging.</p></div>







<p class="date">
<a href="https://gradientscience.org/missingness/"><span class="datestr">at April 20, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/">IDEAL Workshop on Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
April 22-23, 2022 Northwestern University &amp; Online https://www.ideal.northwestern.edu/events/clustering/ We are inviting you to attend the IDEAL Workshop on Clustering. The workshop will take place at Northwestern University on Friday, April 22, and Saturday, April 23. It will be in a hybrid format. If you are interested in participating in the workshop (in-person or remotely), please … <a href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on Clustering</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/"><span class="datestr">at April 19, 2022 03:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/">Postdoc at University of Texas at San Antonio (apply by April 30, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applicants with background in one of the following fields are invited to apply: discrete and convex geometry, algebraic algorithms, convex optimization, randomized numerical analysis, average case complexity theory. One year research only position. Please send your CV, a letter on your technical background and research interests, and two references for recommendation letters to Alperen Ergur.</p>
<p>Website: <a href="http://alpergur.xyz">http://alpergur.xyz</a><br />
Email: alperen.ergur@utsa.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/"><span class="datestr">at April 18, 2022 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/052">TR22-052 |  Verifying The Unseen: Interactive Proofs for Label-Invariant Distribution Properties | 

	Tal Herman, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given i.i.d. samples from an unknown distribution over a large domain $[N]$, approximating several basic quantities, including the distribution's support size, its entropy, and its distance from the uniform distribution, requires $\Theta(N / \log N)$ samples [Valiant and Valiant, STOC 2011].

Suppose, however, that we can interact with a powerful but untrusted prover, who knows the entire distribution (or a good approximation of it). Can we use such a prover to approximate (or rather, to approximately {\em verify}) such statistical quantities more efficiently? We show that this is indeed the case: the support size, the entropy, and the distance from the uniform distribution, can all be approximately verified via a 2-message interactive proof, where the communication complexity, the verifier's running time, and the sample complexity are $\widetilde{O}({\sqrt{N}})$. For all these quantities, the sample complexity is tight up to $\polylog N$ factors (for any interactive proof, regardless of its communication complexity or verification time).

More generally, we give a tolerant interactive proof system with the above sample and communication complexities for verifying a distribution's proximity to any label-invariant property (any property that is invariant to re-labeling of the elements in the distribution's support). The verifier's running time in this more general protocol is also $\widetilde{O}({\sqrt{N}})$, under a mild assumption about the complexity of deciding, given a compact representation of a distribution, whether it is in the property or far from it.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/052"><span class="datestr">at April 18, 2022 06:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/051">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/051">TR22-051 |  Low Degree Testing over the Reals | 

	Noah Fleming, 

	Vipul Arora, 

	Arnab Bhattacharyya, 

	Esty Kelman, 

	Yuichi Yoshida</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the problem of testing whether a function $f: \mathbb{R}^n \to \mathbb{R}$ is a polynomial of degree at most $d$ in the distribution-free testing model. Here, the distance between functions is measured with respect to an unknown distribution $\mathcal{D}$ over $\mathbb{R}^n$ from which we can draw samples. In contrast to previous work, we do not assume that $\mathcal{D}$ has finite support. 
    
We design a tester that given query access to $f$, and sample access to $\mathcal{D}$, makes $(d/\varepsilon)^{O(1)}$ many queries to $f$, accepts with probability $1$ if $f$ is a polynomial of degree $d$, and rejects with probability at least $2/3$ if every degree-$d$ polynomial $P$ disagrees with $f$ on a set of mass at least $\varepsilon$ with respect to $\mathcal{D}$. Our result also holds under mild assumptions when we receive only a polynomial number of bits of precision for each query to $f$, or when $f$ can only be queried on rational points representable using a logarithmic number of bits. Along the way, we prove a new stability theorem for multivariate polynomials that may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/051"><span class="datestr">at April 18, 2022 06:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8738724093968037845">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html">1-week long Summer School for Ugrads Interested in Theory, and my comments on it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Recently a grad student in CS at UMCP emailed me the following email he got,  thinking (correctly) that I should forward it to interested ugrads. </p><p>-------------------------------------------------------</p><p>Are you interested in theoretical computer science including topics like algorithms, cryptography, machine learning, and others? If so, please consider applying to the New Horizons in Theoretical Computer Science week-long online summer school! The school will contain several mini-courses from top researchers in the field. The course is free of charge,and we welcome applications from undergraduates majoring in computer science or related fields. We particularly encourage applications from students that are members of groups that are currently under-represented in theoretical computer science.</p><p>Students from previous years have shared with us that the mini-lectures, online group activities, and interactions with other students and the friendly TAs were extraordinarily engaging and fun.</p><p>For full consideration, please complete the application (it’s short and easy!) by April 25, 2022. The summer school will take place online from June 6 to June 10.</p><p>Please see our website for details: see <a href="https://tcs-summerschool.ttic.edu/">here</a> </p><p>Any questions can be directed to summer-school-admin-2022@ttic.edu.</p><p>--------------------------------------------------------------------------</p><p>A few points about this</p><p>1) I emailed them asking `why do people need to apply if its online and free?'</p><p>I had one answer in mind, but they gave me another one</p><p><i>Their Answer: </i>They want to have SMALL online activities in groups. If they had X students and want groups of size g then if X is large, X/g may be too large. </p><p><i>My Answer</i>: If people REGISTER for something they are more likely to actually show up. (I know of a conference that got MORE people going once they had registation, and even MORE when they began charging for it.) </p><p>2) I emailed them asking if the talks will, at some later point, be on line. They will be. I then realized that there are already LOTS of theory talks online that I have not gotten around to watching, and perhaps never will. Even so, the talks on line may well benefit people who goto the summer school if they want to look back and something. </p><p>3) Online conferences PROS and CONS:</p><p>PROS: Free (or very low cost), no hassle getting airfare and hotel, and if talks are recorded then you can see them later (that applies to in-person as well). </p><p>CONS: Less committed to going to it. Can go in a half-ass way. For example, you can go and then in the middle of a talk go do your laundry. Being FORCED to be in a ROOM with the SPEAKER may be good. Also, of course, no informal conversations in the hallways.  Also, less serendipity. </p><p>I want to say <i>It would to be good to see talks outside of my area </i>however, this may only be true for easy talks, perhaps talks in a new field, OR talks that are just barely outside my area so I have some context. </p><p>4) I was surprised I didn't get the email directly since I have more contact with ugrads (and I have this blog) then the grad student who alerted me to it. However, I have learned that information gets to people in random ways so perhaps not to surprising. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html"><span class="datestr">at April 18, 2022 01:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/04/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/04/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://twitter.com/RobFathauerArt/status/1494044593486131200">As Robert Fathauer points out</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108064413451021876">\(\mathbb{M}\)</a>),</span> the <a href="https://www.nmc.edu/about/nmc-stories/success-stories/viscomm-grad-designs-stamp.html">US “Star Ribbon” postage stamp</a> depicts a star-shaped Möbius strip. If you slice the depicted ribbon lengthwise into two linked rings, its central white portion and outer red and blue portions, the inner part is again a Möbius strip while the outer part becomes doubly twisted and twice as long, with red on one side and blue on the other.</p>
  </li>
  <li>
    <p>When its Japanese-American students were released from the concentration camps at the end of World War II, the University of Southern California refused to readmit them or even allow them to transfer their credits elsewhere <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108069391491491798">\(\mathbb{M}\)</a>).</span> Now, 80 years after the students were taken to the camps, <a href="https://www.latimes.com/california/story/2022-04-02/usc-offers-posthumous-degrees-japanese-students">USC is giving them posthumous honorary degrees</a>.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/father-son-team-solves-geometry-problem-with-infinite-folds-20220404/">Erik and Marty Demaine profiled for their work on squishing polyhedral surfaces flat</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108074681771364763">\(\mathbb{M}\)</a>),</span> in <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://www.npr.org/2022/04/01/1090279187/russia-wikipedia-fine">Russian government threatens to fine Wikipedia for not sticking to Russian propaganda in its reporting of the invasion of Ukraine</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108082233497808371">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30899708">via</a>, <a href="https://boingboing.net/2022/04/05/putins-government-threatens-to-fine-wikipedia-for-publishing-unreliable-socially-significant-information.html">see also</a>). Laughable, but the likely outcome appears to be that Wikipedia becomes blocked within Russia (as there is no possibility of complying with these demands) and Russia deepens its self-imposed isolation from civilized society.</p>
  </li>
  <li>
    <p>Longstanding but still-active theoretical CS blog “Gödel’s Lost Letter and P=NP” makes a <a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/">roundup of other still-active math and theoretical CS blogs</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108089353162729425">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.acm.org/articles/bulletins/2022/april/50-years-backfile">ACM makes its early publications, from 1951 to 2000, open access</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108091645356237331">\(\mathbb{M}\)</a>).</span> It’s not everything I’d want, but it’s a start…</p>
  </li>
  <li>
    <p>You’re probably aware that Twitter has gradually joined Facebook and Instagram in deliberately walling itself off from the open web and making itself close-to-unusable by those of us without accounts. Manuel Grabowski writes that <a href="https://annoying.technology/posts/e6901c0ea272f57d/">it’s also becoming significantly less usable even by those with accounts</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108100999866052164">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30949048">via</a>).</p>
  </li>
  <li>
    <p>I’m sad to hear that <a href="https://algo.rwth-aachen.de/">algorithmist Gerhard Woeginger died recently, only aged 57</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108106575959194943">\(\mathbb{M}\)</a>).</span> Much of his early work was in computational geometry (how we became coauthors), and I’ve posted on other papers of his, but some of my favorites include:</p>

    <ul>
      <li>
        <p>Surveys on exact algorithms for hard problems, in <a href="https://doi.org/10.1007/3-540-36478-1_17"><em>Combinatorial Optimization — Eureka, You Shrink!</em></a> and <a href="https://doi.org/10.1016/j.dam.2007.03.023"><em>Disc. Appl. Math.</em> 2007</a>.</p>
      </li>
      <li>
        <p>Arranging lines with given numbers of crossings, in <a href="https://doi.org/10.1016/j.tcs.2004.04.006"><em>Theor. Comput. Sci.</em> 2004</a>.</p>
      </li>
      <li>
        <p>On river-crossing puzzles, in <a href="https://doi.org/10.1137/080736661"><em>SIAM J. Disc. Math.</em> 2010</a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/108106319690622089">Christian Lawson-Perfect finds his perfect love match</a>: someone who agrees on the best method of stacking eggs. Which raises “Kepler’s egg-packing problem”: what is that  method, in an egg box of bounded width?</p>

    <p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/kepleregg/Kepler-m.jpg" style="border-style: solid; border-color: black;" alt="Eggs stacked in an egg box" /></p>
  </li>
  <li>
    <p>It’s the long-overdue Python2 apocalypse! <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108114873951435174">\(\mathbb{M}\)</a>).</span>  I upgraded my laptop to MacOS 12.3.1 and somehow the upgrade nuked any old python2 executables I might have had in my path, so from now on everything has to be python3. Fortunately any of my old python2 scripts that I’ve tried to use so far have been very easy to fix (most complicated issue: installing Pillow to replace PIL). But I suspect that if I ever want to update and rebuild my old PyObjC-based apps, the changes will be more substantial.</p>
  </li>
  <li>
    <p><a href="https://thehighergeometer.wordpress.com/2022/04/12/its-a-messy-job-but-someone-had-to-do-it-fixing-all-the-links/">David Roberts on fixing all the old broken links from StackExchange to the “Front for the Mathematics arXiv”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108125670540879313">\(\mathbb{M}\)</a>),</span> a user interface veneer for arXiv that used to be available from UC Davis and has since gone away. You would think it would be a small matter of parsing the arXiv id in the link and replacing it with a current arXiv url for the same id, but…</p>
  </li>
  <li>
    <p><a href="https://www.science.org/content/article/russian-website-peddles-authorships-linked-reputable-journals">You may have seen <em>Science</em> on cracking a Russian pay-for-coauthorship ring</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108129289678280113">\(\mathbb{M}\)</a>).</span> As the <a href="https://arxiv.org/abs/2112.13322">associated preprint arXiv:2112.13322</a> details, many reputable publishers had their journals caught up in publications of this ring. But it also points to suspicious patterns in MDPI journals, where several people from one country were both coauthors and editors of ring papers, and coauthorship slots were advertised as reserved for those editors.</p>
  </li>
  <li>
    <p>Zugzwang describes a situation where moving first is a disadvantage, especially in games where it might usually be an advantage <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108136997760810042">\(\mathbb{M}\)</a>).</span> <em>Quanta</em> outlines <a href="https://www.quantamagazine.org/zugzwang-in-chess-math-and-pizzas-20220222/">four zugzwang puzzles in games ranging from chess to pizza-division</a>. <a href="https://www.quantamagazine.org/the-secrets-of-zugzwang-in-chess-math-and-pizzas-20220408/">Solutions</a>.</p>
  </li>
  <li>
    <p><a href="https://www.acm.org/media-center/2022/april/allen-award-2021">The ACM has given Carla Brodley their inaugural ACM Frances E. Allen Award for Outstanding Mentoring</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108139193039016625">\(\mathbb{M}\)</a>),</span> citing her work to improve the representation of women among students and faculty in her work as dean of computer science at Northeastern University, her founding of the Center for Inclusive Computing, which helps other institutions make similar improvements, and her leadership on the Computing Research Association Committee on Widening Participation.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/04/15/linkage.html"><span class="datestr">at April 15, 2022 06:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/">AMSI–AustMS Workshop on Bridging Maths and Computer Science</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 31 – June 3, 2022 Sydney, Australia https://sites.google.com/view/2022-workshop-bridgingmathstcs/ This 4-day workshop will bring together Australian and Australasian researchers in mathematics and theoretical computer science, in view of fostering exchanges and collaborations. Specifically, the workshop will focus on two themes, “Computational Complexity and Cryptography” and “Graph Theory and Combinatorics,” from the point of view of … <a href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/" class="more-link">Continue reading <span class="screen-reader-text">AMSI–AustMS Workshop on Bridging Maths and Computer Science</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/"><span class="datestr">at April 15, 2022 07:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/">IDEAL Workshop on “Clustering” -Friday &amp; Saturday, April 22-23, 2022, 8:40 am-4:00 pm Central Time in Mudd Library 3514</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
April 22-23, 2022 Mudd Library 3514 https://www.ideal.northwestern.edu/events/clustering/ IDEAL Workshop on Clustering. The workshop will take place at Northwestern University on Friday, April 22, and Saturday, April 23, 8:40 am- 4:00 pm CST (Chicago Time) in MUDD 3514. It will be in a hybrid format. If you are interested in participating in the workshop (in-person or … <a href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on “Clustering” -Friday &amp; Saturday, April 22-23, 2022, 8:40 am-4:00 pm Central Time in Mudd Library 3514</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/"><span class="datestr">at April 15, 2022 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=126">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2022/04/14/wednesday-april-20th-2022-gabriel-peyre-from-cnrs-and-ens/">Wednesday April 20th 2022 — Gabriel Peyré from CNRS and ENS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 20th</strong> at <strong>12:00 PM Pacific Time</strong> (15:00 Eastern Time, 21:00 Central European Time, 19:00 UTC). <a href="http://www.gpeyre.com/">Gabriel Peyré</a> from<strong> CNRS and Ecole Normale Supérieure</strong> will speak about “Scaling Optimal Transport for High dimensional Learning.<em>”</em></p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p><strong>Abstract</strong>:  Optimal transport (OT) has recently gained lot of interest in machine learning. It is a natural tool to compare in a geometrically faithful way probability distributions. It finds applications in both supervised learning (using geometric loss functions) and unsupervised learning (to perform generative model fitting). OT is however plagued by the curse of dimensionality, since it might require a number of samples which grows exponentially with the dimension. In this talk, I will explain how to leverage entropic regularization methods to define computationally efficient loss functions, approximating OT with a better sample complexity. More information and references can be found on <a href="https://optimaltransport.github.io/">the website of our book</a> “Computational Optimal Transport.”</p>



<p> The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2022/04/14/wednesday-april-20th-2022-gabriel-peyre-from-cnrs-and-ens/"><span class="datestr">at April 14, 2022 01:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=616">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/04/14/tcs-talk-wednesday-april-20-rasmus-kyng-eth-zurich/">TCS+ talk: Wednesday, April 20 — Rasmus Kyng, ETH Zürich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://rasmuskyng.com/"><strong>Rasmus Kyng</strong></a> from ETH Zürich will speak about “<em>Almost-Linear Time Algorithms for Maximum Flow and More</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We give the first almost-linear time algorithm for computing exact maximum flows and minimum-cost flows on directed graphs. By well-known reductions, this implies almost-linear time algorithms for several problems including bipartite matching, optimal transport, and undirected vertex connectivity.</p>
<p>Our algorithm uses a new Interior Point Method (IPM) that builds the optimal flow as a sequence of an almost-linear number of approximate undirected minimum-ratio cycles, each of which is computed and processed very efficiently using a new dynamic data structure.</p>
<p>Our framework extends to give an almost-linear time algorithm for computing flows that minimize general edge-separable convex functions to high accuracy. This gives the first almost-linear time algorithm for several problems including entropy-regularized optimal transport, matrix scaling, p-norm flows, and Isotonic regression.</p>
<p>Joint work with Li Chen, Yang Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/04/14/tcs-talk-wednesday-april-20-rasmus-kyng-eth-zurich/"><span class="datestr">at April 14, 2022 12:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6339126729709498263">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">The Roeder Seq Problems was Solved Before I Posed it (Math)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>  (Joint Post by Bill Gasarch, David Harris, and Tomas
Harris) </p><p class="MsoNormal"></p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">The divisor graph D(n) is an undirected graph with</p>

<p class="MsoNormal">vertex set V={1,...,n}$ and</p>

<p class="MsoNormal">edge set E={(a,b) :  a  divides  b 
or  b  divides  a }</p>

<p class="MsoNormal">We denote the length of the longest simple path in D(n) by
L(n).</p>

<p class="MsoNormal">EXAMPLE: if n=10 then one long-ish sequence is</p>

<p class="MsoNormal">1,8,4,2,6,3,9</p>

<p class="MsoNormal">so L(10) GE 7. I leave it to the reader to do better OR to
show its optimal. </p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">In 2017 Oliver Roeder asked for L(100) (see <a href="https://fivethirtyeight.com/features/pick-a-number-any-number/">here</a>)
In a later post Roeder reported that Anders Kaseorg claimed L(100)=77
(see  <a href="https://fivethirtyeight.com/features/is-this-bathroom-occupied/">here</a>).
Anders gave a sequence and claimed that, by a computer search, this was
optimal. The column also claims that other people also claimed 77 and nobody
got a sequence of length 78, so the answer probably is 77 (it is now known that
it IS 77).  Roeder also mentions the case of n=1000 for which Kaseorg
showed L(1000) GE 418. No nontrivial lower bounds are known. </p>

<p class="MsoNormal">In 2019 I (Gasarch) asked about asymptotic results for
L(n)  (see my blog post <a href="https://blog.computationalcomplexity.org/2017/09/a-problem-i-thought-was-interesting-now.html">here</a> and
my open problems column <a href="https://www.cs.umd.edu/~gasarch/open/nate.pdf">here</a>.)
I began working on it with David and Tomas Harris. David proved that </p>

<p class="MsoNormal">Omega( n/( (log n)^{1.68} )  LE  L(n) 
LE  O( n/( (log n)^{0.79} ).</p>

<p class="MsoNormal">We also studied human-readable proofs that L(100) LE X for
some reasonable X, though getting a human-readable proof for X=77 seemed
impossible. We did get L(100) LE 83, in a human-readable proof. (Some
commenters on my post to sketched a proof  that L(100) LE 83 and another
that L(100) LE 80 as well.) </p>

<p class="MsoNormal"> But it turned out that this problem had already been
studied, predating Roeder's column. (This blog post is all about the math, bout
the math, no treble.  My next post will be about how we didn't know the
literature until our paper was close to being finished.) </p>

<p class="MsoNormal">In 1982 Pomerance showed L(n)  LE o(n) (see <a href="https://math.dartmouth.edu/~carlp/divisorgraph.pdf">here</a>).
Pollington had earlier shown </p>

<p class="MsoNormal">               
                     
    L(n) GE ne^{polylog(n)};</p>

<p class="MsoNormal">however, the paper is not online and hence is lost to
history forever. (If you can find an online copy please email me the pointer
and I will edit this post.) </p>

<p class="MsoNormal">In 1995 Gerald Tenenbaum showed, in a paper written in
French,  that there exists a,b such that </p>

<p class="MsoNormal">               
               n/(log n)^a LE L(n) LE
n/(log n)^b (see <a href="http://www.numdam.org/item/ASENS_1995_4_28_2_115_0/">here</a>). </p>

<p class="MsoNormal">More recently, in 2021, Saias showed, in a paper written in
French, that </p>

<p class="MsoNormal">               
                     
L(n) GE (0.3 - o(1)) n/log n (see <a href="https://arxiv.org/abs/2107.03855">here</a>). </p><p class="MsoNormal">(ADDED LATER:  I got a very angry email telling me that the paper was in English and that I am a moron. It turns out that the abstract is in English but the paper is in French, hence the person who send the letter only read the abstract which explains their mistake.) </p>

<p class="MsoNormal">He conjectures that L(n)  SIM cn/log n where c is
likely in the interval [3,7]. (Apparently, no other information is known about
the relevant constant factors in the estimates.)</p>

<p class="MsoNormal">Interestingly, the work of Tenenbaum and Saias also
demonstrates why the study of L(n)  is not an idle problem in recreational
mathematics. The upper bounds come from results on certain density conditions
for prime factorization of random integers. That is, given an integer x chosen
uniformly at random from the range {1,..., n} with prime factorization p1 GE p2
GE ... one wants to show that, with high probability, the primes pi are close
to each other in a certain sense. Most recent results on L(n) have been tied
closely with improved asymptotic estimates for deep number theory problems.</p>

<p class="MsoNormal">Determining the value of L(100) (i.e., Roeder's problem) was
mentioned in Saias's paper. He claims that L(100) = 77 was discovered by Arnaud
Chadozeau, who himself has written a number of papers on other properties of
D(n). Since this paper was in 2021 it was after Roeder's column; however, we
believe that the different discoveries of L(100) are independent. The recent
work around Roeder's column appears to be done independently from the extensive
French-language literature on the topic.</p>

<p class="MsoNormal">The following problems are  likely still open:</p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">a) Find L(n) exactly for as many n as you can.  This
would clearly need a computer program.</p>

<p class="MsoNormal">A listing of L(n) for n = 1 ... 200, computed by Rob Pratt
and Nathan McNew,</p>

<p class="MsoNormal">appears as OEIS #A337125. This also includes additional
references.</p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">b) Find human-readable proofs for upper bounds on L(n)
(likely not exact) for as many</p>

<p class="MsoNormal">n as you can.</p>

<p class="MsoNormal">  </p><p class="MsoNormal"></p>

<p class="MsoNormal">ADDED LATER: Gaétan Berthe emailed me </p>

<p class="MsoNormal">-----------------------------------------------------------------</p><p class="MsoNormal"></p>

<p class="MsoNormal">I'm the author of the last comment on your article about
Roeder Sequence , as your curious about the subject I can share what we've done
with my friend Paul Revenant those last few years for fun.</p><p class="MsoNormal">
<br />
It all started with a competition between our classmates (see <a href="https://perso.ens-lyon.fr/gaetan.berthe/challenge.php">here</a> though note that its in French) for the 100 and 1000 cases, after a few months Paul using a MIP solver gurobi
was able to found a solution of size 666, and last year by studying the
structure of the 666 solution we were able, with the help of gurobi again, to
prove that there was no 667 solution either.<br />
<br />
Paul then achieve to find very probable value of the sequence for 1 to 1000 (we
didn't automatize the proof of 666 but it should be doable). On my side I tried
to look for good solutions for the 10000 case, again using gurobi and the structure
that appeared in the solution of size 666. The structure enable to cut the
problem in two subpart, so the search goes faster I was able to find a solution
of size 5505.<br />
<br />
So I would say that the two mains reasons we're able to prove optimality for high
numbers as 1000 are:<br />
<br />
- MIP solver such as gurobi are very powerful tools.<br />
<br />
- The longest path in the divisor graph are highly structured.<br />
<br />
I joined our informal proof of the 666 case (the solution at the end), what is
interesting is to understand how the solution is composed of different blocks
depending of the prime decomposition of the elements. I joined lower bound from
1 to 1000 computed by Paul, that are very likely to be optimal.</p>

<p class="MsoNormal">---------------------------------------------------------------------------------------------------------</p>

<p class="MsoNormal">He also emailed me</p><p class="MsoNormal"></p>

<p class="MsoNormal">1)  a list of the numbers I call L(n) for n=1 to 1000.
These have not been refereed though I think they are correct. The list is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/1-1000.txt">here</a></p><p class="MsoNormal"></p>

<p class="MsoNormal">AND</p>

<p class="MsoNormal">2)  a PROOF that L(1000)\le 666 (and they HAVE a
sequence of length 666, so L(1000)=666).</p>

<p class="MsoNormal">Again, not refereed, but you can read the proof
yourself <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/proof1000.pdf">here</a> WARNING- the proof is in ENGLISH, so you cannot use it to improve your
mathematical French. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html"><span class="datestr">at April 11, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
