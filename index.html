<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 08, 2019 04:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02709">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02709">Hiring Under Uncertainty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghavan:Manish.html">Manish Raghavan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a>, Sreenivas Gollupadi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02709">PDF</a><br /><b>Abstract: </b>In this paper we introduce the hiring under uncertainty problem to model the
questions faced by hiring committees in large enterprises and universities
alike. Given a set of $n$ eligible candidates, the decision maker needs to
choose the sequence of candidates to make offers so as to hire the $k$ best
candidates. However, candidates may choose to reject an offer (for instance,
due to a competing offer) and the decision maker has a time limit by which all
positions must be filled. Given an estimate of the probabilities of acceptance
for each candidate, the hiring under uncertainty problem is to design a
strategy of making offers so that the total expected value of all candidates
hired by the time limit is maximized. We provide a 2-approximation algorithm
for the setting where offers must be made in sequence, an 8-approximation when
offers may be made in parallel, and a 10-approximation for the more general
stochastic knapsack setting with finite probes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02709"><span class="datestr">at May 08, 2019 01:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02687">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02687">The algorithm for the recovery of integer vector via linear measurements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ryutin:K=_S=.html">K. S. Ryutin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02687">PDF</a><br /><b>Abstract: </b>In this paper we continue the studies on the integer sparse recovery problem
that was introduced in \cite{FKS} and studied in \cite{K},\cite{KS}. We provide
an algorithm for the recovery of an unknown sparse integer vector for the
measurement matrix described in \cite{KS} and estimate the number of
arithmetical operations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02687"><span class="datestr">at May 08, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02620">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02620">External Memory Planar Point Location with Fast Updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iacono:John.html">John Iacono</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karsin:Ben.html">Ben Karsin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koumoutsos:Grigorios.html">Grigorios Koumoutsos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02620">PDF</a><br /><b>Abstract: </b>We study dynamic planar point location in the External Memory Model or Disk
Access Model (DAM). Previous work in this model achieves polylog query and
polylog amortized update time. We present a data structure with $O( \log_B^2
N)$ query time and $O(\frac{1}{ B^{1-\epsilon}} \log_B N)$ amortized update
time, where $N$ is the number of segments, $B$ the block size and $\epsilon$ is
a small positive constant. This is a $B^{1-\epsilon}$ factor faster for updates
than the fastest previous structure, and brings the cost of insertion and
deletion down to subconstant amortized time for reasonable choices of $N$ and
$B$. Our structure solves the problem of vertical ray-shooting queries among a
dynamic set of interior-disjoint line segments; this is well-known to solve
dynamic planar point location for a connected subdivision of the plane.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02620"><span class="datestr">at May 08, 2019 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02592">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02592">Distributed Construction of Light Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elkin:Michael.html">Michael Elkin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neiman:Ofer.html">Ofer Neiman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02592">PDF</a><br /><b>Abstract: </b>A $t$-{\em spanner} $H$ of a weighted graph $G=(V,E,w)$ is a subgraph that
approximates all pairwise distances up to a factor of $t$. The {\em lightness}
of $H$ is defined as the ratio between the weight of $H$ to that of the minimum
spanning tree. An $(\alpha,\beta)$-{\em Shallow Light Tree} (SLT) is a tree of
lightness $\beta$, that approximates all distances from a designated root
vertex up to a factor of $\alpha$. A long line of works resulted in efficient
algorithms that produce (nearly) optimal light spanners and SLTs.
</p>
<p>Some of the most notable algorithmic applications of light spanners and SLTs
are in distributed settings. Surprisingly, so far there are no known efficient
distributed algorithms for constructing these objects in general graphs. In
this paper we devise efficient distributed algorithms in the CONGEST model for
constructing light spanners and SLTs, with near optimal parameters.
Specifically, for any $k\ge 1$ and $0&lt;\epsilon&lt;1$, we show a
$(2k-1)\cdot(1+\epsilon)$-spanner with lightness $O(k\cdot n^{1/k})$ can be
built in $\tilde{O}\left(n^{\frac12+\frac{1}{4k+2}}+D\right)$ rounds (where
$n=|V|$ and $D$ is the hop-diameter of $G$). In addition, for any $\alpha&gt;1$ we
provide an $(\alpha,1+\frac{O(1)}{\alpha-1})$-SLT in $(\sqrt{n}+D)\cdot
n^{o(1)}$ rounds. The running time of our algorithms cannot be substantially
improved.
</p>
<p>We also consider spanners for the family of doubling graphs, and devise a
$(\sqrt{n}+D)\cdot n^{o(1)}$ rounds algorithm in the CONGEST model that
computes a $(1+\epsilon)$-spanner with lightness $(\log n)/\epsilon^{O(1)}$. As
a stepping stone, which is interesting in its own right, we first develop a
distributed algorithm for constructing nets (for arbitrary weighted graphs),
generalizing previous algorithms that worked only for unweighted graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02592"><span class="datestr">at May 08, 2019 01:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02589">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02589">Order-Preserving Pattern Matching Indeterminate Strings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Costa:Diogo.html">Diogo Costa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henriques:Rui.html">Rui Henriques</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02589">PDF</a><br /><b>Abstract: </b>Given an indeterminate string pattern $p$ and an indeterminate string text
$t$, the problem of order-preserving pattern matching with character
uncertainties ($\mu$OPPM) is to find all substrings of $t$ that satisfy one of
the possible orderings defined by $p$. When the text and pattern are
determinate strings, we are in the presence of the well-studied exact
order-preserving pattern matching (OPPM) problem with diverse applications on
time series analysis. Despite its relevance, the exact OPPM problem suffers
from two major drawbacks: 1) the inability to deal with indetermination in the
text, thus preventing the analysis of noisy time series; and 2) the inability
to deal with indetermination in the pattern, thus imposing the strict
satisfaction of the orders among all pattern positions. This paper provides the
first polynomial algorithm to answer the $\mu$OPPM problem when indetermination
is observed on the pattern or text. Given two strings with length $m$ and
$O(r)$ uncertain characters per string position, we show that the $\mu$OPPM
problem can be solved in $O(mr\lg r)$ time when one string is indeterminate and
$r\in\mathbb{N}^+$. Mappings into satisfiability problems are provided when
indetermination is observed on both the pattern and the text, and results
concerning the general problem complexity are presented as well, with $\mu$OPPM
problem proved to be NP-hard in general.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02589"><span class="datestr">at May 08, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02518">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02518">Incorporating Weisfeiler-Leman into algorithms for group isomorphism</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brooksbank:Peter_A=.html">Peter A. Brooksbank</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grochow:Joshua_A=.html">Joshua A. Grochow</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yinan.html">Yinan Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qiao:Youming.html">Youming Qiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:James_B=.html">James B. Wilson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02518">PDF</a><br /><b>Abstract: </b>In this paper we combine many of the standard and more recent algebraic
techniques for testing isomorphism of finite groups (GpI) with combinatorial
techniques that have typically been applied to Graph Isomorphism. In
particular, we show how to combine several state-of-the-art GpI algorithms for
specific group classes into an algorithm for general GpI, namely: composition
series isomorphism (Rosenbaum-Wagner, Theoret. Comp. Sci., 2015; Luks, 2015),
recursively-refineable filters (Wilson, J. Group Theory, 2013), and low-genus
GpI (Brooksbank-Maglione-Wilson, J. Algebra, 2017). Recursively-refineable
filters -- a generalization of subgroup series -- form the skeleton of this
framework, and we refine our filter by building a hypergraph encoding low-genus
quotients, to which we then apply a hypergraph variant of the k-dimensional
Weisfeiler-Leman technique. Our technique is flexible enough to readily
incorporate additional hypergraph invariants or additional characteristic
subgroups.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02518"><span class="datestr">at May 08, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02472">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02472">Self-Adjusting Linear Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avin:Chen.html">Chen Avin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duijn:Ingo_van.html">Ingo van Duijn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmid:Stefan.html">Stefan Schmid</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02472">PDF</a><br /><b>Abstract: </b>Emerging networked systems become increasingly flexible and reconfigurable.
This introduces an opportunity to adjust networked systems in a demand-aware
manner, leveraging spatial and temporal locality in the workload for online
optimizations. However, it also introduces a trade-off: while more frequent
adjustments can improve performance, they also entail higher reconfiguration
costs.
</p>
<p>This paper initiates the formal study of linear networks which self-adjust to
the demand in an online manner, striking a balance between the benefits and
costs of reconfigurations. We show that the underlying algorithmic problem can
be seen as a distributed generalization of the classic dynamic list update
problem known from self-adjusting datastructures: in a network, requests can
occur between node pairs. This distributed version turns out to be
significantly harder than the classical problem in generalizes. Our main
results are a $\Omega(\log{n})$ lower bound on the competitive ratio, and a
(distributed) online algorithm that is $O(\log{n})$-competitive if the
communication requests are issued according to a linear order.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02472"><span class="datestr">at May 08, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02469">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02469">Robust two-stage combinatorial optimization problems under convex uncertainty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasperski:Adam.html">Adam Kasperski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zielinski:Pawel.html">Pawel Zielinski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02469">PDF</a><br /><b>Abstract: </b>In this paper a class of robust two-stage combinatorial optimization problems
is discussed. It is assumed that the uncertain second stage costs are specified
in the form of a convex uncertainty set, in particular polyhedral or
ellipsoidal ones. It is shown that the robust two-stage versions of basic
network and selection problems are NP-hard, even in a very restrictive cases.
Some exact and approximation algorithms for the general problem are
constructed. Polynomial and approximation algorithms for the robust two-stage
versions of basic problems, such as the selection and shortest path problems,
are also provided.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02469"><span class="datestr">at May 08, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02424">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02424">Equal-Subset-Sum Faster Than the Meet-in-the-Middle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mucha:Marcin.html">Marcin Mucha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pawlewicz:Jakub.html">Jakub Pawlewicz</a>, Karol Węgrzycki <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02424">PDF</a><br /><b>Abstract: </b>In the Equal-Subset-Sum problem, we are given a set $S$ of $n$ integers and
the problem is to decide if there exist two disjoint nonempty subsets $A,B
\subseteq S$, whose elements sum up to the same value. The problem is
NP-complete. The state-of-the-art algorithm runs in $O^{*}(3^{n/2}) \le
O^{*}(1.7321^n)$ time and is based on the meet-in-the-middle technique. In this
paper, we improve upon this algorithm and give $O^{*}(1.7088^n)$ worst case
Monte Carlo algorithm. This answers the open problem from Woeginger's
inspirational survey.
</p>
<p>Additionally, we analyse the polynomial space algorithm for Equal-Subset-Sum.
A naive polynomial space algorithm for Equal-Subset-Sum runs in $O^{*}(3^n)$
time. With read-only access to the exponentially many random bits, we show a
randomized algorithm running in $O^{*}(2.6817^n)$ time and polynomial space.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02424"><span class="datestr">at May 08, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02383">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02383">Gaussian Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Jinshuo.html">Jinshuo Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Aaron.html">Aaron Roth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Su:Weijie_J=.html">Weijie J. Su</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02383">PDF</a><br /><b>Abstract: </b>Differential privacy has seen remarkable success as a rigorous and practical
formalization of data privacy in the past decade. But it also has some well
known weaknesses: notably, it does not tightly handle composition. This
weakness has inspired several recent relaxations of differential privacy based
on Renyi divergences. We propose an alternative relaxation of differential
privacy, which we term "$f$-differential privacy", which has a number of
appealing properties and avoids some of the difficulties associated with
divergence based relaxations. First, it preserves the hypothesis testing
interpretation of differential privacy, which makes its guarantees easily
interpretable. It allows for lossless reasoning about composition and
post-processing, and notably, a direct way to import existing tools from
differential privacy, including privacy amplification by subsampling. We define
a canonical single parameter family of definitions within our class which we
call "Gaussian Differential Privacy", defined based on the hypothesis testing
of two shifted Gaussian distributions. We show that this family is focal by
proving a central limit theorem, which shows that the privacy guarantees of
\emph{any} hypothesis-testing based definition of privacy (including
differential privacy) converges to Gaussian differential privacy in the limit
under composition. We also prove a finite (Berry-Esseen style) version of the
central limit theorem, which gives a useful tool for tractably analyzing the
exact composition of potentially complicated expressions. We demonstrate the
use of the tools we develop by giving an improved analysis of the privacy
guarantees of noisy stochastic gradient descent.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02383"><span class="datestr">at May 08, 2019 01:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02367">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02367">Adversarially Robust Submodular Maximization under Knapsack Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avdiukhin:Dmitrii.html">Dmitrii Avdiukhin</a>, Slobodan Mitrović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yaroslavtsev:Grigory.html">Grigory Yaroslavtsev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02367">PDF</a><br /><b>Abstract: </b>We propose the first adversarially robust algorithm for monotone submodular
maximization under single and multiple knapsack constraints with scalable
implementations in distributed and streaming settings. For a single knapsack
constraint, our algorithm outputs a robust summary of almost optimal (up to
polylogarithmic factors) size, from which a constant-factor approximation to
the optimal solution can be constructed. For multiple knapsack constraints, our
approximation is within a constant-factor of the best known non-robust
solution.
</p>
<p>We evaluate the performance of our algorithms by comparison to natural
robustifications of existing non-robust algorithms under two objectives: 1)
dominating set for large social network graphs from Facebook and Twitter
collected by the Stanford Network Analysis Project (SNAP), 2) movie
recommendations on a dataset from MovieLens. Experimental results show that our
algorithms give the best objective for a majority of the inputs and show strong
performance even compared to offline algorithms that are given the set of
removals in advance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02367"><span class="datestr">at May 08, 2019 01:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02358">Exponential Separations Between Turnstile Streaming and Linear Sketching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kallaugher:John.html">John Kallaugher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02358">PDF</a><br /><b>Abstract: </b>Almost every known turnstile streaming algorithm is implementable as a linear
sketch. Is this necessarily true, or can there exist turnstile streaming
algorithms that use much less space than any linear sketch?
</p>
<p>It was shown in~\cite{LNW14} that, if a turnstile algorithm works for
arbitrarily long streams with arbitrarily large coordinates at intermediate
stages of the stream, then the turnstile algorithm can be implemented as a
linear sketch. Our results have the opposite form: if either the stream length
or the maximum value of the stream are substantially restricted, there exist
problems where linear sketching is exponentially harder than turnstile
streaming.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02358"><span class="datestr">at May 08, 2019 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02354">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02354">PRSim: Sublinear Time SimRank Computation on Large Power-Law Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhewei.html">Zhewei Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Xiaodong.html">Xiaodong He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Xiaokui.html">Xiaokui Xiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Sibo.html">Sibo Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yu.html">Yu Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Xiaoyong.html">Xiaoyong Du</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wen:Ji=Rong.html">Ji-Rong Wen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02354">PDF</a><br /><b>Abstract: </b>{\it SimRank} is a classic measure of the similarities of nodes in a graph.
Given a node $u$ in graph $G =(V, E)$, a {\em single-source SimRank query}
returns the SimRank similarities $s(u, v)$ between node $u$ and each node $v
\in V$. This type of queries has numerous applications in web search and social
networks analysis, such as link prediction, web mining, and spam detection.
Existing methods for single-source SimRank queries, however, incur query cost
at least linear to the number of nodes $n$, which renders them inapplicable for
real-time and interactive analysis.
</p>
<p>{ This paper proposes \prsim, an algorithm that exploits the structure of
graphs to efficiently answer single-source SimRank queries. \prsim uses an
index of size $O(m)$, where $m$ is the number of edges in the graph, and
guarantees a query time that depends on the {\em reverse PageRank} distribution
of the input graph. In particular, we prove that \prsim runs in sub-linear time
if the degree distribution of the input graph follows the power-law
distribution, a property possessed by many real-world graphs. Based on the
theoretical analysis, we show that the empirical query time of all existing
SimRank algorithms also depends on the reverse PageRank distribution of the
graph.} Finally, we present the first experimental study that evaluates the
absolute errors of various SimRank algorithms on large graphs, and we show that
\prsim outperforms the state of the art in terms of query time, accuracy, index
size, and scalability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02354"><span class="datestr">at May 08, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02322">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02322">Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smid:Michiel.html">Michiel Smid</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02322">PDF</a><br /><b>Abstract: </b>In this paper we study two geometric data structure problems in the special
case when input objects or queries are fat rectangles. We show that in this
case a significant improvement compared to the general case can be achieved.
</p>
<p>We describe data structures that answer two- and three-dimensional orthogonal
range reporting queries in the case when the query range is a \emph{fat}
rectangle. Our two-dimensional data structure uses $O(n)$ words and supports
queries in $O(\log\log U +k)$ time, where $n$ is the number of points in the
data structure, $U$ is the size of the universe and $k$ is the number of points
in the query range. Our three-dimensional data structure needs
$O(n\log^{\varepsilon}U)$ words of space and answers queries in $O(\log \log U
+ k)$ time. We also consider the rectangle stabbing problem on a set of
three-dimensional fat rectangles. Our data structure uses $O(n)$ space and
answers stabbing queries in $O(\log U\log\log U +k)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02322"><span class="datestr">at May 08, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02316">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02316">Parallel Cut Pursuit For Minimization of the Graph Total Variation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raguet:Hugo.html">Hugo Raguet</a>, Loic Landrieu <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02316">PDF</a><br /><b>Abstract: </b>We present a parallel version of the cut-pursuit algorithm for minimizing
functionals involving the graph total variation. We show that the decomposition
of the iterate into constant connected components, which is at the center of
this method, allows for the seamless parallelization of the otherwise costly
graph-cut based refinement stage. We demonstrate experimentally the efficiency
of our method in a wide variety of settings, from simple denoising on huge
graphs to more complex inverse problems with nondifferentiable penalties. We
argue that our approach combines the efficiency of graph-cuts based optimizers
with the versatility and ease of parallelization of traditional proximal
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02316"><span class="datestr">at May 08, 2019 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02313">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02313">Optimal Convergence Rate of Hamiltonian Monte Carlo for Strongly Logconcave Distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zongchen.html">Zongchen Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh_S=.html">Santosh S. Vempala</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02313">PDF</a><br /><b>Abstract: </b>We study Hamiltonian Monte Carlo (HMC) for sampling from a strongly
logconcave density proportional to $e^{-f}$ where $f:\mathbb{R}^d \to
\mathbb{R}$ is $\mu$-strongly convex and $L$-smooth (the condition number is
$\kappa = L/\mu$). We show that the relaxation time (inverse of the spectral
gap) of ideal HMC is $O(\kappa)$, improving on the previous best bound of
$O(\kappa^{1.5})$; we complement this with an example where the relaxation time
is $\Omega(\kappa)$. When implemented using a nearly optimal ODE solver, HMC
returns an $\varepsilon$-approximate point in $2$-Wasserstein distance using
$\widetilde{O}((\kappa d)^{0.5} \varepsilon^{-1})$ gradient evaluations per
step and $\widetilde{O}((\kappa d)^{1.5}\varepsilon^{-1})$ total time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02313"><span class="datestr">at May 08, 2019 01:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02303">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02303">Design Space Exploration as Quantified Satisfaction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Alexander.html">Alexander Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleer:Johan_de.html">Johan de Kleer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matei:Ion.html">Ion Matei</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02303">PDF</a><br /><b>Abstract: </b>We propose novel algorithms for design and design space exploration. The
designs computed by these algorithms are compositions of function types
specified in component libraries. Our algorithms reduce the design problem to
quantified satisfiability and use advanced solvers to find solutions that
represent useful systems. The algorithms we present in this paper are sound and
complete and are guaranteed to discover correct designs of optimal size, if
they exist. We apply our method to the design of Boolean systems and discover
new and more optimal classical and quantum circuits for common arithmetic
functions such as addition and multiplication. The performance of our
algorithms is evaluated through extensive experimentation. We have first
created a benchmark consisting of specifications of scalable synthetic digital
circuits and real-world mirochips. We have then generated multiple circuits
functionally equivalent to the ones in the benchmark. The quantified
satisfiability method shows more than four orders of magnitude speed-up,
compared to a generate and test method that enumerates all non-isomorphic
circuit topologies. Our approach generalizes circuit optimization. It uses
arbitrary component libraries and has applications to areas such as digital
circuit design, diagnostics, abductive reasoning, test vector generation, and
combinatorial optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02303"><span class="datestr">at May 08, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02298">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02298">Even Faster Elastic-Degenerate String Matching via Fast Matrix Multiplication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardini:Giulia.html">Giulia Bernardini</a>, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pisanti:Nadia.html">Nadia Pisanti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pissis:Solon_P=.html">Solon P. Pissis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosone:Giovanna.html">Giovanna Rosone</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02298">PDF</a><br /><b>Abstract: </b>An elastic-degenerate (ED) string is a sequence of $n$ sets of strings of
total length $N$, which was recently proposed to model a set of similar
sequences. The ED string matching (EDSM) problem is to find all occurrences of
a pattern of length $m$ in an ED text. The EDSM problem has recently received
some attention in the combinatorial pattern matching community, and an
$\mathcal{O}(nm^{1.5}\sqrt{\log m} + N)$-time algorithm is known [Aoyama et
al., CPM 2018]. The standard assumption in the prior work on this question is
that $N$ is substantially larger than both $n$ and $m$, and thus we would like
to have a linear dependency on the former. Under this assumption, the natural
open problem is whether we can decrease the 1.5 exponent in the time
complexity, similarly as in the related (but, to the best of our knowledge, not
equivalent) word break problem [Backurs and Indyk, FOCS 2016].
</p>
<p>Our starting point is a conditional lower bound for the EDSM problem. We use
the popular combinatorial Boolean matrix multiplication (BMM) conjecture
stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud
and Williams, FOCS 2014]. By designing an appropriate reduction we show that a
combinatorial algorithm solving the EDSM problem in
$\mathcal{O}(nm^{1.5-\epsilon} + N)$ time, for any $\epsilon&gt;0$, refutes this
conjecture. Of course, the notion of combinatorial algorithms is not clearly
defined, so our reduction should be understood as an indication that decreasing
the exponent requires fast matrix multiplication.
</p>
<p>Two standard tools used in algorithms on strings are string periodicity and
fast Fourier transform. Our main technical contribution is that we successfully
combine these tools with fast matrix multiplication to design a
non-combinatorial $\mathcal{O}(nm^{1.381} + N)$-time algorithm for EDSM. To the
best of our knowledge, we are the first to do so.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02298"><span class="datestr">at May 08, 2019 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02270">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02270">Lifted Multiplicity Codes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Ray.html">Ray Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wootters:Mary.html">Mary Wootters</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02270">PDF</a><br /><b>Abstract: </b>Lifted Reed Solomon Codes (Guo, Kopparty, Sudan 2013) were introduced in the
context of locally correctable and testable codes. They are multivariate
polynomials whose restriction to any line is a codeword of a Reed-Solomon code.
We generalize their construction by introducing lifted multiplicity codes,
multivariate polynomial codes whose restriction to any line is a codeword of a
multiplicity code (Kopparty, Saraf, Yekhanin 2014). We show that lifted
multiplicity codes have a better trade-off between redundancy and a notion of
locality called the $t$-disjoint-repair-group property than previously known
constructions. More precisely, we show that lifted multiplicity codes with
length $N$ and redundancy $O(t^{0.585} \sqrt{N})$ have the property that any
symbol of a codeword can be reconstructed in $t$ different ways, each using a
disjoint subset of the other coordinates. This gives the best known trade-off
for this problem for any super-constant $t &lt; \sqrt{N}$. We also give an
alternative analysis of lifted Reed Solomon codes using dual codes, which may
be of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02270"><span class="datestr">at May 08, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/067">TR19-067 |  Sign rank vs Discrepancy | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Sign-rank and discrepancy are two central notions in communication complexity. The seminal work of  Babai, Frankl, and Simon from 1986  initiated an active line of research that investigates  the gap between these two notions.
In this article, we establish the strongest possible separation  by constructing a Boolean matrix whose sign-rank is only $3$, and yet its discrepancy is  $2^{-\Omega(n)}$. We note that every matrix of sign-rank $2$ has discrepancy $n^{-O(1)}$.
Our result in particular implies that there are Boolean functions with $O(1)$ unbounded error randomized communication complexity while having $\Omega(n)$ weakly unbounded error randomized communication complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/067"><span class="datestr">at May 07, 2019 09:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15840">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/">The Network Coding Conjecture Is Powerful</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>More hard Boolean functions</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg"><img width="197" alt="" src="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg?w=197&amp;h=162" class="alignright wp-image-15841" height="162" /></a></p>
<p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent <a href="https://arxiv.org/abs/1902.10935">paper</a> which we just <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">discussed</a>. </p>
<p>
Today Ken and I will update our discussion. </p>
<p>
Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL is:</p>
<blockquote><p><b>Theorem 1</b> <em> If the NCC is true, then every Boolean circuit that computes the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function has size <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" />. </em>
</p></blockquote>
<p></p><p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function is: Given an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit number <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and a number <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \le k \le n}" class="latex" title="{1 \le k \le n}" />, compute the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k}}" class="latex" title="{2^{k}}" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Ctimes+2%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x \times 2^{k}. " class="latex" title="\displaystyle  x \times 2^{k}. " /></p>
<p>This is a special case of the integer multiplication problem. In symbols it maps <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B0%5Ek+x+0%5E%7Bn-k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^k x 0^{n-k}}" class="latex" title="{0^k x 0^{n-k}}" />, as in our photo above. </p>
<p>
Our point, however, is not about integer multiplication. Nor even about NCC—no knowledge of it will be needed today, so read on even if you are not aware of NCC. No. Our point is that a whole lot of other Boolean functions would inherit the same <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> circuit lower bound as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. And several aspects of that seem troubling.</p>
<p>
</p><p></p><h2> Some Worry </h2><p></p>
<p></p><p>
We are impressed by the AFKL paper but also worried. Proving a super-linear lower bound in the unrestricted Boolean complexity model has long been considered a difficult problem. Maybe a hopeless problem. Yes they are proving it not for a single-output function; they are proving it for a multiple-output function. Still I thought that it seems too good to be correct. Even worse, assuming NCC they also resolve other open problems in complexity theory. I am worried.</p>
<p>
What we suggest is to catalog and study the consequences of their results. If we find that their results lead to a contradiction, then there was something to be worried about. Or perhaps it would mean that NCC is false. If we find no contradiction, then everything we discover is also a consequence of NCC. Either way we learn more.</p>
<p>
</p><p></p><h2> AFKL Functions </h2><p></p>
<p></p><p>
Let’s call a Boolean function an <i>AFKL function</i> provided it has Boolean circuit complexity <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> if the NCC is true. Thanks to AFKL, we now know that integer multiplication is an AFKL function. I started to think about: What functions are in this class? Here are some examples: </p>
<ul>
<li>
Integer multiplication <p></p>
</li><li>
Integer multiplication by a power of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> <p></p>
</li><li>
Discrete convolution <p></p>
</li><li>
Sparse convolution
</li></ul>
<p>
We describe the last three next. We show they have linear size-preserving reductions from the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function.</p>
<p>
</p><p></p><h2> The Flip Function </h2><p></p>
<p></p><p>
Define <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%3A+%5C%7B0%2C1%5C%7D%5E%7Bn%7D+%5Crightarrow+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" class="latex" title="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BFLIP%7D_%7Bn%7D%280%5E%7Bk%7D1%5Calpha%29%3D+1%5Calpha+0%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " class="latex" title="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \ge 1}" class="latex" title="{k \ge 1}" />. For any input not of this form, let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> be <img src="https://s0.wp.com/latex.php?latex=%7B0%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^{n}}" class="latex" title="{0^{n}}" />.</p>
<blockquote><p><b>Theorem 2</b> <em> The Boolean function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is an AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,k}" class="latex" title="{x,k}" /> be the input to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x| = n}" class="latex" title="{|x| = n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \leq n}" class="latex" title="{k \leq n}" /> in binary. In linear size we can test <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 0}" class="latex" title="{k = 0}" />, when there is nothing to do, so presume <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \geq 1}" class="latex" title="{k \geq 1}" />. The first step is to create </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1}. " class="latex" title="\displaystyle  0^{n-k}10^{k-1}. " /></p>
<p>This is just binary-to-unary conversion and has linear-size circuits—as in multiplex decoding and as remarked by AFKL. This becomes the first <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits of an application of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> to the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit string </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D+x.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1} x. " class="latex" title="\displaystyle  0^{n-k}10^{k-1} x. " /></p>
<p>It yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++10%5E%7Bk-1%7D+x+0%5E%7Bn-k%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  10^{k-1} x 0^{n-k}. " class="latex" title="\displaystyle  10^{k-1} x 0^{n-k}. " /></p>
<p>Changing the first bit to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> then leaves the desired output of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The point is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is a super-simple function. It just moves the initial block of <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />‘s in a string to the end. It is amazing that this function should have only non-linear, indeed <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />-sized, circuits. </p>
<p>
This also means that Ken’s <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">function</a>, which takes <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%2C2%5C%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in \{0,1,2\}^*}" class="latex" title="{x \in \{0,1,2\}^*}" /> and moves all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />s to the end of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, is hard even in the special cases where all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />‘s are at the front. What’s strange is that Ken proves his function equivalent to another special case where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x|}" class="latex" title="{|x|}" /> is even and exactly half the characters are <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. This latter case is one in which <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> is easy, but the two cases are separate. All this is touch-and-go enough to compound our “worry.”</p>
<p>
</p><p></p><h2> The Sparse Convolution Function </h2><p></p>
<p></p><p>
The following is also an AFKL function. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi%3D1%7D%5E%7Bn-l%7D+w_%7Bi%7D+%5Cwedge+x_%7Bi%2Bl%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " class="latex" title="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bl%3D1%2C%5Cdots%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{l=1,\dots,n}" class="latex" title="{l=1,\dots,n}" /> where an empty OR is defined to be <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. This can even further be restricted to the case where exactly one of the <img src="https://s0.wp.com/latex.php?latex=%7Bw_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_{i}}" class="latex" title="{w_{i}}" /> are <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and the rest are <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. Call this the <i>sparse convolution function</i>.</p>
<blockquote><p><b>Theorem 3</b> <em> The sparse convolution is a monotone AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  We will give a sketch of why this is true. Define 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi+%3D+1%7D%5E%7Bn-l%7D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D+%5Cwedge+x_%7Bi%2Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " class="latex" title="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " /></p>
<p>It is not hard to show that this yields the FLIP function. We can reduce computing it to a convolution of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{i}}" class="latex" title="{x_{i}}" />‘s and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28i%29+%3D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " class="latex" title="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " /></p>
<p>The key is to note that exactly one <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> will be non-zero, and so the convolution is sparse. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The sparse convolution function raises an interesting question: Are the methods for sparse FFT useful here? The lower bound for AFKL functions suggests that they are not applicable. </p>
<p>
</p><p></p><h2> Is the NCC-Boolean Connection New? </h2><p></p>
<p></p><p>
The subtitle of our <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">post</a> marveled that a core-theory advance on circuits for multiplication had come via the practical side of throughput in computer networks. AFKL deserve plaudits for linking two communities. We should mention that one theoretician we both know well, Mark Braverman, with his students Sumegha Garg and Ariel Schvartzman at Princeton, <a href="https://arxiv.org/pdf/1608.06545.pdf">proved</a> a fact about NCC that is relevant to this discussion:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="BGS"></a> Either NCC is false, or bit-operations save a whole <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7B%5COmega%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{(\log n)^{\Omega(1)}}" class="latex" title="{(\log n)^{\Omega(1)}}" /> factor in the network size. </em>
</p></blockquote>
<p></p><p>
Even this paper, however, does not address lower bounds on Boolean circuits. The only prior link between NCC and Boolean complexity is a 2007 <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v14i1r44">paper</a> by Søren Riis, which is cited by AFKL, and has a 2011 <a href="http://emis.impa.br/EMIS/journals/EJC/Volume_18/PDF/v18i1p192.pdf">followup</a> by Demetres Christofides and Klas Markström. The paper by Riis has a new “guessing game” on graphs and a demonstration that a lower-bound conjecture of Leslie Valiant needs to be rescued by dividing by a <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> factor. Theorem <a href="https://rjlipton.wordpress.com/feed/#BGS">4</a>, however, seems to say that no such <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> shading can apply to NCC.</p>
<p>
When we ask Google “network coding conjecture Boolean circuit lower bounds” (without quotes), the first page shows AFKL, our posts, and this 2014 <a href="https://people.csail.mit.edu/rrw/ccc14-survey.pdf">survey</a> by Ryan Williams—which mentions neural networks but not NCC. On the next page of hits we see Riis and the followup paper but nothing else that seems directly relevant. Nor does appending `-multiplication’ help screen out AFKL and our posts.</p>
<p>
There is said to be empirical evidence for NCC. We wonder, however, whether that has reached the intensity of thought about circuit lower bounds. We say this because the implications from NCC make three giant steps:</p>
<ol>
<li>
Not only does it assert a super-linear circuit lower bound (okay, for a function with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> output bits), but… <p></p>
</li><li>
…it asserts <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />… <p></p>
</li><li>
…for functions that are easily in Turing machine linear time.
</li></ol>
<p>
So one side of our worry is whether NCC can actually shed light on so many fundamental issues from complexity theory, more than absorbing light. At the very least, AFKL have re-stimulated interest in all of these issues. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> hard? Is NCC true? What other Boolean functions are AFKL functions? What about other consequences of the NCC to complexity theory?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/"><span class="datestr">at May 07, 2019 03:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kintali.wordpress.com/?p=1235">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kintali.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/">Preventing future college admissions scandals using Blockchain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="graf graf--p graf-after--h3" id="5bf6">The recent <a href="https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener" target="_blank">college admissions scandal</a> resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases like “broken admissions system”, “rich can buy their way into college”, etc etc.</p>
<p class="graf graf--p graf-after--p" id="4b6a">There must be several other cases of fraud that go unnoticed on a daily basis. It’s in human nature to shortcut the rules, collude and cheat to achieve one’s own short-term goals, hoping to get away with one’s fraudulent actions. Let’s discuss how to use Blockchain technology and create a rigorous system to prevent some aspects of such scandals in future.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b66e"><strong class="markup--strong markup--p-strong">Problems</strong>: Fake athletic certificates and phony athletic profile. Taking photos of students on a stationary rowing machine. Photoshopping students’ face on another athlete’s photo.</p>
<p class="graf graf--p graf-after--p" id="ad90"><strong class="markup--strong markup--p-strong">Solution</strong>: A genuine high-school athlete achieves his/her athletic credentials during a four year period. Getting a genuine athletic certificate involves achieving several intermediate goals. For example, if you achieved a black belt in karate in your 11th grade, you must have progressed through beginner level (with white, yellow and orange belts), intermediate level (with green, blue, purple, brown and red belts) and then reached the advanced level (with a black belt). <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">TrueCerts</a> technology allows sports coaching centers to create certificates for each of these intermediate athletic achievements, sign them using their private keys and post only the SHA-256 hash of the certificate on a public blockchain, thus immutably time-stamping each achievement at the specific day/time the athlete achieved it. The athletic photos can be time-stamped similarly.</p>
<p class="graf graf--p graf-after--p" id="2c0c">This process achieves the seemingly impossible combination of <strong class="markup--strong markup--p-strong">security, privacy and transparency </strong>!! Security is achieved by the asymmetric key encryption. Privacy is achieved by the one-way nature of the cryptographic hash function <a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">SHA 256</a>. Transparency is achieved by the fact that anybody with the access to the sports certificate (upon student’s consent) can compute the SHA 256 hash of the certificate and verify that it is stored on a public blockchain validly signed by the private key of the issuer (sports coaching center). This eliminates the fraudulent behavior of creating a bunch of fake credentials and photos, all at once, in a brief period of time. Using a fairly decentralized public blockchain is very important here.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="c253"><strong class="markup--strong markup--p-strong">Problem</strong>: Bribing coaches to accept certain students in their sports teams or issue fake credentials.</p>
<p class="graf graf--p graf-after--p" id="6290"><strong class="markup--strong markup--p-strong">Solution</strong>: This problem can be solved by having several people (perhaps five coaches, some administrative assistants, one principal, one vice-principal, etc) in the organization collectively responsible (using multi-signature wallets and m-of-n signatures) to issue credentials. Each of the involved person is accountable for every issued certificate.</p>
<p class="graf graf--p graf-after--p" id="eb99">Bribing one coach is easy. Bribing ten people is hard. People often hesitate bribing multiple people. Collusion becomes increasingly hard when you increase the size of the group involved. When there are more people involved, there is a higher chance that at least one of them is honest (and brave) to overcome the pressure of the others and blow the whistle.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="0d42"><strong class="markup--strong markup--p-strong">Problem</strong>: Fake college entrance exam (SAT, ACT) test scores</p>
<p class="graf graf--p graf-after--p" id="fe6e"><strong class="markup--strong markup--p-strong">Solution</strong>: Current paper-based test score issuance and verification system is too time-consuming and error-prone. These credentials can be easily faked or tampered with. An ideal solution involves creating a digital certificate, validly signed (or multi-signed) by the issuer and time-stamped with a <a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SHA 256</a> hash on a public blockchain. See <a href="https://medium.com/@truecerts/dr-kintalis-motivation-behind-developing-truecerts-platform-aba93f4d290a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">my previous post</a> about preventing fraud in academic transcripts and making the entire system efficient.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b957"><strong class="markup--strong markup--p-strong">Problem</strong>: Other students taking entrance tests on your behalf.</p>
<p class="graf graf--p graf-after--p" id="0e07"><strong class="markup--strong markup--p-strong">Solution</strong>: This is a problem of verifying the identity of the student taking the test. The current system of using paper-based credentials is broken. It’s easy to create fake driver’s license, passports etc. At <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>, we have created an automated multi-step identity verification that combines your standard KYC identity procedures, utility bills and more importantly biometrics. We define identity as a several data points achieved over a period of time, not just one piece of paper. It’s very hard to cheat all of these steps. If you have a look-alike twin then consider yourself lucky <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<p> </p>
<p class="graf graf--p graf-after--p" id="a16a"><strong class="markup--strong markup--p-strong">Partially solvable problems</strong>: Bribing officials to change student’s answers in paper-based exam can be solved to certain extent by using a completely computer-based exam. Bribing proctors to tell the answers to a student (during the test) can be solved with computer-vision-based cheating detection software. Some photoshopped images can be detected using image analysis techniques.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="4d0f"><strong class="markup--strong markup--p-strong">Hard to solve problems</strong>: (1) Getting a fake medical certificate that your kid requires an isolated room to take the test and then bribing the proctor to tell him/her all the answers during the test. (2) Laundering bribes using a non-profit entity. Phew…. These things actually happened during this college admissions scandal. As the famous saying goes “a person is capable of as much atrocity as he/she has imagination”.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="5ad0">In summary, combining the existing technologies we can solve several of the above mentioned problems and simultaneously achieve <strong class="markup--strong markup--p-strong">security, privacy and transparency</strong>. The main goal here is to make the bad people’s job as difficult as possible and simultaneously making the good people’s job very efficient.</p>
<p class="graf graf--p graf-after--p" id="a4f5">If you want to know more about how we (at <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>) are using Blockchain and other technologies to prevent fraud and corruption in broad range of areas, <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">contact us</a>.</p>
<p class="graf graf--p graf-after--p" id="e889">Stay tuned for my next post about a huge list of real-world fraud and corruption stories that can be prevented rigorously by using cutting-edge technologies.</p>
<p><img src="https://kintali.files.wordpress.com/2019/05/truecertstrustsimplified.png?w=660" alt="TrueCertsTrustSimplified" class="alignnone size-full wp-image-1236" /></p></div>







<p class="date">
by kintali <a href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/"><span class="datestr">at May 07, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2431469456247088523">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html">Thoughts on the recent Jeopardy streak (SPOILERS)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
James Holzhauer  has won 22 consecutive games of Jeopardy and has made around 1.6 million dollars. Nice work if you can get it. Here are some thoughts no this<br />
<br />
1) Before James H the records for number of consecutive games was, and still is, Ken Jennings winning 74 in a row, and second place was 20. I was surprised that Ken was that much better than the competition.<br />
<br />
2) Before James H the record for amount of money in normal play (not extra from, say, tournament of champions or losing to a computer) was around 400,000. I was surprised that Ken was that much better than the competition.<br />
<br />
3) James is obliterating the records for most wins in a single game. He holds the top 12 records for this.  This is due to his betting A LOT on the daily doubles and the final jeop, as well as of course answering so many questions right.<br />
<br />
4) One reason players in Jeopardy don't have long streaks is fatigue. The actually play<br />
5 games a day, two days of the week.  James H is getting a break since he has two weeks off now since they will soon have the Teachers Tournament. This could work either way--- he gets a break or he loses being in-the-zone.<br />
<br />
5) James strategy is:<br />
<br />
a) Begin with the harder (and more lucrative) questions.<br />
<br />
b) Bet A LOT on the daily doubles (which are more likely to be in the more lucrative questions) and almost always go into final jeop with more than twice your opponent (He failed to do this only once.)<br />
<br />
c) Bet A LOT on Final Jeop- though not enough so that if you lose you lose the game. I think he's gotten every Final Jeop question right.<br />
<br />
For more on his strategy see this article by Oliver Roeder in Nate Silvers Blog: <a href="https://fivethirtyeight.com/features/the-man-who-solved-jeopardy/">here</a><br />
<br />
6) I tend to think of this as being a high-risk, high-reward strategy and thus it is unlikely he will beat Ken Jennings, but every time he wins that thought seems sillier and sillier. While we are here, how likely is it that someone will beat Ken Jennings? In an article before all of this Ben Morrison in Nate Silvers Blog wrote that it was quite likely SOMEONE would break Ken J's record,  see <a href="https://fivethirtyeight.com/features/ken-jennings-has-nothing-on-joe-dimaggio/">here</a>.<br />
<br />
7) OKAY, how does James H compare to Ken J? According to Oliver Roeder in Nate Silvers Blog,<br />
<a href="https://fivethirtyeight.com/features/the-battle-for-jeopardy-supremacy/">here</a>, they are similar in terms of percent of questions answered right, but James H bets so much more (bets better?) which is why he is getting so much money. I'll be curious to see a head-to-head contest at some point. But to the issue at hand, they don't give James H that good a chance to break Ken J's record.<br />
<br />
8) Jeop used to have a  5-game limit. Maybe that was a good idea- its not that interesting seeing the same person with the same strategy win 22 in a row. Also, the short-talk-with-Alex T-- James is running out of interesting things to say. I wonder what Alex did with Ken J after 50 games.<br />
``So Ken, I hear you're good at Jeopardy''<br />
<br />
9) Misc: Ken J was the inspiration for IBM to do Watson.<br />
<br />
10) Will future players use James Strategy? Note that you have to be REALLY GOOD in the first place for it to help you. Maybe a modified version where you go for the lucrative questions and bet a lot on Daily Doubles (more than people have done in the past) when its an area you know really well (I'll take Ramsey Theory for $2000.)<br />
<br />
11) I used to DVR and watch Jeop but didn't mind if I was a few behind. Now I have to stay on top of it so articles like those pointed to above don't give me a spoiler.<br />
<br />
12) My prediction: He will beat Ken Jenning for money but not for number-of-games. I have no real confidence in these predictions.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html"><span class="datestr">at May 07, 2019 12:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02176">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02176">Computation of Circular Area and Spherical Volume Invariants via Boundary Integrals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Riley O'Neill, Pedro Angulo-Umana, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Calder:Jeff.html">Jeff Calder</a>, Bo Hessburg, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olver:Peter_J=.html">Peter J. Olver</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shakiban:Chehrzad.html">Chehrzad Shakiban</a>, Katrina Yezzi-Woodley <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02176">PDF</a><br /><b>Abstract: </b>We show how to compute the circular area invariant of planar curves, and the
spherical volume invariant of surfaces, in terms of line and surface integrals,
respectively. We use the Divergence Theorem to express the area and volume
integrals as line and surface integrals, respectively, against particular
kernels; our results also extend to higher dimensional hypersurfaces. The
resulting surface integrals are computable analytically on a triangulated mesh.
This gives a simple computational algorithm for computing the spherical volume
invariant for triangulated surfaces that does not involve discretizing the
ambient space. We discuss potential applications to feature detection on broken
bone fragments of interest in anthropology.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02176"><span class="datestr">at May 07, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02149">Efficient Second-Order Shape-Constrained Function Fitting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Durfee:David.html">David Durfee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Yu.html">Yu Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Anup_B=.html">Anup B. Rao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wild:Sebastian.html">Sebastian Wild</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02149">PDF</a><br /><b>Abstract: </b>We give an algorithm to compute a one-dimensional shape-constrained function
that best fits given data in weighted-$L_{\infty}$ norm. We give a single
algorithm that works for a variety of commonly studied shape constraints
including monotonicity, Lipschitz-continuity and convexity, and more generally,
any shape constraint expressible by bounds on first- and/or second-order
differences. Our algorithm computes an approximation with additive error
$\varepsilon$ in $O\left(n \log \frac{U}{\varepsilon} \right)$ time, where $U$
captures the range of input values. We also give a simple greedy algorithm that
runs in $O(n)$ time for the special case of unweighted $L_{\infty}$ convex
regression. These are the first (near-)linear-time algorithms for
second-order-constrained function fitting. To achieve these results, we use a
novel geometric interpretation of the underlying dynamic programming problem.
We further show that a generalization of the corresponding problems to directed
acyclic graphs (DAGs) is as difficult as linear programming.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02149"><span class="datestr">at May 07, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02133">Non-clairvoyant Precedence Constrained Scheduling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Naveen.html">Naveen Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02133">PDF</a><br /><b>Abstract: </b>We consider the online problem of scheduling jobs on identical machines,
where jobs have precedence constraints. We are interested in the demanding
setting where the jobs sizes are not known up-front, but are revealed only upon
completion (the non-clairvoyant setting). Such precedence-constrained
scheduling problems routinely arise in map-reduce and large-scale optimization.
In this paper, we make progress on this problem. For the objective of total
weighted completion time, we give a constant-competitive algorithm. And for
total weighted flow-time, we give an $O(1/\epsilon^2)$-competitive algorithm
under $(1+\epsilon)$-speed augmentation and a natural ``no-surprises''
assumption on release dates of jobs (which we show is necessary in this
context).
</p>
<p>Our algorithm proceeds by assigning {\em virtual rates} to all the waiting
jobs, including the ones which are dependent on other uncompleted jobs, and
then use these virtual rates to decide on the actual rates of minimal jobs
(i.e., jobs which do not have dependencies and hence are eligible to run).
Interestingly, the virtual rates are obtained by allocating time in a fair
manner, using a Eisenberg-Gale-type convex program (which we can also solve
optimally using a primal-dual scheme). The optimality condition of this convex
program allows us to show dual-fitting proofs more easily, without having to
guess and hand-craft the duals. We feel that this idea of using fair virtual
rates should have broader applicability in scheduling problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02133"><span class="datestr">at May 07, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02067">Geometric Firefighting in the Half-plane</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Sang=Sub.html">Sang-Sub Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klein:Rolf.html">Rolf Klein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=bel:David.html">David Kübel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langetepe:Elmar.html">Elmar Langetepe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schwarzwald:Barbara.html">Barbara Schwarzwald</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02067">PDF</a><br /><b>Abstract: </b>In 2006, Alberto Bressan suggested the following problem. Suppose a circular
fire spreads in the Euclidean plane at unit speed. The task is to build, in
real time, barrier curves to contain the fire. At each time $t$ the total
length of all barriers built so far must not exceed $t \cdot v$, where $v$ is a
speed constant. How large a speed $v$ is needed? He proved that speed $v&gt;2$ is
sufficient, and that $v&gt;1$ is necessary. This gap of $(1,2]$ is still open. The
crucial question seems to be the following. {\em When trying to contain a fire,
should one build, at maximum speed, the enclosing barrier, or does it make
sense to spend some time on placing extra delaying barriers in the fire's way?}
We study the situation where the fire must be contained in the upper $L_1$
half-plane by an infinite horizontal barrier to which vertical line segments
may be attached as delaying barriers. Surprisingly, such delaying barriers are
helpful when properly placed. We prove that speed $v=1.8772$ is sufficient,
while $v &gt;1.66$ is necessary.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02067"><span class="datestr">at May 07, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01822">FPT Algorithms for Conflict-free Coloring of Graphs and Chromatic Terrain Guarding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agrawal:Akanksha.html">Akanksha Agrawal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashok:Pradeesha.html">Pradeesha Ashok</a>, Meghana M Reddy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Saket.html">Saket Saurabh</a>, Dolly Yadav <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01822">PDF</a><br /><b>Abstract: </b>We present fixed parameter tractable algorithms for the conflict-free
coloring problem on graphs. Given a graph $G=(V,E)$, \emph{conflict-free
coloring} of $G$ refers to coloring a subset of $V$ such that for every vertex
$v$, there is a color that is assigned to exactly one vertex in the closed
neighborhood of $v$. The \emph{k-Conflict-free Coloring} problem is to decide
whether $G$ can be conflict-free colored using at most $k$ colors. This problem
is NP-hard even for $k=1$ and therefore under standard complexity theoretic
assumptions, FPT algorithms do not exist when parameterised by the solution
size. We consider the \emph{k-Conflict-free Coloring} problem parameterised by
the treewidth of the graph and show that this problem is fixed parameter
tractable. We also initiate the study of \emph{Strong Conflict-free Coloring}
of graphs. Given a graph $G=(V,E)$, \emph{strong conflict-free coloring} of $G$
refers to coloring a subset of $V$ such that every vertex $v$ has at least one
colored vertex in its closed neighborhood and moreover all the colored vertices
in $v$'s neighborhood have distinct colors. We show that this problem is in FPT
when parameterised by both the treewidth and the solution size. We further
apply these algorithms to get efficient algorithms for a geometric problem
namely the Terrain Guarding problem, when parameterised by a structural
parameter.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01822"><span class="datestr">at May 07, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01772">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01772">Nostalgin: Extracting 3D City Models from Historical Image Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapoor:Amol.html">Amol Kapoor</a>, Hunter Larco, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kiveris:Raimondas.html">Raimondas Kiveris</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01772">PDF</a><br /><b>Abstract: </b>What did it feel like to walk through a city from the past? In this work, we
describe Nostalgin (Nostalgia Engine), a method that can faithfully reconstruct
cities from historical images. Unlike existing work in city reconstruction, we
focus on the task of reconstructing 3D cities from historical images. Working
with historical image data is substantially more difficult, as there are
significantly fewer buildings available and the details of the camera
parameters which captured the images are unknown. Nostalgin can generate a city
model even if there is only a single image per facade, regardless of viewpoint
or occlusions. To achieve this, our novel architecture combines image
segmentation, rectification, and inpainting. We motivate our design decisions
with experimental analysis of individual components of our pipeline, and show
that we can improve on baselines in both speed and visual realism. We
demonstrate the efficacy of our pipeline by recreating two 1940s Manhattan city
blocks. We aim to deploy Nostalgin as an open source platform where users can
generate immersive historical experiences from their own photos.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01772"><span class="datestr">at May 07, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01748">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01748">MapReduce Meets Fine-Grained Complexity: MapReduce Algorithms for APSP, Matrix Multiplication, 3-SUM, and Beyond</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lattanzi:Silvio.html">Silvio Lattanzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seddighin:Saeed.html">Saeed Seddighin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stein:Cliff.html">Cliff Stein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01748">PDF</a><br /><b>Abstract: </b>Distributed processing frameworks, such as MapReduce, Hadoop, and Spark are
popular systems for processing large amounts of data. The design of efficient
algorithms in these frameworks is a challenging problem, as the systems both
require parallelism---since datasets are so large that multiple machines are
necessary---and limit the degree of parallelism---since the number of machines
grows sublinearly in the size of the data. Although MapReduce is over a dozen
years old~\cite{dean2008mapreduce}, many fundamental problems, such as Matrix
Multiplication, 3-SUM, and All Pairs Shortest Paths,
</p>
<p>lack efficient MapReduce algorithms. We study these problems in the MapReduce
setting. Our main contribution is to exhibit smooth trade-offs between the
memory available on each machine, and the total number of machines necessary
for each problem. Overall, we take the memory available to each machine as a
parameter, and aim to minimize the number of rounds and number of machines.
</p>
<p>In this paper, we build on the well-known MapReduce theoretical framework
initiated by Karloff, Suri, and Vassilvitskii ~\cite{karloff2010model} and give
algorithms for many of these problems. The key to efficient algorithms in this
setting lies in defining a sublinear number of large (polynomially sized)
subproblems, that can then be solved in parallel. We give strategies for
MapReduce-friendly partitioning, that result in new algorithms for all of the
above problems. Specifically, we give constant round algorithms for the
Orthogonal Vector (OV) and 3-SUM problems, and $O(\log n)$-round algorithms for
Matrix Multiplication, All Pairs Shortest Paths (APSP), and Fast Fourier
Transform (FFT), among others. In all of these we exhibit trade-offs between
the number of machines and memory per machine.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01748"><span class="datestr">at May 07, 2019 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01745">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01745">Faster algorithms for polytope rounding, sampling, and volume computation via a sublinear "Ball Walk''</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mangoubi:Oren.html">Oren Mangoubi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vishnoi:Nisheeth_K=.html">Nisheeth K. Vishnoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01745">PDF</a><br /><b>Abstract: </b>We study the problem of "isotropically rounding" a polytope
$K\subseteq\mathbb{R}^n$, that is, computing a linear transformation which
makes the uniform distribution on the polytope have roughly identity covariance
matrix. We assume that $K$ is defined by $m$ linear inequalities, with
guarantee that $rB\subseteq K\subseteq RB$, where $B$ is the unit ball. We
introduce a new variant of the ball walk Markov chain and show that, roughly,
the expected number of arithmetic operations per-step of this Markov chain is
$O(m)$ that is sublinear in the input size $mn$--the per-step time of all prior
Markov chains. Subsequently, we give a rounding algorithm that succeeds with
probability $1-\varepsilon$ in
$\tilde{O}(mn^{4.5}\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r}))$
arithmetic operations. This gives a factor of $\sqrt{n}$ improvement on the
previous bound of $\tilde{O}(mn^{5}
\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r}))$ for rounding, which uses
the hit-and-run algorithm. Since the cost of the rounding preprocessing step is
in many cases the bottleneck in improving sampling or volume computation, our
results imply these tasks can also be achieved in roughly
$\tilde{O}(mn^{4.5}\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r})+mn^4\delta^{-2})$
operations for computing the volume of $K$ up to a factor $1+\delta$ and
$\tilde{O}(m n^{4.5}\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r})))$ for
uniformly sampling on $K$ with TV error $\varepsilon$. This improves on the
previous bounds of
$\tilde{O}(mn^{5}\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r})+mn^4\delta^{-2})$
for volume computation and
$\tilde{O}(mn^{5}\mathrm{polylog}(\frac{1}{\varepsilon},\frac{R}{r}))$ for
sampling. We achieve this improvement by a novel method of computing polytope
membership, where one avoids checking inequalities which are estimated to have
a very low probability of being violated.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01745"><span class="datestr">at May 07, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01644">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01644">Testable Properties in General Graphs and Random Order Streaming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Czumaj:Artur.html">Artur Czumaj</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fichtenberger:Hendrik.html">Hendrik Fichtenberger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Pan.html">Pan Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sohler:Christian.html">Christian Sohler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01644">PDF</a><br /><b>Abstract: </b>We present a novel framework closely linking the areas of property testing
and data streaming algorithms in the setting of general graphs. It has been
recently shown (Monemizadeh et al. 2017) that for bounded-degree graphs, any
constant-query tester can be emulated in the random order streaming model by a
streaming algorithm that uses only space required to store a constant number of
words. However, in a more natural setting of general graphs, with no
restriction on the maximum degree, no such results were known because of our
lack of understanding of constant-query testers in general graphs and lack of
techniques to appropriately emulate in the streaming setting off-line
algorithms allowing many high-degree vertices.
</p>
<p>In this work we advance our understanding on both of these challenges. First,
we provide canonical testers for all constant-query testers for general graphs,
both, for one-sided and two-sided errors. Such canonizations were only known
before (in the adjacency matrix model) for dense graphs (Goldreich and Trevisan
2003) and (in the adjacency list model) for bounded degree (di-)graphs
(Goldreich and Ron 2011, Czumaj et al. 2016). Using the concept of canonical
testers, we then prove that every property of general graphs that is
constant-query testable with one-sided error can also be tested in
constant-space with one-sided error in the random order streaming model.
</p>
<p>Our results imply, among others, that properties like $(s,t)$
disconnectivity, $k$-path-freeness, etc. are constant-space testable in random
order streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01644"><span class="datestr">at May 07, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01498">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01498">DynComm R Package -- Dynamic Community Detection for Evolving Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sarmento:Rui_Portocarrero.html">Rui Portocarrero Sarmento</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lemos:Lu=iacute=s.html">Luís Lemos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cordeiro:M=aacute=rio.html">Mário Cordeiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossetti:Giulio.html">Giulio Rossetti</a>, Douglas Cardoso <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01498">PDF</a><br /><b>Abstract: </b>Nowadays, the analysis of dynamics in networks represents a great deal in the
Social Network Analysis research area. To support students, teachers,
developers, and researchers in this work we introduce a novel R package, namely
DynComm. It is designed to be a multi-language package, that can be used for
community detection and analysis on dynamic networks. The package introduces
interfaces to facilitate further developments and the addition of new and
future developed algorithms to deal with community detection in evolving
networks. This new package has the goal of abstracting the programmatic
interface of the algorithms, whether they are written in R or other languages,
and expose them as functions in R.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01498"><span class="datestr">at May 07, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01495">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01495">New Notions and Constructions of Sparsification for Graphs and Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bansal:Nikhil.html">Nikhil Bansal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trevisan:Luca.html">Luca Trevisan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01495">PDF</a><br /><b>Abstract: </b>A sparsifier of a graph $G$ (Bencz\'ur and Karger; Spielman and Teng) is a
sparse weighted subgraph $\tilde G$ that approximately retains the cut
structure of $G$. For general graphs, non-trivial sparsification is possible
only by using weighted graphs in which different edges have different weights.
Even for graphs that admit unweighted sparsifiers, there are no known
polynomial time algorithms that find such unweighted sparsifiers.
</p>
<p>We study a weaker notion of sparsification suggested by Oveis Gharan, in
which the number of edges in each cut $(S,\bar S)$ is not approximated within a
multiplicative factor $(1+\epsilon)$, but is, instead, approximated up to an
additive term bounded by $\epsilon$ times $d\cdot |S| + \text{vol}(S)$, where
$d$ is the average degree, and $\text{vol}(S)$ is the sum of the degrees of the
vertices in $S$. We provide a probabilistic polynomial time construction of
such sparsifiers for every graph, and our sparsifiers have a near-optimal
number of edges $O(\epsilon^{-2} n {\rm polylog}(1/\epsilon))$. We also provide
a deterministic polynomial time construction that constructs sparsifiers with a
weaker property having the optimal number of edges $O(\epsilon^{-2} n)$. Our
constructions also satisfy a spectral version of the ``additive
sparsification'' property.
</p>
<p>Our construction of ``additive sparsifiers'' with $O_\epsilon (n)$ edges also
works for hypergraphs, and provides the first non-trivial notion of
sparsification for hypergraphs achievable with $O(n)$ hyperedges when
$\epsilon$ and the rank $r$ of the hyperedges are constant. Finally, we provide
a new construction of spectral hypergraph sparsifiers, according to the
standard definition, with ${\rm poly}(\epsilon^{-1},r)\cdot n\log n$
hyperedges, improving over the previous spectral construction (Soma and
Yoshida) that used $\tilde O(n^3)$ hyperedges even for constant $r$ and
$\epsilon$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01495"><span class="datestr">at May 07, 2019 11:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01468">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01468">New reduction rules for the tree bisection and reconnection distance</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kelk:Steven.html">Steven Kelk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Linz:Simone.html">Simone Linz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01468">PDF</a><br /><b>Abstract: </b>Recently it was shown that, if the subtree and chain reduction rules have
been applied exhaustively to two unrooted phylogenetic trees, the reduced trees
will have at most 15k-9 taxa where k is the TBR (Tree Bisection and
Reconnection) distance between the two trees, and that this bound is tight.
Here we propose five new reduction rules and show that these further reduce the
bound to 11k-9. The new rules combine the ``unrooted generator'' approach
introduced in [Kelk and Linz 2018] with a careful analysis of agreement forests
to identify (i) situations when chains of length 3 can be further shortened
without reducing the TBR distance, and (ii) situations when small subtrees can
be identified whose deletion is guaranteed to reduce the TBR distance by 1. To
the best of our knowledge these are the first reduction rules that strictly
enhance the reductive power of the subtree and chain reduction rules.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01468"><span class="datestr">at May 07, 2019 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01428">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01428">Pandora's Problem with Nonobligatory Inspection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beyhaghi:Hedyeh.html">Hedyeh Beyhaghi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleinberg:Robert.html">Robert Kleinberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01428">PDF</a><br /><b>Abstract: </b>Martin Weitzman's "Pandora's problem" furnishes the mathematical basis for
optimal search theory in economics. Nearly 40 years later, Laura Doval
introduced a version of the problem in which the searcher is not obligated to
pay the cost of inspecting an alternative's value before selecting it. Unlike
the original Pandora's problem, the version with nonobligatory inspection
cannot be solved optimally by any simple ranking-based policy, and it is
unknown whether there exists any polynomial-time algorithm to compute the
optimal policy. This motivates the study of approximately optimal policies that
are simple and computationally efficient. In this work we provide the first
non-trivial approximation guarantees for this problem. We introduce a family of
"committing policies" such that it is computationally easy to find and
implement the optimal committing policy. We prove that the optimal committing
policy is guaranteed to approximate the fully optimal policy within a
$1-\frac1e = 0.63\ldots$ factor, and for the special case of two boxes we
improve this factor to 4/5 and show that this approximation is tight for the
class of committing policies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01428"><span class="datestr">at May 07, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01373">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01373">Exploring Differential Obliviousness</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beimel:Amos.html">Amos Beimel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nissim:Kobbi.html">Kobbi Nissim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zaheri:Mohammad.html">Mohammad Zaheri</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01373">PDF</a><br /><b>Abstract: </b>In a recent paper Chan et al. [SODA '19] proposed a relaxation of the notion
of (full) memory obliviousness, which was introduced by Goldreich and Ostrovsky
[J. ACM '96] and extensively researched by cryptographers. The new notion,
differential obliviousness, requires that any two neighboring inputs exhibit
similar memory access patterns, where the similarity requirement is that of
differential privacy. Chan et al. demonstrated that differential obliviousness
allows achieving improved efficiency for several algorithmic tasks, including
sorting, merging of sorted lists, and range query data structures.
</p>
<p>In this work, we continue the exploration and mapping of differential
obliviousness, focusing on algorithms that do not necessarily examine all their
input. This choice is motivated by the fact that the existence of logarithmic
overhead ORAM protocols implies that differential obliviousness can yield at
most a logarithmic improvement in efficiency for computations that need to
examine all their input. In particular, we explore property testing, where we
show that differential obliviousness yields an almost linear improvement in
overhead in the dense graph model, and at most quadratic improvement in the
bounded degree model. We also explore tasks where a non-oblivious algorithm
would need to explore different portions of the input, where the latter would
depend on the input itself, and where we show that such a behavior can be
maintained under differential obliviousness, but not under full obliviousness.
Our examples suggest that there would be benefits in further exploring which
class of computational tasks are amenable to differential obliviousness.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01373"><span class="datestr">at May 07, 2019 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.01325">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.01325">Breaking the Bellman-Ford Shortest-Path Bound</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elmasry:Amr.html">Amr Elmasry</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.01325">PDF</a><br /><b>Abstract: </b>In this paper we give a single-source shortest-path algorithm that breaks,
after over 60 years, the $O(n \cdot m)$ time bound for the Bellman-Ford
algorithm, where $n$ is the number of vertices and $m$ is the number of arcs of
the graph. Our algorithm converts the input graph to a graph with nonnegative
weights by performing at most $\min(\sqrt{n},\sqrt{m/\log n})$ calls to
Dijkstra's algorithm, such that the shortest-path tree is the same for the new
graph as that for the original. When Dijkstra's algorithm is implemented using
Fibonacci heaps, the running time of our algorithm is therefore $O(\sqrt{n}
\cdot m + n \cdot \sqrt{m \log n})$. We also give a second implementation that
performs few calls to Dijkstra's algorithm if the graph contains few negative
arcs on the shortest-path tree.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.01325"><span class="datestr">at May 07, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/">Answer to TYI 37: Arithmetic Progressions in 3D Brownian Motion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Consider a Brownian motion in three dimensional space. <a href="https://gilkalai.wordpress.com/2019/03/07/test-your-intuition-or-simply-guess-37-arithmetic-progressions-for-brownian-motion-in-space/">We asked (TYI 37)</a> What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, <img src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1,x_2, x_t" class="latex" title="x_1,x_2, x_t" />, so that all <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2B1%7D-x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{i+1}-x_i" class="latex" title="x_{i+1}-x_i" /> are equal.)</p>
<p>Here is what you voted for</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png?w=640" alt="" class="alignnone size-full wp-image-17423" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Final-results</strong></span></p>
<p>Analysis of the poll results:  Almost surely 2 is the winner with 30.14% of the 209 votes, and almost surely infinity (28.71%) comes close at second place. In the  third place is  almost surely 3 (14.83%),  and then comes positive probability for each integer (13.4%), almost surely 5 (5.26%),  almost surely 6 (2.87%), and  almost surely 4 (2.39%).</p>
<h2>Test your political intuition: which coalition is going to be formed?</h2>
<p>Almost surely 2 (briefly AS2) and almost surely infinity (ASI) can form a government  with no need for a larger coalition. But they represent two political extremes. Is AS3 politically closer to AS2 or to ASI? “k with probability p_k for every k&gt;2” (briefly, COM) represent a complicated political massage. Is it closer to AS2 or to ASI?</p>
<a name="pd_a_10312542"></a>
<div style="display: inline-block;" class="PDS_Poll" id="PDI_container10312542"></div>
<div id="PD_superContainer"></div>
<noscript>&lt;a href="https://polldaddy.com/poll/10312542"&gt;Take Our Poll&lt;/a&gt;</noscript>
<p>You are most encouraged to participated in the new political poll for some coalitions that were offered and make a comment on your thoughts on which coalition will be formed.  This could be a lovely discussion. (See the old posts on <a href="https://gilkalai.wordpress.com/2009/02/16/which-coalition/">which coalition</a> <a href="https://gilkalai.wordpress.com/2009/02/17/which-coalition-to-form-2/">will be formed</a>.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-br.png"><img width="131" alt="" src="https://gilkalai.files.wordpress.com/2019/05/poll-br.png?w=131&amp;h=300" class="alignnone size-medium wp-image-17424" height="300" /></a> <a href="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png"><img width="133" alt="" src="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png?w=133&amp;h=300" class="alignnone size-medium wp-image-17425" height="300" /> </a><a href="https://gilkalai.files.wordpress.com/2019/05/poll189.png"><img width="127" alt="" src="https://gilkalai.files.wordpress.com/2019/05/poll189.png?w=127&amp;h=300" class="alignnone size-medium wp-image-17427" height="300" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Partial results. It was exciting to see how the standing of the answers changed in the process of counting the votes.</strong></span></p>
<p>And the correct answer is: <span id="more-17422"></span></p>
<h2><strong>5 (FIVE)</strong></h2>
<p>See the paper:</p>
<p class="title mathjax">Itai Benjamini and Gady Kozma: <a href="https://arxiv.org/abs/1810.10077">Arithmetic progressions in the trace of Brownian motion in space</a></p>
<p>Comments on the mathematics are welcome too!</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/"><span class="datestr">at May 06, 2019 09:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4240">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Online Optimization Post 3: Follow the Regularized Leader</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its analysis have statistical physics interpretations, but even this observation brings up more questions than it answers. The Gibbs distribution, for example, does put more weight on lower-energy states, and so it makes sense in an optimization setting, but to get good approximations one wants to use lower temperatures, while the distributions used by the multiplicative weights algorithms have temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\epsilon}" class="latex" title="{2\epsilon}" /> is the final “amortized” regret bound, so that one uses, quite counterintuitively, higher temperatures for better approximations. </p>
<p>
Furthermore, it is not clear how we would generalize the ideas of multiplicative weights to the case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is anything other than the set of distributions.</p>
<p>
Today we discuss the <em>“Follow the Regularized Leader”</em> method, which provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will then see how we can “discover” the definition and analysis of multiplicative weights, and how to “discover” another online algorithm which can be seen as a generalization of projected gradient descent (that is, one can derive the projected gradient descent algorithm and its analysis from this other online algorithm).</p>
<p>
<span id="more-4240"></span></p>
<p>
</p><p><b>1. Follow The Regularized Leader </b></p>
<p></p><p>
We will first state some results in full generality, making no assumptions on the set <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of feasible solutions or on the set of loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" title="{f_t : K \rightarrow {\mathbb R}}" /> encountered by the algorithm at each step.</p>
<p>
Let us try to define an online optimization algorithm from scratch. The solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> proposed by the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> can only depend on the previous cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_{t-1}}" class="latex" title="{f_1,\ldots,f_{t-1}}" />; how should it depend on it? If the offline optimal solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is consistently better than all others at each time step, then we would like <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be that solution, so we want <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be a solution that would have worked well in the previous steps. The most extreme way of implementing this idea is the <em>Follow the Leader</em> algorithm (abbreviated FTL), in which we set the solution at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /></p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> to be the best solution for the previous steps. (Note that the algorithm does not prescribe what solution to use at step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1}" class="latex" title="{t=1}" />.)</p>
<p>
It is possible for FTL to perform very badly. Consider for example the “experts” setting in which we analyzed multiplicative weights: the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta}" class="latex" title="{\Delta}" /> of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\ldots,n\}}" class="latex" title="{\{1,\ldots,n\}}" />, and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Csum_i+%5Cell_t%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \sum_i \ell_t(i) x(i)}" class="latex" title="{f_t(x) = \sum_i \ell_t(i) x(i)}" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+%5Cell_t%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0\leq \ell_t(i) \leq 1}" class="latex" title="{0\leq \ell_t(i) \leq 1}" />. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> and that <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%280.5%2C.0.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (0.5,.0.5)}" class="latex" title="{x_1 = (0.5,.0.5)}" />. Then a possible run of the algorithm could be: </p>
<ol>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%28.5%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (.5,.5)}" class="latex" title="{x_1 = (.5,.5)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1+%3D+%280%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_1 = (0,.5)}" class="latex" title="{\ell_1 = (0,.5)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2 = (1,0)}" class="latex" title="{x_2 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2 = (1,0)}" class="latex" title="{\ell_2 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_3 = (0,1)}" class="latex" title="{x_3 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_3 = (0,1)}" class="latex" title="{\ell_3 = (0,1)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_4 = (1,0)}" class="latex" title="{x_4 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_4 = (1,0)}" class="latex" title="{\ell_4 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_5 = (0,1)}" class="latex" title="{x_5 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_5 = (0,1)}" class="latex" title="{\ell_5 = (0,1)}" />
</li></ol>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cvdots&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \vdots" class="latex" title="\displaystyle \vdots" /></p>
<p>
In which, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm suffers a loss of <img src="https://s0.wp.com/latex.php?latex=%7BT-+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T- O(1)}" class="latex" title="{T- O(1)}" /> while the offline optimum is <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2 + O(1)}" class="latex" title="{T/2 + O(1)}" />. Thus, the regret is about <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2}" class="latex" title="{T/2}" />, which compares very unfavorably to the <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt T)}" class="latex" title="{O(\sqrt T)}" /> regret of the multiplicative weight algorithm. For general <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, a similar example shows that the regret of FTL can be as high as about <img src="https://s0.wp.com/latex.php?latex=%7BT%5Ccdot+%5Cleft%28+1-+%5Cfrac+1n+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T\cdot \left( 1- \frac 1n \right)}" class="latex" title="{T\cdot \left( 1- \frac 1n \right)}" />.</p>
<p>
In the above bad example, the algorithm keeps “overfitting” to the past history: if an expert is a bit better than the others, the algorithm puts all its probability mass on that expert, and the algorithm keeps changing its mind at every step. Interestingly, this is the only failure mode of the algorithm.</p>
<blockquote><p><b>Theorem 1 (Analysis of FTL)</b> <em> For any sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_t}" class="latex" title="{f_1,\ldots,f_t}" /> and any number of time steps <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, the FTL algorithm satisfies the regret bound </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t+%29+-+f_t%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " /></p>
</em><p><em> </em></p></blockquote>
<p> So that if the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> are Lipschitz with respect to a distance function on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, then the only way for the regret to be large is for <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to typically be far, in that distance, from <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t+1}}" class="latex" title="{x_{t+1}}" />.</p>
<p>
<em>Proof:</em>  Recalling the definition of regret, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " class="latex" title="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " /></p>
<p> the theorem is equivalent to <a name="ftl.analysis"></a></p><a name="ftl.analysis">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" class="latex" title="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" /></p>
</a><p><a name="ftl.analysis"></a> We will prove <a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a> by induction. The base case <img src="https://s0.wp.com/latex.php?latex=%7BT%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T=1}" class="latex" title="{T=1}" /> is just the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2}" class="latex" title="{x_2}" />. Assuming that $latex {<a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a>}&amp;fg=000000$ is true up to <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+f_%7BT%2B1%7D+%28x_%7BT%2B2%7D%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7BT%2B2%7D%29+%3D+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " class="latex" title="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " /></p>
<p> where the middle step follows from the use of the inductive assumption, which gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7BT%2B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " class="latex" title="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The above example and analysis suggest that we should modify FTL in such a way that the choices of the algorithm don’t change too much from step to step, and that the solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> should be a compromise between optimizing with respect to previous cost functions and not changing too much from step to step.</p>
<p>
In order to do this, we introduce a new function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" />, called a <em>regularizer</em> (more on it later), and, at each step, we compute the solution</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> This algorithm is called <em>Follow the Regularized Leader</em> or FTRL. Typically, the function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is chosen to be strictly convex and to take values that are rather big in magnitude. Then <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> will be the unique minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and, at each subsequent step, <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> will be selected in a way to balance the pull toward the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and the pull toward the FTL solution <img src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_k+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\arg\min_{x\in K} \sum_k f_k(x)}" class="latex" title="{\arg\min_{x\in K} \sum_k f_k(x)}" />. In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is large in magnitude compared to each <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" />, the solution will not change too much from step to step.</p>
<p>
We have the following analysis that makes no assumptions on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, on the cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> and on the regularizer (not even that the regularizer is convex).</p>
<blockquote><p><b>Theorem 2 (Analysis of FTRL)</b> <em> For every sequence of cost functions and every regularizer function, the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps of the FTRL algorithm is bounded as follows: for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_%7Bt%7D%29+-+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+R%28x%29+-+R%28x_1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" /></p>
<p> where </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%28x%29+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " class="latex" title="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let us run for <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the FTRL algorithm with regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" />, and call <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,\ldots,x_T}" class="latex" title="{x_1,\ldots,x_T}" /> the solutions computed by the FTL algorithm. </p>
<p>
Now consider the following mental experiment: we run the FTL algorithm for <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" /> steps, with the sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7BR%2Cf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R,f_1,\ldots,f_t}" class="latex" title="{R,f_1,\ldots,f_t}" />, and we use <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> as a first solution. Then we see that the solutions computed by the FTL algorithm will be precisely <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_1%2Cx_2%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,x_1,x_2,\ldots,x_T}" class="latex" title="{x_1,x_1,x_2,\ldots,x_T}" />. The regret bound for FTL implies that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />,</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x_1%29+-+R%28x%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_t%28x%29+%5Cleq+R%28x_1%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " class="latex" title="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Having established these results, the general recipe to solve an online optimization problem will be to find a regularizer function <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> such that the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> “pulls away from” solutions that would make the FTL algorithm overfit, and such that there is a good balance between how big <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> gets over <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> (because we pay <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%5E%2A%29+-+R%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x^*) - R(x_1)}" class="latex" title="{R(x^*) - R(x_1)}" /> in the regret, where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum) and how stable is the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" class="latex" title="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> varies.</p>
<p>
</p><p><b>2. Negative-Entropy Regularization </b></p>
<p></p><p>
Let us consider again the “experts” setting, that is, the online optimization setup in which <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2C+n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots, n\}}" class="latex" title="{\{ 1,\ldots, n\}}" /> and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+%5Cell_t+%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i \ell_t (i) x(i)}" class="latex" title="{f_t (x) = \sum_i \ell_t (i) x(i)}" /> with bounded coefficients.</p>
<p>
The example we showed above showed that FTL will tend to put all the probability mass on one expert. We would like to choose a regularizer that fights this tendency by penalizing “concentrated” distributions and favoring “spread-out” distributions. This observation might trigger the thought that the <em>entropy</em> of a distribution is a good measure of how concentrated or spread out it is, although the entropy is actually higher for spread-out distribution and smaller for concentrated ones. So we will use as a regularizer <em>minus the entropy</em>, multiplied by an appropriate scaling factor: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> (Entropy is usually defined using logarithms in base 2, but using natural logarithms will make it cleaner to take derivatives, and it only affects the constant factor <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" />.) With this choice of regularizer, we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%5CDelta%7D+%5C+%5C+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> To compute the minimum of the above function we will use the method of Lagrange multipliers. Specialized to our setting, the method of Lagrange multiplier states that if we want to solve the constrained minimization problem </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx+%3A+%5C+a%5ETx+%3D+b+%7D+%5C+%5C+f%28x+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " class="latex" title="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " /></p>
<p> we introduce a new parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> and define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%5Clambda+%28x%29+%3A%3D+f%28x%29+%2B+%5Clambda+%5Ccdot+%28a%5ET+x+-+b+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " class="latex" title="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " /></p>
<p> Then it is possible to prove that if <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a feasible minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />, then there is at least a value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%5E%2A%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x^*) = 0}" class="latex" title="{\nabla f_\lambda (x^*) = 0}" />, that is, such that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a stable point of <img src="https://s0.wp.com/latex.php?latex=%7Bf_%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_\lambda}" class="latex" title="{f_\lambda}" />. So one can proceed by finding all <img src="https://s0.wp.com/latex.php?latex=%7Bx%2C%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,\lambda}" class="latex" title="{x,\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x) = 0}" class="latex" title="{\nabla f_\lambda (x) = 0}" /> and then filtering out the values of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5ET+x+%5Cneq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a^T x \neq b}" class="latex" title="{a^T x \neq b}" />, and finally looking at which of the remaining <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> minimizes <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />.</p>
<p>
Ignoring for a moment the non-negativity constraints, the constraint <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in \Delta}" class="latex" title="{x\in \Delta}" /> reduces to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_i x_i = 1}" class="latex" title="{\sum_i x_i = 1}" />, so we have to consider the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+%5Ccdot+%5Cleft%28+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle+-+1%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " /></p>
<p> The partial derivative of the above expression with respect to <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i}" class="latex" title="{x_i}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+1+%2B+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " /></p>
<p> If we want the gradient to be zero then we want all the above expressions to be zero, which translates to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%7B%5Crm+exp%7D+%5Cleft%28+-1+-+%5Cfrac+%5Clambda+c+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " class="latex" title="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " /></p>
<p> There is only one value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> that makes the above solution a probability distribution, and the corresponding solution is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%5Cfrac+%7B%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+%7D+%7B%5Csum_j+%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28j%29+%5Cright%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " class="latex" title="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " /></p>
<p> Notice that this is exactly the solution computed by the multiplicative weights algorithm, if we choose <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = 1/\epsilon}" class="latex" title="{c = 1/\epsilon}" />. So we have “rediscovered” the multiplicative weights algorithm and we have also “explained” what it does: at every step it balances the goals of finding a solution that is good for the past and that has large entropy.</p>
<p>
Now it remains to bound, at each time step, </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x_t%29+-+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " class="latex" title="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " /></p>
<p> For this, it is convenient to return to the notation that we used in describing the multiplicative weights algorithm, that is, it is convenient to work with the weights defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_1%28i%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_1(i) = 1" class="latex" title="\displaystyle  w_1(i) = 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_%7Bt%2B1%7D+%28i%29+%3D+w_t+%28i%29+%5Ccdot+e%5E%7B+%5Cell_t+%28i%29+%2F+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" class="latex" title="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" /></p>
<p> so that, at each time step </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7Bw_t%28i%29%7D%7B%5Csum_j+w_t+%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " class="latex" title="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " /></p>
<p> We are assuming <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cell_t+%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 \leq \ell_t (i) \leq 1}" class="latex" title="{0 \leq \ell_t (i) \leq 1}" />, so the weights are non-increasing with time. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%28i%29+%3D+%5Cfrac+%7Bw_%7Bt%2B1%7D+%28i%29+%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%3D+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%5Cgeq+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%7D+%28j%29+%7D+%5Cgeq+x_t%28i%29+%5Ccdot+e%5E%7B-1%2Fc%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " class="latex" title="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " /></p>
<p> For every <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c \geq 1}" class="latex" title="{c \geq 1}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-1%2Fc%7D+%5Cgeq+1+-+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-1/c} \geq 1 - 1/c}" class="latex" title="{e^{-1/c} \geq 1 - 1/c}" />, so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%7D%28i%29+-+x_%7Bt%2B1%7D%28i%29+%5Cleq+%5Cfrac+1c+x_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " class="latex" title="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%5Csum_i+%5Cell_t%28i%29+%5Ccdot+%5Cfrac+1c+x_t%28i%29+%5Cleq+%5Cfrac+1c+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " class="latex" title="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " /></p>
<p> Putting it all together, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cfrac+Tc+%2B+c+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " class="latex" title="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " /></p>
<p> Choosing <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B%5Cln+n%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {\ln n}}}" class="latex" title="{c = \sqrt{\frac T {\ln n}}}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
<p> Thus, we have reconstructed the analysis of the multiplicative weights algorithm.</p>
<p>
Interestingly, the analysis that we derived today is not exactly identical to the one from the post on multiplicative weights. There, we derived the bound</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t%5E2+%28i%29+x_t+%28i%29+%5C+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " /></p>
<p> while here, setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = 1/c}" class="latex" title="{\epsilon = 1/c}" />, we derived </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t+%28i%29+x_t%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+-+%5Cfrac+1+%5Cepsilon+H%28x%5E%2A%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum and <img src="https://s0.wp.com/latex.php?latex=%7BH%28x%29+%3D+%5Csum_i+x_i+%5Cln+%5Cfrac+1+%7Bx_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" class="latex" title="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" /> is the entropy function (computed using natural logarithms). </p>
<p>
</p><p><b>3. L2 Regularization </b></p>
<p></p><p>
Now that we have a general method, let us apply it to a new context: suppose that, as before, our cost functions are linear, but let <img src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K = {\mathbb R}^n}" class="latex" title="{K = {\mathbb R}^n}" />. With linear cost functions and no bound on the size of solutions, it will not be possible to talk about regret with respect to the offline optimum, because the offline optimum will always be <img src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-\infty}" class="latex" title="{-\infty}" />, but it will be possible to talk about regret with respect to a particular offline solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, which will already lead to interesting consequences.</p>
<p>
What regularizer should we use? In reasoning about regularizers, it can be helpful to think about what would go wrong if we use FTL, and then considering what regularizer would successfully “pull away” from the bad solutions found by FTL. In this context of linear loss functions and unbounded solutions, FTL will pick an infinitely big solution at each step, or, to be more precise, the “max” in the definition of FTL is undefined. To fight this tendency of FTL to go off to infinity, it makes sense for the regularizer to be a measure of how big a solution is. Since we are going to have to compute derivatives, it is good to use a measure of “bigness” with a nice gradient, and <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{||x ||^2}" class="latex" title="{||x ||^2}" /> is a natural choice. So, for a scale parameter <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> to be optimized later, our regularizer will be </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%7C%7C+x%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c || x||^2 " class="latex" title="\displaystyle  R(x) := c || x||^2 " /></p>
<p> This tells us that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_+1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_ 1 = {\bf 0} " class="latex" title="\displaystyle  x_ 1 = {\bf 0} " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cx%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Clangle+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " /></p>
<p> The function that we are minimizing in the above expression is convex, so we just have to compute the gradient and set it to zero </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2c+x+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " class="latex" title="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+-+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " class="latex" title="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " /></p>
<p> Which can be also expressed as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%7B%5Cbf+0%7D%3B+%5C+%5C+%5C+x_%7Bt%2B1%7D+%3D+x_t+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " class="latex" title="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " /></p>
<p> This makes perfect sense because, in the “experts” interpretation, we want to penalize the experts that performed badly in the past. Here we have no constraints on our allocations, so we simply decrease (additively this time, not multiplicatively) the allocation to the experts that caused a higher loss.</p>
<p>
To compute the regret bound, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%3D+%5Cleft%5Clangle+%5Cell_t+%2C+%5Cfrac+1+%7B2c%7D+%5Cell_t+%5Cright%5Crangle+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t%7C%7C%5E2+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " class="latex" title="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " /></p>
<p> and so the regret with respect to a solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+R%28x%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%7C%7C+x%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " class="latex" title="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " /></p>
<p> If we know a bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+t%3A+%5C+%5C+%7C%7C+%5Cell_t+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall t: \ \ || \ell_t || \leq L " class="latex" title="\displaystyle  \forall t: \ \ || \ell_t || \leq L " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x|| \leq D " class="latex" title="\displaystyle  || x|| \leq D " /></p>
<p> then we can optimize <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B2D%5E2+L%5E2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {2D^2 L^2}}}" class="latex" title="{c = \sqrt{\frac T {2D^2 L^2}}}" /> and we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D+L+%5Csqrt%7B+2+T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " /></p>
<p>
</p><p><b>  3.1. Dealing with Constraints </b></p>
<p></p><p>
Consider now the case in which the loss functions are linear and <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is an arbitrary convex set. Using the same regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) = c || x||^2}" class="latex" title="{R(x) = c || x||^2}" /> we have the algorithm </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " class="latex" title="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+c+%7C%7Cx+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " /></p>
<p> How can we solve the above constrained optimization problem? A very helpful observation is that we can first solve the unconstrained optimization and then project on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, that is we can proceed as follows: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cy+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " class="latex" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5CPi_K+%28y_%7Bt%2B1%7D+%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p> and we claim that we always have <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D+%3D+x_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1} = x_{t+1}}" class="latex" title="{x'_{t+1} = x_{t+1}}" />. The fact that we can reduce a regularized constrained optimization problem to an unconstrained problem and a projection is part of a broader theory that we will describe in a later post. For now, we will limit to prove the equivalence in this specific setting. First of all, we already have an expression for <img src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_{t+1}}" class="latex" title="{y_{t+1}}" />, namely </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " class="latex" title="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " /></p>
<p> Now the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1}}" class="latex" title="{x'_{t+1}}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+%2B+%7C%7C+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft%5Clangle+x+%2C+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx%7C%7C%5E2+-+%5Csum_%7Bk%3D1%7D%5Et+%5Cleft%5Clangle+x+%2C+%5Cell_t+%5Cright%5Crangle+%3D+x_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " class="latex" title="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " /></p>
<p>
In order to bound the regret, we have to compute </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_t%28x_%7Bt%2B1%7D+%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%7C%7C+%5Cell_t+%7C%7C+%5Ccdot+%7C%7Cx_t+-+x_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " class="latex" title="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " /></p>
<p> and since L2 projections cannot increase L2 distances, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_t+-+x_%7Bt%2B1%7D+%7C%7C+%5Cleq+%7C%7C+y_t+-+y_%7Bt%2B1%7D+%7C%7C+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " class="latex" title="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " /></p>
<p>
So the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+%7C%7Cx%5E%2A%7C%7C%5E2+-+c%7C%7C+x_1%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is an upper bound to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{x\in K} || x||}" class="latex" title="{\max_{x\in K} || x||}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> is an upper bound to the norm <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+%5Cell_t+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| \ell_t ||}" class="latex" title="{|| \ell_t ||}" /> of all the loss vectors, then</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+D%5E2+%2B+%5Cfrac+1+%7B2c%7D+T+L%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " /></p>
<p> which can be optimized to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+DL+%5Csqrt+%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " class="latex" title="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " /></p>
<p>
</p><p><b>  3.2. Deriving the Analysis of Gradient Descent </b></p>
<p></p><p>
Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: K \rightarrow {\mathbb R}}" class="latex" title="{g: K \rightarrow {\mathbb R}}" /> is a convex function whose gradient <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla g}" class="latex" title="{\nabla g}" /> is well defined at all points in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, and that we are interested in minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />. Then a way to reduce this problem to online optimization would be to use the function <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as loss function at each step. Then the offline optimum would be the minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and achieving small regret means that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1T+%5Csum_t+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1T \sum_t g(x_t)}" class="latex" title="{\frac 1T \sum_t g(x_t)}" /> is close to the minimum of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and so the best <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer.</p>
<p>
Unfortunately, this is not a very helpful idea, because if we ran an FTRL algorithm against an adversary that keeps proposing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost function at each step then we would have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29+%2B+t+%5Ccdot+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " /></p>
<p> which, for large <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, is essentially the same problem as minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, so we have basically reduced the problem of minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to itself.</p>
<p>
Indeed, the power of the FTRL algorithm is that the algorithm does well even though it does not know the cost function, and if we keep using the same cost function at each step we are not making a good use of its power. Now, suppose that we use cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> such that </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+x%5Cin+K+%5C+%5C+f_t%28x%29+%5Cleq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\forall x\in K \ \ f_t(x) \leq g(x)}" class="latex" title="{\forall x\in K \ \ f_t(x) \leq g(x)}" />
</li></ul>
<p> Then, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%3D+%7B%5Crm+Regret%7D_T+%2B+%5Cmin%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cleq+%7B%5Crm+Regret%7D_T+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> meaning </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so one of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer. Indeed, using convexity, we also have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright%29+%5Cleq+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so the average of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is also an approximate minimizer. From the point of view of exploiting FTRL do to minimize <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> as above work just as well as presenting <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost functions at each step.</p>
<p>
How do we find cost functions that satisfy the above two properties and for which the FTRL algorithm is easy to implement? The idea is to let <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> be the linear approximation of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+g%28x_t%29+%2B+%5Clangle+%5Cnabla+g+%28x_t%29%2C+x+-+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " class="latex" title="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " /></p>
<p> The <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" /> condition is immediate, and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%5Cgeq+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x) \geq f_t (x) " class="latex" title="\displaystyle  g(x) \geq f_t (x) " /></p>
<p> is a consequence of the convexity of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />.</p>
<p>
The cost functions that we have defined are affine functions, that is, each of them equals a constant plus a linear function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x%29+%3D+%5Cleft%28+g%28x_t%29+-+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x_t%5Crangle+%5Cright%29+%2B+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " class="latex" title="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " /></p>
<p>
Adding a constant term to a cost function does not change the iteration of FTRL, and does not change the regret (because the same term is added both to the solution found by the algorithm and to the offline optimum), so the algorithm is just initialized with</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_1 = {\bf 0} " class="latex" title="\displaystyle  y_1 = {\bf 0} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5CPi_K%28%7B%5Cbf+0%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " class="latex" title="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " /></p>
<p> and then continues with the update rules </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3Dy_t+-%5Cfrac+1+%7B2c%7D+%5Cnabla+g+%28x_t%29+%5Cmbox%7B+for+%7D+t+%5Cgeq+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" class="latex" title="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5CPi_K%28y_%7Bt%2B1%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+%5Cmbox%7B+for+%7D+t+%5Cgeq+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " class="latex" title="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " /></p>
<p> which is just projected gradient descent.</p>
<p>
If we have known upper bounds </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+%5Cnabla+g%28x%29+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " class="latex" title="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+x+%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || x || \leq D " class="latex" title="\displaystyle  \forall x \in K \ \ || x || \leq D " /></p>
<p> then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1+T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright+%29+%5Cleq+DL+%5Ccdot+%5Csqrt%7B%5Cfrac+2+T%7D+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> which means that to achieve additive error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> it is enough to proceed for <img src="https://s0.wp.com/latex.php?latex=%7B2D%5E2L%5E2+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2D^2L^2 / \epsilon^2}" class="latex" title="{2D^2L^2 / \epsilon^2}" /> steps. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"><span class="datestr">at May 06, 2019 02:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
