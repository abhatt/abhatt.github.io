<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 31, 2022 09:38 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14718">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14718">Theory and Applications of Probabilistic Kolmogorov Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Zhenjian.html">Zhenjian Lu</a>, Igor C. Oliveira <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14718">PDF</a><br /><b>Abstract: </b>Diverse applications of Kolmogorov complexity to learning [CIKK16], circuit
complexity [OPS19], cryptography [LP20], average-case complexity [Hir21], and
proof search [Kra22] have been discovered in recent years. Since the running
time of algorithms is a key resource in these fields, it is crucial in the
corresponding arguments to consider time-bounded variants of Kolmogorov
complexity. While fruitful interactions between time-bounded Kolmogorov
complexity and different areas of theoretical computer science have been known
for quite a while (e.g., [Sip83, Ko91, ABK+06, AF09], to name a few), the
aforementioned results have led to a renewed interest in this topic.
</p>
<p>The theory of Kolmogorov complexity is well understood, but many useful
results and properties of Kolmogorov complexity are not known to hold in
time-bounded settings. This creates technical difficulties or leads to
conditional results when applying methods from time-bounded Kolmogorov
complexity to algorithms and complexity theory. Perhaps even more importantly,
in many cases it is necessary to consider randomised algorithms. Since random
strings have high complexity, the classical theory of time-bounded Kolmogorov
complexity might be inappropriate in such contexts.
</p>
<p>To mitigate these issues and develop a theory of time-bounded Kolmogorov
complexity that survives in the setting of randomised computations, some recent
papers [Oli19, LO21, LOS21, GKLO22, LOZ22] have explored probabilistic notions
of time-bounded Kolmogorov complexity, such as $\mathsf{rKt}$ complexity
[Oli19], $\mathsf{rK}^t$ complexity [LOS21], and $\mathsf{pK}^t$ complexity
[GKLO22]. These measures consider different ways of encoding an object via a
probabilistic representation. In this survey, we provide an introduction to
probabilistic time-bounded Kolmogorov complexity and its applications,
highlighting many open problems and research directions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14718"><span class="datestr">at May 31, 2022 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14717">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14717">Generalized Stochastic Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farhadi:Alireza.html">Alireza Farhadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gilbert:Jacob.html">Jacob Gilbert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14717">PDF</a><br /><b>Abstract: </b>In this paper, we generalize the recently studied Stochastic Matching problem
to more accurately model a significant medical process, kidney exchange, and
several other applications. Up until now the Stochastic Matching problem that
has been studied was as follows: given a graph G = (V, E), each edge is
included in the realized sub-graph of G mutually independently with probability
p_e, and the goal is to find a degree-bounded sub-graph Q of G that has an
expected maximum matching that approximates the expected maximum matching of
the realized sub-graph. This model does not account for possibilities of vertex
dropouts, which can be found in several applications, e.g. in kidney exchange
when donors or patients opt out of the exchange process as well as in online
freelancing and online dating when online profiles are found to be faked. Thus,
we will study a more generalized model of Stochastic Matching in which vertices
and edges are both realized independently with some probabilities p_v, p_e,
respectively, which more accurately fits important applications than the
previously studied model.
</p>
<p>We will discuss the first algorithms and analysis for this generalization of
the Stochastic Matching model and prove that they achieve good approximation
ratios. In particular, we show that the approximation factor of a natural
algorithm for this problem is at least $0.6568$ in unweighted graphs, and $1/2
+ \epsilon$ in weighted graphs for some constant $\epsilon &gt; 0$. We further
improve our result for unweighted graphs to $2/3$ using edge degree constrained
subgraphs (EDCS).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14717"><span class="datestr">at May 31, 2022 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14543">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14543">Spatial Locality and Granularity Change in Caching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beckmann:Nathan.html">Nathan Beckmann</a>, Phillip B Gibbons, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGuffey:Charles.html">Charles McGuffey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14543">PDF</a><br /><b>Abstract: </b>Caches exploit temporal and spatial locality to allow a small memory to
provide fast access to data stored in large, slow memory. The temporal aspect
of locality is extremely well studied and understood, but the spatial aspect
much less so. We seek to gain an increased understanding of spatial locality by
defining and studying the Granularity-Change Caching Problem. This problem
modifies the traditional caching setup by grouping data items into blocks, such
that a cache can choose any subset of a block to load for the same cost as
loading any individual item in the block.
</p>
<p>We show that modeling such spatial locality significantly changes the caching
problem. This begins with a proof that Granularity-Change Caching is
NP-Complete in the offline setting, even when all items have unit size and all
blocks have unit load cost. In the online setting, we show a lower bound for
competitive ratios of deterministic policies that is significantly worse than
traditional caching. Moreover, we present a deterministic replacement policy
called Item-Block Layered Partitioning and show that it obtains a competitive
ratio close to that lower bound. Moreover, our bounds reveal a new issue
arising in the Granularity-Change Caching Problem where the choice of offline
cache size affects the competitiveness of different online algorithms relative
to one another. To deal with this issue, we extend a prior (temporal) locality
model to account for spatial locality, and provide a general lower bound in
addition to an upper bound for Item-Block Layered Partitioning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14543"><span class="datestr">at May 31, 2022 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14478">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14478">Overcoming Congestion in Distributed Coloring</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Halld=oacute=rsson:Magn=uacute=s_M=.html">Magnús M. Halldórsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nolin:Alexandre.html">Alexandre Nolin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tonoyan:Tigran.html">Tigran Tonoyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14478">PDF</a><br /><b>Abstract: </b>We present a new technique to efficiently sample and communicate a large
number of elements from a distributed sampling space. When used in the context
of a recent LOCAL algorithm for $(\operatorname{degree}+1)$-list-coloring
(D1LC), this allows us to solve D1LC in $O(\log^5 \log n)$ CONGEST rounds, and
in only $O(\log^* n)$ rounds when the graph has minimum degree $\Omega(\log^7
n)$, w.h.p.
</p>
<p>The technique also has immediate applications in testing some graph
properties locally, and for estimating the sparsity/density of local subgraphs
in $O(1)$ CONGEST rounds, w.h.p.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14478"><span class="datestr">at May 31, 2022 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14452">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14452">Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharma:Chhavi.html">Chhavi Sharma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Vishnu.html">Vishnu Narayanan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balamurugan:P=.html">P. Balamurugan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14452">PDF</a><br /><b>Abstract: </b>We propose two stochastic gradient algorithms to solve a class of
saddle-point problems in a decentralized setting (without a central server).
The proposed algorithms are the first to achieve sub-linear/linear computation
and communication complexities using respectively stochastic
gradient/stochastic variance reduced gradient oracles with compressed
information exchange to solve non-smooth strongly-convex strongly-concave
saddle-point problems in decentralized setting. Our first algorithm is a
Restart-based Decentralized Proximal Stochastic Gradient method with
Compression (C-RDPSG) for general stochastic settings. We provide rigorous
theoretical guarantees of C-RDPSG with gradient computation complexity and
communication complexity of order $\mathcal{O}( (1+\delta)^4
\frac{1}{L^2}{\kappa_f^2}\kappa_g^2 \frac{1}{\epsilon} )$, to achieve an
$\epsilon$-accurate saddle-point solution, where $\delta$ denotes the
compression factor, $\kappa_f$ and $\kappa_g$ denote respectively the condition
numbers of objective function and communication graph, and $L$ denotes the
smoothness parameter of the smooth part of the objective function. Next, we
present a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm
with Compression (C-DPSVRG) for finite sum setting which exhibits gradient
computation complexity and communication complexity of order
$\mathcal{O}((1+\delta)\kappa_f^2 \kappa_g \log(\frac{1}{\epsilon}))$.
Extensive numerical experiments show competitive performance of the proposed
algorithms and provide support to the theoretical results obtained.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14452"><span class="datestr">at May 31, 2022 01:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14434">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14434">A Theory of L-shaped Floor-plans</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Raveena, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shekhawat:Krishnendra.html">Krishnendra Shekhawat</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14434">PDF</a><br /><b>Abstract: </b>Existing graph theoretic approaches are mainly restricted to floor-plans with
rectangular boundary. In this paper, we introduce floor-plans with L-shaped
boundary (boundary with only one concave corner). To ensure the L-shaped
boundary, we introduce the concept of non-triviality of a floor-plan. A
floor-plan with a rectilinear boundary with at least one concave corner is
non-trivial if the number of concave corners can not be reduced, without
affecting the modules adjacencies within it. Further, we present necessary and
sufficient conditions for the existence of a non-trivial L-shaped floor-plan
corresponding to a properly triangulated planar graph (PTPG) G. Also, we
develop an O(n^2) algorithm for its construction, if it exists.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14434"><span class="datestr">at May 31, 2022 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14414">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14414">Theoretical Foundation of the Stretch Energy Minimization for Area-Preserving Mappings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yueh:Mei=Heng.html">Mei-Heng Yueh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14414">PDF</a><br /><b>Abstract: </b>The stretch energy is a fully nonlinear energy functional that has been
applied to the numerical computation of area-preserving mappings. However, this
approach lacks theoretical support and the analysis is complicated due to the
full nonlinearity of the functional. In this paper, we provide a theoretical
foundation of the stretch energy minimization for the computation of
area-preserving mappings, including a neat formulation of the gradient of the
functional, and the proof of the minimizers of the functional being
area-preserving mappings. In addition, the geometric interpretation of the
stretch energy is also provided to better understand this energy functional.
Furthermore, numerical experiments are demonstrated to validate the
effectiveness and accuracy of the stretch energy minimization for the
computation of square-shaped area-preserving mappings of simplicial surfaces.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14414"><span class="datestr">at May 31, 2022 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14407">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14407">An efficient polynomial-time approximation scheme for parallel multi-stage open shops</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Jianming.html">Jianming Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ruyan.html">Ruyan Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Guohui.html">Guohui Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Su:Bing.html">Bing Su</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tong:Weitian.html">Weitian Tong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yao.html">Yao Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14407">PDF</a><br /><b>Abstract: </b>Various new scheduling problems have been arising from practical production
processes and spawning new research areas in the scheduling field. We study the
parallel multi-stage open shops problem, which generalizes the classic open
shop scheduling and parallel machine scheduling problems. Given m identical
k-stage open shops and a set of n jobs, we aim to process all jobs on these
open shops with the minimum makespan, i.e., the completion time of the last
job, under the constraint that job preemption is not allowed. We present an
efficient polynomial-time approximation scheme (EPTAS) for the case when both m
and k are constant. The main idea for our EPTAS is the combination of several
categorization, scaling, and linear programming rounding techniques. Jobs
and/or operations are first scaled and then categorized carefully into multiple
types so that different types of jobs and/or operations are scheduled
appropriately without increasing the makespan too much.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14407"><span class="datestr">at May 31, 2022 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14390">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14390">Topological phase estimation method for reparameterized periodic functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonis:Thomas.html">Thomas Bonis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chazal:Fr=eacute=d=eacute=ric.html">Frédéric Chazal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Michel:Bertrand.html">Bertrand Michel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reise:Wojciech.html">Wojciech Reise</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14390">PDF</a><br /><b>Abstract: </b>We consider a signal composed of several periods of a periodic function, of
which we observe a noisy reparametrisation. The phase estimation problem
consists of finding that reparametrisation, and, in particular, the number of
observed periods. Existing methods are well-suited to the setting where the
periodic function is known, or at least, simple. We consider the case when it
is unknown and we propose an estimation method based on the shape of the
signal. We use the persistent homology of sublevel sets of the signal to
capture the temporal structure of its local extrema. We infer the number of
periods in the signal by counting points in the persistence diagram and their
multiplicities. Using the estimated number of periods, we construct an
estimator of the reparametrisation. It is based on counting the number of
sufficiently prominent local minima in the signal. This work is motivated by a
vehicle positioning problem, on which we evaluated the proposed method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14390"><span class="datestr">at May 31, 2022 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14358">Fair Labeled Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esmaeili:Seyed_A=.html">Seyed A. Esmaeili</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duppala:Sharmila.html">Sharmila Duppala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dickerson:John_P=.html">John P. Dickerson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brubach:Brian.html">Brian Brubach</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14358">PDF</a><br /><b>Abstract: </b>Numerous algorithms have been produced for the fundamental problem of
clustering under many different notions of fairness. Perhaps the most common
family of notions currently studied is group fairness, in which proportional
group representation is ensured in every cluster. We extend this direction by
considering the downstream application of clustering and how group fairness
should be ensured for such a setting. Specifically, we consider a common
setting in which a decision-maker runs a clustering algorithm, inspects the
center of each cluster, and decides an appropriate outcome (label) for its
corresponding cluster. In hiring for example, there could be two outcomes,
positive (hire) or negative (reject), and each cluster would be assigned one of
these two outcomes. To ensure group fairness in such a setting, we would desire
proportional group representation in every label but not necessarily in every
cluster as is done in group fair clustering. We provide algorithms for such
problems and show that in contrast to their NP-hard counterparts in group fair
clustering, they permit efficient solutions. We also consider a well-motivated
alternative setting where the decision-maker is free to assign labels to the
clusters regardless of the centers' positions in the metric space. We show that
this setting exhibits interesting transitions from computationally hard to easy
according to additional constraints on the problem. Moreover, when the
constraint parameters take on natural values we show a randomized algorithm for
this setting that always achieves an optimal clustering and satisfies the
fairness constraints in expectation. Finally, we run experiments on real world
datasets that validate the effectiveness of our algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14358"><span class="datestr">at May 31, 2022 01:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14352">Travelling Salesman Problem: Parallel Implementations &amp; Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Amey Gohil, Manan Tayal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sahu:Tezan.html">Tezan Sahu</a>, Vyankatesh Sawalpurkar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14352">PDF</a><br /><b>Abstract: </b>The Traveling Salesman Problem (often called TSP) is a classic algorithmic
problem in the field of computer science and operations research. It is an
NP-Hard problem focused on optimization. TSP has several applications even in
its purest formulation, such as planning, logistics, and the manufacture of
microchips; and can be slightly modified to appear as a sub-problem in many
areas, such as DNA sequencing. In this paper, a study on parallelization of the
Brute Force approach (under several paradigms) of the Travelling Salesman
Problem is presented. Detailed timing studies for the serial and various
parallel implementations of the Travelling Salesman Problem have also been
illustrated.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14352"><span class="datestr">at May 31, 2022 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14324">Differentially Private Covariance Revisited</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Wei.html">Wei Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liang:Yuting.html">Yuting Liang</a>, Ke Yi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14324">PDF</a><br /><b>Abstract: </b>In this paper, we present three new error bounds, in terms of the Frobenius
norm, for covariance estimation under differential privacy: (1) a worst-case
bound of $\tilde{O}(d^{1/4}/\sqrt{n})$, which improves the standard Gaussian
mechanism $\tilde{O}(d/n)$ for the regime $d&gt;\widetilde{\Omega}(n^{2/3})$; (2)
a trace-sensitive bound that improves the state of the art by a
$\sqrt{d}$-factor, and (3) a tail-sensitive bound that gives a more
instance-specific result. The corresponding algorithms are also simple and
efficient. Experimental results show that they offer significant improvements
over prior work.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14324"><span class="datestr">at May 31, 2022 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14284">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14284">Provably Auditing Ordinary Least Squares in Low Dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohatgi:Dhruv.html">Dhruv Rohatgi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14284">PDF</a><br /><b>Abstract: </b>Measuring the stability of conclusions derived from Ordinary Least Squares
linear regression is critically important, but most metrics either only measure
local stability (i.e. against infinitesimal changes in the data), or are only
interpretable under statistical assumptions. Recent work proposes a simple,
global, finite-sample stability metric: the minimum number of samples that need
to be removed so that rerunning the analysis overturns the conclusion,
specifically meaning that the sign of a particular coefficient of the estimated
regressor changes. However, besides the trivial exponential-time algorithm, the
only approach for computing this metric is a greedy heuristic that lacks
provable guarantees under reasonable, verifiable assumptions; the heuristic
provides a loose upper bound on the stability and also cannot certify lower
bounds on it.
</p>
<p>We show that in the low-dimensional regime where the number of covariates is
a constant but the number of samples is large, there are efficient algorithms
for provably estimating (a fractional version of) this metric. Applying our
algorithms to the Boston Housing dataset, we exhibit regression analyses where
we can estimate the stability up to a factor of $3$ better than the greedy
heuristic, and analyses where we can certify stability to dropping even a
majority of the samples.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14284"><span class="datestr">at May 31, 2022 01:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14198">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14198">Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knittel:Marina.html">Marina Knittel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dickerson:John_P=.html">John P. Dickerson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14198">PDF</a><br /><b>Abstract: </b>Clustering is a fundamental building block of modern statistical analysis
pipelines. Fair clustering has seen much attention from the machine learning
community in recent years. We are some of the first to study fairness in the
context of hierarchical clustering, after the results of Ahmadian et al. from
NeurIPS in 2020. We evaluate our results using Dasgupta's cost function,
perhaps one of the most prevalent theoretical metrics for hierarchical
clustering evaluation. Our work vastly improves the previous
$O(n^{5/6}poly\log(n))$ fair approximation for cost to a near polylogarithmic
$O(n^\delta poly\log(n))$ fair approximation for any constant $\delta\in(0,1)$.
This result establishes a cost-fairness tradeoff and extends to broader
fairness constraints than the previous work. We also show how to alter existing
hierarchical clusterings to guarantee fairness and cluster balance across any
level in the hierarchy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14198"><span class="datestr">at May 31, 2022 01:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14151">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14151">Interactive and Robust Mesh Booleans</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherchi:Gianmarco.html">Gianmarco Cherchi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pellacini:Fabio.html">Fabio Pellacini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Attene:Marco.html">Marco Attene</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livesu:Marco.html">Marco Livesu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14151">PDF</a><br /><b>Abstract: </b>Boolean operations are among the most used paradigms to create and edit
digital shapes. Despite being conceptually simple, the computation of mesh
Booleans is notoriously challenging. Main issues come from numerical
approximations that make the detection and processing of intersection points
inconsistent and unreliable, exposing implementations based on floating point
arithmetic to many kinds of degeneracy and failure. Numerical methods based on
rational numbers or exact geometric predicates have the needed robustness
guarantees, that are achieved at the cost of increased computation times that,
as of today, has always restricted the use of robust mesh Booleans to offline
applications. We introduce the first algorithm for Boolean operations with
robustness guarantees that is capable of operating at interactive frame rates
on meshes with up to 200K triangles. We evaluate our tool thoroughly,
considering not only interactive applications but also batch processing of
large collections of meshes, processing of huge meshes containing millions of
elements and variadic Booleans of hundreds of shapes altogether. In all these
experiments, we consistently outperform prior art by at least one order of
magnitude.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14151"><span class="datestr">at May 31, 2022 01:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14101">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14101">Adaptive Massively Parallel Algorithms for Cut Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knittel:Marina.html">Marina Knittel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olkowski:Jan.html">Jan Olkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saleh:Hamed.html">Hamed Saleh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14101">PDF</a><br /><b>Abstract: </b>We study the Weighted Min Cut problem in the Adaptive Massively Parallel
Computation (AMPC) model. In 2019, Behnezhad et al. [3] introduced the AMPC
model as an extension of the Massively Parallel Computation (MPC) model. In the
past decade, research on highly scalable algorithms has had significant impact
on many massive systems. The MPC model, introduced in 2010 by Karloff et al.
[16], which is an abstraction of famous practical frameworks such as MapReduce,
Hadoop, Flume, and Spark, has been at the forefront of this research. While
great strides have been taken to create highly efficient MPC algorithms for a
range of problems, recent progress has been limited by the 1-vs-2 Cycle
Conjecture [20], which postulates that the simple problem of distinguishing
between one and two cycles requires $\Omega(\log n)$ MPC rounds. In the AMPC
model, each machine has adaptive read access to a distributed hash table even
when communication is restricted (i.e., in the middle of a round). While
remaining practical [4], this gives algorithms the power to bypass limitations
like the 1-vs-2 Cycle Conjecture.
</p>
<p>We give the first sublogarithmic AMPC algorithm, requiring $O(\log\log n)$
rounds, for $(2+\epsilon)$-approximate weighted Min Cut. Our algorithm is
inspired by the divide and conquer approach of Ghaffari and Nowicki [11], which
solves the $(2+\epsilon)$-approximate weighted Min Cut problem in $O(\log
n\log\log n)$ rounds of MPC using the classic result of Karger and Stein [15].
Our work is fully-scalable in the sense that the local memory of each machine
is $O(n^\epsilon)$ for any constant $0 &lt; \epsilon &lt; 1$. There are no $o(\log
n)$-round MPC algorithms for Min Cut in this memory regime assuming the 1-vs-2
Cycle Conjecture holds. The exponential speedup in AMPC is the result of
decoupling the different layers of the divide and conquer algorithm and solving
all layers in $O(1)$ rounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14101"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.14039">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.14039">Group-invariant max filtering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cahill:Jameson.html">Jameson Cahill</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iverson:Joseph_W=.html">Joseph W. Iverson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mixon:Dustin_G=.html">Dustin G. Mixon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Packer:Daniel.html">Daniel Packer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.14039">PDF</a><br /><b>Abstract: </b>Given a real inner product space $V$ and a group $G$ of linear isometries, we
construct a family of $G$-invariant real-valued functions on $V$ that we call
max filters. In the case where $V=\mathbb{R}^d$ and $G$ is finite, a suitable
max filter bank separates orbits, and is even bilipschitz in the quotient
metric. In the case where $V=L^2(\mathbb{R}^d)$ and $G$ is the group of
translation operators, a max filter exhibits stability to diffeomorphic
distortion like that of the scattering transform introduced by Mallat. We
establish that max filters are well suited for various classification tasks,
both in theory and in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.14039"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.13983">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.13983">Quantum Augmented Dual Attack</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albrecht:Martin_R=.html">Martin R. Albrecht</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Yixin.html">Yixin Shen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.13983">PDF</a><br /><b>Abstract: </b>We present a quantum augmented variant of the dual lattice attack on the
Learning with Errors (LWE) problem, using classical memory with quantum random
access (QRACM). Applying our results to lattice parameters from the literature,
we find that our algorithm outperforms previous algorithms, assuming unit cost
access to a QRACM. On a technical level, we show how to obtain a quantum
speedup on the search for Fast Fourier Transform (FFT) coefficients above a
given threshold by leveraging the relative sparseness of the FFT and using
quantum amplitude estimation. We also discuss the applicability of the Quantum
Fourier Transform in this context. Furthermore, we give a more rigorous
analysis of the classical and quantum expected complexity of guessing part of
the secret vector where coefficients follow a discrete Gaussian (mod \(q\)).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.13983"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.13919">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.13919">Fast Causal Orientation Learning in Directed Acyclic Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ramin Safaeian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salehkaleybar:Saber.html">Saber Salehkaleybar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabandeh:Mahmoud.html">Mahmoud Tabandeh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.13919">PDF</a><br /><b>Abstract: </b>Causal relationships among a set of variables are commonly represented by a
directed acyclic graph. The orientations of some edges in the causal DAG can be
discovered from observational/interventional data. Further edges can be
oriented by iteratively applying so-called Meek rules. Inferring edges'
orientations from some previously oriented edges, which we call Causal
Orientation Learning (COL), is a common problem in various causal discovery
tasks. In these tasks, it is often required to solve multiple COL problems and
therefore applying Meek rules could be time-consuming. Motivated by Meek rules,
we introduce Meek functions that can be utilized in solving COL problems. In
particular, we show that these functions have some desirable properties,
enabling us to speed up the process of applying Meek rules. In particular, we
propose a dynamic programming (DP) based method to apply Meek functions.
Moreover, based on the proposed DP method, we present a lower bound on the
number of edges that can be oriented as a result of intervention. We also
propose a method to check whether some oriented edges belong to a causal DAG.
Experimental results show that the proposed methods can outperform previous
work in several causal discovery tasks in terms of running-time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.13919"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.13725">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.13725">Low-Degree Polynomials Extract from Local Sources</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alrabiah:Omar.html">Omar Alrabiah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chattopadhyay:Eshan.html">Eshan Chattopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goodman:Jesse.html">Jesse Goodman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xin.html">Xin Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ribeiro:Jo=atilde=o.html">João Ribeiro</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.13725">PDF</a><br /><b>Abstract: </b>We continue a line of work on extracting random bits from weak sources that
are generated by simple processes. We focus on the model of locally samplable
sources, where each bit in the source depends on a small number of (hidden)
uniformly random input bits. Also known as local sources, this model was
introduced by De and Watson (TOCT 2012) and Viola (SICOMP 2014), and is closely
related to sources generated by $\mathsf{AC}^0$ circuits and bounded-width
branching programs. In particular, extractors for local sources also work for
sources generated by these classical computational models.
</p>
<p>Despite being introduced a decade ago, little progress has been made on
improving the entropy requirement for extracting from local sources. The
current best explicit extractors require entropy $n^{1/2}$, and follow via a
reduction to affine extractors. To start, we prove a barrier showing that one
cannot hope to improve this entropy requirement via a black-box reduction of
this form. In particular, new techniques are needed.
</p>
<p>In our main result, we seek to answer whether low-degree polynomials (over
$\mathbb{F}_2$) hold potential for breaking this barrier. We answer this
question in the positive, and fully characterize the power of low-degree
polynomials as extractors for local sources. More precisely, we show that a
random degree $r$ polynomial is a low-error extractor for $n$-bit local sources
with min-entropy $\Omega(r(n\log n)^{1/r})$, and we show that this is tight.
</p>
<p>Our result leverages several new ingredients, which may be of independent
interest. Our existential result relies on a new reduction from local sources
to a more structured family, known as local non-oblivious bit-fixing sources.
To show its tightness, we prove a "local version" of a structural result by
Cohen and Tal (RANDOM 2015), which relies on a new "low-weight"
Chevalley-Warning theorem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.13725"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.13598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.13598">Multiwinner Elections under Minimax Chamberlin-Courant Rule in Euclidean Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sonar:Chinmay.html">Chinmay Sonar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suri:Subhash.html">Subhash Suri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Jie.html">Jie Xue</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.13598">PDF</a><br /><b>Abstract: </b>We consider multiwinner elections in Euclidean space using the minimax
Chamberlin-Courant rule. In this setting, voters and candidates are embedded in
a $d$-dimensional Euclidean space, and the goal is to choose a committee of $k$
candidates so that the rank of any voter's most preferred candidate in the
committee is minimized. (The problem is also equivalent to the ordinal version
of the classical $k$-center problem.) We show that the problem is NP-hard in
any dimension $d \geq 2$, and also provably hard to approximate. Our main
results are three polynomial-time approximation schemes, each of which finds a
committee with provably good minimax score. In all cases, we show that our
approximation bounds are tight or close to tight. We mainly focus on the
$1$-Borda rule but some of our results also hold for the more general
$r$-Borda.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.13598"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2205.13596">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2205.13596">A Simplified Treatment of Ramana's Exact Dual for Semidefinite Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Louren=ccedil=o:Bruno_F=.html">Bruno F. Lourenço</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pataki:G=aacute=bor.html">Gábor Pataki</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2205.13596">PDF</a><br /><b>Abstract: </b>In semidefinite programming the dual may fail to attain its optimal value and
there could be a duality gap, i.e., the primal and dual optimal values may
differ. In a striking paper, Ramana proposed a polynomial size extended dual
that does not have these deficiencies and yields a number of fundamental
results in complexity theory. In this work we walk the reader through a concise
and self-contained derivation of Ramana's dual, relying mostly on elementary
linear algebra.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2205.13596"><span class="datestr">at May 30, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/30/easy-as-abc/">Easy as ABC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<i>A modern mathematical proof is not very different from a modern machine: the simple fundamental principles are hidden and almost invisible under a mass of technical details—Hermann Weyl.</i></font></p><font color="#0044cc">
<p>
<font color="#000000"></font></p><font color="#000000">
<p>
Shinichi Mochizuki is a mathematician who is at the center of a decade old claim. He has—he says since 2012—solved a famous open problem in number theory called the <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> conjecture. This conjecture is in number theory and would revolutionize our understanding of the structure of the natural numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/sm-3/"><img width="400" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/sm-1.jpg?resize=400%2C267&amp;ssl=1" class="aligncenter wp-image-20108" height="267" /></a></p>
<p>
</p><p><b> What is the ABC? </b></p>
<p></p><p>
The <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> conjecture is a conjecture in number theory, first proposed by Joseph Oesterle and David Masser independently around 1985. It is stated in terms of three positive integers, <img src="https://s0.wp.com/latex.php?latex=%7Ba%2C+b%2C+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a, b, c}" class="latex" /> (hence the name) that are relatively prime and satisfy <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b = c}" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> denotes the product of the distinct prime factors of <img src="https://s0.wp.com/latex.php?latex=%7Babc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{abc}" class="latex" />, the conjecture essentially states that <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> is usually not much smaller than <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>
The common term for the product <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> of the distinct prime factors of a positive integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is the “radical” of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, written <img src="https://s0.wp.com/latex.php?latex=%7Brad%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{rad(n)}" class="latex" /> by Wikipedia, in Timothy Gowers’s <em>Princeton Companion</em> <a href="https://books.google.com/books?id=ZOfUsvemJDMC&amp;pg=PA681#v=onepage&amp;q&amp;f=false">article</a>, and in other sources in abc. We demur from doubling up on an established term with a different meaning and suggest calling it the “wingspan” <img src="https://s0.wp.com/latex.php?latex=%7Bw%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w(n)}" class="latex" />. Then square-free integers have the largest possible wingspan, while large prime powers minimize it. Now we can state the conjecture formally:</p>
<blockquote><p>
For every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 0}" class="latex" />, all but finitely many triples of relatively prime positive numbers giving <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+c%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b = c}" class="latex" /> have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w%28abc%29+%3E+c%5E%7B1-%5Cepsilon%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  w(abc) &gt; c^{1-\epsilon}. " class="latex" /></p>
</blockquote>
<p>
Intuitively what it says is that numbers that have large powers of different primes cannot be related by addition. As <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon \rightarrow 0}" class="latex" />, it says that the wingspan of the triple’s product <img src="https://s0.wp.com/latex.php?latex=%7Bn%3Dabc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=abc}" class="latex" /> must at least approach <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />, which is greater than the cube root of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. Note, incidentally, that if any two of <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a,b,c}" class="latex" /> share a prime factor <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> then so does the third, so we can divide out all such <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> to get <img src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%2Cc%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a',b',c'}" class="latex" /> meeting the hypothesis. </p>
<p>
The point of the conjecture is that it relates addition and multiplication. It allows making inferences about the multiplicative structure of natural numbers from additive properties and vice-versa. The formal theory of the natural numbers with respect to addition alone, called <a href="https://en.wikipedia.org/wiki/Presburger_arithmetic">Presburger arithmetic</a>, is decidable, as is the theory of multiplication alone, called <a href="https://en.wikipedia.org/wiki/Skolem_arithmetic">Skolem arithmetic</a>. The theory of both <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" />, <a href="https://en.wikipedia.org/wiki/Peano_axioms">Peano arithmetic</a>, is of course undecidable. But <a href="https://en.wikipedia.org/wiki/Abc_conjecture">abc</a> gives a playbook for leveraging the decidable sub-systems.</p>
<p>
</p><p><b> ABC Implies What? </b></p>
<p></p><p>
The conjecture has high <em>explanatory power</em> in that many other conjectures (listed <a href="https://en.wikipedia.org/wiki/Abc_conjecture#Some_consequences">here</a>) follow from it. Among them, we note:</p>
<ol>
<li> Whereas no one has significantly simplified Andrew Wiles’s famously difficult proof strategy for Fermat’s Last Theorem (FLT), the theorem for <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \ge 6}" class="latex" /> follows quickly from a weaker analogue of the abc conjecture.
</li><li> A generalization of FLT concerning powers that are sums of powers, called the Fermat-Catalan conjecture, also follows from abc.
</li><li> If a polynomial <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x)}" class="latex" /> with integer coefficients has at least three simple zeroes, then there are only finitely many positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x)}" class="latex" /> is a perfect power (i.e., such that <img src="https://s0.wp.com/latex.php?latex=%7BP%28x%29+%3D+m%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P(x) = m^k}" class="latex" /> for some integers <img src="https://s0.wp.com/latex.php?latex=%7Bm%2Ck+%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m,k \geq 2}" class="latex" />).
</li></ol>
<p>
This raises the following natural question for us computational complexity theorists:</p>
<blockquote><p>
 Does the abc conjecture imply any “shocks” in complexity theory—namely, resolving basic open questions that have been open for over half a century?
</p></blockquote>
<p>
Ken and I are not aware of any. It does not seem to affect factoring, or complexity theory, or any main ingredients of our favorite P=NP problem. But it does have great impact on anything that concerns Diophantine questions. It is possible that connections may emerge at this level of detail. </p>
<p>
</p><p><b> Is The ABC Proved? </b></p>
<p></p><p>
Here is a timeline of Mochizuki <a href="https://www.scientificamerican.com/article/math-mystery-shinichi-mochizuki-and-the-impenetrable-proof/">proof</a>: </p>
<p>
Mochizuki made his work public in August 2012 without any fanfare. Soon it was picked up and the mathematical community was made aware of the claim he has proven the abc conjecture. This started the quest to determine if his proof is correct.</p>
<p>
The proof is long and complex. Workshops were held in 2015 and 2016 on it. The presentations did not lead to acceptance of Mochizuki’s ideas, and the proof remains unclear.</p>
<p>
Enter Peter Scholze and Jakob Stix—two world experts on number theory. They visited Kyoto University for five days of discussions with Mochizuki in 2018. It did not resolve the correctness of the proof but did bring into focus where the difficulties lay.</p>
<p>
They wrote a report <a href="https://ncatlab.org/nlab/files/why_abc_is_still_a_conjecture.pdf">Why abc is still a conjecture</a>. It starts:<br />
<em><br />
We, the authors of this note, came to the conclusion that there is no proof. We are going to explain where, in our opinion, the suggested proof has a problem, a problem so severe that in our opinion small modifications will not rescue the proof strategy.<br />
</em></p>
<p>
Then, Mochizuki wrote a response of his view of why their claims were wrong: <a href="https://www.kurims.kyoto-u.ac.jp/~motizuki/Cmt2018-05.pdf">Comments On The Manuscript by Scholze-Stix</a>. He said:<br />
<em><br />
It should be stated clearly that the assertion that “these are inessential to the point we are making” is completely false! I made numerous attempts to explain this during the March discussions, and it is most unfortunate that we were ultimately unable to communicate regarding this issue.<br />
</em></p>
<p>
The disagreement over the correctness remains:  Other authors have pointed to the unresolved dispute between Mochizuki and Scholze over the correctness of this work as an instance in which the peer review process of mathematical journal publication has failed in its usual function of convincing the mathematical community as a whole of the validity of a result. </p>
<p>
</p><p><b> Explaining Math </b></p>
<p></p><p>
Albert Einstein may have said:</p>
<blockquote><p>
“If you can’t explain it to a six year old, you don’t understand it yourself.”
</p></blockquote>
<p>
Some attribute this instead to Richard Feynman. But whoever said it the mathematical community generally agrees with the point. In the 1962 <a href="https://www.biblio.com/book/new-perspectives-physics-broglie-louis-translated/d/999571270">book</a> New Perspectives in Physics, by Louis De Broglie, states that Einstein, when discussing theories, said:</p>
<blockquote><p>
“<img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> ought to lend themselves to as simple a description as that even a child could understand…”
</p></blockquote>
<p>
I wonder if the requirement “explain it to a six year old” does not mean the six year old must understand it. Rather that when you explain it to them they listen politely. That is the requirement is they listen. What do you think?</p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Mochizuki still claims his <a href="https://www.nature.com/articles/nature.2012.11378">proof</a>. He violates the above rule: he cannot explain it to a six year old—not even a senior expert. It still has not yet been accepted as passing the peer review stage. See <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">this</a> for some general comments. And <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12220">this</a> for some more. It has lots of comments.</p>
<p>
Can the abc ever be resolved? I wonder if there is some way to say suppose that the abc is true. And then prove some surprising complexity consequence holds? Perhaps violate P=NP for example, or some other result. </p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/30/easy-as-abc/"><span class="datestr">at May 30, 2022 08:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-5678234482420567140">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2022/05/orna-kupfermans-interview-with-christel.html">Orna Kupferman's Interview with Christel Baier, Holger Hermanns and Joost-Pieter Katoen, CONCUR 2022 ToT Award Recipients</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i><span class="im">I am delighted to post <a href="https://www.cs.huji.ac.il/~ornak/" target="_blank">Orna Kupferman</a>'s interview with <a href="https://concur2022.mimuw.edu.pl/tot-award/" target="_blank">CONCUR 2022 Test-of-Time Award</a> recipients <a href="https://wwwtcs.inf.tu-dresden.de/~baier/" target="_blank">Christel Baier</a>, <a href="https://depend.cs.uni-saarland.de/~hermanns/" target="_blank">Holger Hermanns</a> and<a href="https://www-i2.informatik.rwth-aachen.de/~katoen/" target="_blank"> Joost-Pieter Katoen</a>. </span></i></p><p><span class="im"><i><br />Thanks to Christel, Holger and Joost-Pieter for their answers (labelled BHK in what follows) and to Orna (Q below) for conducting the interview. Enjoy and watch this space for upcoming interviews with the other award recipients!</i>  <br /></span></p><p><span class="im">Q: You receive the CONCUR Test-of-Time Award 2022 for your paper "<a href="https://link.springer.com/content/pdf/10.1007/3-540-48320-9_12.pdf" target="_blank">Approximate symbolic model checking of continuous-time Markov chains</a>," which appeared <br />at CONCUR 1998. In that article, you combine three different challenges: symbolic algorithms, real-time systems, and probabilistic systems. Could you briefly explain to our readers what the main challenge in such a combination is?<br /><br /></span>BHK: The main challenge is to provide a fixed-point characterization of time-bounded reachability probabilities: the probability to reach a given target state within a given deadline. Almost all works in the field up to 1999 treated discrete-time probabilistic models and focused on "just" reachability probabilities: what is the probability to eventually end up in a given target state? This can be characterized as a unique solution of a linear equation system. The question at stake was: how to incorporate a real-valued deadline <i>d</i>? The main insight was <br />to split the problem in staying a certain amount of time, <i>x</i> say, in the current state and using the remaining <i>d-x</i> time to reach the target from its successor state. This yields a <a href="https://en.wikipedia.org/wiki/Volterra_integral_equation" target="_blank">Volterra integral equation system</a>; indeed time-bounded reachability probabilities are unique solutions of such equation systems. In the CONCUR'99 paper we suggested to use symbolic data structures to do the numerical integration; later we found out that much more efficient techniques can be applied.<span class="im"><br /><br />Q: Could you tell us how you started your collaboration on the award-winning paper? In particular, as the paper combines three different challenges, is it the case that each of you has brought to the research different expertise?<br /><br /></span>BHK: Christel and Joost-Pieter were both in Birmingham, where a meeting of a <br />collaboration project between German and British research groups on stochastic systems and process algebra took place. There the first ideas of model checking continuous-time Markov chains arose, especially for time-bounded reachability: with stochastic process algebras there were means to model CTMCs in a compositional manner, but verification was lacking. Back in Germany, Holger suggested to include a steady-state operator, the counterpart of transient properties that can be expressed using timed reachability probabilities. We then also developed the symbolic data structure to support the verification of the entire logic.<span class="im"></span><span class="im"><br /><br />Q: Your contribution included a generalization of <a href="https://en.wikipedia.org/wiki/Binary_decision_diagram" target="_blank">BDDs (binary decision diagrams)</a> to <a href="https://www.cs.cmu.edu/~emc/papers/Contributions%20to%20Edited%20Volumes/Multi-Terminal%20Binary%20Decision%20Diagrams%20and%20Hybrid%20Decision%20Diagrams.pdf" target="_blank">MTDDs (multi-terminal decision diagrams)</a>, which allow both Boolean and real-valued variables. What do you think about the current state of symbolic algorithms, in particular the choice between SAT-based methods and methods that are based on decision diagrams?<br /><br /></span>BHK: BDD-based techniques entered probabilistic model checking in the mid 1990's for discrete-time models such as Markov chains. Our paper was one of the first, perhaps even the first, that proposed to use BDD structures for real-time stochastic processes. Nowadays, SAT, and in particular SMT-based techniques belong to the standard machinery in probabilistic model checking. SMT techniques are e.g., used in bisimulation minimization at the language level, counterexample generation, and parameter synthesis. This includes both linear as well as non-linear theories. BDD techniques are still used, mostly in combination with sparse representations, but it is fair to say that SMT is becoming more and more relevant.<span class="im"><br /><br />Q: What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved?<br /><br /></span>BHK: This depends a bit on whom you ask! Christel's recent work is about cause-effect reasoning and notions of responsibility in the verification context. This ties into the research interest of Holger who looks at the foundations of perspicuous software systems. This research is rooted in the observation that the explosion of opportunities for software-driven innovations comes with an implosion of human opportunities and capabilities to understand and control these innovations. Joost-Pieter focuses on pushing the borders of automation in weakest-precondition reasoning of probabilistic programs. This involves loop invariant synthesis, probabilistic termination proofs, the development of deductive verifiers, and so forth. Challenges are to come up with good techniques for synthesizing quantitative loop invariants, or even complete probabilistic programs.<span class="im"><br /><br />Q: What advice would you give to a young researcher who is keen to start working on topics related to symbolic algorithms, real-time systems, and probabilistic systems?<br /><br /></span>BHK: Try to keep it smart and simple.</p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2022/05/orna-kupfermans-interview-with-christel.html"><span class="datestr">at May 30, 2022 03:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1695433635483317846">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/05/discussions-i-wish-we-were-having.html">Discussions I wish we were having</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>1) Democrats think the best way to avoid school shootings (and other problems with guns) is to have regulations on Guns. They have proposed legislation. The Republicans think its a mental health issue. They have proposed legislation for this. NO THEY HAVEN"T. I would respect the <i>its a mental health</i> <i>issue</i> argument if the people saying this  respected it. They do not. See <a href="https://www.kvue.com/article/news/local/report-texas-last-access-mental-health-care/269-1283fcee-0dc4-4948-9dda-886f55c49ca3">here</a>. Idea: Politico should leak a (false) memo  by Gov Abbott where he says </p><p><i>We have a serious mental health crisis in Texas which caused the recent event. I am not just saying this to deflect from the gun issue. I have drawn up a bill to fund mental health care, providing more money, for care and for studies. I call on Republicans and Democrats to pass it ASAP.</i></p><p>I wonder- if this false memo was leaked, would he deny it and say </p><p><i>I didn't write that. I am using mental health only as a way to deflect from the gun issue. How dare they say that I am reasonable and am proposing actual solutions. </i></p><p>Or would he be forced to follow through?</p><p>2) Democrats think Biden won the election. Some Republicans think Trump won the election. One issue was Arizona. So some republicans organized a recount of Arizona. And when they found out that Biden really did win it they said, as the good Popperian scientists they are, <i>we had a falsifiable hypothesis and it</i> <i>was shown to be false, so now we acknowledge the original hypothesis was wrong. </i>NO THEY DIDN"T. They seem to point to the Arizona audit as proof that they were right, even though it proves the opposite. (Same for all the court cases they lost.)</p><p>3) At one time I read some books that challenged evolution (Darwin on Trial by Phillip Johnson was one of them). Some of them DID raise some good points about how science is done (I am NOT being sarcastic). Some of them DID raise some questions like the gap in the fossil record and Michael Behe's notion of irreducible complexity.  (In hindsight these were window dressing and not what they cared about.) MY thought at the time was <i>its good to have people</i> v<i>iew a branch of science with a different viewpoint. Perhaps the scientists at the Discovery Institute will find something interesting. </i>(The Discovery institute is a think tank and one of their interests is Int. Design.) Alas, the ID people seem to spend their time either challenging the teaching of Evolution in school OR doing really bad science. Could intelligent people who think Evolution is not correct look at it in a different way than scientists do, and do good science, or at least raise good questions,  and come up with something interesting? I used to think so. Now I am not so sure.</p><p>4) I wish the debate was <i>what to do about global warming </i>and not <i>is global warming happening? </i>Conjecture: there will come a time when environmentalists finally come around to nuclear power being part of the answer. At that point, Republicans will be against Nuclear power just because the Democrats are for it. </p><p>5) I sometimes get email discussions like the following (I will call the emailer Mel for no good reason.)</p><p>------------------------------------------------------</p><p>MEL: Dr. Gasarch, I have shown that R(5)=45.</p><p>BILL: Great! Can you email me your 2-coloring of K_{44} that has no mono K_5?</p><p>MEL: You are just being stubborn. Look at my proof!</p><p>------------------------------------------------------------</p><p>Clyde has asked me <i>what if Mel had a nonconstructive proof?</i></p><p>FINE- then MEL can tell me that. But Mel doesn't know math well enough to make that</p><p>kind of argument. Here is the discussion I wish we had</p><p>-------------------------------------------</p><p>MEL: Dr. Gasarch, I have shown that R(5)=45.</p><p>BILL: Great! Can you email me your coloring of K_{44} that has no mono K_5?</p><p>MEL: The proof is non-constructive.</p><p>BILL: Is it a probabilistic proof? If so then often the prob is not just nonzero but close to 1. Perhaps you could write a program that does the coin flipping and finds the coloring.</p><p>MEL: The proof uses the Local Lovasz Lemma so the probe is not close to 1.</p><p>BILL: Even so, that can be coded up.</p><p>MEL: Yes but... (AND THIS IS AN INTELLIGENT CONVERSATION)</p><p>----------------------------------</p><p>Maybe Mel really did prove R(5)=44, or maybe not, but the above conversation would lead to</p><p>enlightenment. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/05/discussions-i-wish-we-were-having.html"><span class="datestr">at May 30, 2022 05:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6444">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6444">An understandable failing?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I hereby precommit that this will be my last post, for a long time, around the twin themes of (1) the horribleness in the United States and the world, and (2) my desperate attempts to reason with various online commenters who hold me personally complicit in all this horribleness.  I should really focus my creativity more on <em>actually fixing</em> the world’s horribleness, than on seeking out every random social-media mudslinger who blames me for it, shouldn’t I?  Still, though, isn’t undue obsession with the latter a pretty ordinary human failing, a pretty understandable one?</p>



<p>So anyway, if you’re one of the thousands of readers who come here simply to learn more about quantum computing and computational complexity, rather than to try to provoke me into mounting a public defense of my own existence (which defense will then, ironically but inevitably, stimulate <em>even more</em> attacks that need to be defended against) … well, either scroll down to the very end of this post, or wait for the next post.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Thanks so much to all my readers who donated to <a href="https://fundtexaschoice.org/">Fund Texas Choice</a>.  As promised, I’ve personally given them a total of $4,106.28, to match the donations that came in by the deadline.  I’d encourage people to continue donating anyway, while for my part I’ll probably run some more charity matching campaigns soon.  These things are addictive, like pulling the lever of a slot machine, but where the rewards go to making the world an infinitesimal amount more consistent with your values.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Of course, now there’s a brand-new atrocity to shame my adopted state of Texas before the world.  While the Texas government will go to extraordinary lengths to protect unborn children, the world has now witnessed 19 of its<em>born</em> children consigned to gruesome deaths, as the “good guys with guns”—waited outside and prevented parents from entering the classrooms where their children were being shot.  I have nothing original to add to the global outpourings of rage and grief.  Forget about the statistical frequency of these events: I know perfectly well that the risk from car crashes and home accidents is orders-of-magnitude greater.  Think about it this way: the United States is now known to the world as “the country that can’t or won’t do anything to stop its children from semi-regularly being gunned down in classrooms,” not even measures that virtually every other comparable country on earth has successfully taken.  It’s become <em>the</em> symbol of national decline, dysfunction, and failure.  If so, then the stakes here could fairly be called existential ones—not because of its <em>direct</em> effects on child life expectancy or GDP or any other index of collective well-being that you can define and measure, but rather, because a country that lacks the will to solve this will be judged by the world, and probably accurately, as lacking the will to solve anything else.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>In return for the untold thousands of hours I’ve poured into this blog, which has never once had advertising or asked for subscriptions, my reward has been years of vilification by sneerers and trolls.  Some of the haters even compare me to Elliot Rodger and other aggrieved mass shooters.  And I mean: yes, it’s <em>true</em> that I was bullied and miserable for years.  It’s true that Elliot Rodger, Salvador Ramos (the Uvalde shooter), and most other mass shooters were also bullied and miserable for years.  But, Scott-haters, if we’re being intellectually honest about this, we might say that the similarities between the mass shooter story and the Scott Aaronson story end at a certain point not very long after that.  We might say: it’s not just that Aaronson didn’t respond by hurting anybody—rather, it’s that his response loudly <em>affirmed</em> the values of the Enlightenment, meaning like, the whole package, from individual autonomy to science and reason to the rejection of sexism and racism to everything in between.  Affirmed it in a manner that’s not secretly about popularity (demonstrably so, because it doesn’t <em>get</em> popularity), affirmed it via self-questioning methods intellectually honest enough that they’d probably <em>still</em> have converged on the right answer even in situations where it’s now obvious that almost everyone you around would’ve been converging on the <em>wrong</em> answer, like (say) Nazi Germany or the antebellum South.</p>



<p>I’ve been to the valley of darkness.  While there, I decided that the only “revenge” against the bullies that was possible or desirable was to <em>do</em> something with my life, to achieve something in science that at least some bullies might envy, while also starting a loving family and giving more than most to help strangers on the Internet and whatever good cause comes to his attention and so on.  And after 25 years of effort, some people might say I’ve sort of <em>achieved</em> the “revenge” as I’d then defined it.  And they might further say: if you could get every school shooter to redefine “revenge” as “becoming another Scott Aaronson,” that would be, you know, like, a step upwards.  An improvement.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>And let this be the final word on the matter that I ever utter in all my days, to the thousands of SneerClubbers and Twitter randos who pursue this particular line of attack against Scott Aaronson (yes, we do mean the <em>thousands</em>—which means, it both <em>feels to its recipient like</em> the entire earth yet <em>actually is</em> less than 0.01% of the earth). </p>



<p><strong>We see what <em>Scott</em> did with <em>his</em> life, when subjected for a decade to forms of psychological pressure that are infamous for causing young males to lash out violently.  What would <em>you</em> have done with <em>your</em> life?</strong></p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>A couple weeks ago, when the trolling attacks were arriving minute by minute, I toyed with the idea of permanently shutting down this blog.  <em>What’s the point?</em> I asked myself.  <em>Back in 2005, the open Internet was fun; now it’s a charred battle zone.  Why not restrict conversation to my academic colleagues and friends?  Haven’t I done enough for a public that gives me so much grief?</em>  I was dissuaded by many messages of support from loyal readers.  Thank you so much.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>If anyone needs something to cheer them up, you should really watch <a href="https://tv.apple.com/us/show/prehistoric-planet/umc.cmc.4lh4bmztauvkooqz400akxav?ign-itscg=MC_20000&amp;ign-itsct=atvp_brand_omd&amp;mttn3pid=Google%20AdWords&amp;mttnagencyid=a5e&amp;mttncc=US&amp;mttnsiteid=143238&amp;mttnsubad=OUS2019950_1-600303192626-c&amp;mttnsubkw=144248666108__J1lRfctc_&amp;mttnsubplmnt=">Prehistoric Planet</a>, narrated by an excellent, 96-year-old David Attenborough.  Maybe 35 years from now, people will believe dinosaurs looked or acted somewhat differently from these portrayals, just like they believe somewhat differently now from when I was a kid.  On the other hand, if you literally took a time machine to the Late Cretaceous and starting filming, you couldn’t get a result that <em>seemed</em> more realistic, let’s say to a documentary-watching child, than these CGI dinosaurs on their CGI planet seem.  So, in the sense of passing that child’s Turing Test, you might argue, the problem of bringing back the dinosaurs has now been solved.</p>



<p>If you … err … <em>really</em> want to be cheered up, you can follow up with <a href="https://www.pbs.org/wgbh/nova/series/dinosaur-apocalypse/">Dinosaur Apocalypse</a>, <em>also</em> narrated by Attenborough, where you can (again, as if you were there) watch the dinosaurs being drowned and burned alive in their billions when the asteroid hits.  We’d still be scurrying under rocks, were it not for that lucky event that only a monster could’ve called lucky at the time.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Several people asked me to comment on the recent <a href="https://scorpioncapital.s3.us-east-2.amazonaws.com/reports/IONQ.pdf">savage investor review</a> against the quantum computing startup IonQ.  The review amusingly mixed together every imaginable line of criticism, with every imaginable degree of reasonableness from 0% to 100%.  Like, quantum computing is impossible even in theory, <em>and</em> (in the very next sentence) other companies are much closer to realizing quantum computing than IonQ is.  And <a href="https://ionq.com/posts/may-12-2022-ionq-founders-respond">IonQ’s response</a> to the criticism, and see also <a href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/">this</a> by the indefatigable Gil Kalai.</p>



<p>Is it, err, OK if I sit this one out for now?  There’s probably, like, actually an <em>already-existing</em> machine learning model where, if you trained it on all of my previous quantum computing posts, it would know exactly what to say about this.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6444"><span class="datestr">at May 29, 2022 06:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/082">TR22-082 |  Low-Degree Polynomials Extract from Local Sources | 

	Omar Alrabiah, 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	Xin Li, 

	João Ribeiro</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue a line of work on extracting random bits from weak sources that are generated by simple processes. We focus on the model of locally samplable sources, where each bit in the source depends on a small number of (hidden) uniformly random input bits. Also known as local sources, this model was introduced by De and Watson (TOCT 2012) and Viola (SICOMP 2014), and is closely related to sources generated by AC$^0$ circuits and bounded-width branching programs. In particular, extractors for local sources also work for sources generated by these classical computational models.

Despite being introduced a decade ago, little progress has been made on improving the entropy requirement for extracting from local sources. The current best explicit extractors require entropy $n^{1/2}$, and follow via a reduction to affine extractors. To start, we prove a barrier showing that one cannot hope to improve this entropy requirement via a black-box reduction of this form. In particular, new techniques are needed.

In our main result, we seek to answer whether low-degree polynomials (over $\mathbb{F}_2$) hold potential for breaking this barrier. We answer this question in the positive, and fully characterize the power of low-degree polynomials as extractors for local sources.
More precisely, we show that a random degree $r$ polynomial is a low-error extractor for $n$-bit local sources with min-entropy $\Omega(r(n\log n)^{1/r})$, and we show that this is tight.

Our result leverages several new ingredients, which may be of independent interest. Our existential result relies on a new reduction from local sources to a more structured family, known as local non-oblivious bit-fixing sources. To show its tightness, we prove a ``local version'' of a structural result by Cohen and Tal (RANDOM 2015), which relies on a new ``low-weight'' Chevalley-Warning theorem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/082"><span class="datestr">at May 28, 2022 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20092">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/">CCC 2022 Conference</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>I have a private plane. But I fly commercial when I go to environmental conferences—Arnold Schwarzenegger</i></p>
<p>
Computational Complexity Conference <a href="https://computationalcomplexity.org">CCC</a> is about to happen:  </p>
<p><i>July 20—23, Philadelphia, PA, USA</i>  </p>
<p>It is an annual conference on the inherent difficulty of computational problems in terms of the resources they require.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/ccc/" rel="attachment wp-att-20094"><img width="600" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/ccc.png?resize=600%2C490&amp;ssl=1" class="aligncenter size-full wp-image-20094" height="490" /></a> </p>
<p>
Ryan Williams just sent out a request on behalf of CCC:  The travel allowances for CCC are available for students from US universities. The deadline for applying is June 8. Here is the <a href="https://computationalcomplexity.org/travelAllowance/travelAllowance2022.php">link</a>. </p>
<p>
</p><p><b> Accepted Papers </b></p>
<p></p><p>
Here is a list of the accepted papers with pointers to versions of the papers:</p>
<ul>
<li> Gal Beniamini. <a href="https://arxiv.org/abs/2004.14318">The Approximate Degree of Bipartite Perfect Matching </a>
</li><li> Till Tantau. <a href="https://arxiv.org/abs/2201.08895">On the Satisfaction Probability of k-CNF Formulas</a>
</li><li> Andrej Bogdanov, William Hoza, Gautam Prakriya and Edward Pyne. <a href="https://eccc.weizmann.ac.il/report/2021/143/">Hitting Sets for Regular Branching Programs</a>
</li><li> Svyatoslav Gryaznov, Pavel Pudlak and Navid Talebanfard. <a href="https://arxiv.org/abs/2201.10997">Linear Branching Programs and Directional Affine Extractors</a>
</li><li> Sandy Irani, Anand Natarajan, Chinmay Nirkhe, Sujit Rao and Henry Yuen. <a href="https://arxiv.org/abs/2111.02999">Quantum search-to-decision reductions and the state synthesis problem</a>
</li><li> Karthik C. S. and Subhash Khot. <a href="https://arxiv.org/abs/2112.03983">Almost Polynomial Factor Inapproximability for Parameterized k-Clique</a>
</li><li> Venkatesan Guruswami, Peter Manohar and Jonathan Mosheiff. <a href="https://arxiv.org/abs/2205.06738">l_p-Spread and Restricted Isometry Properties of Sparse Random Matrices</a>
</li><li> Ian Mertz and James Cook. <a href="https://eccc.weizmann.ac.il/report/2022/026/">Trading Time and Space in Catalytic Branching Programs</a>
</li><li> Harm Derksen, Visu Makam and Jeroen Zuiddam. <a href="https://staff.fnwi.uva.nl/j.zuiddam/">Subrank and Optimal Reduction of Scalar Multiplications to Generic Tensors</a>
</li><li> Guy Blanc and Dean Doron. <a href="https://eccc.weizmann.ac.il/report/2022/027/">New Near-Linear Time Decodable Codes Closer to the GV Bound</a>
</li><li> Jun-Ting Hsieh, Sidhanth Mohanty and Jeff Xu. <a href="https://arxiv.org/abs/2106.12710">Certifying solution geometry in random CSPs: counts, clusters and balance</a>
</li><li> Vikraman Arvind and Pushkar Joglekar. <a href="https://arxiv.org/abs/2202.09883">On Efficient Noncommutative Polynomial Factorization via Higman Linearization</a>
</li><li> Ivan Mihajlin and Anastasia Sofronova. <a href="https://eccc.weizmann.ac.il/report/2022/033/">A better-than-3log(n) depth lower bound for De Morgan formulas with restrictions on top gates</a>
</li><li> Dan Karliner, Roie Salama and Amnon Ta-Shma. <a href="https://eccc.weizmann.ac.il/report/2022/028/">The plane test is a local tester for Multiplicity Codes</a>
</li><li> Dmitry Sokolov. <a href="https://eccc.weizmann.ac.il/report/2021/076/">Pseudorandom Generators, Resolution and Heavy Width</a>
</li><li> Halley Goldberg, Valentine Kabanets, Zhenjian Lu and Igor C. Oliveira. <a href="https://eccc.weizmann.ac.il/report/2022/072/">Probabilistic Kolmogorov Complexity with Applications to Average-Case Complexity</a>
</li><li> Erfan Khaniki. <a href="https://eccc.weizmann.ac.il/report/2022/023/">Nisan–Wigderson generators in Proof Complexity: New lower bounds</a>
</li><li> Ryan O’Donnell and Kevin Pratt. <a href="https://arxiv.org/abs/2203.03705">High-Dimensional Expanders from Chevalley Groups</a>
</li><li> Victor Lecomte, Prasanna Ramakrishnan and Li-Yang Tan. <a href="https://arxiv.org/abs/2205.02374">The composition complexity of majority</a>
</li><li> Scott Aaronson, Devon Ingram and William Kretschmer. <a href="https://arxiv.org/abs/2111.10409">The Acrobatics of BQP</a>
</li><li> Zander Kelley and Raghu Meka. <a href="https://arxiv.org/abs/2103.14134">Random restrictions and PRGs for PTFs in Gaussian Space</a>
</li><li> Amol Aggarwal and Josh Alman. <a href="https://arxiv.org/abs/2205.06249">Optimal-Degree Polynomial Approximations for Exponentials and Gaussian Kernel Density Estimation</a>
</li><li> Lijie Chen, Jiatu Li and Tianqi Yang. <a href="https://tianqiyang.org">Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs</a>
</li><li> Pavel Hubacek. <a href="https://arxiv.org/abs/1902.10337">Snakes on Ladders: On the Complexity of the Parity Argument on Directed Cycles</a>
</li><li> Gal Arnon, Alessandro Chiesa and Eylon Yogev. <a href="https://eprint.iacr.org/2022/168">Hardness of Approximation for Stochastic Problems via Interactive Oracle Proofs</a>
</li><li> Shuichi Hirahara and Mikito Nanashima. <a href="https://simons.berkeley.edu/people/shuichi-hirahara">Finding Errorless Pessiland in Error-Prone Heuristica</a>
</li><li> Shuichi Hirahara. <a href="https://simons.berkeley.edu/people/shuichi-hirahara">Symmetry of Information from Meta-Complexity</a>
</li><li> Louis Golowich and Salil Vadhan. <a href="https://eccc.weizmann.ac.il/report/2022/024/">Pseudorandomness of Expander Random Walks for Symmetric Functions and Permutation Branching Programs</a>
</li><li> Nikhil Bansal, Makrand Sinha and Ronald de Wolf. <a href="https://arxiv.org/abs/2203.00212">Influence in Completely Bounded Block-multilinear Forms and Classical Simulation of Quantum Algorithms</a>
</li><li> Michael Saks and Rahul Santhanam. <a href="https://eccc.weizmann.ac.il/report/2022/074/">On Randomized Reductions to the Random Strings</a>
</li><li> Sarah Bordage, Mathieu Lhotel, Jade Nardi and Hugues Randriam. <a href="https://arxiv.org/abs/2011.04295">Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes</a>
</li><li> Siddharth Bhandari, Prahladh Harsha, Ramprasad Saptharishi and Srikanth Srinivasan. <a href="https://arxiv.org/abs/2205.10749">Vanishing Spaces of Random Sets and Applications to Reed-Muller Codes</a>
</li><li> Nutan Limaye, Srikanth Srinivasan and Sebastien Tavenas. <a href="https://eccc.weizmann.ac.il/report/2021/081/">On the Partial Derivative Method Applied to Lopsided Set-Multilinear Polynomials</a>
</li><li> Mika Goos, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere and Ran Tao. <a href="https://arxiv.org/abs/2202.07761">Further Collapses in TFNP</a>
</li><li> Xin Lyu. <a href="https://eccc.weizmann.ac.il/report/2022/021/">Improved Pseudorandom Generators for <img src="https://s0.wp.com/latex.php?latex=%7BAC%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{AC^0}" class="latex" /> Circuits</a>
</li><li> Yanyi Liu and Rafael Pass. <a href="https://www.cs.cornell.edu/information/news/newsitem12262/rafael-pass-and-yanyi-liu-explore-whether-one-way-functions-exist">Characterizing Derandomization Through Fine-Grained Hardness of Levin-Kolmogorov Complexity</a>
</li><li> Yanyi Liu and Rafael Pass. <a href="https://eprint.iacr.org/2021/513.pdf">On One-way Functions from NP-Complete Problems</a>
</li><li> Oliver Korten. <a href="https://eccc.weizmann.ac.il/report/2022/025/">Efficient Low-Space Simulations From the Failure of the Weak Pigeonhole Principle</a>
</li><li> Deepanshu Kush and Shubhangi Saraf. <a href="https://arxiv.org/abs/2205.00611">Improved Low-Depth Set-Multilinear Circuit Lower Bounds</a>
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Is this list with pointers helpful? It took a few minutes to form this list—had to add the pointers.</p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/27/ccc-2022-conference/"><span class="datestr">at May 27, 2022 08:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/081">TR22-081 |  Theory and Applications of Probabilistic Kolmogorov Complexity | 

	Zhenjian Lu, 

	Igor Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Diverse applications of Kolmogorov complexity to learning [CIKK16], circuit complexity [OPS19], cryptography [LP20], average-case complexity [Hir21], and proof search [Kra22] have been discovered in recent years. Since the running time of algorithms is a key resource in these fields, it is crucial in the corresponding arguments to consider time-bounded variants of Kolmogorov complexity. While fruitful interactions between time-bounded Kolmogorov complexity and different areas of theoretical computer science have been known for quite a while (e.g., [Sip83, Ko91, ABK+06, AF09], to name a few), the aforementioned results have led to a renewed interest in this topic. 

The theory of Kolmogorov complexity is well understood, but many useful results and properties of Kolmogorov complexity are not known to hold in time-bounded settings. Unfortunately, this creates technical difficulties or leads to conditional results when applying methods from time-bounded Kolmogorov complexity to algorithms and complexity theory. Perhaps even more importantly, in many cases it is desirable or even necessary to consider randomised algorithms. Since random strings have high complexity, the classical theory of time-bounded Kolmogorov complexity might be inappropriate or simply cannot be applied in such contexts.

To mitigate these issues and develop a more robust theory of time-bounded Kolmogorov complexity that survives in the important setting of randomised computations, some recent papers [Oli19, LO21, LOS21, GKLO22, LOZ22] have explored probabilistic notions of time-bounded Kolmogorov complexity, such as $\mathrm{rKt}$ complexity [Oli19], $\mathrm{rK}^t$ complexity [LOS21], and $\mathrm{pK}^t$ complexity [GKLO22]. These measures consider different ways of encoding an object via a probabilistic representation. In this survey, we provide an introduction to probabilistic time-bounded Kolmogorov complexity and its applications, highlighting many open problems and  research directions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/081"><span class="datestr">at May 27, 2022 01:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/27/being-different/">Being Different</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><i>In mathematics you don’t understand things. You just get used to them—John von Neumann.</i></p>
<p>
<a href="https://en.wikipedia.org/wiki/Harvey_Friedman">Harvey Friedman</a> is a famous mathematical logician who spent most of his career at Ohio State University. He worked not on proving new theorems as much as finding the axioms needed to prove them. Later in his career it was the axioms needed for certain <a href="https://en.wikipedia.org/wiki/Large_cardinal">large cardinal</a> theorems. These questions can be very subtle and difficult—not unlike lower bounds in complexity theory. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/hf/"><img width="194" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/hf.png?resize=194%2C259&amp;ssl=1" class="aligncenter size-full wp-image-20086" height="259" /></a><br />
Harvey was listed in the Guinness Book of World Records for being the world’s youngest professor when he taught at Stanford University at age 18 as an assistant professor of philosophy. He got his Ph.D. from Massachusetts Institute of Technology in 1967—hence the young age. </p>
<p>
</p><p><b> The Difference </b></p>
<p></p><p>
Harvey has taken a viewpoint for most of his career that is different from other logicians. Godel’s incompleteness theorems apply to most logic systems—certainly those that are strong enough for central areas of mathematics. But the majority of mathematicians believe that incompleteness does not apply to their everyday work. In short most do not think:</p>
<p>
	<i>I cannot prove P not equal to NP, so it must be incomplete.</i> 	 </p>
<p>Rather they feel that this means I am not smart enough to resolve P vs NP. Which of these is true? </p>
<p>
For example, Harvey has created numerous algebraic and geometric systems that make no explicit reference to logic but which, under a suitable coding, contain a logical system to which Godel’s incompleteness theorems apply. Furthermore, these <a href="https://www.encyclopedia.com/humanities/encyclopedias-almanacs-transcripts-and-maps/modern-logic-godel-friedman-and-reverse-mathematics">systems</a> look similar to many systems used by mathematicians in their everyday work. Harvey uses these examples to argue that incompleteness cannot be dismissed as a phenomenon that occurs only in overly general foundational frameworks contrived by logicians. He argues that it applies often to their everyday world.</p>
<p>
</p><p><b> An Example </b></p>
<p></p><p>
Harvey is also able to find numerous combinatorial statements with clear geometric meaning that are proved using large cardinals axioms and shown to require them. These <a href="https://arxiv.org/pdf/math/9811187.pdf">results</a> are famous to Harvey. Such axioms previously seemed to require statements that are not geometric. </p>
<p>
Gill Williamson can show that this can be used to connect these problems to the assumption that subset sum is solvable in polynomial time. See the paper <a href="https://arxiv.org/pdf/1907.11707.pdf">On The Difficulty Of Proving P Equals NP In ZFC</a>. This curious connection between the P vs. NP problem and the theory of large cardinals seems to suggest that either P=NP is false or otherwise not provable in ZFC. This connection is <a href="http://people.cs.uchicago.edu/~fortnow/beatcs/column81.pdf">surprising</a>.</p>
<p>
</p><p><b> Incompleteness </b></p>
<p></p><p>
Harvey does have a conjecture that shows that certain statements are not incomplete. He is interested in both sides of this coin: sometimes statements are provable and sometimes not. Concretely many mathematical theorems, such as Fermat’s Last Theorem, can be proved in very weak systems such as EFA. The <a href="https://hal.archives-ouvertes.fr/hal-00670733/document">Grand Conjecture</a> says:  Every theorem published in the Annals of Mathematics whose statement involves only finitary mathematical objects (i.e. what logicians call an arithmetical statement) can be proved in EFA. </p>
<p>
EFA is the weak fragment of Peano Arithmetic based on the usual quantifier-free axioms for <img src="https://s0.wp.com/latex.php?latex=%7B0%2C+1%2C+%2B%2C+%5Ctimes%2C+x%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0, 1, +, \times, x^y}" class="latex" />, together with the scheme of induction for all formulas whose quantifiers are bounded. While it is easy to construct artificial arithmetical statements that are true but not provable in EFA the point of the grand conjecture is that natural examples of such statements seem to be rare. </p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
 When Harvey was just starting to <a href="https://nautil.us/this-man-is-about-to-blow-up-mathematics-5857/">read</a>, at age 4 or 5, he remembers pointing to a dictionary and asking his mother what it was. It’s used to find out what words mean, she explained. A few days later, he returned to her with his verdict: The volume was completely worthless. For every word he’d looked up, the dictionary had taken him in circles: from “large” to “big” to “great” and so on, until he eventually arrived back at “large” again. She just looked at me as if I were a really strange, peculiar child, Friedman laughs. </p>
<p>
This is an <a href="https://nautil.us/this-man-is-about-to-blow-up-mathematics-5857/">insight</a> reported in an article by Jordana Cepelewicz. I cannot imagine Harvey was four or five when he told his mom this. For more stuff—when not so young—see <a href="https://www.amazon.com/Harvey-Friedmans-Research-Foundations-Mathematics/dp/0444558160?asin=0444558160&amp;revisionId=&amp;format=4&amp;depth=1">the book</a>. <a href="https://rjlipton.wpcomstaging.com/book-9/"><img width="333" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/book.jpg?resize=333%2C499&amp;ssl=1" class="aligncenter size-full wp-image-20087" height="499" /></a></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/05/27/being-different/"><span class="datestr">at May 27, 2022 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=22716">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/">Quantum Computers: A Brief Assessment of Progress in the Past Decade</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="article-headline-container">
<div class="header-content-container">
<p>In this post I give a brief  assessment of progress in the past decade, triggered by a recent article in Forbes Magazine that mentions my view on the matter.</p>
<h3 class="fs-headline speakable-headline font-base font-size should-redesign"><span style="color: #ff0000;">Waging War On Quantum</span> – A Forbes Article by Arthur Herman</h3>
</div>
</div>
<div class="top-contrib-block">
<div class="contribs">
<div class="contrib-container top-contrib bottom-padding">
<div class="fs-author-group-wrapper">
<div class="contrib-byline"></div>
</div>
</div>
</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Arthur_L._Herman">Arthur Herman</a> is a popular Historian and a senior fellow at the Hudson Institute. On Forbes Magazine he “comments on quantum computing and AI and American national security”, and his recent Forbes article <a href="https://www.forbes.com/sites/arthurherman/2022/05/19/waging-war-on-quantum/?sh=2044c5318896">Waging War on Quantum</a> (thanks, Alexander Vlasov) starts as follows:</p>
<blockquote><p><span style="color: #993300;"><em><strong>Quantum computing will never work. Keeping enough qubits stable long enough to do any significant calculating or processing, is a mathematical impossibility. The whole idea that one day quantum computers will discover new miracle drugs, or crack public encryption systems, is a mirage. Even worse, it’s a hoax.</strong></em></span></p>
<p><span style="color: #993300;"><em><strong>That’s been the message from so-called quantum skeptics for a decade or more, including physicists like Gil Kalai of Hebrew University and Mikhail Dyakonov of the University of Montepellier—all in spite of the fact that quantum computers have continued to grow in sophistication and qubit power. Most experts now agree it’s not a question if a large-scale quantum will emerge that can break into public encryption systems using Shor’s algorithm, but </strong></em><strong>when</strong><em><strong>.”</strong></em></span></p></blockquote>
<p><span style="color: #000000;">The first paragraph gives a reasonable description of my views, however I <strong>never</strong> referred to the whole idea of quantum computing as a hoax. </span><span style="color: #0000ff;"><span style="color: #000000;">Regarding the second paragraph, it is indeed correct that quantum computers have continued to grow in sophistication and qubit power, however <a href="https://arxiv.org/abs/1908.02499">my theory</a> (based on a computational complexity argument) is that progress for reducing the error rate will reach a wall and that recent progress merely approaches this limit. Let me elaborate a little on the development of the past decade as I see it</span>.<br />
</span></p>
<p>Before moving to my assessment I would like to note that Arthur Herman’s offers an outrageous conclusion to his article. He suggests that skepticism of quantum computers (and of the company <a href="https://ionq.com/">IonQ</a>) puts the skeptics’ countries at risk. In my opinion, the militant rhetoric of the title and of the conclusion is very inappropriate.</p>
<h2>Assessment of progress in the past decade</h2>
<p>The past quantum computing decade is characterized both by notable progress, adjustment of expectations, larger investments, much enthusiasm, and some hype. The overall picture is unclear and might be clearer 5-10 years from now.</p>
<p><span style="color: #0000ff;">The following picture (click to enlarge) describes the shift in the community view over the last decade (as I see it).</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2022/05/2010-2020d.png"><img width="724" alt="2010-2020d" src="https://gilkalai.files.wordpress.com/2022/05/2010-2020d.png?w=724&amp;h=308" class="alignnone  wp-image-22721" height="308" /></a></p>
<p>On the left you can see David DiVincenzo’s famous 7-steps road map to quantum computers. DiVincenzo put forward these steps in his 2000 paper  <a href="https://arxiv.org/abs/quant-ph/0002077">The physical implementation of quantum computation</a>,  and the above picture on the left is a graphic description of these steps in a <a href="https://www.science.org/doi/10.1126/science.1231930">2013 review paper</a> by Michel Devoret and Rob Schoelkopf.  The caption under the Figure asserts that <span style="color: #0000ff;">“Superconducting qubits are the only solid-state implementation at the third stage, and they now aim at reaching the fourth stage (green arrow). In the domain of atomic physics and quantum optics, the third stage had been previously attained by trapped ions and by Rydberg atoms. No implementation has yet reached the fourth stage, where a logical qubit can be stored, via error correction, for a time substantially longer than the decoherence time of its physical qubit components.”</span> The fourth step “logical memory with (substantial) longer lifetime than physical qubits” looked to many like a near term goal ten years ago.</p>
<p>One important development of the last ten years was to introduce building NISQ computers and achieving “quantum supremacy” (and related tasks like high “quantum volume”) as an intermediate goal towards DiVincenzo’s step four. (See the picture on the right.) Of course, there is nothing wrong with setting intermediate goals, we do it all the time and this can be very fruitful.</p>
<p>For me, from the skeptical point of view, these intermediate goals were an opportunity, allowing me to present a <a href="https://gilkalai.wordpress.com/2020/12/29/the-argument-against-quantum-computers-a-very-short-introduction/">clear computational theoretic argument</a> for why “quantum supremacy” is out of reach and to connect the problem with the theory of noise-sensitivity and noise-stability and with Fourier methods that I and my colleagues developed in the 90s.</p>
<p>Adding the intermediate goal of quantum supremacy also represented a much slower time-table than what people previously anticipated. For example, nine years ago in 2013  John Martinis <a href="https://youtu.be/7Pok08VvkeU">gave a lecture</a> at <a href="https://gilkalai.wordpress.com/2013/04/12/qstart/">QSTART</a>,  the opening conference of our HUJI quantum science center. At that time, John expected to have the ability of building distance-3 and distance-5 surface codes within a few years and the tasks of demonstrating logical gates and of logical qubits with <img src="https://s0.wp.com/latex.php?latex=10%5E%7B-15%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="10^{-15}" class="latex" /> error rates some years later. John also mentioned the ability to control 20 qubits within one month (to this Ray Laflamme commented that it is going to be a long month). All these targets are today still out of reach. It is undisputed that considerably lower noise rates are required even for achieving distance-3 surface codes and it is still not possible to have good control of 20-qubit (and perhaps even 10-qubit) quantum computation.</p>
<p>Of course, John Martinis himself was the leader of the Google efforts towards “quantum supremacy” which are now being carefully evaluated, and his vision and technology from 2013 was important for the Sycamore NISQ experiments. Let me mention that Google’s fantastic “quantum supremacy” claims were <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">largely (but not fully) refuted</a>.</p>
<p>There was a similar level of optimism from various other researchers. <a href="https://en.wikipedia.org/wiki/Robert_J._Schoelkopf#Schoelkopf.27s_law">It was expected</a> that coherence time would increase by a factor of ten every three years and there was a proposed “<a href="https://www.quantamagazine.org/does-nevens-law-describe-quantum-computings-rise-20190618/">double exponential law</a>” prediction for the classical computational power required to simulate quantum devices as time proceeds.  I personally don’t regard these specific claims as hype but rather as (at times, over-the-top) reasoned optimism, but both the reasoning and the predictions themselves should carefully be examined.</p>
<p>NISQ computers are interesting, and they allow interesting quantum physics experiments. Herman asserts that <strong><em>“quantum hybrid systems are making the qubit revolution something that’s happening now, not just a distant dream” </em></strong>and this echoes hopes of several researchers in academia and industry. My analysis asserts that NISQ computers are, from the computational complexity perspective, primitive classical computational devices with inherent chaotic behavior, and therefore, I don’t see how hybrid systems and the interface with conventional computers would turn them into useful computational devices. (They can still be useful for quantum physics experimentation.)</p>
<p>Let me repeat: slower progress than anticipated is very common, setting new intermediate goals is both common and welcome. By themselves they do not imply that the target of “<em><strong>large-scale quantum computers that can break into public encryption systems using Shor’s algorithm” </strong></em>is unrealistic, and indeed many experts in the field believe that it is a matter of time for this ultimate goal to be reached. My view is different, I try to explain my argument to other experts, and to offer experimental predictions and theoretical implications. There are good reasons to hope that the matter will be tested experimentally in the years to come, but my assessment is that the experimental picture from the past decade <strong>is not clear</strong>.</p>
<h3>IonQ, trapped ion quantum computation, and Elon Musk</h3>
<p>Herman’s article was triggered by a 183-page document written by a group called “Scorpion Capital.” The document attacks Maryland-based quantum computer company IonQ and among various concerns it also briefly mentions my and Michel Dyakonov’s positions about quantum computers.  I myself share the common view that ion-trap methods for quantum computers form a major avenue and that Chris Monroe (a co-founder of IonQ) is a major player in this direction. I don’t know much about IonQ’s specific efforts, but I would expect that large scale investment is required to put ion-trap methods to test and I personally would like to see it being tested. So I would be quite pleased to see <strong>Elon Musk</strong> deciding to buy IonQ or the Israeli trapped ion QC company of <a href="https://www.weizmann.ac.il/complex/ozeri/welcome-weizmann-trapped-ions-lab">Roee Ozeri</a> (or both) <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> and to make  trapped ion technology his quantum computing signature. Incidentally, the comment section of my 2018 <a href="https://www.quantamagazine.org/gil-kalais-argument-against-quantum-computers-20180207/">Quanta Magazine interview</a> presented an interesting exchange between Monroe and me (starting <a href="https://www.quantamagazine.org/gil-kalais-argument-against-quantum-computers-20180207/#comment-4559220424">here</a>).</p>
<h3>A few more remarks:</h3>
<p>1) Herman’s article raises several other interesting issues like when (and if) is the appropriate time to transfer to “post quantum cryptography” protocols.</p>
<p>2) There are a few researchers skeptical of quantum computers that actually conduct research and write papers (and books) about it. (There are others that regard the idea as absurd nonsense of absolutely no interest.) A notable researcher who wrote several important papers in the skeptical direction since the late 90s is Robert Alicki from the University of Gdansk.</p>
<p>3) Here is Herman’s crazy conclusion: “No one is saying the Scorpion Capital short-sellers are in Chinese pay, or that skeptics like Dyakonov and Kalai are knowingly putting their countries at risk. But waging war on the U.S. quantum industry can have serious consequences, unless quantum companies and labs show that they are not intimidated, and reassure the public that the quantum future doesn’t rest on hype but significant achievements—achievements that will make our country and our world safer, stronger, and more confident about our future as a whole.”</p>
<p>4) I changed the title to reflect the main topic of the post.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2022/05/26/waging-war-on-quantum/"><span class="datestr">at May 26, 2022 09:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/080">TR22-080 |  QBF Merge Resolution is powerful but unnatural | 

	Meena Mahajan, 

	Gaurav Sood</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Merge Resolution proof system (M-Res) for QBFs, proposed by Beyersdorff et al. in 2019, explicitly builds partial strategies inside refutations. The original motivation for this approach was to overcome the limitations encountered in long-distance Q-Resolution proof system (LD-Q-Res), where the syntactic side-conditions, while prohibiting all unsound resolutions, also end up prohibiting some sound resolutions. However, while the advantage of M-Res over many other resolution-based QBF proof systems was already demonstrated, a comparison with LD-Q-Res itself had remained open. In this paper, we settle this question. We show that M-Res has an exponential advantage over not only LD-Q-Res, but even over LQU$^+$-Res and IRM, the most powerful among currently known resolution-based QBF proof systems. Combining this with results from Beyersdorff et al. 2020, we conclude that M-Res is incomparable with LQU-Res and LQU$^+$-Res.

Our proof method reveals two additional and curious features about M-Res: (i) MRes is not closed under restrictions, and is hence not a natural proof system, and (ii) weakening axiom clauses with existential variables provably yields an exponential advantage over M-Res without weakening. We further show that in the context of regular derivations, weakening axiom clauses with universal variables provably yields an exponential advantage over M-Res without weakening. These results suggest that MRes is better used with weakening, though whether M-Res with weakening is closed under restrictions remains open. We note that even with weakening, M-Res continues to be simulated by eFrege $+ $ $\forall$red (the simulation of ordinary M-Res was shown recently by Chew and Slivovsky).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/080"><span class="datestr">at May 26, 2022 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/05/25/tcs-talk-wednesday-june-1-inbal-talgam-cohen-technion-israel-institute-of-technology/">TCS+ talk: Wednesday, June 1 — Inbal Talgam-Cohen, Technion – Israel Institute of Technology</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p></p>



<p></p>


<p>The next TCS+ talk will take place this coming Wednesday, June 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://www.inbaltalgam.com/"><strong>Inbal Talgam-Cohen</strong></a> from the Technion – Israel Institute of Technology will speak about “<em>Algorithmic contract theory</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote">
<p>Abstract: Algorithms increasingly interact with strategic, self-interested players; their design must take player incentives into account or risk being “gamed” and failing miserably. The algorithmic game theory literature traditionally focused on “mechanisms” – algorithms that incentivize players to truthfully report the <strong>input</strong>. In this talk we shift focus from mechanisms to “contracts”, which are concerned with the algorithm’s <strong>output</strong> and players’ incentives to carry it out. The goal of this talk is to describe where we’re at in forming a new algorithmic theory of contract design.</p>
<p>I will demonstrate how algorithmic approaches – in particular the approach of <strong>beyond worst-case analysis</strong> – can shed light on a classic economic puzzle regarding simple contracts. Within the realm of <strong>incentives and learning</strong>, I will discuss how classifiers induce incentives and show a formal relation to contracts.</p>
<p>Based on joint works with Tal Alon, Magdalen Dobson, Paul Duetting, Ron Lavi, Ariel Procaccia, Tim Roughgarden, Elisheva Shamash and Jamie Tucker-Foltz.</p>
</blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/05/25/tcs-talk-wednesday-june-1-inbal-talgam-cohen-technion-israel-institute-of-technology/"><span class="datestr">at May 25, 2022 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/079">TR22-079 |  Lower Bound Methods for Sign-rank and their Limitations | 

	Hamed Hatami, 

	Pooya Hatami, 

	William Pires, 

	Ran Tao, 

	Rosie Zhao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The sign-rank of a matrix $A$ with $\pm 1$ entries is the smallest rank of a real matrix with the same sign pattern as $A$. To the best of our knowledge, there are only three known methods for proving lower bounds on the sign-rank of explicit matrices:  (i) Sign-rank is at least the VC-dimension; (ii) Forster's method, which states that sign-rank is at least the inverse of the largest possible average margin among the representations of the matrix by points and half-spaces; (iii) Sign-rank is at least a logarithmic function of the density of the largest monochromatic rectangle. 
 

We prove several results regarding the limitations of these methods. 


$\bullet$ We prove that, qualitatively,  the monochromatic rectangle density is the strongest of these three lower bounds. If it fails to provide a super-constant lower bound for the sign-rank of a  matrix, then the other two methods will fail as well.   
$\bullet$ We show that there exist $n \times n$ matrices with sign-rank $n^{\Omega(1)}$ for which none of these methods can provide a super-constant lower bound.   
$\bullet$ We show that sign-rank is at most an exponential function of the deterministic communication complexity with access to an equality oracle. We combine this result with Green and Sanders' quantitative version of  Cohen's idempotent theorem to show that for a large class of sign matrices (e.g., XOR-lifts), sign-rank is at most an exponential function of the $\gamma_2$ norm of the matrix. We conjecture that this holds for all sign matrices. 
$\bullet$ Towards answering a question of Linial,  Mendelson,   Schechtman, and  Shraibman regarding the relation between sign-rank and discrepancy, we conjecture that sign-ranks of the $\pm1$ adjacency matrices of hypercube graphs can be arbitrarily large. We prove that none of the three lower bound techniques can resolve this conjecture in the affirmative.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/079"><span class="datestr">at May 25, 2022 08:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/078">TR22-078 |  Improved local testing for multiplicity codes | 

	Dan Karliner, 

	Amnon Ta-Shma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Multiplicity codes are a generalization of Reed-Muller codes which include derivatives as well as the values of low degree polynomials, evaluated in every point in $\mathbb{F}_p^m$. 
Similarly to Reed-Muller codes, multiplicity codes have a local nature that allows for local correction and local testing. 
Recently, the authors and Salama showed that the plane test, which tests the degree of the codeword on a random plane, is a good local tester for small enough degrees.
In this work we simplify and extend the analysis of local testing for multiplicity codes, giving a more general and tighter analysis. In particular, we show that multiplicity codes $\mathrm{MRM}_{p}(m, d, s)$ over prime fields with arbitrary $d$ are locally testable by an appropriate $k$-flat test, which tests the degree of the codeword on a random $k$-dimensional affine subspace. The relationship between the degree parameter $d$ and the required dimension $k$ is shown to be nearly optimal, and improves on the degree previously known in the case of planes.

Our analysis relies on a generalization of the technique of canonincal monomials introduced by Haramaty, Shpilka and Sudan (FOCS 2011). Generalizing canonical monomials to the multiplicity case requires substantially different proofs which exploit the algebraic structure of multiplicity codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/078"><span class="datestr">at May 25, 2022 03:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/077">TR22-077 |  Explicit Lower Bounds Against $\Omega(n)$-Rounds of Sum-of-Squares | 

	Max Hopkins, 

	Ting-Chun Lin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We construct an explicit family of 3-XOR instances hard for $\Omega(n)$-levels of the Sum-of-Squares (SoS) semi-definite programming hierarchy. Not only is this the first explicit construction to beat brute force search (beyond low-order improvements (Tulsiani 2021, Pratt 2021)), combined with standard gap amplification techniques it also matches the (optimal) hardness of random instances up to imperfect completeness (Grigoriev TCS 2001, Schoenebeck FOCS 2008). 

Our result is based on a new form of small-set high dimensional expansion (SS-HDX) inspired by recent breakthroughs in locally testable and quantum LDPC codes. Adapting the recent framework of Dinur, Filmus, Harsha, and Tulsiani (ITCS 2021) for SoS lower bounds from the Ramanujan complex to this setting, we show any (bounded-degree) SS-HDX can be transformed into a highly unsatisfiable 3-XOR instance that cannot be refuted by $\Omega(n)$-levels of SoS. We then show Leverrier and Z\'emor's (Arxiv 2022) recent qLDPC construction gives the desired explicit family of bounded-degree SS-HDX. Incidentally, this gives the strongest known form of bi-directional high dimensional expansion to date.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/077"><span class="datestr">at May 25, 2022 03:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/privacy-doona/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/privacy-doona/">Privacy Doona&amp;#58; Why We Should Hide Among The Clones</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this blog post, we will discuss a recent(ish) result of Feldman, McMillan, and Talwar <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, which provides an improved and simple analysis of the so-called “amplification by shuffling” formally connecting local privacy (LDP) and shuffle privacy.<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup> Now, I’ll assume the reader is familiar with both LDP and Shuffle DP: if not, a quick-and-dirty refresher (with less quick, and less dirty references) can be found <a href="https://differentialprivacy.org/\trustmodels">here</a>, and of course there is also Albert Cheu’s excellent survey on Shuffle DP <a href="https://arxiv.org/abs/2107.11839" title="Albert Cheu. Differential Privacy in the Shuffle Model: A Survey of Separations. arXiv 2021"><strong>[Cheu21]</strong></a>.</p>

<p>I will also ignore most of the historical details, but it is worth mentioning that <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> is not the first paper on this “amplification by shuffling,” (which, for local reasons, I’ll just call a <em>privacy <a href="https://www.collinsdictionary.com/dictionary/english/doona">doona</a></em>) but rather is the culmination of a rather long line of work involving many cool ideas and papers, starting with <a href="https://arxiv.org/abs/1808.01394" title="Albert Cheu, Adam D. Smith, Jonathan Ullman, David Zeber, Maxim Zhilyaev. Distributed Differential Privacy via Shuffling. EUROCRYPT 2019"><strong>[CSUZZ19</strong></a>, <a href="https://arxiv.org/abs/1811.12469" title="Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, Abhradeep Thakurta. Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity. SODA 2019"><strong>EFMRTT19]</strong></a>: I’d refer the reader to <strong>Table 1</strong> in <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> for an overview.</p>

<p>Alright, now that the caveats are behind us, what <em>is</em> “amplification by shuffling”? In a nutshell, it is capturing the (false!) intuition that “anonymization provides privacy” (which, again, is false! Don’t do this!) and making it… less false. The idea is that while <em>anonymization does not provide in itself any meaningful privacy guarantee</em>, it can <em>amplify existing, rigorous privacy guarantee</em>. So if I start with a somewhat lousy LDP guarantee, but then all the messages sent by all users are completely anonymized, then my lousy LDP guarantee suddenly gets <em>much</em> stronger (roughly speaking, the \(\varepsilon\) parameter goes down with the square root of of the number of users involved). Which is wonderful! Let’s see what this means, quantitatively.</p>

<h3 id="the-result-of-feldman-mcmillan-and-talwar">The result of Feldman, McMillan, and Talwar.</h3>
<p>Here, we will focus on the simpler case of <em>noninteractive</em> protocols (one-shot messages from the users to the central server, no funny business with messages going back and forth); which is conceptually simpler to state and parse, still very rich and interesting, and, well, very relevant in practice (being the easiest and cheapest to deploy). If you want the results in their full glorious generality, though, they are in the paper.</p>

<p>What the main theorem of <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a> is saying for this noninteractive setting can then be stated as follows: if I have an <em>\(\varepsilon_L\)-locally private</em> (LDP) protocol for a task, where all \(n\) users pass their data through the same randomizer (algorithm) \(R\) and send the resulting message \(y_i \gets R(x_i)\), then just permuting the messages \(y_1\dots,y_n\) immediately gives an \((\varepsilon,\delta)\)-<em>shuffle</em> private protocol for the same task, for any pair \((\varepsilon,\delta)\) which satisfies
<a name="eq:eps:epsL"></a>
\begin{equation}
		\varepsilon \leq \log\left( 1+ 16\frac{e^{\varepsilon_{L}}-1}{e^{\varepsilon_{L}}+1}\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{1}
\end{equation}
as long as \(n \gg e^{\varepsilon_{L}}\log(1/\delta)\). That is quite a lot to parse, though: what does this actually <em>mean</em>?</p>

<p><strong>First</strong>, the assumption that all users have the same randomizer (or at least cannot be distinguished by their randomizer) is quite natural: if they didn’t, then we wouldn’t be able to say anything in general, since the randomizer they use could just give away their identity completely. For instance, as an extreme case, the randomizer of user \(i\) could just append \(i\) to the message (it’s OK, still LDP!), and then shuffling achieves exactly nothing: we know who sent what. So OK, asking for all randomizers to be the same is not really a restriction.</p>

<p><strong>Second</strong>, each user only sends one message, and this preserves its length (we just shuffled the messages, didn’t modify them!). So if you start with an LDP protocol with amazing features XYZ (e.g., the messages are \(1\)-bit long, or users don’t share a random seed, or the randomizers run in time \(O(1)\)), then the shuffle protocol enjoys exactly the same properties. (It only enjoys naturally some <em>robustness</em>, in the sense that if \(10\%\) if the \(n\) users maliciously deviate from the protocol, they can’t really jeopardize the privacy of the remaining \(90\%\) of users.<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote" rel="footnote">2</a></sup> Which is… good.)</p>

<p><strong>Third</strong>, this is inherently approximate DP. Here we started with pure LDP (you can also extend that to approximate LDP) and ended up with approximate Shuffle DP: this is not a mistake, that’s how it is. I am not a purist (erm) myself, and that looks more than good enough to me; but if you seek pure Shuffle DP, then this result is not the droid you’re looking for.</p>

<p align="center">
  <img width="50%" alt="This is not the pure DP guarantee you are looking for." src="https://differentialprivacy.org/../images/droids-looking.png" />
</p>
<p><br /></p>

<p>Alright, <em>what</em> is this guarantee stated in <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a> giving us? Let’s interpret the expression in <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a> in two parameter regimes, focusing on \(\varepsilon\) (fixing some small \(\delta&gt;0\)). If we start with \(\varepsilon_{L} \ll 1\) for our LDP randomizers \(R\), then a first-order Taylor expansion shows that we get
\[
		\varepsilon \approx \varepsilon_{L}\cdot 8\sqrt{\frac{\log\frac{4}{\delta}}{n}}
\]
so that <em>shuffling improved our privacy parameter by a factor \(\sqrt{n}\)</em>.<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote" rel="footnote">3</a></sup> 😲 This is great! With more users, comes more privacy!</p>

<p>But that was starting with small \(\varepsilon_{L}\), that is, already pretty good privacy guarantees for our LDP “building block” \(R\). What happens if we start with “somewhat lousy” privacy guarantees, that is, \(\varepsilon_{L} \gg 1\)? Do we get anything interesting then?
Another Taylor expansion (everything is a Taylor expansion) shows us that, then,
<a name="eq:epsL:ll:one"></a>
\[
\begin{equation}
		\varepsilon \approx \log\left( 1+ 8\sqrt{\frac{e^{\varepsilon_{L}}\log\frac{4}{\delta}}{n}}\right) \tag{2}
\end{equation}
\]
or, put differently,
<a name="eq:epsL:gg:one"></a>
\[
\begin{equation}
		\varepsilon \approx 8e^{\varepsilon_{L}/2}\sqrt{\frac{\log\frac{4}{\delta}}{n}} \tag{3}
\end{equation}
\]
That’s a bit harder to interpret, but that seems… useful? It is: let us see how much, with a couple examples.</p>

<h4 id="learning">Learning.</h4>
<p>The first one is distribution learning, a.k.a. density estimation: you have \(n\) i.i.d. samples (one per user) from an unknown probability distribution \(\mathbf{p}\) over a discrete domain of size \(k\), and your goal is to output an estimate \(\widehat{\mathbf{p}}\) such that, with high (say, constant) probability, \(\mathbf{p}\) and \(\widehat{\mathbf{p}}\) are close in <em>total variation distance</em>:
\[
		\operatorname{TV}(\mathbf{p},\widehat{\mathbf{p}}) = \sup_{S\subseteq [k]} (\mathbf{p}(S) - \widehat{\mathbf{p}}(S) ) \leq \alpha
\]
(if total variation distance seems a bit mysterious, it’s exactly half the \(\ell_1\) distance between the probability mass functions). We know how to solve this problem in the non-private setting: \(n=\Theta\left( \frac{k}{\alpha^2} \right)\) samples are necessary and sufficient. We know how to solve this problem in the (central) DP setting: \(n=\Theta\left( \frac{k}{\alpha^2} + \frac{k}{\alpha\varepsilon} \right)\) samples are necessary and sufficient <a href="https://proceedings.neurips.cc/paper/2015/hash/2b3bf3eee2475e03885a110e9acaab61-Abstract.html" title="Ilias Diakonikolas, Moritz Hardt, Ludwig Schmidt. Differentially Private Learning of Structured Discrete Distributions. NeurIPS 2015"><strong>[DHS15]</strong></a>. We know how to solve this problem in the LDP setting: 
<a name="eq:learning:ldp"></a>
\begin{equation}
n=\Theta\left(\frac{k^2}{\alpha^2(e^\varepsilon-1)^2}+\frac{k^2}{\alpha^2e^\varepsilon}+\frac{k}{\alpha^2}\right) \tag{4}
\end{equation}
samples are necessary and sufficient <a href="http://proceedings.mlr.press/v89/acharya19a.html" title="Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication. AISTATS 2019"><strong>[ASZ19]</strong></a> (note that the first term is just \(k/(\alpha^2\varepsilon^2)\) for small \(\varepsilon\)). Now, as they say in Mulan: <em>let’s make a shuffle DP algo out of you.</em></p>

<p>If we want to achieve \((\varepsilon,\delta)\)-shuffle DP, we need to select \(\varepsilon_L\). Based on <a href="https://differentialprivacy.org/feed.xml#eq:epsL:ll:one">(2)</a> and <a href="https://differentialprivacy.org/feed.xml#eq:epsL:gg:one">(3)</a>, and ignoring pesky constants we will choose it so that
<a name="eq:choice:epsL"></a>
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,. \tag{5}
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging that back in <a href="https://differentialprivacy.org/feed.xml#eq:learning:ldp">(4)</a>, we see that the first case corresponds to the first term (small \(\varepsilon_{L}\)) and the second to the second term (\(\varepsilon_{L} \geq 1\)), and overall the condition on \(n\) for the original LDP algorithm to 
successful learn the distribution becomes
\[
	n \gtrsim 
	\frac{k^2}{\alpha^2(e^{\varepsilon_{L}}-1)^2}+\frac{k^2}{\alpha^2e^{\varepsilon_{L}}}+\frac{k}{\alpha^2}
	\approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2} 
	 \approx \frac{k^2\log(1/\delta)}{\alpha^2\varepsilon^2 n}+\frac{k}{\alpha^2}
\]
(where \(\gtrsim\) means “let’s ignore constants”). There is an \(n\) in the RHS as well, so reorganizing and handling the two terms separately the condition on \(n\) becomes
\[
	n \gtrsim \frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}+\frac{k}{\alpha^2}
\]
which… is great? We immediately get a sample complexity \(O\left(\frac{k}{\alpha^2}+\frac{k \sqrt{\log(1/\delta)}}{\alpha\varepsilon}\right)\) in the shuffle DP model, which (ignoring the \(\sqrt{\log(1/\delta)}\)) matches the one in the <em>central</em> DP setting!</p>

<p><strong>tl;dr:</strong> Taking an optimal LDP algorithm and just shuffling the messages <em>immediately</em> gives an optimal shuffle DP algorithm, no extra work needed.</p>

<h4 id="uniformity-testing">(Uniformity) Testing.</h4>
<p>Alright, maybe it was a fluke? Let’s look at another “basic” problem close to my heart: we don’t want to learn the probability distribution \(\mathbf{p}\), just test whether it is actually <em>the</em> uniform distribution<sup id="fnref:4"><a href="https://differentialprivacy.org/feed.xml#fn:4" class="footnote" rel="footnote">4</a></sup> \(\mathbf{u}\) on the domain \([k]={1,2,\dots,k}\). So if \(\mathbf{p} =\mathbf{u}\), you’ve got to say “yes” with probability at least \(2/3\), and if \(\operatorname{TV}(\mathbf{p},\mathbf{u})&gt;\alpha\), then you need to say “no” with probability at least \(2/3\).</p>

<p>This is also well understood in the non-private setting (\(n=\Theta(\sqrt{k}/\alpha^2)\)) <a href="https://ieeexplore.ieee.org/document/4626074" title="Liam Paninski. A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data. IEEE Trans. Inf. Theory 2008"><strong>[Paninski08]</strong></a> [see also <a href="https://ccanonne.github.io/survey-topics-dt.html}{my upcoming survey">my upcoming survey</a>], in the central DP setting (\(n=\Theta\left( \frac{\sqrt{k}}{\alpha^2} + \frac{\sqrt{k}}{\alpha\sqrt{\varepsilon}}+\frac{k^{1/3}}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{1}{\alpha\varepsilon} \right)\)) <a href="https://arxiv.org/abs/1707.05128" title="Jayadev Acharya, Ziteng Sun, Huanyu Zhang. Differentially Private Testing of Identity and Closeness of Discrete Distributions. NeurIPS 2018"><strong>[ASZ18</strong></a>, <a href="https://arxiv.org/abs/1707.05497" title="Maryam Aliakbarpour, Ilias Diakonikolas, Ronitt Rubinfeld. Differentially Private Identity and Equivalence Testing of Discrete Distributions. ICML 2018"><strong>ADR18]</strong></a>, and in the LDP setting, where the result differs on whether the users can communicate or share a common random seed 
<a name="eq:testing:ldp:publiccoin"></a>
\begin{equation}
	n=\Theta\left( \frac{k}{\alpha^2(e^\varepsilon-1)^2} + \frac{k}{\alpha^2e^{\varepsilon/2}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{6}
\end{equation} or not
<a name="eq:testing:ldp:privatecoin"></a> 
\begin{equation}
	n=\Theta\left( \frac{k^{3/2}}{\alpha^2(e^\varepsilon-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon}} + \frac{\sqrt{k}}{\alpha^2}\right) \tag{7}
\end{equation}
as established in a sequence of papers <a href="https://arxiv.org/abs/1812.11476" title="Inference under Information Constraints: Lower Bounds from Chi-Square Contraction. COLT 2019"><strong>[ACT19</strong></a>, <a href="https://arxiv.org/abs/1911.01452" title="Kareem Amin, Matthew Joseph, Jieming Mao. Pan-Private Uniformity Testing. COLT 2020"><strong>AJM20</strong></a>, <a href="https://arxiv.org/abs/2101.07981" title="Jayadev Acharya, Clément L. Canonne, Cody Freitag, Ziteng Sun, Himanshu Tyagi.  Inference Under Information Constraints III: Local Privacy Constraints. IEEE J. Sel. Areas Inf. Theory 2021"><strong>ACFST21</strong></a>, <a href="https://arxiv.org/abs/2007.10976" title="Jayadev Acharya, Clément L. Canonne, Yuhan Liu, Ziteng Sun, Himanshu Tyagi. Interactive Inference Under Information Constraints. IEEE Trans. Inf. Theory 2022"><strong>ACLST22</strong></a>, <a href="https://arxiv.org/abs/2108.08987" title="Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022"><strong>CL22]</strong></a>.</p>

<p>Now, say you want an (\(\varepsilon,\delta)\)-shuffle DP algorithm for uniformity testing, but don’t want to design one from scratch (though it <em>is</em> possible to do so, and some did <a href="https://arxiv.org/abs/2004.09481" title="Victor Balcer, Albert Cheu, Matthew Joseph, Jieming Mao. Connecting Robust Shuffle Privacy and Pan-Privacy. SODA 2021"><strong>[BCJM21</strong></a>, <a href="https://arxiv.org/abs/2108.08987" title="Clément L. Canonne, Hongyi Lyu. Uniformity Testing in the Shuffle Model: Simpler, Better, Faster. SOSA 2022"><strong>CL22</strong></a>, <a href="https://arxiv.org/abs/2112.10032" title="Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021"><strong>CY21]</strong></a>). Let’s say you want to look at the “no-common-random-seed-shared-by-users” model (a.k.a. <em>private-coin</em> setting): so you stare at the corresponding LDP communication complexity, <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:privatecoin">(7)</a>, and try to choose \(\varepsilon_L\) to start with before shuffling. This will be the same as in the learning example (i.e., <a href="https://differentialprivacy.org/feed.xml#eq:choice:epsL">(5)</a>): based on <a href="https://differentialprivacy.org/feed.xml#eq:epsL:ll:one">(2)</a> and <a href="https://differentialprivacy.org/feed.xml#eq:epsL:gg:one">(3)</a>, we will set
\begin{equation}
	 \varepsilon_{L} \approx \varepsilon \sqrt{\frac{n}{\log(1/\delta)}}  \quad\text{ or }\quad e^{\varepsilon_{L}} \approx \varepsilon^2 \cdot \frac{n}{\log(1/\delta)}\,.
\end{equation}
depending on whether \(\frac{\varepsilon^2 n}{\log(1/\delta)}\geq 1\). Plugging this back in <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:privatecoin">(7)</a> and quickly checking which case corresponds to each term, we then easily get that for our algorithm to correctly solve the uniformity testing problem, it suffices that the sample complexity (number of users) \(n\) satisfies
\[
	n \gtrsim \frac{k^{3/2}}{\alpha^2(e^{\varepsilon_L}-1)^2} + \frac{k^{3/2}}{\alpha^2e^{\varepsilon_L}} + \frac{\sqrt{k}}{\alpha^2}
	 \approx \frac{k^{3/2}\log(1/\delta)}{\alpha^2\varepsilon^2 n } + \frac{\sqrt{k}}{\alpha^2}
\]
which, reorganizing and solving for \(n\), means that it suffices to have 
\[
	n \gtrsim \frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon} + \frac{\sqrt{k}}{\alpha^2}\,.
\]
And, <em>voilà</em>! Even better, we also have strong evidence to suspect that this sample complexity \(O\Big(\frac{k^{3/4}\sqrt{\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)\) is tight among all private-coin algorithms.<sup id="fnref:5"><a href="https://differentialprivacy.org/feed.xml#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>Now, if you wanted to look at <em>public-coin</em> shuffle DP protocols (with a common random seed available), then you would start with an optimal public-coin LDP algorithm (and look at <a href="https://differentialprivacy.org/feed.xml#eq:testing:ldp:publiccoin">(6)</a>), and setting \(\varepsilon_L\) the same way you’d get a shuffle DP algorithm with sample complexity
\[
n=O\Big(\frac{k^{2/3}\log^{1/3}(1/\delta)}{\alpha^{4/3}\varepsilon^{2/3}} + \frac{\sqrt{k\log(1/\delta)}}{\alpha\varepsilon}+ \frac{\sqrt{k}}{\alpha^2}\Big)
\]
which, well, is <em>also</em> strongly believed to be optimal!</p>

<p><strong>tl;dr:</strong> Here again, taking an optimal off-the-shelf LDP algorithm and just shuffling the messages <em>immediately</em> gives an optimal shuffle DP algorithm, no extra work needed.</p>

<h3 id="conclusion">Conclusion.</h3>
<p>I hope the above convinced you of how useful this privacy amplification can be: from an optimal LDP algorithm, featuring any extra appealing characteristics you like, <em>just adding an extra shuffling step as postprocessing</em> yields an (often optimal? At least good) shuffle DP algorithm, <em>with the same characteristics</em> and built-in robustness against malicious users.</p>

<p>All you need is to make sure that your starting point, the LDP algorithm satisfies a couple things: (1) all users have the same randomizer,<sup id="fnref:6"><a href="https://differentialprivacy.org/feed.xml#fn:6" class="footnote" rel="footnote">6</a></sup> and (2) it works in all regimes of \(\varepsilon\) (both high-privacy, \(\varepsilon \leq 1\), <em>and</em> low-privacy, \(\varepsilon \gg 1\)). Once you’ve got this, Bob’s your uncle! You get shuffle DP algorithms for free.</p>

<p>It is not only appealing from a theoretical point of view, by the way! The authors of the paper worked hard to make their empirical analysis compelling as well, and their code is available <a href="https://github.com/apple/ml-shuffling-amplification">on GitHub</a> 📝. But more importantly, from a practitioner’s point of view, this means it is enough to design, implement, and test <em>one</em> algorithm (the LDP one we start with) to automatically get a trusted one in the shuffle DP model as well: this reduces the risks of bugs, security failures, the amount of work spending tuning, testing…</p>

<p>So yes, whenever possible, we <em>should</em> hide among the clones!</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The title of this post is a reference to the title of <a href="https://arxiv.org/abs/2012.12803" title="Vitaly Feldman, Audra McMillan, Kunal Talwar. Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. FOCS 2021"><strong>[FMT21]</strong></a>, “Hiding Among The Clones,” and to the notion of <em>privacy blanket</em> introduced by Balle, Bell, Gascón, and Nissim <a href="https://link.springer.com/chapter/10.1007/978-3-030-26951-7_22" title="Borja Balle, James Bell, Adrià Gascón, Kobbi Nissim. The Privacy Blanket of the Shuffle Model. CRYPTO 2019"><strong>[BBGN19]</strong></a>. Intuitively, the “amplification by shuffling” paradigm can be seen as anonymizing the messages from local randomizers, whose message distribution can be mathematically decomposed as a mixture of “noise distribution not depending on the user’s input” and “distribution actually depending on their input.” As a result, each user randomly sends a message from the first or second distribution of the mixture.  But the shuffling then hides the informative messages (drawn from the second part of the mixture) among the non-informative (noise) ones: so the noise messages end up providing a “privacy blanket” in which sensitive information is safely and soundly wrapped. <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>More specifically, they can completely jeopardize the <em>utility</em> (accuracy) of the result, but in terms of privacy, all they can do is slightly reduce it: if \(10\%\) of users are malicious, the remaining \(90\%\) still get the privacy amplification of guarantee of <a href="https://differentialprivacy.org/feed.xml#eq:eps:epsL">(1)</a>, but with \(0.9n\) instead of \(n\). <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Of course, we started with a local privacy guarantee, and ended up with a shuffle privacy guarantee: so the two are incomparable, and one has to interpret this “amplification” in that context. <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>You can here replace uniform by any known distribution \(\mathbf{q}\) of your choosing, that doesn’t change the question (and result), but uniform is nice. <a href="https://differentialprivacy.org/feed.xml#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>As long as one is happy with approximate DP. One can achieve that in pure DP as well, but it’s a bit more complicated <a href="https://arxiv.org/abs/2112.10032" title="Albert Cheu, Chao Yan. Pure Differential Privacy from Secure Intermediaries. arXiv 2021"><strong>[CY21]</strong></a>. <a href="https://differentialprivacy.org/feed.xml#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>This is not such a big assumption usually, and there are somewhat-general ways to get to that using a logarithmic factor in the number of users. <a href="https://differentialprivacy.org/feed.xml#fnref:6" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Clément Canonne <a href="https://differentialprivacy.org/privacy-doona/"><span class="datestr">at May 24, 2022 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/">IDEAL Workshop on High-Dimensional Geometry and Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 27, 2022 Northwestern University Mudd 3514 https://www.ideal.northwestern.edu/events/high-dimensional-analysis/ Speakers Ainesh Bakshi (Carnegie Mellon University), Arnold Filtser (Bar Ilan University), Weiyun Ma (Stanford University), Assaf Naor (Princeton University), Erik Waingarten (Stanford University) Logistics Dates: Friday, May 27 Location: Northwestern University Rooms: Mudd 3514 Streaming: Zoom Registration Link on website- https://www.ideal.northwestern.edu/events/high-dimensional-analysis/ PRELIMINARY Schedule All times are in … <a href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on High-Dimensional Geometry and Analysis</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/05/23/ideal-workshop-on-high-dimensional-geometry-and-analysis/"><span class="datestr">at May 23, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8334">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/">Why I am not a longtermist</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Apologies for yet another <a href="https://windowsontheory.org/category/philosophizing/">“philosophizing” </a>blog post, hope to get back to posts with more equations soon… Should also mention that one response to this piece was that “anyone who writes a piece called “Why I am not a longtermist” is probably more of a longtermist than 90% of the population</em>” <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> <em>–Boaz]</em></p>



<p>“<a href="https://en.wikipedia.org/wiki/Longtermism">Longtermism</a>” is a moral philosophy that places much more weight on the well-being of all future generations than on the current one. It <a href="https://www.effectivealtruism.org/articles/longtermism">holds</a> that “positively influencing the long-term future is a key moral priority of our time,” where “long term” can be <em>really</em> long term, e.g., “many thousands of years in the future, or much further still.”   At its core is the belief that each one of the potential quadrillion or more people that may exist in the future is as important as any single person today.</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img width="439" alt="" src="https://lh4.googleusercontent.com/JibhteUU2RwXMZCy3FU7bwjTqOYcNds8u29ny_iPP3aK5N-idd1vJtp4RKCi-Lv-7E7u3-nziFPYCNdD8C78f58X3CVM3HPbQoIsrPL3EvdLDNnzgeWuWpahl04cL9QMaG_lZVjcKL0G-r5s4w" height="596" /></figure></div>


<p>Longtermism has recently attracted attention, some of it in <a href="https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk">alarming tones</a>. The reasoning behind longtermism is natural: if we assume that human society will continue to exist for at least a few millennia, many more people will be born in the future than are alive today. However, since predictions are famously hard to make, especially about the future, longtermism invariably gets wrapped up with probabilities. Once you do these calculations, preventing an infinitely bad outcome, even if it would only happen with tiny probability, will have infinite utility. Hence longtermism tends to focus on so-called “existential risk”:  The risk that humanity will go through in an extinction event, like the one suffered by the Neanderthals or Dinosaurs, or another type of irreversible humanity-wise calamity.<br /></p>



<p>This post explains why I do not subscribe to this philosophy. Let me clarify that I am not saying that all longtermists are bad people. Many “longtermists” have given generously to improve people’s lives worldwide, particularly in developing countries. For example, none of the top charities of <a href="https://www.givewell.org/">Givewell</a> (an organization associated with the effective altruism movement, in which many prominent longtermists are members) focus on hypothetical future risks. Instead, they all deal with current pressing issues, including Malaria, childhood vaccinations, and extreme poverty. Overall the effective altruism movement has done much to benefit currently living people. Some of its members <a href="https://www.vox.com/future-perfect/2018/10/15/17962134/future-perfect-podcast-kidney-donation">donated their kidneys to strangers</a>: These are good people- morally better than me. It is hardly fair to fault people that are already contributing more than most others for caring about issues that I think are less significant.</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img width="506" alt="" src="https://lh5.googleusercontent.com/wGuGQ0ly7gKlSuGx80WXjdthvUrDt9ZSOgN5mma-p_dJ0_rQdjlSyAet9g5zLui8K6sw_uG8lzzd1VHb3l32QVEbNbj2wuzwLDi-VsH4Pn24RR-C3u2-9nB5-xWakbNP3tPlmgthzGBRQhsdfQ" height="496" /><a href="https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/">Benjamin Todd’s</a> estimates of Effective Altruism resource allocations</figure></div>


<p></p>



<p>This post critiques the philosophy of longtermism rather than the particular actions or beliefs of “longtermists.” In particular, the following are often highly correlated with one another:</p>



<ol><li>Belief in the philosophy of longtermism.</li><li>A belief that existential risk is not just a concern for the far-off future and a low-probability event, but there is a very significant chance of it happening in the near future (next few decades or at most a century).</li><li>A belief that the most significant existential risk could arise from artificial intelligence and that this is a real risk in the near future.</li></ol>



<p>Here I focus on (1) and explain why I disagree with this philosophy. While I might disagree on specific calculations of (2) and (3), I fully agree with the need to think and act regarding near-term risks.  Society tends to err on the side of being too myopic. We prepare too little even for risks that are not just predictable but are also predicted, including climate change, pandemics, nuclear conflict, and even software hacks. It is hard to motivate people to spend resources for safety when the outcome (bad event not happening) is invisible. It is also true that over the last decades, humanity’s technological capacities have grown so much that for the first time in history, we are capable of doing irreversible damage to our planet. </p>



<p>In addition to the above, I agree that we need to think carefully about the risks of any new technology, particularly one that, like artificial intelligence, can be very powerful but not fully understood.  Some AI risks are relevant to the shorter term: they are likely over the next decade or are already happening. There <a href="https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815">are</a> <a href="https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205">several</a> <a href="https://fairmlbook.org/">books</a> on these challenges. None of my critiques apply to such issues. At some point, I might write a separate blog post about artificial intelligence and its short and long-term risks. </p>



<p>My reasons for not personally being a “longtermist” are the following:</p>



<p><strong>The probabilities are too small to reason about.</strong></p>



<p>Physicists know that there is no point in writing a measurement up to 3 significant digits if your measurement device has only one-digit accuracy. Our ability to reason about events that are decades or more into the future is severely limited. At best, we could estimate probabilities up to an order of magnitude, and even that may be optimistic. Thus, claims such as <a href="https://www.existential-risk.org/concept.html">Nick Bostrom’s</a>, that <em>“the expected value of reducing existential risk by a mere one billionth of one billionth of one percentage point is worth a hundred billion times as much as a billion human lives”</em> make no sense to me.  This is especially the case since these “probabilities” are Bayesian, i.e., correspond to degrees of belief. If, for example, you evaluate the existential-risk probability by aggregating the responses of 1000 experts, then what one of these experts had for breakfast is likely to have an impact larger than 0.001 percent (which, according to Bostrom, would correspond to much more than 10²⁰ human lives). To the extent we can quantify existential risks in the far future, we can only say something like “extremely likely,” “possible,” or “can’t be ruled out.” Assigning numbers to such qualitative assessments is an exercise in futility. </p>



<p><strong>I cannot justify sacrificing current living humans for abstract probabilities.</strong></p>



<p>Related to the above, rather than focusing on specific, measurable risks (e.g., earthquakes, climate change), longtermism is often concerned with extremely hard to quantify risks. In truth, we cannot know what will happen 100 years into the future and what would be the impact of any particular technology. Even if our actions will have drastic consequences for future generations, the dependence of the impact on our choices is likely to be chaotic and unpredictable. To put things in perspective, many of the risks we are worried about today, including nuclear war, climate change, and AI safety, only emerged in the last century or decades. It is hard to underestimate our ability to predict even a decade into the future, let alone a century or more.</p>



<p>Given that there is so much suffering and need in the world right now, I cannot accept a philosophy that prioritizes abstract armchair calculations over actual living humans. (This concern is not entirely hypothetical: <a href="https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/">Greaves and MacAskill</a> estimate that $100 spent on AI safety would, in expectation, correspond to saving a <em>trillion lives</em> and hence would be “far more than the near-future benefits of bednet distribution [for preventing Malaria],” and recommend that it is better that individuals “fund AI safety rather than developing world poverty reduction.”)</p>



<p><a href="https://www.effectivealtruism.org/articles/longtermism">Moorhouse</a> compares future humans to ones living far away from us. He says that just like “something happening far away from us in space isn’t less intrinsically bad just because it’s far away,” we should care about humans in the far-off future as much as we care about present ones. But I think that we <em>should</em> care less about very far away events, especially if it’s so far away that we cannot observe them. E.g., as far as we know, there may well be trillions of sentient beings in the universe right now whose welfare can somehow be impacted by our actions. </p>



<p><strong>We cannot improve what we cannot measure.</strong></p>



<p>An inherent disadvantage of probabilities is that they are invisible until they occur. We have no direct way to measure whether a probability of an event X has increased or decreased. So, we cannot tell whether our efforts are working or not. The <a href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/">scientific revolution</a> involved moving from armchair philosophizing to making measurable predictions. I do not believe we can make meaningful progress without concrete goals. For some risks, we do have quantifiable goals (Carbon emissions, number of nuclear warheads). Still, there are significant challenges to finding a measurable proxy for very low-probability and far-off events. Hence, even if we accept that the risks are real and vital, I do not think we can directly do anything about them before finding such proxies. </p>



<p>Proxies do not have to be perfect: Theoretical computer science made much progress using the imperfect measure of worst-case asymptotic complexity. The same holds for machine learning and artificial benchmarks. It is enough that proxies encourage the generation of new ideas or technologies and achieve gradual improvement. One lesson from modern machine learning is that the objective (aka loss function) doesn’t have to perfectly match the task for it to be useful.</p>



<p><strong>Long-term risk mitigation can only succeed through short-term progress.</strong></p>



<p>Related to the above, I believe that addressing long-term risks can only be successful if it’s tied to shorter-term advances that have clear utility. For example, consider the following two extinction scenarios:</p>



<p>1. The actual Neanderthal extinction.<br />2. A potential human extinction 50 years from now due to total nuclear war.</p>



<p>I argue that the only realistic scenario to avoid extinction in both cases is a sequence of actions that improve some measurable outcome. While sometimes extinction could theoretically be avoided by a society making a huge sacrifice to eliminate a hypothetical scenario, this could never actually happen.</p>



<p>While the reasons for the Neanderthal extinction are not fully known, most researchers believe that Neanderthals were out-competed by our ancestors – modern humans – who had better tools and ways to organize society. The crucial point is that the approaches to prevent extinction for Neanderthal were the same ones to improve their lives in their current environment. They may not have been capable of doing so, but it wasn’t because they were working on the wrong problems.</p>



<p>Contrast this with the scenario of human extinction through total nuclear war. In such a case, our conventional approaches for keeping nuclear arms in check, such as international treaties and sanctions, have failed. Perhaps in hindsight, humanity’s optimum course of action would have been a permanent extension of the middle ages, stopping the scientific revolution from happening through restricting education, religious oppression, and vigorous burning-at-stake of scientists.  Or perhaps humanity could even now make a collective decision to go back and delete all traces of post 17th-century science and technology.</p>



<p>I cannot rule out the possibility that, in hindsight, one of those outcomes would have had more aggregate utility than our current trajectory. But even if this is the case, such an outcome is simply not possible. Humanity can not and will not halt its progress, and solutions to significant long-term problems have to arise as a sequence of solutions to shorter-range measurable ones, each of which shows positive progress. Our only hope to avoid a total nuclear war is through piecemeal quantifiable progress.  We need to use diplomacy, international cooperation, and monitoring technologies to reduce the world’s nuclear arsenal one warhead at a time. This piecemeal, incremental approach may or may not work, but it’s the only one we have. </p>



<p><strong>Summary: think of the long term, but act and measure in the short term.</strong></p>



<p>It is appropriate for philosophers to speculate on hypothetical scenarios centuries into the future and wonder whether actions we take today could influence them. However, I do not believe such an approach will, in practice, lead to a positive impact on humanity and, if taken to the extreme, may even have negative repercussions. We should maintain epistemic humility. Statements about probabilities involving fractions of percentage points, or human lives in the trillions, should raise alarm bells. Such calculations can be particularly problematic since they can lead to a “the end justifies the means” attitude, which can accept any harm to currently living people in the name of the practically infinite multitudes of future hypothetical beings. </p>



<p>We need to maintain the invariant that, even if motivated by the far-off future, our actions “first do no harm” to living, breathing humans. Indeed, as I mentioned, even longtermists don’t wake up every morning thinking about how to reduce the chance that something terrible happens in the year 1,000,000 AD by 0.001%. Instead, many longtermists care about particular risks because they believe these risks are likely in the near-term future. If you manage to make a convincing case that humanity faces a real chance of near-term total destruction, then most people would agree that this is very very bad, and we should act to prevent it. It doesn’t matter whether humanity’s extinction is two times or a zillion times worse than the death of half the world’s population. Talking about trillions of hypothetical beings thousands of years into the future only turns people off. There is a reason that <a href="https://en.wikipedia.org/wiki/Pascal%27s_wager">Pascal’s Wager</a> is not such a winning argument, and I have yet to meet someone who converted to a particular religion because it had the grisliest version of hell.</p>



<p>This does not mean that thinking and preparing for longer-term risks is pointless. Maintaining seed banks, monitoring asteroids, researching pathogens, designing vaccine platforms, and working toward nuclear disarmament, are all essential activities that society should take. Whenever a new technology emerges, artificial intelligence included, it is crucial to consider how it can be misused or lead to unintended consequences. By no means do I argue that humanity should spend all of its resources only on actions that have a direct economic benefit. Indeed, the whole enterprise of basic science is built on pursuing directions that, in the short term, increase our knowledge but do not have practical utility. Progress is not measured only in dollars, but it should be measured somehow. Epistemic humility also means that we should be content with working on direct, measurable proxies, even if they are not perfect matches for the risk at hand. For example, the probability of extinction via total nuclear war might not be a direct function of the number of deployed nuclear warheads. However, the latter is still a pretty good proxy for it.</p>



<p>Similarly, even if you are genuinely worried about long-term risk, I suggest you spend most of your time in the present. Try to think of short-term problems whose solutions can be verified, which might advance the long-term goal. A “problem” does not have to be practical: it can be a mathematical question, a computational challenge, or an empirically verifiable prediction. The advantage is that even if the long-term risk stays hypothetical or the short-term problem turns out to be irrelevant to it, you have still made measurable progress. As <a href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/">has happened before</a>, to make actual progress on solving existential risk, the topic needs to move from philosophy books and blog discussions into empirical experiments and concrete measures.</p>



<p><strong>Acknowledgments:</strong> Thanks to Scott Aaronson and Ben Edelman for commenting on an earlier version of this post.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/"><span class="datestr">at May 23, 2022 02:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20050">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">Hilbert’s Lost Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<i>Mathematics consists in proving the most obvious thing in the least obvious way—Pólya.</i><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/285px-david_hilbert_1886/" rel="attachment wp-att-20070"><img width="130" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/285px-David_Hilbert_1886.jpg?resize=130%2C170&amp;ssl=1" class="alignright wp-image-20070" height="170" /></a></p>
<p>
David Hilbert famously presented 23 important open mathematical problems at the International Congress of Mathematicians in Paris in 1900. He intended to include a 24th on the simplicity of proofs.</p>
<p>
Today we discuss this problem, which was “lost” until the historian Rüdiger Thiele discovered it in Hilbert’s papers in 2000 and brought it to light in a <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=C970003BC10AB452B346D45B8AAE14B6?doi=10.1.1.97.7633&amp;rep=rep1&amp;type=pdf">paper</a> for the American Mathematical Monthly.</p>
<p>
I (Ken) have long wondered why Hilbert targeted the number 23 for his list. That some of his <a href="https://en.wikipedia.org/wiki/Hilbert's_problems">problems</a> have broad, vague statements tells me he could have altered their number. To be sure, he discussed only 10 in his actual 1900 lecture; the rest appeared in his long <a href="http://www.ams.org/journals/bull/1902-08-10/S0002-9904-1902-00923-3/S0002-9904-1902-00923-3.pdf">article</a> that year. Until Dick drafted this post and I found Thiele’s article, I had not known about a No. 24—a nice factorial number. </p>
<p>
As <a href="https://royalsocietypublishing.org/doi/epdf/10.1098/rsta.2018.0040">discussed</a> by Inês Hipolito and Reinhard Kahle, in a <a href="https://royalsocietypublishing.org/toc/rsta/2019/377/2140">special issue</a> of the UK Royal Society <em>Philosophical Transactions A</em> devoted to Hilbert’s 24th, there were only faint traces of it until Thiele’s discovery. They decline to guess Hilbert’s motives for leaving it out, but I venture this: Hilbert could not agree with himself on the terms by which to state it, even broadly. How to quantify simplicity was already the subject of Dick’s draft, into which we blend.</p>
<p>
</p><p></p><h2> Pólya on Proofs </h2><p></p>
<p></p><p>
One simple question is whether a human notion of simpler proof would accord with, say, that of a Martian. George Pólya was one of a vanguard of Hungarian emigré scientists who came to be called <a href="https://en.wikipedia.org/wiki/The_Martians_(scientists)">The Martians</a> after another of them, the physicist Leo Szilard, gave this answer to Enrico Fermi’s question of why aliens had not been detected when life in the universe should be plentiful:</p>
<blockquote><p>
“They are among us, but they call themselves Hungarians.”
</p></blockquote>
<p>
Kidding aside, this could set up a <a href="https://en.wikipedia.org/wiki/Linguistic_relativity">Sapir-Whorf</a> type hypothesis that humans from disparate linguistic groups and mathematical cultures could have different values for proofs. Even if so, the joke would be on non-Hungarians, because Paul Erdős, whose idea of “Proofs from The Book” set the internationally recognized standard of proof elegance, was also a “Martian.”</p>
<p>
Pólya’s quote above hits the nail on the head. For an example, consider the <a href="https://www.math.utah.edu/~alfeld/math/q1.html">theorem</a> that: </p>
<blockquote><p><b>Theorem 1</b> <em> The square root of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> is irrational. </em></p></blockquote>
<p>
The well-known proof begins by supposing not: </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/proof-6/" rel="attachment wp-att-20055"><img width="450" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/05/proof.png?resize=450%2C460&amp;ssl=1" class="aligncenter wp-image-20055" height="460" /></a></p>
<p>
One might claim that this famous proof is not obvious—do you agree?</p>
<p>
</p><p></p><h2> A Measure </h2><p></p>
<p></p><p>
Now consider the general task of proving statements of this form:</p>
<blockquote><p><b>Theorem 2</b> <em><a name="pos"></a> For all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28n%29+%3E+0.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(n) &gt; 0. " class="latex" /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
Think this as an encoding of an interesting property: For example, for each <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> there is a prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%3En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p&gt;n}" class="latex" />. Or for each planar graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> there is some four coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /></p>
<p>
To make this more concrete, let us base proofs on the Peano axioms for arithmetic, and suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f}" class="latex" /> is well-defined in this system. Note that proving <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f}" class="latex" /> to be total computable is not the same as proving that all its values are <em>positive</em> as Theorem <a href="https://rjlipton.wpcomstaging.com/feed/#pos">2</a> asserts. In case Theorem <a href="https://rjlipton.wpcomstaging.com/feed/#pos">2</a> is provable, we have two key parameters:</p>
<li>D: The length of the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(n)}" class="latex" />.
</li><li>P: The length of the full proof of Theorem <a href="https://rjlipton.wpcomstaging.com/feed/#pos">2</a>.
<p></p><p><br />
Our proof simplicity measure is then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%3D%5Cfrac%7BP%7D%7BD%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  M=\frac{P}{D} " class="latex" /></p>
<p></p><h2> Some Instances </h2><p></p>
<p></p><p>
Here are some keys to understanding the value <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> intuitively:</p>
<ol>
<li> For known provable problems we might have that <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> is not too large
</li><li> For classic problems like the Four-Color problem we might have that <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> is huge. Think of the proof that arises in this case.
</li><li> For many chess positions where one side can win, say checkmate in 30 moves, the proof of that is likewise huge. Crafting chess problems where the proof is both crisp and hard to find is a centuries-old art.
</li><li> For open problems, <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> is currently infinite.
</li><li> But for practical software we get something bounded. The trouble here is that <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> is potentially huge and could be the size of <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" />.
</li></ol>
<p>
Consider for the last case a program that calculates your tax payment based on the US tax code, for example. The code has wording like:  </p>
<blockquote><p>
An apparently insignificant discrepancy between the wording in the Statutes at Large and the wording in some editions of the U.S. Code with respect to a provision of the Internal Revenue Code (specifically, the words “any papers” in the first sentence of 6104(a)(1)(A) is described by the U.S. Court of Appeals for the District of Columbia in the case of Tax Analysts v. Internal Revenue Serv., 214 F.3d 179 (D.C. Cir. 2000), in footnote 1. According to the Court, some versions of the U.S. Code show the text as “any paper” while the Statutes at Large version shows the text as “any papers.”
</p></blockquote>
<p>
Thus the proof <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> could be the same size of <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" />. Thus,</p>
<p>
Case: <img src="https://s0.wp.com/latex.php?latex=%7BP+%3E%3E+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P &gt;&gt; D}" class="latex" />. This is the usual proof of a classic problem like the four-color problem. Huge proof. </p>
<p>
Case: <img src="https://s0.wp.com/latex.php?latex=%7BP+%5Capprox+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P \approx D}" class="latex" />. This is like a simple problem or like a proof of a complex program with tons of bits in its description. </p>
<p>
</p><p></p><h2> Proofs and Software </h2><p></p>
<p></p><p>
My famous—infamous?—<a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">paper</a> with Alan Perlis and Rich de Millo raised the related issue in the first sentence of our abstract:</p>
<blockquote>
<p>
It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics.</p>
</blockquote>
<p>
This argument made over four decades ago has been argued by critics as being wrong. Take a look at <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">this</a> for comments about the recent <a href="https://scp.cc.gatech.edu/2021/05/26/debate-that-changed-programming-living-history/">debate</a> on the paper moderated by Harry Lewis.</p>
<p>
We claim that we were correct but for a different reason that we made in the original paper. We got it <b>right</b> but for the wrong reason. The real reason is alluded to in this <a href="http://www.cs.ox.ac.uk/publications/publication8316-abstract.html">statement</a> by Tony Hoare, whose programme of proofs for programs partly spurred our paper:</p>
<blockquote>
<p>
Ten years ago, researchers into formal methods (and I was the most mistaken among them) predicted that the programming world would embrace with gratitude every assistance promised by formalisation to solve the problems of reliability that arise when programs get large and more safety-critical. Programs have now got very large and very critical—well beyond the scale which can be comfortably tackled by formal methods. … It has turned out that the world just does not suffer significantly from the kind of problem that our research was originally intended to solve.
</p></blockquote>
<p>
See also his 1996 <a href="http://176.9.41.242/docs/math/1996-hoare.pdf">position paper</a>, “How Did Software Get So Reliable Without Proof?”</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Does this measure <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> shed some light on the problems of program verification? Is it useful? Would it have satisfied Hilbert? Hilbert’s handwritten note discovered by Thiele showed him thinking in terms of proofs via polynomial invariants and equations—hence maybe not at a brute level of counting symbols.</p>
<p></p><p><br />
[some format fixes, young Hilbert picture]</p></li></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/"><span class="datestr">at May 23, 2022 11:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
