<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at December 01, 2019 06:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/">Tropical Geometry, Berkovich Spaces, Arithmetic D-Modules and p-adic Local Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 15-19, 2020 Imperial College of London (UK) https://www-fourier.ujf-grenoble.fr/~pulitaa/Imperial-Conference/Imperial-Conference.html Registration deadline: March 31, 2020 With this workshop we would like to promote the interaction between the following five fields: Berkovich spaces Tropical geometry p-adic differential equations Arithmetic D-modules and representations of p-adic Lie groups Arithmetic applications of p-adic local systems While the first two are … <a href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/" class="more-link">Continue reading <span class="screen-reader-text">Tropical Geometry, Berkovich Spaces, Arithmetic D-Modules and p-adic Local Systems</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/"><span class="datestr">at December 01, 2019 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5474">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2019/12/01/teenagers-doing-mathematical-research/">Teenagers doing Mathematical Research</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I’d like to ramble about another program that our group is running. We are searching for unusually promising high-school-age students and mentor each in a serious research project. We started doing this already before our REU program. However, I never advertised this program before, because I felt that we were still learning how to do […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2019/12/01/teenagers-doing-mathematical-research/"><span class="datestr">at December 01, 2019 03:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/173">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/173">TR19-173 |  Extractor Lower Bounds, Revisited | 

	Divesh Aggarwal, 

	Siyao  Guo, 

	Maciej Obremski, 

	Joao Ribeiro, 

	Noah Stephens-Davidowitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We revisit the fundamental problem of determining seed length lower bounds for strong extractors and natural variants thereof. These variants stem from a ``change in quantifiers'' over the seeds of the extractor: While a strong extractor requires that the average output bias (over all seeds) is small for all input sources with sufficient min-entropy, a somewhere extractor only requires that there exists a seed whose output bias is small. More generally, we study what we call probable extractors, which on input a source with sufficient min-entropy guarantee that a large enough fraction of seeds have small enough associated output bias. Such extractors have played a key role in many constructions of pseudorandom objects, though they are often defined implicitly and have not been studied extensively.

Prior known techniques fail to yield good seed length lower bounds when applied to the variants above. Our novel approach yields significantly improved lower bounds for somewhere and probable extractors. To complement this, we construct a somewhere extractor that implies our lower bound for such functions is tight in the high min-entropy regime. Surprisingly, this means that a random function is far from an optimal somewhere extractor in this regime. The techniques that we develop also yield an alternative, simpler proof of the celebrated optimal lower bound for strong extractors originally due to Radhakrishnan and Ta-Shma (SIAM J. Discrete Math., 2000).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/173"><span class="datestr">at December 01, 2019 02:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Hoffman%27s_packing_puzzle">Hoffman’s packing puzzle, and its connection to the inequality of arithmetic and geometric means</a> (<a href="https://mathstodon.xyz/@11011110/103151390413241726"></a>). The one I have is not quite so colorful as the illustration for this new Wikipedia article. My father-in-law made it for me some 30 years ago; you can see it in a corner of the photo at <a href="https://11011110.github.io/blog/2018/05/17/book-arrival.html">this post</a>. I don’t unpack it very often, though, because I lost track of the handwritten table of solutions that I made when I first got it and it’s quite difficult to re-pack.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/iran-internet-shutoff/">How the Iranian government shut off the internet</a> (<a href="https://mathstodon.xyz/@11011110/103155401533962129"></a>). According to this story, they have effected “a near-total internet and mobile data blackout” in an attempt to quell gasoline-price protests.</p>
  </li>
  <li>
    <p><a href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/">A Market for TCS Papers??</a> (<a href="https://mathstodon.xyz/@11011110/103162041586643628"></a>, with Vijay Vazirani, on the “Turing’s Invisible Hand” blog.) The current situation with theoretical computer science conference reviewing is a mess of long publication delays and reviewer overload caused by repeated submissions and rejections. Vijay and I argue that it should instead be treated as a matching market with pooled submissions and stable matching, getting better results for less time and effort.</p>
  </li>
  <li>
    <p><a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda20-accepted-papers">SODA 2020 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/103167433760168313"></a>). It only lists titles and authors, but if you notice a title you find intriguing you can find find more detail elsewhere. However, this depends on avoiding obscure titles; if, say, you found a breakthrough on clustered planarity showing that it’s in polynomial time, but you titled your paper “Atomic Embeddability, Clustered Planarity, and Thickenability”, others might not notice.</p>
  </li>
  <li>
    <p>Did you know that <a href="https://en.wikipedia.org/wiki/William_Chapple_(surveyor)">William Chapple</a> (<a href="https://mathstodon.xyz/@11011110/103174753258552519"></a>) discovered Euler’s formula for circumcenter-incenter distance before Euler, Poncelet’s porism on families of triangles inscribed and circumscribed by the same two circles before Poncelet, and was the first to publish a proof that Euclid missed, on the existence of orthocenters of triangles? Did you know that a street in Witheridge is named for him? Have you even heard of William Chapple before? Or Witheridge? Now you have.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/21/portal-icosahedron-sculpture-l.html">Portal Icosahedron by Anthony James</a> (<a href="https://mathstodon.xyz/@11011110/103177399066941274"></a>). An icosahedral frame, infinity mirrors, and LED lighting create a view into an infinite icosahedral grid, creating an effect that, in the jargon of the art world, “is both esoteric and industrial, orphic and distinctly concrete”. Whatever that’s supposed to mean.</p>
  </li>
  <li>
    <p><a href="https://www.theregister.co.uk/2019/11/20/org_registry_sale_shambles/">Get ready to change all of your bookmarks for non-profit organizations</a> (<a href="https://mathstodon.xyz/@11011110/103186257585749674"></a>, <a href="https://www.metafilter.com/184269/Seems-bad">via</a>) as the top-level .org domain name registry is sold to profiteers, drops its own non-profit status, and eliminates price caps on domain name renewals.</p>
  </li>
  <li>
    <p><a href="ttp://thelaborastory.com/stories/professor-ian-wanless-eliyahu-rips/">Ian Wanless on mathematician Eliyahu Rips and his Ig Nobel Prize for Literature</a> (<a href="https://mathstodon.xyz/@11011110/103189764743270099"></a>). An entertaining general-audience talk; audio only.</p>
  </li>
  <li>
    <p><a href="https://math.indiana.edu/research/gallery/tree.html">Charles Darwin’s first drawing of an evolutionary tree</a> (<a href="https://mathstodon.xyz/@11011110/103196622232243519"></a>).</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/24/bechdelgrams-are-beautiful.html">Bechdelgrams illustrate of whether a movie passes the Bechdel test</a> (<a href="https://mathstodon.xyz/@11011110/103208740086306490"></a>). A nice use of color to highlight the information you’re looking for in a social network: Here, the network consists of interactions between characters in a film, and the women and conversations not about men are given distinctive colors to show the test criteria: does the film have at least two named female characters, who speak to each other, about something other than men?</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/27/paper-sculptures-of-microorgan.html">Rogan Brown creates intricate paper sculptures inspired by microorganisms</a> (<a href="https://mathstodon.xyz/@11011110/103211347688747567"></a>).</p>
  </li>
  <li>
    <p>Some recent open-access conference proceedings (<a href="https://mathstodon.xyz/@11011110/103222522284012005"></a>): <a href="http://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16123">27th European Symp. on Algorithms (ESA)</a>; <a href="http://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16131">30th Int. Symp. on Algorithms and Computation (ISAAC)</a>; <a href="http://www.jcdcgg.u-tokai.ac.jp/JCDCG3_2019_abstracts_v1.pdf">22nd Japan Conf. on Discrete and Computational Geometry, Graphs, and Games (JCDCGGG)</a>. JCDCGGG is not very selective (think CCCG but more so), but I have <a href="https://erikdemaine.org/papers/MinimalUnunfoldable_JCDCGGG2019/">a paper there with several co-authors on ununfoldable polyhedra with few vertices</a>.</p>
  </li>
  <li>
    <p><a href="https://slate.com/technology/2019/11/nefertiti-bust-neues-museum-3d-printing.html">The Nefertiti bust meets the 21st century</a> (<a href="https://mathstodon.xyz/@11011110/103228129647767519"></a>, <a href="https://news.ycombinator.com/item?id=21670786">via</a>). Interesting essay on claims of intellectual property on ancient artifacts (in this case a high-resolution 3d scan of a bust of Nefertiti), clearly invalid under both US law and still-being-implemented EU law and “dangerously close to committing copy fraud”.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/30/linkage.html"><span class="datestr">at November 30, 2019 10:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/">Postdoc at University of Waterloo (apply by December 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms &amp; Complexity group at the University of Waterloo is offering one postdoctoral position starting in the Fall of 2020. We seek candidates from all areas of TCS.<br />
Interested applicants should have their CV, research statement, and three recommendation letters emailed to theory.waterloo@gmail.com. Applications are due December 31st.<br />
Questions should be sent to the email above.</p>
<p>Website: <a href="https://algcomp.uwaterloo.ca/#MoreInfo">https://algcomp.uwaterloo.ca/#MoreInfo</a><br />
Email: theory.waterloo@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/"><span class="datestr">at November 29, 2019 08:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/">Predicating Predictivity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>Plus predicaments of error modeling</em><br />
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/spiegelhalterbacon/" rel="attachment wp-att-16422"><img src="https://rjlipton.files.wordpress.com/2019/11/spiegelhalterbacon.jpg?w=200&amp;h=168" alt="" width="200" class="alignright wp-image-16422" height="168" /></a></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Bacon Sandwich <a href="https://www.youtube.com/watch?v=4szyEbU94ig">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Sir David Spiegelhalter is a British statistician. He is a strong voice for the public understanding of statistics. His work extends to all walks of life, including <a href="https://www.regulation.org.uk/library/2017-Spiegelhalter-Risk_and_Uncertainty_Communication.pdf">risk</a>, <a href="https://understandinguncertainty.org/coincidences">coincidences</a>, <a href="https://www.spectator.co.uk/2019/04/i-could-have-stopped-harold-shipmans-killing-spree-and-saved-175-lives/">murder</a>, and <a href="https://www.ft.com/content/f8793aaa-dfa1-11e4-a06a-00144feab7de">sex</a>.</p>
<p>
Today we talk about extending one of his inventions.</p>
<p>
His invention has to do with grading the performance of people and models that make predictions. A <b>scoring rule</b> grades how often predictions are right. But it may not tell how difficult the situations are. It is easy to look good with predictions when they start with a high chance of success. A weather forecaster predicting sunny-versus-rainy will be right more often in Las Vegas than in Boston. Quoting this FiveThirtyEight <a href="https://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/">item</a>:</p>
<blockquote><p><b> </b> <em> If you want to have an easy life as a weather forecaster, you should get a job in Las Vegas, Phoenix or Los Angeles. Predict that it won’t rain in one of those cities, and you’ll be right about 90 percent of the time. </em>
</p></blockquote>
<p></p><p>
In a 1986 <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780050506">paper</a>, for a particular scoring rule <a href="https://www.semanticscholar.org/paper/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF-Brier/feee6551179612b9691f021b583d8a99b81b9b86">defined</a> by Glenn Brier in 1950, Spiegelhalter worked out how to equalize the forecaster grading. He applied his <b>Z-test</b> not to weather as Brier was concerned with but to medical prognoses and clinical trials. </p>
<p>
What I am doing with a small group of graduate students in Buffalo is trying to turn Spiegelhalter’s kind of Z-test around once more. If a forecaster fares poorly, we will try to flag not the model but the behavior of the subjects being modeled. In weather we would want to tell when Mother Nature, not the models, has gone off the rails. Well, we are actually looking for ways to tell when a human being has left the bounds of human predictability for reasons that are inhuman—such as cheating with a computer at chess. And maybe it can shed more light on whether our computers can possibly “cheat” with quantum mechanics.</p>
<p>
</p><p></p><h2> Prediction Scores </h2><p></p>
<p></p><p>
Let’s consider situations <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> in which the number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell = \ell_t}" class="latex" title="{\ell = \ell_t}" /> is usually more than <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, that is, usually more than “rain” or “no rain.” The forecaster lays down projections <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bq%7D_t+%3D+%28q_1%2C%5Cdots%2Cq_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}" class="latex" title="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}" /> for the chance of each outcome. If outcome <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> happens, then the <em>Brier score</em> for that forecast is <a name="Brier"></a></p><a name="Brier">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+%281+-+q_r%29%5E2+%2B+%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)" class="latex" title="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)" /></p>
</a><p><a name="Brier"></a> If the forecaster was certain that <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> would happen and so put <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r = 1}" class="latex" title="{q_r = 1}" />, all other <img src="https://s0.wp.com/latex.php?latex=%7Bq_j+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_j = 0}" class="latex" title="{q_j = 0}" />, then the score would be zero. Thus lower is better for the Brier score. </p>
<p>
If you put probability <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r &lt; 1}" class="latex" title="{q_r &lt; 1}" /> on the outcome that happened, then you get penalized both for the difference and for the remaining probability which you put on outcomes that did not happen. It is possible to <em>decompose</em> the score in another way that changes the emphasis: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+1+%2B+Q+-+2q_r+%5Cqquad%5Ctext%7Bwhere%7D%5Cqquad+Q+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+q_j%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. " class="latex" title="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. " /></p>
<p>
Then <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> is a fixed measure of how you spread your forecasts around, while all the variability in your score comes from how much stock you placed in the outcome that happened. The worst case is having put <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r = 0}" class="latex" title="{q_r = 0}" />, whereupon your Brier penalty is <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. </p>
<p>
We would like our forecasts always to be perfect, but reality gives us situations that are inherently nondeterministic—with unknown “true probabilities” <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D_t+%3D+%28p_1%2C%5Cdots%2Cp_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{p}_t = (p_1,\dots,p_\ell)}" class="latex" title="{\vec{p}_t = (p_1,\dots,p_\ell)}" />. The vital point is that the forecaster should not try to hit <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+r_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r = r_t}" class="latex" title="{r = r_t}" /> on the nose at every time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> but rather to match the true probabilities. Once we postulate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{p}}" class="latex" title="{\vec{p}}" />, the <em>expected Brier score</em> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+B%5E%7B%5Cvec%7Bq%7D%7D%28i%29%5C%5C+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+%281+-+2q_i+%2B+Q%29%5C%5C+%26%3D%26+1+%2B+Q+-+2%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+q_i.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} " /></p>
<p>This is uniquely minimized by setting <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i}" class="latex" title="{q_i = p_i}" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />, which defines <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> as a <b>strictly proper</b> scoring rule. Without the second term <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{j \neq r} q_j^2}" class="latex" title="{\sum_{j \neq r} q_j^2}" /> in (<a href="https://rjlipton.wordpress.com/feed/#Brier">1</a>) the rule would not be proper for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3E+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell &gt; 2}" class="latex" title="{\ell &gt; 2}" />. When <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" />, <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> becomes equal to <img src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+p_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P = \sum_{j=1}^\ell p_j^2}" class="latex" title="{P = \sum_{j=1}^\ell p_j^2}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> represents an unavoidable prediction penalty from the intrinsic variance. If all <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> are equal, <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = \frac{1}{\ell}}" class="latex" title="{p_i = \frac{1}{\ell}}" />, then the expected score cannot be less than <img src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 - \frac{1}{\ell}}" class="latex" title="{1 - \frac{1}{\ell}}" />. </p>
<p>
A second example, the log-likelihood prediction scoring rule, is in the original longer <a href="https://cse.buffalo.edu/~regan/GLL/wspiegelhalterLong.pdf">draft</a> of this post.</p>
<p></p><h2> Spiegelhalter’s Z </h2><p></p>
<p></p><p>
Spiegelhalter’s <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" />-score neatly drops out the unavoidable penalty term by taking the difference of the score with the expectation. Schematically it is defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D%5BB%5D+%3D+%5Cfrac%7BB+-+%5Cmathsf%7BE%7D%5BB%5D%7D%7B%5Csqrt%7B%5Cmathsf%7BVar%7D%5BB%5D%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, " class="latex" title="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BVar%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Var}[B]}" class="latex" title="{\mathsf{Var}[B]}" /> means the projected variance <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E2%5D+-+%28%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}" class="latex" title="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}" />. However, here is where it is important to notate the whole series of forecasting situations <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%2C%5Cdots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t = 1,\dots,T}" class="latex" title="{t = 1,\dots,T}" /> with outcomes <img src="https://s0.wp.com/latex.php?latex=%7Br_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r_t}" class="latex" title="{r_t}" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. The actual statistic is <a name="ZB"></a></p><a name="ZB">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+B%5E%7B%5Cvec%7Bq%7D_t%7D%28r_t%29+-+%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)" class="latex" title="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)" /></p>
</a><p><a name="ZB"></a> The denominator presumes that the forecast situations are independent so that the variances add. The numerator expands to be </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%282%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+p_%7Bi%2Ct%7D+q_%7Bi%2Ct%7D%5Cright%29+-+2q_%7Br%2Ct%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. " class="latex" title="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. " /></p>
<p>
The original application is a confidence test of the “null hypothesis” that the projections <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q}}" class="latex" title="{\vec{q}}" /> are good. Thus we plug in <img src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%2Ct%7D+%3D+q_%7Bi%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_{i,t} = q_{i,t}}" class="latex" title="{p_{i,t} = q_{i,t}}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> so that we test </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+2%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+q_%7Bi%2Ct%7D%5E2+%5Cright%29+-+q_%7Br%2Ct%7D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bq%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. " class="latex" title="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. " /></p>
<p>
To illustrate, suppose we do ten independent trials of an event with four outcomes whose true probabilities are <img src="https://s0.wp.com/latex.php?latex=%7B%280.1%2C0.2%2C0.3%2C0.4%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(0.1,0.2,0.3,0.4)}" class="latex" title="{(0.1,0.2,0.3,0.4)}" />. The sum in parentheses is <img src="https://s0.wp.com/latex.php?latex=%7B10%280.01+%2B+0.04+%2B+0.09+%2B+0.16%29+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}" class="latex" title="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}" />. If the outcomes conform exactly to these probabilities then <img src="https://s0.wp.com/latex.php?latex=%7Bq_%7Br%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_{r,t}}" class="latex" title="{q_{r,t}}" /> equals <img src="https://s0.wp.com/latex.php?latex=%7B0.1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.1}" class="latex" title="{0.1}" /> once, <img src="https://s0.wp.com/latex.php?latex=%7B0.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.2}" class="latex" title="{0.2}" /> twice, <img src="https://s0.wp.com/latex.php?latex=%7B0.3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.3}" class="latex" title="{0.3}" /> three times, and <img src="https://s0.wp.com/latex.php?latex=%7B0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.4}" class="latex" title="{0.4}" /> four times. This exactly cancels the <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}" class="latex" title="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}" />, as expected. Most trials will give a nonzero numerator, but in the long run, the numerator divided by <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> tends toward zero and the denominator scales to match it, thus keeping the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-statistic normally distributed.</p>
<p>
A high <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />, on the other hand—highly positive or highly negative—indicates that the forecasting is way off. That (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>) is an aggregate statistic over independent trials justifies treating the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-values as <a href="https://en.wikipedia.org/wiki/Standard_score">standard</a> <a href="https://en.wikipedia.org/wiki/Z-test">scores</a>. This applies also to <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-tests made similarly from other scoring rules besides the Brier score. The test thus becomes a verdict on the model. High <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-values on certain subsets of the data may reveal biases. </p>
<p>
Our idea is the opposite. Suppose we know that the forecasts are true, or suppose they have biases that are known and correctable over moderately large data sets. We may then be able to fit <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}[B]}" class="latex" title="{\mathsf{Z}[B]}" /> as an unbiased estimator (of zero) over large training sets. Then it can become a judgment of whether the data has become unnatural. </p>
<p>
</p><p></p><h2> Why This Z? </h2><p></p>
<p></p><p>
As I have detailed in <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">numerous</a> <a href="https://rjlipton.wordpress.com/2013/09/17/littlewoods-law/">posts</a> <a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/">on</a> <a href="https://rjlipton.wordpress.com/2013/07/27/thirteen-sigma/">this</a> <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">blog</a>, my system for detecting cheating with computers at chess already provides several statistical <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" />-scores. Why would I want another one?</p>
<p>
The motive involves the presence of multiple strong chess-playing programs, each with its own quirks and distribution of values for moves. They are used in two different ways:</p>
<ol>
<li>
As inputs telling the relative values <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i}" class="latex" title="{v_i}" /> of moves <img src="https://s0.wp.com/latex.php?latex=%7Bm_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m_i}" class="latex" title="{m_i}" />, which my model converts into its probability projections <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. <p></p>
</li><li>
As output predicates telling how often the player chose the move recommended by a specific program and/or quantifying the magnitude of error for different played moves.
</li></ol>
<p>
Having multiple engines helps point 1. My intent to blend the <em>values</em> <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i}" class="latex" title="{v_i}" /> from different engines has been blunted by issues I discussed <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">here</a>.  Thus I now have to train my model separately (and expensively) for each (new version of each) program. I can then blend the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />, but point 2 still remains at issue: My tests measure concordance with a specific program. Originally the program Rybka 3 was primary and Houdini 4B secondary. Now Stockfish 7 is primary and Komodo 10.0 secondary—until I update to their latest versions. The second engine is supposed to confirm a positive result from the first one.  This already means that my model is not trying to detect exactly which program was used.</p>
<p>
Nevertheless, my results often vary between testing engines. The engines <a href="https://rjlipton.wordpress.com/2014/12/28/the-new-chess-world-champion/">compete</a> against each other and may be crafted to disagree on certain kinds of moves. They agree with each other barely 75–80% in my tests. I would like to factor these differences out. </p>
<p>
The Spiegelhalter <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-test appeals because its reference is not to a particular chess program, but to the prediction quality of my model itself—which per point 1 can be informed by many programs in concert. It gives a way to <em>predicate predictivity</em>. A high value will attest that the sequence of played moves falls outside the range of predictability for human players of the same rated skill level. </p>
<p>
</p><p></p><h2> The Method </h2><p></p>
<p></p><p>
To harness <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}[F]}" class="latex" title="{\mathsf{Z}[F]}" /> for some scoring rule <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, we need to quantify the nature of my model’s <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> projections. In fact, my model has a clear bias toward conservatism in judging the frequency of particular non-optimal moves. This is discussed in my August <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">post</a> on my model upgrade and shown graphically in an appended <a href="https://cse.buffalo.edu/~regan/chess/computer/ModelTradeoffs.png">note</a> on why the conservative setting of a “gradient” parameter is needed to preserve dynamical stability. The fitting offsets this in a way that creates an opposite bias elsewhere. I hope to correct both biases at the same stroke by a specific means of modeling how the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> err with respect to the postulated true probabilities <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" />.</p>
<p>
We postulate an original source of error terms <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> all <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d.</a> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{N}(0,\delta^2)}" class="latex" title="{\mathcal{N}(0,\delta^2)}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> governs the magnitude of Gaussian noise. This noise can be <em>transformed</em> and related in various ways, e.g.:</p>
<ol>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i \pm \epsilon_i}" class="latex" title="{q_i = p_i \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i(1 \pm \epsilon_i)}" class="latex" title="{q_i = p_i(1 \pm \epsilon_i)}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bq_i%7D+%3D+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}" class="latex" title="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}" class="latex" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}" class="latex" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cln%28%5Cfrac%7Bq_i%7D%7B1+-+q_i%7D%29+%3D+%5Cln%28%5Cfrac%7Bp_i%7D%7B1+-+p_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}" class="latex" title="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}" />.
</li></ol>
<p>
There are further forms to consider and it is not yet clear from data within my model which one most applies. We would be interested in examples where these representations have been employed and in observations about their natures. </p>
<p>
Given the error terms, we can write each <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> as a function of <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" />. One issue is having at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell-1}" class="latex" title="{\ell-1}" /> degrees of freedom among <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_1%2C%5Cdots%2C%5Cepsilon_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_1,\dots,\epsilon_\ell}" class="latex" title="{\epsilon_1,\dots,\epsilon_\ell}" />, owing to the constraint that the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> as well as <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> sum to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. We handle this by choosing some fixed <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> as the “pivot” and using the constraints to eliminate <img src="https://s0.wp.com/latex.php?latex=%7Bp_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_k}" class="latex" title="{p_k}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bq_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_k}" class="latex" title="{q_k}" />, leaving the other error terms free. In all cases, the proposed method of defining what we notate as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" /> is:</p>
<ul>
<li>
Substitute the terms with <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%2C%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i,\epsilon_i}" class="latex" title="{q_i,\epsilon_i}" /> for each free <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> into <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BF%5E%7B%5Cvec%7Bq%7D%7D%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}" class="latex" title="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}" />. <p></p>
</li><li>
Compute the expectation over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}" class="latex" title="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}" /> for the numerator and denominator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), separately. <p></p>
</li><li>
Holding the other previously-fitted model parameters in place, fit <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" /> is zero over the training set (or sets, for each level of Elo rating <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> becomes a function of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />).
</li></ul>
<p>
If the resulting <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-scores parameterized by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta_R}" class="latex" title="{\delta_R}" /> make sense, the last step will be adjusting them to conform to normal distribution, via the resampling process mentioned recently <a href="https://rjlipton.wordpress.com/2019/08/20/our-trip-to-monte-carlo/">here</a> and earlier <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">here</a>. We are not there yet. But observations from Spiegelhalter tests with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" /> (equivalently, with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta_R}" class="latex" title="{\delta_R}" /> fixed to zero) suggest that the resulting single, authoritative, “pure” predictivity test may rival the sharpness of my current tests involving specific chess programs.</p>
<p>
</p><p></p><h2> Error Quirks and Queries </h2><p></p>
<p></p><p>
To see a key wrinkle, consider the first error form. It is symmetrical: <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i \pm \epsilon_i}" class="latex" title="{p_i = q_i \pm \epsilon_i}" />. When we substitute <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%2B+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i + \epsilon_i}" class="latex" title="{q_i + \epsilon_i}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> and take <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Ccdots%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}" class="latex" title="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}" />, the symmetry of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> around <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> makes it drop out of the numerator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), and out of everything in the denominator except one place where <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i^2}" class="latex" title="{p_i^2}" /> becomes <img src="https://s0.wp.com/latex.php?latex=%7B%28q_i%5E2+%2B+2%5Cepsilon+q_i+%2B+%5Cepsilon_i%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}" class="latex" title="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}" />. There is hence nothing for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> to fit and we are basically left with the original Spiegelhalter <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />. </p>
<p>
In the second form, however, we get <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Ccdot+%5Cfrac%7B1%7D%7B1+%2B+%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}" class="latex" title="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}" />. If we presume <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> small enough to make the distribution of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{N}(0,\delta^2)}" class="latex" title="{\mathcal{N}(0,\delta^2)}" /> outside <img src="https://s0.wp.com/latex.php?latex=%7B%28-1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(-1,1)}" class="latex" title="{(-1,1)}" /> negligible, then we can use the series expansion to approximate </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_i+%5Capprox+q_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). " class="latex" title="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). " /></p>
<p>Under normal expectation, the odd-power terms drop out (so their signs don’t matter) and we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Bq_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29%5D+%3D+q_i%281+%2B+%5Cdelta%5E2+%2B+3%5Cdelta%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). " /></p>
<p>This credits <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> as being greater than <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. Provided the projections for the substituted indices <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> were generally slightly conservative, this has hope of correcting them.</p>
<p>
Already, however, we have traipsed over some pitfalls of methodology. One is that the normal expectation </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Cfrac%7B1%7D%7B1%2B%5Cepsilon%7D%5D+%3D+%2B%5Cinfty%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, " /></p>
<p>regardless of how small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> is. For any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />, regions around the pole <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = -1}" class="latex" title="{\epsilon = -1}" /> get some fixed finite probability. Another is the simple paradox of our second form saying:</p>
<blockquote><p><b> </b> <em> <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> is an unbiased estimator of <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" />, but <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> is not an unbiased (or even finite) estimator of <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. </em>
</p></blockquote>
<p></p><p>
A third curiosity comes from the fourth error form. It gives <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+e%5E%7B%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i e^{\epsilon_i}}" class="latex" title="{q_i = p_i e^{\epsilon_i}}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+e%5E%7B-%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i e^{-\epsilon_i}}" class="latex" title="{p_i = q_i e^{-\epsilon_i}}" />. We have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Be%5E%7Bb%5Cepsilon%7D%5D+%3D+e%5E%7B0.5b%5E2+%5Cdelta%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} " /></p>
<p>exactly, without approximation. Again the sign of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> does not matter. So we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bp_i%5D+%3D+q_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+q_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. " class="latex" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. " /></p>
<p>But by the original fourth equation we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bq_i%5D+%3D+p_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+p_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. " class="latex" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. " /></p>
<p>So we have <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bq_i%5D+%3E+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}[q_i] &gt; p_i}" class="latex" title="{\mathsf{E}[q_i] &gt; p_i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bp_i%5D+%3E+q_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}[p_i] &gt; q_i}" class="latex" title="{\mathsf{E}[p_i] &gt; q_i}" />, with both expectations being over the same noise terms. This is like the famous Lake Wobegon <a href="https://trustedadvisor.com/trustmatters/lake-wobegon-syndrome-believing-were-all-above-average">syndrome</a>. What it indicates is the need for care in where and how to apply these error representations.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Have you seen this idea of directly testing (un)predictability in the literature? Might it improve the currently much-debated statistical tests for quantum supremacy?</p>
<p>
Which error model seems most likely to apply? Where have the paradoxes in our last section been noted?</p>
<p></p><p><br />
[some wording tweaks]</p></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/"><span class="datestr">at November 29, 2019 02:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1093">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1093">Call for Invited Talk Nominations: HALG 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>

5th Highlights of Algorithms conference (HALG 2020)</p>



<p>ETH Zurich, June 3-5, 2020<br />​<br /><a href="http://2020.highlightsofalgorithms.org/" target="_blank" rel="noreferrer noopener">http://2020.highlightsofalgorithms.org/</a></p>



<p></p>



<p>The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>



<p>A. survey (60 minutes): a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>



<p>B. paper (30 minutes): a significant algorithmic result appearing in a paper in 2019 or later.</p>



<p>To nominate, please email <a href="mailto:halg2020.nominations@gmail.com" target="_blank" rel="noreferrer noopener">halg2020.nominations@gmail.com</a> the following information:</p>



<ol><li>Basic details: speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</li><li>Brief justification: Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</li></ol>



<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>



<p>Nominations deadline: December 20, 2020 (for full consideration).</p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1093"><span class="datestr">at November 29, 2019 01:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/">Tenure-Track Professor at University of British Columbia (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of British Columbia is inviting applications for at least three positions at the rank of Assistant Professor. We invite applications from candidates of outstanding scientific talent in all areas of computer science. Appointment at a higher rank will be considered for an applicant of exceptional qualifications.</p>
<p>Website: <a href="https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0">https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0</a><br />
Email: research-recruiting-chair@cs.ubc.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/"><span class="datestr">at November 28, 2019 11:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/172">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/172">TR19-172 |  Schur Polynomials do not have small formulas if the Determinant doesn&amp;#39;t!  | 

	Chandra Kanta Mohapatra, 

	Mrinal Kumar, 

	Nutan Limaye, 

	Srikanth Srinivasan, 

	Prasad Chaugule, 

	Adrian She</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Schur Polynomials are families of symmetric polynomials that have been
classically studied in Combinatorics and Algebra alike. They play a central
role in the study of Symmetric functions, in Representation theory [Sta99], in
Schubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,
Sta99]. In recent years, they have also shown up in various incarnations in
Computer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric
complexity theory [IP17].
  However, unlike some other families of symmetric polynomials like the
Elementary Symmetric polynomials, the Power Symmetric polynomials and the
Complete Homogeneous Symmetric polynomials, the computational complexity of
syntactically computing Schur polynomials has not been studied much. In
particular, it is not known whether Schur polynomials can be computed
efficiently by algebraic formulas. In this work, we address this question, and
show that unless \emph{every} polynomial with a small algebraic branching
program (ABP) has a small algebraic formula, there are Schur polynomials that
cannot be computed by algebraic formula of polynomial size. In other words,
unless the algebraic complexity class $\mathrm{VBP}$ is equal to the complexity
class $\mathrm{VF}$, there exist Schur polynomials which do not have polynomial
size algebraic formulas.
  As a consequence of our proof, we also show that computing the determinant of
certain \emph{generalized} Vandermonde matrices is essentially as hard as
computing the general symbolic determinant. To the best of our knowledge, these
are one of the first hardness results of this kind for families of polynomials
which are not \emph{multilinear}. A key ingredient of our proof is the study of
composition of \emph{well behaved} algebraically independent polynomials with a homogeneous polynomial, and might be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/172"><span class="datestr">at November 28, 2019 06:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=381">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/11/27/tcs-talk-wednesday-december-4-nihar-shah-cmu/">TCS+ talk: Wednesday, December 4 — Nihar Shah, CMU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk, and last of the Fall season, will take place this coming Wednesday, December 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Nihar Shah</strong> from CMU will speak about “<em>Battling Demons in Peer Review</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Peer review is the backbone of scholarly research. It is however faced with a number of challenges (or “demons”) which cause unfairness to authors, and degrade the overall quality of the process. This talk will present principled and practical approaches to battle these demons in peer review:</p>
<ol>
<li>Subjectivity: How to ensure that all papers are judged by the same yardstick?</li>
<li>Mis-calibration: How to use ratings in presence of arbitrary or adversarial mis-calibration?</li>
<li>Bias: How to rigorously test for existence of (gender/fame/race/…) biases in peer review?</li>
<li>Strategic behavior: How to insulate peer review from strategic behavior of author-reviewers?</li>
<li>Noise: How to assign reviewers to papers to simultaneously ensure fair and accurate evaluations in the presence of review noise?</li>
</ol>
<p>The work uses tools from social choice theory, statistics and learning theory, information theory, game theory and decision theory. No prior knowledge on these topics will be assumed.</p></blockquote>
<p>Bio:<br />
<em><a href="http://cs.cmu.edu/~nihars">Nihar B. Shah</a> is an Assistant Professor in the Machine Learning and Computer Science departments at Carnegie Mellon University (CMU). His research interests include statistics, machine learning, information theory, and game theory, with a focus on applications to learning from people. He is a recipient of the the 2017 David J. Sakrison memorial prize from EECS Berkeley for a “truly outstanding and innovative PhD thesis”, the Microsoft Research PhD Fellowship 2014-16, the Berkeley Fellowship 2011-13, the IEEE Data Storage Best Paper and Best Student Paper Awards for the years 2011/2012, and the SVC Aiya Medal 2010, and has supervised the Best Student Paper at AAMAS 2019.</em></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/11/27/tcs-talk-wednesday-december-4-nihar-shah-cmu/"><span class="datestr">at November 28, 2019 01:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/">Faculty (Asst. and Assoc. Prof) at University of Washington (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This year we have a targeted search in all areas of quantum computing, with a particular emphasis on quantum algorithms and quantum complexity theory.</p>
<p>Website: <a href="https://apply.interfolio.com/64708">https://apply.interfolio.com/64708</a><br />
Email: jrl@cs.washington.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/"><span class="datestr">at November 27, 2019 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/11/27/halg-2020-call-for-nominations-guest-post-by-yossi-azar/">HALG 2020 call for nominations (guest post by Yossi Azar)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Guest post by Yossi Azar – I attended HALG once and enjoyed it quite a lot; I highly recommend people make such nominations –Boaz]</em></p>



<h3><strong>Call for Invited Talk Nominations</strong> :<strong>5th Highlights of Algorithms conference (HALG 2020)</strong></h3>



<p>ETH Zurich, June 3-5, 2020<br />​<br /><a href="http://2020.highlightsofalgorithms.org/" target="_blank" rel="noreferrer noopener">http://2020.highlightsofalgorithms.org/</a></p>



<p></p>



<p>The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>



<p>A. survey (60 minutes): a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>



<p>B. paper (30 minutes): a significant algorithmic result appearing in a paper in 2019 or later.</p>



<p>To nominate, please email <a href="mailto:halg2020.nominations@gmail.com" target="_blank" rel="noreferrer noopener">halg2020.nominations@gmail.com</a> the following information:</p>



<ol><li>Basic details: speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</li><li>Brief justification: Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</li></ol>



<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>



<p>Nominations deadline: <strong>December 20, 2020 </strong>(for full consideration).</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/11/27/halg-2020-call-for-nominations-guest-post-by-yossi-azar/"><span class="datestr">at November 27, 2019 06:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/27/new-book-introduction-to-multi-armed-bandits-by-alex-slivkins/">New Book: Introduction to Multi-Armed Bandits by Alex Slivkins</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Here’s Alex’s announcement of his new book, which I am very excited about, and many in our community would no doubt find extremely useful (there’s even an open version on arXiv!):</p>
<hr />
<p>I am pleased to announce <a href="https://www.nowpublishers.com/article/Details/MAL-068">Introduction to multi-armed bandits</a>, a broad and accessible introduction to the area which emphasizes connections to operations research, game theory, and mechanism design. The said connections have generated a considerable amount of interest (and publications) in the Economics and Computation community.</p>
<p>The book is teachable by design: each chapter corresponds to one week of my class. Each chapter handles one big direction in the literature on bandits, covers the first-order concepts and results on a technical level, and provides a detailed literature review for further exploration. There are no prerequisites other than a certain level of mathematical maturity.</p>
<p>The chapters are as follows: stochastic bandits; lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual bandits; bandits and games; bandits with knapsacks; bandits and incentives.</p>
<p>The book is also <a href="https://arxiv.org/abs/1904.07272">available on arxiv</a> (in a plain-format version).</p>
<p>Aleksandrs Slivkins<br />
Microsoft Research NYC</p></div>







<p class="date">
by Yannai A. Gonczarowski <a href="https://agtb.wordpress.com/2019/11/27/new-book-introduction-to-multi-armed-bandits-by-alex-slivkins/"><span class="datestr">at November 27, 2019 05:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/27/recoloring-infinite-paths">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/27/recoloring-infinite-paths.html">Recoloring infinite paths</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Suppose you have a uniformly random 3-coloring of a doubly-infinite path graph. This can be generated by choosing any one vertex, choosing any one of the three colors for it, and then propagating the coloring outwards from it, choosing one of the two available colors for each successive vertex. It’s convenient to imagine the three colors as being represented by the three numbers 0, 1, and 2 mod 3. Now perform the following process, repeatedly: change the color of every cell whose two neighbors both have the color that is one plus its color (mod 3). In other words, the middle vertex of a triple of vertices that is colored 1–0–1 changes to color 2, the middle vertex of 2–1–2 changes to 0, and the middle vertex of 0–2–0 changes to 1. What does this do to the coloring?</p>

<p>You might think that it stays uniformly random, but it doesn’t. What happens is that you get increasingly large 2-colored regions, whose typical size is proportional to the square root of the number of recoloring steps, separated by triples of vertices that use all three colors. Within each 2-colored region, each recoloring step changes one of the two colors to the third color. The 3-colored triples form the boundaries between these regions and move leftwards or rightwards (depending on the ordering of their three colors). When two boundaries moving in opposite directions collide, they annihilate each other, leaving a larger 2-colored region.</p>

<p>I think the easiest way to see this is to use height functions for colorings. As described in <a href="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings.html">my previous post</a>, a height function is a mapping from the vertices to integers (the heights of the vertices), so that taking the height of each vertex mod 3 gives its color, and such that neighboring vertices have heights that differ by . It’s easy to construct these for colorings of the infinite path, again by starting from an arbitrary choice of height for a single arbitrarily-chosen vertex and propagating outwards. More strongly, for infinite bipartite graphs, the existence of height functions is equivalent to the non-existence of certain special graph homomorphisms to a six-cycle, as described for the finite case in my previous post. However in the infinite case the existence of height functions does not ensure the connectivity of the space of 3-colorings; for instance there is no way to change the infinite periodic 3-coloring …0–1–2–0–1–2… into anything else.</p>

<p>In terms of height functions, what’s happening in each recoloring step is that the height increases by two at each of its local minima. You can think of the height function as giving the height of a pile of particles over each vertex; then each recoloring step adds a particle wherever it can sit in place without rolling downhill.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/rule-184-deposition.svg" alt="Particle deposition view of Rule 184" /></p>

<p>The image above is one I drew several years ago for the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Rule_184">cellular automaton rule 184</a>. This cellular automaton has two states per cell (which we might as well think of as 1 and 0), and it acts by repeatedly swapping the states of pairs of consecutive cells containing the pattern 10. In the figure, the 1’s and 0’s translate to pieces of surface boundary that slope downwards or upwards (respectively), so a 10 pattern is a local minimum of the surface, and filling it in gives a 10 instead.</p>

<p>Rule 184 has an amazing variety of seemingly-unrelated interpretations; for instance, you can think of the cells of the automaton as a gridlocked highway, and the cells with 1’s in them as the cars of a traffic jam (perhaps <a href="https://www.latimes.com/california/story/2019-11-27/how-405-freeway-gridlock-became-the-iconic-image-of-l-a-thanksgiving">stuck in traffic for their Thanksgiving Day travels</a>). When a 1 has an open space ahead of it (a 0 cell) it moves forward, and otherwise it stays in place. This seemingly basic model displays a lot of the same features of real traffic such as freely flowing traffic when the total number of vehicles is small but waves of stop-and-go motion when the number of vehicles is high. It forms the basis for many more-sophisticated models of traffic flow.</p>

<p>But it is a different interpretation of Rule 184 that works best for understanding the question I started with, of what happens to a random coloring under recoloring operations. These operations are exactly what happens when you apply Rule 184 to the height function of a random coloring. But you can also view Rule 184 as describing a system of two kinds of particles, left-moving ones (11 patterns) and right-moving ones (00 patterns), separated by empty space (alternating 0’s and 1’s), that annihilate each other when they collide. In terms of the coloring, these particles are just the triples of vertices colored with three colors, and the parts of the cellular automaton with no particles are the 2-colored regions. The uniformly random coloring that we started with has the convenient property that particles of both types are equally likely. A right-moving particle will survive to step  if, in every prefix of the random sequence of particles to the right of it of length proportional to , there are more right-moving particles than left-moving particles. A standard calculation on random walks shows that this survival probability is , and this is also the density of remaining particles after  steps.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/103212704886997345">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/27/recoloring-infinite-paths.html"><span class="datestr">at November 27, 2019 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/171">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/171">TR19-171 |  Improved bounds on the AN-complexity of multilinear functions | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider arithmetic circuits with arbitrary large (multi-linear) gates for computing multi-linear functions. An adequate complexity measure for such circuits is the maximum between the arity of the gates and their number. 
This model and the corresponding complexity measure were introduced by Goldreich and Wigderson (ECCC, TR13-043, 2013), and is called the AN-complexity.

The AN-complexity of a multi-linear function yields an upper bound on the size of depth-three Boolean circuits for computing the function, and it is not clear whether or not significantly smaller depth-three Boolean functions exist. Specifically, the depth-three size of Boolean circuits is at most exponential in the AN-complexity of the function. Hence, proving linear lower bounds on the AN-complexity of explicit multi-linear function is a essential step towards proving that depth-three Boolean circuits for these functions requires exponential size.

In this work we present explicit multi-linear functions that require depth-two multi-linear circuits of almost linear AN-complexity. Specifically, for every $\epsilon&gt;0$, we show an explicit $\poly(1/\epsilon)$-linear function $f:\{0,1\}^{\poly(1/\epsilon)\cdot n}\to\{0,1\}$ such that any depth-two circuit (with general multi-linear gates) that computes $f$ must use gates of arity at least $n^{1-\epsilon}$. This improves over a corresponding lower bound of $\tildeOM(n^{2/3})$ that was known for an explicit tri-linear function
(Goldreich and Tal, Computational Complexity, 2018), but leaves open the problem of showing similar AN-complexity lower bounds for multi-linear circuits of larger depth. 

A key aspect in our proof is considering many (extremely skewed) random restrictions, and contrasting the sum of the values of the original function and the circuit (which supposedly computes it) taken over a (carefully chosen) subset of these random restrictions. We show that if the original circuit has too low AN-complexity, then these two sums cannot be equal, which yields a contradiction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/171"><span class="datestr">at November 27, 2019 02:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/170">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/170">TR19-170 |  A Quadratic Lower Bound for  Algebraic Branching Programs | 

	Prerona Chatterjee, 

	Mrinal Kumar, 

	Adrian She, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that any Algebraic Branching Program (ABP) computing the polynomial $\sum_{i = 1}^n x_i^n$ has at least $\Omega(n^2)$ vertices. This improves upon the lower bound of $\Omega(n\log n)$, which follows from the classical result of Baur and Strassen [Str73, BS83], and extends the results by Kumar [Kum19], which showed a quadratic lower bound  for $\text{homogeneous}$ ABPs computing the same polynomial.

Our proof relies on a notion of depth reduction which is reminiscent of similar statements in the context of matrix rigidity, and shows that any small enough ABP computing the polynomial $\sum_{i=1}^n x_i^n$ can be depth reduced to essentially a homogeneous ABP of the same size which computes the polynomial $\sum_{i = 1}^n x_i^n + \varepsilon(\mathbf{x})$, for a structured ``error polynomial'' $\varepsilon(\mathbf{x})$. To complete the proof, we then observe that the lower bound in [Kum19] is robust enough and continues to hold for all polynomials $\sum_{i = 1}^n x_i^n + \varepsilon(\mathbf{x})$, where  $\varepsilon(\mathbf{x})$ has the appropriate structure.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/170"><span class="datestr">at November 27, 2019 10:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3448">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/27/youngec-workshop-in-tel-aviv-31-12-19-2-1-20/">YoungEC Workshop in Tel-Aviv, 31/12/19-2/1/20</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/index.php">“Young” Workshop on Economics and Computation (YoungEC)</a> will be held in Tel-Aviv University, Israel, during December 31st, 2019 to January 2nd, 2020. The <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/participants.php">list of speakers</a> includes a small number of established central figures in the field together with a larger number of bright rising stars worldwide.</p>
<p>The workshop is now open for <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/registration.php">registration</a>.</p></div>







<p class="date">
by algorithmicgametheory <a href="https://agtb.wordpress.com/2019/11/27/youngec-workshop-in-tel-aviv-31-12-19-2-1-20/"><span class="datestr">at November 27, 2019 08:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/27/tenure-track-assistant-professor-at-university-of-victoria-apply-by-december-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/27/tenure-track-assistant-professor-at-university-of-victoria-apply-by-december-1-2019/">Tenure-track assistant professor at University of Victoria (apply by December 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Victoria is seeking applicants for two positions at the rank of Assistant Professor with eligibility for tenure and with an anticipated start date of July 1, 2020. We are particularly seeking candidates in the areas of Graphics, Systems, AI and Theory.</p>
<p>Website: <a href="https://www.uvic.ca/engineering/computerscience/people/employment-opportunities/">https://www.uvic.ca/engineering/computerscience/people/employment-opportunities/</a><br />
Email: search@csc.uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/27/tenure-track-assistant-professor-at-university-of-victoria-apply-by-december-1-2019/"><span class="datestr">at November 27, 2019 05:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/26/assistant-professor-at-university-of-california-san-diego-apply-by-january-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/26/assistant-professor-at-university-of-california-san-diego-apply-by-january-15-2020/">Assistant Professor  at University of California – San Diego (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at Assistant Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering. A Ph.D. or advancement to candidacy in Computer Science &amp; Engineering or related disciplines is required at the time of application.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF02337">https://apol-recruit.ucsd.edu/JPF02337</a><br />
Email: nherrera@eng.ucsd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/26/assistant-professor-at-university-of-california-san-diego-apply-by-january-15-2020/"><span class="datestr">at November 26, 2019 11:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/26/postdoc-at-boston-college-apply-by-january-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/26/postdoc-at-boston-college-apply-by-january-15-2020/">Postdoc at Boston College (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and MPC algorithms.</p>
<p>To apply, please send the materials indicated in the link to Hsin-Hao Su by January 15, 2020.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br />
Email: suhx@bc.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/26/postdoc-at-boston-college-apply-by-january-15-2020/"><span class="datestr">at November 26, 2019 09:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4432">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4432">Guest post by Greg Kuperberg: Archimedes’ other principle and quantum supremacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong>Scott’s Introduction:</strong> Happy Thanksgiving!  Please join me in giving thanks for the beautiful post below, by friend-of-the-blog <a href="https://www.math.ucdavis.edu/~greg/">Greg Kuperberg</a>, which tells a mathematical story that stretches from the 200s BC all the way to Google’s quantum supremacy result last month.</p>



<h2>Archimedes’ other principle and quantum supremacy</h2>



<p>by Greg Kuperberg</p>



<p><strong>Note:</strong> UC Davis is <a href="https://recruit.ucdavis.edu/JPF03248">hiring in CS theory</a>! Scott offered me free ad space if I wrote a guest post, so here we are.  The position is in all areas of CS theory, including QC theory although the search is not limited to that.</p>



<p>In this post, I wear the hat of a pure mathematician in a box provided by Archimedes.  I thus set aside what everyone else thinks is important about Google’s 53-qubit quantum supremacy experiment, that it is a dramatic milestone in quantum computing technology.  That’s only news about the physical world, whose significance pales in comparison to the Platonic world of mathematical objects.  In my happy world, I like quantum supremacy as a demonstration of a beautiful coincidence in mathematics that has been known for more than 2000 years in a special case. The single-qubit case was discovered by Archimedes, duh.  As Scott mentions in <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565/ref=sr_1_1?keywords=quantum+computing+since+democritus&amp;qid=1574801358&amp;sr=8-1"><em>Quantum Computing Since Democritus</em></a>, Bill Wootters stated the general result in a <a href="https://link.springer.com/article/10.1007/BF01883491">1990 paper</a>, but Wootters credits a <a href="https://link.springer.com/article/10.1007/BF01019475">1974 paper</a> by the Czech physicist Stanislav Sýkora.  I learned of it in the substantially more general context of symplectic geometric that mathematicians developed independently between Sýkora’s prescient paper and Wootters’ more widely known citation.  Much as I would like to clobber you with highly abstract mathematics, I will wait for some other time.</p>



<p>Suppose that you pick a pure state \(|\psi\rangle\) in the Hilbert space \(\mathbb{C}^d\) of a \(d\)-dimensional qudit, and then make many copies and fully measure each one, so that you sample many times from some distribution \(\mu\) on the \(d\) outcomes.  You can think of such a distribution \(\mu\) as a classical randomized state on a digit of size \(d\).  The set of all randomized states on a \(d\)-digit makes a \((d-1)\)-dimensional simplex \(\Delta^{d-1}\) in the orthant \(\mathbb{R}_{\ge 0}^d\).  The coincidence is that if \(|\psi\rangle\) is uniformly random in the unit sphere in \(\mathbb{C}^d\), then \(\mu\) is uniformly random in \(\Delta^{d-1}\).  I will call it the Born map, since it expresses the Born rule of quantum mechanics that amplitudes yield probabilities.  Here is a diagram of the Born map of a qutrit, except with the laughable simplification of the 5-sphere in \(\mathbb{C}^3\) drawn as a 2-sphere. </p>



<figure class="wp-block-image"><img src="https://www.scottaaronson.com/f1-qutrit.png" alt="" /></figure>



<p>If you pretend to be a bad probability student, then you might not be surprised by this coincidence, because you might suppose that all probability distributions are uniform other than treacherous exceptions to your intuition.  However, the principle is certainly not true for a “rebit” (a qubit with real amplitudes) or for higher-dimensional “redits.”  With real amplitudes, the probability density goes to infinity at the sides of the simplex \(\Delta^{d-1}\) and is even more favored at the corners.  It also doesn’t work for mixed qudit states; the projection then favors the middle of \(\Delta^{d-1}\). </p>



<h3>Archimedes’ theorem</h3>



<p> The theorem of Archimedes is that a natural projection from the unit sphere to a circumscribing vertical cylinder preserves area.  The projection is the second one that you might think of: Project radially from a vertical axis rather than radially in all three directions.  Since Archimedes was a remarkably prescient mathematician, he was looking ahead to the Bloch sphere of pure qubit states \(|\psi\rangle\langle\psi|\) written in density operator form.  If you measure \(|\psi\rangle\langle\psi|\) in the computational basis, you get a randomized bit state \(\mu\) somewhere on the interval from guaranteed 0 to guaranteed 1. </p>



<figure class="wp-block-image"><img src="https://www.scottaaronson.com/f2-bloch.png" alt="" /></figure>



<p>This transformation from a quantum state to a classical randomized state is a linear projection to a vertical axis.  It is the same as Archimedes’ projection, except without the angle information.  It doesn’t preserve dimension, but it does preserve measure (area or length, whatever) up to a factor of \(2\pi\).  In particular, it takes a uniformly random \(|\psi\rangle\langle\psi|\) to a uniformly random \(\mu\).</p>



<p>Archimedes’ projection is also known as the Lambert cylindrical map of the world.  This is the map that squishes Greenland along with the top of North America and Eurasia to give them proportionate area. </p>



<figure class="wp-block-image"><img src="https://www.scottaaronson.com/f3-lambert.jpg" alt="" /></figure>



<p>(I forgive Lambert if he didn’t give prior credit to Archimedes.  There was no Internet back then to easily find out who did what first.)  Here is a calculus-based proof of Archimedes’ theorem: In spherical coordinates, imagine an annular strip on the sphere at a polar angle of \(\theta\).  (The polar angle is the angle from vertical in spherical coordinates, as depicted in red in the Bloch sphere diagram.)   The strip has a radius of \(\sin\theta\), which makes it shorter than its unit radius friend on the cylinder.  But it’s also tilted from vertical by an angle of \(\frac{\pi}2-\theta\), which makes it wider by a factor of \(1/(\sin \theta)\) than the height of its projection onto the \(z\) axis.  The two factors exactly cancel out, making the area of the strip exactly proportional to the length of its projection onto the \(z\) axis.  This is a coincidence which is special to the 2-sphere in 3 dimensions.  As a corollary, we get that the surface area of a unit sphere is \(4\pi\), the same as an open cylinder with radius 1 and height 2.  If you want to step through this in even more detail, Scott mentioned to me an <a href="https://www.youtube.com/watch?v=GNcFjFmqEc8">action video</a> which is vastly spiffier than anything that I could ever make.</p>



<p>The projection of the Bloch sphere onto an interval also shows what goes wrong if you try this with a rebit.  The pure rebit states — again expressed in density operator form \(|\psi\rangle\langle\psi|\) are a great circle in the Bloch sphere.  If you linearly project a circle onto an interval, then the length of the circle is clearly bunched up at the ends of the interval and the projected measure on the interval is not uniform. </p>



<h3>Sýkora’s generalization</h3>



<p> It is a neat coincidence that the Born map of a qubit preserves measure, but a proof that relies on Archimedes’ theorem seems to depend on the special geometry of the Bloch sphere of a qubit.  That the higher-dimensional Born map also preserves measure is downright eerie.  Scott challenged me to write an intuitive explanation.  I  remembered two different (but similar) proofs, neither of which is original to me. Scott and I disagree as to which proof is nicer.</p>



<p>As a first step of the first proof, it is easy to show that the Born map \(p = |z|^2\) for a single amplitude \(z\) preserves measure as a function from the complex plane \(\mathbb{C}\) to the ray \(\mathbb{R}_{\ge 0}\).  The region in the complex numbers \(\mathbb{C}\) where the length of \(z\) is between \(a\) and \(b\), or \(a \le |z| \le b\), is \(\pi(b^2 – a^2)\).  The corresponding interval for the probability is \(a^2 \le p \le b^2\), which thus has length \(b^2-a^2\).  That’s all, we’ve proved it!  More precisely, the area of any circularly symmetric region in \(\mathbb{C}\) is \(\pi\) times the length of its projection onto \(\mathbb{R}_{\ge 0}\). </p>



<figure class="wp-block-image"><img src="https://www.scottaaronson.com/f4-washer.png" alt="" /></figure>



<p>The second step is to show the same thing for the Born map from the \(d\)-qudit Hilbert space \(\mathbb{C}^d\) to the \(d\)-digit orthant \(\mathbb{R}_{\ge 0}^d\), again without unit normalization.  It’s also measure-preserving, up to a factor of \(\pi^d\) this time, because it’s the same thing in each coordinate separately.  To be precise, the volume ratio holds for any region in \(\mathbb{C}^d\) that is invariant under separately rotating each of the \(d\) phases. (Because you can approximate any such region with a union of products of thin annuli.)</p>



<p>The third and final step is the paint principle for comparing surface areas.  If you paint the hoods of two cars with the same thin layer of paint and you used the same volume of paint for each one, then you can conclude that the two car hoods have nearly same area.  In our case, the Born map takes the region \[ 1 \le |z_0|^2 + |z_1|^2 + \cdots + |z_{d-1}|^2 \le 1+\epsilon \] in \(\mathbb{C}^d\) to the region \[ 1 \le p_0 + p_1 + \cdots + p_{d-1} \le 1+\epsilon \] in the orthant \(\mathbb{R}_{\ge 0}^d\).  The former is the unit sphere \(S^{2d-1}\) in \(\mathbb{C}^d\) painted to a thickness of roughly \(\epsilon/2\).  The latter is the probability simplex \(\Delta^{n-1}\) painted to a thickness of exactly \(\epsilon\). Taking the limit \(\epsilon \to 0\), the Born map from \(S^{2d-1}\) to \(\Delta^{n-1}\) preserves measure up to a factor of \(2\pi^n\).</p>



<p>You might wonder “why” this argument works even if you accept that it does work.  The key is that the exponent 2 appears in two different ways: as the dimension of the complex numbers, and as the exponent used to set probabilities and define spheres.  If we try the same argument with real amplitudes, then the volume between “spheres” of radius \(a\) and \(b\) is just \(2(b-a)\), which does not match the length \(b^2-a^2\).  The Born map for a single real amplitude is the parabola \(p = x^2\), which clearly distorts length since it is not linear.  The higher-dimensional real Born map similarly distorts volumes, whether or not you restrict to unit-length states.</p>



<p>If you’re a bitter-ender who still wants Archimedes’ theorem for real amplitudes, then you might consider the variant formula \(p = |x|\) to obtain a probability \(p\) from a “quantum amplitude” \(x\).  Then the “Born” map does preserve measure, but for the trivial reason that \(x = \pm p\) is not really a quantum amplitude, it is a probability with a vestigial sign.  Also the unit “sphere” in \(\mathbb{R}^d\) is not really a sphere in this theory, it is a hyperoctahedron.  The only “unitary” operators that preserve the unit hyperoctahedron are signed permutation matrices.  You can only use them for reversible classical computing or symbolic dynamics; they don’t have the strength of true quantum computing or quantum mechanics.</p>



<p>The fact that the Born map preserves measure also yields a bonus calculation of the volume of the unit ball in \(2d\) real dimensions, if we interpret that as \(d\) complex dimensions.  The ball \[ |z_0|^2 + |z_1|^2 + \cdots + |z_{d-1}|^2 \le 1 \] in \(\mathbb{C}^d\) is sent to a different simplex \[ p_0 + p_1 + \cdots + p_{d-1} \le 1 \] in \(\mathbb{R}_{\ge 0}^d\).  If we recall that the volume of a \(d\)-dimensional pyramid is \(\frac1d\) times base times height and calculate by induction on \(d\), we get that this simplex has volume \(\frac1{d!}\).  Thus, the volume of the \(2d\)-dimensional unit ball is \(\frac{\pi^d}{d!}\).</p>



<p>You might ask whether the volume of a \(d\)-dimensional unit ball is always \(\frac{\pi^{d/2}}{(d/2)!}\) for both \(d\) even and odd.  The answer is yes if we interpret factorials using the gamma function formula \(x! = \Gamma(x+1)\) and look up that \(\frac12! = \Gamma(\frac32) = \frac{\sqrt{\pi}}2\).  The gamma function was discovered by Euler as a solution to the question of defining fractional factorials, but the notation \(\Gamma(x)\) and the cumbersome shift by 1 is due to Legendre.  Although Wikipedia says that no one knows why Legendre defined it this way, I wonder if his goal was to do what the Catholic church later did for itself in 1978: It put a Pole at the origin.</p>



<p>(Scott wanted to censor this joke. In response, I express my loyalty to my nation of birth by quoting the opening of the Polish national anthem: “Poland has not yet died, so long as we still live!”  I thought at first that Stanislav Sýkora is Polish since Stanisław and Sikora are both common Polish names, but his name has Czech spelling and he is Czech. Well, the Czechs are cool too.)</p>



<p>Sýkora’s 1974 proof of the generalized Archimedes’ theorem is different from this one.  He calculates multivariate moments of the space of unit states \(S^{2d-1} \subseteq \mathbb{C}^d\), and confirms that they match the moments in the probability simplex \(\Delta^{d-1}\).  There are inevitably various proofs of this result, and I will give another one. </p>



<h3>Another proof, and quantum supremacy</h3>



<p>There is a well-known and very useful algorithm to generate a random point on the unit sphere in either \(\mathbb{R}^d\) or \(\mathbb{C}^d\), and a similar algorithm to generate a random point in a simplex.  In the former algorithm, we make each real coordinate \(x\) into an independent Gaussian random variable with density proportional to \(e^{-x^2}\;dx\), and then rescale the result to unit length.  Since the exponents combine as \[ e^{-x_0^2}e^{-x_1^2}\cdots e^{-x_{d-1}^2} =       e^{-(x_0^2 + x_1^2 + \cdots + x_{d-1}^2)}, \] we learn that the total exponent is spherically symmetric.  Therefore after rescaling, the result is a uniformly random point on the unit sphere \(S^{d-1} \subseteq \mathbb{R}^d\).  Similarly, the other algorithm generates a point in the orthant \(\mathbb{R}_{\ge 0}^d\) by making each real coordinate \(p \ge 0\) an independent random variable with exponential distribution \(e^{-p}\;dp\).  This time we rescale the vector until its sum is 1.  This algorithm likewise produces a uniformly random point in the simplex \(\Delta^{d-1} \subseteq \mathbb{R}_{\ge 0}^d\) because the total exponent of the product \[ e^{-p_0}e^{-p_1}\cdots e^{-p_{d-1}} =       e^{-(p_0 + p_1 + \cdots + p_{d-1})} \] only depends on the sum of the coordinates.  Wootters describes both of these algorithms in his 1990 paper, but instead of relating them to give his own proof of the generalized Archimedes’ theorem, he cites Sýkora.</p>



<p>The gist of the proof is that the Born map takes the Gaussian algorithm to the exponential algorithm.  Explicitly, the Gaussian probability density for a single complex amplitude \[ z = x+iy = re^{i\theta} \] can be converted from Cartesian to polar coordinate as follows: \[ \frac{e^{-|z|^2}\;dx\;dy}{\pi} = \frac{e^{-r^2}r\;dr\;d\theta}{\pi}. \] I have included the factor of \(r\) that is naturally present in an area integral in polar coordinates.  We will need it, and it is another way to see that the theorem relies on the fact that the complex numbers are two-dimensional.  To complete the proof, we substitute \(p = r^2\) and remember that \(dp = 2r\;dr\), and then integrate over \(\theta\) (trivially, since the integrand does not depend on \(\theta\)).  The density simplifies to \(e^{-p}\;dp\), which is exactly the exponential distribution for a real variable \(p \ge 0\).  Since the Born map takes the Gaussian algorithm to the exponential algorithm, and since each algorithm produces a uniformly random point, the Born map must preserve uniform measure.  (Scott likes this proof better because it is algorithmic, and because it is probabilistic.)</p>



<p>Now about quantum supremacy.  You might think that a random chosen quantum circuit on \(n\) qubits produces a nearly uniformly random quantum state \(|\psi\rangle\) in their joint Hilbert space, but it’s not quite not that simple.  When \(n=53\), or otherwise as \(n \to \infty\), a manageable random circuit is not nearly creative enough to either reach or approximate most of the unit states in the colossal Hilbert space of dimension \(d = 2^n\).  The state \(|\psi\rangle\) that you get from (say) a polynomial-sized circuit resembles a fully random state in various statistical and computational respects, both proven and conjectured.  As a result, if you measure the qubits in the computational basis, you get a randomized state on \(n\) bits that resembles a uniformly random point in \(\Delta^{2^n-1}\).</p>



<p>If you choose \(d\) probabilities, and if each one is an independent exponential random variable, then the law of large numbers says that the sum (which you use for rescaling) is close to \(d\) when \(d\) is large. When \(d\) is really big like \(2^{53}\), a histogram of the probabilities of the bit strings of a supremacy experiment looks like an exponential curve \(f(p) \propto e^{-pd}\).  In a sense, the statistical distribution of the bit strings is almost the same almost every time, independent of which random quantum circuit you choose to generate them.  The catch is that the position of any given bit string does depend on the circuit and is highly scrambled.  I picture it in my mind like this: </p>



<figure class="wp-block-image"><img src="https://www.scottaaronson.com/f5-samples.png" alt="" /></figure>



<p>

It is thought to be computationally intractable to calculate where each bit string lands on this exponential curve, or even where just one of them does.  (The exponential curve is attenuated by noise in the actual experiment, but it’s the same principle.)  That is one reason that random quantum circuits are supreme.</p>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4432"><span class="datestr">at November 26, 2019 08:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/26/postdoctoral-fellowships-at-umass-amherst-apply-by-december-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/26/postdoctoral-fellowships-at-umass-amherst-apply-by-december-1-2019/">Postdoctoral Fellowships at UMass Amherst (apply by December 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The UMass Amherst TRIPODS Institute for Theoretical Foundations of Data Science invites applications for postdoctoral fellowships. Research areas of interest include: algorithms and computational models for processing massive data sets; statistical performance and data acquisition in interactive data collection; model robustness, approximate inference and uncertainty quantification.</p>
<p>Website: <a href="https://www.cics.umass.edu/job/postdoctoral-research-associate-tripods-institute-theoretical-foundations-data-science">https://www.cics.umass.edu/job/postdoctoral-research-associate-tripods-institute-theoretical-foundations-data-science</a><br />
Email: mcgregor@cs.umass.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/26/postdoctoral-fellowships-at-umass-amherst-apply-by-december-1-2019/"><span class="datestr">at November 26, 2019 04:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/">Quantum Computer Science School 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 14-16, 2020 University of Technology Sydney, Australia http://conference.iiis.tsinghua.edu.cn/QCSS2020/ The Quantum Computer Science School 2020 will consist of three days of lectures and academic activities, targeting at senior undergraduates and graduate students in Australian and Asian universities. The lecturers are Mingsheng Ying, Luming Duan, Michael Bremner, Troy Lee, and Ran Duan, and the topics include … <a href="https://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/" class="more-link">Continue reading <span class="screen-reader-text">Quantum Computer Science School 2020</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/"><span class="datestr">at November 26, 2019 01:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings.html">Reconfiguring 3-colorings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I talked briefly about how to get from one 3-coloring of a grid graph to another by changing one vertex color at a time, in <a href="https://11011110.github.io/blog/2019/10/16/from-one-fold.html">a recent blog post about analogous problems for origami folding patterns</a>. It turns out that this problem of reconfiguring 3-colorings has been treated in much greater generality in the 2007 doctoral dissertation of Luis Cereceda, “<a href="http://etheses.lse.ac.uk/131/">Mixing graph colourings</a>”, as I discovered when I read the dissertation in the process of writing a new Wikipedia article on <a href="https://en.wikipedia.org/wiki/Cereceda%27s_conjecture">Cereceda’s conjecture</a>, the conjecture that the space of -colorings of -degenerate graphs (under moves that change the color of one vertex at a time) has at most quadratic diameter. Here’s an illustration from the new article showing the space of 3-colorings of a path graph:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/path-3-colorings.svg" alt="The space of 3-colorings of a path graph" /></p>

<p>Cereceda proved that the following properties of a graph are equivalent:</p>

<ul>
  <li>It has a connected space of 3-colorings.</li>
  <li>Every 3-coloring has a height function.</li>
  <li>It is bipartite and is not “pinchable” to a 6-cycle</li>
</ul>

<p>If we consider the three colors to be used for 3-colorings to be the numbers 0, 1, and 2 (mod 3) then a height function can be defined as an assignment of integers (not mod 3) to the vertices, such that taking them mod 3 produces the given coloring, and such that adjacent vertices have heights that differ by . If it exists for a given coloring, it can be constructed easily by choosing the height of one vertex arbitrarily and then using the  requirement for adjacent heights to propagate this choice to neighbors in the graph until every vertex has a height. What can go wrong is that this propagation somehow leaves two adjacent vertices with heights that are too far apart. For instance, if the colors around a 6-cycle have the cyclic order 0–1–2–0–1–2 then you will come back to the start six units higher than you started.</p>

<p>Continuing to explain the terms in Cereceda’s equivalence, I think the 6-cycle pinchability condition is most easily explained in terms of <a href="https://en.wikipedia.org/wiki/Graph_homomorphism">graph homomorphisms</a>, maps from one graph to another that preserve adjacency. Consider a 6-cycle, 3-colored 0–1–2–0–1–2. It is also a bipartite graph, 2-colored black–white–black–white–black–white, with one vertex for each combination of colors in the 3-coloring and the 2-coloring. If you have any 3-coloring of a bipartite graph, you can map it onto the 6-cycle so that both the 3-coloring and the bipartition are preserved. If your 3-coloring does not have a height function (that is, there is a cycle that, when you propagate heights around it, comes back to its start at a different height) then the winding number of this cycle, as it maps around the 6-cycle, will tell you the difference in heights from start to end (divided by six). So a coloring without a height function gives you a homomorphism to the 6-cycle in which some cycle has nonzero winding number. On the other hand, if you have such a homomorphism, you can lift the colors from the 6-cycle back to the starting graph to give you a coloring without a height function. Cereceda’s “pinching” operations are a special type of homomorphism in which vertices at distance two from each other are repeatedly merged. Not every homomorphism comes from pinching in this way (with pinches you can only map a 12-cycle once around a 6-cycle, not twice around, for instance) so the proof that pinching homomorphisms exist for graphs without height functions is a little more complicated.</p>

<p>It’s possible to construct in polynomial time the shortest sequence of color changes to go from one 3-coloring to another, whenever the space of colorings is connected. Cereceda almost finds this algorithm but misses a small trick and ends up stating its existence only as a conjecture. As I explained in my previous post, if you choose the correct offset for the height functions of the two colorings, you can find this shortest sequence by greedily changing the color at a vertex of one color where the current height is farthest from the goal height. The trick that Cereceda misses is that if you don’t know the correct offset between the two height functions, you can just try them all (or more quickly use a median algorithm to find the optimal offset and the distance between colorings in linear time).</p>

<p>Unfortunately, as Cereceda proved, testing pinchability to a 6-cycle is -complete and therefore testing the connectivity of the space of 3-colorings (and the applicability of the height-based shortest reconfiguration algorithm) is -complete. For instance, I’m pretty sure the 10-vertex Möbius ladder shown below has height functions for all its 3-colorings, but I don’t know of an elegant way to prove this, and -completeness suggests that not all graphs have elegant proofs for this. This graph has lots of 6-cycles, for instance formed by any diagonal and the path around the outer cycle connecting its two endpoints. But trying to map the whole graph to a 6-cycle by folding it flat across a diagonal doesn’t work because it would map some edges to non-edges, and nothing else seems to work either.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/M%C3%B6bius-ladder-10.svg" alt="A 10-vertex Möbius ladder" /></p>

<p>To save this post from being content-free, I want to describe a more easily recognized family of graphs that meets Cereceda’s criterion, and does have height functions for all its 3-colorings (so we can quickly find the shortest reconfiguration sequences in these graphs). It is the class of graphs in which the <a href="https://en.wikipedia.org/wiki/Cycle_space">cycle space</a> is generated by the 4-cycles, or in other words the graphs in which there exists a <a href="https://en.wikipedia.org/wiki/Cycle_basis">cycle basis</a> consisting only of 4-cycles. These graphs can be recognized simply by doing some mod-2 linear algebra to test whether the space generated by the 4-cycles has the same dimension as the cycle space. For instance, the Möbius ladder above is not in this class because the dimension of its cycle space (the number of edges beyond the ones in a spanning forest) is six but its number of 4-cycles (the cycles between two consecutive diagonals) is only five. Despite this example, the graphs generated by 4-cycles include a lot of natural classes of graphs that we might want to reconfigure 3-colorings of:</p>

<ul>
  <li>Trees</li>
  <li>Rectangular grid graphs and higher-dimensional hyperrectangular grid graphs</li>
  <li><a href="https://en.wikipedia.org/wiki/Squaregraph">Squaregraphs</a></li>
  <li>The dual graphs of simple line arrangements</li>
  <li>Complete bipartite graphs</li>
  <li><a href="https://en.wikipedia.org/wiki/Chordal_bipartite_graph">Chordal bipartite graphs</a></li>
  <li>The <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">Cartesian products</a> of other graphs generated by 4-cycles</li>
</ul>

<p>To prove that Cartesian products preserve the property of being generated by 4-cycles, consider any cycle  in the product graph; we want to represent it as a mod-2 sum of 4-cycles. If two consecutive edges of  come from the two different factors, then the product of these edges is a 4-cycle, and adding this 4-cycle to  swaps the order of the two edges within . By repeated swaps we can segregate all the edges from the two factors from each other, changing  into two cycles, one from each factor, that meet at a common vertex. Then we can represent the two cycles separately within the two factors.</p>

<p>It’s straightforward to see that when 4-cycles generate the -homology of a graph, then height functions always exist, because the height difference around any cycle is the sum of the height differences of the 4-cycles generating it, which are all zero. But here by using the binary cycle space we’re looking at the -homology, which might be a different thing. There could be graphs whose -homology is generated by 4-cycles but whose -homology isn’t. For instance I think that adding one bipartite edge to the 10-vertex Möbius ladder produces an example of this phenomenon. So we need a proof that -homology is good enough. Or in simpler terms, when the cycle space is generated by 4-cycles, all 3-colorings have height functions.</p>

<p>We’ll prove this by contradiction, so let’s suppose that we have the smallest possible counterexample. That is, we have a graph , a 3-coloring of , and a cycle  in , such that the cycle space of  is generated by 4-cycles but going around  using the  rule to propagate heights produces a different height than you started with. By “smallest” I mean first, that  is as short as possible, second, that for that length of  the rest of  has as few vertices as possible, and third, that the number of 4-cycles in the representation of  is as small as possible.</p>

<p>Then  cannot have any chords, because if it did then a chord plus one of the two segments of  connecting the chord endpoints would form a shorter cycle with the same inconsistent heights. Because we are assuming that  is generated by 4-cycles, consider any set  of 4-cycles whose sum is . That is, each edge of  appears an odd number of times in cycles of , and each other edge of  appears an even number of times (possibly zero). Each 4-cycle in  must have a coloring of the form ––– or ––– for some colors , , and . If at least one of the two -colored vertices does not belong to , form a smaller graph  by merging these two -colored vertices. This merger does not change  or the coloring, so  stays a bad cycle. And it doesn’t change the property of  that it is generated by 4-cycles, because any cycle in the merged graph can be lifted to a cycle in the unmerged graph (possibly passing through –– in the merged 4-cycle and possibly not simple), represented by 4-cycles in  itself, and then merged back down to get a representation in the smaller graph. But because we’re assuming  was the smallest possible counterexample, this shrinkage can’t happen, so all of the 4-cycles in  have both -vertices in .</p>

<p>Now if we have a 4-cycle with two -vertices in , at least one of the other two vertices does not belong to , because  must be longer than four edges (else it would not have a bad coloring) and we’ve already ruled out the possibility that  has the chords that would be needed to make a 4-cycle using vertices only from . Let  be this vertex outside of , so we have a path –– connecting the two -vertices in . One of two things can happen: First, the two -vertices might only be two units apart in . But then, replacing the two-edge path between them by the path through  produces a new bad cycle of the same length, in the same graph , representable by a smaller set of 4-cycles. This contradicts our choice of  and  as being the smallest possible counterexample. Second, when the two -vertices are farther apart in , there are two cycles formed by path –– and by one of the two paths in  connecting the same two -vertices. Both of these cycles are shorter than , and at least one of them continues to have a bad coloring. So again we have found a smaller counterexample and a contradiction.</p>

<p>This case analysis leading in all cases to a contradiction shows that a smallest counterexample cannot exist, and therefore that all graphs generated by 4-cycles have height functions for all of their 3-colorings.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/103202898221719422">Discuss on Mastodon</a>; see also <a href="https://11011110.github.io/blog/2010/09/12/rapid-mixing-for.html">an earlier post on reconfiguring 3-colorings of cycles using stronger moves</a>; edited 2019-11-27 to correct rectraction vs pinchability in Cereceda’s characterization)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings.html"><span class="datestr">at November 25, 2019 10:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/25/postdoctoral-at-yale-university-apply-by-december-20-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/25/postdoctoral-at-yale-university-apply-by-december-20-2019/">POSTDOCTORAL at YALE UNIVERSITY (apply by December 20, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are solicited for multiple postdoctoral positions at Yale in Algorithms, Optimization, Sampling, and Fairness. The positions are expected to start in Fall 2020 but can start earlier. Applicants should have their CV, research statement, and three letters emailed directly to nisheeth.vishnoi@gmail.com. Applications completed by December 20, 2019, will receive full consideration.</p>
<p>Website: <a href="http://cs.yale.edu/homes/vishnoi/Positions.html">http://cs.yale.edu/homes/vishnoi/Positions.html</a><br />
Email: nisheeth.vishnoi@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/25/postdoctoral-at-yale-university-apply-by-december-20-2019/"><span class="datestr">at November 25, 2019 03:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1539">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2019/11/24/motwani-postdoctoral-fellowship-2/">Motwani Postdoctoral Fellowship</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below.</p>
<p>Applications will be accepted until the positions are filled, but review of applicants will begin after Dec 15.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/15578">https://academicjobsonline.org/ajo/jobs/15578</a><br />
Email: theory.stanford@gmail.com</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2019/11/24/motwani-postdoctoral-fellowship-2/"><span class="datestr">at November 25, 2019 12:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/25/postdoctoral-position-in-computational-social-choice-at-university-of-toronto-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/25/postdoctoral-position-in-computational-social-choice-at-university-of-toronto-apply-by-december-15-2019/">Postdoctoral position in Computational Social Choice at University of Toronto (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Postdoctoral position beginning in Fall 2020; fellow will work with Prof. Nisarg Shah on topics such as (but not limited to): computational social choice, fairness and incentives in machine learning, algorithmic game theory, and mechanism design. Applicants should have (prior to starting) a PhD in computer science, economics, operations research, or a related field.</p>
<p>Website: <a href="https://www.cs.toronto.edu/theory/positions.html">https://www.cs.toronto.edu/theory/positions.html</a><br />
Email: nisarg@cs.toronto.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/25/postdoctoral-position-in-computational-social-choice-at-university-of-toronto-apply-by-december-15-2019/"><span class="datestr">at November 25, 2019 12:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/24/postdocs-at-university-of-toronto-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/24/postdocs-at-university-of-toronto-apply-by-december-15-2019/">Postdocs at University of Toronto (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The theory group at the University of Toronto anticipates up to three postdoctoral positions beginning September 2020. We seek candidates from all areas of theoretical computer science including algorithms, complexity theory, cryptography, differential privacy, distributed computing, graph theory, quantum computing, and theoretical aspects of machine learning.</p>
<p>Website: <a href="https://www.cs.toronto.edu/theory/positions.html">https://www.cs.toronto.edu/theory/positions.html</a><br />
Email: hyuen@cs.toronto.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/24/postdocs-at-university-of-toronto-apply-by-december-15-2019/"><span class="datestr">at November 24, 2019 11:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/169">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/169">TR19-169 |  On Exponential-Time Hypotheses, Derandomization, and Circuit Lower Bounds | 

	Roei Tell, 

	Lijie Chen, 

	Ron Rothblum, 

	Eylon Yogev</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Exponential-Time Hypothesis ($ETH$) is a strengthening of the $\mathcal{P} \neq \mathcal{NP}$ conjecture, stating that $3\text{-}SAT$ on $n$ variables cannot be solved in time $2^{\epsilon\cdot n}$, for some $\epsilon&gt;0$. In recent years, analogous hypotheses that are ``exponentially-strong'' forms of other classical complexity conjectures (such as $\mathcal{NP}\not\subseteq\mathcal{BPP}$ or $co\text{-}\mathcal{NP}\not\subseteq \mathcal{NP}$) have also been considered. These Exponential-Time Hypotheses have been widely influential across different areas of complexity theory. However, their connections to *derandomization and circuit lower bounds* have yet to be systematically studied. Such study is indeed the focus of the current work, and we prove a sequence of results demonstrating that *the connections between exponential-time hypotheses, derandomization, and circuit lower bounds are remarkably strong*.

First, we show that if $3\text{-}SAT$ (or even $TQBF$) cannot be solved by probabilistic algorithms that run in time $2^{n/\mathrm{polylog}(n)}$, then $\mathcal{BPP}$ can be deterministically simulated ``on average case'' in (nearly-)polynomial-time (i.e., in time $n^{\mathrm{polyloglog}(n)}$). This result addresses a long-standing lacuna in uniform ``hardness-to-randomness'' results, which did not previously extend to such parameter settings. Moreover, we extend this result to support an ``almost-always'' derandomization conclusion from an ``almost-always'' lower bound hypothesis.

Secondly, we show that *disproving* certain exponential-time hypotheses requires proving breakthrough circuit lower bounds. In particular, if $CircuitSAT$ for circuits over $n$ bits of size $\mathrm{poly}(n)$ can be solved by *probabilistic algorithms* in time $2^{n/\mathrm{polylog}(n)}$, then $\mathcal{BPE}$ does not have circuits of quasilinear size. The main novel feature of this result is that we only assume the existence of a *randomized* circuit-analysis algorithm, whereas previous similar results crucially relied on the hypothesis that the circuit-analysis algorithm does not use randomness.

Thirdly, we show that a very weak exponential-time hypothesis is closely-related to the classical question of whether derandomization and circuit lower bounds are *equivalent*. Specifically, we show two-way implications between the hypothesis that the foregoing equivalence holds and the hypothesis that $\mathcal{E}$ cannot be decided by ``small'' circuits that are *uniformly generated* by relatively-efficient non-deterministic machines. This highlights a sufficient-and-necessary path for progress towards proving that derandomization and circuit lower bounds are indeed equivalent.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/169"><span class="datestr">at November 24, 2019 11:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/168">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/168">TR19-168 |  Beyond Natural Proofs: Hardness Magnification and Locality | 

	Ján Pich, 

	Lijie Chen, 

	Shuichi Hirahara, 

	Igor Carboni Oliveira, 

	Ninad Rajgopal, 

	Rahul Santhanam</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Hardness magnification reduces major complexity separations (such as $EXP \not\subseteq NC^1$) to proving lower bounds for some natural problem $Q$ against weak circuit models. Several recent works [OS18, MMW19, CT19, OPS19, CMMW19, Oli19, CJW19a] have established results of this form. In the most intriguing cases, the required lower bound is known for problems that appear to be significantly easier than $Q$, while $Q$ itself is susceptible to lower bounds but these are not yet sufficient for magnification. 

In this work, we provide more examples of this phenomenon, and investigate the prospects of proving new lower bounds using this approach. In particular, we consider the following essential questions associated with the hardness magnification program:

– Does hardness magnification avoid the natural proofs barrier of Razborov and Rudich [RR97]? 
– Can we adapt known lower bound techniques to establish the desired lower bound for $Q$?

We establish that some instantiations of hardness magnification overcome the natural proofs barrier in the following sense: slightly superlinear-size circuit lower bounds for certain versions of the minimum circuit size problem MCSP imply the non-existence of natural proofs. As a corollary of our result, we show that certain magnification theorems not only imply strong worst-case circuit lower bounds but also rule out the existence of efficient learning algorithms. 

Hardness magnification might sidestep natural proofs, but we identify a source of difficulty when trying to adapt existing lower bound techniques to prove strong lower bounds via magnification. This is captured by a locality barrier: existing magnification theorems unconditionally show that the problems $Q$ considered above admit highly efficient circuits extended with small fan-in oracle gates, while lower bound techniques against weak circuit models quite often easily extend to circuits containing such oracles. This explains why direct adaptations of certain lower bounds are unlikely to yield strong complexity separations via hardness magnification.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/168"><span class="datestr">at November 24, 2019 05:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7582">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/11/23/harvard-opportunity-lecturing-advising-position/">Harvard opportunity: lecturing / advising position</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Harvard Computer Science is seeking a Lecturer/Assistant Director of Undergraduate Studies. A great candidate would be someone passionate about teaching and mentoring and excited to build a diverse and inclusive Undergraduate Computer Science community at Harvard. The position requires a Ph.D and is open to all areas of computer science and related fields, but of course personally I would love to have a theorist fill this role.</p>



<p>Key responsibilities are:</p>



<p>* Teach (or co-teach) one undergraduate Computer Science course per semester.</p>



<p>* Join and help lead the Computer Science Undergraduate Advising team (which includes mentoring and advising undergraduate students and developing materials, initiatives, and events to foster a welcoming and inclusive Harvard Computer Science community.)</p>



<p>The job posting with all details is at <a href="https://tiny.cc/harvardadus">https://tiny.cc/harvardadus</a> <br /></p>



<p>Any questions about this position, feel free to contact me or Steve Chong  (the co directors of undergraduate studies for CS at Harvard) at cs-dus at seas.harvard.edu <br /></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/11/23/harvard-opportunity-lecturing-advising-position/"><span class="datestr">at November 23, 2019 05:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=680">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/11/21/manucomic-1/">manucomic #1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<figure class="wp-block-image size-large"><img src="https://emanueleviola.files.wordpress.com/2019/11/manucomic1.jpg?w=746" alt="" class="wp-image-681" /></figure></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2019/11/21/manucomic-1/"><span class="datestr">at November 21, 2019 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/167">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/167">TR19-167 |  UTIME Easy-witness Lemma &amp;amp; Some Consequences | 

	Anant Dhayal, 

	Russell Impagliazzo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove an easy-witness lemma ($\ewl$) for unambiguous non-deterministic verfiers. We show that if $\utime(t)\subset\mathcal{C}$, then for every $L\in\utime(t)$, for every $\utime(t)$ verifier $V$ for $L$, and for every $x\in L$, there is a certificate $y$ satisfing $V(x,y)=1$, that can be encoded as a truth-table of a $\mathcal{C}$ circuit. Our technique is simple compared to the $\ntime$ $\ewl$s \cite{IKW02,Wil-ES13,MW18}, and yields fine-grained results in terms of the time and size parameters. It also works for all {\it typical} non-uniform circuit classes without any additional machinery. Using this $\ewl$ we prove a Karp-Lipton \cite{KL80} style theorem ($\klt$) for $\uexp$. We show that $\uexp\subset\size(poly)\implies\uexp=\ma$. We also prove similar $\ewl$ and $\klt$ for $\uexp\cap\couexp$ and $\fewexp$.

Circuit lower bound techniques that entail natural properties of Razborov and Rudich \cite{RR97} are called natural, and are known to contradict widely believed cryptographic assumptions in the course of proving strong lower bounds. Thus attempts have been made to understand un-natural techniques. Natural properties satisfy three conditions: usefulness, constructiveness, and largeness. Usefulness is unavoidable in any lower-bound technique. In \cite{Wil-NP16,Ig13} it was shown that obtaining $\nexp$ lower bounds is equivalent to obtaining $\pt$-constructive (with $\log n$ advice) properties.

In this paper we consider properties that avoid largeness. We introduce a new notion called unique properties, which is opposite to natural properties in the sense of largeness. A unique property contains exactly one element of each input length (that is a power of 2). We show that $\pt$-constructivity and uniqueness (opposite of largeness) both are unavoidable for certain lower bounds. We prove, $\uexp\cap\couexp\not\subset\mathcal{C}$ if and only if there is a $\pt$-constructive unique property against $\mathcal{C}$. We also establish equivalences between lower bounds against $\uexp$ (with and without advice), and the existence of different restrictions of $\pt$-constructive unique properties that use advice. 

The ``derandomization (of $\bpp$) from uniform/non-uniform lower bounds for $\Gamma$'' type of results are known for $\Gamma=\expo,\nexp,\nexp\cap\conexp,\rexp$ \cite{NW94,BFNW93,IW01,IKW02,Wil-NP16}. Using the above equivalences we obtain a super-set of these results that also includes the classes $\uexp,\uexp\cap\couexp,\zpexp$.  

One important application of the $\nexp$ $\ewl$ and $\klt$ is the connection between fast ($\sat$ and learning) algorithms and $\nexp$ lower bounds \cite{Wil-ES13,FK09,Ig13}. Using our $\utime$ $\ewl$ and $\klt$ we derive connections between fast unambiguous algorithms and $\utime$ lower bounds. Finally we show results that generalize the lower bound frameworks -- that work only for unrestricted Boolean circuits -- such that they work for any restricted typical circuit class. This will help us to get lower bounds against any typical circuit class from fast algorithms that work for that particular class (and not for the super-class of unrestricted Boolean circuits).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/167"><span class="datestr">at November 21, 2019 01:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/166">TR19-166 |  Top-down induction of decision trees: rigorous guarantees and inherent limitations | 

	Guy Blanc, 

	Jane Lange, 

	Li-Yang Tan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the following heuristic for building a decision tree for a function $f : \{0,1\}^n \to \{\pm 1\}$.  Place the most influential variable $x_i$ of $f$ at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the left and right subtrees respectively; terminate once the tree is an $\varepsilon$-approximation of $f$.   We analyze the quality of this heuristic, obtaining near-matching upper and lower bounds:  

$\circ$ Upper bound: For every $f$ with decision tree size $s$ and every $\varepsilon \in (0,\frac1{2})$, this heuristic builds a decision tree of size at most $s^{O(\log(s/\varepsilon)\log(1/\varepsilon))}$. 

$\circ$ Lower bound: For every $\varepsilon \in (0,\frac1{2})$ and $s \le 2^{\tilde{O}(\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that this heuristic builds a decision tree of size $s^{\tilde{\Omega}(\log s)}$. 

We also obtain upper and lower bounds for monotone functions: $s^{O(\sqrt{\log s}/\varepsilon)}$ and $s^{\tilde{\Omega}(\sqrt[4]{\log s }
)}$ respectively.  The lower bound disproves conjectures of Fiat and Pechyony (2004) and Lee (2009).

Our upper bounds yield new algorithms for properly learning decision trees under the uniform distribution.  We show that these algorithms---which are motivated by widely employed and empirically successful top-down decision tree learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees that compare favorably with those of the current fastest algorithm (Ehrenfeucht and Haussler, 1989), and even have certain qualitative advantages. Our lower bounds shed new light on the limitations of these heuristics. 

Finally, we revisit the classic work of Ehrenfeucht and Haussler.  We extend it to give the first uniform-distribution proper learning algorithm that achieves polynomial sample and memory complexity, while matching its state-of-the-art quasipolynomial runtime.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/166"><span class="datestr">at November 20, 2019 07:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3445">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/20/test-of-time-award-call-for-nominations/">Test of time award: call for nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="font-weight: 400;">Please nominate for the SIGecom Test of Time Award.</p>
<p style="font-weight: 400;">The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation.</p>
<p style="font-weight: 400;">To be eligible, a paper or series of papers must be on a topic in the intersection of economics and computation, and must have been first published, in preliminary or final form, in an archival journal or conference proceedings no less than ten years and no more than twenty-five years before the year the award is conferred. Papers for which all authors are deceased at the time the Award Committee makes its decision are not eligible for the award.</p>
<p style="font-weight: 400;">The 2020 SIGecom Test of Time Award will be given for papers published no earlier than 1995 and no later than 2010. <strong>Nominations are due by February 29th, 2020</strong>, and must be made by email to the Award Committee with “2020 ACM SIGecom Test of Time Award” in the subject.</p>
<p style="font-weight: 400;">See details at <a href="https://www.sigecom.org/awardt.html">https://www.sigecom.org/awardt.html</a></p>
<p> </p>
<p style="font-weight: 400;">The 2020 Test of Time Award Committee</p>
<p style="font-weight: 400;">Paul Milgrom, Stanford University</p>
<p style="font-weight: 400;">Noam Nisan, The Hebrew University of Jerusalem</p>
<p style="font-weight: 400;">Éva Tardos (chair), Cornell University</p></div>







<p class="date">
by algorithmicgametheory <a href="https://agtb.wordpress.com/2019/11/20/test-of-time-award-call-for-nominations/"><span class="datestr">at November 20, 2019 07:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/">A Clever Way To Find Compiler Bugs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Your comments are valuable, we thank you.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/profile_image174/" rel="attachment wp-att-16401"><img width="220" alt="" class="alignright  wp-image-16401" src="https://rjlipton.files.wordpress.com/2019/11/profile_image174.jpg?w=220" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.flux.utah.edu/profile/jxyang">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>, “Finding and Understanding Bugs in C Compilers.”</p>
<p>
Today Ken and I discuss a clever idea from that paper.<br />
<span id="more-16399"></span></p>
<p>
The paper was brought to our attention just now in a meaty <a href="https://rjlipton.wordpress.com/2019/10/21/a-polemical-overreach/#comment-106276">comment</a> by Paul D. We thank him for it—the topic interests both of us. We don’t think Paul D. means to be anonymous, but in keeping with that we’ll give just a cryptic hint to his identity: The saying “a man on the make” is widely known, but for more than the millennium he has been the unique person in the world to whom it applies literally.  <b>Update 11/20</b>: Turns out we (I, Ken) were wrong about the identity, see <a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/#comment-106348">this</a>.</p>
<p>
Yang was made unique by being listed out of alphabetical order on the paper. This is notable because the most common practice in our field is to list alphabetically irrespective of prominence. Hence we’ve invented the term ‘DFA’ for “Designated” or “Distinguished” First Author. The other authors are Yang Chen, Eric Eide, and John Regehr, all from the University of Utah. </p>
<p>
</p><p></p><h2> The Topic </h2><p></p>
<p></p><p>
Paul D.’s comment notes that there was evidence that verification methods could improve compiler correctness. By <i>compiler</i> we mean the program that transforms high level code into machine code. These programs are used countless times every day and their correctness is clearly very important. </p>
<p>
Their correctness is tricky for several reasons. The main one is that almost all compilers try to optimize code. That is when they transform code into instructions they try to rewrite or rearrange the instructions to yield better performance. Compilers have been doing this forever. The trouble is that changing instructions to increase performance is dangerous. The changes must not affect the values that are computed. If they are not done carefully they can actually make the answers faster, but incorrect. This is the reason correctness is tricky.</p>
<p>
Formal verification requires a lot of effort. The highest effort should go into mission-critical software. But compilers are mission-critical <em>already</em>, unless we know mission-critical software won’t be compiled on a particular one. Hence it is notable when formal verification makes a compiler more reliable. </p>
<p>
</p><p></p><h2> The Paper </h2><p></p>
<p></p><p>
The idea in the paper Paul referenced is quite elegant. They built a program called Csmith. It operates as follows: </p>
<blockquote><p><b> </b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is a compiler they wish to test. Then generate various legal C programs <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" />. For each of these let <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> be the answer that <img src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{X(P)}" class="latex" title="{X(P)}" /> yields. Here <img src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{X(P)}" class="latex" title="{X(P)}" /> is the compiled program. Then check whether <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is correct. </em>
</p></blockquote>
<p></p><p>
For example:  </p>
<pre>int foo (void) { 
    signed char x = 1; 
    unsigned char y = 255; 
    return x &gt; y; 
} 
</pre>
<p>
Some compilers returned <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, but the correct answer is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. There are further examples in a 2012 companion <a href="https://www.cs.utah.edu/~regehr/papers/pldi12-preprint.pdf">paper</a> and these <a href="https://www.flux.utah.edu/download?uid=115&amp;slides=1&amp;type=pptx">slides</a> from an earlier version. The Csmith <a href="https://embed.cs.utah.edu/csmith/">homepage</a> has long lists of compiler bugs they found. </p>
<p>
Of course if <img src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(P)}" class="latex" title="{X(P)}" /> crashes or refuse to compile <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> then the compiler is wrong. But what happens if <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is computed. How does Csmith know if the answer is correct? This seems to be really hard. This correctness testing must be automated: the whole approach is based on allowing tons of random programs to be tested. They cannot assume that humans will be used to check the outputs.</p>
<p>
This is the clever idea of this paper. They assume that there are at least two compilers say <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" />. Then let <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> be the output of <img src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(P)}" class="latex" title="{X(P)}" /> and let <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> be the output of <img src="https://s0.wp.com/latex.php?latex=%7BY%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y(P)}" class="latex" title="{Y(P)}" />. The key insight is: </p>
<blockquote><p>
<b>If <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is not equal to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, then one of the compilers is wrong</b>.
</p></blockquote>
<p>
A very neat and elegant idea. For software in general it is called <a href="https://en.wikipedia.org/wiki/Differential_testing">differential</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.445">testing</a>. </p>
<p>
This at least alerts when there are problems with some compilers and some programs. One can use this trick to discover programs that cause at least some compilers to have problems. This is extremely valuable. It allowed Csmith to discover hundreds of errors in production compilers—errors that previously were missed.</p>
<p>
</p><p></p><h2> Smart Fuzzing </h2><p></p>
<p></p><p>
<a href="https://en.wikipedia.org/wiki/Fuzzing">Fuzzing</a> is defined by Wikipedia as testing by “providing invalid, unexpected, or random data as inputs to a computer program.” An early historical example, Apple’s “<a href="https://en.wikipedia.org/wiki/Monkey_testing">Monkey</a>” program, worked completely randomly. To ensure that the found bugs are <em>meaningful</em> and <em>analyzable</em>, Csmith needed a deeper, structured, “intelligent” design, not just the generation of <a href="https://en.wikipedia.org/wiki/Mayhem_(advertising_character)">Mayhem</a>.</p>
<p>
For one, Csmith needed to avoid programs <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> than do not have deterministic behavior. The formal C standards itemize cases in which compilers are allowed to have arbitrary, even self-inconsistent, behavior. There are lots of them in C. A bug with dubious code could be dismissed out of hand.</p>
<p>
For another, the probability that a program <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> built haphazardly by the original Csmith version would reveal bugs was observed to peak at about 80KB source-code size, about 1,000 lines across multiple pages. Those don’t make great examples. So Csmith has its own routines to compress bug instances it has found. Simple tricks are shortening numerical expressions to use only the bug-sensitive parts. Others are lifting local variables out of blocks and bypassing pointer jumps.</p>
<p>
A third goal is that the generator should branch out to all aspects of the language—in this case, C—not just the “grungy” parts that are ripe for finding compiler bugs. The paper talks about this at length. Regehr, who was Yang’s advisor, is also a blogger. His current <a href="https://blog.regehr.org/archives/1700">post</a>, dated November 4, is titled, “Helping Generative Fuzzers Avoid Looking Only Where the Light is Good, Part 1.” We guess that “Part 2” will go even more into details.</p>
<p>
</p><p></p><h2> Formal Methods as Bugscreen </h2><p></p>
<p></p><p>
Regarding the formally-verified CompCert compiler, Paul D. quoted from the <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>:</p>
<blockquote><p><b> </b> <em> The striking thing about our CompCert results is that the middle-end bugs we found in all other compilers are absent. As of early 2011, the under-development version of CompCert is the only compiler we have tested for which Csmith cannot find wrong-code errors. This is not for lack of trying: we have devoted about six CPU-years to the task. The apparent unbreakability of CompCert supports a strong argument that developing compiler optimizations within a proof framework, where safety checks are explicit and machine-checked, has tangible benefits for compiler users. </em>
</p></blockquote>
<p></p><p>
This August 2019 <a href="https://arxiv.org/pdf/1902.09334.pdf">paper</a> by Michaël Marcozzi, Qiyi Tang, Alastair Donaldson, and Cristian Cadar gives recent results involving Csmith and other tools. They have an interesting discussion on page 2, from which we excerpt:</p>
<blockquote><p><b> </b> <em> In our experience working in the area […], we have found compiler fuzzing to be a contentious topic. Research talks on compiler fuzzing are often followed by questions about the importance of the discovered bugs, and whether compiler fuzzers might be improved by taking inspiration from bugs encountered by users of compilers “in the wild.” Some … argue that any miscompilation bug, whether fuzzer-found or not, is a ticking bomb that should be regarded as severe, or avoided completely via formal verification (in the spirit of CompCert). </em>
</p></blockquote>
<p></p><p>
They go on to say, however, that when a fully-developed compiler is used for non-critical software, the kinds of bugs typically found by fuzzing tend to have questionable importance. Their paper is titled, “A Systematic Impact Study for Fuzzer-Found Compiler Bugs.” </p>
<p>
So far they have found definite results that seem to have mixed implications. In their future-work section they note that they have evaluated the impact of bugs in compilers on the intended function of programs they compile, but not on possible security holes—which as we noted in our Cloudflare <a href="https://rjlipton.wordpress.com/2017/03/08/is-computer-security-possible/">post</a> can come from (misuse of) simple code that is completely correct. This leads us further to wonder, coming full-circle, whether formal methods might help quantify the relative importance of aspects of a language and areas of a compiler to guide more-intelligent generation of test cases.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
The above comment is interesting, but perhaps finding obscure bugs is important. Perhaps such bugs could be used to attack systems. That is perhaps some one could use them to break into a system. Security may be compromised by any error, even an unlikely one to occur in the wild. </p>
<p>
What do you think?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/"><span class="datestr">at November 20, 2019 12:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/">A Market for TCS Papers??</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>By David Eppstein &amp; Vijay Vazirani</em></p>
<p>No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – a matching market – for matching papers to conferences.</p>
<p>First, a short preamble on how the idea emerged.</p>
<p><strong>Preamble</strong> (by Vijay):  Soon after my recent <a href="https://www.youtube.com/watch?v=MLr6Ud5qmt4&amp;t=74s"><u>Simons talk</u></a> on Matching Markets, I sent its url to Al Roth. Obviously, I wasn’t expecting a return email. However, the perfect gentleman and ultimate scholar that Al is, he did reply, and mentioned that he did not like my “definition” of matching markets and said, “I guess I would say matching markets are markets because they aggregate information that is held by the participants, which is what markets do (even if they don’t use prices to do it..).” This hit me like lightening from the sky – suddenly it crystallized the innate intuition about markets which I had formed through work on algorithmic aspects of markets! I thanked Al profusely and added, “This definitely helps in me get the right perspective on the notion!”</p>
<p>About a week ago, while updating my talk for a seminar at Columbia University, I included this beautiful insight in it and then a thought occurred: Each PC meeting involves aggregation of information from a large number of agents: PC members as well as external experts. Hence, isn’t a conference a matching market? Excitedly, I sent this question to Al. He replied, “… the conference process, matching papers to conferences, is a market and a particular conference might be a marketplace … ”</p>
<p>When I returned home, my esteemed colleague, David Eppstein, stunned me by declaring that he had thought of a market relevant to our field in which no money changes hands. I immediately knew he was thinking of the conference process. But he got to it out of the blue … and not the long process it took me!</p>
<p><strong>Back to the idea:  </strong>In the past, matching markets have brought immense efficiency and order in allocation problems in which use of money is considered repugnant, the prime examples being matching medical residents to hospitals, kidney exchange, and assignment of students of a large city to its schools.</p>
<p>At present we are faced with massive inefficiencies in the conference process – numerous researchers are trapped in unending cycles of submit … get reject … incorporate comments … resubmit — often to the next deadline which has been conveniently arranged a couple of days down the road so the unwitting participants are conditioned into mindlessly keep coming back for more, much like Pavlov’s dog.</p>
<p>We are proposing a matching market approach to finally obliterate this madness. We believe such a market is feasible using the following ideas. No doubt our scheme will have some drawbacks; however, as should be obvious, the advantages far outweigh them.</p>
<p>First, for co-located symposia within a larger umbrella conference, such as the<br />
conferences within ALGO or FCRC, the following process should be a no-brainer:</p>
<p>1). Ensure a common deadline for all symposia; denote the latter by <em>S.</em></p>
<p>2). Let <em>R</em> denote the set of researchers who wish to submit one paper to a symposium in this umbrella conference – assume that researchers submitting more than one paper will have multiple names, one for each submission. Each researcher will provide a strict preference order over the subset of symposia to which they wish to submit their paper. Let <em>G</em> denote the bipartite graph with vertex sets (<em>R, S</em>) and an edge (<em>r, s</em>) only if researcher <em>r</em> chose symposium <em>s.</em></p>
<p>3). The umbrella conference will have a large common PC with experts representing all of its symposia. The process of assigning papers to PC members will of course use <em>G</em> in a critical way.</p>
<p>Once papers are reviewed by PC members and external reviewers, each symposium will rank its submissions using its own criteria of acceptance. We believe the overhead of ranking each paper multiple times is minimal since that is just an issue of deciding how “on-topic” a paper is – an easy task once the reviews of the paper are available.</p>
<p>4). Finally, using all these preference lists, a researcher-proposing stable matching is computed using the Gale-Shapley algorithm. As is well-known, this mechanism will be dominant strategy incentive compatible for researchers.</p>
<p>With a little extra effort, a similar scheme can also be used for a group of conferences at diverse locations but similar times, such as some of the annual summer theory conferences, STOC, ICALP, ESA, STAC, WADS/SWAT, etc.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/"><span class="datestr">at November 19, 2019 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3436178446517561376">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html">Fields used to be closer together than they are now. Good? Bad?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
There was a retired software Eng professor that I had heard two very non-controversial rumors about:<br />
<br />
1) He got his PhD in Numerical Analysis<br />
<br />
2) He got his PhD in Compiler Optimization.<br />
<br />
So I asked him which was true.<br />
<br />
The answer: Both! In those days you had to optimize your code to get your NA code to run fast enough.<br />
<br />
We cannot imagine that anymore. Or at least I cannot.<br />
<br />
Over time the fields of computer science advance more so its hard to be  master of more than one field.  But its not that simple: there has been work recently applying Machine Learning to... well<br />
everything really. Even so, I think the trend is more towards separation. Or perhaps it oscillates.<br />
<br />
I am NOT going to be the grumpy old man (Google once thought I was 70, see <a href="https://blog.computationalcomplexity.org/2018/10/google-added-years-to-my-life.html">here</a>) who says things were better in my day when the fields were closer together. But I will ask the question:<br />
<br />
1) Are people more specialized new? While I think yes since each field has gotten more complicated and harder to master. There are exceptions: Complexity theory uses much more sophisticated mathematics then when I was a grad student (1980-1985), and of course Quantum Computing has lead to more comp sci majors knowing physics.<br />
<br />
2) Is it good for the field that people are specialized? I am supposed to say that it is terrible and that great advances are made when people are interdiscplinary. But there are many more small advances that are made by someone who has a mastery of one (or two) fields.<br />
<br />
3) The PhD Process and the Tenure Process encourage specialization. This I think IS bad since there are different modes of research that should all be respected.'<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html"><span class="datestr">at November 18, 2019 04:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/165">TR19-165 |  Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning | 

	Clement Canonne, 

	Xi Chen, 

	Gautam Kamath, 

	Amit Levi, 

	Erik Waingarten</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/165"><span class="datestr">at November 18, 2019 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4414">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4414">The Aaronson-Ambainis Conjecture (2008-2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><em>(see new update at end of post)</em></p>



<p>Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that <a href="https://arxiv.org/abs/cs/9811023">Fortnow and Rogers</a> suggested in a paper: is it possible to separate <a href="https://en.wikipedia.org/wiki/P_(complexity)">P</a> from <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> relative to a <a href="https://en.wikipedia.org/wiki/Random_oracle">random oracle</a>?  (That is, without first needing to separate P from PSPACE or whatever in the real world?)  Or to the contrary: suppose that a quantum algorithm Q makes T queries to a Boolean input string X.  Is there then a classical simulation algorithm that makes poly(T) queries to X, and that approximates Q’s acceptance probability for <em>most</em> values of X?  Such a classical simulation, were it possible, would still be consistent with the existence of quantum algorithms like <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon’s</a> and <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s</a>, which are able to achieve exponential (and even greater) speedups in the black-box setting.  It would simply demonstrate the importance, for Simon’s and Shor’s algorithms, of global structure that makes the string X extremely <em>non</em>-random: for example, encoding a periodic function (in the case of Shor’s algorithm), or encoding a function that hides a secret string s (in the case of Simon’s).  It would underscore that superpolynomial quantum speedups depend on structure.</p>



<p>I never managed to solve this problem.  Around 2008, though, I noticed that a solution would follow from a perhaps-not-obviously-related conjecture, about <em>influences</em> in low-degree polynomials.  Namely, let p:R<sup>n</sup>→R be a degree-d real polynomial in n variables, and suppose p(x)∈[0,1] for all x∈{0,1}<sup>n</sup>.  Define the <i>variance</i> of p to be<br />   Var(p):=E<sub>x,y</sub>[|p(x)-p(y)|],<br />and define the <i>influence</i> of the i<sup>th</sup> variable to be<br />   Inf<sub>i</sub>(p):=E<sub>x</sub>[|p(x)-p(x<sup>i</sup>)|].<br />Here the expectations are over strings in {0,1}<sup>n</sup>, and x<sup>i</sup> means x with its i<sup>th</sup> bit flipped between 0 and 1.  Then the conjecture is this: there must be some variable i such that Inf<sub>i</sub>(p) ≥ poly(Var(p)/d) (in other words, that “explains” a non-negligible fraction of the variance of the entire polynomial).</p>



<p>Why would this conjecture imply the statement about quantum algorithms?  Basically, because of the seminal result of <a href="https://arxiv.org/abs/quant-ph/9802049">Beals et al.</a> from 1998: that if a quantum algorithm makes T queries to a Boolean input X, then its acceptance probability can be written as a real polynomial over the bits of X, of degree at most 2T.  Given that result, if you wanted to classically simulate a quantum algorithm Q on most inputs—and if you only cared about query complexity, not computation time—you’d simply need to do the following:<br />(1) Find the polynomial p that represents Q’s acceptance probability.<br />(2) Find a variable i that explains at least a 1/poly(T) fraction of the total remaining variance in p, and query that i.<br />(3) Keep repeating step (2), until p has been restricted to a polynomial with not much variance left—i.e., to nearly a constant function p(X)=c.  Whenever that happens, halt and output the constant c.<br />The key is that by hypothesis, this algorithm will halt, with high probability over X, after only poly(T) steps.</p>



<p>Anyway, around the same time, Andris Ambainis had a major break on a different problem that I’d told him about: namely, whether randomized and quantum query complexities are polynomially related for all partial functions with permutation symmetry (like the collision and the element distinctness functions).  Andris and I decided to write up the two directions jointly.  The result was our 2011 paper entitled <a href="https://arxiv.org/abs/0911.0996">The Need for Structure in Quantum Speedups</a>.</p>



<p>Of the two contributions in the “Need for Structure” paper, the one about random oracles and influences in low-degree polynomials was clearly the weaker and less satisfying one.  As the reviewers pointed out, that part of the paper didn’t solve anything: it just reduced one unsolved problem to a new, slightly different problem that was <em>also</em> unsolved.  Nevertheless, that part of the paper acquired a life of its own over the ensuing decade, as the world’s experts in analysis of Boolean functions and polynomials began referring to the “Aaronson-Ambainis Conjecture.”  Ryan O’Donnell, Guy Kindler, and many others had a stab.  I even got Terry Tao to spend an hour or two on the problem when I visited UCLA.</p>



<p>Now, at long last, Nathan Keller and Ohad Klein say they’ve proven the Aaronson-Ambainis Conjecture, in a preprint whose title is a riff on ours: <a href="https://arxiv.org/abs/1911.03748">“Quantum Speedups Need Structure.”</a></p>



<p>Their paper hasn’t yet been peer-reviewed, and I haven’t yet carefully studied it, but I <em>could</em> and <em>should</em>: at 19 pages, it looks very approachable and clear, if not as radically short as (say) <a href="https://www.scottaaronson.com/blog/?p=4229">Huang’s proof of the Sensitivity Conjecture</a>.  Keller and Klein’s argument subsumes all the earlier results that I knew would need to be subsumed, and involves all the concepts (like a real analogue of block sensitivity) that I knew would need to be involved somehow.</p>



<p>My plan had been as follows:<br />(1) Read their paper in detail.  Understand every step of their proof.<br />(2) Write a blog post that reflects my detailed understanding.</p>



<p>Unfortunately, this plan did not sufficiently grapple with the fact that I now have two kids.  It got snagged for a week at step (1).  So I’m now executing an alternative plan, which is to jump immediately to the blog post.</p>



<p>Assuming Keller and Klein’s result holds up—as I expect it will—by combining it with the observations in my and Andris’s paper, one immediately gets an explanation for why no one has managed to separate P from BQP relative to a <em>random</em> oracle, but only relative to non-random oracles.  This complements the work of <a href="https://www.uncg.edu/mat/faculty/cdsmyth/thesis.pdf">Kahn, Saks, and Smyth</a>, who around 2000 gave a precisely analogous explanation for the difficulty of separating P from NP∩coNP relative to a random oracle.</p>



<p>Unfortunately, the polynomial blowup is quite enormous: from a quantum algorithm making T queries, Keller and Klein apparently get a classical algorithm making O(T<sup>18</sup>) queries.  But such things can almost always be massively improved.</p>



<p>Feel free to use the comments to ask any questions about this result or its broader context.  I’ll either do my best to answer from the limited amount I know, or else I’ll pass the questions along to Nathan and Ohad themselves.  Maybe, at some point, I’ll even be forced to understand the new proof.</p>



<p>Congratulations to Nathan and Ohad!</p>



<p><strong><font color="red">Update (Nov. 20):</font></strong> Tonight I finally did what I should’ve done two weeks ago, and worked through the paper from start to finish.  Modulo some facts about noise operators, hypercontractivity, etc. that I took on faith, I now have a reasonable (albeit imperfect) understanding of the proof.  It’s great!</p>



<p>In case it’s helpful to anybody, here’s my one-paragraph summary of how it works.  First, you hit your bounded degree-d function f with a random restriction to attenuate its higher-degree Fourier coefficients (reminiscent of <a href="http://www.ma.huji.ac.il/~ehudf/courses/anal09/LMN.pdf">Linial-Mansour-Nisan</a>).  Next, in that attenuated function, you find a small “coalition” of influential variables—by which we mean, a set of variables for which there’s <em>some</em> assignment that substantially biases f.  You keep iterating—finding influential coalitions in subfunctions on n/4, n/8, etc. variables.  All the while, you keep track of <em>the norm of the vector of all the block-sensitivities of all the inputs</em> (the authors don’t clearly explain this in the intro, but they reveal it near the end).  Every time you find another influential coalition, that norm goes down by a little, but by approximation theory, it can only go down O(d<sup>2</sup>) times until it hits rock bottom and your function is nearly constant.  By the end, you’ll have approximated f itself by a decision tree of depth poly(d, 1/ε, log(n)).  Finally, you get rid of the log(n) term by using the fact that f essentially depended on at most exp(O(d)) variables anyway. </p>



<p>Anyway, I’m not sure how helpful it is to write more: the <a href="https://arxiv.org/pdf/1911.03748.pdf">paper itself</a> is about 95% as clear as it could possibly be, and even where it isn’t, you’d probably need to read it first (and, uh, know something about influences, block sensitivity, random restrictions, etc.) before any further clarifying remarks would be of use.  But happy to discuss more in the comments, if anyone else is reading it.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4414"><span class="datestr">at November 17, 2019 11:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
